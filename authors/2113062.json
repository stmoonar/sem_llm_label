{
    "authorId": "2113062",
    "papers": [
        {
            "paperId": "7947d22cb949c9a8e11b381ea5611a792955b844",
            "title": "Deep Non-Parametric Time Series Forecaster",
            "abstract": "This paper presents non-parametric baseline models for time series forecasting. Unlike classical forecasting models, the proposed approach does not assume any parametric form for the predictive distribution and instead generates predictions by sampling from the empirical distribution according to a tunable strategy. By virtue of this, the model is always able to produce reasonable forecasts (i.e., predictions within the observed data range) without fail unlike classical models that suffer from numerical stability on some data distributions. Moreover, we develop a global version of the proposed method that automatically learns the sampling strategy by exploiting the information across multiple related time series. The empirical evaluation shows that the proposed methods have reasonable and consistent performance across all datasets, proving them to be strong baselines to be considered in one's forecasting toolbox.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "26320653",
                    "name": "Syama Sundar Rangapuram"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "2066263165",
                    "name": "Lorenzo Stella"
                },
                {
                    "authorId": "2067154581",
                    "name": "Valentin Flunkert"
                },
                {
                    "authorId": "2276206710",
                    "name": "David Salinas"
                },
                {
                    "authorId": "2276262553",
                    "name": "Yuyang Wang"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                }
            ]
        },
        {
            "paperId": "3145014e65861f3981a652222daf964bca3bbe62",
            "title": "Diverse Counterfactual Explanations for Anomaly Detection in Time Series",
            "abstract": "Data-driven methods that detect anomalies in times series data are ubiquitous in practice, but they are in general unable to provide helpful explanations for the predictions they make. In this work we propose a model-agnostic algorithm that generates counterfactual ensemble explanations for time series anomaly detection models. Our method generates a set of diverse counterfactual examples, i.e, multiple perturbed versions of the original time series that are not considered anomalous by the detection model. Since the magnitude of the perturbations is limited, these counterfactuals represent an ensemble of inputs similar to the original time series that the model would deem normal. Our algorithm is applicable to any differentiable anomaly detection model. We investigate the value of our method on univariate and multivariate real-world datasets and two deep-learning-based anomaly detection models, under several explainability criteria previously proposed in other data domains such as Validity, Plausibility, Closeness and Diversity. We show that our algorithm can produce ensembles of counterfactual examples that satisfy these criteria and thanks to a novel type of visualisation, can convey a richer interpretation of a model's internal mechanism than existing methods. Moreover, we design a sparse variant of our method to improve the interpretability of counterfactual explanations for high-dimensional time series anomalies. In this setting, our explanation is localised on only a few dimensions and can therefore be communicated more efficiently to the model's user.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2285351482",
                    "name": "Deborah Sulem"
                },
                {
                    "authorId": "2192704",
                    "name": "Michele Donini"
                },
                {
                    "authorId": "2625318",
                    "name": "M. B. Zafar"
                },
                {
                    "authorId": "51055581",
                    "name": "Fran\u00e7ois-Xavier Aubet"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "2155897297",
                    "name": "Sanjiv Das"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "356069daf4d4628559da92c9245aad39e578e090",
            "title": "Multivariate Quantile Function Forecaster",
            "abstract": "We propose Multivariate Quantile Function Forecaster (MQF$^2$), a global probabilistic forecasting method constructed using a multivariate quantile function and investigate its application to multi-horizon forecasting. Prior approaches are either autoregressive, implicitly capturing the dependency structure across time but exhibiting error accumulation with increasing forecast horizons, or multi-horizon sequence-to-sequence models, which do not exhibit error accumulation, but also do typically not model the dependency structure across time steps. MQF$^2$ combines the benefits of both approaches, by directly making predictions in the form of a multivariate quantile function, defined as the gradient of a convex function which we parametrize using input-convex neural networks. By design, the quantile function is monotone with respect to the input quantile levels and hence avoids quantile crossing. We provide two options to train MQF$^2$: with energy score or with maximum likelihood. Experimental results on real-world and synthetic datasets show that our model has comparable performance with state-of-the-art methods in terms of single time step metrics while capturing the time dependency structure.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "49283927",
                    "name": "Kelvin K. Kan"
                },
                {
                    "authorId": "150098869",
                    "name": "Franccois-Xavier Aubet"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "1845861038",
                    "name": "Youngsuk Park"
                },
                {
                    "authorId": "1915597",
                    "name": "Konstantinos Benidis"
                },
                {
                    "authorId": "49418655",
                    "name": "Lars Ruthotto"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                }
            ]
        },
        {
            "paperId": "363d88aa10012b4469670a1d7ba7c8602e6b8e61",
            "title": "Intrinsic Anomaly Detection for Multi-Variate Time Series",
            "abstract": "We introduce a novel, practically relevant variation of the anomaly detection problem in multi-variate time series: intrinsic anomaly detection. It appears in diverse practical scenarios ranging from DevOps to IoT, where we want to recognize failures of a system that operates under the influence of a surrounding environment. Intrinsic anomalies are changes in the functional dependency structure between time series that represent an environment and time series that represent the internal state of a system that is placed in said environment. We formalize this problem, provide under-studied public and new purpose-built data sets for it, and present methods that handle intrinsic anomaly detection. These address the short-coming of existing anomaly detection methods that cannot differentiate between expected changes in the system's state and unexpected ones, i.e., changes in the system that deviate from the environment's influence. Our most promising approach is fully unsupervised and combines adversarial learning and time series representation learning, thereby addressing problems such as label sparsity and subjectivity, while allowing to navigate and improve notoriously problematic anomaly detection data sets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "29938263",
                    "name": "Stephan Rabanser"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2101840120",
                    "name": "Oliver Borchert"
                },
                {
                    "authorId": "4708988",
                    "name": "Richard Kurle"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "1405223072",
                    "name": "Michael Bohlke-Schneider"
                },
                {
                    "authorId": "1967156",
                    "name": "Nicolas Papernot"
                },
                {
                    "authorId": "2067154581",
                    "name": "Valentin Flunkert"
                }
            ]
        },
        {
            "paperId": "3da74e3763e15e5c964d92a736be2822901a0826",
            "title": "On the detrimental effect of invariances in the likelihood for variational inference",
            "abstract": "Variational Bayesian posterior inference often requires simplifying approximations such as mean-field parametrisation to ensure tractability. However, prior work has associated the variational mean-field approximation for Bayesian neural networks with underfitting in the case of small datasets or large model sizes. In this work, we show that invariances in the likelihood function of over-parametrised models contribute to this phenomenon because these invariances complicate the structure of the posterior by introducing discrete and/or continuous modes which cannot be well approximated by Gaussian mean-field distributions. In particular, we show that the mean-field approximation has an additional gap in the evidence lower bound compared to a purpose-built posterior that takes into account the known invariances. Importantly, this invariance gap is not constant; it vanishes as the approximation reverts to the prior. We proceed by first considering translation invariances in a linear model with a single data point in detail. We show that, while the true posterior can be constructed from a mean-field parametrisation, this is achieved only if the objective function takes into account the invariance gap. Then, we transfer our analysis of the linear model to neural networks. Our analysis provides a framework for future work to explore solutions to the invariance problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4708988",
                    "name": "Richard Kurle"
                },
                {
                    "authorId": "3234984",
                    "name": "R. Herbrich"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "49416149",
                    "name": "Bernie Wang"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                }
            ]
        },
        {
            "paperId": "4d0ba2094b0e2adf9a6ff02e92892ea97fefe872",
            "title": "Online Time Series Anomaly Detection with State Space Gaussian Processes",
            "abstract": "We propose r-ssGPFA, an unsupervised online anomaly detection model for uni- and multivariate time series building on the efficient state space formulation of Gaussian processes. For high-dimensional time series, we propose an extension of Gaussian process factor analysis to identify the common latent processes of the time series, allowing us to detect anomalies efficiently in an interpretable manner. We gain explainability while speeding up computations by imposing an orthogonality constraint on the mapping from the latent to the observed. Our model's robustness is improved by using a simple heuristic to skip Kalman updates when encountering anomalous observations. We investigate the behaviour of our model on synthetic data and show on standard benchmark datasets that our method is competitive with state-of-the-art methods while being computationally cheaper.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1491231041",
                    "name": "Christian Bock"
                },
                {
                    "authorId": "150098869",
                    "name": "Franccois-Xavier Aubet"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "2063962275",
                    "name": "Andrey Kan"
                },
                {
                    "authorId": "2108633020",
                    "name": "Ming Chen"
                },
                {
                    "authorId": "11788397",
                    "name": "Laurent Callot"
                }
            ]
        },
        {
            "paperId": "aae275290596b3e721cd32dfad5635d4c6f6132f",
            "title": "8th SIGKDD International Workshop on Mining and Learning from Time Series -- Deep Forecasting: Models, Interpretability, and Applications",
            "abstract": "Time series data are ubiquitous, and is one of the fastest growing and richest types of data. Recent advances in sensing technologies has resulted in a rapid growth in the size and complexity of time series archives. This demands development of new tools and solutions. The goals of this workshop are to: (1) highlight the significant challenges that underpin learning and mining from time series data (e.g. irregular sampling, spatiotemporal structure, uncertainty quantification), (2) discuss recent algorithmic, theoretical, statistical, or systems-based developments for tackling these problems, and (3) exploring new frontiers in time series analysis and their connections with important topics such as knowledge representation, reasoning, control, and business intelligence. In summary, our workshop will focus on both the theoretical and practical aspects of time series data analysis and will provide a platform for researchers and practitioners from both academia and industry to discuss potential research directions, key technical issues, and present solutions to tackle related issues in practical applications. We will invite researchers and practitioners from the related areas of AI, machine learning, data science, statistics, and many others to contribute to this workshop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144735626",
                    "name": "S. Purushotham"
                },
                {
                    "authorId": "2181310178",
                    "name": "Jun Huan"
                },
                {
                    "authorId": "2113392954",
                    "name": "Cong Shen"
                },
                {
                    "authorId": "2451800",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "49416149",
                    "name": "Bernie Wang"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "31743390",
                    "name": "Hilaf Hasson"
                },
                {
                    "authorId": "1845861038",
                    "name": "Youngsuk Park"
                },
                {
                    "authorId": "145260557",
                    "name": "Sungyong Seo"
                },
                {
                    "authorId": "2246897295",
                    "name": "Yuriy Nevmyvaka"
                }
            ]
        },
        {
            "paperId": "0b85207ac9a9271850fe8c44f5e8c84d00ff3409",
            "title": "A Study of Joint Graph Inference and Forecasting",
            "abstract": "We study a recent class of models which uses graph neural networks (GNNs) to improve forecasting in multivariate time series. The core assumption behind these models is that there is a latent graph between the time series (nodes) that governs the evolution of the multivariate time series. By parameterizing a graph in a differentiable way, the models aim to improve forecasting quality. We compare four recent models of this class on the forecasting task. Further, we perform ablations to study their behavior under changing conditions, e.g., when disabling the graph-learning modules and providing the ground-truth relations instead. Based on our findings, we propose novel ways of combining the existing architectures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3156540",
                    "name": "Daniel Z\u00fcgner"
                },
                {
                    "authorId": "51055581",
                    "name": "Fran\u00e7ois-Xavier Aubet"
                },
                {
                    "authorId": "73240341",
                    "name": "Victor Garcia Satorras"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "3075189",
                    "name": "Stephan G\u00fcnnemann"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                }
            ]
        },
        {
            "paperId": "46bdaaedc16ad932f9fac0642d94817bb0e4df09",
            "title": "Neural Contextual Anomaly Detection for Time Series",
            "abstract": "We introduce Neural Contextual Anomaly Detection (NCAD), a framework for anomaly detection on time series that scales seamlessly from the unsupervised to supervised setting, and is applicable to both univariate and multivariate time series. This is achieved by combining recent developments in representation learning for multivariate time series, with techniques for deep anomaly detection originally developed for computer vision that we tailor to the time series setting. Our window-based approach facilitates learning the boundary between normal and anomalous classes by injecting generic synthetic anomalies into the available data. NCAD can effectively take advantage of domain knowledge and of any available training labels. We demonstrate empirically on standard benchmark datasets that our approach obtains a state-of-the-art performance in the supervised, semi-supervised, and unsupervised settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40341648",
                    "name": "Chris U. Carmona"
                },
                {
                    "authorId": "150098869",
                    "name": "Franccois-Xavier Aubet"
                },
                {
                    "authorId": "2067154581",
                    "name": "Valentin Flunkert"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                }
            ]
        },
        {
            "paperId": "506ed2b755c8b207c4678d05c292e43b4d30605b",
            "title": "Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting",
            "abstract": "Quantile regression is an effective technique to quantify uncertainty, fit challenging underlying distributions, and often provide full probabilistic predictions through joint learnings over multiple quantile levels. A common drawback of these joint quantile regressions, however, is \\textit{quantile crossing}, which violates the desirable monotone property of the conditional quantile function. In this work, we propose the Incremental (Spline) Quantile Functions I(S)QF, a flexible and efficient distribution-free quantile estimation framework that resolves quantile crossing with a simple neural network layer. Moreover, I(S)QF inter/extrapolate to predict arbitrary quantile levels that differ from the underlying training ones. Equipped with the analytical evaluation of the continuous ranked probability score of I(S)QF representations, we apply our methods to NN-based times series forecasting cases, where the savings of the expensive re-training costs for non-trained quantile levels is particularly significant. We also provide a generalization error analysis of our proposed approaches under the sequence-to-sequence setting. Lastly, extensive experiments demonstrate the improvement of consistency and accuracy errors over other baselines.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1845861038",
                    "name": "Youngsuk Park"
                },
                {
                    "authorId": "40792304",
                    "name": "Danielle C. Maddix"
                },
                {
                    "authorId": "150098869",
                    "name": "Franccois-Xavier Aubet"
                },
                {
                    "authorId": "49283927",
                    "name": "Kelvin K. Kan"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "2145199388",
                    "name": "Yuyang Wang"
                }
            ]
        }
    ]
}