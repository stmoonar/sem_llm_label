{
    "authorId": "1685102",
    "papers": [
        {
            "paperId": "190934f87cbe8aead42ed28d3f15e50164bfea1f",
            "title": "A survey on the impact of AI-based recommenders on human behaviours: methodologies, outcomes and future directions",
            "abstract": "Recommendation systems and assistants (in short, recommenders) are ubiquitous in online platforms and influence most actions of our day-to-day lives, suggesting items or providing solutions based on users' preferences or requests. This survey analyses the impact of recommenders in four human-AI ecosystems: social media, online retail, urban mapping and generative AI ecosystems. Its scope is to systematise a fast-growing field in which terminologies employed to classify methodologies and outcomes are fragmented and unsystematic. We follow the customary steps of qualitative systematic review, gathering 144 articles from different disciplines to develop a parsimonious taxonomy of: methodologies employed (empirical, simulation, observational, controlled), outcomes observed (concentration, model collapse, diversity, echo chamber, filter bubble, inequality, polarisation, radicalisation, volume), and their level of analysis (individual, item, model, and systemic). We systematically discuss all findings of our survey substantively and methodologically, highlighting also potential avenues for future research. This survey is addressed to scholars and practitioners interested in different human-AI ecosystems, policymakers and institutional stakeholders who want to understand better the measurable outcomes of recommenders, and tech companies who wish to obtain a systematic view of the impact of their recommenders.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3145906",
                    "name": "Luca Pappalardo"
                },
                {
                    "authorId": "145284780",
                    "name": "Emanuele Ferragina"
                },
                {
                    "authorId": "150293810",
                    "name": "Salvatore Citraro"
                },
                {
                    "authorId": "1796264540",
                    "name": "Giuliano Cornacchia"
                },
                {
                    "authorId": "1717192",
                    "name": "M. Nanni"
                },
                {
                    "authorId": "2292395168",
                    "name": "Giulio Rossetti"
                },
                {
                    "authorId": "2302559951",
                    "name": "Gizem Gezici"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "2309247025",
                    "name": "Margherita Lalli"
                },
                {
                    "authorId": "153796118",
                    "name": "D. Gambetta"
                },
                {
                    "authorId": "2243455722",
                    "name": "Giovanni Mauro"
                },
                {
                    "authorId": "1869835702",
                    "name": "Virginia Morini"
                },
                {
                    "authorId": "2148682266",
                    "name": "Valentina Pansanella"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        },
        {
            "paperId": "343565145bd1d5cad907dec579491b719f1004a3",
            "title": "AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems",
            "abstract": "Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261749096",
                    "name": "Clara Punzi"
                },
                {
                    "authorId": "24269254",
                    "name": "Roberto Pellungrini"
                },
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        },
        {
            "paperId": "3a33c229432f0603f813d9cafc054805b3ea3d82",
            "title": "Understanding Any Time Series Classifier with a Subsequence-based Explainer",
            "abstract": "The growing availability of time series data has increased the usage of classifiers for this data type. Unfortunately, state-of-the-art time series classifiers are black-box models and, therefore, not usable in critical domains such as healthcare or finance, where explainability can be a crucial requirement. This paper presents a framework to explain the predictions of any black-box classifier for univariate and multivariate time series. The provided explanation is composed of three parts. First, a saliency map highlighting the most important parts of the time series for the classification. Second, an instance-based explanation exemplifies the black-box\u2019s decision by providing a set of prototypical and counterfactual time series. Third, a factual and counterfactual rule-based explanation, revealing the reasons for the classification through logical conditions based on subsequences that must, or must not, be contained in the time series. Experiments and benchmarks show that the proposed method provides faithful, meaningful, stable, and interpretable explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047242662",
                    "name": "Francesco Spinnato"
                },
                {
                    "authorId": "1704327",
                    "name": "Riccardo Guidotti"
                },
                {
                    "authorId": "2296962",
                    "name": "A. Monreale"
                },
                {
                    "authorId": "1717192",
                    "name": "M. Nanni"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                }
            ]
        },
        {
            "paperId": "4b3c8f3cc8760b8f95546431b4fe635b8a0f0e18",
            "title": "HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis",
            "abstract": "Authorship Analysis, also known as stylometry, has been an essential aspect of Natural Language Processing (NLP) for a long time. Likewise, the recent advancement of Large Language Models (LLMs) has made authorship analysis increasingly crucial for distinguishing between human-written and AI-generated texts. However, these authorship analysis tasks have primarily been focused on written texts, not considering spoken texts. Thus, we introduce the largest benchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark). HANSEN encompasses meticulous curation of existing speech datasets accompanied by transcripts, alongside the creation of novel AI-generated spoken text datasets. Together, it comprises 17 human datasets, and AI-generated spoken texts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To evaluate and demonstrate the utility of HANSEN, we perform Authorship Attribution (AA)&Author Verification (AV) on human-spoken datasets and conducted Human vs. AI spoken text detection using state-of-the-art (SOTA) models. While SOTA methods, such as, character ngram or Transformer-based model, exhibit similar AA&AV performance in human-spoken datasets compared to written ones, there is much room for improvement in AI-generated spoken text detection. The HANSEN benchmark is available at: https://huggingface.co/datasets/HANSEN-REPO/HANSEN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66674465",
                    "name": "Nafis Irtiza Tripto"
                },
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "2260345102",
                    "name": "Thai Le"
                },
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "b5596d12d8acd2af34ea2db0795c469aa70e621d",
            "title": "Trustworthy AI at KDD Lab",
            "abstract": "This document summarizes the activities regarding the development of Responsible AI (Responsible Artificial Intelligence) conducted by the Knowledge Discovery and Data mining group (KDD-Lab), a joint research group of the Institute of Information Science and Technologies \u201cAlessandro Faedo\u201d (ISTI) of the National Research Council of Italy (CNR), the Department of Computer Science of the University of Pisa, and the Scuola Normale Superiore of Pisa.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1704327",
                    "name": "Riccardo Guidotti"
                },
                {
                    "authorId": "2296962",
                    "name": "A. Monreale"
                },
                {
                    "authorId": "3145906",
                    "name": "Luca Pappalardo"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "24269254",
                    "name": "Roberto Pellungrini"
                },
                {
                    "authorId": "33769943",
                    "name": "Francesca Pratesi"
                },
                {
                    "authorId": "2120595",
                    "name": "S. Rinzivillo"
                },
                {
                    "authorId": "2243467268",
                    "name": "Salvatore Ruggieri"
                },
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "2243462850",
                    "name": "Rosaria Deluca"
                }
            ]
        },
        {
            "paperId": "e1b0248a873ce98729a3bafb02475aa47d2ef424",
            "title": "Co-design of Human-centered, Explainable AI for Clinical Decision Support",
            "abstract": "eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback with a two-fold outcome: First, we obtain evidence that explanations increase users\u2019 trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so we can re-design a better, more human-centered explanation interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14644743",
                    "name": "Cecilia Panigutti"
                },
                {
                    "authorId": "2075793906",
                    "name": "Andrea Beretta"
                },
                {
                    "authorId": "39284160",
                    "name": "D. Fadda"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "26582424",
                    "name": "A. Perotti"
                },
                {
                    "authorId": "2120595",
                    "name": "S. Rinzivillo"
                }
            ]
        },
        {
            "paperId": "ec0d64bfa9867faadb2f58665f1db093571803af",
            "title": "Human-AI Coevolution",
            "abstract": "Human-AI coevolution, defined as a process in which humans and AI algorithms continuously influence each other, increasingly characterises our society, but is understudied in artificial intelligence and complexity science literature. Recommender systems and assistants play a prominent role in human-AI coevolution, as they permeate many facets of daily life and influence human choices on online platforms. The interaction between users and AI results in a potentially endless feedback loop, wherein users' choices generate data to train AI models, which, in turn, shape subsequent user preferences. This human-AI feedback loop has peculiar characteristics compared to traditional human-machine interaction and gives rise to complex and often ``unintended'' social outcomes. This paper introduces Coevolution AI as the cornerstone for a new field of study at the intersection between AI and complexity science focused on the theoretical, empirical, and mathematical investigation of the human-AI feedback loop. In doing so, we: (i) outline the pros and cons of existing methodologies and highlight shortcomings and potential ways for capturing feedback loop mechanisms; (ii) propose a reflection at the intersection between complexity science, AI and society; (iii) provide real-world examples for different human-AI ecosystems; and (iv) illustrate challenges to the creation of such a field of study, conceptualising them at increasing levels of abstraction, i.e., technical, epistemological, legal and socio-political.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "3145906",
                    "name": "Luca Pappalardo"
                },
                {
                    "authorId": "1389957009",
                    "name": "R. Baeza-Yates"
                },
                {
                    "authorId": "1745069",
                    "name": "A. Barab\u00e1si"
                },
                {
                    "authorId": "79774420",
                    "name": "F. Dignum"
                },
                {
                    "authorId": "71343039",
                    "name": "V. Dignum"
                },
                {
                    "authorId": "1397398770",
                    "name": "Tina Eliassi-Rad"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "2199142249",
                    "name": "J. Kert\u00e9sz"
                },
                {
                    "authorId": "1980629",
                    "name": "A. Knott"
                },
                {
                    "authorId": "2156374043",
                    "name": "Yannis E. Ioannidis"
                },
                {
                    "authorId": "2199139669",
                    "name": "P. Lukowicz"
                },
                {
                    "authorId": "2174176466",
                    "name": "A. Passarella"
                },
                {
                    "authorId": "1682773",
                    "name": "A. Pentland"
                },
                {
                    "authorId": "1404459229",
                    "name": "J. Shawe-Taylor"
                },
                {
                    "authorId": "1690690",
                    "name": "Alessandro Vespignani"
                }
            ]
        },
        {
            "paperId": "619bee80a12b0fb4014204ad73aadf934d412bce",
            "title": "Transparent Latent Space Counterfactual Explanations for Tabular Data",
            "abstract": "Artificial Intelligence decision-making systems have dramatically increased their predictive performance in recent years, beating humans in many different specific tasks. However, with increased performance has come an increase in the complexity of the black-box models adopted by the AI systems, making them entirely obscure for the decision process adopted. Explainable AI is a field that seeks to make AI decisions more transparent by producing explanations. In this paper, we propose T-LACE, an approach able to retrieve post-hoc counterfactual explanations for a given pre-trained black-box model. T-LACE exploits the similarity and linearity proprieties of a custom-created transparent latent space to build reliable counterfactual explanations. We tested T-LACE on several tabular datasets and provided qualitative evaluations of the generated explanations in terms of similarity, robustness, and diversity. Comparative analysis against various state-of-the-art counterfactual explanation methods shows the higher effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1885229792",
                    "name": "F. Bodria"
                },
                {
                    "authorId": "1704327",
                    "name": "Riccardo Guidotti"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        },
        {
            "paperId": "8a6618c1524e1637ff66037007f0527e32049fcb",
            "title": "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems",
            "abstract": "The field of eXplainable Artificial Intelligence (XAI) focuses on providing explanations for AI systems\u2019 decisions. XAI applications to AI-based Clinical Decision Support Systems (DSS) should increase trust in the DSS by allowing clinicians to investigate the reasons behind its suggestions. In this paper, we present the results of a user study on the impact of advice from a clinical DSS on healthcare providers\u2019 judgment in two different cases: the case where the clinical DSS explains its suggestion and the case it does not. We examined the weight of advice, the behavioral intention to use the system, and the perceptions with quantitative and qualitative measures. Our results indicate a more significant impact of advice when an explanation for the DSS decision is provided. Additionally, through the open-ended questions, we provide some insights on how to improve the explanations in the diagnosis forecasts for healthcare assistants, nurses, and doctors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14644743",
                    "name": "Cecilia Panigutti"
                },
                {
                    "authorId": "2075793906",
                    "name": "Andrea Beretta"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "2156887580",
                    "name": "Dino Pedreschi"
                }
            ]
        },
        {
            "paperId": "8f89084e94ffd6d147ec9d5064366003abe3236c",
            "title": "Benchmark Analysis of Black Box Local Explanation Methods",
            "abstract": "In recent years, Explainable AI (XAI) has seen increasing interest: new theoretical approaches and libraries providing computationally efficient explanation algorithms are proposed daily. Given the increasing number of algorithms, as well as the fact that there is a lack of standardized evaluation metrics, it is difficult to evaluate the goodness of explanation methods from a quantitative point of view. In this paper, we propose a benchmark of explanation methods. In particular, we focused on post-hoc methods that produce explanations of a black-box. We target our analysis for most used XAI methods. Using the metrics proposed in the literature, we quantitatively compare different explanation methods categorizing them with respect to the type of data required in input and the type of explanation output.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1998031158",
                    "name": "Francesca Naretto"
                },
                {
                    "authorId": "1885229792",
                    "name": "F. Bodria"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        }
    ]
}