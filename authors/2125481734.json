{
    "authorId": "2125481734",
    "papers": [
        {
            "paperId": "0be41c94be4989f44970ffba209bccbce79518e6",
            "title": "DeFaktS: A German Dataset for Fine-Grained Disinformation Detection through Social Media Framing",
            "abstract": "In today\u2019s rapidly evolving digital age, disinformation poses a significant threat to public sentiment and socio-political dynamics. To address this, we introduce a new dataset \u201cDeFaktS\u201d, designed to understand and counter disinformation within German media. Distinctively curated across various news topics, DeFaktS offers an unparalleled insight into the diverse facets of disinformation. Our dataset, containing 105,855 posts with 20,008 meticulously labeled tweets, serves as a rich platform for in-depth exploration of disinformation\u2019s diverse characteristics. A key attribute that sets DeFaktS apart is, its fine-grain annotations based on polarized categories. Our annotation framework, grounded in the textual characteristics of news content, eliminates the need for external knowledge sources. Unlike most existing corpora that typically assign a singular global veracity value to news, our methodology seeks to annotate every structural component and semantic element of a news piece, ensuring a comprehensive and detailed understanding. In our experiments, we employed a mix of classical machine learning and advanced transformer-based models. The results underscored the potential of DeFaktS, with transformer models, especially the German variant of BERT, exhibiting pronounced effectiveness in both binary and fine-grained classifications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301581850",
                    "name": "Shaina Ashraf"
                },
                {
                    "authorId": "2296331162",
                    "name": "Isabel Bezzaoui"
                },
                {
                    "authorId": "2301581587",
                    "name": "Ionut Andone"
                },
                {
                    "authorId": "1747393",
                    "name": "Alexander Markowetz"
                },
                {
                    "authorId": "2301581698",
                    "name": "Jonas Fegert"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                }
            ]
        },
        {
            "paperId": "0ea9db84b9fa3e71bb82ff89102dbbad8d13937b",
            "title": "Language-specific Calibration for Pruning Multilingual Language Models",
            "abstract": "Recent advances in large language model (LLM) pruning have shown state-of-the-art compression results in post-training and retraining-free settings while maintaining high predictive performance. However, such research mainly considers calibrating pruning using English text, despite the multilingual nature of modern LLMs and their frequent uses in non-English languages. In this paper, we set out to explore effective strategies for calibrating the pruning of multilingual language models. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse tasks, models, and state-of-the-art pruning techniques. Our results present practical suggestions, for example, calibrating in the target language can efficiently yield lower perplexity, but does not necessarily benefit downstream tasks. Our further analysis experiments unveil that calibration in the target language mainly contributes to preserving language-specific features related to fluency and coherence, but might not contribute to capturing language-agnostic features such as language understanding and reasoning. Last, we provide practical recommendations for future practitioners.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316949789",
                    "name": "Simon Kurz"
                },
                {
                    "authorId": "2316961671",
                    "name": "Jian-Jia Chen"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                },
                {
                    "authorId": "2300129468",
                    "name": "Zhixue Zhao"
                }
            ]
        },
        {
            "paperId": "2bbc52c4dc0d8081c851ea7a0e15de74834a3d17",
            "title": "LeadEmpathy: An Expert Annotated German Dataset of Empathy in Written Leadership Communication",
            "abstract": "Empathetic leadership communication plays a pivotal role in modern workplaces as it is associated with a wide range of positive individual and organizational outcomes. This paper introduces LeadEmpathy, an innovative expert-annotated German dataset for modeling empathy in written leadership communication. It features a novel theory-based coding scheme to model cognitive and affective empathy in asynchronous communication. The final dataset comprises 770 annotated emails from 385 participants who were allowed to rewrite their emails after receiving recommendations for increasing empathy in an online experiment. Two independent annotators achieved substantial inter-annotator agreement of >= .79 for all categories, indicating that the annotation scheme can be applied to produce high-quality, multidimensional empathy ratings in current and future applications. Beyond outlining the dataset\u2019s development procedures, we present a case study on automatic empathy detection, establishing baseline models for predicting empathy scores in a range of ten possible scores that achieve a Pearson correlation of 0.816 and a mean squared error of 0.883. Our dataset is available at https://github.com/caisa-lab/LEAD-empathy-dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292460530",
                    "name": "Didem Sedefoglu"
                },
                {
                    "authorId": "1801600316",
                    "name": "Allison Lahnala"
                },
                {
                    "authorId": "2301585241",
                    "name": "Jasmin Wagner"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                },
                {
                    "authorId": "2301582814",
                    "name": "Sandra Ohly"
                }
            ]
        },
        {
            "paperId": "448ee7eac381c12dcc927fcc2b1ae7b68d0ae08f",
            "title": "Unveiling Information Through Narrative In Conversational Information Seeking",
            "abstract": "Searching through conversational interactions has been emphasized as the next frontier. Nowadays, conversational agents can generate natural language responses, transforming how we search for information. A key challenge in conversational information-seeking is how these agents present information: should they only reflect facts, cater to human cognitive preferences, or strike a balance between them? These challenges raise questions about aligning conversational agents with human cognitive processes. Our position paper emphasizes the role of narrative in addressing these questions. We explore how narratives influence human comprehension and propose a framework for optimal conversational narratives. These narratives aim to enhance interaction between humans and conversational agents in explanatory information-seeking scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309925500",
                    "name": "Vahid Sadiri Javadi"
                },
                {
                    "authorId": "2291230556",
                    "name": "Johanne Trippas"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                }
            ]
        },
        {
            "paperId": "47f9ec4dbbbd12d280fcf219a815e3c2cf10219d",
            "title": "Appraisal Framework for Clinical Empathy: A Novel Application to Breaking Bad News Conversations",
            "abstract": "Empathy is essential in healthcare communication. We introduce an annotation approach that draws on well-established frameworks for clinical empathy and breaking bad news (BBN) conversations for considering the interactive dynamics of discourse relations. We construct Empathy in BBNs, a span-relation task dataset of simulated BBN conversations in German, using our annotation scheme, in collaboration with a large medical school to support research on educational tools for medical didactics. The annotation is based on 1) Pounds (2011)\u2019s appraisal framework for clinical empathy, which is grounded in systemic functional linguistics, and 2) the SPIKES protocol for breaking bad news (Baile et al., 2000), commonly taught in medical didactics training. This approach presents novel opportunities to study clinical empathic behavior and enables the training of models to detect causal relations involving empathy, a highly desirable feature of systems that can provide feedback to medical professionals in training. We present illustrative examples, discuss applications of the annotation scheme, and insights we can draw from the framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1801600316",
                    "name": "Allison Lahnala"
                },
                {
                    "authorId": "2165383978",
                    "name": "B\u00e9la Neuendorf"
                },
                {
                    "authorId": "2301580846",
                    "name": "Alexander Thomin"
                },
                {
                    "authorId": "2263601750",
                    "name": "Charles Welch"
                },
                {
                    "authorId": "2301580853",
                    "name": "Tina Stibane"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                }
            ]
        },
        {
            "paperId": "5504b93c7925bb0841b46400cb2bfb03b193b115",
            "title": "USDC: A Dataset of User Stance and Dogmatism in Long Conversations",
            "abstract": "Identifying user's opinions and stances in long conversation threads on various topics can be extremely critical for enhanced personalization, market research, political campaigns, customer service, conflict resolution, targeted advertising, and content moderation. Hence, training language models to automate this task is critical. However, to train such models, gathering manual annotations has multiple challenges: 1) It is time-consuming and costly; 2) Conversation threads could be very long, increasing chances of noisy annotations; and 3) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Inspired by the recent success of large language models (LLMs) for complex natural language processing (NLP) tasks, we leverage Mistral Large and GPT-4 to automate the human annotation process on the following two tasks while also providing reasoning: i) User Stance classification, which involves labeling a user's stance of a post in a conversation on a five-point scale; ii) User Dogmatism classification, which deals with labeling a user's overall opinion in the conversation on a four-point scale. The majority voting on zero-shot, one-shot, and few-shot annotations from these two LLMs on 764 multi-user Reddit conversations helps us curate the USDC dataset. USDC is then used to finetune and instruction-tune multiple deployable small language models for the 5-class stance and 4-class dogmatism classification tasks. We make the code and dataset publicly available [https://anonymous.4open.science/r/USDC-0F7F].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25910248",
                    "name": "Mounika Marreddy"
                },
                {
                    "authorId": "8307724",
                    "name": "S. Oota"
                },
                {
                    "authorId": "2127986181",
                    "name": "Venkata Charan Chinni"
                },
                {
                    "authorId": "2152950492",
                    "name": "Manish Gupta"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                }
            ]
        },
        {
            "paperId": "9a0d39d9e3a5e320e19f74070f2eef1dd6ec0c02",
            "title": "EmPO: Emotion Grounding for Empathetic Response Generation through Preference Optimization",
            "abstract": "Empathetic response generation is a desirable aspect of conversational agents, crucial for facilitating engaging and emotionally intelligent multi-turn conversations between humans and machines. Leveraging large language models for this task has shown promising results, yet challenges persist in ensuring both the empathetic quality of the responses and retention of the generalization performance of the models. We propose a novel approach where we construct theory-driven preference datasets based on emotion grounding and use them to align LLMs with preference optimization algorithms to address these challenges. To evaluate empathetic response generation, we employ the EmpatheticDialogues dataset, assessing empathy with the diff-Epitome and BERTscore metrics and with multi-dimensional human evaluation. Additionally, we measure diversity and emotional valence using feature-based methods. We also evaluate the impact of training on the generalization performance using the MMLU benchmark and tasks from the Open LLM Leaderboard. The results show that LLMs can be aligned for empathetic response generation by preference optimization while retaining their general performance and that emotion grounding can guide preference dataset creation. We make all datasets, source code, and models publicly available. https://github.com/justtherightsize/empo",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2125479140",
                    "name": "Ond\u0159ej Sotol\u00e1\u0159"
                },
                {
                    "authorId": "2321450860",
                    "name": "Vojtech Formanek"
                },
                {
                    "authorId": "2321450826",
                    "name": "Alok Debnath"
                },
                {
                    "authorId": "1801600316",
                    "name": "Allison Lahnala"
                },
                {
                    "authorId": "2263601750",
                    "name": "Charles Welch"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                }
            ]
        },
        {
            "paperId": "b9efaa781df0a2fd05e4ed7359ee36be46b36f32",
            "title": "Perspective Taking through Generating Responses to Conflict Situations",
            "abstract": "Although language model performance across diverse tasks continues to improve, these models still struggle to understand and explain the beliefs of other people. This skill requires perspective-taking, the process of conceptualizing the point of view of another person. Perspective taking becomes challenging when the text reflects more personal and potentially more controversial beliefs. We explore this task through natural language generation of responses to conflict situations. We evaluate novel modifications to recent architectures for conditioning generation on an individual\u2019s comments and self-disclosure statements. Our work extends the Social-Chem-101 corpus, using 95k judgements written by 6k authors from English Reddit data, for each of whom we obtained 20-500 self-disclosure statements. Our evaluation methodology borrows ideas from both personalized generation and theory of mind literature. Our proposed perspective-taking models outperform recent work, especially the twin encoder model conditioned on self-disclosures with high similarity to the conflict situation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053815176",
                    "name": "Joan Plepi"
                },
                {
                    "authorId": "2263601750",
                    "name": "Charles Welch"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                }
            ]
        },
        {
            "paperId": "bcbf498506085206716a17a88e7977b246ffe03b",
            "title": "Do Multilingual Large Language Models Mitigate Stereotype Bias?",
            "abstract": "While preliminary findings indicate that multilingual LLMs exhibit reduced bias compared to monolingual ones, a comprehensive understanding of the effect of multilingual training on bias mitigation, is lacking. This study addresses this gap by systematically training six LLMs of identical size (2.6B parameters) and architecture: five monolingual models (English, German, French, Italian, and Spanish) and one multilingual model trained on an equal distribution of data across these languages, all using publicly available data. To ensure robust evaluation, standard bias benchmarks were automatically translated into the five target languages and verified for both translation quality and bias preservation by human annotators. Our results consistently demonstrate that multilingual training effectively mitigates bias. Moreover, we observe that multilingual models achieve not only lower bias but also superior prediction accuracy when compared to monolingual models with the same amount of training data, model architecture, and size.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2310339837",
                    "name": "Shangrui Nie"
                },
                {
                    "authorId": "2258551638",
                    "name": "Michael Fromm"
                },
                {
                    "authorId": "2291362527",
                    "name": "Charles Welch"
                },
                {
                    "authorId": "2197531622",
                    "name": "Rebekka G\u00f6rge"
                },
                {
                    "authorId": "2310337468",
                    "name": "Akbar Karimi"
                },
                {
                    "authorId": "2053815176",
                    "name": "Joan Plepi"
                },
                {
                    "authorId": "2310339035",
                    "name": "Nazia Afsan Mowmita"
                },
                {
                    "authorId": "1403909672",
                    "name": "Nicolas Flores-Herr"
                },
                {
                    "authorId": "2258668067",
                    "name": "Mehdi Ali"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                }
            ]
        },
        {
            "paperId": "efb1f5ad5715cdff23dc5841ae9db853d901dd4a",
            "title": "Corpus Considerations for Annotator Modeling and Scaling",
            "abstract": "Recent trends in natural language processing research and annotation tasks affirm a paradigm shift from the traditional reliance on a single ground truth to a focus on individual perspectives, particularly in subjective tasks. In scenarios where annotation tasks are meant to encompass diversity, models that solely rely on the majority class labels may inadvertently disregard valuable minority perspectives. This oversight could result in the omission of crucial information and, in a broader context, risk disrupting the balance within larger ecosystems. As the landscape of annotator modeling unfolds with diverse representation techniques, it becomes imperative to investigate their effectiveness with the fine-grained features of the datasets in view. This study systematically explores various annotator modeling techniques and compares their performance across seven corpora. From our findings, we show that the commonly used user token model consistently outperforms more complex models. We introduce a composite embedding approach and show distinct differences in which model performs best as a function of the agreement with a given dataset. Our findings shed light on the relationship between corpus statistics and annotator modeling performance, which informs future work on corpus construction and perspectivist NLP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185649949",
                    "name": "O. O. Sarumi"
                },
                {
                    "authorId": "2165383978",
                    "name": "B\u00e9la Neuendorf"
                },
                {
                    "authorId": "2053815176",
                    "name": "Joan Plepi"
                },
                {
                    "authorId": "2125481734",
                    "name": "Lucie Flek"
                },
                {
                    "authorId": "3044872",
                    "name": "J\u00f6rg Schl\u00f6tterer"
                },
                {
                    "authorId": "2263601750",
                    "name": "Charles Welch"
                }
            ]
        }
    ]
}