{
    "authorId": "2030973547",
    "papers": [
        {
            "paperId": "3f5711a7b882b9e02eb8db26ee5f966756d0b93d",
            "title": "InversionView: A General-Purpose Method for Reading Information from Neural Activations",
            "abstract": "The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. Computing such subsets is nontrivial as the input space is exponentially large. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present four case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2303617629",
                    "name": "Xinting Huang"
                },
                {
                    "authorId": "2030973547",
                    "name": "Madhur Panwar"
                },
                {
                    "authorId": "2286065792",
                    "name": "Navin Goyal"
                },
                {
                    "authorId": "2303471059",
                    "name": "Michael Hahn"
                }
            ]
        },
        {
            "paperId": "9b9e44d43eb6bc0be7a07a92b2100dd6a7f3b158",
            "title": "Learning Syntax Without Planting Trees: Understanding When and Why Transformers Generalize Hierarchically",
            "abstract": "Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge. We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically. We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure. When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order). Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2030973547",
                    "name": "Madhur Panwar"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2298441996",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2286065792",
                    "name": "Navin Goyal"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "68768d4dca5b71e88094d63d546da9574b09c7b2",
            "title": "TAN-NTM: Topic Attention Networks for Neural Topic Modeling",
            "abstract": "Topic models have been widely used to learn text representations and gain insight into document corpora. To perform topic discovery, most existing neural models either take document bag-of-words (BoW) or sequence of tokens as input followed by variational inference and BoW reconstruction to learn topic-word distribution. However, leveraging topic-word distribution for learning better features during document encoding has not been explored much. To this end, we develop a framework TAN-NTM, which processes document as a sequence of tokens through a LSTM whose contextual outputs are attended in a topic-aware manner. We propose a novel attention mechanism which factors in topic-word distribution to enable the model to attend on relevant words that convey topic related cues. The output of topic attention module is then used to carry out variational inference. We perform extensive ablations and experiments resulting in ~9-15 percentage improvement over score of existing SOTA topic models in NPMI coherence on several benchmark datasets - 20Newsgroups, Yelp Review Polarity and AGNews. Further, we show that our method learns better latent document-topic features compared to existing topic models through improvement on two downstream tasks: document classification and topic guided keyphrase generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2030973547",
                    "name": "Madhur Panwar"
                },
                {
                    "authorId": "2030976071",
                    "name": "Shashank Shailabh"
                },
                {
                    "authorId": "6657914",
                    "name": "Milan Aggarwal"
                },
                {
                    "authorId": "145846953",
                    "name": "Balaji Krishnamurthy"
                }
            ]
        }
    ]
}