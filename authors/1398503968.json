{
    "authorId": "1398503968",
    "papers": [
        {
            "paperId": "9ea8105a7bc03dbcde05bf953f3b90f1db61f6bd",
            "title": "URL-BERT: Training Webpage Representations via Social Media Engagements",
            "abstract": "Understanding and representing webpages is crucial to online social networks where users may share and engage with URLs. Common language model (LM) encoders such as BERT can be used to understand and represent the textual content of webpages. However, these representations may not model thematic information of web domains and URLs or accurately capture their appeal to social media users. In this work, we introduce a new pre-training objective that can be used to adapt LMs to understand URLs and webpages. Our proposed framework consists of two steps: (1) scalable graph embeddings to learn shallow representations of URLs based on user engagement on social media and (2) a contrastive objective that aligns LM representations with the aforementioned graph-based representation. We apply our framework to the multilingual version of BERT to obtain the model URL-BERT. We experimentally demonstrate that our continued pre-training approach improves webpage understanding on a variety of tasks and Twitter internal and external benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47633438",
                    "name": "A. Qamar"
                },
                {
                    "authorId": "2261673163",
                    "name": "Chetan Verma"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "31028278",
                    "name": "Sumit Binnani"
                },
                {
                    "authorId": "20402239",
                    "name": "Sneha Mehta"
                },
                {
                    "authorId": "2261673876",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "c817d9901f788e40279c76068ea2622365cd9a1d",
            "title": "Simple Temporal Adaptation to Changing Label Sets: Hashtag Prediction via Dense KNN",
            "abstract": "User-generated social media data is constantly changing as new trends influence online discussion and personal information is deleted due to privacy concerns. However, traditional NLP models rely on fixed training datasets, which means they are unable to adapt to temporal change\u2014both test distribution shift and deleted training data\u2014without frequent, costly re-training. In this paper, we study temporal adaptation through the task of longitudinal hashtag prediction and propose a non-parametric dense retrieval technique, which does not require re-training, as a simple but effective solution. In experiments on a newly collected, publicly available, year-long Twitter dataset exhibiting temporal distribution shift, our method improves by 64% over the best static parametric baseline while avoiding costly gradient-based re-training. Our approach is also particularly well-suited to dynamically deleted user data in line with data privacy laws, with negligible computational cost/performance loss.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254272878",
                    "name": "Niloofar Mireshghallah"
                },
                {
                    "authorId": "46623434",
                    "name": "Nikolai Vogler"
                },
                {
                    "authorId": "2273587768",
                    "name": "Junxian He"
                },
                {
                    "authorId": "2257289795",
                    "name": "Omar Florez"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "2261673876",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "1520d780ac2b71a281c4aa6bba593b716a434b56",
            "title": "Learning Stance Embeddings from Signed Social Graphs",
            "abstract": "A challenge in social network analysis, is understanding the position, or stance, of people on a large set of topics. While past work has modeled (dis)agreement in social networks using signed graphs, these approaches have not modeled agreement patterns across a range of correlated topics. For instance, disagreement on one topic may make disagreement (or agreement) more likely for related topics. Recognizing topics influence agreement and disagreement, we propose the Stance Embeddings Model (SEM), which jointly learns embeddings for each user and topic in signed social graphs with distinct edge types for each topic. By jointly learning user and topic embeddings, SEM can perform cold-start topic stance detection, predicting the stance of a user on topics for which we have not observed their engagement. We demonstrate the effectiveness of SEM using two large-scale Twitter signed graph datasets that we open-source. One dataset, TwitterSG, labels (dis)agreements using engagements between users via tweets to derive topic-informed, signed edges. The other, BirdwatchSG, leverages community reports on misinformation and misleading content. On TwitterSG and BirdwatchSG, SEM shows a 39% and 26% error reduction respectively against strong topic-agnostic baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151243828",
                    "name": "John N. Pougu'e-Biyong"
                },
                {
                    "authorId": "50178628",
                    "name": "A. Gupta"
                },
                {
                    "authorId": "1761880",
                    "name": "A. Haghighi"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                }
            ]
        },
        {
            "paperId": "160472215dfeee9f79b5da5cc0d5bd6b3bef6a34",
            "title": "Non-Parametric Temporal Adaptation for Social Media Topic Classification",
            "abstract": "User-generated social media data is constantly changing as new trends influence online discussion and personal information is deleted due to privacy concerns. However, most current NLP models are static and rely on fixed training data, which means they are unable to adapt to temporal change -- both test distribution shift and deleted training data -- without frequent, costly re-training. In this paper, we study temporal adaptation through the task of longitudinal hashtag prediction and propose a non-parametric dense retrieval technique, which does not require re-training, as a simple but effective solution. In experiments on a newly collected, publicly available, year-long Twitter dataset exhibiting temporal distribution shift, our method improves by 64.12% over the best parametric baseline without any of its costly gradient-based updating. Our dense retrieval approach is also particularly well-suited to dynamically deleted user data in line with data privacy laws, with negligible computational cost and performance loss.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115471757",
                    "name": "Fatemehsadat Mireshghallah"
                },
                {
                    "authorId": "46623434",
                    "name": "Nikolai Vogler"
                },
                {
                    "authorId": "2109932032",
                    "name": "Junxian He"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "478d2e1e1711f46e6a5047cd0b4525b433b3cac9",
            "title": "Graph-based Representation Learning for Web-scale Recommender Systems",
            "abstract": "Recommender systems are fundamental building blocks of modern consumer web applications that seek to predict user preferences to better serve relevant items. As such, high-quality user and item representations as inputs to recommender systems are crucial for personalized recommendation. To construct these user and item representations, self-supervised graph embedding has emerged as a principled approach to embed relational data such as user social graphs, user membership graphs, user-item engagements, and other heterogeneous graphs. In this tutorial we discuss different families of approaches to self-supervised graph embedding. Within each family, we outline a variety of techniques, their merits and disadvantages, and expound on latest works. Finally, we demonstrate how to effectively utilize the resultant large embedding tables to improve candidate retrieval and ranking in modern industry-scale deep-learning recommender systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "2149583375",
                    "name": "Michael M. Bronstein"
                },
                {
                    "authorId": "2122804028",
                    "name": "Ying Xiao"
                },
                {
                    "authorId": "1761880",
                    "name": "A. Haghighi"
                }
            ]
        },
        {
            "paperId": "52a814bb540cb2fd7f0a0c11e430c97859f18345",
            "title": "NTULM: Enriching Social Media Text Representations with Non-Textual Units",
            "abstract": "On social media, additional context is often present in the form of annotations and meta-data such as the post\u2019s author, mentions, Hashtags, and hyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit that NTUs provide social context beyond their textual semantics and leveraging these units can enrich social media text representations. In this work we construct an NTU-centric social heterogeneous network to co-embed NTUs. We then principally integrate these NTU embeddings into a large pretrained language model by fine-tuning with these additional units. This adds context to noisy short-text social media. Experiments show that utilizing NTU-augmented text representations significantly outperforms existing text-only baselines by 2-5% relative points on many downstream tasks highlighting the importance of context to social media NLP. We also highlight that including NTU context into the initial layers of language model alongside text is better than using it after the text embedding is generated. Our work leads to the generation of holistic general purpose social media content embedding.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109377974",
                    "name": "Jinning Li"
                },
                {
                    "authorId": "2112134877",
                    "name": "Shubhanshu Mishra"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "20402239",
                    "name": "Sneha Mehta"
                },
                {
                    "authorId": "144592382",
                    "name": "Vivek Kulkarni"
                }
            ]
        },
        {
            "paperId": "55795745fe47e3fbd8a57c0bdcc5df4497cc5602",
            "title": "MiCRO: Multi-interest Candidate Retrieval Online",
            "abstract": "Providing personalized recommendations in an environment where items exhibit ephemerality and temporal relevancy (e.g. in social media) presents a few unique challenges: (1) inductively understanding ephemeral appeal for items in a setting where new items are created frequently, (2) adapting to trends within engagement patterns where items may undergo temporal shifts in relevance, (3) accurately modeling user preferences over this item space where users may express multiple interests. In this work we introduce MiCRO, a generative statistical framework that models multi-interest user preferences and temporal multi-interest item representations. Our framework is specifically formulated to adapt to both new items and temporal patterns of engagement. MiCRO demonstrates strong empirical performance on candidate retrieval experiments performed on two large scale user-item datasets: (1) an open-source temporal dataset of ( User , User ) follow interactions and (2) a temporal dataset of ( User , Tweet ) favorite interactions which we will open-source as an additional contribution to the community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1661216672",
                    "name": "Frank Portman"
                },
                {
                    "authorId": "2448309",
                    "name": "Stephen Ragain"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                }
            ]
        },
        {
            "paperId": "76453afd79968f2de8356beaca5e0468715feab8",
            "title": "Non-Parametric Temporal Adaptation for Social Media Topic Classification",
            "abstract": "User-generated social media data is constantly changing as new trends in\ufb02uence online discussion, causing distribution shift in test data for social media NLP applications. In addition, training data is often subject to change as user data is deleted. Most current NLP systems are static and rely on \ufb01xed training data. As a result, they are unable to adapt to temporal change \u2013 both test distribution shift and deleted training data \u2013 without frequent, costly re-training. In this paper, we study temporal adaptation through the task of longitudinal hash-tag prediction and propose a non-parametric technique as a simple but effective solution: non-parametric classi\ufb01ers use datastores which can be updated, either to adapt to test distribution shift or training data deletion, without re-training. We release a new benchmark dataset comprised of 7 . 13 M Tweets from 2021, along with their hashtags, broken into consecutive temporal buckets. We compare parametric neural hash-tag classi\ufb01cation and hashtag generation models, which need re-training for adaptation, with a non-parametric, training-free dense retrieval method that returns the nearest neighbor\u2019s hashtags based on text embedding distance. In experiments on our longitudinal Twitter dataset we \ufb01nd that dense nearest neighbor retrieval has a relative performance gain of 64 . 12% over the best parametric baseline on test sets that exhibit distribution shift without requiring gradient-based re-training. Furthermore, we show that our datastore approach is particularly well-suited to dynamically deleted user data, with negligible computational cost and performance loss. Our novel benchmark dataset and empirical analysis can support future inquiry into the important challenges presented by temporal-ity in the deployment of AI systems on real-world user data",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115471757",
                    "name": "Fatemehsadat Mireshghallah"
                },
                {
                    "authorId": "46623434",
                    "name": "Nikolai Vogler"
                },
                {
                    "authorId": "2109932032",
                    "name": "Junxian He"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "d74f2399adb29ec8bfde74d4d5e8be4843e4888c",
            "title": "TwHIN: Embedding the Twitter Heterogeneous Information Network for Personalized Recommendation",
            "abstract": "Social networks, such as Twitter, form a heterogeneous information network (HIN) where nodes represent domain entities (e.g., user, content, advertiser, etc.) and edges represent one of many entity interactions (e.g, a user re-sharing content or \"following\" another). Interactions from multiple relation types can encode valuable information about social network entities not fully captured by a single relation; for instance, a user's preference for accounts to follow may depend on both user-content engagement interactions and the other users they follow. In this work, we investigate knowledge-graph embeddings for entities in the Twitter HIN (TwHIN); we show that these pretrained representations yield significant offline and online improvement for a diverse range of downstream recommendation and classification tasks: personalized ads rankings, account follow-recommendation, offensive content detection, and search ranking. We discuss design choices and practical challenges of deploying industry-scale HIN embeddings, including compressing them to reduce end-to-end model latency and handling parameter drift across versions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "47966426",
                    "name": "Thomas Markovich"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "34777224",
                    "name": "C. Verma"
                },
                {
                    "authorId": "94100042",
                    "name": "Baekjin Kim"
                },
                {
                    "authorId": "37558091",
                    "name": "R. Eskander"
                },
                {
                    "authorId": "2104662",
                    "name": "Yury Malkov"
                },
                {
                    "authorId": "1661216672",
                    "name": "Frank Portman"
                },
                {
                    "authorId": "151367676",
                    "name": "Sofia Samaniego"
                },
                {
                    "authorId": "2122804028",
                    "name": "Ying Xiao"
                },
                {
                    "authorId": "1761880",
                    "name": "A. Haghighi"
                }
            ]
        },
        {
            "paperId": "f5888d776f122f53292973bd3693628ebd265bc6",
            "title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter",
            "abstract": "Pre-trained language models (PLMs) are fundamental for natural language processing applications. Most existing PLMs are not tailored to the noisy user-generated text on social media, and the pre-training does not factor in the valuable social engagement logs available in a social network. We present TwHIN-BERT, a multilingual language model productionized at Twitter, trained on in-domain data from the popular social network. TwHIN-BERT differs from prior pre-trained language models as it is trained with not only text-based self-supervision but also with a social objective based on the rich social engagements within a Twitter heterogeneous information network (TwHIN). Our model is trained on 7 billion tweets covering over 100 distinct languages, providing a valuable representation to model short, noisy, user-generated text. We evaluate our model on various multilingual social recommendation and semantic understanding tasks and demonstrate significant metric improvement over established pre-trained language models. We open-source TwHIN-BERT and our curated hashtag prediction and social engagement benchmark datasets to the research community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108262811",
                    "name": "Xinyang Zhang"
                },
                {
                    "authorId": "2104662",
                    "name": "Yury Malkov"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "2953855",
                    "name": "B. McWilliams"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                }
            ]
        }
    ]
}