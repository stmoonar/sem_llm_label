{
    "authorId": "2697425",
    "papers": [
        {
            "paperId": "222f289cb96ac4dfef7849cd068af6af02233c52",
            "title": "Evaluating In-Context Learning of Libraries for Code Generation",
            "abstract": "Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1443788809",
                    "name": "Arkil Patel"
                },
                {
                    "authorId": "145732771",
                    "name": "Siva Reddy"
                },
                {
                    "authorId": "3335364",
                    "name": "Dzmitry Bahdanau"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                }
            ]
        },
        {
            "paperId": "a1fa960fdfcc08510d348f7c66028f3d91b497f8",
            "title": "The Art of Saying No: Contextual Noncompliance in Language Models",
            "abstract": "Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of\"unsafe\"queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223951216",
                    "name": "Faeze Brahman"
                },
                {
                    "authorId": "2308339428",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "2279337376",
                    "name": "Sarah Wiegreffe"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "2302810573",
                    "name": "K. Chandu"
                },
                {
                    "authorId": "2689239",
                    "name": "Jack Hessel"
                },
                {
                    "authorId": "2258958466",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2292425227",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2259707400",
                    "name": "Yejin Choi"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "ac45bbf9940512d9d686cf8cd3a95969bc313570",
            "title": "OLMo: Accelerating the Science of Language Models",
            "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3458736",
                    "name": "Dirk Groeneveld"
                },
                {
                    "authorId": "2260133345",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "2158819969",
                    "name": "Pete Walsh"
                },
                {
                    "authorId": "2166136235",
                    "name": "Akshita Bhagia"
                },
                {
                    "authorId": "2282136328",
                    "name": "Rodney Kinney"
                },
                {
                    "authorId": "3385516",
                    "name": "Oyvind Tafjord"
                },
                {
                    "authorId": "47286118",
                    "name": "A. Jha"
                },
                {
                    "authorId": "2056776606",
                    "name": "Hamish Ivison"
                },
                {
                    "authorId": "2124977543",
                    "name": "Ian Magnusson"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2259924223",
                    "name": "Shane Arora"
                },
                {
                    "authorId": "2282136757",
                    "name": "David Atkinson"
                },
                {
                    "authorId": "2202417686",
                    "name": "Russell Authur"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "2527954",
                    "name": "Arman Cohan"
                },
                {
                    "authorId": "2282136556",
                    "name": "Jennifer Dumas"
                },
                {
                    "authorId": "51131518",
                    "name": "Yanai Elazar"
                },
                {
                    "authorId": "2261456046",
                    "name": "Yuling Gu"
                },
                {
                    "authorId": "2689239",
                    "name": "Jack Hessel"
                },
                {
                    "authorId": "2236429",
                    "name": "Tushar Khot"
                },
                {
                    "authorId": "2282138468",
                    "name": "William Merrill"
                },
                {
                    "authorId": "2146964035",
                    "name": "Jacob Daniel Morrison"
                },
                {
                    "authorId": "2037383772",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "23175870",
                    "name": "Aakanksha Naik"
                },
                {
                    "authorId": "2282136595",
                    "name": "Crystal Nam"
                },
                {
                    "authorId": "2267244582",
                    "name": "Matthew E. Peters"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "2264248042",
                    "name": "Dustin Schwenk"
                },
                {
                    "authorId": "2282190660",
                    "name": "Saurabh Shah"
                },
                {
                    "authorId": "2282155558",
                    "name": "Will Smith"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "34202134",
                    "name": "Nishant Subramani"
                },
                {
                    "authorId": "52193502",
                    "name": "Mitchell Wortsman"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "2267244197",
                    "name": "Nathan Lambert"
                },
                {
                    "authorId": "46666605",
                    "name": "Kyle Richardson"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "34176020",
                    "name": "Jesse Dodge"
                },
                {
                    "authorId": "46258841",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "3328733",
                    "name": "Luca Soldaini"
                },
                {
                    "authorId": "2264002618",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "37680e5cb6030e01f1a44a5abe2257972196ae26",
            "title": "Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2",
            "abstract": "Since the release of T\\\"ULU [Wang et al., 2023b], open resources for instruction tuning have developed quickly, from better base models to new finetuning techniques. We test and incorporate a number of these advances into T\\\"ULU, resulting in T\\\"ULU 2, a suite of improved T\\\"ULU models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences. Concretely, we release: (1) T\\\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2) T\\\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\\\"ULU 2+DPO, T\\\"ULU 2 models trained with direct preference optimization (DPO), including the largest DPO-trained model to date (T\\\"ULU 2+DPO 70B); (4) CODE T\\\"ULU 2, CODE LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple perspectives shows that the T\\\"ULU 2 suite achieves state-of-the-art performance among open models and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data, training and evaluation code to facilitate future open efforts on adapting large language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056776606",
                    "name": "Hamish Ivison"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "2267244197",
                    "name": "Nathan Lambert"
                },
                {
                    "authorId": "2267244582",
                    "name": "Matthew E. Peters"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "2000091730",
                    "name": "Joel Jang"
                },
                {
                    "authorId": "30051202",
                    "name": "David Wadden"
                },
                {
                    "authorId": "2264002618",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2260133345",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "47c824591f37ebb0101aac93b01a1332b52025a4",
            "title": "ReFIT: Relevance Feedback from a Reranker during Inference",
            "abstract": "Retrieve-and-rerank is a prevalent framework in neural information retrieval, wherein a bi-encoder network initially retrieves a pre-defined number of candidates (e.g., K=100), which are then reranked by a more powerful cross-encoder model. While the reranker often yields improved candidate scores compared to the retriever, its scope is confined to only the top K retrieved candidates. As a result, the reranker cannot improve retrieval performance in terms of Recall@K. In this work, we propose to leverage the reranker to improve recall by making it provide relevance feedback to the retriever at inference time. Specifically, given a test instance during inference, we distill the reranker's predictions for that instance into the retriever's query representation using a lightweight update mechanism. The aim of the distillation loss is to align the retriever's candidate scores more closely with those produced by the reranker. The algorithm then proceeds by executing a second retrieval step using the updated query vector. We empirically demonstrate that this method, applicable to various retrieve-and-rerank frameworks, substantially enhances retrieval recall across multiple domains, languages, and modalities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113583612",
                    "name": "R. Reddy"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "2937809",
                    "name": "Md Arafat Sultan"
                },
                {
                    "authorId": "2527954",
                    "name": "Arman Cohan"
                },
                {
                    "authorId": "2707234",
                    "name": "Avirup Sil"
                },
                {
                    "authorId": "2072975661",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ]
        },
        {
            "paperId": "c3b8fbdd0ee1cd663a9b308d067d40c90c05095f",
            "title": "Evaluating In-Context Learning of Libraries for Code Generation",
            "abstract": "Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1443788809",
                    "name": "Arkil Patel"
                },
                {
                    "authorId": "2313693935",
                    "name": "Siva Reddy"
                },
                {
                    "authorId": "3335364",
                    "name": "Dzmitry Bahdanau"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                }
            ]
        },
        {
            "paperId": "cce2174048a8e5cf0bc3c9875b32607a02aaf44c",
            "title": "TRAM: Bridging Trust Regions and Sharpness Aware Minimization",
            "abstract": "Sharpness-aware minimization (SAM) reports improving domain generalization by reducing the loss surface curvature in the parameter space. However, generalization during fine-tuning is often more dependent on the transferability of representations in the function space. Trust-region methods (TR) target this goal by regularizing representation curvature to reduce catastrophic forgetting of pre-trained task-agnostic information while adopting task-specific skills. We consider unifying these strategies for low curvature in both parameter space and function space to improve out-of-domain (OOD) generalization. We propose Trust Region Aware Minimization (TRAM), a SAM algorithm fine-tuning for low parameter sharpness and smooth, informative representations preserving pre-trained structure. TRAM uses a trust region bound to inform the SAM adversarial neighborhood, introducing an awareness of function curvature within optimization for flatter minima. We empirically validate TRAM in vision (cross-dataset adaptation) and text (OOD language modeling, zero-shot cross-lingual transfer) tasks where robust domain transfer and representation generality are critical. TRAM outperforms SAM- and TR-based optimization across all tasks, notably surpassing competing methods for hard transfer between anticorrelated domains. TRAM establishes a novel standard in fine-tuning for domain-generalizable models with minimal additional computation over previous sharpness-aware methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20662387",
                    "name": "Tom Sherborne"
                },
                {
                    "authorId": "2362960",
                    "name": "Naomi Saphra"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "2256555239",
                    "name": "Hao Peng"
                }
            ]
        },
        {
            "paperId": "f1f6c61ed0b80a785e4e5d0d97a454dbe6126c63",
            "title": "LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization",
            "abstract": "While human evaluation remains best practice for accurately judging the faithfulness of automatically-generated summaries, few solutions exist to address the increased difficulty and workload when evaluating long-form summaries. Through a survey of 162 papers on long-form summarization, we first shed light on current human evaluation practices surrounding long-form summaries. We find that 73% of these papers do not perform any human evaluation on model-generated summaries, while other works face new difficulties that manifest when dealing with long documents (e.g., low inter-annotator agreement). Motivated by our survey, we present LongEval, a set of guidelines for human evaluation of faithfulness in long-form summaries that addresses the following challenges: (1) How can we achieve high inter-annotator agreement on faithfulness scores? (2) How can we minimize annotator workload while maintaining accurate faithfulness scores? and (3) Do humans benefit from automated alignment between summary and source snippets? We deploy LongEval in annotation studies on two long-form summarization datasets in different domains (SQuALITY and PubMed), and we find that switching to a finer granularity of judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a partial annotation of fine-grained units highly correlates with scores from a full annotation workload (0.89 Kendall\u2019s tau using 50% judgements). We release our human judgments, annotation templates, and software as a Python library for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26161085",
                    "name": "Kalpesh Krishna"
                },
                {
                    "authorId": "2203427167",
                    "name": "Erin Bransom"
                },
                {
                    "authorId": "2003338023",
                    "name": "Bailey Kuehl"
                },
                {
                    "authorId": "2136562",
                    "name": "Mohit Iyyer"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "2527954",
                    "name": "Arman Cohan"
                },
                {
                    "authorId": "46258841",
                    "name": "Kyle Lo"
                }
            ]
        },
        {
            "paperId": "fbd2c8089870814449f9254a711041bbae145a82",
            "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources",
            "abstract": "In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce T\\\"ulu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources. Our experiments show that different instruction-tuning datasets can uncover or enhance specific skills, while no single dataset (or combination) provides the best performance across all evaluations. Interestingly, we find that model and human preference-based evaluations fail to reflect differences in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of systemic evaluation performed in this work. Our evaluations show that the best model in any given evaluation reaches on average 87% of ChatGPT performance, and 73% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap. We release our instruction-tuned models, including a fully finetuned 65B T\\\"ulu, along with our code, data, and evaluation framework at https://github.com/allenai/open-instruct to facilitate future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2056776606",
                    "name": "Hamish Ivison"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "2689239",
                    "name": "Jack Hessel"
                },
                {
                    "authorId": "2236429",
                    "name": "Tushar Khot"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "30051202",
                    "name": "David Wadden"
                },
                {
                    "authorId": "2071601396",
                    "name": "Kelsey MacMillan"
                },
                {
                    "authorId": "144365875",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "46181066",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ]
        },
        {
            "paperId": "1cfe67bad95a16bc249941b829d113d830031cf5",
            "title": "Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets",
            "abstract": "Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39417610",
                    "name": "Yuxiang Wu"
                },
                {
                    "authorId": "40642935",
                    "name": "Matt Gardner"
                },
                {
                    "authorId": "1918552",
                    "name": "Pontus Stenetorp"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                }
            ]
        }
    ]
}