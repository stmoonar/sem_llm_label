{
    "authorId": "1500649148",
    "papers": [
        {
            "paperId": "68dfea218c1d38f7255e4218926d24a88afcd639",
            "title": "Graph Augmentation for Recommendation",
            "abstract": "Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation. Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model. The outcomes consistently unveil its superiority over existing baseline methods. The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "1830455155",
                    "name": "Lianghao Xia"
                },
                {
                    "authorId": "2206218723",
                    "name": "Xuheng Cai"
                },
                {
                    "authorId": "151225851",
                    "name": "Siu-keung Yiu"
                },
                {
                    "authorId": "2110926729",
                    "name": "Chao Huang"
                },
                {
                    "authorId": "2293317939",
                    "name": "Christian S. Jensen"
                }
            ]
        },
        {
            "paperId": "7673abf7150aeaf8db6b2e32f4f877a29c183d24",
            "title": "TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with Unanswerable Questions",
            "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable progress on visual perception and linguistic interpretation. Despite their impressive capabilities across various tasks, LVLMs still suffer from the issue of hallucination, which involves generating content that is incorrect or unfaithful to the visual or textual inputs. Traditional benchmarks, such as MME and POPE, evaluate hallucination in LVLMs within the scope of Visual Question Answering (VQA) using answerable questions. However, some questions are unanswerable due to insufficient information in the images, and the performance of LVLMs on such unanswerable questions remains underexplored. To bridge this research gap, we propose TUBench, a benchmark specifically designed to evaluate the reliability of LVLMs using unanswerable questions. TUBench comprises an extensive collection of high-quality, unanswerable questions that are meticulously crafted using ten distinct strategies. To thoroughly evaluate LVLMs, the unanswerable questions in TUBench are based on images from four diverse domains as visual contexts: screenshots of code snippets, natural images, geometry diagrams, and screenshots of statistical tables. These unanswerable questions are tailored to test LVLMs' trustworthiness in code reasoning, commonsense reasoning, geometric reasoning, and mathematical reasoning related to tables, respectively. We conducted a comprehensive quantitative evaluation of 28 leading foundational models on TUBench, with Gemini-1.5-Pro, the top-performing model, achieving an average accuracy of 69.2%, and GPT-4o, the third-ranked model, reaching 66.7% average accuracy, in determining whether questions are answerable. TUBench is available at https://github.com/NLPCode/TUBench.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273771682",
                    "name": "Xingwei He"
                },
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "15796861",
                    "name": "Alex Jin"
                },
                {
                    "authorId": null,
                    "name": "Yuan Yuan"
                },
                {
                    "authorId": "2184000509",
                    "name": "S. Yiu"
                }
            ]
        },
        {
            "paperId": "99b65263b82b3d9d532377706ceb5d6733e5f0a9",
            "title": "A Survey of Generative Techniques for Spatial-Temporal Data Mining",
            "abstract": "This paper focuses on the integration of generative techniques into spatial-temporal data mining, considering the significant growth and diverse nature of spatial-temporal data. With the advancements in RNNs, CNNs, and other non-generative techniques, researchers have explored their application in capturing temporal and spatial dependencies within spatial-temporal data. However, the emergence of generative techniques such as LLMs, SSL, Seq2Seq and diffusion models has opened up new possibilities for enhancing spatial-temporal data mining further. The paper provides a comprehensive analysis of generative technique-based spatial-temporal methods and introduces a standardized framework specifically designed for the spatial-temporal data mining pipeline. By offering a detailed review and a novel taxonomy of spatial-temporal methodology utilizing generative techniques, the paper enables a deeper understanding of the various techniques employed in this field. Furthermore, the paper highlights promising future research directions, urging researchers to delve deeper into spatial-temporal data mining. It emphasizes the need to explore untapped opportunities and push the boundaries of knowledge to unlock new insights and improve the effectiveness and efficiency of spatial-temporal data mining. By integrating generative techniques and providing a standardized framework, the paper contributes to advancing the field and encourages researchers to explore the vast potential of generative techniques in spatial-temporal data mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "2279762001",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2301457384",
                    "name": "Cheng Long"
                },
                {
                    "authorId": "2302286242",
                    "name": "Liangcai Su"
                },
                {
                    "authorId": "2273771682",
                    "name": "Xingwei He"
                },
                {
                    "authorId": "2294434616",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2279761135",
                    "name": "Tailin Wu"
                },
                {
                    "authorId": "2302520304",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "2184000509",
                    "name": "S. Yiu"
                },
                {
                    "authorId": "2301457785",
                    "name": "Qi Tian"
                },
                {
                    "authorId": "2293317939",
                    "name": "Christian S. Jensen"
                }
            ]
        },
        {
            "paperId": "e955fc889f86f82864d0aad6b8429ff79136d1f3",
            "title": "A Survey on Point-of-Interest Recommendation: Models, Architectures, and Security",
            "abstract": "The widespread adoption of smartphones and Location-Based Social Networks has led to a massive influx of spatio-temporal data, creating unparalleled opportunities for enhancing Point-of-Interest (POI) recommendation systems. These advanced POI systems are crucial for enriching user experiences, enabling personalized interactions, and optimizing decision-making processes in the digital landscape. However, existing surveys tend to focus on traditional approaches and few of them delve into cutting-edge developments, emerging architectures, as well as security considerations in POI recommendations. To address this gap, our survey stands out by offering a comprehensive, up-to-date review of POI recommendation systems, covering advancements in models, architectures, and security aspects. We systematically examine the transition from traditional models to advanced techniques such as large language models. Additionally, we explore the architectural evolution from centralized to decentralized and federated learning systems, highlighting the improvements in scalability and privacy. Furthermore, we address the increasing importance of security, examining potential vulnerabilities and privacy-preserving approaches. Our taxonomy provides a structured overview of the current state of POI recommendation, while we also identify promising directions for future research in this rapidly advancing field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "2324725719",
                    "name": "Peng Yang"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2279762001",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2273771682",
                    "name": "Xingwei He"
                },
                {
                    "authorId": "2184000509",
                    "name": "S. Yiu"
                },
                {
                    "authorId": null,
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "fdbef3cff38c3f96d91983821aed1dcbcf103a6b",
            "title": "Billiards Sports Analytics: Datasets and Tasks",
            "abstract": "Nowadays, it becomes a common practice to capture some data of sports games with devices such as GPS sensors and cameras and then use the data to perform various analyses on sports games, including tactics discovery, similar game retrieval, performance study, etc. While this practice has been conducted to many sports such as basketball and soccer, it remains largely unexplored on the billiards sports, which is mainly due to the lack of publicly available datasets. Motivated by this, we collect a dataset of billiards sports, which includes the layouts (i.e., locations) of billiards balls after performing break shots, called break shot layouts, the traces of the balls as a result of strikes (in the form of trajectories), and detailed statistics and performance indicators. We then study and develop techniques for three tasks on the collected dataset, including (1) prediction and (2) generation on the layouts data, and (3) similar billiards layout retrieval on the layouts data, which can serve different users such as coaches, players and fans. We conduct extensive experiments on the collected dataset and the results show that our methods perform effectively and efficiently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "2268029809",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "2301457384",
                    "name": "Cheng Long"
                },
                {
                    "authorId": "2184000509",
                    "name": "S. Yiu"
                }
            ]
        },
        {
            "paperId": "234112590ad71053798c146e2eeb0c51545265e4",
            "title": "Automated Spatio-Temporal Graph Contrastive Learning",
            "abstract": "Among various region embedding methods, graph-based region relation learning models stand out, owing to their strong structure representation ability for encoding spatial correlations with graph neural networks. Despite their effectiveness, several key challenges have not been well addressed in existing methods: i) Data noise and missing are ubiquitous in many spatio-temporal scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g., mobility traces) usually exhibits distribution heterogeneity across space and time. In such cases, current methods are vulnerable to the quality of the generated region graphs, which may lead to suboptimal performance. In this paper, we tackle the above challenges by exploring the Automated Spatio-Temporal graph contrastive learning paradigm (AutoST) over the heterogeneous region graph generated from multi-view data sources. Our AutoST framework is built upon a heterogeneous graph neural architecture to capture the multi-view region dependencies with respect to POI semantics, mobility flow patterns and geographical positions. To improve the robustness of our GNN encoder against data noise and distribution issues, we design an automated spatio-temporal augmentation scheme with a parameterized contrastive view generator. AutoST can adapt to the spatio-temporal heterogeneous graph with multi-view semantics well preserved. Extensive experiments for three downstream spatio-temporal mining tasks on several real-world datasets demonstrate the significant performance gain achieved by our AutoST over a variety of baselines. The code is publicly available at https://github.com/HKUDS/AutoST.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "2110926729",
                    "name": "Chao Huang"
                },
                {
                    "authorId": "1830455155",
                    "name": "Lianghao Xia"
                },
                {
                    "authorId": "50219447",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "2215613396",
                    "name": "Zhonghang Li"
                },
                {
                    "authorId": "151225851",
                    "name": "Siu-keung Yiu"
                }
            ]
        },
        {
            "paperId": "9fef56e98cce62dee65ef623d43ece7cb54f01ba",
            "title": "Improving Factual Error Correction by Learning to Inject Factual Errors",
            "abstract": "Factual error correction (FEC) aims to revise factual errors in false claims with minimal editing, making them faithful to the provided evidence. This task is crucial for alleviating the hallucination problem encountered by large language models. Given the lack of paired data (i.e., false claims and their corresponding correct claims), existing methods typically adopt the \u2018mask-then-correct\u2019 paradigm. This paradigm relies solely on unpaired false claims and correct claims, thus being referred to as distantly supervised methods. These methods require a masker to explicitly identify factual errors within false claims before revising with a corrector. However, the absence of paired data to train the masker makes accurately pinpointing factual errors within claims challenging. To mitigate this, we propose to improve FEC by Learning to Inject Factual Errors (LIFE), a three-step distantly supervised method: \u2018mask-corrupt-correct\u2019. Specifically, we first train a corruptor using the \u2018mask-then-corrupt\u2019 procedure, allowing it to deliberately introduce factual errors into correct text. The corruptor is then applied to correct claims, generating a substantial amount of paired data. After that, we filter out low-quality data, and use the remaining data to train a corrector. Notably, our corrector does not require a masker, thus circumventing the bottleneck associated with explicit factual error identification. Our experiments on a public dataset verify the effectiveness of LIFE in two key aspects: Firstly, it outperforms the previous best-performing distantly supervised method by a notable margin of 10.59 points in SARI Final (19.3% improvement). Secondly, even compared to ChatGPT prompted with in-context examples, LIFE achieves a superiority of 7.16 points in SARI Final.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273771682",
                    "name": "Xingwei He"
                },
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "15796861",
                    "name": "Alex Jin"
                },
                {
                    "authorId": "2273909915",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2273924496",
                    "name": "Yuan Yuan"
                },
                {
                    "authorId": "145964453",
                    "name": "S. Yiu"
                }
            ]
        },
        {
            "paperId": "aeb95aef649c6d8d031e83960f77cd798d069a0a",
            "title": "Spatial-Temporal Graph Learning with Adversarial Contrastive Adaptation",
            "abstract": "Spatial-temporal graph learning has emerged as a promising solution for modeling structured spatial-temporal data and learning region representations for various urban sensing tasks such as crime forecasting and traffic flow prediction. However, most existing models are vulnerable to the quality of the generated region graph due to the inaccurate graph-structured information aggregation schema. The ubiquitous spatial-temporal data noise and incompleteness in real-life scenarios pose challenges in generating high-quality region representations. To address this challenge, we propose a new spatial-temporal graph learning model (GraphST) for enabling effective self-supervised learning. Our proposed model is an adversarial contrastive learning paradigm that automates the distillation of crucial multi-view self-supervised information for robust spatial-temporal graph augmentation. We empower GraphST to adaptively identify hard samples for better self-supervision, enhancing the representation discrimination ability and robustness. In addition, we introduce a cross-view contrastive learning paradigm to model the inter-dependencies across view-specific region representations and preserve underlying relation heterogeneity. We demonstrate the superiority of our proposed GraphST method in various spatial-temporal prediction tasks on real-life datasets. We release our model implementation via the link: \\url{https://github.com/HKUDS/GraphST}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "2110926729",
                    "name": "Chao Huang"
                },
                {
                    "authorId": "1830455155",
                    "name": "Lianghao Xia"
                },
                {
                    "authorId": "50219447",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "151225851",
                    "name": "Siu-keung Yiu"
                },
                {
                    "authorId": "2066468314",
                    "name": "Ruihua Han"
                }
            ]
        },
        {
            "paperId": "07cf6ec1c80af4d3efaa3f07033cce184c19ab2c",
            "title": "On Inferring User Socioeconomic Status with Mobility Records",
            "abstract": "When users move in a physical space (e.g., an urban space), they would have some records called mobility records (e.g., trajectories) generated by devices such as mobile phones and GPS devices. Naturally, mobility records capture essential information of how users work, live and entertain in their daily lives, and therefore, they have been used in a wide range of tasks such as user profile inference, mobility prediction and traffic management. In this paper, we expand this line of research by investigating the problem of inferring user socioeconomic statuses (such as prices of users\u2019 living houses as a proxy of users\u2019 socioeconomic statuses) based on their mobility records, which can potentially be used in real-life applications such as the car loan business. For this task, we propose a socioeconomic-aware deep model called DeepSEI. The DeepSEI model incorporates two networks called deep network and recurrent network, which extract the features of the mobility records from three aspects, namely spatiality, temporality and activity, one at a coarse level and the other at a detailed level. We conduct extensive experiments on real mobility records data, POI data and house prices data. The results verify that the DeepSEI model achieves superior performance than existing studies. All datasets used in this paper will be made publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50219447",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "14697929",
                    "name": "Mingrui Liu"
                },
                {
                    "authorId": "49527357",
                    "name": "Cheng Long"
                },
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "112875106",
                    "name": "Jiangneng Li"
                },
                {
                    "authorId": "40567279",
                    "name": "Chun-Hua Miao"
                }
            ]
        },
        {
            "paperId": "a43ddb31e572142a9452f49f35b2ca6461bf9f6b",
            "title": "On Predicting and Generating a Good Break Shot in Billiards Sports",
            "abstract": "1 New Billiards Datasets 1.1 Composition The dataset covers 227 players and 94 international professional 9-ball tournaments. We collect the billiard dataset for frames, turns and strikes. In particular, (1) Frame: in each 9-ball game, there are a certain number of rounds (called frames). A frame starts with a break shot and ends when one player pockets the ball 9. (2) Turn: in each frame, the players take turns to pocket the balls with legal strikes and they switch turns when a player misses the ball or makes fouls. For example, if one player pockets all balls in a row without missing any balls or fouls after the break shot, there would be only one turn in this frame. (3) Strike: in each turn of a player, he/she performs strikes so as to pocket the object balls until he/she misses the balls or makes fouls. For example, the first strike in each frame corresponds to the break shot in the frame. Data of Frames. Our dataset covers 3,019 frames and includes for each frame: (1) the break shot layout which consists of locations of the balls that remain on the pool table after the break shot and (2) three performance indicators. The first one is a binary tag indicating clear or not (where clear means that all objects are potted by the player who does the break shot, given the break shot layout and within the first turn and not clear means the other case). The second one is a binary tag indicating win or not (where win means that the player who does the break shot wins for the frame eventually and not win means the other case). The third one is the number of potted balls by the player who does the break shot, given the break shot layout and within the first turn. Data of Turns. Our dataset covers 6,637 turns and includes for each turn: (1) the player name, (2) the number of strikes by the player, (3) the order of balls potted into pockets, (4) the type of the foul if any, and (5) data of the strikes that are within the turn as described below. Data of Strikes. Our dataset covers 2,082 strikes and includes for each strike: (1) the trajectories of the cue ball and object balls that move as a result of the",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500649148",
                    "name": "Qianru Zhang"
                },
                {
                    "authorId": "50219447",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "49527357",
                    "name": "Cheng Long"
                },
                {
                    "authorId": "145964453",
                    "name": "S. Yiu"
                }
            ]
        }
    ]
}