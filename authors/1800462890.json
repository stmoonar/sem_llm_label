{
    "authorId": "1800462890",
    "papers": [
        {
            "paperId": "1b516059eee07c1f7dae4008d7109e0f6bff64bb",
            "title": "Large Language Models in Mental Health Care: a Scoping Review",
            "abstract": "The integration of large language models (LLMs) in mental health care is an emerging field. There is a need to systematically review the application outcomes and delineate the advantages and limitations in clinical settings. This review aims to provide a comprehensive overview of the use of LLMs in mental health care, assessing their efficacy, challenges, and potential for future applications. A systematic search was conducted across multiple databases including PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and PsyArXiv in November 2023. All forms of original research, peer-reviewed or not, published or disseminated between October 1, 2019, and December 2, 2023, are included without language restrictions if they used LLMs developed after T5 and directly addressed research questions in mental health care settings. From an initial pool of 313 articles, 34 met the inclusion criteria based on their relevance to LLM application in mental health care and the robustness of reported outcomes. Diverse applications of LLMs in mental health care are identified, including diagnosis, therapy, patient engagement enhancement, etc. Key challenges include data availability and reliability, nuanced handling of mental states, and effective evaluation methods. Despite successes in accuracy and accessibility improvement, gaps in clinical applicability and ethical considerations were evident, pointing to the need for robust data, standardized evaluations, and interdisciplinary collaboration. LLMs hold substantial promise for enhancing mental health care. For their full potential to be realized, emphasis must be placed on developing robust datasets, development and evaluation frameworks, ethical guidelines, and interdisciplinary collaborations to address current limitations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "2218961178",
                    "name": "Fenglin Liu"
                },
                {
                    "authorId": "2003396186",
                    "name": "Kailai Yang"
                },
                {
                    "authorId": "2278650455",
                    "name": "Zehan Li"
                },
                {
                    "authorId": "2284592548",
                    "name": "Hongbin Na"
                },
                {
                    "authorId": "48236249",
                    "name": "Y. Sheu"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2278425645",
                    "name": "Lauren V. Moran"
                },
                {
                    "authorId": "2240623492",
                    "name": "Sophia Ananiadou"
                },
                {
                    "authorId": "2313483382",
                    "name": "Andrew Beam"
                }
            ]
        },
        {
            "paperId": "c229cdee4e37435a8622f62b159a15d2d611ea39",
            "title": "Vision-Language Models Meet Meteorology: Developing Models for Extreme Weather Events Detection with Heatmaps",
            "abstract": "Real-time detection and prediction of extreme weather protect human lives and infrastructure. Traditional methods rely on numerical threshold setting and manual interpretation of weather heatmaps with Geographic Information Systems (GIS), which can be slow and error-prone. Our research redefines Extreme Weather Events Detection (EWED) by framing it as a Visual Question Answering (VQA) problem, thereby introducing a more precise and automated solution. Leveraging Vision-Language Models (VLM) to simultaneously process visual and textual data, we offer an effective aid to enhance the analysis process of weather heatmaps. Our initial assessment of general-purpose VLMs (e.g., GPT-4-Vision) on EWED revealed poor performance, characterized by low accuracy and frequent hallucinations due to inadequate color differentiation and insufficient meteorological knowledge. To address these challenges, we introduce ClimateIQA, the first meteorological VQA dataset, which includes 8,760 wind gust heatmaps and 254,040 question-answer pairs covering four question types, both generated from the latest climate reanalysis data. We also propose Sparse Position and Outline Tracking (SPOT), an innovative technique that leverages OpenCV and K-Means clustering to capture and depict color contours in heatmaps, providing ClimateIQA with more accurate color spatial location information. Finally, we present Climate-Zoo, the first meteorological VLM collection, which adapts VLMs to meteorological applications using the ClimateIQA dataset. Experiment results demonstrate that models from Climate-Zoo substantially outperform state-of-the-art general VLMs, achieving an accuracy increase from 0% to over 90% in EWED verification. The datasets and models in this study are publicly available for future climate science research: https://github.com/AlexJJJChen/Climate-Zoo.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301490231",
                    "name": "Jian Chen"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "52290752",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "2265583496",
                    "name": "Meng Cao"
                },
                {
                    "authorId": "2307300961",
                    "name": "Yaowei Li"
                },
                {
                    "authorId": "2307415853",
                    "name": "Zixuan Yuan"
                },
                {
                    "authorId": "2301770063",
                    "name": "Bing Zhu"
                },
                {
                    "authorId": "2301541118",
                    "name": "Junwei Liang"
                }
            ]
        },
        {
            "paperId": "ffac965686a42acc99e5e6628485cdf2bfb41e1b",
            "title": "FinTextQA: A Dataset for Long-form Financial Question Answering",
            "abstract": "Accurate evaluation of financial question answering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces FinTextQA, a novel dataset for long-form question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301490231",
                    "name": "Jian Chen"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "2301456174",
                    "name": "Yingxin Loh"
                },
                {
                    "authorId": "2301592651",
                    "name": "Kehui Chen"
                },
                {
                    "authorId": "2224621614",
                    "name": "Ziyuan Li"
                },
                {
                    "authorId": "2301770063",
                    "name": "Bing Zhu"
                },
                {
                    "authorId": "2301541118",
                    "name": "Junwei Liang"
                }
            ]
        },
        {
            "paperId": "4b4ee637ef5107299212479c37a6594db5a72227",
            "title": "Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset",
            "abstract": "Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could generate relevant reasoning and demonstrate improved performance after finetuning, they fall short of a desired standard, indicating ample room for improvement. To the best of our knowledge, CMExam is the first Chinese medical exam dataset to provide comprehensive medical annotations. The experiments and findings of LLM evaluation also provide valuable insights into the challenges and potential solutions in developing Chinese medical QA systems and LLM evaluation pipelines. The dataset and relevant code are available at https://github.com/williamliujl/CMExam.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "52290752",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "2069521803",
                    "name": "Zhongyu Tian"
                },
                {
                    "authorId": "2170752745",
                    "name": "Andrew Liu"
                },
                {
                    "authorId": "3408469",
                    "name": "Helin Wang"
                },
                {
                    "authorId": "2061592207",
                    "name": "Chenyu You"
                },
                {
                    "authorId": "2107781537",
                    "name": "Zhenhua Guo"
                },
                {
                    "authorId": "145081293",
                    "name": "Lei Zhu"
                },
                {
                    "authorId": "2154744792",
                    "name": "Michael Lingzhi Li"
                }
            ]
        },
        {
            "paperId": "62a4b1ea4d091f1c5700dc06c9bf2503ea9ecfac",
            "title": "Streamlining Social Media Information Retrieval for COVID-19 Research with Deep Learning (preprint)",
            "abstract": "Objective: Social media-based public health research is crucial for epidemic surveillance, but most studies identify relevant corpora with keyword-matching. This study develops a system to streamline the process of curating colloquial medical dictionaries. We demonstrate the pipeline by curating a UMLS-colloquial symptom dictionary from COVID-19-related tweets as proof of concept. Methods: COVID-19-related tweets from February 1, 2020, to April 30, 2022 were used. The pipeline includes three modules: a named entity recognition module to detect symptoms in tweets; an entity normalization module to aggregate detected entities; and a mapping module that iteratively maps entities to Unified Medical Language System concepts. A random 500 entity sample were drawn from the final dictionary for accuracy validation. Additionally, we conducted a symptom frequency distribution analysis to compare our dictionary to a pre-defined lexicon from previous research. Results: We identified 498,480 unique symptom entity expressions from the tweets. Pre-processing reduces the number to 18,226. The final dictionary contains 38,175 unique expressions of symptoms that can be mapped to 966 UMLS concepts (accuracy = 95%). Symptom distribution analysis found that our dictionary detects more symptoms and is effective at identifying psychiatric disorders like anxiety and depression, often missed by pre-defined lexicons. Conclusions: This study advances public health research by implementing a novel, systematic pipeline for curating symptom lexicons from social media data. The final lexicon's high accuracy, validated by medical professionals, underscores the potential of this methodology to reliably interpret and categorize vast amounts of unstructured social media data into actionable medical insights across diverse linguistic and regional landscapes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "2174092254",
                    "name": "Shixu Lin"
                },
                {
                    "authorId": "2186950276",
                    "name": "Minghui Li"
                },
                {
                    "authorId": "2145064768",
                    "name": "Yujie Zhang"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2055654560",
                    "name": "Y. Lo"
                },
                {
                    "authorId": "2116636556",
                    "name": "Li Zhou"
                },
                {
                    "authorId": "1688428",
                    "name": "Jie Yang"
                }
            ]
        },
        {
            "paperId": "6ec3b6306b53a5685bab6d1a58b77c0d26e2f562",
            "title": "Rethinking Multi-Interest Learning for Candidate Matching in Recommender Systems",
            "abstract": "Existing research efforts for multi-interest candidate matching in recommender systems mainly focus on improving model architecture or incorporating additional information, neglecting the importance of training schemes. This work revisits the training framework and uncovers two major problems hindering the expressiveness of learned multi-interest representations. First, the current training objective (i.e., uniformly sampled softmax) fails to effectively train discriminative representations in a multi-interest learning scenario due to the severe increase in easy negative samples. Second, a routing collapse problem is observed where each learned interest may collapse to express information only from a single item, resulting in information loss. To address these issues, we propose the REMI framework, consisting of an Interest-aware Hard Negative mining strategy (IHN) and a Routing Regularization (RR) method. IHN emphasizes interest-aware hard negatives by proposing an ideal sampling distribution and developing a Monte-Carlo strategy for efficient approximation. RR prevents routing collapse by introducing a novel regularization term on the item-to-interest routing matrices. These two components enhance the learned multi-interest representations from both the optimization objective and the composition information. REMI is a general framework that can be readily applied to various existing multi-interest candidate matching methods. Experiments on three real-world datasets show our method can significantly improve state-of-the-art methods with easy implementation and negligible computational overhead. The source code is available at https://github.com/Tokkiu/REMI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2154871075",
                    "name": "Yueqi Xie"
                },
                {
                    "authorId": "2118389668",
                    "name": "Jingqi Gao"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2190432576",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "2173708050",
                    "name": "Jaeboum Kim"
                },
                {
                    "authorId": "2397264",
                    "name": "Fangzhao Wu"
                },
                {
                    "authorId": "2118021616",
                    "name": "Sunghun Kim"
                }
            ]
        },
        {
            "paperId": "85722b13631d9846866d45ff2bfc2a2fe1026ac8",
            "title": "LLMRec: Benchmarking Large Language Models on Recommendation Task",
            "abstract": "Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "3741691",
                    "name": "Chao-Hong Liu"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2190432576",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "52290752",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "2165702320",
                    "name": "Kangan Zhou"
                },
                {
                    "authorId": "2154871075",
                    "name": "Yueqi Xie"
                },
                {
                    "authorId": "150346771",
                    "name": "Yuwei Cao"
                },
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2061592207",
                    "name": "Chenyu You"
                },
                {
                    "authorId": "2233087809",
                    "name": "Philip S.Yu"
                }
            ]
        },
        {
            "paperId": "8e6c4425e48b09d64827c64d8de0008f41f9be54",
            "title": "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model",
            "abstract": "Integrating large language models (LLMs) into healthcare holds great potential but faces challenges. Pre-training LLMs from scratch for domains like medicine is resource-heavy and often unfeasible. On the other hand, sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions and may not tap into domain-specific insights. In response, we present a multi-stage training method combining Domain-specific Continued Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). In addition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages. The medical LLM trained with our pipeline, Qilin-Med, shows substantial performance improvement. In the CPT and SFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set, respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by 7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on the Huatuo-26M test set, bringing further improvement to the SFT phase (12.69 in BLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced the model's performance through the Retrieval Augmented Generation (RAG) approach. Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8% on CMExam. These results highlight the contribution of our novel training approach in building LLMs for medical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258550058",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "52290752",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "2170752745",
                    "name": "Andrew Liu"
                }
            ]
        },
        {
            "paperId": "a44d904432cdda9d188bfff8e31619e02f2a4d89",
            "title": "Attention Calibration for Transformer-based Sequential Recommendation",
            "abstract": "Transformer-based sequential recommendation (SR) has been booming in recent years, with the self-attention mechanism as its key component. Self-attention has been widely believed to be able to effectively select those informative and relevant items from a sequence of interacted items for next-item prediction via learning larger attention weights for these items. However, this may not always be true in reality. Our empirical analysis of some representative Transformer-based SR models reveals that it is not uncommon for large attention weights to be assigned to less relevant items, which can result in inaccurate recommendations. Through further in-depth analysis, we find two factors that may contribute to such inaccurate assignment of attention weights:sub-optimal position encoding andnoisy input. To this end, in this paper, we aim to address this significant yet challenging gap in existing works. To be specific, we propose a simple yet effective framework called Attention Calibration for Transformer-based Sequential Recommendation (AC-TSR). In AC-TSR, a novel spatial calibrator and adversarial calibrator are designed respectively to directly calibrates those incorrectly assigned attention weights. The former is devised to explicitly capture the spatial relationships (i.e., order and distance) among items for more precise calculation of attention weights. The latter aims to redistribute the attention weights based on each item's contribution to the next-item prediction. AC-TSR is readily adaptable and can be seamlessly integrated into various existing transformer-based SR models. Extensive experimental results on four benchmark real-world datasets demonstrate the superiority of our proposed AC-TSR via significant recommendation performance enhancements. The source code is available at https://github.com/AIM-SE/AC-TSR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2190432576",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "2154871075",
                    "name": "Yueqi Xie"
                },
                {
                    "authorId": "2118389668",
                    "name": "Jingqi Gao"
                },
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2156009696",
                    "name": "Jae Boum Kim"
                },
                {
                    "authorId": "2061592207",
                    "name": "Chenyu You"
                },
                {
                    "authorId": "2118021616",
                    "name": "Sunghun Kim"
                }
            ]
        },
        {
            "paperId": "afd609e63462aff0e4d2ccd4051a4ce7041511d8",
            "title": "Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study",
            "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive performance across various vision and language tasks, yet their potential applications in recommendation tasks with visual assistance remain unexplored. To bridge this gap, we present a preliminary case study investigating the recommendation capabilities of GPT-4V(ison), a recently released LMM by OpenAI. We construct a series of qualitative test samples spanning multiple domains and employ these samples to assess the quality of GPT-4V's responses within recommendation scenarios. Evaluation results on these test samples prove that GPT-4V has remarkable zero-shot recommendation abilities across diverse domains, thanks to its robust visual-text comprehension capabilities and extensive general knowledge. However, we have also identified some limitations in using GPT-4V for recommendations, including a tendency to provide similar responses when given similar inputs. This report concludes with an in-depth discussion of the challenges and research opportunities associated with utilizing GPT-4V in recommendation scenarios. Our objective is to explore the potential of extending LMMs from vision and language tasks to recommendation tasks. We hope to inspire further research into next-generation multimodal generative recommendation models, which can enhance user experiences by offering greater diversity and interactivity. All images and prompts used in this report will be accessible at https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2265583496",
                    "name": "Meng Cao"
                },
                {
                    "authorId": "2265951415",
                    "name": "You-Liang Huang"
                },
                {
                    "authorId": "2258550058",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "2265621321",
                    "name": "Peiyan Zhang"
                },
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "2154871075",
                    "name": "Yueqi Xie"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "2173708050",
                    "name": "Jaeboum Kim"
                }
            ]
        }
    ]
}