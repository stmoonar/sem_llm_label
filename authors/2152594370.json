{
    "authorId": "2152594370",
    "papers": [
        {
            "paperId": "4ddd2883c73b3292bbad9d70a7c14cf4831f4562",
            "title": "Hierarchical Threshold Pruning Based on Uniform Response Criterion",
            "abstract": "Convolutional neural networks (CNNs) have been successfully applied to various fields. However, CNNs\u2019 overparameterization requires more memory and training time, making it unsuitable for some resource-constrained devices. To address this issue, filter pruning as one of the most efficient ways was proposed. In this article, we propose a feature-discrimination-based filter importance criterion, uniform response criterion (URC), as a key component of filter pruning. It converts the maximum activation responses into probabilities and then measures the importance of the filter through the distribution of these probabilities over classes. However, applying URC directly to global threshold pruning may cause some problems. The first problem is that some layers will be completely pruned under global pruning settings. The second problem is that global threshold pruning neglects that filters in different layers have different importance. To address these issues, we propose hierarchical threshold pruning (HTP) with URC. It performs a pruning step limited in a relatively redundant layer rather than comparing the filters\u2019 importance across all layers, which can avoid some important filters being pruned. The effectiveness of our method benefits from three techniques: 1) measuring filter importance by URC; 2) normalizing filter scores; and 3) conducting prune in relatively redundant layers. Extensive experiments on CIFAR-10/100 and ImageNet show that our method achieves the state-of-the-art performance on multiple benchmarks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1877805",
                    "name": "Yaguan Qian"
                },
                {
                    "authorId": "2152629091",
                    "name": "Zhiqiang He"
                },
                {
                    "authorId": "2155376120",
                    "name": "Yu-qun Wang"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2056681953",
                    "name": "Xiang Ling"
                },
                {
                    "authorId": "1697068",
                    "name": "Zhaoquan Gu"
                },
                {
                    "authorId": "2109042326",
                    "name": "Haijiang Wang"
                },
                {
                    "authorId": "8286187",
                    "name": "Shaoning Zeng"
                },
                {
                    "authorId": "3083255",
                    "name": "Wassim Swaileh"
                }
            ]
        },
        {
            "paperId": "80defb42282001f0d4f639ddeeae7f60cbe96c47",
            "title": "F2AT: Feature-Focusing Adversarial Training via Disentanglement of Natural and Perturbed Patterns",
            "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by well-designed perturbations. This could lead to disastrous results on critical applications such as self-driving cars, surveillance security, and medical diagnosis. At present, adversarial training is one of the most effective defenses against adversarial examples. However, traditional adversarial training makes it difficult to achieve a good trade-off between clean accuracy and robustness since spurious features are still learned by DNNs. The intrinsic reason is that traditional adversarial training makes it difficult to fully learn core features from adversarial examples when adversarial noise and clean examples cannot be disentangled. In this paper, we disentangle the adversarial examples into natural and perturbed patterns by bit-plane slicing. We assume the higher bit-planes represent natural patterns and the lower bit-planes represent perturbed patterns, respectively. We propose a Feature-Focusing Adversarial Training (F$^2$AT), which differs from previous work in that it enforces the model to focus on the core features from natural patterns and reduce the impact of spurious features from perturbed patterns. The experimental results demonstrated that F$^2$AT outperforms state-of-the-art methods in clean accuracy and adversarial robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1877805",
                    "name": "Yaguan Qian"
                },
                {
                    "authorId": "2261362206",
                    "name": "Chenyu Zhao"
                },
                {
                    "authorId": "2261280577",
                    "name": "Zhaoquan Gu"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2261280308",
                    "name": "Shouling Ji"
                },
                {
                    "authorId": "2261287883",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2261328655",
                    "name": "Boyang Zhou"
                },
                {
                    "authorId": "2113325750",
                    "name": "Pan Zhou"
                }
            ]
        },
        {
            "paperId": "8e6d1470e8d52b46b07db84bcdc727963b6bf7dd",
            "title": "MalGraph: Hierarchical Graph Neural Networks for Robust Windows Malware Detection",
            "abstract": "With the ever-increasing malware threats, malware detection plays an indispensable role in protecting information systems. Although tremendous research efforts have been made, there are still two key challenges hindering them from being applied to accurately and robustly detect malwares. Firstly, most of them represent executables with shallow features, but ignore their semantic and structural information. Secondly, they are primarily based on representations that can be easily modified by attackers and thus cannot provide robustness against adversarial attacks. To tackle the challenges, we present MalGraph, which first represents executables with hierarchical graphs and then uses an end-to-end learning framework based on graph neural networks for malware detection. In particular, a hierarchical graph consists of a function call graph that captures the interaction semantics among different functions at the inter-function level and corresponding control-flow graphs for learning the structural semantics of each function at the intra-function level. We argue the abstraction and hierarchy nature of hierarchical graphs makes them not only easy to capture rich structural information of executables, but also be immune to adversarial attacks. Evaluations show that MalGraph not only outperforms state-of-the-art malware detection, but also exhibits stronger robustness against adversarial attacks by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1701151",
                    "name": "Xiang Ling"
                },
                {
                    "authorId": "2116666963",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2066621473",
                    "name": "Wei Deng"
                },
                {
                    "authorId": "2149703865",
                    "name": "Zhenqing Qu"
                },
                {
                    "authorId": "2155241302",
                    "name": "Jiangyu Zhang"
                },
                {
                    "authorId": "38654394",
                    "name": "Shenmin Zhang"
                },
                {
                    "authorId": "1901958",
                    "name": "Tengyu Ma"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2118839825",
                    "name": "Chunming Wu"
                },
                {
                    "authorId": "2081160",
                    "name": "S. Ji"
                }
            ]
        },
        {
            "paperId": "97842b86d317cb48c6c8b097ff405d492ac09a6d",
            "title": "Hessian-Free Second-Order Adversarial Examples for Adversarial Learning",
            "abstract": "Recent studies show deep neural networks (DNNs) are extremely vulnerable to the elaborately designed adversarial examples. Adversarial learning with those adversarial examples has been proved as one of the most effective methods to defend against such an attack. At present, most existing adversarial examples generation methods are based on first-order gradients, which can hardly further improve models' robustness, especially when facing second-order adversarial attacks. Compared with first-order gradients, second-order gradients provide a more accurate approximation of the loss landscape with respect to natural examples. Inspired by this, our work crafts second-order adversarial examples and uses them to train DNNs. Nevertheless, second-order optimization involves time-consuming calculation for Hessian-inverse. We propose an approximation method through transforming the problem into an optimization in the Krylov subspace, which remarkably reduce the computational complexity to speed up the training procedure. Extensive experiments conducted on the MINIST and CIFAR-10 datasets show that our adversarial learning with second-order adversarial examples outperforms other fisrt-order methods, which can improve the model robustness against a wide range of attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1877805",
                    "name": "Yaguan Qian"
                },
                {
                    "authorId": "2155376120",
                    "name": "Yu-qun Wang"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "1697068",
                    "name": "Zhaoquan Gu"
                },
                {
                    "authorId": "2171112014",
                    "name": "Yu-Shuang Guo"
                },
                {
                    "authorId": "3083255",
                    "name": "Wassim Swaileh"
                }
            ]
        },
        {
            "paperId": "da155d953755e95e80acbf4a6ef163a89bb68daf",
            "title": "Towards the Desirable Decision Boundary by Moderate-Margin Adversarial Training",
            "abstract": "Adversarial training, as one of the most effective defense methods against adversarial attacks, tends to learn an inclusive decision boundary to increase the robustness of deep learning models. However, due to the large and unnecessary increase in the margin along adversarial directions, adversarial training causes heavy cross-over between natural examples and adversarial examples, which is not conducive to balancing the trade-off between robustness and natural accuracy. In this paper, we propose a novel adversarial training scheme to achieve a better trade-off between robustness and natural accuracy. It aims to learn a moderate-inclusive decision boundary, which means that the margins of natural examples under the decision boundary are moderate. We call this scheme Moderate-Margin Adversarial Training (MMAT), which generates finer-grained adversarial examples to mitigate the cross-over problem. We also take advantage of logits from a teacher model that has been well-trained to guide the learning of our model. Finally, MMAT achieves high natural accuracy and robustness under both black-box and white-box attacks. On SVHN, for example, state-of-the-art robustness and natural accuracy are achieved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153398079",
                    "name": "Xiaoyu Liang"
                },
                {
                    "authorId": "1877805",
                    "name": "Yaguan Qian"
                },
                {
                    "authorId": "2176928632",
                    "name": "Jianchang Huang"
                },
                {
                    "authorId": "1701151",
                    "name": "Xiang Ling"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2118839825",
                    "name": "Chunming Wu"
                },
                {
                    "authorId": "3083255",
                    "name": "Wassim Swaileh"
                }
            ]
        },
        {
            "paperId": "261e4fd0fcb1e64dd959759259e62ac03009d317",
            "title": "Exploring Security Vulnerabilities of Deep Learning Models by Adversarial Attacks",
            "abstract": "Nowadays, deep learning models play an important role in a variety of scenarios, such as image classification, natural language processing, and speech recognition. However, deep learning models are shown to be vulnerable; a small change to the original data may affect the output of the model, which may incur severe consequences such as misrecognition and privacy leakage. The intentionally modified data is referred to as adversarial examples. In this paper, we explore the security vulnerabilities of deep learning models designed for textual analysis. Specifically, we propose a visual similar word replacement (VSWR) algorithm to generate adversarial examples against textual analysis models. By using adversarial examples as the input of deep learning models, we verified that deep learning models are vulnerable to such adversarial attacks. We have conducted experiments on several sentiment analysis deep learning models to evaluate the performance. The results also confirmed that the generated adversarial examples could successfully attack deep learning models. As the number of modified words increases, the model prediction accuracy becomes lower. This kind of adversarial attack implies security vulnerabilities of deep learning models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2044333182",
                    "name": "Xiaopeng Fu"
                },
                {
                    "authorId": "1697068",
                    "name": "Zhaoquan Gu"
                },
                {
                    "authorId": "2087524383",
                    "name": "Weihong Han"
                },
                {
                    "authorId": "1877805",
                    "name": "Yaguan Qian"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                }
            ]
        },
        {
            "paperId": "5467f96de7af305e01c06dab3ee55aa10f398628",
            "title": "ViR: the Vision Reservoir",
            "abstract": "The most recent year has witnessed the success of applying the Vision Transformer (ViT) for image classification. However, there are still evidences indicating that ViT often suffers following two aspects, i) the high computation and the memory burden from applying the multiple Transformer layers for pre-training on a large-scale dataset, ii) the over-fitting when training on small datasets from scratch. To address these problems, a novel method, namely, Vision Reservoir computing (ViR), is proposed here for image classification, as a parallel to ViT. By splitting each image into a sequence of tokens with fixed length, the ViR constructs a pure reservoir with a nearly fully connected topology to replace the Transformer module in ViT. Two kinds of deep ViR models are subsequently proposed to enhance the network performance. Comparative experiments between the ViR and the ViT are carried out on several image classification benchmarks. Without any pre-training process, the ViR outperforms the ViT in terms of both model and computational complexity. Specifically, the number of parameters of the ViR is about 15% even 5% of the ViT, and the memory footprint is about 20% to 40% of the ViT. The superiority of the ViR performance is explained by Small-World characteristics, Lyapunov exponents, and memory capacity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "98786113",
                    "name": "Xian Wei"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2107989070",
                    "name": "Mingsong Chen"
                },
                {
                    "authorId": "2149088287",
                    "name": "Ji Yuan"
                },
                {
                    "authorId": "2057697721",
                    "name": "Hai Lan"
                },
                {
                    "authorId": "2155607422",
                    "name": "Jiehuang Shi"
                },
                {
                    "authorId": "2143754167",
                    "name": "Xuan Tang"
                },
                {
                    "authorId": "2057048887",
                    "name": "Bo Jin"
                },
                {
                    "authorId": "2155230058",
                    "name": "Guozhang Chen"
                },
                {
                    "authorId": "2152325544",
                    "name": "Dongping Yang"
                }
            ]
        },
        {
            "paperId": "880da0cd5b9e8fe1303b6c17e9ae526e1c61879c",
            "title": "Progressive Depth Learning for Single Image Dehazing",
            "abstract": "The formulation of the hazy image is mainly dominated by the reflected lights and ambient airlight. Existing dehazing methods often ignore the depth cues and fail in distant areas where heavier haze disturbs the visibility. However, we note that the guidance of the depth information for transmission estimation could remedy the decreased visibility as distances increase. In turn, the good transmission estimation could facilitate the depth estimation for hazy images. In this paper, a deep end-to-end model that iteratively estimates image depths and transmission maps is proposed to perform an effective depth prediction for hazy images and improve the dehazing performance with the guidance of depth information. The image depth and transmission map are progressively refined to better restore the dehazed image. Our approach benefits from explicitly modeling the inner relationship of image depth and transmission map, which is especially effective for distant hazy areas. Extensive results on the benchmarks demonstrate that our proposed network performs favorably against the state-of-the-art dehazing methods in terms of depth estimation and haze removal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2734736",
                    "name": "Yudong Liang"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "41127426",
                    "name": "Jiaying Liu"
                },
                {
                    "authorId": "2023739583",
                    "name": "Deyu Li"
                },
                {
                    "authorId": "3373601",
                    "name": "Sanping Zhou"
                },
                {
                    "authorId": "144850642",
                    "name": "Wenqi Ren"
                }
            ]
        },
        {
            "paperId": "a18b85670181ee4a82284168e5c0b58a6d4d54fc",
            "title": "Progressive residual learning for single image dehazing",
            "abstract": "The recent physical model-free dehazing methods have achieved state-of-the-art performances. However, without the guidance of physical models, the performances degrade rapidly when applied to real scenarios due to the unavailable or insufficient data problems. On the other hand, the physical model-based methods have better interpretability but suffer from multi-objective optimizations of parameters, which may lead to sub-optimal dehazing results. In this paper, a progressive residual learning strategy has been proposed to combine the physical model-free dehazing process with reformulated scattering model-based dehazing operations, which enjoys the merits of dehazing methods in both categories. Specifically, the global atmosphere light and transmission maps are interactively optimized with the aid of accurate residual information and preliminary dehazed restorations from the initial physical model-free dehazing process. The proposed method performs favorably against the state-of-the-art methods on public dehazing benchmarks with better model interpretability and adaptivity for complex hazy data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2734736",
                    "name": "Yudong Liang"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "41127426",
                    "name": "Jiaying Liu"
                },
                {
                    "authorId": "2023739583",
                    "name": "Deyu Li"
                },
                {
                    "authorId": "2087074282",
                    "name": "Yuhua Qian"
                },
                {
                    "authorId": "144850642",
                    "name": "Wenqi Ren"
                }
            ]
        },
        {
            "paperId": "ae40dcba390a07a814962df9818d2b94764d7f2d",
            "title": "Towards Speeding up Adversarial Training in Latent Spaces",
            "abstract": "Adversarial training is wildly considered as the most effective way to defend against adversarial examples. However, existing adversarial training methods consume unbearable time cost, since they need to generate adversarial examples in the input space, which accounts for the main part of total time-consuming. For speeding up the training process, we propose a novel adversarial training method that does not need to generate real adversarial examples. We notice that a clean example is closer to the decision boundary of the class with the second largest logit component than any other class besides its own class. Thus, by adding perturbations to logits to generate Endogenous Adversarial Examples(EAEs) -- adversarial examples in the latent space, it can avoid calculating gradients to speed up the training process. We further gain a deep insight into the existence of EAEs by the theory of manifold. To guarantee the added perturbation is within the range of constraint, we use statistical distributions to select seed examples to craft EAEs. Extensive experiments are conducted on CIFAR-10 and ImageNet, and the results show that compare with state-of-the-art\"Free\"and\"Fast\"methods, our EAE adversarial training not only shortens the training time, but also enhances the robustness of the model. Moreover, the EAE adversarial training has little impact on the accuracy of clean examples than the existing methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1877805",
                    "name": "Yaguan Qian"
                },
                {
                    "authorId": "2061559370",
                    "name": "Qiqi Shao"
                },
                {
                    "authorId": "2068296313",
                    "name": "Tengteng Yao"
                },
                {
                    "authorId": "2152594370",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "8286187",
                    "name": "Shaoning Zeng"
                },
                {
                    "authorId": "1697068",
                    "name": "Zhaoquan Gu"
                },
                {
                    "authorId": "3083255",
                    "name": "Wassim Swaileh"
                }
            ]
        }
    ]
}