{
    "authorId": "144733334",
    "papers": [
        {
            "paperId": "120265f96e0fc5961a9db204f5353f6801604b02",
            "title": "On the Importance of Appearance and Interaction Feature Representations for Person Re-identification",
            "abstract": "In recent person re-identification (Re-ID) approaches, combining global and local appearance-based features has been shown to increase performance effectively. These types of models are often characterized by multiple branches that act as experts for specific local regions or global high-level semantic features. We argue that attention mechanisms can be useful for multi-branch Re-ID models by creating more robust representations based on the interaction of informative image features. In this paper, we investigate this idea and propose a novel multi-branch architecture with experts that learn distinct representations based on (i) the global image appearance and (ii) the interaction between features. Unlike former methods with local experts acting on partitions that are fixed a-priori, our feature interaction expert uses a novel attention-based pooling to automatically extract semantically-rich and discriminative features from different regions of a person image. Compared with existing attention-based algorithms, our method maintains the feature interaction information separately in order to discriminate between identities. Our approach achieves state-of-the-art performance across three popular benchmarks - CUHK03, Market1501 and MSMT17. Furthermore, saliency visualizations show that appearance and interaction experts learn complementary representations that attend to multiple discriminant regions, leading to improved classification ability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80428389",
                    "name": "R. Blythman"
                },
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "2052341684",
                    "name": "Christopher Murray"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                }
            ]
        },
        {
            "paperId": "21e4fdd6cfb8d5448a80340677d9b6b3205296d1",
            "title": "Guided Zoom: Zooming into Network Evidence to Refine Fine-Grained Model Decisions",
            "abstract": "In state-of-the-art deep single-label classification models, the top-<inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"bargal-ieq1-3054303.gif\"/></alternatives></inline-formula> <inline-formula><tex-math notation=\"LaTeX\">$(k=2,3,4, \\dots)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mo>\u22ef</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"bargal-ieq2-3054303.gif\"/></alternatives></inline-formula> accuracy is usually significantly higher than the top-1 accuracy. This is more evident in fine-grained datasets, where differences between classes are quite subtle. Exploiting the information provided in the top <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"bargal-ieq3-3054303.gif\"/></alternatives></inline-formula> predicted classes boosts the final prediction of a model. We propose Guided Zoom, a novel way in which explainability could be used to improve model performance. We do so by making sure the model has \u201cthe right reasons\u201d for a prediction. The reason/evidence upon which a deep neural network makes a prediction is defined to be the grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom examines how reasonable the evidence used to make each of the top-<inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"bargal-ieq4-3054303.gif\"/></alternatives></inline-formula> predictions is. Test time evidence is deemed reasonable if it is coherent with evidence used to make similar correct decisions at training time. This leads to better informed predictions. We explore a variety of grounding techniques and study their complementarity for computing evidence. We show that Guided Zoom results in an improvement of a model's classification accuracy and achieves state-of-the-art classification performance on four fine-grained classification datasets. Our code is available at <uri>https://github.com/andreazuna89/Guided-Zoom</uri>.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "50980023",
                    "name": "Vitali Petsiuk"
                },
                {
                    "authorId": "1519062024",
                    "name": "Jianming Zhang"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        },
        {
            "paperId": "a3553a1654118a8bd7d2df585fe6f7be15dc8957",
            "title": "Compact CNN Structure Learning by Knowledge Distillation",
            "abstract": "The concept of compressing deep Convolutional Neural Networks (CNNs) is essential to use limited computation, power, and memory resources on embedded devices. However, existing methods achieve this objective at the cost of a drop in inference accuracy in computer vision tasks. To address such a drawback, we propose a framework that leverages knowledge distillation along with customizable block-wise optimization to learn a lightweight CNN structure while preserving better control over the compression-performance tradeoff. Considering specific resource constraints, e.g., floating-point operations per inference (FLOPs) or model-parameters, our method results in a state of the art network compression while being capable of achieving better inference accuracy. In a comprehensive evaluation, we demonstrate that our method is effective, robust, and consistent with results over a variety of network architectures and datasets, at negligible training overhead. In particular, for the already compact network MobileNet_v2, our method offers up to 2\u00d7 and 5.2\u00d7 better model compression in terms of FLOPs and model-parameters, respectively, while getting 1.05% better model performance than the baseline network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2133706169",
                    "name": "Waqar Ahmed"
                },
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "1389596256",
                    "name": "Pietro Morerio"
                },
                {
                    "authorId": "66457911",
                    "name": "V. Murino"
                }
            ]
        },
        {
            "paperId": "f66d6defa4b6ba3cfb09b410ec5127e3b65a5f66",
            "title": "Explainable Deep Classification Models for Domain Generalization",
            "abstract": "Conventionally, AI models are thought to trade off explainability for lower accuracy. We develop a training strategy that not only leads to a more explainable AI system for object classification, but as a consequence, suffers no perceptible accuracy degradation. Explanations are defined as regions of visual evidence upon which a deep classification network makes a decision. This is represented in the form of a saliency map conveying how much each pixel contributed to the network\u2019s decision. Our training strategy enforces a periodic saliency-based feedback to encourage the model to focus on the image regions that directly correspond to the ground-truth object. We quantify explainability using an automated metric, and using human judgement. We propose explainability as a means for bridging the visual-semantic gap between different domains where model explanations are used as a means of disentagling domain specific information from otherwise relevant features. We demonstrate that this leads to improved generalization to new domains without hindering performance on the original domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "39268286",
                    "name": "Riccardo Volpi"
                },
                {
                    "authorId": "2128305",
                    "name": "M. Sameki"
                },
                {
                    "authorId": "1519062024",
                    "name": "Jianming Zhang"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                }
            ]
        },
        {
            "paperId": "de791d172b73ac7a4820108c5f265ece286b5bee",
            "title": "Personality Traits Classification Using Deep Visual Activity-Based Nonverbal Features of Key-Dynamic Images",
            "abstract": "This paper addresses nonverbal behavior analysis for the classification of perceived personality traits using novel deep visual activity (VA)-based features extracted only from key-dynamic images. Dynamic images represent short-term VA. Key-dynamic images carry more discriminative information i.e., nonverbal features (NFs) extracted from them contribute to the classification more than NFs extracted from other dynamic images. Dynamic image construction, learning long-term VA with CNN+LSTM, and detecting spatio-temporal saliency are applied to determine key-dynamic images. Once VA-based NFs are extracted, they are encoded using covariance, and resulting representation is used for classification. This method was evaluated on two datasets: small group meetings and vlogs. For the first dataset, proposed method outperforms not only the state-of-the-art VA-based methods but also multi-modal approaches for all personality traits. For extraversion classification, it performs better than i) the most popular key-frames selection algorithm, ii) random and uniform dynamic image selection, and iii) NFs extracted from all dynamic images. Furthermore, the ablation study proves the superiority of proposed method. For the further dataset, it performs as well as the state-of-the-art visual-NFs on average, while showing improved performance for agreeableness classification. Proposed method can be adapted to any application based on nonverbal behavior analysis, thanks to being data-driven.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2457914",
                    "name": "Cigdem Beyan"
                },
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "2143628288",
                    "name": "Muhammad Shahid"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                }
            ]
        },
        {
            "paperId": "ec7e51559fc88c97718bda0a5cc9dda5099c739b",
            "title": "Are CNN Predictions based on Reasonable Evidence?",
            "abstract": "We propose Guided Zoom, an approach that utilizes spatial grounding to make more informed predictions. It does so by making sure the model has \u201cthe right reasons\u201d for a prediction, being defined as reasons that are coherent with those used to make similar correct decisions at training time. The reason/evidence upon which a deep neural network makes a prediction is defined to be the spatial grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom questions how reasonable the evidence used to make a prediction is. We show that Guided Zoom results in the refinement of a model\u2019s classification accuracy on two fine-grained classification datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "50980023",
                    "name": "Vitali Petsiuk"
                },
                {
                    "authorId": "1519062024",
                    "name": "Jianming Zhang"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        },
        {
            "paperId": "2fe3c27c7f0b74cad599985268745215bc9aff34",
            "title": "Audio Tracking in Noisy Environments by Acoustic Map and Spectral Signature",
            "abstract": "A novel method is proposed for generic target tracking by audio measurements from a microphone array. To cope with noisy environments characterized by persistent and high energy interfering sources, a classification map (CM) based on spectral signatures is calculated by means of a machine learning algorithm. Next, the CM is combined with the acoustic map, describing the spatial distribution of sound energy, in order to obtain a cleaned joint map in which contributions from the disturbing sources are removed. A likelihood function is derived from this map and fed to a particle filter yielding the target location estimation on the acoustic image. The method is tested on two real environments, addressing both speaker and vehicle tracking. The comparison with a couple of trackers, relying on the acoustic map only, shows a sharp improvement in performance, paving the way to the application of audio tracking in real challenging environments.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1742319",
                    "name": "M. Crocco"
                },
                {
                    "authorId": "2157339",
                    "name": "Samuele Martelli"
                },
                {
                    "authorId": "1700022",
                    "name": "A. Trucco"
                },
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                }
            ]
        },
        {
            "paperId": "a4d3d7c170422a1308c083da99b7ec727a6ab01c",
            "title": "Guided Zoom: Questioning Network Evidence for Fine-grained Classification",
            "abstract": "We propose Guided Zoom, an approach that utilizes spatial grounding of a model's decision to make more informed predictions. It does so by making sure the model has \"the right reasons\" for a prediction, defined as reasons that are coherent with those used to make similar correct decisions at training time. The reason/evidence upon which a deep convolutional neural network makes a prediction is defined to be the spatial grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom examines how reasonable such evidence is for each of the top-k predicted classes, rather than solely trusting the top-1 prediction. We show that Guided Zoom improves the classification accuracy of a deep convolutional neural network model and obtains state-of-the-art results on three fine-grained classification benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "50980023",
                    "name": "Vitali Petsiuk"
                },
                {
                    "authorId": "1519062024",
                    "name": "Jianming Zhang"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        },
        {
            "paperId": "f03a9f4b52392bdd570238569522002aec3f52e4",
            "title": "Video Gesture Analysis for Autism Spectrum Disorder Detection",
            "abstract": "Autism is a behavioral neurological disorder affecting a significant percentage of worldwide population. It especially starts manifesting at very low ages, but it is difficult to early diagnose it since there is not a specific exam or trial that is able to spot it safely. Its detection is in fact mainly dependent from the medical expertise used to assess the patient behavior during direct interviews. This work aims at providing an automatic objective support to the doctor for the assessment of (early) diagnosis of possible autistic subjects by only using video sequences. The underlying idea and rationale come from the psychological and neuroscience studies claiming that the executions of simple motor acts are different between pathological and healthy subjects, and this can be sufficient to discriminate between them. To this end, we devised an experiment in which we recorded, using a standard video camera, patient and healthy children performing the same simple gesture of grasping a bottle. By only processing the video clips depicting the grasping action using a recurrent deep neural network, we are able to discriminate with a good accuracy between the 2 classes of subjects. The designed deep model is also able to provide a sort of attention map in which the zones in the video of major interest are identified in space and time: this \u201cexplains\u201d in a certain way which areas the model deems more relevant to the classification purpose, which could also be used by the doctor to make the diagnosis. In the end, this work constitutes a first step towards the development of an automatic computational system devoted to the early diagnosis of autistic subjects, providing the medical expert of a supportive objective method, potentially simple to use in clinical and also more open settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "2322579",
                    "name": "Pietro Morerio"
                },
                {
                    "authorId": "37783905",
                    "name": "A. Cavallo"
                },
                {
                    "authorId": "1964028",
                    "name": "C. Ansuini"
                },
                {
                    "authorId": "2281974867",
                    "name": "Jessica Podda"
                },
                {
                    "authorId": "31490651",
                    "name": "F. Battaglia"
                },
                {
                    "authorId": "5534996",
                    "name": "E. Veneselli"
                },
                {
                    "authorId": "1834966",
                    "name": "C. Becchio"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                }
            ]
        },
        {
            "paperId": "6da04b460fe9950c3175bce5186f0069ed1b60ae",
            "title": "What Will I Do Next? The Intention from Motion Experiment",
            "abstract": "In computer vision, video-based approaches have been widely explored for the early classification and the prediction of actions or activities. However, it remains unclear whether this modality (as compared to 3D kinematics) can still be reliable for the prediction of human intentions, defined as the overarching goal embedded in an action sequence. Since the same action can be performed with different intentions, this problem is more challenging but yet affordable as proved by quantitative cognitive studies which exploit the 3D kinematics acquired through motion capture systems.In this paper, we bridge cognitive and computer vision studies, by demonstrating the effectiveness of video-based approaches for the prediction of human intentions. Precisely, we propose Intention from Motion, a new paradigm where, without using any contextual information, we consider instantaneous grasping motor acts involving a bottle in order to forecast why the bottle itself has been reached (to pass it or to place in a box, or to pour or to drink the liquid inside).We process only the grasping onsets casting intention prediction as a classification framework. Leveraging on our multimodal acquisition (3D motion capture data and 2D optical videos), we compare the most commonly used 3D descriptors from cognitive studies with state-of-the-art video-based techniques. Since the two analyses achieve an equivalent performance, we demonstrate that computer vision tools are effective in capturing the kinematics and facing the cognitive problem of human intention prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "3393678",
                    "name": "Jacopo Cavazza"
                },
                {
                    "authorId": "34465973",
                    "name": "A. Koul"
                },
                {
                    "authorId": "37783905",
                    "name": "A. Cavallo"
                },
                {
                    "authorId": "1834966",
                    "name": "C. Becchio"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                }
            ]
        }
    ]
}