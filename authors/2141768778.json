{
    "authorId": "2141768778",
    "papers": [
        {
            "paperId": "7687dcf2495c3d70ff0309592a3697d358f209d4",
            "title": "Ten Years after ImageNet: A 360{\\deg} Perspective on AI",
            "abstract": "It is ten years since neural networks made their spectacular comeback. Prompted by this anniversary, we take a holistic perspective on Artificial Intelligence (AI). Supervised Learning for cognitive tasks is effectively solved - provided we have enough high-quality labeled data. However, deep neural network models are not easily interpretable, and thus the debate between blackbox and whitebox modeling has come to the fore. The rise of attention networks, self-supervised learning, generative modeling, and graph neural networks has widened the application space of AI. Deep Learning has also propelled the return of reinforcement learning as a core building block of autonomous decision making systems. The possible harms made possible by new AI technologies have raised socio-technical issues such as transparency, fairness, and accountability. The dominance of AI by Big-Tech who control talent, computing resources, and most importantly, data may lead to an extreme AI divide. Failure to meet high expectations in high profile, and much heralded flagship projects like self-driving vehicles could trigger another AI winter.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50793091",
                    "name": "S. Chawla"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2141768778",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "152252333",
                    "name": "Wendy Hall"
                },
                {
                    "authorId": "2186981636",
                    "name": "Issa M. Khalil"
                },
                {
                    "authorId": "2187086383",
                    "name": "Xiaosong Ma"
                },
                {
                    "authorId": "1772206",
                    "name": "H. Sencar"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "2059979016",
                    "name": "Michael Wooldridge"
                },
                {
                    "authorId": "2151684774",
                    "name": "Tingyue Yu"
                }
            ]
        },
        {
            "paperId": "9b1ab9d5925dc5f9a611d8ebdd475ce1b0777579",
            "title": "Fake News Data Exploration and Analytics",
            "abstract": "Before the internet, people acquired their news from the radio, television, and newspapers. With the internet, the news moved online, and suddenly, anyone could post information on websites such as Facebook and Twitter. The spread of fake news has also increased with social media. It has become one of the most significant issues of this century. People use the method of fake news to pollute the reputation of a well-reputed organization for their benefit. The most important reason for such a project is to frame a device to examine the language designs that describe fake and right news through machine learning. This paper proposes models of machine learning that can successfully detect fake news. These models identify which news is real or fake and specify the accuracy of said news, even in a complex environment. After data-preprocessing and exploration, we applied three machine learning models; random forest classifier, logistic regression, and term frequency-inverse document frequency (TF-IDF) vectorizer. The accuracy of the TFIDF vectorizer, logistic regression, random forest classifier, and decision tree classifier models was approximately 99.52%, 98.63%, 99.63%, and 99.68%, respectively. Machine learning models can be considered a great choice to find reality-based results and applied to other unstructured data for various sentiment analysis applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1476819061",
                    "name": "Mazhar Javed Awan"
                },
                {
                    "authorId": "2090326025",
                    "name": "Awais Yasin"
                },
                {
                    "authorId": "94514899",
                    "name": "Haitham Nobanee"
                },
                {
                    "authorId": "2141768778",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "2137468685",
                    "name": "Zain Shahzad"
                },
                {
                    "authorId": "2138126208",
                    "name": "Muhammad Nabeel"
                },
                {
                    "authorId": "1768864",
                    "name": "A. Zain"
                },
                {
                    "authorId": "2314471987",
                    "name": "Hafiz Muhammad Shahzad"
                }
            ]
        },
        {
            "paperId": "e7a00d7bdc5f9e2d4aaa17a3d44ee1239f33fc30",
            "title": "What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context",
            "abstract": "Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically. Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content. This makes it possible to detect likely \u201cfake news\u201d the moment they are published, by simply checking the reliability of their source. From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium\u2019s audience on social media). We further study (iii) what was written about the target medium (in Wikipedia). The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3490018",
                    "name": "R. Baly"
                },
                {
                    "authorId": "48764610",
                    "name": "Georgi Karadzhov"
                },
                {
                    "authorId": "40660541",
                    "name": "Jisun An"
                },
                {
                    "authorId": "2592694",
                    "name": "Haewoon Kwak"
                },
                {
                    "authorId": "1379925776",
                    "name": "Yoan Dinkov"
                },
                {
                    "authorId": "2141768778",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "afa0f48dab3884c0a6e309699bf67efa2f8d5b61",
            "title": "Predicting the Leading Political Ideology of YouTube Channels Using Acoustic, Textual, and Metadata Information",
            "abstract": "We address the problem of predicting the leading political ideology, i.e., left-center-right bias, for YouTube channels of news media. Previous work on the problem has focused exclusively on text and on analysis of the language used, topics discussed, sentiment, and the like. In contrast, here we study videos, which yields an interesting multimodal setup. Starting with gold annotations about the leading political ideology of major world news media from Media Bias/Fact Check, we searched on YouTube to find their corresponding channels, and we downloaded a recent sample of videos from each channel. We crawled more than 1,000 YouTube hours along with the corresponding subtitles and metadata, thus producing a new multimodal dataset. We further developed a multimodal deep-learning architecture for the task. Our analysis shows that the use of acoustic signal helped to improve bias detection by more than 6% absolute over using text and metadata only. We release the dataset to the research community, hoping to help advance the field of multi-modal political bias detection.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1379925776",
                    "name": "Yoan Dinkov"
                },
                {
                    "authorId": "2141768778",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "52553663",
                    "name": "Ivan Koychev"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "d03c6ba564c18a682aedf0a4b05763f5add484dc",
            "title": "Detecting Deception in Political Debates Using Acoustic and Textual Features",
            "abstract": "We present work on deception detection, where, given a spoken claim, we aim to predict its factuality. While previous work in the speech community has relied on recordings from staged setups where people were asked to tell the truth or to lie and their statements were recorded, here we use real-world political debates. Thanks to the efforts of fact-checking organizations, it is possible to obtain annotations for statements in the context of a political discourse as true, half-true, or false. Starting with such data from the CLEF-2018 CheckThat! Lab, which was limited to text, we performed alignment to the corresponding videos, thus producing a multimodal dataset. We further developed a multimodal deep-learning architecture for the task of deception detection, which yielded sizable improvements over the state of the art for the CLEF-2018 Lab task 2. Our experiments show that the use of the acoustic signal consistently helped to improve the performance compared to using textual and metadata features only, based on several different evaluation measures. We release the new dataset to the research community, hoping to help advance the overall field of multimodal deception detection.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "46203745",
                    "name": "D. Kopev"
                },
                {
                    "authorId": "2141768778",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "52553663",
                    "name": "Ivan Koychev"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                }
            ]
        }
    ]
}