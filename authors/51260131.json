{
    "authorId": "51260131",
    "papers": [
        {
            "paperId": "c37e45be7fdd3156fda4db4f3d58eb5aaa27773e",
            "title": "Study of Abuse Detection in Continuous Speech for Indian Languages",
            "abstract": "The presence of abusive content on social media platforms poses a significant challenge in maintaining a positive online environment for millions of users. While automatic abuse detection has seen extensive use in the text domain, audio abuse detection remains relatively unexplored. This paper addresses the modeling challenges inherent in identifying offensive content within real-life audio recordings, particularly in the multilingual context of two Indian languages. We introduce a cascaded model that combines an automatic speech recognition system with textual keyword spotting and compare it with an end-to-end model utilizing audio-level feature embeddings and neural classifiers. Our findings, based on the ADIMA dataset, demonstrate that both methods achieve similar discriminability, but the cascaded linguistic approach stands out for its enhanced explainability and the potential to leverage well-established Natural Language Processing techniques for abuse detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "2292531106",
                    "name": "Debdoot Mukherjee"
                }
            ]
        },
        {
            "paperId": "05c606a78474da20cd92e79dd5395951bb17ad41",
            "title": "ADIMA: Abuse Detection In Multilingual Audio",
            "abstract": "Abusive content detection in spoken text can be addressed by performing Automatic Speech Recognition (ASR) and leveraging advancements in natural language processing. However, ASR models introduce latency and often perform sub-optimally for abusive words as they are underrepresented in training corpora and not spoken clearly or completely. Exploration of this problem entirely in the audio domain has largely been limited by the lack of audio datasets. Building on these challenges, we propose ADIMA, a novel, linguistically diverse, ethically sourced, expert annotated and well- balanced multilingual abuse detection audio dataset comprising of 11,775 audio samples in 10 Indic languages spanning 65 hours and spoken by 6,446 unique users. Through quantitative experiments across monolingual and cross-lingual zeroshot settings, we take the first step in democratizing audio based content moderation in Indic languages and set forth our dataset to pave future work. Dataset and code are available at: https://github.com/ShareChatAI/Adima",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                },
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "1766159",
                    "name": "Debdoot Mukherjee"
                }
            ]
        },
        {
            "paperId": "268cf38ef76e3b90e28599b9cb685b204536714d",
            "title": "Multilingual and Multimodal Abuse Detection",
            "abstract": "The presence of abusive content on social media platforms is undesirable as it severely impedes healthy and safe social media interactions. While automatic abuse detection has been widely explored in textual domain, audio abuse detection still remains unexplored. In this paper, we attempt abuse detection in conversational audio from a multimodal perspective in a multilingual social media setting. Our key hypothesis is that along with the modelling of audio, incorporating discriminative information from other modalities can be highly beneficial for this task. Our proposed method, MADA, explicitly focuses on two modalities other than the audio itself, namely, the underlying emotions expressed in the abusive audio and the semantic information encapsulated in the corresponding textual form. Observations prove that MADA demonstrates gains over audio-only approaches on the ADIMA dataset. We test the proposed approach on 10 different languages and observe consistent gains in the range 0.6%-5.2% by leveraging multiple modalities. We also perform extensive ablation experiments for studying the contributions of every modality and observe the best results while leveraging all the modalities together. Additionally, we perform experiments to empirically confirm that there is a strong correlation between underlying emotions and abusive behaviour.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "151028594",
                    "name": "Heeth Shah"
                },
                {
                    "authorId": "1766159",
                    "name": "Debdoot Mukherjee"
                },
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                }
            ]
        },
        {
            "paperId": "58551876de0bfcb33725f5347a8e4b24985a41bd",
            "title": "The \"Sound of Silence\" in EEG - Cognitive voice activity detection",
            "abstract": "Speech cognition bears potential application as a brain computer interface that can improve the quality of life for the otherwise communication impaired people. While speech and resting state EEG are popularly studied, here we attempt to explore a \"non-speech\"(NS) state of brain activity corresponding to the silence regions of speech audio. Firstly, speech perception is studied to inspect the existence of such a state, followed by its identification in speech imagination. Analogous to how voice activity detection is employed to enhance the performance of speech recognition, the EEG state activity detection protocol implemented here is applied to boost the confidence of imagined speech EEG decoding. Classification of speech and NS state is done using two datasets collected from laboratory-based and commercial-based devices. The state sequential information thus obtained is further utilized to reduce the search space of imagined EEG unit recognition. Temporal signal structures and topographic maps of NS states are visualized across subjects and sessions. The recognition performance and the visual distinction observed demonstrates the existence of silence signatures in EEG.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "1726726",
                    "name": "H. Murthy"
                }
            ]
        },
        {
            "paperId": "746fe962e309960aa5d1cd8864cf37f5aacacbcf",
            "title": "Correlation based Multi-phasal models for improved imagined speech EEG recognition",
            "abstract": "Translation of imagined speech electroencephalogram(EEG) into human understandable commands greatly facilitates the design of naturalistic brain computer interfaces. To achieve improved imagined speech unit classification, this work aims to profit from the parallel information contained in multi-phasal EEG data recorded while speaking, imagining and performing articulatory movements corresponding to specific speech units. A bi-phase common representation learning module using neural networks is designed to model the correlation and reproducibility between an analysis phase and a support phase. The trained Correlation Network is then employed to extract discriminative features of the analysis phase. These features are further classified into five binary phonological categories using machine learning models such as Gaussian mixture based hidden Markov model and deep neural networks. The proposed approach further handles the non-availability of multi-phasal data during decoding. Topographic visualizations along with result-based inferences suggest that the multi-phasal correlation modelling approach proposed in the paper enhances imagined-speech EEG recognition performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "1726726",
                    "name": "H. Murthy"
                }
            ]
        },
        {
            "paperId": "9cd175fd5d415ae65464a758d754ca6bb14c088f",
            "title": "Comparison of Feature-Model Variants for coSpeech-EEG Classification",
            "abstract": "One of the most significant obstacles that must be overcome in pursuing the utilization of brain signals for device control is the formulation of a robust signal processing method that can extract event specific information from real-time EEG signals. Typical Brain Computer Interface systems comprise of signal acquisition, feature extraction and classification modules. The focus in this paper is to experimentally evaluate various feature extraction and classification modules to comparatively determine the best performing feature-model(FM) pair. Few popular FM variants are implemented to classify units from coSpeech-EEG data collected during speech audition, imagination and production. Performance variations across sessions and subjects are also studied to analyse scalability and robustness of the various FM pairs. Simultaneous diagonalization of multiclass common spatial patterns obtained on EEG data coupled with a Gaussian mixture model based Hidden Markov Model proves to be the best FM pair for the task at hand rendering an average accuracy much higher than chance across 30 subjects in a multi-unit classification problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "1726726",
                    "name": "H. Murthy"
                }
            ]
        },
        {
            "paperId": "b019eb914640c39efbdbf62995b03cef29b288e8",
            "title": "Neural Speech Decoding During Audition, Imagination and Production",
            "abstract": "Interpretation of neural signals to a form that is as intelligible as speech facilitates the development of communication mediums for the otherwise speech/motor-impaired individuals. Speech perception, production, and imagination often constitute phases of human communication. The primary goal of this article is to analyze the similarity between these three phases by studying electroencephalogram(EEG) patterns across these modalities, in order to establish their usefulness for brain computer interfaces. Neural decoding of speech using such non-invasive techniques necessitates the optimal choice of signal analysis and translation protocols. By employing selection-by-exclusion based temporal modeling algorithms, we discover fundamental syllable-like units that reveal similar set of signal signatures across all the three phases. Significantly higher than chance accuracies are recorded for single trial multi-unit EEG classification using machine learning approaches over three datasets across 30 subjects. Repeatability and subject independence tests performed at every step of the analysis further strengthens the findings and holds promise for translating brain signals to speech non-invasively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "145254843",
                    "name": "Shrikanth S. Narayanan"
                },
                {
                    "authorId": "2227142",
                    "name": "M. Sur"
                },
                {
                    "authorId": "2064019323",
                    "name": "A. H. Murthy"
                }
            ]
        },
        {
            "paperId": "b90516f9b6e4ea3dfb4e6f83591ab4cc7b6452e9",
            "title": "P300 based Stereo localization of single frequency audio stimulus",
            "abstract": "P300 is widely used for developing Brain-Computer Interfaces (BCIs) and also in clinical applications for research and diagnosis. In this study, a novel way of performing oddball paradigm by stereo-localization of single frequency audio stimulus is proposed. In the proposed stereo oddball technique, a single frequency audio stimulus is presented to the subject in alternating ears with one ear being the target and the other non-target. Non-target is presented more often than target. The experiments are conducted for two configurations, left (target) - right (non-target) and right (target) - left (non-target). Noninvasive Electroencephalogram (EEG) signals are collected for the above mentioned protocol and the P300 component is detected using event-related potentials (ERPs) and analyzed. The proposed Stereo oddball technique is also compared with classical (target and non-target are beeps of different frequency) oddball technique, where the stimulus is presented simultaneously to both ears. The P300 responses are also analyzed using both temporal regions individually. Despite differing inputs(single frequency and dual frequency), similar P300 responses are observed for stereo localized and binaural stimuli presentations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51264554",
                    "name": "Sidharth Aggarwal"
                },
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "1726726",
                    "name": "H. Murthy"
                }
            ]
        },
        {
            "paperId": "46ef9accea19a8d2f2c5d525f4c48e413f11c026",
            "title": "An Empirical Study of Speech Processing in the Brain by Analyzing the Temporal Syllable Structure in Speech-input Induced EEG",
            "abstract": "Clinical applicability of electroencephalography (EEG) is well established, however the use of EEG as a choice for constructing brain computer interfaces to develop communication platforms is relatively recent. To provide more natural means of communication, there is an increasing focus on bringing together speech and EEG signal processing. Quantifying the way our brain processes speech is one way of approaching the problem of speech recognition using brain waves. This paper analyses the feasibility of recognizing syllable level units by studying the temporal structure of speech reflected in the EEG signals. The slowly varying component of the delta band EEG(0.3-3Hz) is present in all other EEG frequency bands. Analysis shows that removing the delta trend in EEG signals results in signals that reveals syllable like structure. Using a 25 syllable framework, classification of EEG data obtained from 13 subjects yields promising results, underscoring the potential of revealing speech related temporal structure in EEG.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "145254843",
                    "name": "Shrikanth S. Narayanan"
                },
                {
                    "authorId": "2227142",
                    "name": "M. Sur"
                },
                {
                    "authorId": "1726726",
                    "name": "H. Murthy"
                }
            ]
        },
        {
            "paperId": "8a95dc1514b76f07c7c9bbedcbcbf6ce9859d717",
            "title": "Level-wise Subject adaptation to improve classification of motor and mental EEG tasks",
            "abstract": "Classification of various cognitive and motor tasks using electroencephalogram (EEG) signals is necessary for building Brain Computer Interfaces (BCI) that are noninvasive. However, achieving high classification accuracy in a multi-subject multitask scenario is a challenge. A noticeable reduction in accuracy is observed when the subjects between train and test are mismatched. Drawing a similarity from speaker adaptation approaches in speech, we propose a method to perform subject-wise adaptation of EEG in order to improve the task classification performance. A Common Spatial Pattern (CSP) approach is employed for feature extraction. Gaussian Mixture Model (GMM) based subject-specific models are built for each of the tasks. Maximum a-posterior (MAP) adaptation is performed, and an absolute improvement of 1.22-7.26% is observed in the average accuracy.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "51264554",
                    "name": "Sidharth Aggarwal"
                },
                {
                    "authorId": "1560964983",
                    "name": "Purvi Goel"
                },
                {
                    "authorId": "51890352",
                    "name": "Raviraj Joshi"
                },
                {
                    "authorId": "2227142",
                    "name": "M. Sur"
                },
                {
                    "authorId": "1726726",
                    "name": "H. Murthy"
                },
                {
                    "authorId": "1726355",
                    "name": "Sriram Ganapathy"
                }
            ]
        }
    ]
}