{
    "authorId": "67284811",
    "papers": [
        {
            "paperId": "0391aee67b008aee6aa12aeea67a7f90c954c113",
            "title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety",
            "abstract": "Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Additionally, we propose a bi-directional anchoring method that, during the selection process, prioritizes data points that are close to harmful examples and far from benign ones. Our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints surprisingly leads to the fine-tuned model affirmatively responding to>70% of tested harmful requests, compared to<20% after fine-tuning on randomly selected data. We also observe that the selected data frequently appear as lists, bullet points, or math questions, indicating a systematic pattern in fine-tuning data that contributes to jailbreaking.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2294507804",
                    "name": "Luxi He"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2254262712",
                    "name": "Peter Henderson"
                }
            ]
        },
        {
            "paperId": "4b879f069d023e03bf537309a99bdaeb39916ea5",
            "title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training",
            "abstract": "Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks. In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training. Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters. Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision. Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49164966",
                    "name": "Zexuan Zhong"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2261973116",
                    "name": "Mike Lewis"
                }
            ]
        },
        {
            "paperId": "57a8333365bf99b65591e6b2176eacf8fd85d5da",
            "title": "Language Models as Science Tutors",
            "abstract": "NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized in math have a 32K-token context window, and they excel at TutorEval while performing strongly on GSM8K and MATH. Our datasets build on open-source materials, and we release our models, data, and evaluations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284703052",
                    "name": "Alexis Chevalier"
                },
                {
                    "authorId": "2308103990",
                    "name": "Jiayi Geng"
                },
                {
                    "authorId": "2127066887",
                    "name": "Alexander Wettig"
                },
                {
                    "authorId": "2284724648",
                    "name": "Howard Chen"
                },
                {
                    "authorId": "2284686496",
                    "name": "Sebastian Mizera"
                },
                {
                    "authorId": "2284685543",
                    "name": "Toni Annala"
                },
                {
                    "authorId": "2284685834",
                    "name": "Max Jameson Aragon"
                },
                {
                    "authorId": "2284682678",
                    "name": "Arturo Rodr'iguez Fanlo"
                },
                {
                    "authorId": "2127069744",
                    "name": "Simon Frieder"
                },
                {
                    "authorId": "2284683132",
                    "name": "Simon Machado"
                },
                {
                    "authorId": "2309244668",
                    "name": "Akshara Prabhakar"
                },
                {
                    "authorId": "103432855",
                    "name": "Ellie Thieu"
                },
                {
                    "authorId": "2284732304",
                    "name": "Jiachen T. Wang"
                },
                {
                    "authorId": "2260308709",
                    "name": "Zirui Wang"
                },
                {
                    "authorId": "2284683822",
                    "name": "Xindi Wu"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2284681788",
                    "name": "Wenhan Jia"
                },
                {
                    "authorId": "2257230025",
                    "name": "Jiatong Yu"
                },
                {
                    "authorId": "2284821642",
                    "name": "Jun-Jie Zhu"
                },
                {
                    "authorId": "2153387689",
                    "name": "Z. Ren"
                },
                {
                    "authorId": "2283134097",
                    "name": "Sanjeev Arora"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ]
        },
        {
            "paperId": "95898b1f82cf7ad7d96fcc85b4def7f086325af5",
            "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
            "abstract": "Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "49288855",
                    "name": "Sadhika Malladi"
                },
                {
                    "authorId": "40895369",
                    "name": "Suchin Gururangan"
                },
                {
                    "authorId": "2283134097",
                    "name": "Sanjeev Arora"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ]
        },
        {
            "paperId": "aa6a03f3368cbb4a413f7e11650fb8a6a2b71de1",
            "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
            "abstract": "Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\\%$ at the parameter level and $2.5\\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283309803",
                    "name": "Boyi Wei"
                },
                {
                    "authorId": "2242535459",
                    "name": "Kaixuan Huang"
                },
                {
                    "authorId": "2283305597",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "2144071564",
                    "name": "Tinghao Xie"
                },
                {
                    "authorId": "2111683101",
                    "name": "Xiangyu Qi"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2254282852",
                    "name": "Prateek Mittal"
                },
                {
                    "authorId": "2257417692",
                    "name": "Mengdi Wang"
                },
                {
                    "authorId": "2254262712",
                    "name": "Peter Henderson"
                }
            ]
        },
        {
            "paperId": "bd3b3e8197d56aa8c20d18e61fae48753d618c5e",
            "title": "CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs",
            "abstract": "Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an over-optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to 34.5%. In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress. Project page and leaderboard: https://charxiv.github.io/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260308709",
                    "name": "Zirui Wang"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2294507804",
                    "name": "Luxi He"
                },
                {
                    "authorId": "2284724648",
                    "name": "Howard Chen"
                },
                {
                    "authorId": "2243302973",
                    "name": "Yitao Liu"
                },
                {
                    "authorId": "2288884828",
                    "name": "Richard Zhu"
                },
                {
                    "authorId": "2087743517",
                    "name": "Kaiqu Liang"
                },
                {
                    "authorId": "2284683822",
                    "name": "Xindi Wu"
                },
                {
                    "authorId": "2308072184",
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "49288855",
                    "name": "Sadhika Malladi"
                },
                {
                    "authorId": "2284703052",
                    "name": "Alexis Chevalier"
                },
                {
                    "authorId": "2283134097",
                    "name": "Sanjeev Arora"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ]
        },
        {
            "paperId": "c3f1fae241a3c2449e675ab750873d800f95513c",
            "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
            "abstract": "Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further enhancing the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models like Mistral and Llama3. We evaluated on extensive instruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the recent challenging Arena-Hard benchmark. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 53.7 length-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the leaderboard, and a 36.5 win rate on Arena-Hard -- making it the strongest 8B open-source model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ]
        },
        {
            "paperId": "d79927715a88825737d2ff44f70b1d6698e0bae9",
            "title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval",
            "abstract": "Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398 real-world queries collected from diverse domains (such as economics, psychology, robotics, software engineering, earth sciences, etc.), sourced from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0 nDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate that augmenting queries with Chain-of-Thought reasoning generated by large language models (LLMs) improves performance by up to 12.2 points. Moreover, BRIGHT is robust against data leakage during pretraining of the benchmarked models as we validate by showing similar performance even when documents from the benchmark are included in the training data. We believe that BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings. Our code and data are available at https://brightbenchmark.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152173042",
                    "name": "Hongjin Su"
                },
                {
                    "authorId": "2287806228",
                    "name": "Howard Yen"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2257597409",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2037383772",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "2312196704",
                    "name": "Han-yu Wang"
                },
                {
                    "authorId": "2312093005",
                    "name": "Haisu Liu"
                },
                {
                    "authorId": "2312003525",
                    "name": "Quan Shi"
                },
                {
                    "authorId": "2274102473",
                    "name": "Zachary S. Siegel"
                },
                {
                    "authorId": "2312099488",
                    "name": "Michael Tang"
                },
                {
                    "authorId": "2313049168",
                    "name": "Ruoxi Sun"
                },
                {
                    "authorId": "2256335437",
                    "name": "Jinsung Yoon"
                },
                {
                    "authorId": "2676352",
                    "name": "Sercan \u00d6. Arik"
                },
                {
                    "authorId": "2311929494",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2312346274",
                    "name": "Tao Yu"
                }
            ]
        },
        {
            "paperId": "fc080333b3a7e39d20dd362f2f9805014d531aaa",
            "title": "LitSearch: A Retrieval Benchmark for Scientific Literature Search",
            "abstract": "Literature search questions, such as\"where can I find research on the evaluation of consistency in generated summaries?\"pose significant challenges for modern search engines and retrieval systems. These questions often require a deep understanding of research concepts and the ability to reason over entire articles. In this work, we introduce LitSearch, a retrieval benchmark comprising 597 realistic literature search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions generated by GPT-4 based on paragraphs containing inline citations from research papers and (2) questions about recently published papers, manually written by their authors. All LitSearch questions were manually examined or edited by experts to ensure high quality. We extensively benchmark state-of-the-art retrieval models and also evaluate two LLM-based reranking pipelines. We find a significant performance gap between BM25 and state-of-the-art dense retrievers, with a 24.8% difference in absolute recall@5. The LLM-based reranking strategies further improve the best-performing dense retriever by 4.4%. Additionally, commercial search engines and research tools like Google Search perform poorly on LitSearch, lagging behind the best dense retriever by 32 points. Taken together, these results show that LitSearch is an informative new testbed for retrieval systems while catering to a real-world use case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218438150",
                    "name": "Anirudh Ajith"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2284703052",
                    "name": "Alexis Chevalier"
                },
                {
                    "authorId": "2257034822",
                    "name": "Tanya Goyal"
                },
                {
                    "authorId": "2311929494",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "4800645",
                    "name": "Tianyu Gao"
                }
            ]
        },
        {
            "paperId": "3422d5e0cdfdc935d6a84a1e3d3f96659265fe3a",
            "title": "Detecting Pretraining Data from Large Language Models",
            "abstract": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2218438150",
                    "name": "Anirudh Ajith"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "108053318",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "2261780806",
                    "name": "Daogao Liu"
                },
                {
                    "authorId": "3443287",
                    "name": "Terra Blevins"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                }
            ]
        }
    ]
}