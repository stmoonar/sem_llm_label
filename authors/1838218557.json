{
    "authorId": "1838218557",
    "papers": [
        {
            "paperId": "e6f8d9ca8f99b7ecdc295cd05dee4c930aa4d886",
            "title": "Data-Efficient Methods For Improving Hate Speech Detection",
            "abstract": "Scarcity of large-scale datasets, especially for resource-impoverished languages motivates exploration of data-efficient methods for hate speech detection. Hateful intents are expressed explicitly (use of cuss, swear, abusive words) and implicitly (indirect and contextual). In this work, we progress implicit and explicit hate speech detection using an input-level data augmentation technique, task reformulation using entailment and cross-learning across five languages. Our proposed data augmentation technique EasyMix, improves the performance across all english datasets by ~1{% and across multilingual datasets by ~1-9{%. We also observe substantial gains of ~2-8% by reformulating hate speech detection as entail problem. We further probe the contextual models and observe that higher layers encode implicit hate while lower layers focus on explicit hate, highlighting the importance of token-level understanding for explicit and context-level for implicit hate speech detection. Code and Dataset splits - https://anonymous.4open.science/r/data_efficient_hatedetect/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151388180",
                    "name": "Sumegh Roychowdhury"
                },
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                }
            ]
        },
        {
            "paperId": "04134747badf3b6991488b45de4a636d4286af0f",
            "title": "3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos",
            "abstract": "We present 3MASSIV, a multilingual, multimodal and multi-aspect, expertly-annotated dataset of diverse short videos extracted from short-video social media platform - Moj. 3MASSIV comprises of 50k short videos (20 seconds average duration) and 100K unlabeled videos in 11 different languages and captures popular short video trends like pranks, fails, romance, comedy expressed via unique audio-visual formats like self-shot videos, reaction videos, lip-synching, self-sung songs, etc. 3MASSIV presents an opportunity for multimodal and multilingual semantic understanding on these unique videos by annotating them for concepts, affective states, media types, and audio language. We present a thorough analysis of 3MASSIV and highlight the variety and unique aspects of our dataset compared to other contemporary popular datasets with strong baselines. We also show how the social media content in 3MASSIV is dynamic and temporal in nature, which can be used for semantic understanding tasks and cross-lingual analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                },
                {
                    "authorId": "9214782",
                    "name": "Trisha Mittal"
                },
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "2057516053",
                    "name": "Vaibhav Mishra"
                },
                {
                    "authorId": "2065570165",
                    "name": "Mayank Maheshwari"
                },
                {
                    "authorId": "2718563",
                    "name": "Aniket Bera"
                },
                {
                    "authorId": "1766159",
                    "name": "Debdoot Mukherjee"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "05c606a78474da20cd92e79dd5395951bb17ad41",
            "title": "ADIMA: Abuse Detection In Multilingual Audio",
            "abstract": "Abusive content detection in spoken text can be addressed by performing Automatic Speech Recognition (ASR) and leveraging advancements in natural language processing. However, ASR models introduce latency and often perform sub-optimally for abusive words as they are underrepresented in training corpora and not spoken clearly or completely. Exploration of this problem entirely in the audio domain has largely been limited by the lack of audio datasets. Building on these challenges, we propose ADIMA, a novel, linguistically diverse, ethically sourced, expert annotated and well- balanced multilingual abuse detection audio dataset comprising of 11,775 audio samples in 10 Indic languages spanning 65 hours and spoken by 6,446 unique users. Through quantitative experiments across monolingual and cross-lingual zeroshot settings, we take the first step in democratizing audio based content moderation in Indic languages and set forth our dataset to pave future work. Dataset and code are available at: https://github.com/ShareChatAI/Adima",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                },
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "1766159",
                    "name": "Debdoot Mukherjee"
                }
            ]
        },
        {
            "paperId": "1ec6b36387281074886930ab878bc03a0dd4a35a",
            "title": "MAViC: Multimodal Active Learning for Video Captioning",
            "abstract": "A large number of annotated video-caption pairs are required for training video captioning models, resulting in high annotation costs. Active learning can be instrumental in reducing these annotation requirements. However, active learning for video captioning is challenging because multiple semantically similar captions are valid for a video, resulting in high entropy outputs even for less-informative samples. Moreover, video captioning algorithms are multimodal in nature with a visual encoder and language decoder. Further, the sequential and combinatorial nature of the output makes the problem even more challenging. In this paper, we introduce MAViC which leverages our proposed Multimodal Semantics Aware Sequential Entropy (M-SASE) based acquisition function to address the challenges of active learning approaches for video captioning. Our approach integrates semantic similarity and uncertainty of both visual and language dimensions in the acquisition function. Our detailed experiments empirically demonstrate the efficacy of M-SASE for active learning for video captioning and improve on the baselines by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2124854855",
                    "name": "Gyanendra Das"
                },
                {
                    "authorId": "40661962",
                    "name": "Xavier Thomas"
                },
                {
                    "authorId": "1709198",
                    "name": "Anant Raj"
                },
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                }
            ]
        },
        {
            "paperId": "268cf38ef76e3b90e28599b9cb685b204536714d",
            "title": "Multilingual and Multimodal Abuse Detection",
            "abstract": "The presence of abusive content on social media platforms is undesirable as it severely impedes healthy and safe social media interactions. While automatic abuse detection has been widely explored in textual domain, audio abuse detection still remains unexplored. In this paper, we attempt abuse detection in conversational audio from a multimodal perspective in a multilingual social media setting. Our key hypothesis is that along with the modelling of audio, incorporating discriminative information from other modalities can be highly beneficial for this task. Our proposed method, MADA, explicitly focuses on two modalities other than the audio itself, namely, the underlying emotions expressed in the abusive audio and the semantic information encapsulated in the corresponding textual form. Observations prove that MADA demonstrates gains over audio-only approaches on the ADIMA dataset. We test the proposed approach on 10 different languages and observe consistent gains in the range 0.6%-5.2% by leveraging multiple modalities. We also perform extensive ablation experiments for studying the contributions of every modality and observe the best results while leveraging all the modalities together. Additionally, we perform experiments to empirically confirm that there is a strong correlation between underlying emotions and abusive behaviour.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51260131",
                    "name": "Rini A. Sharon"
                },
                {
                    "authorId": "151028594",
                    "name": "Heeth Shah"
                },
                {
                    "authorId": "1766159",
                    "name": "Debdoot Mukherjee"
                },
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                }
            ]
        },
        {
            "paperId": "5bafa5c6796fb5c327d9d844b389a17f55ea21a2",
            "title": "Estimating Emotion Contagion on Social Media via Localized Diffusion in Dynamic Graphs",
            "abstract": "We present a computational approach for estimating emotion contagion on social media networks. Built on a foundation of psychology literature, our approach estimates the degree to which the perceivers' emotional states (positive or negative) start to match those of the expressors, based on the latter's content. We use a combination of deep learning and social network analysis to model emotion contagion as a diffusion process in dynamic social network graphs, taking into consideration key aspects like causality, homophily, and interference. We evaluate our approach on user behavior data obtained from a popular social media platform for sharing short videos. We analyze the behavior of 48 users over a span of 8 weeks (over 200k audio-visual short posts analyzed) and estimate how contagious the users with whom they engage with are on social media. As per the theory of diffusion, we account for the videos a user watches during this time (inflow) and the daily engagements; liking, sharing, downloading or creating new videos (outflow) to estimate contagion. To validate our approach and analysis, we obtain human feedback on these 48 social media platform users with an online study by collecting responses of about 150 participants. We report users who interact with more number of creators on the platform are 12% less prone to contagion, and those who consume more content of `negative' sentiment are 23% more prone to contagion. We will publicly release our code upon acceptance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9214782",
                    "name": "Trisha Mittal"
                },
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "30144577",
                    "name": "Rohan Chandra"
                },
                {
                    "authorId": "2176641447",
                    "name": "Apurva Bhatt"
                },
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                },
                {
                    "authorId": "1766159",
                    "name": "Debdoot Mukherjee"
                },
                {
                    "authorId": "2718563",
                    "name": "Aniket Bera"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "8b35f691ca2204a7ec79b901e0fafda7069bcf18",
            "title": "Multilingual Abusive Comment Detection at Scale for Indic Languages",
            "abstract": "Social media platforms were conceived to act as online \u2018town squares\u2019 where people could get together, share information and communicate with each other peacefully. However, harmful content borne out of bad actors are constantly plaguing these platforms slowly converting them into \u2018mosh pits\u2019 where the bad actors take the liberty to extensively abuse various marginalised groups. Accurate and timely detection of abusive content on social media platforms is therefore very important for facilitating safe interactions between users. However, due to the small scale and sparse linguistic coverage of Indic abusive speech datasets, development of such algorithms for Indic social media users (one-sixth of global population) is severely impeded. To facilitate and encourage research in this important direction, we contribute for the first time MACD - a large-scale (150K), human-annotated, multilingual (5 languages), balanced (49% abusive content) and diverse (70K users) abuse detection dataset of user comments, sourced from a popular social media platform - ShareChat 2 . We also release AbuseXLMR , an abusive content detection model pretrained on large number of social media comments in 15+ Indic languages which outperforms XLM-R and MuRIL on multiple Indic datasets. Along with the annotations, we also release the mapping between comment, post and user id\u2019s to facilitate modelling the relationship between them. We share competitive monolingual, cross-lingual and few-shot baselines so that MACD can be used as",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                },
                {
                    "authorId": "151388180",
                    "name": "Sumegh Roychowdhury"
                },
                {
                    "authorId": "2000633372",
                    "name": "Mithun Das"
                },
                {
                    "authorId": "2246317582",
                    "name": "Somnath Banerjee"
                },
                {
                    "authorId": "2042289364",
                    "name": "Punyajoy Saha"
                },
                {
                    "authorId": "2041989412",
                    "name": "Binny Mathew"
                },
                {
                    "authorId": "1857222",
                    "name": "Hastagiri P. Vanchinathan"
                },
                {
                    "authorId": "46405816",
                    "name": "Animesh Mukherjee"
                }
            ]
        },
        {
            "paperId": "9aea00b076302553c5832afe14422fd7aadc94b1",
            "title": "A Simple Unsupervised Approach for Coreference Resolution using Rule-based Weak Supervision",
            "abstract": "Labeled data for the task of Coreference Resolution is a scarce resource, requiring significant human effort. While state-of-the-art coreference models rely on such data, we propose an approach that leverages an end-to-end neural model in settings where labeled data is unavailable. Specifically, using weak supervision, we transfer the linguistic knowledge encoded by Stanford?s rule-based coreference system to the end-to-end model, which jointly learns rich, contextualized span representations and coreference chains. Our experiments on the English OntoNotes corpus demonstrate that our approach effectively benefits from the noisy coreference supervision, producing an improvement over Stanford?s rule-based system (+3.7 F1) and outperforming the previous best unsupervised model (+0.9 F1). Additionally, we validate the efficacy of our method on two other datasets: PreCo and Litbank (+2.5 and +5 F1 on Stanford\u2019s system, respectively).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2175480389",
                    "name": "Alessandro Stolfo"
                },
                {
                    "authorId": "1813134",
                    "name": "Christy Tanner"
                },
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                },
                {
                    "authorId": "2790926",
                    "name": "Mrinmaya Sachan"
                }
            ]
        },
        {
            "paperId": "7b90fd1b397e0bbc4dc168673806c043ff690287",
            "title": "MeGA-CDA: Memory Guided Attention for Category-Aware Unsupervised Domain Adaptive Object Detection",
            "abstract": "Existing approaches for unsupervised domain adaptive object detection perform feature alignment via adversarial training. While these methods achieve reasonable improvements in performance, they typically perform category-agnostic domain alignment, thereby resulting in negative transfer of features. To overcome this issue, in this work, we attempt to incorporate category information into the domain adaptation process by proposing Memory Guided Attention for Category-Aware Domain Adaptation (MeGA-CDA). The proposed method consists of employing category-wise discriminators to ensure category-aware feature alignment for learning domain-invariant discriminative features. However, since the category information is not available for the target samples, we propose to generate memory-guided category-specific attention maps which are then used to route the features appropriately to the corresponding category discriminator. The proposed method is evaluated on several benchmark datasets and is shown to outperform existing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51005492",
                    "name": "VS Vibashan"
                },
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                },
                {
                    "authorId": "144477698",
                    "name": "Poojan Oza"
                },
                {
                    "authorId": "2577847",
                    "name": "Vishwanath A. Sindagi"
                },
                {
                    "authorId": "1741177",
                    "name": "Vishal M. Patel"
                }
            ]
        },
        {
            "paperId": "a706d28b7f032a120d34d334dade674906630b63",
            "title": "Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training",
            "abstract": "Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel emotion recognition has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leveraging unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on SemEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). We also improve the existing state-of-the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for understanding the impact of different layers of the contextual models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1838218557",
                    "name": "Vikram Gupta"
                }
            ]
        }
    ]
}