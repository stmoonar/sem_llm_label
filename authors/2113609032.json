{
    "authorId": "2113609032",
    "papers": [
        {
            "paperId": "1d2af52456308588c84a4e01e57e936442aa9fb3",
            "title": "Compositional Zero-Shot Artistic Font Synthesis",
            "abstract": "Recently, many researchers have made remarkable achievements in the field of artistic font synthesis, with impressive glyph style and effect style in the results. However, due to less exploration in style disentanglement, it is difficult for existing methods to envision a kind of unseen style (glyph-effect) compositions of artistic font, and thus can only learn the seen style compositions. To solve this problem, we propose a novel compositional zero-shot artistic font synthesis gan (CAFS-GAN), which allows the synthesis of unseen style compositions by exploring the visual independence and joint compatibility of encoding semantics between glyph and effect. Specifically, we propose two contrast-based style encoders to achieve style disentanglement due to glyph and effect intertwining in the image. Meanwhile, to preserve more glyph and effect detail, we propose a generator based on hierarchical dual styles AdaIN to reorganize content-styles representations from structure to texture gradually. Extensive experiments demonstrate the superiority of our model in generating high-quality artistic font images with unseen style compositions against other state-of-the-art methods. The source code and data is available at moonlight03.github.io/CAFS-GAN/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47057383",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "50789878",
                    "name": "L. Wu"
                },
                {
                    "authorId": "115877739",
                    "name": "Changshuo Wang"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "385dd16d1859525e14020987e451acce1e5511d6",
            "title": "Cross-Modal Content Inference and Feature Enrichment for Cold-Start Recommendation",
            "abstract": "Multimedia recommendation aims to fuse the multi-modal information of items for feature enrichment to improve the recommendation performance. However, existing methods typically introduce multi-modal information based on collaborative information to improve the overall recommendation precision, while failing to explore its cold-start recommendation performance. Meanwhile, these above methods are only applicable when such multi-modal data is available. To address this problem, this paper proposes a recommendation framework, named Cross-modal Content Inference and Feature Enrichment Recommendation (CIERec), which exploits the multi-modal information to improve its cold-start recommendation performance. Specifically, CIERec first introduces image annotation as the privileged information to help guide the mapping of unified features from the visual space to the semantic space in the training phase. And then CIERec enriches the content representation with the fusion of collaborative, visual, and cross-modal inferred representations, so as to improve its cold-start recommendation performance. Experimental results on two real-world datasets show that the content representations learned by CIERec are able to achieve superior cold-start recommendation performance over existing visually-aware recommendation algorithms. More importantly, CIERec can consistently achieve significant improvements with different conventional visually-aware backbones, which verifies its universality and effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2143918656",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "4141b35bd1ab3b6f7baad62510a9db2001140b2f",
            "title": "Letter Embedding Guidance Diffusion Model for Scene Text Editing",
            "abstract": "Scene text editing(STE) aims to modify the text in the scene image to the target text while retaining the original style. Existing models are based on GAN, where the source image and the target text are input only once during the generation process, and this approach could not fully obtain the style of the source image and content of the target text. In this paper, we propose an STE method based on the classifier-free guidance diffusion model. To our best knowledge, our model is the first work that developed diffusion models to handle the STE task. Specifically, we divide the STE task into multiple steps and extract style information and text content information in each step. In addition, we introduce the letter embedding method as guidance. We experimentally prove that our method outperforms other STE models in terms of overall realism and maintaining glyphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "115877739",
                    "name": "Changshuo Wang"
                },
                {
                    "authorId": "50789878",
                    "name": "L. Wu"
                },
                {
                    "authorId": "2144230322",
                    "name": "Xu Chen"
                },
                {
                    "authorId": "47057383",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "7217fe2b9659fa01d180a135be88a759cfff5880",
            "title": "Multi-channel Attentive Weighting of Visual Frames for Multimodal Video Classification",
            "abstract": "Multimodal video classification aims to incorporate semantic information to regularize the visual representation learning of videos. Conventional methods typically focus on analyzing all information extracted from different modals rather than key information. However, they usually face the problem of handling the redundant video frames of little categorical information. To address this problem, this paper proposes a novel approach that employs multi-channel weighting of visual frames to mitigate the interference of redundant information. Specifically, the proposed algorithm, termed MCA-WF, includes two main modules, where the multi-channel attentive weighting of video frames (McAW) module performs the multi-granularity and multi-channel frame weighting mechanism based on visual self-attention, contrastive attention and cross-modal attention constraints to filter visual noise and redundant information. The visual frame selection (VFS) module explores the combination of multi-channel attention mechanisms to select the key visual information in the video. Experiments were conducted on MSR-VTT and ActivityNet Captions datasets in terms of performance comparison, ablation study, in-depth analysis, and case studies. The results verified that MCA-WF can notice the key information in the classification and effectively improve the ability of information complementation and integration between modals, which leads to better performance than the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2226670159",
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "48211506",
                    "name": "Jinxing Liu"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "a88d1d22c903a1d57b8f141f44678a47ae584f0f",
            "title": "Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data",
            "abstract": "Federated Learning aims to learn a global model on the server side that generalizes to all clients in a privacy-preserving manner, by leveraging the local models from different clients. Existing solutions focus on either regularizing the objective functions among clients or improving the aggregation mechanism for the improved model generalization capability. However, their performance is typically limited by the dataset biases, such as the heterogeneous data distributions and the missing classes. To address this issue, this paper presents a cross-silo prototypical calibration method (FedCSPC), which takes additional prototype information from the clients to learn a unified feature space on the server side. Specifically, FedCSPC first employs the Data Prototypical Modeling (DPM) module to learn data patterns via clustering to aid calibration. Subsequently, the cross-silo prototypical calibration (CSPC) module develops an augmented contrastive learning method to improve the robustness of the calibration, which can effectively project cross-source features into a consistent space while maintaining clear decision boundaries. Moreover, the CSPC module's ease of implementation and plug-and-play characteristics make it even more remarkable. Experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study, and the results verified that FedCSPC is capable of learning the consistent features across different data sources of the same class under the guidance of calibrated model, which leads to better performance than the state-of-the-art methods. The source codes have been released at https://github.com/qizhuang-qz/FedCSPC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "3353201",
                    "name": "Zitan Chen"
                },
                {
                    "authorId": "2228298634",
                    "name": "Han Hu"
                },
                {
                    "authorId": "2305692093",
                    "name": "Hui Lin"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "b571670c93cd0c784bd826a3ccd10ffb55ed14f0",
            "title": "Cross-Training with Prototypical Distillation for improving the generalization of Federated Learning",
            "abstract": "Cross-training has become a promising strategy to handle data heterogeneity problem in federated learning, which re-train a local model across different clients to improve its generalization capability in a privacy-preserving manner. Its main idea is to make the local models to fit the data of all clients. However, the heterogeneity between data sources may lead the local models to quickly forget the knowledge learned in several rounds of cross-training. To address the problem, this paper presents a novel prototype guided cross training mechanism, termed PGCT, to regularize the change of class-level data representations across clients. It includes two main modules, where the prototype guided representation learning module employs client-aware prototypes of data patterns learned by clustering to guide the learning of consistency representation across feature spaces. This maintains the similar decision boundary across different clients. The prototype-based feature augmentation module uses prototypes as soft attention regularizers to further aggregate rich information to enhance the discrimination of historical features. Experiments were conducted on four datasets in terms of performance comparison, ablation study and case study, and the results verified that PGCT can learn discriminative features with different classes under the guidance of prototypes, which leads to better performance than the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2233769098",
                    "name": "Tianhan Liu"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "3353201",
                    "name": "Zitan Chen"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "ee6afb87ce1102769f1a5771a9e76d796868e67e",
            "title": "Triple Sequence Learning for Cross-domain Recommendation",
            "abstract": "Cross-domain recommendation (CDR) aims at leveraging the correlation of users\u2019 behaviors in both the source and target domains to improve the user preference modeling in the target domain. Conventional CDR methods typically explore the dual-relations between the source and target domains\u2019 behaviors. However, this may ignore the informative mixed behaviors that naturally reflect the user\u2019s global preference. To address this issue, we present a novel framework, termed triple sequence learning for cross-domain recommendation (Tri-CDR), which jointly models the source, target, and mixed behavior sequences to highlight the global and target preference and precisely model the triple correlation in CDR. Specifically, Tri-CDR independently models the hidden representations for the triple behavior sequences and proposes a triple cross-domain attention (TCA) method to emphasize the informative knowledge related to both user\u2019s global and target-domain preference. To comprehensively explore the cross-domain correlations, we design a triple contrastive learning (TCL) strategy that simultaneously considers the coarse-grained similarities and fine-grained distinctions among the triple sequences, ensuring the alignment while preserving information diversity in multi-domain. We conduct extensive experiments and analyses on six cross-domain settings. The significant improvements of Tri-CDR with different sequential encoders verify its effectiveness and universality. The source code is available at https://github.com/hulkima/Tri-CDR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "3360722",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2206486105",
                    "name": "Xin Chen"
                },
                {
                    "authorId": "2115462783",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "4950224",
                    "name": "Leyu Lin"
                },
                {
                    "authorId": "48128428",
                    "name": "Jie Zhou"
                }
            ]
        },
        {
            "paperId": "16c3c20930a28e2e523a8ce9df662ca65daf5212",
            "title": "Disentangled Representations and Hierarchical Refinement of Multi-Granularity Features for Text-to-Image Synthesis",
            "abstract": "In this paper, we focus on generating photo-realistic images from given text descriptions. Current methods first generate an initial image and then progressively refine it to a high-resolution one. These methods typically indiscriminately refine all granularity features output from the previous stage. However, the ability to express different granularity features in each stage is not consistent, and it is difficult to express precise semantics by further refining the features with poor quality generated in the previous stage. Current methods cannot refine different granularity features independently, resulting in that it is challenging to clearly express all factors of semantics in generated image, and some features even become worse. To address this issue, we propose a Hierarchical Disentangled Representations Generative Adversarial Networks (HDR-GAN) to generate photo-realistic images by explicitly disentangling and individually modeling the factors of semantics in the image. HDR-GAN introduces a novel component called multi-granularity feature disentangled encoder to represent image information comprehensively through explicitly disentangling multi-granularity features including pose, shape and texture. Moreover, we develop a novel Multi-granularity Feature Refinement (MFR) containing a Coarse-grained Feature Refinement (CFR) model and a Fine-grained Feature Refinement (FFR) model. CFR utilizes coarse-grained disentangled representations (e.g., pose and shape) to clarify category information, while FFR employs fine-grained disentangled representations (e.g., texture) to reflect instance-level details. Extensive experiments on two well-studied and publicly available datasets (i.e., CUB-200 and CLEVR-SV) demonstrate the rationality and superiority of our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052290287",
                    "name": "Pei Dong"
                },
                {
                    "authorId": "50789878",
                    "name": "L. Wu"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "1759133",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "394c0fe4d4ff4ce3c7e9ca913e7a8abe30b37299",
            "title": "Meta-Causal Feature Learning for Out-of-Distribution Generalization",
            "abstract": "Causal inference has become a powerful tool to handle the out-of-distribution (OOD) generalization problem, which aims to extract the invariant features. However, conventional methods apply causal learners from multiple data splits, which may incur biased representation learning from imbalanced data distributions and difficulty in invariant feature learning from heterogeneous sources. To address these issues, this paper presents a balanced meta-causal learner (BMCL), which includes a balanced task generation module (BTG) and a meta-causal feature learning module (MCFL). Specifically, the BTG module learns to generate balanced subsets by a self-learned partitioning algorithm with constraints on the proportions of sample classes and contexts. The MCFL module trains a meta-learner adapted to different distributions. Experiments conducted on NICO++ dataset verified that BMCL effectively identifies the class-invariant visual regions for classification and may serve as a general framework to improve the performance of the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "1492114275",
                    "name": "Jingyu Li"
                },
                {
                    "authorId": "2173892322",
                    "name": "Xuelong Li"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "43569403fade3cdbed6ed79c85f88f71233e15ae",
            "title": "Unsupervised Contrastive Masking for Visual Haze Classification",
            "abstract": "Haze classification has gained much attention recently as a cost-effective solution for air quality monitoring. Different from conventional image classification tasks, it requires the classifier to capture the haze patterns of different severity degrees. Existing efforts typically focus on the extraction of effective haze features, such as the dark channel and deep features. However, it is observed that the light-haze images are often mis-classified due to the presence of diverse background scenes. To address this issue, this paper presents an unsupervised contrastive masking (UCM) algorithm to segment the haze regions without any supervision, and develops a dual-channel model-agnostic framework, termed magnifier neural network (MagNet), to effectively use the segmented haze regions to enhance the learning of haze features by conventional deep learning models. Specifically, MagNet employs the haze regions to provide the pixel- and feature-level visual information via three strategies, including Input Augmentation, Network Constraint, and Feature Enhancement, which work as a soft-attention regularizer to alleviates the trade-off between capturing the global scene information and the local information in the haze regions. Experiments were conducted on two datasets in terms of performance comparison, parameter estimation, ablation studies, and case studies, and the results verified that UCM can accurately and rapidly segment the haze regions, and the proposed three strategies of MagNet consistently improve the performance of the state-of-the-art deep learning backbones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1492114275",
                    "name": "Jingyu Li"
                },
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2072540723",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        }
    ]
}