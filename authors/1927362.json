{
    "authorId": "1927362",
    "papers": [
        {
            "paperId": "f6a96a07e87179568d6e06230c11ca07d842cfa5",
            "title": "Animated Stickers: Bringing Stickers to Life with Video Diffusion",
            "abstract": "We introduce animated stickers, a video diffusion model which generates an animation conditioned on a text prompt and static sticker image. Our model is built on top of the state-of-the-art Emu text-to-image model, with the addition of temporal layers to model motion. Due to the domain gap, i.e. differences in visual and motion style, a model which performed well on generating natural videos can no longer generate vivid videos when applied to stickers. To bridge this gap, we employ a two-stage finetuning pipeline: first with weakly in-domain data, followed by human-in-the-loop (HITL) strategy which we term ensemble-of-teachers. It distills the best qualities of multiple teachers into a smaller student model. We show that this strategy allows us to specifically target improvements to motion quality while maintaining the style from the static image. With inference optimizations, our model is able to generate an eight-frame video with high-quality, interesting, and relevant motion in under one second.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267338180",
                    "name": "David Yan"
                },
                {
                    "authorId": "2267366054",
                    "name": "Winnie Zhang"
                },
                {
                    "authorId": "2273376477",
                    "name": "Luxin Zhang"
                },
                {
                    "authorId": "2199318406",
                    "name": "Anmol Kalia"
                },
                {
                    "authorId": "2283843884",
                    "name": "Dingkang Wang"
                },
                {
                    "authorId": "1453469113",
                    "name": "Ankit Ramchandani"
                },
                {
                    "authorId": "2283817516",
                    "name": "Miao Liu"
                },
                {
                    "authorId": "2264453654",
                    "name": "Albert Pumarola"
                },
                {
                    "authorId": "2270673864",
                    "name": "Edgar Schoenfeld"
                },
                {
                    "authorId": "2267338968",
                    "name": "Elliot Blanchard"
                },
                {
                    "authorId": "2283762530",
                    "name": "Krishna Narni"
                },
                {
                    "authorId": "2271325625",
                    "name": "Yaqiao Luo"
                },
                {
                    "authorId": "2271703027",
                    "name": "Lawrence Chen"
                },
                {
                    "authorId": "2273066537",
                    "name": "Guan Pang"
                },
                {
                    "authorId": "2276426062",
                    "name": "Ali K. Thabet"
                },
                {
                    "authorId": "2283763097",
                    "name": "Peter Vajda"
                },
                {
                    "authorId": "1927362",
                    "name": "Amy Bearman"
                },
                {
                    "authorId": "2269696579",
                    "name": "Licheng Yu"
                }
            ]
        },
        {
            "paperId": "36d369fd77f01f37347aa13c835c4210c8a6eb67",
            "title": "IMU2CLIP: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning",
            "abstract": "We present IMU2CLIP, a novel pre-training approach to align Inertial Measurement Unit (IMU) motion sensor recordings with text and video, by projecting them into the joint representation space of Contrastive Language-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to translate human motions (as measured by IMU sensors) into their corresponding textual descriptions and videos \u2013 while preserving the transitivity across these modalities. We introduce several new IMU-based Wearable AI applications such as motion-based media search, or an LM-based multi-modal reasoning with motion sensor data \u2013 all using text as the grounding platform. In addition, we show that IMU2CLIP significantly improves downstream performances when fine-tuned for each application, demonstrating its universal usage as a new pre-trained resource. Our code and models will be released publicly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2111680936",
                    "name": "Andrea Madotto"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "51912276",
                    "name": "Aparajita Saraf"
                },
                {
                    "authorId": "1927362",
                    "name": "Amy Bearman"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                }
            ]
        },
        {
            "paperId": "5149436f395caf375c7fd7d3c63f9516e261d02a",
            "title": "Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human Expression",
            "abstract": "We introduce Style Tailoring, a recipe to finetune Latent Diffusion Models (LDMs) in a distinct domain with high visual quality, prompt alignment and scene diversity. We choose sticker image generation as the target domain, as the images significantly differ from photorealistic samples typically generated by large-scale LDMs. We start with a competent text-to-image model, like Emu, and show that relying on prompt engineering with a photorealistic model to generate stickers leads to poor prompt alignment and scene diversity. To overcome these drawbacks, we first finetune Emu on millions of sticker-like images collected using weak supervision to elicit diversity. Next, we curate human-in-the-loop (HITL) Alignment and Style datasets from model generations, and finetune to improve prompt alignment and style alignment respectively. Sequential finetuning on these datasets poses a tradeoff between better style alignment and prompt alignment gains. To address this tradeoff, we propose a novel fine-tuning method called Style Tailoring, which jointly fits the content and style distribution and achieves best tradeoff. Evaluation results show our method improves visual quality by 14%, prompt alignment by 16.2% and scene diversity by 15.3%, compared to prompt engineering the base Emu model for stickers generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147354973",
                    "name": "Animesh Sinha"
                },
                {
                    "authorId": "2267879983",
                    "name": "Bo Sun"
                },
                {
                    "authorId": "2199318406",
                    "name": "Anmol Kalia"
                },
                {
                    "authorId": "2267336768",
                    "name": "Arantxa Casanova"
                },
                {
                    "authorId": "2267338968",
                    "name": "Elliot Blanchard"
                },
                {
                    "authorId": "2267338180",
                    "name": "David Yan"
                },
                {
                    "authorId": "2267366054",
                    "name": "Winnie Zhang"
                },
                {
                    "authorId": "2267338941",
                    "name": "Tony Nelli"
                },
                {
                    "authorId": "2267388165",
                    "name": "Jiahui Chen"
                },
                {
                    "authorId": "2267339004",
                    "name": "Hardik Shah"
                },
                {
                    "authorId": "2269696579",
                    "name": "Licheng Yu"
                },
                {
                    "authorId": "2247874378",
                    "name": "Mitesh Kumar Singh"
                },
                {
                    "authorId": "1453469113",
                    "name": "Ankit Ramchandani"
                },
                {
                    "authorId": "2095979",
                    "name": "Maziar Sanjabi"
                },
                {
                    "authorId": "2267489691",
                    "name": "Sonal Gupta"
                },
                {
                    "authorId": "1927362",
                    "name": "Amy Bearman"
                },
                {
                    "authorId": "2267338678",
                    "name": "Dhruv Mahajan"
                }
            ]
        },
        {
            "paperId": "a5c4e494b0cfa75589b0d462e049d058eb81f754",
            "title": "IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from Egocentric Videos and Text",
            "abstract": "We present IMU2CLIP, a novel pre-training approach to align Inertial Measurement Unit (IMU) motion sensor recordings with video and text, by projecting them into the joint representation space of Contrastive Language-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to translate human motions (as measured by IMU sensors) into their corresponding textual descriptions and videos -- while preserving the transitivity across these modalities. We explore several new IMU-based applications that IMU2CLIP enables, such as motion-based media retrieval and natural language reasoning tasks with motion data. In addition, we show that IMU2CLIP can significantly improve the downstream performance when fine-tuned for each application (e.g. activity recognition), demonstrating the universal usage of IMU2CLIP as a new pre-trained resource. Our code will be made publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2111680936",
                    "name": "Andrea Madotto"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "2674845",
                    "name": "Alireza Dirafzoon"
                },
                {
                    "authorId": "51912276",
                    "name": "Aparajita Saraf"
                },
                {
                    "authorId": "1927362",
                    "name": "Amy Bearman"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                }
            ]
        },
        {
            "paperId": "2d7e4b2c3d69e9d3af12f4e1c90cab1dab4e1776",
            "title": "Connecting What to Say With Where to Look by Modeling Human Attention Traces",
            "abstract": "We introduce a unified framework to jointly model images, text, and human attention traces. Our work is built on top of the recent Localized Narratives annotation frame-work [31], where each word of a given caption is paired with a mouse trace segment. We propose two novel tasks: (1) predict a trace given an image and caption (i.e., visual grounding), and (2) predict a caption and a trace given only an image. Learning the grounding of each word is challenging, due to noise in the human-provided traces and the presence of words that cannot be meaningfully visually grounded. We present a novel model architecture that is jointly trained on dual tasks (controlled trace generation and controlled caption generation). To evaluate the quality of the generated traces, we propose a local bipartite matching (LBM) distance metric which allows the comparison of two traces of different lengths. Extensive experiments show our model is robust to the imperfect training data and outperforms the baselines by a clear margin. More-over, we demonstrate that our model pre-trained on the pro-posed tasks can be also beneficial to the downstream task of COCO\u2019s guided image captioning. Our code1 and project page2 are publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8745352",
                    "name": "Zihang Meng"
                },
                {
                    "authorId": "2112477373",
                    "name": "Licheng Yu"
                },
                {
                    "authorId": "2152329702",
                    "name": "Ning Zhang"
                },
                {
                    "authorId": "1685538",
                    "name": "Tamara L. Berg"
                },
                {
                    "authorId": "2320456981",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "144711711",
                    "name": "Vikas Singh"
                },
                {
                    "authorId": "1927362",
                    "name": "Amy Bearman"
                }
            ]
        }
    ]
}