{
    "authorId": "100832735",
    "papers": [
        {
            "paperId": "05389d52e77a69c37b197a4b77aee65128a9666a",
            "title": "ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning",
            "abstract": "Charts provide visual representations of data and are widely used for analyzing information, addressing queries, and conveying insights to others. Various chart-related downstream tasks have emerged recently, such as question-answering and summarization. A common strategy to solve these tasks is to fine-tune various models originally trained on vision tasks language. However, such task-specific models are not capable of solving a wide range of chart-related tasks, constraining their real-world applicability. To overcome these challenges, we introduce ChartInstruct: a novel chart-specific vision-language Instruction-following dataset comprising 191K instructions generated with 71K charts. We then present two distinct systems for instruction tuning on such datasets: (1) an end-to-end model that connects a vision encoder for chart understanding with a LLM; and (2) a pipeline model that employs a two-step approach to extract chart data tables and input them into the LLM. In experiments on four downstream tasks, we first show the effectiveness of our model--achieving a new set of state-of-the-art results. Further evaluation shows that our instruction-tuning approach supports a wide array of real-world chart comprehension and reasoning scenarios, thereby expanding the scope and applicability of our models to new kinds of tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "2291134806",
                    "name": "Mehrad Shahmohammadi"
                },
                {
                    "authorId": "3405393",
                    "name": "Md. Rizwan Parvez"
                },
                {
                    "authorId": "2274022429",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "334d7fba900eab258cd4fbb5152539e83678b9c4",
            "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild",
            "abstract": "Given the ubiquity of charts as a data analysis, visualization, and decision-making tool across industries and sciences, there has been a growing interest in developing pre-trained foundation models as well as general purpose instruction-tuned models for chart understanding and reasoning. However, existing methods suffer crucial drawbacks across two critical axes affecting the performance of chart representation models: they are trained on data generated from underlying data tables of the charts, ignoring the visual trends and patterns in chart images, and use weakly aligned vision-language backbone models for domain-specific training, limiting their generalizability when encountering charts in the wild. We address these important drawbacks and introduce ChartGemma, a novel chart understanding and reasoning model developed over PaliGemma. Rather than relying on underlying data tables, ChartGemma is trained on instruction-tuning data generated directly from chart images, thus capturing both high-level trends and low-level visual information from a diverse set of charts. Our simple approach achieves state-of-the-art results across $5$ benchmarks spanning chart summarization, question answering, and fact-checking, and our elaborate qualitative studies on real-world charts show that ChartGemma generates more realistic and factually correct summaries compared to its contemporaries. We release the code, model checkpoints, dataset, and demos at https://github.com/vis-nlp/ChartGemma.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2310230414",
                    "name": "Aayush Bajaj"
                },
                {
                    "authorId": "2310228208",
                    "name": "Aaryaman Kartha"
                },
                {
                    "authorId": "2274022429",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "6ee0f922daae002c12b3bc3a9a79807f09a094ee",
            "title": "LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents",
            "abstract": "Document AI is a growing research field that focuses on the comprehension and extraction of information from scanned and digital documents to make everyday business operations more efficient. Numerous downstream tasks and datasets have been introduced to facilitate the training of AI models capable of parsing and extracting information from various document types such as receipts and scanned forms. Despite these advancements, both existing datasets and models fail to address critical challenges that arise in industrial contexts. Existing datasets primarily comprise short documents consisting of a single page, while existing models are constrained by a limited maximum length, often set at 512 tokens. Consequently, the practical application of these methods in financial services, where documents can span multiple pages, is severely impeded. To overcome these challenges, we introduce LongFin, a multimodal document AI model capable of encoding up to 4K tokens. We also propose the LongForms dataset, a comprehensive financial dataset that encapsulates several industrial challenges in financial documents. Through an extensive evaluation, we demonstrate the effectiveness of the LongFin model on the LongForms dataset, surpassing the performance of existing public models while maintaining comparable results on existing single-page benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "2281641358",
                    "name": "Amir Hajian"
                }
            ]
        },
        {
            "paperId": "97ec0d508265f43d3d011a09ffbf599df9ea2b0d",
            "title": "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning? An Extensive Investigation into the Capabilities and Limitations of LVLMs",
            "abstract": "Natural language is a powerful complementary modality of communication for data visualizations, such as bar and line charts. To facilitate chart-based reasoning using natural language, various downstream tasks have been introduced recently such as chart question answering, chart summarization, and fact-checking with charts. These tasks pose a unique challenge, demanding both vision-language reasoning and a nuanced understanding of chart data tables, visual encodings, and natural language prompts. Despite the recent success of Large Language Models (LLMs) across diverse NLP tasks, their abilities and limitations in the realm of data visualization remain under-explored, possibly due to their lack of multi-modal capabilities. To bridge the gap, this paper presents the first comprehensive evaluation of the recently developed large vision language models (LVLMs) for chart understanding and reasoning tasks. Our evaluation includes a comprehensive assessment of LVLMs, including GPT-4V and Gemini, across four major chart reasoning tasks. Furthermore, we perform a qualitative evaluation of LVLMs' performance on a diverse range of charts, aiming to provide a thorough analysis of their strengths and weaknesses. Our findings reveal that LVLMs demonstrate impressive abilities in generating fluent texts covering high-level data insights while also encountering common problems like hallucinations, factual errors, and data bias. We highlight the key strengths and limitations of chart comprehension tasks, offering insights for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2304464834",
                    "name": "Mohammed Saidul Islam"
                },
                {
                    "authorId": "2124881265",
                    "name": "Raian Rahman"
                },
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "46437970",
                    "name": "Md Tahmid Rahman Laskar"
                },
                {
                    "authorId": "1807355",
                    "name": "Mir Tafseer Nayeem"
                },
                {
                    "authorId": "2274022429",
                    "name": "Enamul Hoque"
                }
            ]
        },
        {
            "paperId": "7bef3c961fb1c272b8c3a66d9169b02b64d57766",
            "title": "Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question Answering and Summarization",
            "abstract": "A number of tasks have been proposed recently to facilitate easy access to charts such as chart QA and summarization. The dominant paradigm to solve these tasks has been to fine-tune a pretrained model on the task data. However, this approach is not only expensive but also not generalizable to unseen tasks. On the other hand, large language models (LLMs) have shown impressive generalization capabilities to unseen tasks with zero- or few-shot prompting. However, their application to chart-related tasks is not trivial as these tasks typically involve considering not only the underlying data but also the visual features in the chart image. We propose PromptChart, a multimodal few-shot prompting framework with LLMs for chart-related applications. By analyzing the tasks carefully, we have come up with a set of prompting guidelines for each task to elicit the best few-shot performance from LLMs. We further propose a strategy to inject visual information into the prompts. Our experiments on three different chart-related information consumption tasks show that with properly designed prompts LLMs can excel on the benchmarks, achieving state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060491855",
                    "name": "Do Xuan Long"
                },
                {
                    "authorId": "2265582207",
                    "name": "Mohammad Hassanpour"
                },
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "2275056278",
                    "name": "P. Kavehzadeh"
                },
                {
                    "authorId": "2274022429",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "8ab4863393fceb41d0fa77d632ace4c80a57a154",
            "title": "UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning",
            "abstract": "Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-level tasks to extract the visual elements (e.g., bars, lines) and data from charts, and (ii) high-level tasks to acquire chart understanding and reasoning skills. We find that pretraining the model on a large corpus with chart-specific low- and high-level tasks followed by finetuning on three down-streaming tasks results in state-of-the-art performance on three downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "2275056278",
                    "name": "P. Kavehzadeh"
                },
                {
                    "authorId": "2060491855",
                    "name": "Do Xuan Long"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "8870dd1ee67aa0e52275c3a7b5d8c26da81fb59c",
            "title": "Chart Question Answering: State of the Art and Future Directions",
            "abstract": "Information visualizations such as bar charts and line charts are very common for analyzing data and discovering critical insights. Often people analyze charts to answer questions that they have in mind. Answering such questions can be challenging as they often require a significant amount of perceptual and cognitive effort. Chart Question Answering (CQA) systems typically take a chart and a natural language question as input and automatically generate the answer to facilitate visual data analysis. Over the last few years, there has been a growing body of literature on the task of CQA. In this survey, we systematically review the current state\u2010of\u2010the\u2010art research focusing on the problem of chart question answering. We provide a taxonomy by identifying several important dimensions of the problem domain including possible inputs and outputs of the task and discuss the advantages and limitations of proposed solutions. We then summarize various evaluation techniques used in the surveyed papers. Finally, we outline the open challenges and future research opportunities related to chart question answering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2275056278",
                    "name": "P. Kavehzadeh"
                },
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                }
            ]
        },
        {
            "paperId": "b611c501269224702d1a9942c8600a31ec66ab28",
            "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
            "abstract": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "2060491855",
                    "name": "Do Xuan Long"
                },
                {
                    "authorId": "101161623",
                    "name": "J. Tan"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                }
            ]
        },
        {
            "paperId": "ca2f63950685a97e5ab6b8e6b2db78a8995e94a2",
            "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization",
            "abstract": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158815576",
                    "name": "Shankar Kanthara"
                },
                {
                    "authorId": "2158811816",
                    "name": "Rixie Tiffany Ko Leong"
                },
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                },
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "cd2805a582e4ad0c4e9bc4557da4c3ce0f64d5b0",
            "title": "Chain FL: Decentralized Federated Machine Learning via Blockchain",
            "abstract": "Federated learning is a collaborative machine learning mechanism that allows multiple parties to develop a model without sharing the training data. It is a promising mechanism since it empowers collaboration in fields such as medicine and banking where data sharing is not favorable due to legal, technical, ethical, or safety issues without significantly sacrificing accuracy. In centralized federated learning, there is a single central server, and hence it has a single point of failure. Unlike centralized federated learning, decentralized federated learning does not depend on a single central server for the updates. In this paper, we propose a decentralized federated learning approach named Chain FL that makes use of the blockchain to delegate the responsibility of storing the model to the nodes on the network instead of a centralized server. Chain FL produced promising results on the MNIST digit recognition task with a maximum 0.20% accuracy decrease, and on the CIFAR-10 image classification task with a maximum of 2.57% accuracy decrease as compared to non-FL counterparts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "146385926",
                    "name": "Caner Korkmaz"
                },
                {
                    "authorId": "2034269164",
                    "name": "Halil Eralp Kocas"
                },
                {
                    "authorId": "2062849730",
                    "name": "Ahmet Uysal"
                },
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "9424676",
                    "name": "O. Ozkasap"
                },
                {
                    "authorId": "152451617",
                    "name": "Bar\u0131\u015f Akg\u00fcn"
                }
            ]
        }
    ]
}