{
    "authorId": "3404827",
    "papers": [
        {
            "paperId": "334578ad65ee70f55cbdd4c9b4bd974873889cea",
            "title": "STAB: Speech Tokenizer Assessment Benchmark",
            "abstract": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                },
                {
                    "authorId": "2298362472",
                    "name": "Harman Singh"
                },
                {
                    "authorId": "2136381352",
                    "name": "Shikhar Bharadwaj"
                },
                {
                    "authorId": "1726355",
                    "name": "Sriram Ganapathy"
                },
                {
                    "authorId": "50844587",
                    "name": "Chulayuth Asawaroengchai"
                },
                {
                    "authorId": "3104038",
                    "name": "Kartik Audhkhasi"
                },
                {
                    "authorId": "2279924507",
                    "name": "Andrew Rosenberg"
                },
                {
                    "authorId": "12295226",
                    "name": "Ankur Bapna"
                },
                {
                    "authorId": "1720857",
                    "name": "B. Ramabhadran"
                }
            ]
        },
        {
            "paperId": "cc7310f7cf2729719949a6bc56831fedea8ffa33",
            "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
            "abstract": "Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80494139",
                    "name": "Rachit Bansal"
                },
                {
                    "authorId": "2277741562",
                    "name": "Bidisha Samanta"
                },
                {
                    "authorId": "35186886",
                    "name": "Siddharth Dalmia"
                },
                {
                    "authorId": "2285178",
                    "name": "Nitish Gupta"
                },
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                },
                {
                    "authorId": "1726355",
                    "name": "Sriram Ganapathy"
                },
                {
                    "authorId": "2277740200",
                    "name": "Abhishek Bapna"
                },
                {
                    "authorId": "2277742787",
                    "name": "Prateek Jain"
                },
                {
                    "authorId": "2408872",
                    "name": "P. Talukdar"
                }
            ]
        },
        {
            "paperId": "d64a4a9aa01b1e50b659a077cb9f2c9bde85ad1a",
            "title": "A Morphology-Based Investigation of Positional Encodings",
            "abstract": "Contemporary deep learning models effectively handle languages with diverse morphology despite not being directly integrated into them. Morphology and word order are closely linked, with the latter incorporated into transformer-based models through positional encodings. This prompts a fundamental inquiry: Is there a correlation between the morphological complexity of a language and the utilization of positional encoding in pre-trained language models? In pursuit of an answer, we present the first study addressing this question, encompassing 22 languages and 5 downstream tasks. Our findings reveal that the importance of positional encoding diminishes with increasing morphological complexity in languages. Our study motivates the need for a deeper understanding of positional encoding, augmenting them to better reflect the different languages under consideration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2295672795",
                    "name": "Poulami Ghosh"
                },
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                },
                {
                    "authorId": "3209719",
                    "name": "Raj Dabre"
                },
                {
                    "authorId": "2266470426",
                    "name": "Pushpak Bhattacharyya"
                }
            ]
        },
        {
            "paperId": "e2b3a573496fb4d51c5483269581c0518da7463c",
            "title": "LinguaMeta: Unified Metadata for Thousands of Languages",
            "abstract": "We introduce LinguaMeta, a unified resource for language metadata for thousands of languages, including language codes, names, number of speakers, writing systems, countries, official status, coordinates, and language varieties. The resources are drawn from various existing repositories and supplemented with our own research. Each data point is tagged for its origin, allowing us to easily trace back to and improve existing resources with more up-to-date and complete metadata. The resource is intended for use by researchers and organizations who aim to extend technology to thousands of languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242980871",
                    "name": "Sandy Ritchie"
                },
                {
                    "authorId": "2121364695",
                    "name": "Daan van Esch"
                },
                {
                    "authorId": "2301578963",
                    "name": "Uche Okonkwo"
                },
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                },
                {
                    "authorId": "2301578946",
                    "name": "Emily Drummond"
                }
            ]
        },
        {
            "paperId": "4d628533cbf37d71c658600fe66fe9ed19bcd7fb",
            "title": "MASR: Metadata Aware Speech Representation",
            "abstract": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR , a M etadata A ware S peech R epresentation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identi-\ufb01cation, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate signi\ufb01cant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identi\ufb01cation task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2184503040",
                    "name": "Anjali Raj"
                },
                {
                    "authorId": "2136381352",
                    "name": "Shikhar Bharadwaj"
                },
                {
                    "authorId": "1726355",
                    "name": "Sriram Ganapathy"
                },
                {
                    "authorId": "2244166780",
                    "name": "Min Ma"
                },
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                }
            ]
        },
        {
            "paperId": "581c2809c1099de06ffe335cc8004ea213455e0b",
            "title": "Multimodal Modeling for Spoken Language Identification",
            "abstract": "Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2136381352",
                    "name": "Shikhar Bharadwaj"
                },
                {
                    "authorId": "2244166780",
                    "name": "Min Ma"
                },
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                },
                {
                    "authorId": "12295226",
                    "name": "Ankur Bapna"
                },
                {
                    "authorId": "1726355",
                    "name": "Sriram Ganapathy"
                },
                {
                    "authorId": "82840075",
                    "name": "Vera Axelrod"
                },
                {
                    "authorId": "35186886",
                    "name": "Siddharth Dalmia"
                },
                {
                    "authorId": "72549949",
                    "name": "Wei Han"
                },
                {
                    "authorId": "2243485513",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "8775666",
                    "name": "D. Esch"
                },
                {
                    "authorId": "2242980871",
                    "name": "Sandy Ritchie"
                },
                {
                    "authorId": "2408872",
                    "name": "P. Talukdar"
                },
                {
                    "authorId": "2909504",
                    "name": "Jason Riesa"
                }
            ]
        },
        {
            "paperId": "b186ede7bdf82545240077ba455afbb4dc0edf27",
            "title": "Label Aware Speech Representation Learning For Language Identification",
            "abstract": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                },
                {
                    "authorId": "2136381352",
                    "name": "Shikhar Bharadwaj"
                },
                {
                    "authorId": "1726355",
                    "name": "Sriram Ganapathy"
                },
                {
                    "authorId": "12295226",
                    "name": "Ankur Bapna"
                },
                {
                    "authorId": "145961776",
                    "name": "Min Ma"
                },
                {
                    "authorId": "72549949",
                    "name": "Wei Han"
                },
                {
                    "authorId": "82840075",
                    "name": "Vera Axelrod"
                },
                {
                    "authorId": "2408872",
                    "name": "P. Talukdar"
                }
            ]
        },
        {
            "paperId": "b39b47ea594fe2365a055b6f0ca0a5915e88c6de",
            "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training",
            "abstract": "Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2264977662",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2843215",
                    "name": "Tolga Bolukbasi"
                },
                {
                    "authorId": "1726355",
                    "name": "Sriram Ganapathy"
                },
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                },
                {
                    "authorId": "2408872",
                    "name": "P. Talukdar"
                }
            ]
        },
        {
            "paperId": "ffd86ec4ca77662efa3c7073366186168159aa0f",
            "title": "MASR: Multi-Label Aware Speech Representation",
            "abstract": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2184503040",
                    "name": "Anjali Raj"
                },
                {
                    "authorId": "2136381352",
                    "name": "Shikhar Bharadwaj"
                },
                {
                    "authorId": "1726355",
                    "name": "Sriram Ganapathy"
                },
                {
                    "authorId": "145961776",
                    "name": "Min Ma"
                },
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                }
            ]
        },
        {
            "paperId": "19db2eb6b4906f669f397a7bbb75a974bfcd934e",
            "title": "Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network",
            "abstract": "Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs. We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed. We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting. We find that our model\u2019s performance improvements stem primarily from its robustness to sparsity. We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities. This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398104959",
                    "name": "Justin Lovelace"
                },
                {
                    "authorId": "2111848565",
                    "name": "Denis R. Newman-Griffis"
                },
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                },
                {
                    "authorId": "2142730",
                    "name": "J. Lehman"
                },
                {
                    "authorId": "73771627",
                    "name": "C. Ros'e"
                }
            ]
        }
    ]
}