{
    "authorId": "2206841649",
    "papers": [
        {
            "paperId": "15a35c7f1392e7fb5cf3d9794a9990634298e58a",
            "title": "Towards Multilingual Audio-Visual Question Answering",
            "abstract": "In this paper, we work towards extending Audio-Visual Question Answering (AVQA) to multilingual settings. Existing AVQA research has predominantly revolved around English and replicating it for addressing AVQA in other languages requires a substantial allocation of resources. As a scalable solution, we leverage machine translation and present two multilingual AVQA datasets for eight languages created from existing benchmark AVQA datasets. This prevents extra human annotation efforts of collecting questions and answers manually. To this end, we propose, MERA framework, by leveraging state-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in multiple languages. We introduce a suite of models namely MERA-L, MERA-C, MERA-T with varied model architectures to benchmark the proposed datasets. We believe our work will open new research directions and act as a reference benchmark for future works in multilingual AVQA.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": "2306247699",
                    "name": "Priyabrata Mallick"
                },
                {
                    "authorId": "1500525405",
                    "name": "Swarup Ranjan Behera"
                },
                {
                    "authorId": "2305816505",
                    "name": "Aalekhya Satya Narayani"
                },
                {
                    "authorId": "3170352",
                    "name": "Arun Balaji Buduru"
                },
                {
                    "authorId": "2257126044",
                    "name": "Rajesh Sharma"
                }
            ]
        },
        {
            "paperId": "2e9f917145c70dd3452482cf59a46f5993b03ba0",
            "title": "Are Paralinguistic Representations all that is needed for Speech Emotion Recognition?",
            "abstract": "Availability of representations from pre-trained models (PTMs) have facilitated substantial progress in speech emotion recognition (SER). Particularly, representations from PTM trained for paralinguistic speech processing have shown state-of-the-art (SOTA) performance for SER. However, such paralinguistic PTM representations haven't been evaluated for SER in linguistic environments other than English. Also, paralinguistic PTM representations haven't been investigated in benchmarks such as SUPERB, EMO-SUPERB, ML-SUPERB for SER. This makes it difficult to access the efficacy of paralinguistic PTM representations for SER in multiple languages. To fill this gap, we perform a comprehensive comparative study of five SOTA PTM representations. Our results shows that paralinguistic PTM (TRILLsson) representations performs the best and this performance can be attributed to its effectiveness in capturing pitch, tone and other speech characteristics more effectively than other PTM representations.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": "2138567368",
                    "name": "Gautam Siddharth Kashyap"
                },
                {
                    "authorId": "3170352",
                    "name": "Arun Balaji Buduru"
                },
                {
                    "authorId": "2257126044",
                    "name": "Rajesh Sharma"
                }
            ]
        },
        {
            "paperId": "396ddb2512c581a41baddc017260092459cff6e9",
            "title": "The Reasonable Effectiveness of Speaker Embeddings for Violence Detection",
            "abstract": "In this paper, we focus on audio violence detection (AVD). AVD is necessary for several reasons, especially in the context of maintaining safety, preventing harm, and ensuring security in various environments. This calls for accurate AVD systems. Like many related applications in audio processing, the most common approach for improving the performance, would be by leveraging self-supervised (SSL) pre-trained models (PTMs). However, as these SSL models are very large models with million of parameters and this can hinder real-world deployment especially in compute-constraint environment. To resolve this, we propose the usage of speaker recognition models which are much smaller compared to the SSL models. Experimentation with speaker recognition model embeddings with SVM&Random Forest as classifiers, we show that speaker recognition model embeddings perform the best in comparison to state-of-the-art (SOTA) SSL models and achieve SOTA results.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305752671",
                    "name": "Sarthak Jain"
                },
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": "3170352",
                    "name": "Arun Balaji Buduru"
                },
                {
                    "authorId": "2257126044",
                    "name": "Rajesh Sharma"
                }
            ]
        },
        {
            "paperId": "3c3c8b523a828374d35e40b7262508c8b34e19ba",
            "title": "Heterogeneity over Homogeneity: Investigating Multilingual Speech Pre-Trained Models for Detecting Audio Deepfake",
            "abstract": "In this work, we investigate multilingual speech Pre-Trained models (PTMs) for Audio deepfake detection (ADD). We hypothesize that multilingual PTMs trained on large-scale diverse multilingual data gain knowledge about diverse pitches, accents, and tones, during their pre-training phase and making them more robust to variations. As a result, they will be more effective for detecting audio deepfakes. To validate our hypothesis, we extract representations from state-of-the-art (SOTA) PTMs including monolingual, multilingual as well as PTMs trained for speaker and emotion recognition, and evaluated them on ASVSpoof 2019 (ASV), In-the-Wild (ITW), and DECRO benchmark databases. We show that representations from multilingual PTMs, with simple downstream networks, attain the best performance for ADD compared to other PTM representations, which validates our hypothesis. We also explore the possibility of fusion of selected PTM representations for further improvements in ADD, and we propose a framework, MiO (Merge into One) for this purpose. With MiO, we achieve SOTA performance on ASV and ITW and comparable performance on DECRO with current SOTA works.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": "2138567368",
                    "name": "Gautam Siddharth Kashyap"
                },
                {
                    "authorId": "3170352",
                    "name": "Arun Balaji Buduru"
                },
                {
                    "authorId": "2257126044",
                    "name": "Rajesh Sharma"
                }
            ]
        },
        {
            "paperId": "46c8f36e0b47b672a96658d72be9767d5427e2c6",
            "title": "AVR: Synergizing Foundation Models for Audio-Visual Humor Detection",
            "abstract": "In this work, we present, AVR application for audio-visual humor detection. While humor detection has traditionally centered around textual analysis, recent advancements have spotlighted multimodal approaches. However, these methods lean on textual cues as a modality, necessitating the use of ASR systems for transcribing the audio-data. This heavy reliance on ASR accuracy can pose challenges in real-world applications. To address this bottleneck, we propose an innovative audio-visual humor detection system that circumvents textual reliance, eliminating the need for ASR models. Instead, the proposed approach hinges on the intricate interplay between audio and visual content for effective humor detection.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2307221853",
                    "name": "Sarthak Sharma"
                },
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": null,
                    "name": "Drishti Singh"
                },
                {
                    "authorId": "3170352",
                    "name": "Arun Balaji Buduru"
                },
                {
                    "authorId": "2257126044",
                    "name": "Rajesh Sharma"
                }
            ]
        },
        {
            "paperId": "792695717d3b31b047cd81b509101449519feaaf",
            "title": "ASGIR: Audio Spectrogram Transformer Guided Classification And Information Retrieval For Birds",
            "abstract": "Recognition and interpretation of bird vocalizations are pivotal in ornithological research and ecological conservation efforts due to their significance in understanding avian behaviour, performing habitat assessment and judging ecological health. This paper presents an audio spectrogram-guided classification framework called ASGIR for improved bird sound recognition and information retrieval. Our work is accompanied by a simple-to-use, two-step information retrieval system that uses geographical location and bird sounds to localize and retrieve relevant bird information by scraping Wikipedia page information of recognized birds. ASGIR offers a substantial performance on a random subset of 51 classes of Xeno-Canto dataset Bird sounds from European countries with a median of 100\\% performance on F1, Precision and Sensitivity metrics. Our code is available as follows: https://github.com/MainSample1234/AS-GIR .",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2279023699",
                    "name": "Yashwardhan Chaudhuri"
                },
                {
                    "authorId": "2313629968",
                    "name": "Paridhi Mundra"
                },
                {
                    "authorId": "2313642877",
                    "name": "Arnesh Batra"
                },
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": "3170352",
                    "name": "Arun Balaji Buduru"
                }
            ]
        },
        {
            "paperId": "8359290accee44cf3986a51b7a87bc36fddbe313",
            "title": "ComFeAT: Combination of Neural and Spectral Features for Improved Depression Detection",
            "abstract": "In this work, we focus on the detection of depression through speech analysis. Previous research has widely explored features extracted from pre-trained models (PTMs) primarily trained for paralinguistic tasks. Although these features have led to sufficient advances in speech-based depression detection, their performance declines in real-world settings. To address this, in this paper, we introduce ComFeAT, an application that employs a CNN model trained on a combination of features extracted from PTMs, a.k.a. neural features and spectral features to enhance depression detection. Spectral features are robust to domain variations, but, they are not as good as neural features in performance, suprisingly, combining them shows complementary behavior and improves over both neural and spectral features individually. The proposed method also improves over previous state-of-the-art (SOTA) works on E-DAIC benchmark.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": "2305752671",
                    "name": "Sarthak Jain"
                },
                {
                    "authorId": "2145893727",
                    "name": "Shubham Singh"
                },
                {
                    "authorId": "2292298350",
                    "name": "Muskaan Singh"
                },
                {
                    "authorId": "3170352",
                    "name": "Arun Balaji Buduru"
                },
                {
                    "authorId": "2257126044",
                    "name": "Rajesh Sharma"
                }
            ]
        },
        {
            "paperId": "9a9036cda05dca854bfb2c7d2e12bb6da5ebcb81",
            "title": "PERSONA: An Application for Emotion Recognition, Gender Recognition and Age Estimation",
            "abstract": "Emotion Recognition (ER), Gender Recognition (GR), and Age Estimation (AE) constitute paralinguistic tasks that rely not on the spoken content but primarily on speech characteristics such as pitch and tone. While previous research has made significant strides in developing models for each task individually, there has been comparatively less emphasis on concurrently learning these tasks, despite their inherent interconnectedness. As such in this demonstration, we present PERSONA, an application for predicting ER, GR, and AE with a single model in the backend. One notable point is we show that representations from speaker recognition pre-trained model (PTM) is better suited for such a multi-task learning format than the state-of-the-art (SOTA) self-supervised (SSL) PTM by carrying out a comparative study. Our methodology obviates the need for deploying separate models for each task and can potentially conserve resources and time during the training and deployment phases.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2305685647",
                    "name": "Devyani Koshal"
                },
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": "2305752671",
                    "name": "Sarthak Jain"
                },
                {
                    "authorId": "3170352",
                    "name": "Arun Balaji Buduru"
                },
                {
                    "authorId": "2257126044",
                    "name": "Rajesh Sharma"
                }
            ]
        },
        {
            "paperId": "a6d5bb0e61e4d37dc21670745fc1b75abd23d160",
            "title": "Modality-Order Matters! A Novel Hierarchical Feature Fusion Method for CoSAm: A Code-Switched Autism Corpus",
            "abstract": "Autism Spectrum Disorder (ASD) is a complex neuro-developmental challenge, presenting a spectrum of difficulties in social interaction, communication, and the expression of repetitive behaviors in different situations. This increasing prevalence underscores the importance of ASD as a major public health concern and the need for comprehensive research initiatives to advance our understanding of the disorder and its early detection methods. This study introduces a novel hierarchical feature fusion method aimed at enhancing the early detection of ASD in children through the analysis of code-switched speech (English and Hindi). Employing advanced audio processing techniques, the research integrates acoustic, paralinguistic, and linguistic information using Transformer Encoders. This innovative fusion strategy is designed to improve classification robustness and accuracy, crucial for early and precise ASD identification. The methodology involves collecting a code-switched speech corpus, CoSAm, from children diagnosed with ASD and a matched control group. The dataset comprises 61 voice recordings from 30 children diagnosed with ASD and 31 from neurotypical children, aged between 3 and 13 years, resulting in a total of 159.75 minutes of voice recordings. The feature analysis focuses on MFCCs and extensive statistical attributes to capture speech pattern variability and complexity. The best model performance is achieved using a hierarchical fusion technique with an accuracy of 98.75% using a combination of acoustic and linguistic features first, followed by paralinguistic features in a hierarchical manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292276709",
                    "name": "Mohd Mujtaba Akhtar"
                },
                {
                    "authorId": "2304953446",
                    "name": "Girish"
                },
                {
                    "authorId": "2292298350",
                    "name": "Muskaan Singh"
                },
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                }
            ]
        },
        {
            "paperId": "ab678c919c25977ad7ad9f3cd8f0444cae323a3b",
            "title": "Strong Alone, Stronger Together: Synergizing Modality-Binding Foundation Models with Optimal Transport for Non-Verbal Emotion Recognition",
            "abstract": "In this study, we investigate multimodal foundation models (MFMs) for emotion recognition from non-verbal sounds. We hypothesize that MFMs, with their joint pre-training across multiple modalities, will be more effective in non-verbal sounds emotion recognition (NVER) by better interpreting and differentiating subtle emotional cues that may be ambiguous in audio-only foundation models (AFMs). To validate our hypothesis, we extract representations from state-of-the-art (SOTA) MFMs and AFMs and evaluated them on benchmark NVER datasets. We also investigate the potential of combining selected foundation model representations to enhance NVER further inspired by research in speech recognition and audio deepfake detection. To achieve this, we propose a framework called MATA (Intra-Modality Alignment through Transport Attention). Through MATA coupled with the combination of MFMs: LanguageBind and ImageBind, we report the topmost performance with accuracies of 76.47%, 77.40%, 75.12% and F1-scores of 70.35%, 76.19%, 74.63% for ASVP-ESD, JNV, and VIVAE datasets against individual FMs and baseline fusion techniques and report SOTA on the benchmark datasets.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": "2292276709",
                    "name": "Mohd Mujtaba Akhtar"
                },
                {
                    "authorId": "2304953446",
                    "name": "Girish"
                },
                {
                    "authorId": "1500525405",
                    "name": "Swarup Ranjan Behera"
                },
                {
                    "authorId": "11105946",
                    "name": "Sishir Kalita"
                },
                {
                    "authorId": "3170352",
                    "name": "Arun Balaji Buduru"
                },
                {
                    "authorId": "2257126044",
                    "name": "Rajesh Sharma"
                },
                {
                    "authorId": "2322441913",
                    "name": "S. R. M. Prasanna"
                }
            ]
        }
    ]
}