{
    "authorId": "145964711",
    "papers": [
        {
            "paperId": "03627e32a048ba71ad6ed632df2f3669464a7dae",
            "title": "Domain Information Control at Inference Time for Acoustic Scene Classification",
            "abstract": "Domain shift is considered a challenge in machine learning as it causes significant degradation of model performance. In the Acoustic Scene Classification task (ASC), domain shift is mainly caused by different recording devices. Several studies have already targeted domain generalization to improve the performance of ASC models on unseen domains, such as new devices. Recently, the Controllable Gate Adapter (CONGATER) has been proposed in Natural Language Processing to address the biased training data problem. CONGATER allows controlling the debiasing process at inference time. CONGATER's main advantage is the continuous and selective debiasing of a trained model, during inference. In this work, we adapt CONGATER to the audio spectrogram transformer for an acoustic scene classification task. We show that CONGATER can be used to selectively adapt the learned representations to be invariant to device domain shifts such as recording devices. Our analysis shows that CONGATER can progressively remove device information from the learned representations and improve the model generalization, especially under domain shift conditions (e.g. unseen devices). We show that information removal can be extended to both device and location domain. Finally, we demonstrate CONGATER's ability to enhance specific device performance without further training11Source Code: https://github.com/ShawMaskldcase22_CONGATER.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2184114298",
                    "name": "Shahed Masoudian"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "144125621",
                    "name": "M. Schedl"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                },
                {
                    "authorId": "2844293",
                    "name": "Navid Rekabsaz"
                }
            ]
        },
        {
            "paperId": "22d2bb4082dc53110d2d4d48db61d8d990906a39",
            "title": "Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features",
            "abstract": "Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. This paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score. The proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm. Our results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets. In addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/manoskary/chordgnn",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2020814385",
                    "name": "E. Karystinaios"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "3a2605cec29c0439a4d7975cf4188b82ffd82327",
            "title": "Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets",
            "abstract": "This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12746771",
                    "name": "Paul Primus"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "481fbda08a74cf5632ddb39b264963f68dc04ddb",
            "title": "Discrete Diffusion Probabilistic Models for Symbolic Music Generation",
            "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have made great strides in generating high-quality samples in both discrete and continuous domains.\n\nHowever, Discrete DDPMs (D3PMs) have yet to be applied to the domain of Symbolic Music.\n\nThis work presents the direct generation of Polyphonic Symbolic Music using D3PMs.\n\nOur model exhibits state-of-the-art sample quality, according to current quantitative evaluation metrics, and allows for flexible infilling at the note level.\n\nWe further show, that our models are accessible to post-hoc classifier guidance, widening the scope of possible applications.\n\nHowever, we also cast a critical view on quantitative evaluation of music sample quality via statistical metrics, and present a simple algorithm that can confound our metrics with completely spurious, non-musical samples.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2217347305",
                    "name": "Matthias Plasser"
                },
                {
                    "authorId": "1854910522",
                    "name": "S. Peter"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "48eb6106d3abd2306e2b7deac5eed15a673a2668",
            "title": "Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model",
            "abstract": "Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed\"typical sampling\", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2105451490",
                    "name": "Mathias Rose Bjare"
                },
                {
                    "authorId": "39670670",
                    "name": "S. Lattner"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "49da1126f99fd4c6b5ac7a180a0ef0799e987223",
            "title": "The ACCompanion: Combining Reactivity, Robustness, and Musical Expressivity in an Automatic Piano Accompanist",
            "abstract": "This paper introduces the ACCompanion, an expressive accompaniment system. Similarly to a musician who accompanies a soloist playing a given musical piece, our system can produce a human-like rendition of the accompaniment part that follows the soloist's choices in terms of tempo, dynamics, and articulation. The ACCompanion works in the symbolic domain, i.e., it needs a musical instrument capable of producing and playing MIDI data, with explicitly encoded onset, offset, and pitch for each played note. We describe the components that go into such a system, from real-time score following and prediction to expressive performance generation and online adaptation to the expressive choices of the human player. Based on our experience with repeated live demonstrations in front of various audiences, we offer an analysis of the challenges of combining these components into a system that is highly reactive and precise, while still a reliable musical partner, robust to possible performance errors and responsive to expressive variations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2120477736",
                    "name": "Carlos Eduardo Cancino Chac\u00f3n"
                },
                {
                    "authorId": "1854910522",
                    "name": "S. Peter"
                },
                {
                    "authorId": "2187311543",
                    "name": "Patricia Hu"
                },
                {
                    "authorId": "2020814385",
                    "name": "E. Karystinaios"
                },
                {
                    "authorId": "50509045",
                    "name": "Florian Henkel"
                },
                {
                    "authorId": "74503309",
                    "name": "Francesco Foscarin"
                },
                {
                    "authorId": "2215270134",
                    "name": "Nimrod Varga"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "4ee9085969b24abd3a4e1ac4f5320b6823226f27",
            "title": "Musical Voice Separation as Link Prediction: Modeling a Musical Perception Task as a Multi-Trajectory Tracking Problem",
            "abstract": "This paper targets the perceptual task of separating the different interacting voices, i.e., monophonic melodic streams, in a polyphonic musical piece. We target symbolic music, where notes are explicitly encoded, and model this task as a Multi-Trajectory Tracking (MTT) problem from discrete observations, i.e., notes in a pitch-time space. Our approach builds a graph from a musical piece, by creating one node for every note, and separates the melodic trajectories by predicting a link between two notes if they are consecutive in the same voice/stream. This kind of local, greedy prediction is made possible by node embeddings created by a heterogeneous graph neural network that can capture inter- and intra-trajectory information. Furthermore, we propose a new regularization loss that encourages the output to respect the MTT premise of at most one incoming and one outgoing link for every node, favoring monophonic (voice) trajectories; this loss function might also be useful in other general MTT scenarios. Our approach does not use domain-specific heuristics, is scalable to longer sequences and a higher number of voices, and can handle complex cases such as voice inversions and overlaps. We reach new state-of-the-art results for the voice separation task on classical music of different styles. All code, data, and pretrained models are available on https://github.com/manoskary/vocsep_ijcai2023",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2020814385",
                    "name": "E. Karystinaios"
                },
                {
                    "authorId": "74503309",
                    "name": "Francesco Foscarin"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "587c824fdba8a4966c1af145be8389c74e780b3b",
            "title": "Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems",
            "abstract": "Linking sheet music images to audio recordings remains a key problem for the development of efficient cross-modal music retrieval systems. One of the fundamental approaches toward this task is to learn a cross-modal embedding space via deep neural networks that is able to connect short snippets of audio and sheet music. However, the scarcity of annotated data from real musical content affects the capability of such methods to generalize to real retrieval scenarios. In this work, we investigate whether we can mitigate this limitation with self-supervised contrastive learning, by exposing a network to a large amount of real music data as a pre-training step, by contrasting randomly augmented views of snippets of both modalities, namely audio and sheet images. Through a number of experiments on synthetic and real piano data, we show that pretrained models are able to retrieve snippets with better precision in all scenarios and pre-training configurations. Encouraged by these results, we employ the snippet embeddings in the higher-level task of cross-modal piece identification and conduct more experiments on several retrieval configurations. In this task, we observe that the retrieval quality improves from 30% up to 100% when real music data is present. We then conclude by arguing for the potential of self-supervised contrastive learning for alleviating the annotated data scarcity in multi-modal music retrieval models. Code and trained models are accessible at https://github.com/luisfvc/ucasr.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2073631954",
                    "name": "Luis Carvalho"
                },
                {
                    "authorId": "2219738916",
                    "name": "Tobias Wash\u00fcttl"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "5d433a43aa8b5c4687e9edebf2fac6e8e248bb10",
            "title": "Predicting Music Hierarchies with a Graph-Based Neural Decoder",
            "abstract": "This paper describes a data-driven framework to parse musical sequences into dependency trees, which are hierarchical structures used in music cognition research and music analysis. The parsing involves two steps. First, the input sequence is passed through a transformer encoder to enrich it with contextual information. Then, a classifier filters the graph of all possible dependency arcs to produce the dependency tree. One major benefit of this system is that it can be easily integrated into modern deep-learning pipelines. Moreover, since it does not rely on any particular symbolic grammar, it can consider multiple musical features simultaneously, make use of sequential context information, and produce partial results for noisy inputs. We test our approach on two datasets of musical trees -- time-span trees of monophonic note sequences and harmonic trees of jazz chord sequences -- and show that our approach outperforms previous methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "74503309",
                    "name": "Francesco Foscarin"
                },
                {
                    "authorId": "14901693",
                    "name": "Daniel Harasim"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "8ec3bb3c33f9c6b4279cc00fa966543a22b8801a",
            "title": "Automatic Note-Level Score-to-Performance Alignments in the ASAP Dataset",
            "abstract": "Several MIR applications require fine-grained note alignments between MIDI performances and their musical scores for training and evaluation. However, large and high-quality datasets with this kind of data are not available, and their manual creation is a very time-consuming task that can only be performed by field experts. In this paper, we evaluate state-of-the-art automatic note alignment models applied to dataset generation. We increase the accuracy and reliability of the produced alignments with models that flexibly leverage existing annotations such as beat or measure alignments. We thoroughly evaluate these segment-constrained models and use the best to create note alignments for the ASAP dataset, a large dataset of solo piano MIDI performances beat-aligned to MusicXML scores. The resulting note alignments are manually checked and publicly available at: https://github.com/CPJKU/ asap-dataset. The contributions of this paper are four-fold: (1) we extend the ASAP dataset with reliable note alignments, thus creating (n)ASAP, the largest available fully note-aligned dataset, comprising more than 7 M annotated notes and close to 100 hours of music; (2) we design, evaluate, and publish segment-constrained models for note alignments that flexibly leverage existing annotations and significantly outperform automatic models; (3) we design, evaluate, and publish unconstrained automatic models for note alignment that produce results on par with the state of the art; (4) we introduce Parangonada , a web-interface for visualizing and correcting alignment annotations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1854910522",
                    "name": "S. Peter"
                },
                {
                    "authorId": "2120477736",
                    "name": "Carlos Eduardo Cancino Chac\u00f3n"
                },
                {
                    "authorId": "74503309",
                    "name": "Francesco Foscarin"
                },
                {
                    "authorId": "144745072",
                    "name": "Andrew Mcleod"
                },
                {
                    "authorId": "50509045",
                    "name": "Florian Henkel"
                },
                {
                    "authorId": "2020814385",
                    "name": "E. Karystinaios"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        }
    ]
}