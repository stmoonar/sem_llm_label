{
    "authorId": "123966440",
    "papers": [
        {
            "paperId": "43b839deec89604bb21e888d994bbf15889b921d",
            "title": "OCCAM: Towards Cost-Efficient and Accuracy-Aware Image Classification Inference",
            "abstract": "Image classification is a fundamental building block for a majority of computer vision applications. With the growing popularity and capacity of machine learning models, people can easily access trained image classifiers as a service online or offline. However, model use comes with a cost and classifiers of higher capacity usually incur higher inference costs. To harness the respective strengths of different classifiers, we propose a principled approach, OCCAM, to compute the best classifier assignment strategy over image classification queries (termed as the optimal model portfolio) so that the aggregated accuracy is maximized, under user-specified cost budgets. Our approach uses an unbiased and low-variance accuracy estimator and effectively computes the optimal solution by solving an integer linear programming problem. On a variety of real-world datasets, OCCAM achieves 40% cost reduction with little to no accuracy drop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                },
                {
                    "authorId": "2305484959",
                    "name": "Bicheng Xu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "97b6f4357d1e3ab40a7ee60acb5260a948e3641d",
            "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
            "abstract": "Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                },
                {
                    "authorId": "2297849625",
                    "name": "Ankur Mallick"
                },
                {
                    "authorId": "2298452007",
                    "name": "Chi Wang"
                },
                {
                    "authorId": "2253669181",
                    "name": "Robert Sim"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "3898805",
                    "name": "Victor R\u00fchle"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ]
        },
        {
            "paperId": "d9a47e5bd80268a1b879917d90b042db5e3de6af",
            "title": "LLM Performance Predictors are good initializers for Architecture Search",
            "abstract": "In this work, we utilize Large Language Models (LLMs) for a novel use case: constructing Performance Predictors (PP) that estimate the performance of specific deep neural network architectures on downstream tasks. We create PP prompts for LLMs, comprising (i) role descriptions, (ii) instructions for the LLM, (iii) hyperparameter definitions, and (iv) demonstrations presenting sample architectures with efficiency metrics and `training from scratch' performance. In machine translation (MT) tasks, GPT-4 with our PP prompts (LLM-PP) achieves a SoTA mean absolute error and a slight degradation in rank correlation coefficient compared to baseline predictors. Additionally, we demonstrate that predictions from LLM-PP can be distilled to a compact regression model (LLM-Distill-PP), which surprisingly retains much of the performance of LLM-PP. This presents a cost-effective alternative for resource-intensive performance estimation. Specifically, for Neural Architecture Search (NAS), we introduce a Hybrid-Search algorithm (HS-NAS) employing LLM-Distill-PP for the initial search stages and reverting to the baseline predictor later. HS-NAS performs similarly to SoTA NAS, reducing search hours by approximately 50%, and in some cases, improving latency, GFLOPs, and model size. The code can be found at: https://github.com/UBC-NLP/llmas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065043351",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                }
            ]
        },
        {
            "paperId": "23fe77fd81fe9eaad8e8c7164ce759d2ca0f65f4",
            "title": "On Efficient Approximate Queries over Machine Learning Models",
            "abstract": "\n The question of answering queries over ML predictions has been gaining attention in the database community. This question is challenging because finding high quality answers by invoking an\n oracle\n such as a human expert or an expensive deep neural network model on every single item in the DB and then applying the query, can be prohibitive. We develop a novel unified framework for approximate query answering by leveraging a\n proxy\n to minimize the oracle usage of finding high quality answers for both Precision-Target (PT) and Recall-Target (RT) queries. Our framework uses a judicious combination of invoking the expensive oracle on data samples and applying the cheap proxy on the DB objects. It relies on two assumptions. Under the P\n roxy\n Q\n uality\n assumption, we develop two algorithms: PQA that efficiently finds high quality answers with high probability and no oracle calls, and PQE, a heuristic extension that achieves empirically good performance with a small number of oracle calls. Alternatively, under the C\n ore\n S\n et\n C\n losure\n assumption, we develop two algorithms: CSC that efficiently returns high quality answers with high probability and minimal oracle usage, and CSE, which extends it to more general settings. Our extensive experiments on five real-world datasets on both query types, PT and RT, demonstrate that our algorithms outperform the state-of-the-art and achieve high result quality with provable statistical guarantees.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "2b110a1ca894f331c2173f9676b71b54255e0be8",
            "title": "Uncovering the subtype-specific temporal order of cancer pathway dysregulation",
            "abstract": "Cancer is driven by genetic mutations that dysregulate pathways important for proper cell function. Therefore, discovering these cancer pathways and their dysregulation order is key to understanding and treating cancer. However, the heterogeneity of mutations between different individuals makes this challenging and requires that cancer progression is studied in a subtype-specific way. To address this challenge, we provide a mathematical model, called Subtype-specific Pathway Linear Progression Model (SPM), that simultaneously captures cancer subtypes and pathways and order of dysregulation of the pathways within each subtype. Experiments with synthetic data indicate the robustness of SPM to problem specifics including noise compared to an existing method. Moreover, experimental results on glioblastoma multiforme and colorectal adenocarcinoma show the consistency of SPM\u2019s results with the existing knowledge and its superiority to an existing method in certain cases. The implementation of our method is available at https://github.com/Dalton386/SPM.",
            "fieldsOfStudy": [
                "Medicine",
                "Biology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1976175",
                    "name": "Sahand Khakabimamaghani"
                },
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                },
                {
                    "authorId": "88883482",
                    "name": "Oliver Snow"
                },
                {
                    "authorId": "1766588",
                    "name": "M. Ester"
                }
            ]
        },
        {
            "paperId": "d73912a914315fe419b2270c2ed498628f594d5d",
            "title": "Collaborative intra-tumor heterogeneity detection",
            "abstract": "Abstract Motivation Despite the remarkable advances in sequencing and computational techniques, noise in the data and complexity of the underlying biological mechanisms render deconvolution of the phylogenetic relationships between cancer mutations difficult. Besides that, the majority of the existing datasets consist of bulk sequencing data of single tumor sample of an individual. Accurate inference of the phylogenetic order of mutations is particularly challenging in these cases and the existing methods are faced with several theoretical limitations. To overcome these limitations, new methods are required for integrating and harnessing the full potential of the existing data. Results We introduce a method called Hintra for intra-tumor heterogeneity detection. Hintra integrates sequencing data for a cohort of tumors and infers tumor phylogeny for each individual based on the evolutionary information shared between different tumors. Through an iterative process, Hintra learns the repeating evolutionary patterns and uses this information for resolving the phylogenetic ambiguities of individual tumors. The results of synthetic experiments show an improved performance compared to two state-of-the-art methods. The experimental results with a recent Breast Cancer dataset are consistent with the existing knowledge and provide potentially interesting findings. Availability and implementation The source code for Hintra is available at https://github.com/sahandk/HINTRA.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1976175",
                    "name": "Sahand Khakabimamaghani"
                },
                {
                    "authorId": "2296376",
                    "name": "S. Maliki\u0107"
                },
                {
                    "authorId": "48252004",
                    "name": "Jeffrey M. Tang"
                },
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                },
                {
                    "authorId": "1803995",
                    "name": "Ryan D. Morin"
                },
                {
                    "authorId": "2967774",
                    "name": "L. Chindelevitch"
                },
                {
                    "authorId": "1766588",
                    "name": "M. Ester"
                }
            ]
        }
    ]
}