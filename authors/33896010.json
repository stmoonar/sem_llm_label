{
    "authorId": "33896010",
    "papers": [
        {
            "paperId": "00b167a24f57f96294a4581bb185a57b0a4af365",
            "title": "LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation",
            "abstract": "Layout-to-image generation refers to the task of synthesizing photo-realistic images based on semantic layouts. In this paper, we propose LayoutDiffuse that adapts a foundational diffusion model pretrained on large-scale image or text-image datasets for layout-to-image generation. By adopting a novel neural adaptor based on layout attention and task-aware prompts, our method trains efficiently, generates images with both high perceptual quality and layout alignment, and needs less data. Experiments on three datasets show that our method significantly outperforms other 10 generative models based on GANs, VQ-VAE, and diffusion models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "2208195863",
                    "name": "Xiao Liang"
                },
                {
                    "authorId": "2110332219",
                    "name": "Xingjian Shi"
                },
                {
                    "authorId": "2118328320",
                    "name": "Tong He"
                },
                {
                    "authorId": "39102205",
                    "name": "Tianjun Xiao"
                },
                {
                    "authorId": "1701799",
                    "name": "Mu Li"
                }
            ]
        },
        {
            "paperId": "7dc98ae2967d6ad9c115ccaa705540b7489e0d40",
            "title": "User-Controllable Arbitrary Style Transfer via Entropy Regularization",
            "abstract": "Ensuring the overall end-user experience is a challenging task in arbitrary style transfer (AST) due to the subjective nature of style transfer quality. A good practice is to provide users many instead of one AST result. However, existing approaches require to run multiple AST models or inference a diversified AST (DAST) solution multiple times, and thus they are either slow in speed or limited in diversity. In this paper, we propose a novel solution ensuring both efficiency and diversity for generating multiple user-controllable AST results by systematically modulating AST behavior at run-time. We begin with reformulating three prominent AST methods into a unified assign-and-mix problem and discover that the entropies of their assignment matrices exhibit a large variance. We then solve the unified problem in an optimal transport framework using the Sinkhorn-Knopp algorithm with a user input \u03b5 to control the said entropy and thus modulate stylization. Empirical results demonstrate the superiority of the proposed solution, with speed and stylization quality comparable to or better than existing AST and significantly more diverse than previous DAST works. Code is available at https://github.com/cplusx/eps-Assign-and-Mix.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "2213162331",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                }
            ]
        },
        {
            "paperId": "56f67b02d4a48c68e81e43f65b7e3f2c3d701305",
            "title": "Attack-Agnostic Adversarial Detection",
            "abstract": "The growing number of adversarial attacks in recent years gives attackers an advantage over defenders, as defenders must train detectors after knowing the types of attacks, and many models need to be maintained to ensure good performance in detecting any upcoming attacks. We propose a way to end the tug-of-war between attackers and defenders by treating adversarial attack detection as an anomaly detection problem so that the detector is agnostic to the attack. We quantify the statistical deviation caused by adversarial perturbations in two aspects. The Least Significant Component Feature (LSCF) quantifies the deviation of adversarial examples from the statistics of benign samples and Hessian Feature (HF) reflects how adversarial examples distort the landscape of the model's optima by measuring the local loss curvature. Empirical results show that our method can achieve an overall ROC AUC of 94.9%, 89.7%, and 94.6% on CIFAR10, CIFAR100, and SVHN, respectively, and has comparable performance to adversarial detectors trained with adversarial examples on most of the attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "2052375209",
                    "name": "Mohamed Hussein"
                },
                {
                    "authorId": "2710228",
                    "name": "J. Billa"
                },
                {
                    "authorId": "17806729",
                    "name": "Wael AbdAlmageed"
                }
            ]
        },
        {
            "paperId": "0eea710d388cb0bae1e6a2ebe1691eafcf5da5d3",
            "title": "Style-Aware Normalized Loss for Improving Arbitrary Style Transfer",
            "abstract": "Neural Style Transfer (NST) has quickly evolved from single-style to infinite-style models, also known as Arbitrary Style Transfer (AST). Although appealing results have been widely reported in literature, our empirical studies on four well-known AST approaches (GoogleMagenta [14], AdaIN [19], LinearTransfer [29], and SANet [37]) show that more than 50% of the time, AST stylized images are not acceptable to human users, typically due to under- or over-stylization. We systematically study the cause of this imbalanced style transferability (IST ) and propose a simple yet effective solution to mitigate this issue. Our studies show that the IST issue is related to the conventional AST style loss, and reveal that the root cause is the equal weightage of training samples irrespective of the properties of their corresponding style images, which biases the model towards certain styles. Through investigation of the theoretical bounds of the AST style loss, we propose a new loss that largely overcomes IST . Theoretical analysis and experimental results validate the effectiveness of our loss, with over 80% relative improvement in style deception rate and 98% relatively higher preference in human evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "2119299240",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "145603129",
                    "name": "P. Natarajan"
                }
            ]
        },
        {
            "paperId": "f531733697754b45d57dd0fb9fed00ff577b0f97",
            "title": "SIGN: Spatial-information Incorporated Generative Network for Generalized Zero-shot Semantic Segmentation",
            "abstract": "Unlike conventional zero-shot classification, zero-shot semantic segmentation predicts a class label at the pixel level instead of the image level. When solving zero-shot semantic segmentation problems, the need for pixel-level prediction with surrounding context motivates us to incorporate spatial information using positional encoding. We improve standard positional encoding by introducing the concept of Relative Positional Encoding, which integrates spatial information at the feature level and can handle arbitrary image sizes. Furthermore, while self-training is widely used in zero-shot semantic segmentation to generate pseudo-labels, we propose a new knowledge-distillation-inspired self-training strategy, namely Annealed Self-Training, which can automatically assign different importance to pseudo-labels to improve performance. We systematically study the proposed Relative Positional Encoding and Annealed Self-Training in a comprehensive experimental evaluation, and our empirical results confirm the effectiveness of our method on three benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "144358194",
                    "name": "Soumyaroop Nandi"
                },
                {
                    "authorId": "145603129",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "17806729",
                    "name": "Wael AbdAlmageed"
                }
            ]
        },
        {
            "paperId": "4e910df41181637518563cf729d3bdaa166882c6",
            "title": "Recurrent Convolutional Strategies for Face Manipulation Detection in Videos",
            "abstract": "The spread of misinformation through synthetically generated yet realistic images and videos has become a significant problem, calling for robust manipulation detection methods. Despite the predominant effort of detecting face manipulation in still images, less attention has been paid to the identification of tampered faces in videos by taking advantage of the temporal information present in the stream. Recurrent convolutional models are a class of deep learning models which have proven effective at exploiting the temporal information from image streams across domains. We thereby distill the best strategy for combining variations in these models along with domain specific face preprocessing techniques through extensive experimentation to obtain state-of-the-art performance on publicly available video-based facial manipulation benchmarks. Specifically, we attempt to detect Deepfake, Face2Face and FaceSwap tampered faces in video streams. Evaluation is performed on the recently introduced FaceForensics++ dataset, improving the previous state-of-the-art by up to 4.55% in accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22255475",
                    "name": "Ekraam Sabir"
                },
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "17806729",
                    "name": "Wael AbdAlmageed"
                },
                {
                    "authorId": "11269472",
                    "name": "I. Masi"
                },
                {
                    "authorId": "145603129",
                    "name": "P. Natarajan"
                }
            ]
        },
        {
            "paperId": "88f2e8e1278b2907b2effa70970b45b447b9267f",
            "title": "A Study of Script Language Effects in Deep Neural-Network-Based Scene Text Detection",
            "abstract": "This study is different from most of the recent text detection work which focuses on creating a robust text detector system. In this work we studied how script languages affect a text detector's performance by using a multi-language synthetic dataset\u2014namely, the Synthetic Octa-Language (SOL) dataset. The effect of script languages continues to be largely unexplored. Previously, this kind of experiment was infeasible because too many factors influence the performance of a text detector. We really cannot tell what role the factor X plays, neither positive nor negative. To overcome these difficulties, we used controlled synthesized data, which allows us to explicitly control factors such as base image, script language, text content, text color, font face, and font size. With the SOL dataset, we were able to investigate the effect that script languages have on on deep neural-network (DNN)-based methods under different scenarios. Moreover, this dataset can be used in other script-language-related text detection research as well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "2110618769",
                    "name": "Achin Gupta"
                },
                {
                    "authorId": "2119299240",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "145603129",
                    "name": "P. Natarajan"
                }
            ]
        },
        {
            "paperId": "92f894ed227e42f4d2c4652330ba7889a028c400",
            "title": "Recurrent-Convolution Approach to DeepFake Detection - State-Of-Art Results on FaceForensics++",
            "abstract": "Spread of misinformation has become a significant problem, raising the importance of relevant detection methods. While there are different manifestations of misinformation, in this work we focus on detecting face manipulations in videos. Specifically, we attempt to detect Deepfake, Face2Face and FaceSwap manipulations in videos. We exploit the temporal dynamics of videos with a recurrent approach. Evaluation is done on FaceForensics++ dataset and our method improves upon the previous state-of-the-art up to 4.55%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22255475",
                    "name": "Ekraam Sabir"
                },
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "17806729",
                    "name": "Wael AbdAlmageed"
                },
                {
                    "authorId": "11269472",
                    "name": "I. Masi"
                },
                {
                    "authorId": "145603129",
                    "name": "P. Natarajan"
                }
            ]
        },
        {
            "paperId": "b57e695304e3d9cda4199eb9251e5091cdf8f522",
            "title": "QATM: Quality-Aware Template Matching for Deep Learning",
            "abstract": "Finding a template in a search image is one of the core problems in many computer vision applications, such as template matching, image semantic alignment, image-to-GPS verification \\etc. In this paper, we propose a novel quality-aware template matching method, which is not only used as a standalone template matching algorithm, but also a trainable layer that can be easily plugged in any deep neural network. Specifically, we assess the quality of a matching pair as its soft-ranking among all matching pairs, and thus different matching scenarios like 1-to-1, 1-to-many, and many-to-many will be all reflected to different values. Our extensive studies in the classic template matching problem and deep learning tasks demonstrate the effectiveness of QATM: it not only outperforms SOTA template matching methods when used alone, but also largely improves existing DNN solutions when used in DNN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33896010",
                    "name": "Jiaxin Cheng"
                },
                {
                    "authorId": "2119299240",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "1404588675",
                    "name": "W. Abd-Almageed"
                },
                {
                    "authorId": "145603129",
                    "name": "P. Natarajan"
                }
            ]
        }
    ]
}