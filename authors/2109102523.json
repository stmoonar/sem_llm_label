{
    "authorId": "2109102523",
    "papers": [
        {
            "paperId": "4d32df1f8fbd2305ebc2dcce9a9d52349f140972",
            "title": "FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator",
            "abstract": "This paper presents FOOCTTS, an automatic pipeline for a football commentator that generates speech with background crowd noise. The application gets the text from the user, applies text pre-processing such as vowelization, followed by the commentator's speech synthesizer. Our pipeline included Arabic automatic speech recognition for data labeling, CTC segmentation, transcription vowelization to match speech, and fine-tuning the TTS. Our system is capable of generating speech with its acoustic environment within limited 15 minutes of football commentator recording. Our prototype is generalizable and can be easily applied to different domains and languages.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1380273855",
                    "name": "Massa Baali"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                }
            ]
        },
        {
            "paperId": "98b2463a9b3312776c84ecc2a2b0c6bb8d351634",
            "title": "QVoice: Arabic Speech Pronunciation Learning Application",
            "abstract": "This paper introduces a novel Arabic pronunciation learning application QVoice, powered with end-to-end mispronunciation detection and feedback generator module. The application is designed to support non-native Arabic speakers in enhancing their pronunciation skills, while also helping native speakers mitigate any potential influence from regional dialects on their Modern Standard Arabic (MSA) pronunciation. QVoice employs various learning cues to aid learners in comprehending meaning, drawing connections with their existing knowledge of English language, and offers detailed feedback for pronunciation correction, along with contextual examples showcasing word usage. The learning cues featured in QVoice encompass a wide range of meaningful information, such as visualizations of phrases/words and their translations, as well as phonetic transcriptions and transliterations. QVoice provides pronunciation feedback at the character level and assesses performance at the word level.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1972477667",
                    "name": "Fouad Khnaisser"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2099539780",
                    "name": "S. Afzal"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                }
            ]
        },
        {
            "paperId": "9f87c8e27a10d71500314e7e21853f5a23efce59",
            "title": "LAraBench: Benchmarking Arabic AI with Large Language Models",
            "abstract": "Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1683403",
                    "name": "Ahmed Abdelali"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2171367840",
                    "name": "Basel Mousi"
                },
                {
                    "authorId": "2466162",
                    "name": "Sabri Boughorbel"
                },
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "2177436744",
                    "name": "Daniel Izham"
                },
                {
                    "authorId": "6415321",
                    "name": "Fahim Dalvi"
                },
                {
                    "authorId": "2762811",
                    "name": "Majd Hawasly"
                },
                {
                    "authorId": "2218353460",
                    "name": "Nizi Nazar"
                },
                {
                    "authorId": "2218145245",
                    "name": "Yousseif Elshahawy"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                },
                {
                    "authorId": "145938140",
                    "name": "Nadir Durrani"
                },
                {
                    "authorId": "1398136050",
                    "name": "Natasa Milic-Frayling"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "c60116a51bf66bc363d11b797d97eba84b13cfd7",
            "title": "LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking",
            "abstract": "The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework, which can be seamlessly customized to evaluate LLMs for any NLP task, regardless of language. The framework features generic dataset loaders, several model providers, and pre-implements most standard evaluation metrics. It supports in-context learning with zero- and few-shot settings. A specific dataset and task can be evaluated for a given LLM in less than 20 lines of code while allowing full flexibility to extend the framework for custom datasets, models, or tasks. The framework has been tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We open-sourced LLMeBench for the community (https://github.com/qcri/LLMeBench/) and a video demonstrating the framework is available online (https://youtu.be/9cC2m_abk3A).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "6415321",
                    "name": "Fahim Dalvi"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2466162",
                    "name": "Sabri Boughorbel"
                },
                {
                    "authorId": "2171367840",
                    "name": "Basel Mousi"
                },
                {
                    "authorId": "2249672254",
                    "name": "Samir Abdaljalil"
                },
                {
                    "authorId": "2218353460",
                    "name": "Nizi Nazar"
                },
                {
                    "authorId": "1683403",
                    "name": "Ahmed Abdelali"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                },
                {
                    "authorId": "2762811",
                    "name": "Majd Hawasly"
                },
                {
                    "authorId": "145938140",
                    "name": "Nadir Durrani"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "30f0718795045de35bd06c4b71da064d30d21898",
            "title": "SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation",
            "abstract": "The lack of labeled second language (L2) speech data is a major challenge in designing mispronunciation detection models. We introduce SpeechBlender - a fine-grained data augmentation pipeline for generating mispronunciation errors to overcome such data scarcity. The SpeechBlender utilizes varieties of masks to target different regions of phonetic units, and use the mixing factors to linearly interpolate raw speech signals while augmenting pronunciation. The masks facilitate smooth blending of the signals, generating more effective samples than the `Cut/Paste' method. Our proposed technique achieves state-of-the-art results, with Speechocean762, on ASR dependent mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson Correlation Coefficient (PCC) compared to the previous state-of-the-art [1]. Additionally, we demonstrate a 5.0% improvement at the phoneme level compared to our baseline. We also observed a 4.6% increase in F1-score with Arabic AraVoiceL2 testset.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2099539780",
                    "name": "S. Afzal"
                }
            ]
        },
        {
            "paperId": "3c00e6cc82b49f046b5f36e5d5f8aa4af68cad5a",
            "title": "Textual Data Augmentation for Arabic-English Code-Switching Speech Recognition",
            "abstract": "The pervasiveness of intra-utterance code-switching (CS) in spoken content requires that speech recognition (ASR) systems handle mixed language. Designing a CS-ASR system has many challenges, mainly due to data scarcity, grammatical structure complexity, and domain mismatch. The most common method for addressing CS is to train an ASR system with the available transcribed CS speech, along with monolingual data. In this work, we propose a zero-shot learning methodology for CS-ASR by augmenting the monolingual data with artificially generating CS text. We based our approach on random lexical replacements and Equivalence Constraint (EC) while exploiting aligned translation pairs to generate random and grammatically valid CS content. Our empirical results show a 65.5% relative reduction in language model perplexity, and 7.7% in ASR WER on two ecologically valid CS test sets. The human evaluation of the generated text using EC suggests that more than 80% is of adequate quality.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "152885017",
                    "name": "A. Hussein"
                },
                {
                    "authorId": "143996866",
                    "name": "S. A. Chowdhury"
                },
                {
                    "authorId": "1683403",
                    "name": "Ahmed Abdelali"
                },
                {
                    "authorId": "3135554",
                    "name": "N. Dehak"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                },
                {
                    "authorId": "2803071",
                    "name": "S. Khudanpur"
                }
            ]
        },
        {
            "paperId": "be5074a85ef8166fc173cb51971a2e3f79685134",
            "title": "Code-Switching Text Augmentation for Multilingual Speech Processing",
            "abstract": "The pervasiveness of intra-utterance Code-switching (CS) in spoken content has enforced ASR systems to handle mixed input. Yet, designing a CS-ASR has many challenges, mainly due to the data scarcity, grammatical structure complexity and mismatch along with unbalanced language usage distribution. Recent ASR studies showed the predominance of E2E-ASR using multilingual data to handle CS phenomena with little CS data. However, the dependency on the CS data still remains. In this work, we propose a methodology to augment the monolingual data for arti\ufb01cially generating spoken CS text to improve different speech modules. We based our approach on Equivalence Constraint theory while exploiting aligned translation pairs, to generate grammatically valid CS content. Our empirical results show a relative gain of 29-34% in perplexity and \u2248 2% in WER for two ecological and noisy CS test sets. Finally, the human evaluation suggests that 83.8% of the generated data is acceptable to humans.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152885017",
                    "name": "A. Hussein"
                },
                {
                    "authorId": "143996866",
                    "name": "S. A. Chowdhury"
                },
                {
                    "authorId": "1683403",
                    "name": "Ahmed Abdelali"
                },
                {
                    "authorId": "3135554",
                    "name": "N. Dehak"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                }
            ]
        },
        {
            "paperId": "e0326f90068d14109a5610c0d1d06f881d0d79e7",
            "title": "Benchmarking Evaluation Metrics for Code-Switching Automatic Speech Recognition",
            "abstract": "Code-switching poses a number of challenges and opportunities for multilingual automatic speech recognition. In this paper, we focus on the question of robust and fair evaluation metrics. To that end, we develop a reference benchmark data set of code-switching speech recognition hypotheses with human judgments. We define clear guidelines for minimal editing of automatic hypotheses. We validate the guidelines using 4-way inter-annotator agreement. We evaluate a large number of metrics in terms of correlation with human judgments. The metrics we consider vary in terms of representation (orthographic, phonological, semantic), directness (intrinsic vs extrinsic), granularity (e.g. word, character), and similarity computation method. The highest correlation to human judgment is achieved using transliteration followed by text normalization. We release the first corpus for human acceptance of code-switching speech recognition results in dialectal Arabic/English conversation speech.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "3248560",
                    "name": "Injy Hamed"
                },
                {
                    "authorId": "152885017",
                    "name": "A. Hussein"
                },
                {
                    "authorId": "2192705666",
                    "name": "Oumnia Chellah"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                },
                {
                    "authorId": "1696645",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                }
            ]
        },
        {
            "paperId": "feff37895d9fb7c8583f8ca28aae79d70fa425b0",
            "title": "Creating Speech-to-Speech Corpus from Dubbed Series",
            "abstract": "Dubbed series are gaining a lot of popularity in recent years with strong support from major media service providers. Such popularity is fueled by studies that showed that dubbed versions of TV shows are more popular than their subtitled equivalents. We propose an unsupervised approach to construct speech-to-speech corpus, aligned on short segment levels, to produce a parallel speech corpus in the source- and target- languages. Our methodology exploits video frames, speech recognition, machine translation, and noisy frames removal algorithms to match segments in both languages. To verify the performance of the proposed method, we apply it on long and short dubbed clips. Out of 36 hours TR-AR dubbed series, our pipeline was able to generate 17 hours of paired segments, which is about 47% of the corpus. We applied our method on another language pair, EN-AR, to ensure it is robust enough and not tuned for a specific language or a specific corpus. Regardless of the language pairs, the accuracy of the paired segments was around 70% when evaluated using human subjective evaluation. The corpus will be freely available for the research community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1380273855",
                    "name": "Massa Baali"
                },
                {
                    "authorId": "1402224224",
                    "name": "W. El-Hajj"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                }
            ]
        },
        {
            "paperId": "6e2a57b46a96694e6dfd3c65796c877dd0094f3d",
            "title": "QASR: QCRI Aljazeera Speech Resource A Large Scale Annotated Arabic Speech Corpus",
            "abstract": "We introduce the largest transcribed Arabic speech corpus, QASR, collected from the broadcast domain. This multi-dialect speech dataset contains 2,000 hours of speech sampled at 16kHz crawled from Aljazeera news channel. The dataset is released with lightly supervised transcriptions, aligned with the audio segments. Unlike previous datasets, QASR contains linguistically motivated segmentation, punctuation, speaker information among others. QASR is suitable for training and evaluating speech recognition systems, acoustics- and/or linguistics- based Arabic dialect identification, punctuation restoration, speaker identification, speaker linking, and potentially other NLP modules for spoken data. In addition to QASR transcription, we release a dataset of 130M words to aid in designing and training a better language model. We show that end-to-end automatic speech recognition trained on QASR reports a competitive word error rate compared to the previous MGB-2 corpus. We report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript. We also report the first baseline for Arabic punctuation restoration. We make the corpus available for the research community.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "152885017",
                    "name": "A. Hussein"
                },
                {
                    "authorId": "143996866",
                    "name": "S. A. Chowdhury"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                }
            ]
        }
    ]
}