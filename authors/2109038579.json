{
    "authorId": "2109038579",
    "papers": [
        {
            "paperId": "e7b6083f8671ff615fae785766770bea8079e959",
            "title": "An Empirical Study of Starting Salaries and Employment Trends of Engineering Students in India",
            "abstract": ": This paper presents an empirical study of a recently compiled workforce analytics data-set modeling employment outcomes of Engineering students. The contributions reported in this paper won the data challenge of the ACM IKDD 2016 Conference on Data Science. Two problems are addressed - regression using heterogeneous information types and the extraction of insights/trends from data to make recommendations; these goals are supported by a range of visualizations. Whereas the data-set is specific to a nation, the underlying techniques and visualization methods are generally applicable. Gaussian processes are proposed to model and predict salary as a function of heterogeneous independent attributes. Key novelties the GP approach brings to the domain of understanding workforce analytics are (a) statistically sound notion of uncertainty of prediction that is data dependent, (b) automatic relevance determination of various independent attributes to the dependent variable (salary),(c) seamless incorporation of both numeric and string attributes within the same regression frame- work without dichotomization; specifically, string attributes include single-word or categorical (e.g. gender) or nominal attributes (e.g. college tier) or multi-word attributes (e.g. specialization) and (d) treatment of all data as being correlated towards making predictions. Insights from both predictive modeling approaches and data analysis were used to suggest factors, that if improved, might lead to better starting salaries for Engineering students. A range of visualization techniques were used to extract key employment patterns from the data. dataset Aspiring Minds (2015) contains background information about engineering candidates and their employment outcomes. A number of parametric, non-parametric and machine learning methods were attempted towards predicting salary for job-candidates, given their background information. This paper reports the use of Gaussian processes to model salaries; it demonstrates the use of kernels as a means of integrating heterogeneous background information types. The technique is new to the domain of workforce analytics. The paper also presents several insightful trends observed from the data and makes recommendations for candidates seeking higher starting salaries. This paper thus views the workforce- analytics / labour market / student salary expectation problem from a data science perspective; it reports on the application of a state-of-the-art probabilistic modeling approach to the problem and trends that are intended to enable the Engineering student population of India.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145500216",
                    "name": "Shrihari Vasudevan"
                },
                {
                    "authorId": "38488764",
                    "name": "Ritwik Chaudhuri"
                },
                {
                    "authorId": "34756776",
                    "name": "M. Pallan"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                }
            ]
        },
        {
            "paperId": "579536a479cdb833932bc9795f46e3b0ef9f271b",
            "title": "Predicting Link Sign in Online Social Networks based on Social Psychology Theory and PageRank",
            "abstract": "Recently, the social networks are evolved as the primary platform for internet users to share their ideas and views. Social media interactions can be modelled as links between two users, where the link weight is different for different types of relationships and interactions. Social networks are dynamic in nature where the links are formed and deformed with time. Broadly, two types of connections, i.e. Positive and Negative can exist in most social networks. Positive connections signify trust and friendship, whereas negative connections show distrust and enmity. Reliable prediction of edge signs can greatly influence in enhancing user experience. Prediction of edge signs has been explored previously also. In this paper, the primary aim is to predict the sign of edges based on extracted features of nodes constructed upon theories of social psychology that includes classical balance theory, the status theory and emotional information theory. The ranking of nodes is used to extract features of nodes that quantify the popularity and trust of nodes on the network. These features are combined with the features extracted from socio psychological theories and try to improve the performance of our sign prediction model. Our results show that the proposed methodology achieves a reasonable performance when implemented on two real-life datasets namely Slashdot and Epinions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2022404080",
                    "name": "Sanjay Kumar"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": null,
                    "name": "Vipul Gupta"
                }
            ]
        },
        {
            "paperId": "1abd1efae8c3849e28de926e52d166b7800965a1",
            "title": "DeepAggregation: A New Approach for Aggregating Incomplete Ranked Lists using Multi-Layer Graph Embedding",
            "abstract": "Preference aggregation, and specifically rank aggregation, is a well known problem in the fields of computational social choice and preference handling with broad application including web search and recommendation systems. Inspired by the recent advances in the area of deep neural representation learning, for the first time in the literature, in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation. It takes as input a set of incomplete rank lists and constructs a multi-layer graph wherein the nodes are the alternatives that are ranked and the edges capture information contained in the incomplete rank lists. We then compute deep neural representation vectors (i.e. embeddings) for the nodes and then derive the aggregated order using these representation vectors. Our proposed algorithm can handle incomplete rank lists with or without ties. We conduct thorough empirical analysis of the proposed DeepAggregation algorithm using various real life data sets such as TripAdvisor reviews data. We empirically observe that DeepAggregation generates impressive results in comparison with a number of well-known state-of-the-art preference aggregation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2637492",
                    "name": "R. D. Vallam"
                },
                {
                    "authorId": "3196118",
                    "name": "Ramasuri Narayanam"
                },
                {
                    "authorId": "144063173",
                    "name": "Srikanth G. Tamilselvam"
                },
                {
                    "authorId": "143999398",
                    "name": "Nicholas Mattei"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "30092b1f5313a6424194691792eccfaee4bb2958",
            "title": "Predicting Link Sign in Online Social Networks based on Social Psychology Theory and Machine Learning Techniques",
            "abstract": "Online social networks provide a great platform for internet users to share their views and ideas. Social media provides a dynamic platform that includes the formation and deformation of connections. Two types of connections, i.e., Positive and Negative, can exist in a social network. Positive connections are a sign of friendship or trust, while negative connections show enmity or distrust. Various applications in several fields have networks containing both positive and negative edges. Reliable prediction of edge sign can greatly influence in recommending friendly relationships while preventing enemy relationships across the network. Prediction of edge signs has been explored previously also. However, we intend to predict the sign of edges based on extracted features of nodes constructed upon theories of social psychology that includes classical balance theory and the status theory. Moreover, we employ emotional information theory and use the combined extracted features from all the theories to analyze networks for better prediction. Our results show that the proposed methodology has obtained significant accuracy when implemented on two real-life datasets, namely Slashdot and Epinions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2022404080",
                    "name": "Sanjay Kumar"
                },
                {
                    "authorId": "35174037",
                    "name": "Rohit Beniwal"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": null,
                    "name": "Vipul Gupta"
                }
            ]
        },
        {
            "paperId": "7ddfb35c329b5f4cb1be5bde0c56b11e3f81988a",
            "title": "Collaborative Reinforcement Learning Model for Sustainability of Cooperation in Sequential Social Dilemmas",
            "abstract": "Learning the emergence of cooperation in conflicting scenarios such as social dilemmas is a centerpiece of research. Many reinforcement learning based theories exist in the literature to address this problem. The well-known fact about RL based model's very slow learning capabilities coupled with large state space exhibit significant negative effects especially in repeated version of social dilemma settings such as repeated Public Goods Game (PGG) and thereby making them ineffective to model sustainability of cooperation. In this paper, we address this research challenge by augmenting the reinforcement learning based models with a notion of collaboration among the agents, motivated by the fact that humans learn not only through their own actions but also by following the actions of other agents who also continuously learn about the environment. In particular, we propose a novel model, which we refer to as Collaborative Reinforcement Learning (CRL), wherein we define collaboration among the agents as the ability of agents to fully follow other agent's actions/decisions. This is also termed as social learning. The proposed CRL model significantly influences the speed of individual learning, which eventually has a large effect on the collective behavior as compared to that of RL only models and thereby effectively explaining the sustainability of cooperation in repeated PGG settings. We also extend the CRL model for PGGs over different generations where agents die out and new agents are born following a birth-death process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38488764",
                    "name": "Ritwik Chaudhuri"
                },
                {
                    "authorId": "2620644",
                    "name": "K. Mukherjee"
                },
                {
                    "authorId": "3196118",
                    "name": "Ramasuri Narayanam"
                },
                {
                    "authorId": "2637492",
                    "name": "R. D. Vallam"
                },
                {
                    "authorId": "2109263513",
                    "name": "Ayush Kumar"
                },
                {
                    "authorId": "123831016",
                    "name": "Antriksh Mathur"
                },
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "13021322e15cbc3c747b57d0ec6d442eaa104e95",
            "title": "CVBed: Structuring CVs usingWord Embeddings",
            "abstract": "Automatic analysis of curriculum vitae (CVs) of applicants is of tremendous importance in recruitment scenarios. The semi-structuredness of CVs, however, makes CV processing a challenging task. We propose a solution towards transforming CVs to follow a unified structure, thereby, paving ways for smoother CV analysis. The problem of restructuring is posed as a section relabeling problem, where each section of a given CV gets reassigned to a predefined label. Our relabeling method relies on semantic relatedness computed between section header, content and labels, based on phrase-embeddings learned from a large pool of CVs. We follow different heuristics to measure semantic relatedness. Our best heuristic achieves an F-score of 93.17% on a test dataset with gold-standard labels obtained using manual annotation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "1746093",
                    "name": "Abhijit Mishra"
                },
                {
                    "authorId": "144710196",
                    "name": "K. Dey"
                }
            ]
        },
        {
            "paperId": "270046171591fe3bf17fbe2e3a59baa27ebf9bc9",
            "title": "Outbrain Click Prediction",
            "abstract": "In this paper, we explore various data manipulation and machine learning techniques to build an advertisement recommendation engine that prioritizes content to be presented to users. Companies like Outbrain have made it their mission to deliver quality content to their users and provide a platform for advertisers to reach their target audiences. Using Outbrain\u2019s click and user profile information, we manicured a data set using techniques such as binning and normalization. This data was used to train a logistic regression model and a random forest classifier to rank a set of ads on a given page in order of decreasing likelihood. We scored these classifiers using a mean average precision at 12 metric. In the end, we found that random forest performed the best and coupled really well with the binning technique used. Introduction In modern society, the advent of technology has revolutionized the way people communicate and retrieve information, starting an era of constant information consumption. Mobile devices \u2013 laptops, tablets, and cell phones \u2013 are ubiquitous, providing a large scale of information to the public, such as technology, sports, weather, and international news. Due to the increasingly large amount of data that could be accessed, it is crucial to prioritize the content to present to users. Presenting optimal news that interest individual users, resulting in a higher likelihood of being clicked, improves user engagement and experience. The mission of Outbrain, a leading publishing and marketing platform, is to enrich the consumer with engaging content by building an advertisement recommendation engine. Machine learning algorithms could be used to predict users\u2019 behaviors and display pieces of content based their previous selections and features, ultimately providing a more personalized user experience. To accomplish this task, Outbrain provides a large relational data (exceeding 100GB or 2 billion examples), providing a sample of users\u2019 page views and clicks observed across multiple publisher sites, platforms (desktop, mobile, tablet), and geographic locations between 06/14/2016 and 06/28/2016. The input to our algorithm is a set of key features that characterize the user, documents (originally viewed and promoted content), and the page view event (as shown in Table I). Most features were given numerical identifications, which is inappropriate for categorical features (i.e. platform: 1 = desktop, 2 = mobile, and 3 = tablet). These categorical features were one-hot encoded to properly treat them as categorical values rather than numerical values. Given a set of advertisements per document, we used logistic regression, support vector machines, and random forest algorithms to output an ordered list of advertisements in decreasing likelihood of being clicked for each document. Machine Learning Pipeline As illustrated in Fig.1., we used the Amazon Web Services (AWS) Redshift which uses Massive Parallel Processing to manage and query the large dataset, and Apache Spark on Microsoft Azure and Google Cloud platforms to train our models using distributed machine learning algorithms over the cloud. Initially, we were using a local computer and immediately realized the large computational power the task required. Given our time and resource constraint, it was required to setup this pipeline to process and iterate over this large dataset. Dataset and Features The Outbrain dataset provided a total of eight datasets: \uf0b7 Page views describes features of all viewed pages, regardless of an advertisement being clicked. \uf0b7 Events consists of features of pages viewed when one displayed advertisement was clicked. \uf0b7 Promoted content provides advertisement features. \uf0b7 Clicks train/test provides examples with labels to be used for training and examples without labels to be used for the Outbrain competition. Figure 1. Machine learning pipeline for click prediction \uf0b7 Documents meta describes documents\u2019 metadata. \uf0b7 Documents entities, documents topic, and documents categories provide mentioned entities (person, place, or location), topic, and taxonomy of categories of the documents, respectively. According to [1], most data preprocessing take up to 80% to complete real-world data mining projects, especially those with high-cardinality categorical data fields such as this project. One of the main challenges was building the training and test sets for our models. Out of the 2 billion examples provided, we decided to exclude examples found in Page Views and only consider the 87 million examples contained in Events. These examples of page views that resulted in a click for one of the featured advertisements contain useful information to make click predictions. By using these examples, the first advertisement in the ordered list (output) represents the advertisement that we predict to be clicked for a particular document. In addition, Documents entities because it was too distinct, not providing much information. At one instance, training with entity as a feature prevented an algorithm from converging. Hence, features unique to Page Views (traffic source) and Documents entities (entity id and confidence level) were ignored. The remaining datasets could be mapped to each other using certain features as a key as illustrated in Fig.2. TABLE I. Features provided by Outbrain (n = 19) Bolded = features used in our click prediction Using AWS Redshift, we discovered that unique user id (uuid) were mostly distinct, indicating that observations were rarely made on the same user. Thus, it was impractical to use uuid as a feature. Also, display id was also not included in our feature because it is unique to each page view event. Display id represents a particular session of users viewing a document and allows us to group advertisements and features involved in the same event. However, it is not useful to include as a feature by itself. As a result, Fig.2 displays the dataset and features used for our prediction. Figure 2. Datasets used for click prediction Features used as keys are color-coded.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "103438187",
                    "name": "Julien Hoachuck"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "2071874735",
                    "name": "Lisa Yamada"
                }
            ]
        },
        {
            "paperId": "27bf66b7915e4664239f0943f61ad5fca1836002",
            "title": "Outplacement time and probability estimation using discrete event simulation",
            "abstract": "In today's rapidly changing technological scenario, tech giants revise their strategic alignment every couple of years. As a result, their workforce has to be adapted to the organization's strategy. Members of the workforce who are neither relevant to the strategic alignment, nor can be made relevant by reskilling, have to be either outplaced (i.e. placed in an another job within organization) or separated from the organization. In geographies like Europe, where the cost of separation is very high, it becomes very important to make the right decision for each employee. In this paper, we describe a simulation based methodology to find the probability and time of outplacement of an employee. These numbers are inputs to a global problem of making the optimal decision for the entire workforce.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "1999599",
                    "name": "R. Pimplikar"
                },
                {
                    "authorId": "38488764",
                    "name": "Ritwik Chaudhuri"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "944412e60a607f8905bb7284f9601cc151e5fe64",
            "title": "Near-Optimal Nonmyopic Contact Center Planning Using Dual Decomposition",
            "abstract": "\n \n We address the problem of minimizing staffing cost in a contact center subject to service level requirements over multiple weeks. We handle both the capacity planning and agent schedule generation aspect of this problem. Our work incorporates two unique business requirements. First, we develop techniques that can provide near-optimal staffing for 247 contact centers over long term, upto eight weeks, rather than planning myopically on a week-on-week basis. Second, our approach is usable in an online interactive setting in which staffing managers using our system expect high quality plans within a short time period. Results on large real world and synthetic instances show that our Lagrangian relaxation based technique can achieve a solution within 94% of optimal on an average, for eight week problems within ten minutes, whereas a generic integer programming solver can only achieve a solution within 80% of optimal. Our approach is also deployed in live business environment and reduces headcount by a decile over techniques used previously by our client business units.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40305195",
                    "name": "Akshat Kumar"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "2130387885",
                    "name": "Pranav Gupta"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "e8b098187a0b6d2885dd6383ed641bef28c9d854",
            "title": "Analyses for Service Interaction Networks with Applications to Service Delivery",
            "abstract": "One of the distinguishing features of the services industry is the high emphasis on people interacting with other people and serving customers rather than transforming physical goods like in the traditional manufacturing processes. It is evident that analysis of such interactions is an essential aspect of designing effective and efficient services delivery. In this work we focus on learning individual and team behavior of different people or agents of a service organization by studying the patterns and outcomes of historical interactions. For each past interaction, we assume that only the list of participants and an outcome indicating the overall effectiveness of the interaction are known. Note that this offers limited information on the mutual (pairwise) compatibility of different participants. We develop the notion of service interaction networks which is an abstraction of the historical data and allows one to cast practical problems in a formal setting. We identify the unique characteristics of analyzing service interaction networks when compared to traditional analyses considered in social network analysis and establish a need for new modeling and algorithmic techniques for such networks. On the algorithmic front, we develop new algorithms to infer attributes of agents individually and in team settings. Our first algorithm is based on a novel modification to the eigen-vector based centrality for ranking the agents and the second algorithm is an iterative update technique that can be applied for subsets of agents as well. One of the challenges of conducting research in this setting is the sensitive and proprietary nature of the data. Therefore, there is a need for a realistic simulator for studying service interaction networks. We present the initial version of our simulator that is geared to capture several characteristics of service interaction networks that arise in real-life.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738048",
                    "name": "S. Kameshwaran"
                },
                {
                    "authorId": "145640180",
                    "name": "S. Mehta"
                },
                {
                    "authorId": "143691844",
                    "name": "Vinayaka Pandit"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "1800833",
                    "name": "N. Viswanadham"
                }
            ]
        }
    ]
}