{
    "authorId": "144171096",
    "papers": [
        {
            "paperId": "0086d4e5b8a16cb1f1fb7c050495d22f8ce70764",
            "title": "Hybrid Structured and Similarity Queries over Wikidata plus Embeddings with Kypher-V",
            "abstract": "Augmenting large knowledge graphs such as Wikidata with unstructured sources represented by embedding vectors is important for a number of applications, such as similarity-based search, recommendation systems, question answering, entity linking, etc. However, building such hybrid systems today requires ad hoc combinations of disparate technologies such as structured query languages with approximate nearest neighbor search, since there are no truly hybrid query systems available aimed at simultaneously querying large knowledge graphs and vector data. Moreover, similarity search over large vector data is very resource and main-memory intensive requiring high-end compute servers with large amounts of RAM. This paper introduces Kypher-V , an extension to KGTK\u2019s Kypher query language and processor, that allows users to augment Wikidata and other KGs with large-scale vector data and efficiently query it through a hybrid query system using a structured query language based on Cypher. We show that Kypher-V can efficiently store, index and query 40M 1024-d vectors and effectively filter and join results with Wikidata-size datasets on a laptop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284176",
                    "name": "Hans Chalupsky"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                }
            ]
        },
        {
            "paperId": "04220fc47381875f6aba4f5d80adaef7e23eee16",
            "title": "Evaluating Machine Common Sense via Cloze Testing",
            "abstract": "Language models (LMs) show state of the art performance for common sense (CS) question answering, but whether this ability implies a human-level mastery of CS remains an open question. Understanding the limitations and strengths of LMs can help researchers improve these models, potentially by developing novel ways of integrating external CS knowledge. We devise a series of tests and measurements to systematically quantify their performance on different aspects of CS. We propose the use of cloze testing combined with word embeddings to measure the LM's robustness and confidence. Our results show than although language models tend to achieve human-like accuracy, their confidence is subpar. Future work can leverage this information to build more complex systems, such as an ensemble of symbolic and distributed knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064322437",
                    "name": "Ehsan Qasemi"
                },
                {
                    "authorId": "51132560",
                    "name": "Lee Kezar"
                },
                {
                    "authorId": "2634786",
                    "name": "J. Pujara"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                }
            ]
        },
        {
            "paperId": "087aa21e0969bcdfb3454dbecc489fa1f8ec2521",
            "title": "Evaluating the Feasibility of a Provably Secure Privacy-Preserving Entity Resolution Adaptation of PPJoin using Homomorphic Encryption",
            "abstract": "Entity resolution is the task of disambiguating records that refer to the same entity in the real world. In this work, we explore adapting one of the most efficient and accurate Jaccard-based entity resolution algorithms - PPJoin, to the private domain via homomorphic encryption. Towards this, we present our precise adaptation of PPJoin (HE-PPJoin) that details certain subtle data structure modifications and algorithmic additions needed for correctness and privacy. We implement HE-PPJoin by extending the PALISADE homomorphic encryption library and evaluate over it for accuracy and incurred overhead. Furthermore, we directly compare HE-PPJoin against P4Join, an existing privacy-preserving variant of PPJoin which uses fingerprinting for raw content obfuscation, by demonstrating a rigorous analysis of the efficiency, accuracy, and privacy properties achieved by our adaptation as well as a characterization of those same attributes in P4Join.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122929137",
                    "name": "Tanmay Ghai"
                },
                {
                    "authorId": "27039165",
                    "name": "Yixiang Yao"
                },
                {
                    "authorId": "40020483",
                    "name": "Srivatsan Ravi"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                }
            ]
        },
        {
            "paperId": "2d471a31d55ab1317167e4d9df270ca02d3749bc",
            "title": "Enriching Wikidata with Linked Open Data",
            "abstract": "Large public knowledge graphs, like Wikidata, contain billions of statements about tens of millions of entities, thus inspiring various use cases to exploit such knowledge graphs. However, practice shows that much of the relevant information that fits users' needs is still missing in Wikidata, while current linked open data (LOD) tools are not suitable to enrich large graphs like Wikidata. In this paper, we investigate the potential of enriching Wikidata with structured data sources from the LOD cloud. We present a novel workflow that includes gap detection, source selection, schema alignment, and semantic validation. We evaluate our enrichment method with two complementary LOD sources: a noisy source with broad coverage, DBpedia, and a manually curated source with a narrow focus on the art domain, Getty. Our experiments show that our workflow can enrich Wikidata with millions of novel statements from external LOD sources with high quality. Property alignment and data quality are key challenges, whereas entity alignment and source selection are well-supported by existing Wikidata mechanisms. We make our code and data available to support future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2141918632",
                    "name": "Bohui Zhang"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                }
            ]
        },
        {
            "paperId": "387a916fa4927a5998cbef2e0b7e1f5e6a2e9c3e",
            "title": "Augmenting Knowledge Graphs for Better Link Prediction",
            "abstract": "Embedding methods have demonstrated robust performance on the task of link prediction in knowledge graphs, by mostly encoding entity relationships. Recent methods propose to enhance the loss function with a literal-aware term. In this paper, we propose KGA: a knowledge graph augmentation method that incorporates literals in an embedding model without modifying its loss function. KGA discretizes quantity and year values into bins, and chains these bins both horizontally, modeling neighboring values, and vertically, modeling multiple levels of granularity. KGA is scalable and can be used as a pre-processing step for any existing knowledge graph embedding model. Experiments on legacy benchmarks and a new large benchmark, DWD, show that augmenting the knowledge graph with quantities and years is beneficial for predicting both entities and numbers, as KGA outperforms the vanilla models and other relevant baselines. Our ablation studies confirm that both quantities and years contribute to KGA's performance, and that its performance depends on the discretization and binning settings. We make the code, models, and the DWD benchmark publicly available to facilitate reproducibility and future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2141735343",
                    "name": "Jiang Wang"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "39601186",
                    "name": "Ke-Thia Yao"
                }
            ]
        },
        {
            "paperId": "47b23ebaeb60184569c22aa484167bc32f914fe3",
            "title": "Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning",
            "abstract": "Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47939052",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "1491852518",
                    "name": "Zhewei Xu"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "5abf79226a8ffa2dcf33ca98bfc36c8bb01cc4fc",
            "title": "Multilayer graphs: a unified data model for graph databases",
            "abstract": "In this short position paper, we argue that there is a need for a unifying data model that can support popular graph formats such as RDF, RDF* and property graphs, while at the same time being powerful enough to naturally store information from complex knowledge graphs, such as Wikidata, without the need for a complex reification scheme. Our proposal, called the multilayer graph model, presents a simple and flexible data model for graphs that can naturally support all of the above, and more. We also observe that the idea of multilayer graphs has appeared in existing graph systems from different vendors and research groups, illustrating its versatility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2772109",
                    "name": "Renzo Angles"
                },
                {
                    "authorId": "144007515",
                    "name": "Aidan Hogan"
                },
                {
                    "authorId": "35003282",
                    "name": "O. Lassila"
                },
                {
                    "authorId": "2059572334",
                    "name": "Carlos Rojas"
                },
                {
                    "authorId": "145479025",
                    "name": "D. Schwabe"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "2434366",
                    "name": "D. Vrgo\u010d"
                }
            ]
        },
        {
            "paperId": "67f05393b58835d7a100603c1b033f8f0dce5b16",
            "title": "Identifying Surprising Facts in Wikidata",
            "abstract": "To expand their boundary of knowledge, to solve a certain task, or merely to entertain themselves, people on the Web often hunt for surprising facts. While large repositories of structured knowledge like Wikidata hold the promise to provide surprising facts to their users and developers, currently no mechanism exists to identify surprising facts in Wikidata. In this paper, we study the ability of popular embedding models to estimate the surprise level of a fact in Wikidata. We formulate a novel task of Surprising Fact Identification , and we create two associated benchmarks: Trivia and Survey . We propose two scalable methods based on outlier detection and link prediction to estimate surprise scores for any statement in a graph like Wikidata. We evaluate our methods with various embedding models on the two benchmarks. We perform further analysis of the predictions for outlier and non-outlier facts to investigate to what extent link prediction models regress to the mean.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2141760556",
                    "name": "Nicholas Klein"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "1704838537",
                    "name": "Hayden Freedman"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                }
            ]
        },
        {
            "paperId": "0f21f87dc9460356627d6ac7c53cc92d19233d4d",
            "title": "User-friendly Comparison of Similarity Algorithms on Wikidata",
            "abstract": "While the similarity between two concept words has been evaluated and studied for decades, much less attention has been devoted to algorithms that can compute the similarity of nodes in very large knowledge graphs, like Wikidata. To facilitate investigations and head-to-head comparisons of similarity algorithms on Wikidata, we present a user-friendly interface that allows flexible computation of similarity between Qnodes in Wikidata. At present, the similarity interface supports four algorithms, based on: graph embeddings (TransE, ComplEx), text embeddings (BERT), and class-based similarity. We demonstrate the behavior of the algorithms on representative examples about semantically similar, related, and entirely unrelated entity pairs. To support anticipated applications that require efficient similarity computations, like entity linking and recommendation, we also provide a REST API that can compute most similar neighbors for any Qnode in Wikidata.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2512264",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "1840215",
                    "name": "Gleb Satyukov"
                },
                {
                    "authorId": "2116288277",
                    "name": "Amandeep Singh"
                }
            ]
        },
        {
            "paperId": "221ce6b97e4128ea3af592c885239367e48df095",
            "title": "Retrieving Complex Tables with Multi-Granular Graph Representation Learning",
            "abstract": "The task of natural language table retrieval (NLTR) seeks to retrieve semantically relevant tables based on natural language queries. Existing learning systems for this task often treat tables as plain text based on the assumption that tables are structured as dataframes. However, tables can have complex layouts which indicate diverse dependencies between subtable structures, such as nested headers. As a result, queries may refer to different spans of relevant content that is distributed across these structures. Moreover, such systems fail to generalize to novel scenarios beyond those seen in the training set. Prior methods are still distant from a generalizable solution to the NLTR problem, as they fall short in handling complex table layouts or queries over multiple granularities. To address these issues, we propose Graph-based Table Retrieval (GTR), a generalizable NLTR framework with multi-granular graph representation learning. In our framework, a table is first converted into a tabular graph, with cell nodes, row nodes and column nodes to capture content at different granularities. Then the tabular graph is input to a Graph Transformer model that can capture both table cell content and the layout structures. To enhance the robustness and generalizability of the model, we further incorporate a self-supervised pre-training task based on graph-context matching. Experimental results on two benchmarks show that our method leads to significant improvements over the current state-of-the-art systems. Further experiments demonstrate promising performance of our method on cross-dataset generalization, and enhanced capability of handling complex tables and fulfilling diverse query intents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47939052",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "35329068",
                    "name": "Kexuan Sun"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                },
                {
                    "authorId": "2634786",
                    "name": "J. Pujara"
                },
                {
                    "authorId": "144171096",
                    "name": "Pedro A. Szekely"
                }
            ]
        }
    ]
}