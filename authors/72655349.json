{
    "authorId": "72655349",
    "papers": [
        {
            "paperId": "5982c074413f0a73b4b72db6f01cd6b9b909a6d0",
            "title": "Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries",
            "abstract": "Summarizing clinical text is crucial in health decision-support and clinical research. Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health. Holistically evaluating text summaries is challenging because they may contain unsubstantiated information. Here, we explore a general mitigation framework using Attribute Structuring (AS), which structures the summary evaluation process. It decomposes the evaluation process into a grounded procedure that uses an LLM for relatively simple structuring and scoring tasks, rather than the full task of holistic summary evaluation. Experiments show that AS consistently improves the correspondence between human annotations and automated metrics in clinical text summarization. Additionally, AS yields interpretations in the form of a short text span corresponding to each output, which enables efficient human auditing, paving the way towards trustworthy evaluation of clinical information in resource-constrained scenarios. We release our code, prompts, and an open-source benchmark at https://github.com/microsoft/attribute-structuring.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1395101702",
                    "name": "Zelalem Gero"
                },
                {
                    "authorId": "2261286637",
                    "name": "Chandan Singh"
                },
                {
                    "authorId": "2267021210",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "ab88cdefc888fb102b91140bd6e6f8eafef3d135",
            "title": "Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation",
            "abstract": "The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world clinics. Frontier general-domain models such as GPT-4V still have significant performance gaps in multimodal biomedical applications. More importantly, less-acknowledged pragmatic issues, including accessibility, model cost, and tedious manual evaluation make it hard for clinicians to use state-of-the-art large models directly on private patient data. Here, we explore training open-source small multimodal models (SMMs) to bridge competency gaps for unmet clinical needs in radiology. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space, as exemplified by LLaVA-Med. For training, we assemble a large dataset of over 697 thousand radiology image-text pairs. For evaluation, we propose CheXprompt, a GPT-4-based metric for factuality evaluation, and demonstrate its parity with expert evaluation. For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training. The resulting LlaVA-Rad (7B) model attains state-of-the-art results on standard radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B). The inference of LlaVA-Rad is fast and can be performed on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2291065818",
                    "name": "Juan Manuel Zambrano Chaves"
                },
                {
                    "authorId": "2267360148",
                    "name": "Shih-Cheng Huang"
                },
                {
                    "authorId": "2279740020",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2263936549",
                    "name": "Hanwen Xu"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2291048834",
                    "name": "Yujia Xie"
                },
                {
                    "authorId": "2268760479",
                    "name": "Mahmoud Khademi"
                },
                {
                    "authorId": "2291073936",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "3032929",
                    "name": "H. Awadalla"
                },
                {
                    "authorId": "2268383237",
                    "name": "Julia Gong"
                },
                {
                    "authorId": "2291084567",
                    "name": "Houdong Hu"
                },
                {
                    "authorId": "2279705714",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2244470181",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2260374573",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                },
                {
                    "authorId": "2188270295",
                    "name": "M. Lungren"
                },
                {
                    "authorId": "2275638250",
                    "name": "Serena Yeung-Levy"
                },
                {
                    "authorId": "2249123829",
                    "name": "Curtis P. Langlotz"
                },
                {
                    "authorId": "2261294491",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "fe8527541afd578326a51b885aef82da6bb32e95",
            "title": "T-Rex: Text-assisted Retrosynthesis Prediction",
            "abstract": "As a fundamental task in computational chemistry, retrosynthesis prediction aims to identify a set of reactants to synthesize a target molecule. Existing template-free approaches only consider the graph structures of the target molecule, which often cannot generalize well to rare reaction types and large molecules. Here, we propose T-Rex, a text-assisted retrosynthesis prediction approach that exploits pre-trained text language models, such as ChatGPT, to assist the generation of reactants. T-Rex first exploits ChatGPT to generate a description for the target molecule and rank candidate reaction centers based both the description and the molecular graph. It then re-ranks these candidates by querying the descriptions for each reactants and examines which group of reactants can best synthesize the target molecule. We observed that T-Rex substantially outperformed graph-based state-of-the-art approaches on two datasets, indicating the effectiveness of considering text information. We further found that T-Rex outperformed the variant that only use ChatGPT-based description without the re-ranking step, demonstrate how our framework outperformed a straightforward integration of ChatGPT and graph information. Collectively, we show that text generated by pre-trained language models can substantially improve retrosynthesis prediction, opening up new avenues for exploiting ChatGPT to advance computational chemistry. And the codes can be found at https://github.com/lauyikfung/T-Rex.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2264342410",
                    "name": "Yifeng Liu"
                },
                {
                    "authorId": "2263936549",
                    "name": "Hanwen Xu"
                },
                {
                    "authorId": "2264489734",
                    "name": "Tangqi Fang"
                },
                {
                    "authorId": "2281644064",
                    "name": "Haocheng Xi"
                },
                {
                    "authorId": "2265222599",
                    "name": "Zixuan Liu"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "2257325568",
                    "name": "Sheng Wang"
                }
            ]
        },
        {
            "paperId": "0413a92b91e2dbcbfaa042d0519f3d919bfe57e7",
            "title": "Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology",
            "abstract": "Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2112677245",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "7016395",
                    "name": "C. Moung"
                },
                {
                    "authorId": "2228760409",
                    "name": "Jacob Abel"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "6150927",
                    "name": "R. Weerasinghe"
                },
                {
                    "authorId": "35695043",
                    "name": "B. Piening"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "46392172",
                    "name": "C. Bifulco"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "2d9310132cfe9046f4b61f6e90a5ef92c6e0ba71",
            "title": "BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys",
            "abstract": "Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion model for counterfactual biomedical image generation. Given the relative scarcity of image time series data, we introduce a two-stage curriculum that first pretrains the denoising network using the much more abundant single image-report pairs (with dummy prior image), and then continues training using the counterfactual triples. Experiments using the standard MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive battery of tests on counterfactual medical image generation, BiomedJourney substantially outperforms prior state-of-the-art methods in instruction image editing and medical image generation such as InstructPix2Pix and RoentGen. To facilitate future study in counterfactual medical generation, we plan to release our instruction-learning code and pretrained models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260374573",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "120157163",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "2109738542",
                    "name": "Chun-yue Li"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "4204731",
                    "name": "M. Lungren"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "44caf26949b4799e21f7b0754fe61315e8b71542",
            "title": "Compositional Zero-Shot Domain Transfer with Text-to-Text Models",
            "abstract": "Abstract Label scarcity is a bottleneck for improving task performance in specialized domains. We propose a novel compositional transfer learning framework (DoT51) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from masked language modelling of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: We simultaneously train natural language generation (NLG) for in-domain label-to-data generation, which enables data augmentation for self-finetuning and natural language understanding (NLU) for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on natural language inference, text summarization, and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current state-of-the-art in zero-shot transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with ablations and a case study demonstrating its ability to solve challenging NLI examples requiring in-domain expertise.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144097210",
                    "name": "Fangyu Liu"
                },
                {
                    "authorId": "50383712",
                    "name": "Qianchu Liu"
                },
                {
                    "authorId": "82303325",
                    "name": "Shruthi Bannur"
                },
                {
                    "authorId": "1398728014",
                    "name": "Fernando P\u00e9rez-Garc\u00eda"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "2064953709",
                    "name": "A. Nori"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "2029689505",
                    "name": "Javier Alvarez-Valle"
                },
                {
                    "authorId": "2941969",
                    "name": "O. Oktay"
                },
                {
                    "authorId": "34918424",
                    "name": "Stephanie L. Hyland"
                }
            ]
        },
        {
            "paperId": "5814bd146b37e13115af4330caf3a751159a156f",
            "title": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs",
            "abstract": "Biomedical data is inherently multimodal, comprising physical measurements and natural language narratives. A generalist biomedical AI model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image-text pairs. Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image-text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision-language processing. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering (VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of standard datasets, substantially outperforming prior approaches. Intriguingly, by large-scale pretraining on diverse biomedical image types, BiomedCLIP even outperforms state-of-the-art radiology-specific models such as BioViL in radiology-specific tasks such as RSNA pneumonia detection. In summary, BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications. We release our models at https://aka.ms/biomedclip to facilitate future research in multimodal biomedical AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2143558044",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "6157638",
                    "name": "J. Bagga"
                },
                {
                    "authorId": "1846722967",
                    "name": "Robert Tinn"
                },
                {
                    "authorId": "2159543147",
                    "name": "Sam Preston"
                },
                {
                    "authorId": "35794791",
                    "name": "Rajesh N. Rao"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "48269128",
                    "name": "Naveen Valluri"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "4204731",
                    "name": "M. Lungren"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "6c12769939dd75bd681d37ea17cce7e6a57f5c6e",
            "title": "Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events",
            "abstract": "Large language models (LLMs), such as GPT-4, have demonstrated remarkable capabilities across a wide range of tasks, including health applications. In this paper, we study how LLMs can be used to scale biomedical knowledge curation. We find that while LLMs already possess decent competency in structuring biomedical text, by distillation into a task-specific student model through self-supervised learning, substantial gains can be attained over out-of-box LLMs, with additional advantages such as cost, efficiency, and white-box model access. We conduct a case study on adverse drug event (ADE) extraction, which is an important area for improving care. On standard ADE extraction evaluation, a GPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised state-of-the-art models without using any labeled data. Despite being over 1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by over 6 absolute points in F1 and GPT-4 by over 5 absolute points. Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT) and ADE extraction architecture shed light on best practice for biomedical knowledge extraction. Similar gains were attained by distillation for other standard biomedical knowledge extraction tasks such as gene-disease associations and protected health information, further illustrating the promise of this approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112677245",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "3310870",
                    "name": "Yonas G. Woldesenbet"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2223542076",
                    "name": "Praneeth Sanapathi"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "48269128",
                    "name": "Naveen Valluri"
                },
                {
                    "authorId": "35009788",
                    "name": "Erika Strandberg"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "76b99439d524ae1813c30edc4bcad487a30a1f8c",
            "title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
            "abstract": "Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2203076",
                    "name": "Wenxuan Zhou"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2112677245",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "91664510519b562d17ad5e3cde3a282b92a77fac",
            "title": "DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation",
            "abstract": "Medical text generation aims to assist with administrative work and highlight salient information to support decision-making. To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness, conciseness, and attribution of the generated text at a fine-grained level. The metrics can be computed by various types of evaluators including instruction-following (both proprietary and open-source) and supervised entailment models. We demonstrate the effectiveness of the resulting framework, DocLens, with three evaluators on three tasks: clinical note generation, radiology report summarization, and patient question summarization. A comprehensive human study shows that DocLens exhibits substantially higher agreement with the judgments of medical experts than existing metrics. The results also highlight the need to improve open-source evaluators and suggest potential directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267021210",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "1395101702",
                    "name": "Zelalem Gero"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        }
    ]
}