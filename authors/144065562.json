{
    "authorId": "144065562",
    "papers": [
        {
            "paperId": "802ad59ad0dfe4c49015dc3903886a354eef341e",
            "title": "You are a Bot! - Studying the Development of Bot Accusations on Twitter",
            "abstract": "The characterization and detection of bots with their presumed ability to manipulate society on social media platforms have been subject to many research endeavors over the last decade. In the absence of ground truth data (i.e., accounts that are labeled as bots by experts or self-declare their automated nature), researchers interested in the characterization and detection of bots may want to tap into the wisdom of the crowd. But how many people need to accuse another user as a bot before we can assume that the account is most likely automated? And more importantly, are bot accusations on social media at all a valid signal for the detection of bots? \n\nOur research presents the first large-scale study of bot accusations on Twitter and shows how the term bot became an instrument of dehumanization in social media conversations since it is predominantly used to deny the humanness of conversation partners. Consequently, bot accusations on social media should not be naively used as a signal to train or test bot detection models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12054937",
                    "name": "Dennis Assenmacher"
                },
                {
                    "authorId": "2087432433",
                    "name": "Leon Fr\u00f6hling"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                }
            ]
        },
        {
            "paperId": "ea5ffe9d45dc8120e537122ddf01d3cc7ef4ab5d",
            "title": "ValiTex - a unified validation framework for computational text-based measures of social science constructs",
            "abstract": ": Guidance on how to validate computational text-based measures of social science constructs is fragmented. Although scholars generally acknowledge the importance of validating their text-based measures, they often lack common terminology and a unified framework to do so. This paper introduces ValiTex, a new validation framework designed to assist scholars in validly measuring social science constructs based on textual data. The framework draws on a long-estab-lished validity concept in psychometrics but extends these concepts to cover the specific needs of computational text analysis. ValiTex consists of two components, a conceptual framework and a dynamic checklist. Whereas the conceptual framework provides a general structure along distinct phases on how to approach validation, the dynamic checklist defines specific validation steps and provides guidance on which steps might be considered recommendable (i.e., providing relevant and necessary validation evidence) or optional (i.e., useful for providing additional supporting validation evidence). We demonstrate the utility of the framework by applying it to a use case of detecting sexism from social media data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2221227690",
                    "name": "Lukas Birkenmaier"
                },
                {
                    "authorId": "39900730",
                    "name": "Clemens M. Lechner"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                }
            ]
        },
        {
            "paperId": "3597d42ed900c7c49a2267bcfe7a8acc0c1f4e0c",
            "title": "Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection",
            "abstract": "Counterfactually Augmented Data (CAD) aims to improve out-of-domain generalizability, an indicator of model robustness. The improvement is credited to promoting core features of the construct over spurious artifacts that happen to correlate with it. Yet, over-relying on core features may lead to unintended model bias. Especially, construct-driven CAD\u2014perturbations of core features\u2014may induce models to ignore the context in which core features are used. Here, we test models for sexism and hate speech detection on challenging data: non-hate and non-sexist usage of identity and gendered terms. On these hard cases, models trained on CAD, especially construct-driven CAD, show higher false positive rates than models trained on the original, unperturbed data. Using a diverse set of CAD\u2014construct-driven and construct-agnostic\u2014reduces such unintended bias.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33770417",
                    "name": "Indira Sen"
                },
                {
                    "authorId": "3071381",
                    "name": "Mattia Samory"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                },
                {
                    "authorId": "1736067",
                    "name": "Isabelle Augenstein"
                }
            ]
        },
        {
            "paperId": "f73c796a23d1b852cdb60287098c19b07e087aed",
            "title": "Link recommendations: Their impact on network structure and minorities",
            "abstract": "Network-based people recommendation algorithms are widely employed on the Web to suggest new connections in social media or professional platforms. While such recommendations bring people together, the feedback loop between the algorithms and the changes in network structure may exacerbate social biases. These biases include rich-get-richer effects, filter bubbles, and polarization. However, social networks are diverse complex systems and recommendations may affect them differently, depending on their structural properties. In this work, we explore five people recommendation algorithms by systematically applying them over time to different synthetic networks. In particular, we measure to what extent these recommendations change the structure of bi-populated networks and show how these changes affect the minority group. Our systematic experimentation helps to better understand when link recommendation algorithms are beneficial or harmful to minority groups in social networks. In particular, our findings suggest that, while all algorithms tend to close triangles and increase cohesion, all algorithms except Node2Vec are prone to favor and suggest nodes with high in-degree. Furthermore, we found that, especially when both classes are heterophilic, recommendation algorithms can reduce the visibility of minorities.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2056226695",
                    "name": "Antonio Ferrara"
                },
                {
                    "authorId": "2957808",
                    "name": "Lisette Espin Noboa"
                },
                {
                    "authorId": "51118506",
                    "name": "F. Karimi"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                }
            ]
        },
        {
            "paperId": "88ee971d2b00ba0b374aab81daac42e7cc7b8d4d",
            "title": "How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?",
            "abstract": "As NLP models are increasingly deployed in socially situated settings such as online abusive content detection, it is crucial to ensure that these models are robust. One way of improving model robustness is to generate counterfactually augmented data (CAD) for training models that can better learn to distinguish between core features and data artifacts. While models trained on this type of data have shown promising out-of-domain generalizability, it is still unclear what the sources of such improvements are. We investigate the benefits of CAD for social NLP models by focusing on three social computing constructs \u2014 sentiment, sexism, and hate speech. Assessing the performance of models trained with and without CAD across different types of datasets, we find that while models trained on CAD show lower in-domain performance, they generalize better out-of-domain. We unpack this apparent discrepancy using machine explanations and find that CAD reduces model reliance on spurious features. Leveraging a novel typology of CAD to analyze their relationship with model performance, we find that CAD which acts on the construct directly or a diverse set of CAD leads to higher performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33770417",
                    "name": "Indira Sen"
                },
                {
                    "authorId": "3071381",
                    "name": "Mattia Samory"
                },
                {
                    "authorId": "1724463",
                    "name": "Fabian Fl\u00f6ck"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                },
                {
                    "authorId": "49757932",
                    "name": "I. Augenstein"
                }
            ]
        },
        {
            "paperId": "d6a6058b3e6755da5731889db317feb37155d176",
            "title": "The FairCeptron: A Framework for Measuring Human Perceptions of Algorithmic Fairness",
            "abstract": "Measures of algorithmic fairness often do not account for human perceptions of fairness that can substantially vary between different sociodemographics and stakeholders. The FairCeptron framework is an approach for studying perceptions of fairness in algorithmic decision making such as in ranking or classification. It supports (i) studying human perceptions of fairness and (ii) comparing these human perceptions with measures of algorithmic fairness. The framework includes fairness scenario generation, fairness perception elicitation and fairness perception analysis. We demonstrate the FairCeptron framework by applying it to a hypothetical university admission context where we collect human perceptions of fairness in the presence of minorities. An implementation of the FairCeptron framework is openly available, and it can easily be adapted to study perceptions of algorithmic fairness in other application contexts. We hope our work paves the way towards elevating the role of studies of human fairness perceptions in the process of designing algorithmic decision making systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047999478",
                    "name": "Georg Ahnert"
                },
                {
                    "authorId": "2057782666",
                    "name": "I. Smirnov"
                },
                {
                    "authorId": "2101037",
                    "name": "Florian Lemmerich"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                }
            ]
        },
        {
            "paperId": "15f374cab6999dc0571bddb6811eec2438b51159",
            "title": "On the Reliability and Validity of Detecting Approval of Political Actors in Tweets",
            "abstract": "Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors\u2019 approval. However, new challenges related to the reliability and validity of social-media-based estimates arise. Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users\u2019 political opinions based on their content on social media. In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors\u2019 approval by benchmarking them across several datasets. We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of models trained on minimal custom data. We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust. Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians\u2019 approval from tweets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33770417",
                    "name": "Indira Sen"
                },
                {
                    "authorId": "66042341",
                    "name": "Fabian Fl\u00f6ck"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                }
            ]
        },
        {
            "paperId": "17f423a5e542a4bd4de0243548e127038dea6ab5",
            "title": "Bias in data\u2010driven artificial intelligence systems\u2014An introductory survey",
            "abstract": "Artificial Intelligence (AI)\u2010based systems are widely employed nowadays to make decisions that have far\u2010reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well\u2010grounded in a legal frame. In this survey, we focus on data\u2010driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "2516584",
                    "name": "U. Gadiraju"
                },
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                },
                {
                    "authorId": "143858195",
                    "name": "Maria-Esther Vidal"
                },
                {
                    "authorId": "2325428",
                    "name": "S. Ruggieri"
                },
                {
                    "authorId": "1707206",
                    "name": "F. Turini"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                },
                {
                    "authorId": "28075447",
                    "name": "Emmanouil Krasanakis"
                },
                {
                    "authorId": "119661806",
                    "name": "I. Kompatsiaris"
                },
                {
                    "authorId": "1404596968",
                    "name": "K. Kinder-Kurlanda"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                },
                {
                    "authorId": "51118506",
                    "name": "F. Karimi"
                },
                {
                    "authorId": "152179862",
                    "name": "Miriam Fern\u00e1ndez"
                },
                {
                    "authorId": "145842687",
                    "name": "Harith Alani"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "3144221",
                    "name": "Tina Kruegel"
                },
                {
                    "authorId": "49729602",
                    "name": "C. Heinze"
                },
                {
                    "authorId": "2102011",
                    "name": "Klaus Broelemann"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                },
                {
                    "authorId": "1726746",
                    "name": "T. Tiropanis"
                },
                {
                    "authorId": "1752093",
                    "name": "Steffen Staab"
                }
            ]
        },
        {
            "paperId": "8de3c0d82673657e39aeefef589c4a06b45d4ae1",
            "title": "Bias in Data-driven AI Systems - An Introductory Survey",
            "abstract": "AI-based systems are widely employed nowadays to make decisions that have far-reaching impacts on individuals and society. Their decisions might affect everyone, everywhere and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multi-disciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well-grounded in a legal frame. In this survey, we focus on data-driven AI, as a large part of AI is powered nowadays by (big) data and powerful Machine Learning (ML) algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features like race, sex, etc.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "2516584",
                    "name": "U. Gadiraju"
                },
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                },
                {
                    "authorId": "143858195",
                    "name": "Maria-Esther Vidal"
                },
                {
                    "authorId": "2325428",
                    "name": "S. Ruggieri"
                },
                {
                    "authorId": "1707206",
                    "name": "F. Turini"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                },
                {
                    "authorId": "28075447",
                    "name": "Emmanouil Krasanakis"
                },
                {
                    "authorId": "119661806",
                    "name": "I. Kompatsiaris"
                },
                {
                    "authorId": "1404596968",
                    "name": "K. Kinder-Kurlanda"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                },
                {
                    "authorId": "51118506",
                    "name": "F. Karimi"
                },
                {
                    "authorId": "152179862",
                    "name": "Miriam Fern\u00e1ndez"
                },
                {
                    "authorId": "145842687",
                    "name": "Harith Alani"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "3144221",
                    "name": "Tina Kruegel"
                },
                {
                    "authorId": "49729602",
                    "name": "C. Heinze"
                },
                {
                    "authorId": "2102011",
                    "name": "Klaus Broelemann"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                },
                {
                    "authorId": "1726746",
                    "name": "T. Tiropanis"
                },
                {
                    "authorId": "1752093",
                    "name": "Steffen Staab"
                }
            ]
        },
        {
            "paperId": "b29171a0de88c5278360e7d101ded3201e4cb493",
            "title": "Web Routineness and Limits of Predictability: Investigating Demographic and Behavioral Differences Using Web Tracking Data",
            "abstract": "Understanding human activities and movements on the Web is not only important for computational social scientists but can also offer valuable guidance for the design of online systems for recommendations, caching, advertising, and personalization. In this work, we demonstrate that people tend to follow routines on the Web, and these repetitive patterns of web visits increase their browsing behavior's achievable predictability. We present an information-theoretic framework for measuring the uncertainty and theoretical limits of predictability of human mobility on the Web. We systematically assess the impact of different design decisions on the measurement. We apply the framework to a web tracking dataset of German internet users. Our empirical results highlight that individual's routines on the Web make their browsing behavior predictable to 85% on average, though the value varies across individuals. We observe that these differences in the users' predictabilities can be explained to some extent by their demographic and behavioral attributes.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2709512",
                    "name": "Juhi Kulshrestha"
                },
                {
                    "authorId": "2110649249",
                    "name": "Marcos A. C. Oliveira"
                },
                {
                    "authorId": "2043234640",
                    "name": "Orkut Karacalik"
                },
                {
                    "authorId": "3139034",
                    "name": "Denis Bonnay"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                }
            ]
        }
    ]
}