{
    "authorId": "2120473483",
    "papers": [
        {
            "paperId": "31e4dbddca73399120f145f5b2180bf5be491d44",
            "title": "GraphSL: An Open-Source Library for Graph Source Localization Approaches and Benchmark Datasets",
            "abstract": "We introduce GraphSL, a new library for studying the graph source localization problem. graph diffusion and graph source localization are inverse problems in nature: graph diffusion predicts information diffusions from information sources, while graph source localization predicts information sources from information diffusions. GraphSL facilitates the exploration of various graph diffusion models for simulating information diffusions and enables the evaluation of cutting-edge source localization approaches on established benchmark datasets. The source code of GraphSL is made available at Github Repository (https://github.com/xianggebenben/GraphSL). Bug reports and feedback can be directed to the Github issues page (https://github.com/xianggebenben/GraphSL/issues).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2256780039",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "60d35b20fde8bb5e9e0eb5c0d43860576090cd92",
            "title": "Non-Euclidean Spatial Graph Neural Network",
            "abstract": "Spatial networks are networks whose graph topology is constrained by their embedded spatial space. Understanding the coupled spatial-graph properties is crucial for extracting powerful representations from spatial networks. Therefore, merely combining individual spatial and network representations cannot reveal the underlying interaction mechanism of spatial networks. Besides, existing spatial network representation learning methods can only consider networks embedded in Euclidean space, and can not well exploit the rich geometric information carried by irregular and non-uniform non-Euclidean space. In order to address this issue, in this paper we propose a novel generic framework to learn the representation of spatial networks that are embedded in non-Euclidean manifold space. Specifically, a novel message-passing-based neural network is proposed to combine graph topology and spatial geometry, where spatial geometry is extracted as messages on the edges. We theoretically guarantee that the learned representations are provably invariant to important symmetries such as rotation or translation, and simultaneously maintain sufficient ability in distinguishing different geometric structures. The strength of our proposed method is demonstrated through extensive experiments on both synthetic and real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2021011947",
                    "name": "Zhengwu Zhang"
                },
                {
                    "authorId": "2275541506",
                    "name": "Sirui Li"
                },
                {
                    "authorId": "2275708655",
                    "name": "Jingcheng Zhou"
                },
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2142231513",
                    "name": "Abhinav Angirekula"
                },
                {
                    "authorId": "2275878297",
                    "name": "Allen Zhang"
                },
                {
                    "authorId": "2256780039",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "653b406e03cac1c03f38e47211c3156f7bbe2c2f",
            "title": "Deep Graph Representation Learning and Optimization for Influence Maximization",
            "abstract": "Influence maximization (IM) is formulated as selecting a set of initial users from a social network to maximize the expected number of influenced users. Researchers have made great progress in designing various traditional methods, and their theoretical design and performance gain are close to a limit. In the past few years, learning-based IM methods have emerged to achieve stronger generalization ability to unknown graphs than traditional ones. However, the development of learning-based IM methods is still limited by fundamental obstacles, including 1) the difficulty of effectively solving the objective function; 2) the difficulty of characterizing the diversified underlying diffusion patterns; and 3) the difficulty of adapting the solution under various node-centrality-constrained IM variants. To cope with the above challenges, we design a novel framework DeepIM to generatively characterize the latent representation of seed sets, and we propose to learn the diversified information diffusion pattern in a data-driven and end-to-end manner. Finally, we design a novel objective function to infer optimal seed sets under flexible node-centrality-based budget constraints. Extensive analyses are conducted over both synthetic and real-world datasets to demonstrate the overall performance of DeepIM. The code and data are available at: https://github.com/triplej0079/DeepIM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988349",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "2157887017",
                    "name": "Junji Jiang"
                },
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "1698253",
                    "name": "M. Thai"
                },
                {
                    "authorId": "2216002544",
                    "name": "Lukas Xue"
                },
                {
                    "authorId": "2216029466",
                    "name": "James Song"
                },
                {
                    "authorId": "1471335567",
                    "name": "M. Qiu"
                },
                {
                    "authorId": "144000223",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "6847b9658f287f430098199cd81bf26308da13f9",
            "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
            "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988575",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2182238045",
                    "name": "Can Zheng"
                },
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2123930262",
                    "name": "Tanmoy Chowdhury"
                },
                {
                    "authorId": "2110425042",
                    "name": "Yun-Qing Li"
                },
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2211987764",
                    "name": "Tian-yu Zhao"
                },
                {
                    "authorId": "2218486790",
                    "name": "Amit Panalkar"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": null,
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2204622281",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2218495127",
                    "name": "Chris White"
                },
                {
                    "authorId": "144966687",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "2188744953",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "a700cea9ab4a42c0fe74ccc9a89cdafc23ba621c",
            "title": "Prompt-based Domain Discrimination for Multi-source Time Series Domain Adaptation",
            "abstract": "Time series domain adaptation stands as a pivotal and intricate challenge with diverse applications, including but not limited to human activity recognition, sleep stage classification, and machine fault diagnosis. Despite the numerous domain adaptation techniques proposed to tackle this complex problem, their primary focus has been on the common representations of time series data. This concentration might inadvertently lead to the oversight of valuable domain-specific information originating from different source domains. To bridge this gap, we introduce POND, a novel prompt-based deep learning model designed explicitly for multi-source time series domain adaptation. POND is tailored to address significant challenges, notably: 1) The unavailability of a quantitative relationship between meta-data information and time series distributions, and 2) The dearth of exploration into extracting domain-specific meta-data information. In this paper, we present an instance-level prompt generator and a fidelity loss mechanism to facilitate the faithful learning of meta-data information. Additionally, we propose a domain discrimination technique to discern domain-specific meta-data information from multiple source domains. Our approach involves a simple yet effective meta-learning algo-rithm to optimize the objective efficiently. Furthermore, we augment the model\u2019s performance by incorporating the Mixture of Expert (MoE) technique. The efficacy and robustness of our proposed POND model are extensively validated through experiments across 50 scenarios encompassing five datasets, which demonstrates that our proposed POND model outperforms the state-of-the-art methods by up to 66% on the F1-score.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "7583867",
                    "name": "Guangji Bai"
                },
                {
                    "authorId": "2249879747",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "1720777201",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "2282545112",
                    "name": "Haifeng Chen"
                }
            ]
        },
        {
            "paperId": "d8c5db7ba743e4af2410aee5035e038d4d3b1aed",
            "title": "POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning",
            "abstract": "Time series domain adaptation stands as a pivotal and intricate challenge with diverse applications, including but not limited to human activity recognition, sleep stage classification, and machine fault diagnosis. Despite the numerous domain adaptation techniques proposed to tackle this complex problem, they primarily focus on domain adaptation from a single source domain. Yet, it is more crucial to investigate domain adaptation from multiple domains due to the potential for greater improvements. To address this, three important challenges need to be overcome: 1). The lack of exploration to utilize domain-specific information for domain adaptation, 2). The difficulty to learn domain-specific information that changes over time, and 3). The difficulty to evaluate learned domain-specific information. In order to tackle these challenges simultaneously, in this paper, we introduce PrOmpt-based domaiN Discrimination (POND), the first framework to utilize prompts for time series domain adaptation. Specifically, to address Challenge 1, we extend the idea of prompt tuning to time series analysis and learn prompts to capture common and domain-specific information from all source domains. To handle Challenge 2, we introduce a conditional module for each source domain to generate prompts from time series input data. For Challenge 3, we propose two criteria to select good prompts, which are used to choose the most suitable source domain for domain adaptation. The efficacy and robustness of our proposed POND model are extensively validated through experiments across 50 scenarios encompassing four datasets. Experimental results demonstrate that our proposed POND model outperforms all state-of-the-art comparison methods by up to $66\\%$ on the F1-score.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "7583867",
                    "name": "Guangji Bai"
                },
                {
                    "authorId": "2249879747",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "1720777201",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                }
            ]
        },
        {
            "paperId": "e45031dc8bac71d86206a6b4b8ad9891b3c43dac",
            "title": "Curriculum Learning for Graph Neural Networks: Which Edges Should We Learn First",
            "abstract": "Graph Neural Networks (GNNs) have achieved great success in representing data with dependencies by recursively propagating and aggregating messages along the edges. However, edges in real-world graphs often have varying degrees of difficulty, and some edges may even be noisy to the downstream tasks. Therefore, existing GNNs may lead to suboptimal learned representations because they usually treat every edge in the graph equally. On the other hand, Curriculum Learning (CL), which mimics the human learning principle of learning data samples in a meaningful order, has been shown to be effective in improving the generalization ability and robustness of representation learners by gradually proceeding from easy to more difficult samples during training. Unfortunately, existing CL strategies are designed for independent data samples and cannot trivially generalize to handle data dependencies. To address these issues, we propose a novel CL strategy to gradually incorporate more edges into training according to their difficulty from easy to hard, where the degree of difficulty is measured by how well the edges are expected given the model training status. We demonstrate the strength of our proposed method in improving the generalization ability and robustness of learned representations through extensive experiments on nine synthetic datasets and nine real-world datasets. The code for our proposed method is available at https://github.com/rollingstonezz/Curriculum_learning_for_GNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2021011947",
                    "name": "Zhengwu Zhang"
                },
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2256780039",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "63bcd049a495ed6341c9985b400a75aacbbaa0d0",
            "title": "Source Localization of Graph Diffusion via Variational Autoencoders for Graph Inverse Problems",
            "abstract": "Graph diffusion problems such as the propagation of rumors, computer viruses, or smart grid failures are ubiquitous and societal. Hence it is usually crucial to identify diffusion sources according to the current graph diffusion observations. Despite its tremendous necessity and significance in practice, source localization, as the inverse problem of graph diffusion, is extremely challenging as it is ill-posed: different sources may lead to the same graph diffusion patterns. Different from most traditional source localization methods, this paper focuses on a probabilistic manner to account for the uncertainty of different candidate sources. Such endeavors require to overcome significant challenges along the way including: 1) the uncertainty in graph diffusion source localization is hard to be quantified; 2) the complex patterns of the graph diffusion sources are difficult to be probabilistically characterized; 3) the generalization under any underlying diffusion patterns is hard to be imposed. To solve the above challenges, this paper presents a generic framework: Source Localization Variational AutoEncoder (SL-VAE) for locating the diffusion sources under arbitrary diffusion patterns. Particularly, we propose a probabilistic model that leverages the forward diffusion estimation model along with deep generative models to approximate the diffusion source distribution for quantifying the uncertainty. SL-VAE further utilizes prior knowledge of the source-observation pairs to characterize the complex patterns of diffusion sources by a learned generative prior. Lastly, a unified objective that integrates the forward diffusion estimation model is derived to enforce the model to generalize under arbitrary diffusion patterns. Extensive experiments are conducted on $7$ real-world datasets to demonstrate the superiority of SL-VAE in reconstructing the diffusion sources by excelling the state-of-the-arts on average 20% in AUC score. The code and data are available at: https://github.com/triplej0079/SLVAE.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2059988349",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "2157887017",
                    "name": "Junji Jiang"
                },
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2116735669",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "7ed9a6e175bedd4d054130a48cfd861a268e9cd0",
            "title": "Toward Quantized Model Parallelism for Graph-Augmented MLPs Based on Gradient-Free ADMM Framework",
            "abstract": "While graph neural networks (GNNs) are popular in the deep learning community, they suffer from several challenges including over-smoothing, over-squashing, and gradient vanishing. Recently, a series of models have attempted to relieve these issues by first augmenting the node features and then imposing node-wise functions based on multilayer perceptron (MLP), which are widely referred to as graph-augmented MLP (GA-MLP) models. However, while GA-MLP models enjoy deeper architectures for better accuracy, their efficiency largely deteriorates. Moreover, popular acceleration techniques such as stochastic-version or data-parallelism cannot be effectively applied due to the dependency among samples (i.e., nodes) in graphs. To address these issues, in this article, instead of data parallelism, we propose a parallel graph deep learning Alternating Direction Method of Multipliers (pdADMM-G) framework to achieve model parallelism: parameters in each layer of GA-MLP models can be updated in parallel. The extended pdADMM-G-Q algorithm reduces communication costs by introducing the quantization technique. Theoretical convergence to a (quantized) stationary point of the pdADMM-G algorithm and the pdADMM-G-Q algorithm is provided with a sublinear convergence rate $o(1/k)$ , where $k$ is the number of iterations. Extensive experiments demonstrate the convergence of two proposed algorithms. Moreover, they lead to a more massive speedup and better performance than all state-of-the-art comparison methods on nine benchmark datasets. Last but not least, the proposed pdADMM-G-Q algorithm reduces communication overheads by up to 45% without loss of performance. Our code is available at https://github.com/xianggebenben/pdADMM-G.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2126331789",
                    "name": "Hongyi Li"
                },
                {
                    "authorId": "1712184172",
                    "name": "Zheng Chai"
                },
                {
                    "authorId": "47905087",
                    "name": "Yongchao Wang"
                },
                {
                    "authorId": "47585127",
                    "name": "Yue Cheng"
                },
                {
                    "authorId": "2116734918",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "97f959c34f24a9bb4717c72249410877233d199a",
            "title": "DeepGAR: Deep Graph Learning for Analogical Reasoning",
            "abstract": "Analogical reasoning is the process of discovering and mapping correspondences from a target subject to a base subject. As the most well-known computational method of analogical reasoning, Structure-Mapping Theory (SMT) abstracts both target and base subjects into relational graphs and forms the cognitive process of analogical reasoning by finding a corresponding subgraph (i.e., correspondence) in the target graph that is aligned with the base graph. However, incorporating deep learning for SMT is still under-explored due to several obstacles: 1) the combinatorial complexity of searching for the correspondence in the target graph; 2) the correspondence mining is restricted by various cognitive theory-driven constraints. To address both challenges, we propose a novel framework for Analogical Reasoning (DeepGAR) that identifies the correspondence between source and target domains by assuring cognitive theory-driven constraints. Specifically, we design a geometric constraint embedding space to induce subgraph relation from node embeddings for efficient subgraph search. Furthermore, we develop novel learning and optimization strategies that could end-to-end identify correspondences that are strictly consistent with constraints driven by the cognitive theory. Extensive experiments are conducted on synthetic and real-world datasets to demonstrate the effectiveness of the proposed DeepGAR over existing methods. The code and data are available at: https://github.com/triplej0079/DeepGAR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988349",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "2123930262",
                    "name": "Tanmoy Chowdhury"
                },
                {
                    "authorId": "2157887017",
                    "name": "Junji Jiang"
                },
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "144000223",
                    "name": "Liang Zhao"
                }
            ]
        }
    ]
}