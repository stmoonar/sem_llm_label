{
    "authorId": "145756130",
    "papers": [
        {
            "paperId": "0ea3d5c936948b1b63328e4b626ff27e117adfdd",
            "title": "Parallel Dispatching: An ACP-Based High-Speed Railway Intelligent Dispatching System",
            "abstract": "The antiquated method of train dispatching relying solely on dispatchers' knowledge is unable to meet the need for rapid response and effective solutions in emergency situations. This article proposes an ACP method (Artificial System, Computational Experiment, and Parallel Execution) to explore the potential of automating the management of high-speed railway trains. A parallel high-speed railway dispatching system is created, comprised of an actual system and its artificial counterpart. Various computational experiments were conducted to investigate the effects of different factors on high-speed railway dispatching, and the results were used to devise an optimal train operation adjustment scheme and strategies. Through the parallel operations and virtual-real interactions between the artificial dispatching system and the actual dispatching system, dispatchers are able to execute the most effective strategies, and achieve optimal control of the high-speed railway system. This technique eliminates the difficulty in managing the complex network of high-speed railway dispatching, ensuring that emergency commands are issued quickly and efficiently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110579979",
                    "name": "Weiguang Xu"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "22200602",
                    "name": "Xingyuan Dai"
                },
                {
                    "authorId": "1384408230",
                    "name": "Zhiming Yuan"
                },
                {
                    "authorId": "2146341682",
                    "name": "Tao Zhang"
                },
                {
                    "authorId": "1514738707",
                    "name": "Tao Wang"
                },
                {
                    "authorId": "2197785995",
                    "name": "Yuhai Ren"
                },
                {
                    "authorId": "34859953",
                    "name": "Yisheng Lv"
                }
            ]
        },
        {
            "paperId": "03c3bdf9430db2d3c0ddf7abb956222e6c43afbf",
            "title": "Decentralized Autonomous Operations and Organizations in TransVerse: Federated Intelligence for Smart Mobility",
            "abstract": "Human and social factors are essential to transportation systems, yet top-down management fails to consider them sufficiently. Consequently, management strategies are not tailored to human needs and are inadequate in providing transportation intelligence. This article investigates a management architecture based on decentralized/distributed autonomous operations/organizations (DAOs) that considers both the technical and societal aspects in our transportation metaverse, TransVerse. This design maps people\u2019s transportation needs in physical space to their digital counterparts in cyberspace, utilizing blockchain technology to guarantee the secure exchange of information and ultimately bring about the Internet of Minds (IoM). With the federated intelligence that emerged in IoM, we can devise reliable and prompt traffic decisions by incorporating consensus, community voting, and smart contracts into the organizational, coordination, and execution structure. Details on operational procedures and key technologies are also covered. To demonstrate the efficacy of DAOs-based management, a case study of world model-driven cooperative signal control is provided, indicating its promising application in future transportation management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "22200602",
                    "name": "Xingyuan Dai"
                },
                {
                    "authorId": "34859953",
                    "name": "Yisheng Lv"
                },
                {
                    "authorId": "3487858",
                    "name": "Jinglong Niu"
                },
                {
                    "authorId": "3396678",
                    "name": "Yilun Lin"
                }
            ]
        },
        {
            "paperId": "112964aa0caee373cb8c87ab73d1e7d39124cd8b",
            "title": "Contrastive Representation Learning Based on Multiple Node-centered Subgraphs",
            "abstract": "As the basic element of graph-structured data, node has been recognized as the main object of study in graph representation learning. A single node intuitively has multiple node-centered subgraphs from the whole graph (e.g., one person in a social network has multiple social circles based on his different relationships). We study this intuition under the framework of graph contrastive learning, and propose a multiple node-centered subgraphs contrastive representation learning method to learn node representation on graphs in a self-supervised way. Specifically, we carefully design a series of node-centered regional subgraphs of the central node. Then, the mutual information between different subgraphs of the same node is maximized by contrastive loss. Experiments on various real-world datasets and different downstream tasks demonstrate that our model has achieved state-of-the-art results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152290312",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2140054413",
                    "name": "Wenjun Wang"
                },
                {
                    "authorId": "19066746",
                    "name": "Minglai Shao"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                }
            ]
        },
        {
            "paperId": "25352b70195a48a851bb45d1ea539e87317735bd",
            "title": "Transformer-Based Macroscopic Regulation for High-Speed Railway Timetable Rescheduling",
            "abstract": "Unexpected delays in train operations can cause a cascade of negative consequences in a high-speed railway system. In such cases, train timetables need to be rescheduled. However, timely and efficient train timetable rescheduling is still a challenging problem due to its modeling difficulties and low optimization efficiency. This paper presents a Transformer-based macroscopic regulation approach which consists of two stages including Transformer-based modeling and policy-based decision-making. Firstly, the relationship between various train schedules and operations is described by creating a macroscopic model with the Transformer, providing the better understanding of overall operation in the high-speed railway system. Then, a policy-based approach is used to solve a continuous decision problem after macro-modeling for fast convergence. Extensive experiments on various delay scenarios are conducted. The results demonstrate the effectiveness of the proposed method in comparison to other popular methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110579979",
                    "name": "Weiguang Xu"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "2118082597",
                    "name": "Jie Cheng"
                },
                {
                    "authorId": "2231870477",
                    "name": "Yin Wang"
                },
                {
                    "authorId": "2231907838",
                    "name": "Yiqing Tang"
                },
                {
                    "authorId": "2146341682",
                    "name": "Tao Zhang"
                },
                {
                    "authorId": "1384408230",
                    "name": "Zhiming Yuan"
                },
                {
                    "authorId": "34859953",
                    "name": "Yisheng Lv"
                },
                {
                    "authorId": "1682816",
                    "name": "Fei Wang"
                }
            ]
        },
        {
            "paperId": "3200a0d6fef7164f0341cf1938f584da6057ffd6",
            "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs",
            "abstract": "Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "13336152",
                    "name": "Angelica Chen"
                },
                {
                    "authorId": "80842917",
                    "name": "Jason Phang"
                },
                {
                    "authorId": "119389860",
                    "name": "Alicia Parrish"
                },
                {
                    "authorId": "2044959912",
                    "name": "Vishakh Padmakumar"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "1799822",
                    "name": "Sam Bowman"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                }
            ]
        },
        {
            "paperId": "341d516cc858dc92ba14a788ef40d0559b5a2b26",
            "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations",
            "abstract": "Despite significant progress having been made in question answering on tabular data (Table QA), it\u2019s unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the robustness of Table QA models, we propose a benchmark called RobuT, which builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and includes human-annotated adversarial perturbations in terms of table header, table content, and question. Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets. We propose to address this problem by using large language models to generate adversarial examples to enhance training, which significantly improves the robustness of Table QA models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46316984",
                    "name": "Yilun Zhao"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "51990260",
                    "name": "Linyong Nan"
                },
                {
                    "authorId": "2186056193",
                    "name": "Zhenting Qi"
                },
                {
                    "authorId": "2220571367",
                    "name": "Wenlin Zhang"
                },
                {
                    "authorId": "47274259",
                    "name": "Xiangru Tang"
                },
                {
                    "authorId": "2124195718",
                    "name": "Boyu Mi"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                }
            ]
        },
        {
            "paperId": "44628f304bd47212aaa9b4215da53eb67676517f",
            "title": "Galliot: Path Merging Based Betweenness Centrality Algorithm on GPU",
            "abstract": "Betweenness centrality (BC) is widely used to measure a vertex\u2019s significance by using the frequency of a vertex appearing in the shortest path between other vertices. However, most recent algorithms in BC computation suffer from the problem of high auxiliary memory consumption. To reduce BC computing\u2019s memory consumption, we propose a path-mergingbased algorithm called Galliot to calculate the BC values on GPU, which aims to minimize the on-board memory consumption and enable the BC computation of large-scale graphs. The proposed algorithm requires $\\mathcal{O}$(n) space and runs in $\\mathcal{O}$(mn) time on unweighted graphs. We present the theoretical principle for the proposed path merging method. Moreover, we propose a locality-oriented policy to maintain and update the worklist to improve GPU data locality. In addition, we conducted extensive experiments on NVIDIA GPUs to show the performance of Galliot. The results show that Galliot can process the larger graphs, which have 11.32\u00d7 more vertices and 5.67\u00d7 more edges than the graphs that recent works. Moreover, Galliot can achieve up to 38.77\u00d7 speedup over the existing methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115546236",
                    "name": "Zhigao Zheng"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "2217495380",
                    "name": "Peicheng Xie"
                },
                {
                    "authorId": "2236508431",
                    "name": "Bo DuM"
                }
            ]
        },
        {
            "paperId": "527e4a2615ab9214b630a313642e40b300fc27cb",
            "title": "Adaptation Speed Analysis for Fairness-aware Causal Models",
            "abstract": "For example, in machine translation tasks, to achieve bidirectional translation between two languages, the source corpus is often used as the target corpus, which involves the training of two models with opposite directions. The question of which one can adapt most quickly to a domain shift is of significant importance in many fields. Specifically, consider an original distribution p that changes due to an unknown intervention, resulting in a modified distribution p*. In aligning p with p*, several factors can affect the adaptation rate, including the causal dependencies between variables in p. In real-life scenarios, however, we have to consider the fairness of the training process, and it is particularly crucial to involve a sensitive variable (bias) present between a cause and an effect variable. To explore this scenario, we examine a simple structural causal model (SCM) with a cause-bias-effect structure, where variable A acts as a sensitive variable between cause (X) and effect (Y). The two models respectively exhibit consistent and contrary cause-effect directions in the cause-bias-effect SCM. After conducting unknown interventions on variables within the SCM, we can simulate some kinds of domain shifts for analysis. We then compare the adaptation speeds of two models across four shift scenarios. Additionally, we prove the connection between the adaptation speeds of the two models across all interventions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2190146991",
                    "name": "Yujie Lin"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "19066746",
                    "name": "Minglai Shao"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2204622281",
                    "name": "Haifeng Chen"
                }
            ]
        },
        {
            "paperId": "5d4abbd2166e6e04de22426415778aef96792867",
            "title": "Multi-crop Contrastive Learning for Unsupervised Image-to-Image Translation",
            "abstract": "\uf020 Abstract \u2014Recently, image-to-image translation methods based on contrastive learning achieved state-of-the-art results in many tasks. However, the negatives are sampled from the input feature spaces in the previous work, which makes the negatives lack diversity. Moreover, in the latent space of the embedings,the previous methods ignore domain consistency between the generated image and the real images of target domain. In this paper, we propose a novel contrastive learning framework for unpaired image-to-image translation, called MCCUT. We utilize the multi-crop views to generate the negatives via the center-crop and the random-crop, which can improve the diversity of negatives and meanwhile increase the quality of negatives. To constrain the embedings in the deep feature space,, we formulate a new domain consistency loss function, which encourages the generated images to be close to the real images in the embedding space of same domain. Furthermore, we present a dual coordinate channel attention network by embedding positional information into SENet, which called DCSE module. We employ the DCSE module in the design of generator, which makes the generator pays more attention to channels with greater weight. In many image-to-image translation tasks, our method achieves state-of-the-art results, and the advantages of our method have been proved through extensive comparison experiments and ablation research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "2153359026",
                    "name": "Weixi Cai"
                },
                {
                    "authorId": "2112341070",
                    "name": "Zheng Yuan"
                },
                {
                    "authorId": "2184242420",
                    "name": "Cheng Hu"
                }
            ]
        },
        {
            "paperId": "69f2ba0f33a54e01de32c616b64e85d5d7194067",
            "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
            "abstract": "Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers\"yes\"to the input question\"Can eagles fly?\"with the explanation\"all birds can fly\", then humans would infer from the explanation that it would also answer\"yes\"to the counterfactual input\"Can penguins fly?\". If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109268730",
                    "name": "Yanda Chen"
                },
                {
                    "authorId": "51011000",
                    "name": "Ruiqi Zhong"
                },
                {
                    "authorId": "2218040104",
                    "name": "Narutatsu Ri"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "2140062900",
                    "name": "He He"
                },
                {
                    "authorId": "5164568",
                    "name": "J. Steinhardt"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "145590324",
                    "name": "K. McKeown"
                }
            ]
        }
    ]
}