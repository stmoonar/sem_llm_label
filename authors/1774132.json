{
    "authorId": "1774132",
    "papers": [
        {
            "paperId": "0456bc829c199d5f8adf9ee5a3460c9dc5dc1453",
            "title": "YAGO 4.5: A Large and Clean Knowledge Base with a Rich Taxonomy",
            "abstract": "Knowledge Bases (KBs) find applications in many knowledge-intensive tasks and, most notably, in information retrieval. Wikidata is one of the largest public general-purpose KBs. Yet, its collaborative nature has led to a convoluted schema and taxonomy. The YAGO 4 KB cleaned up the taxonomy by incorporating the ontology of Schema.org, resulting in a cleaner structure amenable to automated reasoning. However, it also cut away large parts of the Wikidata taxonomy, which is essential for information retrieval. In this paper, we extend YAGO 4 with a large part of the Wikidata taxonomy - while respecting logical constraints and the distinction between classes and instances. This yields YAGO 4.5, a new, logically consistent version of YAGO that adds a rich layer of informative classes. An intrinsic and an extrinsic evaluation show the value of the new resource.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                },
                {
                    "authorId": "33973438",
                    "name": "Mehwish Alam"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                },
                {
                    "authorId": "36560957",
                    "name": "Pierre-Henri Paris"
                },
                {
                    "authorId": "2233086268",
                    "name": "Jules Soria"
                }
            ]
        },
        {
            "paperId": "18211b42de4b12eb40572716cdc27f52ebdab72c",
            "title": "Ontology matching using textual class descriptions",
            "abstract": "In this paper, we propose TEXTO, a TEXT -based O ntology matching system. This matcher leverages the rich semantic information of classes available in most ontologies by a combination of a pre-trained word embedding model and a pre-trained language model. Its performance is evaluated on the datasets of the OAEI Common Knowledge Graphs Track, augmented with the description of each class, and a new dataset based on the refreshed alignment of Schema.org and Wikidata. Our results demonstrate that TEXTO outperforms all state-of-art matchers in terms of precision, recall, and F1 score. In particular, we show that almost perfect class alignment can be achieved using textual content only, excluding any structural information like the graph of classes or the instances of each class.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2255821740",
                    "name": "Yiwen Peng"
                },
                {
                    "authorId": "2258297597",
                    "name": "Mehwish Alam"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                }
            ]
        },
        {
            "paperId": "45bdb5f9165777fc9e3def9baaa141b7dc85f8db",
            "title": "A Consistent Diffusion-Based Algorithm for Semi-Supervised Graph Learning",
            "abstract": "The task of semi-supervised classification aims at assigning labels to all nodes of a graph based on the labels known for a few nodes, called the seeds. One of the most popular algorithms relies on the principle of heat diffusion, where the labels of the seeds are spread by thermoconductance and the temperature of each node at equilibrium is used as a score function for each label. In this paper, we prove that this algorithm is not consistent unless the temperatures of the nodes at equilibrium are centered before scoring. This crucial step does not only make the algorithm provably consistent on a block model but brings significant performance gains on real graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                },
                {
                    "authorId": "51502169",
                    "name": "Nathan de Lara"
                }
            ]
        },
        {
            "paperId": "886180ccd1e578c0f00d1bfc91f2bc93518408d9",
            "title": "KNNs of Semantic Encodings for Rating Prediction",
            "abstract": "This paper explores a novel application of textual semantic similarity to user-preference representation for rating prediction. The approach represents a user's preferences as a graph of textual snippets from review text, where the edges are defined by semantic similarity. This textual, memory-based approach to rating prediction enables review-based explanations for recommendations. The method is evaluated quantitatively, highlighting that leveraging text in this way outperforms both strong memory-based and model-based collaborative filtering baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51921740",
                    "name": "Leo Laugier"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                },
                {
                    "authorId": "2065639113",
                    "name": "Lucas Dixon"
                },
                {
                    "authorId": "32211627",
                    "name": "Raghuram Vadapalli"
                }
            ]
        },
        {
            "paperId": "945c6856e64226440915250125ecf79b7540b6da",
            "title": "A Self-Encoder for Learning Nearest Neighbors",
            "abstract": "We present the self-encoder, a neural network trained to guess the identity of each data sample. Despite its simplicity, it learns a very useful representation of data, in a self-supervised way. Specifically, the self-encoder learns to distribute the data samples in the embedding space so that they are linearly separable from one another. This induces a geometry where two samples are close in the embedding space when they are not easy to differentiate. The self-encoder can then be combined with a nearest-neighbor classifier or regressor for any subsequent supervised task. Unlike regular nearest neighbors, the predictions resulting from this encoding of data are invariant to any scaling of features, making any preprocessing like min-max scaling not necessary. The experiments show the efficiency of the approach, especially on heterogeneous data mixing numerical features and categorical features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "136806596",
                    "name": "Armand Boschin"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                },
                {
                    "authorId": "10836493",
                    "name": "Marc Jeanmougin"
                }
            ]
        },
        {
            "paperId": "9fa28468f15daf74659e5ea91782a5f4fcabf819",
            "title": "Enriching Wikidata with Semantified Wikipedia Hyperlinks",
            "abstract": ". We propose a novel approach to enrich Wikidata with the textual content of Wikipedia. Speci\ufb01cally, we leverage knowledge graph (KG) embedding models to classify the hyperlinks between Wikipedia articles and predict the corresponding facts. For instance, we would like to complete the triple ( Berlin , *, Germany ) with the relation capital of , given a hyperlink from Berlin to Germany in Wikipedia. While existing KG embedding models can be used for this task of relation prediction, they were not explicitly designed for it and their performance is not satisfactory. In this paper, we propose two methods that greatly improve the performance of these models on this task: \ufb01rst, a new negative sampling method that balances the roles of entities and relations during training; second, a method to exploit the types of entities in the selection of candidate relations. We obtain accuracy scores as high as 94% on the popular FB15k237 dataset and 75% on WDV5, an extraction of Wikidata. The e\ufb03ciency of the approach is illustrated on some Wikipedia pages, where new facts unknown to Wikidata are predicted by our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "136806596",
                    "name": "Armand Boschin"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                }
            ]
        },
        {
            "paperId": "da0c88debde7558f7f311116ea05b86071a60c80",
            "title": "Pairwise Adjusted Mutual Information",
            "abstract": "A well-known metric for quantifying the similarity between two clusterings is the adjusted mutual information. Compared to mutual information, a corrective term based on random permutations of the labels is introduced, preventing two clusterings being similar by chance. Unfortunately, this adjustment makes the metric computationally expensive. In this paper, we propose a novel adjustment based on {pairwise} label permutations instead of full label permutations. Specifically, we consider permutations where only two samples, selected uniformly at random, exchange their labels. We show that the corresponding adjusted metric, which can be expressed explicitly, behaves similarly to the standard adjusted mutual information for assessing the quality of a clustering, while having a much lower time complexity. Both metrics are compared in terms of quality and performance on experiments based on synthetic and real data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2081988592",
                    "name": "Denys Lazarenko"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                }
            ]
        },
        {
            "paperId": "168e0fdf98717245217c5677ef45718c1f31b953",
            "title": "Time Series Source Separation with Slow Flows",
            "abstract": "In this paper, we show that slow feature analysis (SFA), a common time series decomposition method, naturally fits into the flow-based models (FBM) framework, a type of invertible neural latent variable models. Building upon recent advances on blind source separation, we show that such a fit makes the time series decomposition identifiable.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "50988081",
                    "name": "Edouard Pineau"
                },
                {
                    "authorId": "3072865",
                    "name": "S. Razakarivony"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                }
            ]
        },
        {
            "paperId": "424186a0a7f70a798a17d7526e55657464000769",
            "title": "Scikit-network: Graph Analysis in Python",
            "abstract": "Scikit-network is a Python package inspired by scikit-learn for the analysis of large graphs. Graphs are represented by their adjacency matrix in the sparse CSR format of SciPy. The package provides state-of-the-art algorithms for ranking, clustering, classifying, embedding and visualizing the nodes of a graph. High performance is achieved through a mix of fast matrix-vector products (using SciPy), compiled code (using Cython) and parallel processing. The package is distributed under the BSD license, with dependencies limited to NumPy and SciPy. It is compatible with Python 3.6 and newer. Source code, documentation and installation instructions are available online.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                },
                {
                    "authorId": "51502169",
                    "name": "Nathan de Lara"
                },
                {
                    "authorId": "1945935181",
                    "name": "Quentin Lutz"
                },
                {
                    "authorId": "50997190",
                    "name": "Bertrand Charpentier"
                }
            ]
        },
        {
            "paperId": "44ee628db6b804bde0771e8689017807299edcc0",
            "title": "Unsupervised ageing detection of mechanical systems on a causality graph",
            "abstract": "Multivariate time series (MTS) have specific features that complicate their analysis: interactions in space and time between the MTS components, variable length, absence of trivial alignment between samples and high dimensionality. Hence, finding a representation of MTS from which we can extract meaningful information is a challenging task. In general, specific assumptions are needed to obtain a valuable representation. In this paper, we assume that a dataset of MTS samples has an underlying causal structure that we can exploit to represent samples. Our contribution is a new representation framework that consists of first finding the overall causality graph in a studied dataset and then mapping each sample onto G to obtain a causality-based representation. Since causality isG an important feature underlying MTS data, we claim and show that representating samples on G is meaningful. We name this method Sequence-to-Graph (Seq2Graph). We apply Seq2Graph on health monitoring tasks, using two MTS datasets coming from ageing mechanical systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50988081",
                    "name": "Edouard Pineau"
                },
                {
                    "authorId": "3072865",
                    "name": "S. Razakarivony"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                }
            ]
        }
    ]
}