{
    "authorId": "1751802",
    "papers": [
        {
            "paperId": "5063d1b976c20853a966d3a29b78ffd7302c9f22",
            "title": "Fast-FedUL: A Training-Free Federated Unlearning with Provable Skew Resilience",
            "abstract": "Federated learning (FL) has recently emerged as a compelling machine learning paradigm, prioritizing the protection of privacy for training data. The increasing demand to address issues such as ``the right to be forgotten'' and combat data poisoning attacks highlights the importance of techniques, known as \\textit{unlearning}, which facilitate the removal of specific training data from trained FL models. Despite numerous unlearning methods proposed for centralized learning, they often prove inapplicable to FL due to fundamental differences in the operation of the two learning paradigms. Consequently, unlearning in FL remains in its early stages, presenting several challenges. Many existing unlearning solutions in FL require a costly retraining process, which can be burdensome for clients. Moreover, these methods are primarily validated through experiments, lacking theoretical assurances. In this study, we introduce Fast-FedUL, a tailored unlearning method for FL, which eliminates the need for retraining entirely. Through meticulous analysis of the target client's influence on the global model in each round, we develop an algorithm to systematically remove the impact of the target client from the trained model. In addition to presenting empirical findings, we offer a theoretical analysis delineating the upper bound of our unlearned model and the exact retrained model (the one obtained through retraining using untargeted clients). Experimental results with backdoor attack scenarios indicate that Fast-FedUL effectively removes almost all traces of the target client, while retaining the knowledge of untargeted clients (obtaining a high accuracy of up to 98\\% on the main task). Significantly, Fast-FedUL attains the lowest time complexity, providing a speed that is 1000 times faster than retraining. Our source code is publicly available at \\url{https://github.com/thanhtrunghuynh93/fastFedUL}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152399820",
                    "name": "T. T. Huynh"
                },
                {
                    "authorId": "2223074507",
                    "name": "Trong Bang Nguyen"
                },
                {
                    "authorId": "2290449405",
                    "name": "Phi Le Nguyen"
                },
                {
                    "authorId": "2117824517",
                    "name": "T. Nguyen"
                },
                {
                    "authorId": "2257265858",
                    "name": "Matthias Weidlich"
                },
                {
                    "authorId": "2264230011",
                    "name": "Quoc Viet Hung Nguyen"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        },
        {
            "paperId": "5a8a71a24c3604e847650b3fb0379d49945fd7b6",
            "title": "Stance Detection on Social Media with Fine-Tuned Large Language Models",
            "abstract": "Stance detection, a key task in natural language processing, determines an author's viewpoint based on textual analysis. This study evaluates the evolution of stance detection methods, transitioning from early machine learning approaches to the groundbreaking BERT model, and eventually to modern Large Language Models (LLMs) such as ChatGPT, LLaMa-2, and Mistral-7B. While ChatGPT's closed-source nature and associated costs present challenges, the open-source models like LLaMa-2 and Mistral-7B offers an encouraging alternative. Initially, our research focused on fine-tuning ChatGPT, LLaMa-2, and Mistral-7B using several publicly available datasets. Subsequently, to provide a comprehensive comparison, we assess the performance of these models in zero-shot and few-shot learning scenarios. The results underscore the exceptional ability of LLMs in accurately detecting stance, with all tested models surpassing existing benchmarks. Notably, LLaMa-2 and Mistral-7B demonstrate remarkable efficiency and potential for stance detection, despite their smaller sizes compared to ChatGPT. This study emphasizes the potential of LLMs in stance detection and calls for more extensive research in this field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214990867",
                    "name": "Ilker G\u00fcl"
                },
                {
                    "authorId": "2875254",
                    "name": "R. Lebret"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        },
        {
            "paperId": "0766410db4a987ebebeb0eb5f132ac9f1fdd8fda",
            "title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages",
            "abstract": "Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning.The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.We utilize a cross-lingual contextualised token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora. Our code is available at https://github.com/Yasminekaroui/CliCoTea.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107084945",
                    "name": "Yasmine Karoui"
                },
                {
                    "authorId": "2875254",
                    "name": "R. Lebret"
                },
                {
                    "authorId": "9737058",
                    "name": "Negar Foroutan"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        },
        {
            "paperId": "1af16f432f124c4b5c5f67b0d6d639bfb539131c",
            "title": "Firearms on Twitter: A Novel Object Detection Pipeline",
            "abstract": "Social media is an important source of real-time imagery concerning world events. One subset of social media posts which may be of particular interest are those featuring firearms. These posts can give insight into weapon movements, troop activity and civilian safety. Object detection tools offer important opportunities for insight into these images. Unfortunately, these images can be visually complex, poorly lit and generally challenging for object detection models. We present an analysis of existing gun detection datasets, and find that these datasets to not effectively address the challenge of gun detection on real-life images. Following this, we present a novel object detection pipeline. We train our pipeline on a number of datasets including one created for this investigation made up of Twitter images of the Russo-Ukrainian War. We compare the performance of our model as trained on the different datasets to baseline numbers provided by original authors as well as a YOLO v5 benchmark. We find that our model outperforms the state-of-the-art benchmarks on contextually rich, real-life-derived imagery of firearms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2221419258",
                    "name": "Ryan Harvey"
                },
                {
                    "authorId": "2875254",
                    "name": "R. Lebret"
                },
                {
                    "authorId": "2186551984",
                    "name": "St\u00e9phane Massonnet"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                }
            ]
        },
        {
            "paperId": "36d916c261e18b91577730d10d5cd23a0464f046",
            "title": "Efficient and Effective Multi-Modal Queries through Heterogeneous Network Embedding (Extended Abstract)",
            "abstract": "Recent information retrieval (IR) systems answer a multi-modal query by considering it as a set of separate uni-modal queries. However, depending on the chosen operationalisation, such an approach is inefficient or ineffective. It either requires multiple passes over the data or leads to inaccuracies since the relations between data modalities are neglected in the relevance assessment. To mitigate these challenges, we present an IR system that has been designed to answer genuine multi-modal queries. It relies on a heterogeneous network embedding, so that features from diverse modalities can be incorporated when representing both, a query and the data over which it shall be evaluated. An experimental evaluation using diverse real-world and synthetic datasets illustrates that our approach returns twice the amount of relevant information compared to baseline techniques, while scaling to large multi-modal databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117824517",
                    "name": "T. Nguyen"
                },
                {
                    "authorId": "39822639",
                    "name": "Chi Thang Duong"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "2315762",
                    "name": "M. Weidlich"
                },
                {
                    "authorId": "2925880",
                    "name": "S. T. Mai"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                },
                {
                    "authorId": "144133815",
                    "name": "Q. Nguyen"
                }
            ]
        },
        {
            "paperId": "feea0452e03b78f7c85f40e5daa1bd08b61bb44b",
            "title": "Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models",
            "abstract": "Recent transformer language models achieve outstanding results in many natural language processing (NLP) tasks. However, their enormous size often makes them impractical on memory-constrained devices, requiring practitioners to compress them to smaller networks. In this paper, we explore offline compression methods, meaning computationally-cheap approaches that do not require further fine-tuning of the compressed model. We challenge the classical matrix factorization methods by proposing a novel, better-performing autoencoder-based framework. We perform a comprehensive ablation study of our approach, examining its different aspects over a diverse set of evaluation settings. Moreover, we show that enabling collaboration between modules across layers by compressing certain modules together positively impacts the final model performance. Experiments on various NLP tasks demonstrate that our approach significantly outperforms commonly used factorization-based offline compression methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1739201368",
                    "name": "Mohammadreza Banaei"
                },
                {
                    "authorId": "1641590550",
                    "name": "Klaudia Ba\u0142azy"
                },
                {
                    "authorId": "145172609",
                    "name": "A. Kasymov"
                },
                {
                    "authorId": "2875254",
                    "name": "R. Lebret"
                },
                {
                    "authorId": "145541197",
                    "name": "J. Tabor"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        },
        {
            "paperId": "428900d3e1059ed93a00d641421ff6edce903794",
            "title": "An Efficient Active Learning Pipeline for Legal Text Classification",
            "abstract": "Active Learning (AL) is a powerful tool for learning with less labeled data, in particular, for specialized domains, like legal documents, where unlabeled data is abundant, but the annotation requires domain expertise and is thus expensive. Recent works have shown the effectiveness of AL strategies for pre-trained language models. However, most AL strategies require a set of labeled samples to start with, which is expensive to acquire. In addition, pre-trained language models have been shown unstable during fine-tuning with small datasets, and their embeddings are not semantically meaningful. In this work, we propose a pipeline for effectively using active learning with pre-trained language models in the legal domain. To this end, we leverage the available {textit{unlabeled} data in three phases. First, we continue pre-training the model to adapt it to the downstream task. Second, we use knowledge distillation to guide the model\u2019s embeddings to a semantically meaningful space. Finally, we propose a simple, yet effective, strategy to find the initial set of labeled samples with fewer actions compared to existing methods. Our experiments on Contract-NLI, adapted to the classification task, and LEDGAR benchmarks show that our approach outperforms standard AL strategies, and is more efficient. Furthermore, our pipeline reaches comparable results to the fully-supervised approach with a small performance gap, and dramatically reduced annotation cost. Code and the adapted data will be made available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122924160",
                    "name": "Sepideh Mamooler"
                },
                {
                    "authorId": "2875254",
                    "name": "R. Lebret"
                },
                {
                    "authorId": "2186551984",
                    "name": "St\u00e9phane Massonnet"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        },
        {
            "paperId": "4cea7fb4cbaf819760aed95654f645d4fce4602b",
            "title": "Beyond S-curves: Recurrent Neural Networks for Technology Forecasting",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1642188369",
                    "name": "Alexander Glavackij"
                },
                {
                    "authorId": "29898269",
                    "name": "Dimitri Percia David"
                },
                {
                    "authorId": "122986258",
                    "name": "Alain Mermoud"
                },
                {
                    "authorId": "1910588458",
                    "name": "Angelika Romanou"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        },
        {
            "paperId": "5563c6075b35916974c052d2905f5259d2507402",
            "title": "MoZuMa: A Model Zoo for Multimedia Applications",
            "abstract": "Lots of machine learning models with applications in Multimedia Search are released as Open Source Software. However, integrating these models into an application is not always an easy task due to the lack of a consistent interface to run, train or distribute models. With MoZuMa, we aim at reducing this effort by providing a model zoo for image similarity, text-to-image retrieval, face recognition, object similarity search, video key-frames detection and multilingual text search implemented in a generic interface with a modular architecture. The code is released as Open Source Software at https://github.com/mozuma/mozuma.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186551984",
                    "name": "St\u00e9phane Massonnet"
                },
                {
                    "authorId": "1423797914",
                    "name": "Marco Romanelli"
                },
                {
                    "authorId": "2875254",
                    "name": "R. Lebret"
                },
                {
                    "authorId": "2187308638",
                    "name": "Niels Poulsen"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        },
        {
            "paperId": "bb6322d0b4bebfd483e0b91e7ae60e1e800e2cbf",
            "title": "SciLander: Mapping the Scientific News Landscape",
            "abstract": "The COVID-19 pandemic has fueled the spread of misinformation on social media and the Web as a whole.\nThe phenomenon dubbed `infodemic' has taken the challenges of information veracity and trust to new heights by massively introducing seemingly scientific and technical elements into misleading content.\nDespite the existing body of work on modeling and predicting misinformation, the coverage of very complex scientific topics with inherent uncertainty and an evolving set of findings, such as COVID-19, provides many new challenges that are not easily solved by existing tools. \nTo address these issues, we introduce SciLander, a method for learning representations of news sources reporting on science-based topics.\nWe extract four heterogeneous indicators for the sources; two generic indicators that capture (1) the copying of news stories between sources, and (2) the use of the same terms to mean different things (semantic shift), and two scientific indicators that capture (1) the usage of jargon and (2) the stance towards specific citations.\nWe use these indicators as signals of source agreement, sampling pairs of positive (similar) and negative (dissimilar) samples, and combine them in a unified framework to train unsupervised news source embeddings with a triplet margin loss objective.\nWe evaluate our method on a novel COVID-19 dataset containing nearly 1M news articles from 500 sources spanning a period of 18 months since the beginning of the pandemic in 2020.\nOur results show that the features learned by our model outperform state-of-the-art baseline methods on the task of news veracity classification.\nFurthermore, a clustering analysis suggests that the learned representations encode information about the reliability, political leaning, and partisanship bias of these sources.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "51034107",
                    "name": "Mauricio G. Gruppi"
                },
                {
                    "authorId": "3314639",
                    "name": "Panayiotis Smeros"
                },
                {
                    "authorId": "3139418",
                    "name": "Sibel Adali"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                }
            ]
        }
    ]
}