{
    "authorId": "26956796",
    "papers": [
        {
            "paperId": "bfbd0718dacf3eb6aa1a21ac491579904db99cc4",
            "title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
            "abstract": "The integration of Large Language Models (LLMs) into the drug discovery and development field marks a significant paradigm shift, offering novel methodologies for understanding disease mechanisms, facilitating drug discovery, and optimizing clinical trial processes. This review highlights the expanding role of LLMs in revolutionizing various stages of the drug development pipeline. We investigate how these advanced computational models can uncover target-disease linkage, interpret complex biomedical data, enhance drug molecule design, predict drug efficacy and safety profiles, and facilitate clinical trial processes. Our paper aims to provide a comprehensive overview for researchers and practitioners in computational biology, pharmacology, and AI4Science by offering insights into the potential transformative impact of LLMs on drug discovery and development.",
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2134585717",
                    "name": "Huan Yee Koh"
                },
                {
                    "authorId": "2320290334",
                    "name": "Maddie Yang"
                },
                {
                    "authorId": "2320348966",
                    "name": "Li Li"
                },
                {
                    "authorId": "2243970839",
                    "name": "Lauren T. May"
                },
                {
                    "authorId": "2243967976",
                    "name": "Geoffrey I. Webb"
                },
                {
                    "authorId": "2256999516",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2312726184",
                    "name": "George M Church"
                }
            ]
        },
        {
            "paperId": "47d819f79d44d9cb5145b92e438f7b2a773451e8",
            "title": "A Survey on Fairness-aware Recommender Systems",
            "abstract": "As information filtering services, recommender systems have extremely enriched our daily life by providing personalized suggestions and facilitating people in decision-making, which makes them vital and indispensable to human society in the information era. However, as people become more dependent on them, recent studies show that recommender systems potentially own unintentional impacts on society and individuals because of their unfairness (e.g., gender discrimination in job recommendations). To develop trustworthy services, it is crucial to devise fairness-aware recommender systems that can mitigate these bias issues. In this survey, we summarise existing methodologies and practices of fairness in recommender systems. Firstly, we present concepts of fairness in different recommendation scenarios, comprehensively categorize current advances, and introduce typical methods to promote fairness in different stages of recommender systems. Next, after introducing datasets and evaluation metrics applied to assess the fairness of recommender systems, we will delve into the significant influence that fairness-aware recommender systems exert on real-world industrial applications. Subsequently, we highlight the connection between fairness and other principles of trustworthy recommender systems, aiming to consider trustworthiness principles holistically while advocating for fairness. Finally, we summarize this review, spotlighting promising opportunities in comprehending concepts, frameworks, the balance between accuracy and fairness, and the ties with trustworthiness, with the ultimate goal of fostering the development of fairness-aware recommender systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "82527705",
                    "name": "Luzhi Wang"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2218582225",
                    "name": "Weiping Ding"
                },
                {
                    "authorId": "2218637525",
                    "name": "Feng Xia"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "4885a3f1bb3aceea208df1b92af5025ea72158df",
            "title": "Contrastive Graph Similarity Networks",
            "abstract": "Graph similarity learning is a significant and fundamental issue in the theory and analysis of graphs, which has been applied in a variety of fields, including object tracking, recommender systems, similarity search, and so on. Recent methods for graph similarity learning that utilize deep learning typically share two deficiencies: (1) they leverage graph neural networks as backbones for learning graph representations but have not well captured the complex information inside data, and (2) they employ a cross-graph attention mechanism for graph similarity learning, which is computationally expensive. Taking these limitations into consideration, a method for graph similarity learning is devised in this study, namely, Contrastive Graph Similarity Network (CGSim). To enhance graph similarity learning, CGSim makes use of the complementary information of two input graphs and captures pairwise relations in a contrastive learning framework. By developing a dual contrastive learning module with a node-graph matching and a graph-graph matching mechanism, our method significantly reduces the quadratic time complexity for cross-graph interaction modeling to linear time complexity. Jointly learning in an end-to-end framework, the graph representation embedding module and the well-designed contrastive learning module can be beneficial to one another. A comprehensive series of experiments indicate that CGSim outperforms state-of-the-art baselines on six datasets and significantly reduces the computational cost, which demonstrates our CGSim model\u2019s superiority over other baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "82527705",
                    "name": "Luzhi Wang"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2203372264",
                    "name": "Fuyi Li"
                },
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "64f52b64d7fdd0f2e83f696d02481d0957849795",
            "title": "Dual Intent Enhanced Graph Neural Network for Session-based New Item Recommendation",
            "abstract": "Recommender systems are essential to various fields, e.g., e-commerce, e-learning, and streaming media. At present, graph neural networks (GNNs) for session-based recommendations normally can only recommend items existing in users\u2019 historical sessions. As a result, these GNNs have difficulty recommending items that users have never interacted with (new items), which leads to a phenomenon of information cocoon. Therefore, it is necessary to recommend new items to users. As there is no interaction between new items and users, we cannot include new items when building session graphs for GNN session-based recommender systems. Thus, it is challenging to recommend new items for users when using GNN-based methods. We regard this challenge as \u201cGNN Session-based New Item Recommendation (GSNIR)\u201d. To solve this problem, we propose a dual-intent enhanced graph neural network for it. Due to the fact that new items are not tied to historical sessions, the users\u2019 intent is difficult to predict. We design a dual-intent network to learn user intent from an attention mechanism and the distribution of historical data respectively, which can simulate users\u2019 decision-making process in interacting with a new item. To solve the challenge that new items cannot be learned by GNNs, inspired by zero-shot learning (ZSL), we infer the new item representation in GNN space by using their attributes. By outputting new item probabilities, which contain recommendation scores of the corresponding items, the new items with higher scores are recommended to users. Experiments on two representative real-world datasets show the superiority of our proposed method. The case study from the real-world verifies interpretability benefits brought by the dual-intent module and the new item reasoning module.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152284495",
                    "name": "Di Jin"
                },
                {
                    "authorId": "82527705",
                    "name": "Luzhi Wang"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2090871",
                    "name": "Guojie Song"
                },
                {
                    "authorId": "2167028948",
                    "name": "Fei Jiang"
                },
                {
                    "authorId": "47875796",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "71666773",
                    "name": "Wei Lin"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "89ba59ab6b086d86ce27df32652b7499498d70fb",
            "title": "Integrating Graphs With Large Language Models: Methods and Prospects",
            "abstract": "Large language models (LLMs) such as Generative Pre-trained Transformer 4 have emerged as frontrunners, showcasing unparalleled prowess in diverse applications including answering queries, code generation, and more. Parallelly, graph-structured data, intrinsic data types, are pervasive in real-world scenarios. Merging the capabilities of LLMs with graph-structured data has been a topic of keen interest. This article bifurcates such integrations into two predominant categories. The first leverages LLMs for graph learning, where LLMs can not only augment existing graph algorithms but also stand as prediction models for various graph tasks. Conversely, the second category underscores the pivotal role of graphs in advancing LLMs. Mirroring human cognition, we solve complex tasks by adopting graphs in either reasoning or collaboration. Integrating with such structures can significantly boost the performance of LLMs in various complicated tasks. We also discuss and propose open questions for integrating LLMs with graph-structured data for the future direction of the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256999516",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2215634631",
                    "name": "San Murugesan"
                }
            ]
        },
        {
            "paperId": "9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632",
            "title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
            "abstract": "Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of\"knowledge\", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this\"knowledge\"by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2134585717",
                    "name": "Huan Yee Koh"
                },
                {
                    "authorId": "2237806895",
                    "name": "Jiaxin Ju"
                },
                {
                    "authorId": "12245599",
                    "name": "A. T. Nguyen"
                },
                {
                    "authorId": "2243970839",
                    "name": "Lauren T. May"
                },
                {
                    "authorId": "2243967976",
                    "name": "Geoffrey I. Webb"
                },
                {
                    "authorId": "2256999516",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "ebfa6bfa3251bd516d68e2854ef9258ff0653d02",
            "title": "PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection",
            "abstract": "Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies - reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreover, our method demonstrated robustness and effectiveness in five datasets. Notably, when validated on the ACM dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training speed, and sharply reduce memory usage compared to the most efficient baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260294610",
                    "name": "Junjun Pan"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2256999516",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "70f091c93721bdb384f7488a7d3e68228d2bd69f",
            "title": "CGMN: A Contrastive Graph Matching Network for Self-Supervised Graph Similarity Learning",
            "abstract": "Graph similarity learning refers to calculating the similarity score between two graphs, which is required in many realistic applications, such as visual tracking, graph classification, and collaborative filtering. As most of the existing graph neural networks yield effective graph representations of a single graph, little effort has been made for jointly learning two graph representations and calculating their similarity score. In addition, existing unsupervised graph similarity learning methods are mainly clustering-based, which ignores the valuable information embodied in graph pairs. To this end, we propose a contrastive graph matching network (CGMN) for self-supervised graph similarity learning in order to calculate the similarity between any two input graph objects. Specifically, we generate two augmented views for each graph in a pair respectively. Then, we employ two strategies, namely cross-view interaction and cross-graph interaction, for effective node representation learning. The former is resorted to strengthen the consistency of node representations in two views. The latter is utilized to identify node differences between different graphs. Finally, we transform node representations into graph-level representations via pooling operations for graph similarity computation. We have evaluated CGMN on eight real-world datasets, and the experiment results show that the proposed new approach is superior to the state-of-the-art methods in graph similarity learning downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "82527705",
                    "name": "Luzhi Wang"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "47875796",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2167028948",
                    "name": "Fei Jiang"
                },
                {
                    "authorId": "71666773",
                    "name": "Wei Lin"
                },
                {
                    "authorId": "2585415",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "aa145398718a09f9e93bb96940f935d46bfed018",
            "title": "Rethinking and Scaling Up Graph Contrastive Learning: An Extremely Efficient Approach with Group Discrimination",
            "abstract": "Graph contrastive learning (GCL) alleviates the heavy reliance on label information for graph representation learning (GRL) via self-supervised learning schemes. The core idea is to learn by maximising mutual information for similar instances, which requires similarity computation between two node instances. However, GCL is inefficient in both time and memory consumption. In addition, GCL normally requires a large number of training epochs to be well-trained on large-scale datasets. Inspired by an observation of a technical defect (i.e., inappropriate usage of Sigmoid function) commonly used in two representative GCL works, DGI and MVGRL, we revisit GCL and introduce a new learning paradigm for self-supervised graph representation learning, namely, Group Discrimination (GD), and propose a novel GD-based method called Graph Group Discrimination (GGD). Instead of similarity computation, GGD directly discriminates two groups of node samples with a very simple binary cross-entropy loss. In addition, GGD requires much fewer training epochs to obtain competitive performance compared with GCL methods on large-scale datasets. These two advantages endow GGD with very efficient property. Extensive experiments show that GGD outperforms state-of-the-art self-supervised methods on eight datasets. In particular, GGD can be trained in 0.18 seconds (6.44 seconds including data preprocessing) on ogbn-arxiv, which is orders of magnitude (10,000+) faster than GCL baselines while consuming much less memory. Trained with 9 hours on ogbn-papers100M with billion edges, GGD outperforms its GCL counterparts in both accuracy and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2585415",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2158117746",
                    "name": "Vincent C. S. Lee"
                },
                {
                    "authorId": "2149514422",
                    "name": "Yu Zheng"
                },
                {
                    "authorId": "152297693",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "b29aec89e64bb20f2b963e5615c79b9008ecfd88",
            "title": "Beyond Smoothing: Unsupervised Graph Representation Learning with Edge Heterophily Discriminating",
            "abstract": "Unsupervised graph representation learning (UGRL) has drawn increasing research attention and achieved promising results in several graph analytic tasks. Relying on the homophily assumption, existing UGRL methods tend to smooth the learned node representations along all edges, ignoring the existence of heterophilic edges that connect nodes with distinct attributes. As a result, current methods are hard to generalize to heterophilic graphs where dissimilar nodes are widely connected, and also vulnerable to adversarial attacks. To address this issue, we propose a novel unsupervised Graph Representation learning method with Edge hEterophily discriminaTing (GREET) which learns representations by discriminating and leveraging homophilic edges and heterophilic edges. To distinguish two types of edges, we build an edge discriminator that infers edge homophily/heterophily from feature and structure information. We train the edge discriminator in an unsupervised way through minimizing the crafted pivot-anchored ranking loss, with randomly sampled node pairs acting as pivots. Node representations are learned through contrasting the dual-channel encodings obtained from the discriminated homophilic and heterophilic edges. With an effective interplaying scheme, edge discriminating and representation learning can mutually boost each other during the training phase. We conducted extensive experiments on 14 benchmark datasets and multiple learning scenarios to demonstrate the superiority of GREET.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116018493",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2589584",
                    "name": "Daokun Zhang"
                },
                {
                    "authorId": "2187919914",
                    "name": "V. Lee"
                },
                {
                    "authorId": "2585415",
                    "name": "Shirui Pan"
                }
            ]
        }
    ]
}