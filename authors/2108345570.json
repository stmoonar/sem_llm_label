{
    "authorId": "2108345570",
    "papers": [
        {
            "paperId": "7498c80eb1c4765472133aa29d92661cc7612f7c",
            "title": "Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic",
            "abstract": "Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111682654",
                    "name": "Connor Pryor"
                },
                {
                    "authorId": "2117782275",
                    "name": "Quan Yuan"
                },
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "2173102324",
                    "name": "Mehran Kazemi"
                },
                {
                    "authorId": "143812128",
                    "name": "Deepak Ramachandran"
                },
                {
                    "authorId": "2113816700",
                    "name": "Tania Bedrax-Weiss"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                }
            ]
        },
        {
            "paperId": "2361f6eb6ebdd65048dd14829fb9218db3fd2e98",
            "title": "Uncertainty Estimation and Out-of-Distribution Detection for Deep Learning-Based Image Reconstruction Using the Local Lipschitz",
            "abstract": "Accurate image reconstruction is at the heart of diagnostics in medical imaging. Supervised deep learning-based approaches have been investigated for solving inverse problems including image reconstruction. However, these trained models encounter unseen data distributions that are widely shifted from training data during deployment. Therefore, it is essential to assess whether a given input falls within the training data distribution. Current uncertainty estimation approaches focus on providing an uncertainty map to radiologists, rather than assessing the training distribution fit. In this work, we propose a method based on the local Lipschitz metric to distinguish out-of-distribution images from in-distribution with an area under the curve of 99.94% for True Positive Rate versus False Positive Rate. We demonstrate a very strong relationship between the local Lipschitz value and mean absolute error (MAE), supported by a Spearman's rank correlation coefficient of 0.8475, to determine an uncertainty estimation threshold for optimal performance. Through the identification of false positives, we demonstrate the local Lipschitz and MAE relationship can guide data augmentation and reduce uncertainty. Our study was validated using the AUTOMAP architecture for sensor-to-image Magnetic Resonance Imaging (MRI) reconstruction. We demonstrate our approach outperforms baseline techniques of Monte-Carlo dropout and deep ensembles as well as the state-of-the-art Mean Variance Estimation network approach. We expand our application scope to MRI denoising and Computed Tomography sparse-to-full view reconstructions using UNET architectures. We show our approach is applicable to various architectures and applications, especially in medical imaging, where preserving diagnostic accuracy of reconstructed images remains paramount.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "81229584",
                    "name": "D. Bhutto"
                },
                {
                    "authorId": "2118250848",
                    "name": "Bo Zhu"
                },
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "4287872",
                    "name": "Neha Koonjoo"
                },
                {
                    "authorId": "2268417358",
                    "name": "H. Li"
                },
                {
                    "authorId": "2161154755",
                    "name": "B. Rosen"
                },
                {
                    "authorId": "144148239",
                    "name": "M. Rosen"
                }
            ]
        },
        {
            "paperId": "296cad46f56329843ded4a8b7d7633c9d436f113",
            "title": "On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study",
            "abstract": "Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world applications. Probabilistic deep learning methods are common solutions to the miscalibration problem. However, their relative effectiveness in complex autoregressive summarization tasks are not well-understood. In this work, we thoroughly investigate different state-of-the-art probabilistic methods' effectiveness in improving the uncertainty quality of the neural summarization models, across three large-scale benchmarks with varying difficulty. We show that the probabilistic methods consistently improve the model's generation and uncertainty quality, leading to improved selective generation performance (i.e., abstaining from low-quality summaries) in practice. We also reveal notable failure patterns of probabilistic methods widely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout), cautioning the importance of choosing appropriate method for the data setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7164154",
                    "name": "Polina Zablotskaia"
                },
                {
                    "authorId": "2104794081",
                    "name": "Du Phan"
                },
                {
                    "authorId": "2124977868",
                    "name": "Joshua Maynez"
                },
                {
                    "authorId": "143790499",
                    "name": "Shashi Narayan"
                },
                {
                    "authorId": "2205548664",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                }
            ]
        },
        {
            "paperId": "67c0b0f0b1efb2e25084d23ed310109fb3c451aa",
            "title": "Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via Compositional Uncertainty Quantification",
            "abstract": "Pre-trained seq2seq models excel at graph semantic parsing with rich annotated data, but generalize worse to out-of-distribution (OOD) and long-tail examples. In comparison, symbolic parsers under-perform on population-level metrics, but exhibit unique strength in OOD and tail generalization. In this work, we study compositionality-aware approach to neural-symbolic inference informed by model confidence, performing fine-grained neural-symbolic reasoning at subgraph level (i.e., nodes and edges) and precisely targeting subgraph components with high uncertainty in the neural parser. As a result, the method combines the distinct strength of the neural and symbolic approaches in capturing different aspects of the graph prediction, leading to well-rounded generalization performance both across domains and in the tail. We empirically investigate the approach in the English Resource Grammar (ERG) parsing problem on a diverse suite of standard in-domain and seven OOD corpora. Our approach leads to 35.26% and 35.60% error reduction in aggregated SMATCH score over neural and symbolic approaches respectively, and 14% absolute accuracy gain in key tail linguistic categories over the neural model, outperforming prior state-of-art methods that do not account for compositionality or uncertainty.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112304965",
                    "name": "Zi Lin"
                },
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "877e27a1d89095fcf686ab675f62a8432d3285ee",
            "title": "A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models",
            "abstract": "Contrastively trained text-image models have the remarkable ability to perform zero-shot classification, that is, classifying previously unseen images into categories that the model has never been explicitly trained to identify. However, these zero-shot classifiers need prompt engineering to achieve high accuracy. Prompt engineering typically requires hand-crafting a set of prompts for individual downstream tasks. In this work, we aim to automate this prompt engineering and improve zero-shot accuracy through prompt ensembling. In particular, we ask\"Given a large pool of prompts, can we automatically score the prompts and ensemble those that are most suitable for a particular downstream dataset, without needing access to labeled validation data?\". We demonstrate that this is possible. In doing so, we identify several pathologies in a naive prompt scoring method where the score can be easily overconfident due to biases in pre-training and test data, and we propose a novel prompt scoring method that corrects for the biases. Using our proposed scoring method to create a weighted average prompt ensemble, our method outperforms equal average ensemble, as well as hand-crafted prompts, on ImageNet, 4 of its variants, and 11 fine-grained classification benchmarks, all while being fully automatic, optimization-free, and not requiring access to labeled validation data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1491706991",
                    "name": "J. Allingham"
                },
                {
                    "authorId": "2205548664",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "144477225",
                    "name": "Michael W. Dusenberry"
                },
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "10734287",
                    "name": "Xiuye Gu"
                },
                {
                    "authorId": "50355189",
                    "name": "Yin Cui"
                },
                {
                    "authorId": "47497262",
                    "name": "Dustin Tran"
                },
                {
                    "authorId": "40627523",
                    "name": "Balaji Lakshminarayanan"
                }
            ]
        },
        {
            "paperId": "995c3f5b6776d58e5dd9abfe62f0435751dc9e48",
            "title": "On Compositional Uncertainty Quantification for Seq2seq Graph Parsing",
            "abstract": "Recent years have witnessed the success of applying seq2seq models to graph parsing tasks, where the outputs are compositionally structured (e.g., a graph or a tree). However, these seq2seq approaches pose a challenge in quantifying the model\u2019s compositional uncertainty on graph structures due to the gap between seq2seq output probability and structural probability on the graph. This work is the first to quantify and evaluate compositional uncertainty for seq2seq graph parsing tasks. First, we proposed a generic, probabilistically interpretable framework that allows correspondences between seq2seq output probability to structural probability on the graph. This framework serves as a powerful medium for quantifying a seq2seq model\u2019s compositional uncertainty on graph elements (i.e., nodes or edges). Second, to evaluate uncertainty quality in terms of calibration, we propose a novel metric called Compositional Expected Calibration Error (CECE) which can measure a model\u2019s calibration behavior in predicting graph structures. By a thorough evaluation for compositional uncertainty on three different tasks across ten domains, we demonstrate that CECE is a better reflection for distributional shift compared to vanilla sequence ECE. Finally, we validate the effectiveness of compositional uncertainty considering the task of collaborative semantic parsing, where the model is allowed to send limited subgraphs for human review. The results show that the collaborative performance based on uncertain subgraph selection consistently outperforms random subgraph selection (30% average error reduction rate) and performs comparably to oracle subgraph selection (only 0.33 difference in average prediction error), indicating that compositional uncertainty is an ideal signal for model errors and can benefit various downstream tasks. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143872641",
                    "name": "Zi Lin"
                },
                {
                    "authorId": "2104794081",
                    "name": "Du Phan"
                },
                {
                    "authorId": "2616463",
                    "name": "Panupong Pasupat"
                },
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "b1ab0635586fc7677f9a54590e56dcb0717db664",
            "title": "Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play",
            "abstract": "Standard empirical risk minimization (ERM) training can produce deep neural network (DNN) models that are accurate on average but under-perform in under-represented population subgroups, especially when there are imbalanced group distributions in the long-tailed training data. Therefore, approaches that improve the accuracy-group robustness trade-off frontier of a DNN model (i.e. improving worst-group accuracy without sacrificing average accuracy, or vice versa) is of crucial importance. Uncertainty-based active learning (AL) can potentially improve the frontier by preferentially sampling underrepresented subgroups to create a more balanced training dataset. However, the quality of uncertainty estimates from modern DNNs tend to degrade in the presence of spurious correlations and dataset bias, compromising the effectiveness of AL for sampling tail groups. In this work, we propose Introspective Self-play (ISP), a simple approach to improve the uncertainty estimation of a deep neural network under dataset bias, by adding an auxiliary introspection task requiring a model to predict the bias for each data point in addition to the label. We show that ISP provably improves the bias-awareness of the model representation and the resulting uncertainty estimates. On two real-world tabular and language tasks, ISP serves as a simple\"plug-in\"for AL model training, consistently improving both the tail-group sampling rate and the final accuracy-fairness trade-off frontier of popular AL methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "1729912",
                    "name": "Krishnamurthy Dvijotham"
                },
                {
                    "authorId": "2108586983",
                    "name": "Jihyeon Lee"
                },
                {
                    "authorId": "1642024996",
                    "name": "Quan Yuan"
                },
                {
                    "authorId": "35149351",
                    "name": "Martin Strobel"
                },
                {
                    "authorId": "40627523",
                    "name": "Balaji Lakshminarayanan"
                },
                {
                    "authorId": "143812128",
                    "name": "Deepak Ramachandran"
                }
            ]
        },
        {
            "paperId": "3e60cba99b4e8a45f2e3ba3df462ac949a720833",
            "title": "Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty",
            "abstract": "Recent work in task-independent graph semantic parsing has shifted from grammar-based symbolic approaches to neural models, showing strong performance on different types of meaning representations. However, it is still unclear that what are the limitations of these neural parsers, and whether these limitations can be compensated by incorporating symbolic knowledge into model inference. In this paper, we address these questions by taking English Resource Grammar (ERG) parsing as a case study. Specifically, we first develop a state-of-the-art, T5-based neural ERG parser, and conduct detail analyses of parser performance within fine-grained linguistic categories.The neural parser attains superior performance on in-distribution test set, but degrades significantly on long-tail situations, while the symbolic parser performs more robustly. To address this, we further propose a simple yet principled collaborative framework for neural-symbolic semantic parsing, by designing a decision criterion for beam search that incorporates the prior knowledge from a symbolic parser and accounts for model uncertainty. Experimental results show that the proposed framework yields comprehensive improvement over neural baseline across long-tail categories, yielding the best known Smatch score (97.01) on the well-studied DeepBank benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143872641",
                    "name": "Zi Lin"
                },
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "60f2ae3b448b035e957603e84d1a073ad708879b",
            "title": "A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness",
            "abstract": "Accurate uncertainty quantification is a major challenge in deep learning, as neural networks can make overconfident errors and assign high confidence predictions to out-of-distribution (OOD) inputs. The most popular approaches to estimate predictive uncertainty in deep learning are methods that combine predictions from multiple neural networks, such as Bayesian neural networks (BNNs) and deep ensembles. However their practicality in real-time, industrial-scale applications are limited due to the high memory and computational cost. Furthermore, ensembles and BNNs do not necessarily fix all the issues with the underlying member networks. In this work, we study principled approaches to improve uncertainty property of a single network, based on a single, deterministic representation. By formalizing the uncertainty quantification as a minimax learning problem, we first identify distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs with two simple changes: (1) applying spectral normalization to hidden weights to enforce bi-Lipschitz smoothness in representations and (2) replacing the last output layer with a Gaussian process layer. On a suite of vision and language understanding benchmarks, SNGP outperforms other single-model approaches in prediction, calibration and out-of-domain detection. Furthermore, SNGP provides complementary benefits to popular techniques such as deep ensembles and data augmentation, making it a simple and scalable building block for probabilistic deep learning. Code is open-sourced at https://github.com/google/uncertainty-baselines",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "151216770",
                    "name": "Shreyas Padhy"
                },
                {
                    "authorId": "92095972",
                    "name": "Jie Jessie Ren"
                },
                {
                    "authorId": "143872641",
                    "name": "Zi Lin"
                },
                {
                    "authorId": "38356166",
                    "name": "Yeming Wen"
                },
                {
                    "authorId": "3451901",
                    "name": "Ghassen Jerfel"
                },
                {
                    "authorId": "81408931",
                    "name": "Zachary Nado"
                },
                {
                    "authorId": "144108062",
                    "name": "Jasper Snoek"
                },
                {
                    "authorId": "47497262",
                    "name": "Dustin Tran"
                },
                {
                    "authorId": "40627523",
                    "name": "Balaji Lakshminarayanan"
                }
            ]
        },
        {
            "paperId": "9da634823416e96417530f0a2197b9a4936eee3e",
            "title": "Plex: Towards Reliability using Pretrained Large Model Extensions",
            "abstract": "A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 40 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it improves the out-of-the-box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "47497262",
                    "name": "Dustin Tran"
                },
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "144477225",
                    "name": "Michael W. Dusenberry"
                },
                {
                    "authorId": "2104794081",
                    "name": "Du Phan"
                },
                {
                    "authorId": "153247100",
                    "name": "Mark Collier"
                },
                {
                    "authorId": "92095972",
                    "name": "Jie Jessie Ren"
                },
                {
                    "authorId": "10360536",
                    "name": "Kehang Han"
                },
                {
                    "authorId": "1687515",
                    "name": "Z. Wang"
                },
                {
                    "authorId": "1867856",
                    "name": "Zelda E. Mariet"
                },
                {
                    "authorId": "38488570",
                    "name": "Huiyi Hu"
                },
                {
                    "authorId": "1726096678",
                    "name": "Neil Band"
                },
                {
                    "authorId": "2107677984",
                    "name": "Tim G. J. Rudner"
                },
                {
                    "authorId": "122902793",
                    "name": "K. Singhal"
                },
                {
                    "authorId": "81408931",
                    "name": "Zachary Nado"
                },
                {
                    "authorId": "3038326",
                    "name": "Joost R. van Amersfoort"
                },
                {
                    "authorId": "145408381",
                    "name": "Andreas Kirsch"
                },
                {
                    "authorId": "2068720",
                    "name": "Rodolphe Jenatton"
                },
                {
                    "authorId": "2665391",
                    "name": "Nithum Thain"
                },
                {
                    "authorId": "2046816002",
                    "name": "Honglin Yuan"
                },
                {
                    "authorId": "144136748",
                    "name": "E. Kelly Buchanan"
                },
                {
                    "authorId": "2056418611",
                    "name": "Kevin Murphy"
                },
                {
                    "authorId": "1733143",
                    "name": "D. Sculley"
                },
                {
                    "authorId": "2681954",
                    "name": "Y. Gal"
                },
                {
                    "authorId": "1983575",
                    "name": "Z. Ghahramani"
                },
                {
                    "authorId": "144108062",
                    "name": "Jasper Snoek"
                },
                {
                    "authorId": "40627523",
                    "name": "Balaji Lakshminarayanan"
                }
            ]
        }
    ]
}