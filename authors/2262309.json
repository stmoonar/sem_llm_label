{
    "authorId": "2262309",
    "papers": [
        {
            "paperId": "193d21d8b2ed44ab3fe06d880631822f974310ee",
            "title": "Recent Advances on Machine Learning for Computational Fluid Dynamics: A Survey",
            "abstract": "This paper explores the recent advancements in enhancing Computational Fluid Dynamics (CFD) tasks through Machine Learning (ML) techniques. We begin by introducing fundamental concepts, traditional methods, and benchmark datasets, then examine the various roles ML plays in improving CFD. The literature systematically reviews papers in recent five years and introduces a novel classification for forward modeling: Data-driven Surrogates, Physics-Informed Surrogates, and ML-assisted Numerical Solutions. Furthermore, we also review the latest ML methods in inverse design and control, offering a novel classification and providing an in-depth discussion. Then we highlight real-world applications of ML for CFD in critical scientific and engineering disciplines, including aerodynamics, combustion, atmosphere&ocean science, biology fluid, plasma, symbolic regression, and reduced order modeling. Besides, we identify key challenges and advocate for future research directions to address these challenges, such as multi-scale representation, physical knowledge encoding, scientific foundation model and automatic scientific discovery. This review serves as a guide for the rapidly expanding ML for CFD community, aiming to inspire insights for future advancements. We draw the conclusion that ML is poised to significantly transform CFD research by enhancing simulation accuracy, reducing computational time, and enabling more complex analyses of fluid dynamics. The paper resources can be viewed at https://github.com/WillDreamer/Awesome-AI4CFD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287754681",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2257354179",
                    "name": "Yadi Cao"
                },
                {
                    "authorId": "12318198",
                    "name": "Zijie Huang"
                },
                {
                    "authorId": "2316666972",
                    "name": "Yuxuan Liu"
                },
                {
                    "authorId": "2162366648",
                    "name": "Peiyan Hu"
                },
                {
                    "authorId": "2241568233",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2316672978",
                    "name": "Zezheng Song"
                },
                {
                    "authorId": "2257129965",
                    "name": "Wanjia Zhao"
                },
                {
                    "authorId": "2316666314",
                    "name": "Jilin Liu"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2280889006",
                    "name": "Long Wei"
                },
                {
                    "authorId": "2273332939",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2279761135",
                    "name": "Tailin Wu"
                },
                {
                    "authorId": "2273345182",
                    "name": "Zhi-Ming Ma"
                },
                {
                    "authorId": "2260436235",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "b38ae14be97018d917c7ce28bd6768453de7d6fc",
            "title": "ROSE: Relational and Prototypical Structure Learning for Universal Domain Adaptive Hashing",
            "abstract": "As an important problem in searching system development, domain adaptive retrieval seeks to train a retrieval model with both labeled source samples and unlabeled target samples. Although several domain adaptive hashing algorithms have been proposed to handle the problem with high efficiency, they often presume that source and target domains share all classes. However, prior knowledge about the label space on the target domain is hard to obtain in reality. To tackle this, we study a novel and challenging problem of universal domain adaptive retrieval, which evidently increases the difficulty of effective domain alignment. In this paper, we propose a hashing method named Relational and prOtotypical Structure lEarning (ROSE) to solve the problem. In particular, to overcome domain shift, we construct a relational structure depicting cross-domain similar pairs based on ranking statistics, then learn from the structure by maximizing the similarity between similar pairs compared with challenging negatives. Moreover, target private samples are detected using the min-max criterion, which helps to construct hashing prototypes in the Hamming space. On this basis, we combine prototypical structure learning with online clustering in the Hamming space, which improves target semantic learning under label deficiency. Extensive experiments on several benchmarks demonstrate that our proposed ROSE significantly outperforms a wide range of state-of-the-art methods. Our source code is available at https://github.com/WillDreamer/Rose.git.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265575121",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2287754681",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2316779096",
                    "name": "Yijia Xiao"
                },
                {
                    "authorId": "2187310139",
                    "name": "Wei Xiang"
                },
                {
                    "authorId": "2257378991",
                    "name": "Chong Chen"
                },
                {
                    "authorId": "2238119871",
                    "name": "Xian-Sheng Hua"
                },
                {
                    "authorId": "2241568233",
                    "name": "Xiao Luo"
                }
            ]
        },
        {
            "paperId": "e8043acbd13ac25abf977929416731acced0ea45",
            "title": "DIOR: Learning to Hash With Label Noise Via Dual Partition and Contrastive Learning",
            "abstract": "Due to the excellent computing efficiency, learning to hash has acquired broad popularity for Big Data retrieval. Although supervised hashing methods have achieved promising performance recently, they presume that all training samples are appropriately annotated. Unfortunately, label noise is ubiquitous owing to erroneous annotations in real-world applications, which could seriously deteriorate the retrieval performance due to imprecise supervised guidance and severe memorization of noisy data. Here we propose a comprehensive method DIOR to handle the difficulties of learning to hash with label noise. DIOR performs partitions from two complementary levels, namely sample level and parameter level. On the one hand, DIOR divides the dataset into a labeled set with clean samples and an unlabeled set with noisy samples using an ensemble of perturbed views. Then we train the network in a contrastive semi-supervised manner by reconstructing label embeddings for both reliable supervision of clean data and sufficient exploration of noisy data. On the other hand, inspired by recent pruning techniques, DIOR divides the parameters in the hashing network into crucial parameters and non-crucial parameters, and then optimizes them separately to reduce the overfitting of noisy data. Extensive experiments on four popular benchmark datasets demonstrate the effectiveness of DIOR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591322",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2229011998",
                    "name": "Huiyu Jiang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "48240541",
                    "name": "C. Chen"
                },
                {
                    "authorId": "2238119871",
                    "name": "Xian-Sheng Hua"
                },
                {
                    "authorId": "2241568233",
                    "name": "Xiao Luo"
                }
            ]
        },
        {
            "paperId": "0c7ce5898dab92da540457b754254d72b8592fc2",
            "title": "Parameter-efficient Tuning of Large-scale Multimodal Foundation Model",
            "abstract": "Driven by the progress of large-scale pre-training, parameter-efficient transfer learning has gained immense popularity across different subfields of Artificial Intelligence. The core is to adapt the model to downstream tasks with only a small set of parameters. Recently, researchers have leveraged such proven techniques in multimodal tasks and achieve promising results. However, two critical issues remain unresolved: how to further reduce the complexity with lightweight design and how to boost alignment between modalities under extremely low parameters. In this paper, we propose A graceful prompt framework for cross-modal transfer (Aurora) to overcome these challenges. Considering the redundancy in existing architectures, we first utilize the mode approximation to generate 0.1M trainable parameters to implement the multimodal prompt tuning, which explores the low intrinsic dimension with only 0.04% parameters of the pre-trained model. Then, for better modality alignment, we propose the Informative Context Enhancement and Gated Query Transformation module under extremely few parameters scenes. A thorough evaluation on six cross-modal benchmarks shows that it not only outperforms the state-of-the-art but even outperforms the full fine-tuning approach. Our code is available at: https://github.com/WillDreamer/Aurora.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2150441370",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "29a7644583d7042c4476af126f3fe0a372897abe",
            "title": "LION: Implicit Vision Prompt Tuning",
            "abstract": "Despite recent promising performances across a range of vision tasks, vision Transformers still have an issue of high computational costs.\nRecently, vision prompt learning has provided an economical solution to this problem without fine-tuning the whole large-scale model. \nHowever, the efficiency and effectiveness of existing models are still far from satisfactory due to the parameter cost of extensive prompt blocks and tricky prompt framework designs. \nIn this paper, we propose a light-weight prompt framework named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep implicit models with stable low memory costs for various complex tasks.\nIn particular, we merely insect two equilibrium implicit layers in two ends of the pre-trained backbone with parameters frozen. Moreover, according to the lottery hypothesis, we further prune the parameters to relieve the computation burden in implicit layers. Various experiments have validated that our LION obtains promising performances on a wide range of datasets. Most importantly, LION reduces up to 11.5 % of training parameter numbers while obtaining higher performance than the state-of-the-art VPT, especially under challenging scenes. Furthermore, we find that our proposed LION has an excellent generalization performance, making it an easy way to boost transfer learning in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "33383055",
                    "name": "Zhouchen Lin"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "2e7e6280e1ef7e2acb4b11f801b06975548538ae",
            "title": "Exploiting Pseudo Image Captions for Multimodal Summarization",
            "abstract": "Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1657285750",
                    "name": "Chaoya Jiang"
                },
                {
                    "authorId": "2143721734",
                    "name": "Rui Xie"
                },
                {
                    "authorId": "145235143",
                    "name": "Wei Ye"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                }
            ]
        },
        {
            "paperId": "852ab98a42204ea905ba41b3b1354820be4f201c",
            "title": "When Parameter-efficient Tuning Meets General-purpose Vision-language Models",
            "abstract": "Instruction tuning has shown promising potential for developing general-purpose AI capabilities by using large-scale pre-trained models and boosts growing research to integrate multimodal information for creative applications. However, existing works still face two main limitations: the high training costs and heavy computing resource dependence of full model fine-tuning, and the lack of semantic information in instructions, which hinders multimodal alignment. Addressing these challenges, this paper proposes a novel approach to utilize Parameter-Efficient Tuning for generAl-purpose vision-Language models, namely PETAL. PETAL revolutionizes the training process by requiring only 0.5% of the total parameters, achieved through a unique mode approximation technique, which significantly reduces the training costs and reliance on heavy computing resources. Furthermore, PETAL enhances the semantic depth of instructions in two innovative ways: 1) by introducing adaptive instruction mixture-of-experts(MOEs), and 2) by fortifying the score-based linkage between parameter-efficient tuning and mutual information. Our extensive experiments across five multimodal downstream benchmarks reveal that PETAL not only outperforms current state-of-the-art methods in most scenarios but also surpasses full fine-tuning models in effectiveness. Additionally, our approach demonstrates remarkable advantages in few-shot settings, backed by comprehensive visualization analyses. Our source code is available at: https://github. com/melonking32/PETAL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275115509",
                    "name": "Yihang Zhai"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2265575121",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "b1e6fd86e81dba75626abd163d8c6bc6dd7b859e",
            "title": "DANCE: Learning A Domain Adaptive Framework for Deep Hashing",
            "abstract": "This paper studies unsupervised domain adaptive hashing, which aims to transfer a hashing model from a label-rich source domain to a label-scarce target domain. Current state-of-the-art approaches generally resolve the problem by integrating pseudo-labeling and domain adaptation techniques into deep hashing paradigms. Nevertheless, they usually suffer from serious class imbalance in pseudo-labels and suboptimal domain alignment caused by the neglection of the intrinsic structures of two domains. To address this issue, we propose a novel method named unbiaseD duAl hashiNg Contrastive lEarning (DANCE) for domain adaptive image retrieval. The core of our DANCE is to perform contrastive learning on hash codes from both instance level and prototype level. To begin, DANCE utilizes label information to guide instance-level hashing contrastive learning in the source domain. To generate unbiased and reliable pseudo-labels for semantic learning in the target domain, we uniformly select samples around each label embedding in the Hamming space. A momentum-update scheme is also utilized to smooth the optimization process. Additionally, we measure the semantic prototype representations in both source and target domains and incorporate them into a domain-aware prototype-level contrastive learning paradigm, which enhances domain alignment in the Hamming space while maximizing the model capacity. Experimental results on a number of well-known domain adaptive retrieval benchmarks validate the effectiveness of our proposed DANCE compared to a variety of competing baselines in different settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2215550446",
                    "name": "Xiang Wei"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "48240541",
                    "name": "C. Chen"
                },
                {
                    "authorId": "143863244",
                    "name": "Xiansheng Hua"
                },
                {
                    "authorId": "2115826917",
                    "name": "Xiao Luo"
                }
            ]
        },
        {
            "paperId": "ba15f6b2af45eeb7508ea5ca6d5c244154b890e6",
            "title": "Toward Effective Domain Adaptive Retrieval",
            "abstract": "This paper studies the problem of unsupervised domain adaptive hashing, which is less-explored but emerging for efficient image retrieval, particularly for cross-domain retrieval. This problem is typically tackled by learning hashing networks with pseudo-labeling and domain alignment techniques. Nevertheless, these approaches usually suffer from overconfident and biased pseudo-labels and inefficient domain alignment without sufficiently exploring semantics, thus failing to achieve satisfactory retrieval performance. To tackle this issue, we present PEACE, a principled framework which holistically explores semantic information in both source and target data and extensively incorporates it for effective domain alignment. For comprehensive semantic learning, PEACE leverages label embeddings to guide the optimization of hash codes for source data. More importantly, to mitigate the effects of noisy pseudo-labels, we propose a novel method to holistically measure the uncertainty of pseudo-labels for unlabeled target data and progressively minimize them through alternative optimization under the guidance of the domain discrepancy. Additionally, PEACE effectively removes domain discrepancy in the Hamming space from two views. In particular, it not only introduces composite adversarial learning to implicitly explore semantic information embedded in hash codes, but also aligns cluster semantic centroids across domains to explicitly exploit label information. Experimental results on several popular domain adaptive retrieval benchmarks demonstrate the superiority of our proposed PEACE compared with various state-of-the-art methods on both single-domain and cross-domain retrieval tasks. Our source codes are available at https://github.com/WillDreamer/PEACE.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2115826917",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2187310139",
                    "name": "Wei Xiang"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "48240541",
                    "name": "C. Chen"
                },
                {
                    "authorId": "143863244",
                    "name": "Xiansheng Hua"
                }
            ]
        },
        {
            "paperId": "cb3ddc59c3d56d5d6c83649080abd8a18e0fe93a",
            "title": "Prototypical Mixing and Retrieval-based Refinement for Label Noise-resistant Image Retrieval",
            "abstract": "Label noise is pervasive in real-world applications, which influences the optimization of neural network models. This paper investigates a realistic but understudied problem of image retrieval under label noise, which could lead to severe overfitting or memorization of noisy samples during optimization. Moreover, identifying noisy samples correctly is still a challenging problem for retrieval models. In this paper, we propose a novel approach called Prototypical Mixing and Retrieval-based Refinement (TITAN) for label noise-resistant image retrieval, which corrects label noise and mitigates the effects of the memorization simultaneously. Specifically, we first characterize numerous prototypes with Gaussian distributions in the hidden space, which would direct the Mixing procedure in providing synthesized samples. These samples are fed into a similarity learning framework with varying emphasis based on the prototypical structure to learn semantics with reduced overfitting. In addition, we retrieve comparable samples for each prototype from simple to complex, which refine noisy samples in an accurate and class-balanced manner. Comprehensive experiments on five benchmark datasets demonstrate the superiority of our proposed TITAN compared with various competing baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265575121",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2257378991",
                    "name": "Chong Chen"
                },
                {
                    "authorId": "2238119871",
                    "name": "Xian-Sheng Hua"
                },
                {
                    "authorId": "2241568233",
                    "name": "Xiao Luo"
                }
            ]
        }
    ]
}