{
    "authorId": "2145539796",
    "papers": [
        {
            "paperId": "025ca4c125d6ecabc816a56f160e5c992abc76d9",
            "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
            "abstract": "With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2213872863",
                    "name": "Dadi Guo"
                },
                {
                    "authorId": "2262397115",
                    "name": "Wei Fan"
                },
                {
                    "authorId": "2153556700",
                    "name": "Mingshi Xu"
                },
                {
                    "authorId": "1490651934",
                    "name": "Jie Huang"
                },
                {
                    "authorId": "1809614",
                    "name": "Yangqiu Song"
                }
            ]
        },
        {
            "paperId": "2dae1b1364822760154332327101ce768b674585",
            "title": "Bi-Touch: Bimanual Tactile Manipulation With Sim-to-Real Deep Reinforcement Learning",
            "abstract": "Bimanual manipulation with tactile feedback will be key to human-level robot dexterity. However, this topic is less explored than single-arm settings, partly due to the availability of suitable hardware along with the complexity of designing effective controllers for tasks with relatively large state-action spaces. Here we introduce a dual-arm tactile robotic system (Bi-Touch) based on the Tactile Gym 2.0 setup that integrates two affordable industrial-level robot arms with low-cost high-resolution tactile sensors (TacTips). We present a suite of bimanual manipulation tasks tailored towards tactile feedback: bi-pushing, bi-reorienting, and bi-gathering. To learn effective policies, we introduce appropriate reward functions for these tasks and propose a novel goal-update mechanism with deep reinforcement learning. We also apply these policies to real-world settings with a tactile sim-to-real approach. Our analysis highlights and addresses some challenges met during the sim-to-real application, e.g. the learned policy tended to squeeze an object in the bi-reorienting task due to the sim-to-real gap. Finally, we demonstrate the generalizability and robustness of this system by experimenting with different unseen objects with applied perturbations in the real world.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "81983572",
                    "name": "Yijiong Lin"
                },
                {
                    "authorId": "40976774",
                    "name": "Alex Church"
                },
                {
                    "authorId": "2157685104",
                    "name": "Max Yang"
                },
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2054582550",
                    "name": "John Lloyd"
                },
                {
                    "authorId": "2174772457",
                    "name": "Dandan Zhang"
                },
                {
                    "authorId": "2467565",
                    "name": "N. Lepora"
                }
            ]
        },
        {
            "paperId": "32ffb5e528b54f03a8a359e4573e0f3926a1202b",
            "title": "CgT-GAN: CLIP-guided Text GAN for Image Captioning",
            "abstract": "The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to \"see\" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the semantic guidance reward computed by the CLIP-based reward module. In addition to the cosine similarity as the semantic guidance reward (i.e., CLIP-cos), we further introduce a novel semantic guidance reward called CLIP-agg, which aligns the generated caption with a weighted text embedding by attentively aggregating the entire corpus. Experimental results on three subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms state-of-the-art methods significantly across all metrics. Code is available at https://github.com/Lihr747/CgtGAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7177756",
                    "name": "Jiarui Yu"
                },
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "48387349",
                    "name": "Y. Hao"
                },
                {
                    "authorId": "2106718459",
                    "name": "B. Zhu"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "5c0eb412790a0e47b25714a5857f9a1d0d95c6a6",
            "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence",
            "abstract": "Sentence-level representations are beneficial for various natural language processing tasks. It is commonly believed that vector representations can capture rich linguistic properties. Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage. In this work, we further investigate the information leakage issue and propose a generative embedding inversion attack (GEIA) that aims to reconstruct input sequences based only on their sentence embeddings. Given the black-box access to a language model, we treat sentence embeddings as initial tokens' representations and train or fine-tune a powerful decoder model to decode the whole sequences directly. We conduct extensive experiments to demonstrate that our generative inversion attack outperforms previous embedding inversion attacks in classification metrics and generates coherent and contextually similar sentences as the original inputs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2153556700",
                    "name": "Mingshi Xu"
                },
                {
                    "authorId": "1809614",
                    "name": "Yangqiu Song"
                }
            ]
        },
        {
            "paperId": "6ad3c17cbd0fb47e64d1175db6b6099d45657fa2",
            "title": "Visual-Tactile Robot Grasping Based on Human Skill Learning From Demonstrations Using a Wearable Parallel Hand Exoskeleton",
            "abstract": "The soft fingers and strategic grasping skills enable the human hands to grasp objects in a stable manner. This letter is to model human grasping skills and transfer the learned skills to robots to improve grasping quality and success rate. First, we designed a wearable tool-like parallel hand exoskeleton equipped with optical tactile sensors to acquire multimodal information, including hand positions and postures, the relative distance of the exoskeleton claws, and tactile images. Using the demonstration data, we summarized three characteristics observed from human demonstrations, involving varying-speed actions, grasping effect read from tactile images and grasping strategies for different positions. The characteristics were then utilized in the robot skill modelling to achieve a more human-like grasp. Since no force sensors are fixed to the claws, we introduced a new variable, called \u201cgrasp depth\u201d, to represent the grasping effect on the object. The robot grasping strategy diagram is constructed as follows: First, grasp quality is predicted using a linear array network (LAN) and global visual images as inputs. The conditions such as grasp width, depth, position, and angle are also predicted. Second, with the grasp width and depth of the object determined, dynamic movement primitives (DMPs) are employed to mimic human grasp actions with varying velocities. To further enhance grasp quality, a final action adjustment based on tactile detection is performed during the near-grasp time. The proposed strategy was validated through experiments conducted with a Franka robot with a self-designed gripper. The results demonstrate that robot grasping test achieved an increase in the grasping success rate from 82% to 96%, compared to the results obtained by pure LAN and constant grasp depth testing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48462127",
                    "name": "Zhenyu Lu"
                },
                {
                    "authorId": "30534531",
                    "name": "Lu-yi Chen"
                },
                {
                    "authorId": "2187481985",
                    "name": "Hengtai Dai"
                },
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2224290299",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2224096925",
                    "name": "Bofang Zheng"
                },
                {
                    "authorId": "2467565",
                    "name": "N. Lepora"
                },
                {
                    "authorId": "2175874139",
                    "name": "Chenguang Yang"
                }
            ]
        },
        {
            "paperId": "8931ae9eef5df0ea6fcbde018f73452b9a8f29d6",
            "title": "TKN: Transformer-based Keypoint Prediction Network For Real-time Video Prediction",
            "abstract": "Video prediction is a complex time-series forecasting task with great potential in many use cases. However, conventional methods overemphasize accuracy while ignoring the slow prediction speed caused by complicated model structures that learn too much redundant information with excessive GPU memory consumption. Furthermore, conventional methods mostly predict frames sequentially (frame-by-frame) and thus are hard to accelerate. Consequently, valuable use cases such as real-time danger prediction and warning cannot achieve fast enough inference speed to be applicable in reality. Therefore, we propose a transformer-based keypoint prediction neural network (TKN), an unsupervised learning method that boost the prediction process via constrained information extraction and parallel prediction scheme. TKN is the first real-time video prediction solution to our best knowledge, while significantly reducing computation costs and maintaining other performance. Extensive experiments on KTH and Human3.6 datasets demonstrate that TKN predicts 11 times faster than existing methods while reducing memory consumption by 17.4% and achieving state-of-the-art prediction performance on average.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "72050450",
                    "name": "Pengyuan Zhou"
                },
                {
                    "authorId": "46393904",
                    "name": "Yi-Wen Lin"
                },
                {
                    "authorId": "48387349",
                    "name": "Y. Hao"
                },
                {
                    "authorId": "145071284",
                    "name": "Haiyong Xie"
                },
                {
                    "authorId": "2114120921",
                    "name": "Yong Liao"
                }
            ]
        },
        {
            "paperId": "c8abff83109b5bd8566c330e9c250a85d03060d4",
            "title": "Tactile-Driven Gentle Grasping for Human-Robot Collaborative Tasks",
            "abstract": "This paper presents a control scheme for force sensitive, gentle grasping with a Pisa/IIT anthropomorphic SoftHand equipped with a miniaturised version of the TacTip optical tactile sensor on all five fingertips. The tactile sensors provide high-resolution information about a grasp and how the fingers interact with held objects. We first describe a series of hardware developments for performing asynchronous sensor data acquisition and processing, resulting in a fast control loop sufficient for real-time grasp control. We then develop a novel grasp controller that uses tactile feedback from all five fingertip sensors simultaneously to gently and stably grasp 43 objects of varying geometry and stiffness, which is then applied to a human-to-robot handover task. These developments open the door to more advanced manipulation with underactuated hands via fast reflexive control using high-resolution tactile sensing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2173605714",
                    "name": "Christopher J. Ford"
                },
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2054582550",
                    "name": "John Lloyd"
                },
                {
                    "authorId": "2182032796",
                    "name": "M. G. Catalano"
                },
                {
                    "authorId": "34882711",
                    "name": "M. Bianchi"
                },
                {
                    "authorId": "1910114",
                    "name": "Efi Psomopoulou"
                },
                {
                    "authorId": "2467565",
                    "name": "N. Lepora"
                }
            ]
        },
        {
            "paperId": "24dc21e3e31f3ecb331dafa1139245ff00268723",
            "title": "Towards Query-limited Adversarial Attacks on Graph Neural Networks",
            "abstract": "Graph Neural Network (GNN) is a graph representation learning approach for graph-structured data, which has witnessed a remarkable progress in the past few years. As a counterpart, the robustness of such a model has also received considerable attention. Previous studies show that the performance of a well-trained GNN can be faded by black-box adversarial examples significantly. In practice, the attacker can only query the target model with very limited counts, yet the existing methods require hundreds of thousand queries to extend attacks, leading the attacker to be exposed easily. To perform a step forward in addressing this issue, in this paper, we propose a novel attack methods, namely Graph Query-limited Attack (GQA), in which we generate adversarial examples on the surrogate model to fool the target model. Specifically, in GQA, we use contrastive learning to fit the feature extraction layers of the surrogate model in a query-free manner, which can reduce the need of queries. Furthermore, in order to utilize query results sufficiently, we obtain a series of queries with rich information by changing the input iteratively, and storing them in a buffer for recycling usage. Experiments show that GQA can decrease the accuracy of the target model by 4.8%, with only 1% edges modified and 100 queries performed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2210040166",
                    "name": "Jinhong Zhang"
                },
                {
                    "authorId": "2112170760",
                    "name": "Song Gao"
                },
                {
                    "authorId": "50789796",
                    "name": "Liwen Wu"
                },
                {
                    "authorId": "2153566938",
                    "name": "Wei Zhou"
                },
                {
                    "authorId": "2156024559",
                    "name": "Ruxin Wang"
                }
            ]
        },
        {
            "paperId": "9255f622354233e949bf07d38fb0005ee06b7510",
            "title": "Federated Knowledge Graphs Embedding",
            "abstract": ".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49349645",
                    "name": "Hao Peng"
                },
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "1809614",
                    "name": "Yangqiu Song"
                },
                {
                    "authorId": "3113725",
                    "name": "V. Zheng"
                },
                {
                    "authorId": "1492113939",
                    "name": "Jianxin Li"
                }
            ]
        },
        {
            "paperId": "ef1ef85adc38356023ada0b4abc3db4d395587dd",
            "title": "Differentially Private Federated Knowledge Graphs Embedding",
            "abstract": "Knowledge graph embedding plays an important role in knowledge representation, reasoning, and data mining applications. However, for multiple cross-domain knowledge graphs, state-of-the-art embedding models cannot make full use of the data from different knowledge domains while preserving the privacy of exchanged data. In addition, the centralized embedding model may not scale to the extensive real-world knowledge graphs. Therefore, we propose a novel decentralized scalable learning framework, Federated Knowledge Graphs Embedding (FKGE), where embeddings from different knowledge graphs can be learnt in an asynchronous and peer-to-peer manner while being privacy-preserving. FKGE exploits adversarial generation between pairs of knowledge graphs to translate identical entities and relations of different domains into near embedding spaces. In order to protect the privacy of the training data, FKGE further implements a privacy-preserving neural network structure to guarantee no raw data leakage. We conduct extensive experiments to evaluate FKGE on 11 knowledge graphs, demonstrating a significant and consistent improvement in model quality with at most 17.85% and 7.90% increases in performance on triple classification and link prediction tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Hao Peng"
                },
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "1809614",
                    "name": "Yangqiu Song"
                },
                {
                    "authorId": "3113725",
                    "name": "V. Zheng"
                },
                {
                    "authorId": "1492113939",
                    "name": "Jianxin Li"
                }
            ]
        }
    ]
}