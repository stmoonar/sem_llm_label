{
    "authorId": "2167508843",
    "papers": [
        {
            "paperId": "16e2c1e988b13329e3b7fc2dbbf854f9414721d7",
            "title": "RATIONALYST: Pre-training Process-Supervision for Improving Reasoning",
            "abstract": "The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285428192",
                    "name": "Dongwei Jiang"
                },
                {
                    "authorId": "2323937529",
                    "name": "Guoxuan Wang"
                },
                {
                    "authorId": "2146424602",
                    "name": "Yining Lu"
                },
                {
                    "authorId": "2323801913",
                    "name": "Andrew Wang"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2323832094",
                    "name": "Chuyu Liu"
                },
                {
                    "authorId": "2292194313",
                    "name": "Benjamin Van Durme"
                },
                {
                    "authorId": "2319593572",
                    "name": "Daniel Khashabi"
                }
            ]
        },
        {
            "paperId": "289533e16e509d0ba8f499a371b4e470f3e492de",
            "title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation",
            "abstract": "Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a \u201cbigram paraphrase\u201d attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257002716",
                    "name": "Abe Bohan Hou"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2125062703",
                    "name": "Yichen Wang"
                },
                {
                    "authorId": "2257002712",
                    "name": "Yung-Sung Chuang"
                },
                {
                    "authorId": "2257103558",
                    "name": "Hongwei Wang"
                },
                {
                    "authorId": "2121272448",
                    "name": "Lingfeng Shen"
                },
                {
                    "authorId": "7536576",
                    "name": "Benjamin Van Durme"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "2edfe299bbed8c8769fa0716769e6510fe083223",
            "title": "Core: Robust Factual Precision Scoring with Informative Sub-Claim Identification",
            "abstract": "Hallucinations -- the generation of untrue claims -- pose a challenge to the application of large language models (LLMs) [1] thereby motivating the development of metrics to evaluate factual precision. We observe that popular metrics using the Decompose-Then-Verify framework, such as FActScore [2], can be manipulated by adding obvious or repetitive claims to artificially inflate scores. We expand the FActScore dataset to design and analyze factual precision metrics, demonstrating that models can be trained to achieve high scores under existing metrics through exploiting the issues we identify. This motivates our new customizable plug-and-play subclaim selection component called Core, which filters down individual subclaims according to their uniqueness and informativeness. Metrics augmented by Core are substantially more robust as shown in head-to-head comparisons. We release an evaluation framework supporting the modular use of Core (https://github.com/zipJiang/Core) and various decomposition strategies, and we suggest its adoption by the LLM community. [1] Hong et al.,\"The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models\", arXiv:2404.05904v2 [cs.CL]. [2] Min et al.,\"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\", arXiv:2305.14251v2 [cs.CL].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "6600801",
                    "name": "Zhengping Jiang"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "38912638",
                    "name": "Nathaniel Weir"
                },
                {
                    "authorId": "78150202",
                    "name": "Seth Ebner"
                },
                {
                    "authorId": "2292188343",
                    "name": "Miriam Wanner"
                },
                {
                    "authorId": "2187060946",
                    "name": "Kate Sanders"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                },
                {
                    "authorId": "2284574974",
                    "name": "Anqi Liu"
                },
                {
                    "authorId": "2292194313",
                    "name": "Benjamin Van Durme"
                }
            ]
        },
        {
            "paperId": "341da3f8af6edd31edd8f5a3d9452957aeaaa744",
            "title": "Tur[k]ingBench: A Challenge Benchmark for Web Agents",
            "abstract": "Can advanced multi-modal models effectively tackle complex web-based tasks? Such tasks are often found on crowdsourcing platforms, where crowdworkers engage in challenging micro-tasks within web-based environments. Building on this idea, we present TurkingBench, a benchmark consisting of tasks presented as web pages with textual instructions and multi-modal contexts. Unlike previous approaches that rely on artificially synthesized web pages, our benchmark uses natural HTML pages originally designed for crowdsourcing workers to perform various annotation tasks. Each task's HTML instructions are instantiated with different values derived from crowdsourcing tasks, creating diverse instances. This benchmark includes 32.2K instances spread across 158 tasks. To support the evaluation of TurkingBench, we have developed a framework that links chatbot responses to actions on web pages (e.g., modifying a text box, selecting a radio button). We assess the performance of cutting-edge private and open-source models, including language-only and vision-language models (such as GPT4 and InternVL), on this benchmark. Our results show that while these models outperform random chance, there is still significant room for improvement. We hope that this benchmark will drive progress in the evaluation and development of web-based agents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292213227",
                    "name": "Kevin Xu"
                },
                {
                    "authorId": "2156538832",
                    "name": "Yeganeh Kordi"
                },
                {
                    "authorId": "2187060946",
                    "name": "Kate Sanders"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2292197723",
                    "name": "Adam Byerly"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2292194313",
                    "name": "Benjamin Van Durme"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                }
            ]
        },
        {
            "paperId": "3cd81b0123b5f8477f6b5777681030ef6b05dd46",
            "title": "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts",
            "abstract": "As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, training in lower-resource languages yields minimal improvement. This suggests that the bottleneck of cross-lingual alignment is rooted in the pretraining stage. Our findings highlight the challenges in cross-lingual LLM safety, and we hope they inform future research in this direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121272448",
                    "name": "Lingfeng Shen"
                },
                {
                    "authorId": "28000727",
                    "name": "Weiting Tan"
                },
                {
                    "authorId": "2281058538",
                    "name": "Sihao Chen"
                },
                {
                    "authorId": "104375103",
                    "name": "Yunmo Chen"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2280999275",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "2112670551",
                    "name": "Boyuan Zheng"
                },
                {
                    "authorId": "2264196844",
                    "name": "Philipp Koehn"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                }
            ]
        },
        {
            "paperId": "94c28609614719c68469081ed99315f54cb1fb6a",
            "title": "k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text",
            "abstract": "Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257002716",
                    "name": "Abe Bohan Hou"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2125062703",
                    "name": "Yichen Wang"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                }
            ]
        },
        {
            "paperId": "9689b5fdb0d3a1bad802d03d348bd32aa5a4c2df",
            "title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data",
            "abstract": "To trust the fluent generations of large language models (LLMs), humans must be able to verify their correctness against trusted, external sources. Recent efforts, such as providing citations via retrieved documents or post-hoc provenance, enhance verifiability but still provide no guarantees on their correctness. To address these limitations, we tackle the verifiability goal with a different philosophy: trivializing the verification process by developing models that quote verbatim statements from trusted sources in pre-training data. We propose Quote-Tuning, and demonstrate it is feasible to align LLMs to provide quoted statements from data memorized during pre-training. The core of Quote-Tuning is a fast membership inference function (Marone and Van Durme, 2023) that efficiently verifies text against a trusted corpus. We leverage this tool to design a reward function to quantify quotes in model responses, which is then used to create a dataset for preference learning. Experimental results show that Quote-Tuning significantly increases verbatim quotes from high-quality pre-training documents by 55% to 130% relative to un-tuned models while maintaining response quality. Quote-Tuning also generalizes quoting to out-of-domain data, is applicable in different tasks, and provides additional benefits to truthfulness. Our method not only serves as a hassle-free method to increase quoting but also opens up avenues for improving LLM trustworthiness through better verifiability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "82893387",
                    "name": "Marc Marone"
                },
                {
                    "authorId": "2256304994",
                    "name": "Tianjian Li"
                },
                {
                    "authorId": "2292194313",
                    "name": "Benjamin Van Durme"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                }
            ]
        },
        {
            "paperId": "9e3d14e4697a325fcabc6d952232a3ad7e9fa809",
            "title": "SELF-[IN]CORRECT: LLMs Struggle with Refining Self-Generated Responses",
            "abstract": "Can LLMs continually improve their previous outputs for better results? An affirmative answer would require LLMs to be better at discriminating among previously-generated alternatives, than generating initial responses. We explore the validity of this hypothesis in practice. We first introduce a unified framework that allows us to compare the generative and discriminative capability of any model on any task. Then, in our resulting experimental analysis of several LLMs, we do not observe those models\u2019 performance on discrimination to be reliably better than generation. We hope these findings inform the growing literature on self-improvement AI systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285428192",
                    "name": "Dongwei Jiang"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "47433471",
                    "name": "Orion Weller"
                },
                {
                    "authorId": "38912638",
                    "name": "Nathaniel Weir"
                },
                {
                    "authorId": "2292194313",
                    "name": "Benjamin Van Durme"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                }
            ]
        },
        {
            "paperId": "f2b48782b86f4f8bc0b537d1d91bae6cf642c8c4",
            "title": "DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation",
            "abstract": "Non-autoregressive Transformers (NATs) are recently applied in direct speech-to-speech translation systems, which convert speech across different languages without intermediate text data. Although NATs generate high-quality outputs and offer faster inference than autoregressive models, they tend to produce incoherent and repetitive results due to complex data distribution (e.g., acoustic and linguistic variations in speech). In this work, we introduce DiffNorm, a diffusion-based normalization strategy that simplifies data distributions for training NAT models. After training with a self-supervised noise estimation objective, DiffNorm constructs normalized target data by denoising synthetically corrupted speech features. Additionally, we propose to regularize NATs with classifier-free guidance, improving model robustness and translation quality by randomly dropping out source information during training. Our strategies result in a notable improvement of about +7 ASR-BLEU for English-Spanish (En-Es) and +2 ASR-BLEU for English-French (En-Fr) translations on the CVSS benchmark, while attaining over 14x speedup for En-Es and 5x speedup for En-Fr translations compared to autoregressive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28000727",
                    "name": "Weiting Tan"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2248152247",
                    "name": "Lingfeng Shen"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                },
                {
                    "authorId": "2264196844",
                    "name": "Philipp Koehn"
                }
            ]
        },
        {
            "paperId": "94be9ee72de86a909fd29a739f203e1aa6bc165e",
            "title": "On the Zero-Shot Generalization of Machine-Generated Text Detectors",
            "abstract": "The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2243189215",
                    "name": "Xiao Pu"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2257023881",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                }
            ]
        }
    ]
}