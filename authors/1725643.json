{
    "authorId": "1725643",
    "papers": [
        {
            "paperId": "21b4777948797377deedf4a9f1f58ad13f6b8b5d",
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2887412",
                    "name": "Jinchao Li"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2066415714",
                    "name": "Zhengcong Fei"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2257374643",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2116575668",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "1979505",
                    "name": "A. Geramifard"
                },
                {
                    "authorId": "1765212",
                    "name": "Chiori Hori"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "369bc24b935357d661dbefd66dc3c3b6403c4cb9",
            "title": "SalesBot 2.0: A Human-Like Intent-Guided Chit-Chat Dataset",
            "abstract": "In recent research on dialogue systems and corpora, there has been a significant focus on two distinct categories: task-oriented (TOD) and open-domain (chit-chat) dialogues. TOD systems aim to satisfy specific user goals, such as finding a movie to watch, whereas open-domain systems primarily focus on generating engaging conversations. A recent study by Chiu et al. (2022) introduced SalesBot, which provides simulators and a dataset with one-turn transition from chit-chat to task-oriented dialogues. However, the previously generated data solely relied on BlenderBot, which raised concerns about its long-turn naturalness and consistency during a conversation. To address this issue, this paper aims to build SalesBot 2.0, a revised version of the published data, by leveraging the commonsense knowledge of large language models (LLMs) through proper prompting. The objective is to gradually bridge the gap between chit-chat and TOD towards better naturalness and consistency. The newly released large-scale dataset with detailed annotations exhibits smoother transitions between topics and is more human-like in terms of naturalness and consistency. It can serve as a valuable resource for both academic research and commercial applications. Furthermore, our proposed framework can be applied to generate numerous dialogues with various target intents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50310256",
                    "name": "Wen-Yu Chang"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                }
            ]
        },
        {
            "paperId": "3c89fdc20c1aa8eda4cc49dffc1f806c238c8077",
            "title": "CONVERSER: Few-shot Conversational Dense Retrieval with Synthetic Data Generation",
            "abstract": "Conversational search provides a natural interface for information retrieval (IR). Recent approaches have demonstrated promising results in applying dense retrieval to conversational IR. However, training dense retrievers requires large amounts of in-domain paired data. This hinders the development of conversational dense retrievers, as abundant in-domain conversations are expensive to collect. In this paper, we propose Converser, a framework for training conversational dense retrievers with at most 6 examples of in-domain dialogues. Specifically, we utilize the in-context learning capability of large language models to generate conversational queries given a passage in the retrieval corpus. Experimental results on conversational retrieval benchmarks OR-QuAC and TREC CAsT 19 show that the proposed Converser achieves comparable performance to fully-supervised models, demonstrating the effectiveness of our proposed framework in few-shot conversational dense retrieval. All source code and generated datasets are available: https://github.com/MiuLab/CONVERSER",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47396497",
                    "name": "Chao-Wei Huang"
                },
                {
                    "authorId": "2237414990",
                    "name": "Chen-Yu Hsu"
                },
                {
                    "authorId": "2057635233",
                    "name": "Tsung-Yuan Hsu"
                },
                {
                    "authorId": "2186278878",
                    "name": "Chen-An Li"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                }
            ]
        },
        {
            "paperId": "4f480bae3196dbbc27ab383bce33478ea963f9b3",
            "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
            "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1557269413",
                    "name": "Yen-Ting Lin"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                }
            ]
        },
        {
            "paperId": "7751958d2f58aa952a72ec6f4864f613e8757cef",
            "title": "MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling",
            "abstract": "Video-and-language understanding has a variety of applications in the industry, such as video question answering, text-video retrieval and multi-label classi\ufb01cation. Existing video-and-language understanding methods generally adopt heavy multi-modal encoders and feature fusion modules, which consume large amounts of GPU memory. Especially, they have dif\ufb01culty dealing with dense video frames or long text that are prevalent in industrial applications. In this paper, we propose MuLTI, a highly accurate and memory-ef\ufb01cient video-and-language understanding model that achieves ef\ufb01cient and effective feature fusion through feature sampling and attention modules. Speci\ufb01cally, we design a MultiWay-Sampler based on self-attention modules and attention-based residual mapping to sample long sequence features and fuse multi-modal features, which both reduces the memory cost and improves the performance. Therefore, MuLTI can handle longer sequences with limited GPU memory. Then, we introduce an attention-based adapter to the encoders, which \ufb01netunes the shallow features to improve the model\u2019s performance with low GPU memory consumption. Finally, to further enhance the model\u2019s performance, we propose a new pretraining task named Multiple Choice Modeling. This task is designed to bridge the gap between pretraining and down-stream tasks and to improve the model\u2019s ability to align video and text. Bene\ufb01ting from the ef\ufb01cient feature fusion module, the attention-based adapter and the new pretraining task, MuLTI achieves state-of-the-art performance on multiple datasets. Implementation and pretrained models will be released.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110524480",
                    "name": "Jiaqi Xu"
                },
                {
                    "authorId": "2156641263",
                    "name": "Bo Liu"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "9181437",
                    "name": "Mengli Cheng"
                },
                {
                    "authorId": "143794227",
                    "name": "Xing Shi"
                }
            ]
        },
        {
            "paperId": "2a4b6fdf4fd74429431a730c14d0087e00b2a4fa",
            "title": "Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning",
            "abstract": "Intelligent virtual assistants are currently designed to perform tasks or services explicitly mentioned by users, so multiple related domains or tasks need to be performed one by one through a long conversation with many explicit intents. Instead, human assistants are capable of reasoning (multiple) implicit intents based on user utterances via commonsense knowledge, reducing complex interactions and improving practicality. Therefore, this paper proposes a framework of multi-domain dialogue systems, which can automatically infer implicit intents based on user utterances and then perform zero-shot prompting using a large pre-trained language model to trigger suitable single task-oriented bots. The proposed framework is demonstrated effective to realize implicit intents and recommend associated bots in a zero-shot manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2165384490",
                    "name": "Hui-Chi Kuo"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                }
            ]
        },
        {
            "paperId": "2c763b82c09000333b53663fbc5993afdd952413",
            "title": "SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues",
            "abstract": "Dialogue systems are usually categorized into two types, open-domain and task-oriented. The first one focuses on chatting with users and making them engage in the conversations, where selecting a proper topic to fit the dialogue context is essential for a successful dialogue. The other one focuses on a specific task instead of casual talks, e.g., finding a movie on Friday night, playing a song. These two directions have been studied separately due to their different purposes. However, how to smoothly transition from social chatting to task-oriented dialogues is important for triggering the business opportunities, and there is no any public data focusing on such scenarios. Hence, this paper focuses on investigating the conversations starting from open-domain social chatting and then gradually transitioning to task-oriented purposes, and releases a large-scale dataset with detailed annotations for encouraging this research direction. To achieve this goal, this paper proposes a framework to automatically generate many dialogues without human involvement, in which any powerful open-domain dialogue generation model can be easily leveraged. The human evaluation shows that our generated dialogue data has a natural flow at a reasonable quality, showing that our released data has a great potential of guiding future research directions and commercial activities. Furthermore, the released models allow researchers to automatically generate unlimited dialogues in the target scenarios, which can greatly benefit semi-supervised and unsupervised approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163391523",
                    "name": "Ssu Chiu"
                },
                {
                    "authorId": "2118481980",
                    "name": "Maolin Li"
                },
                {
                    "authorId": "1557269413",
                    "name": "Yen-Ting Lin"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                }
            ]
        },
        {
            "paperId": "3448bf2ad698c00c78331b06ed8f7cbc970bfebb",
            "title": "Islander: A Real-Time News Monitoring and Analysis System",
            "abstract": "With thousands of news articles from hun-dreds of sources distributing and sharing every-day, news consumption and information acqui-sition have been increasingly dif\ufb01cult for readers. Additionally, the content of news articles are becoming catchy or even inciting to attract readership, harming the accuracy of news re-porting. We present Islander, an online news analyzing system for online news. The system allows users to browse trending topics with articles from multiple sources and perspectives. We de\ufb01ne several metrics as proxies to news quality, and develop algorithms for automatic estimation. The quality estimation results are delivered through a web interface to news readers for easy access to news and information 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47396497",
                    "name": "Chao-Wei Huang"
                },
                {
                    "authorId": "2161612777",
                    "name": "Kai-Chou Yang"
                },
                {
                    "authorId": "8157332",
                    "name": "Zi-Yuan Chen"
                },
                {
                    "authorId": "2117983247",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "2163523410",
                    "name": "Po-Yu Wu"
                },
                {
                    "authorId": "2108581502",
                    "name": "Yu-Yang Huang"
                },
                {
                    "authorId": "2896016",
                    "name": "Chung-Kai Hsieh"
                },
                {
                    "authorId": "2163451182",
                    "name": "G. Fann"
                },
                {
                    "authorId": "143984233",
                    "name": "Ting-Yin Cheng"
                },
                {
                    "authorId": "2163451029",
                    "name": "Ethan Tu"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                }
            ]
        },
        {
            "paperId": "6e6cbec40092e0028632e855f3f4431a3aef73d4",
            "title": "BARCOR: Towards A Unified Framework for Conversational Recommendation Systems",
            "abstract": "Recommendation systems focus on helping users find items of interest in the situations of information overload, where users' preferences are typically estimated by the past observed behaviors. In contrast, conversational recommendation systems (CRS) aim to understand users' preferences via interactions in conversation flows. CRS is a complex problem that consists of two main tasks: (1) recommendation and (2) response generation. Previous work often tried to solve the problem in a modular manner, where recommenders and response generators are separate neural models. Such modular architectures often come with a complicated and unintuitive connection between the modules, leading to inefficient learning and other issues. In this work, we propose a unified framework based on BART for conversational recommendation, which tackles two tasks in a single model. Furthermore, we also design and collect a lightweight knowledge graph for CRS in the movie domain. The experimental results show that the proposed methods achieve the state-of-the-art performance in terms of both automatic and human evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155389801",
                    "name": "Tingting Wang"
                },
                {
                    "authorId": "27629426",
                    "name": "Shang-Yu Su"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                }
            ]
        },
        {
            "paperId": "b083707f45a9c7e88eaeaf09ba66d94301139c0c",
            "title": "Controllable User Dialogue Act Augmentation for Dialogue State Tracking",
            "abstract": "Prior work has demonstrated that data augmentation is useful for improving dialogue state tracking. However, there are many types of user utterances, while the prior method only considered the simplest one for augmentation, raising the concern about poor generalization capability. In order to better cover diverse dialogue acts and control the generation quality, this paper proposes controllable user dialogue act augmentation (CUDA-DST) to augment user utterances with diverse behaviors. With the augmented data, different state trackers gain improvement and show better robustness, achieving the state-of-the-art performance on MultiWOZ 2.1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2179205500",
                    "name": "Chun-Mao Lai"
                },
                {
                    "authorId": "15407184",
                    "name": "Ming-Hao Hsu"
                },
                {
                    "authorId": "47396497",
                    "name": "Chao-Wei Huang"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                }
            ]
        }
    ]
}