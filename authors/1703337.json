{
    "authorId": "1703337",
    "papers": [
        {
            "paperId": "33ba6ff1d2b599178e60d029da10b41f7a3c4729",
            "title": "\u201cOne-Size-Fits-All\u201d? Examining Expectations around What Constitute \u201cFair\u201d or \u201cGood\u201d NLG System Behaviors",
            "abstract": "Fairness-related assumptions about what constitute appropriate NLG system behaviors range from invariance, where systems are expected to behave identically for social groups, to adaptation, where behaviors should instead vary across them. To illuminate tensions around invariance and adaptation, we conduct five case studies, in which we perturb different types of identity-related language features (names, roles, locations, dialect, and style) in NLG system inputs. Through these cases studies, we examine people\u2019s expectations of system behaviors, and surface potential caveats of these contrasting yet commonly held assumptions. We find that motivations for adaptation include social norms, cultural differences, feature-specific information, and accommodation; in contrast, motivations for invariance include perspectives that favor prescriptivism, view adaptation as unnecessary or too difficult for NLG systems to do appropriately, and are wary of false assumptions. Our findings highlight open challenges around what constitute \u201cfair\u201d or \u201cgood\u201d NLG system behaviors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261388674",
                    "name": "Li Lucy"
                },
                {
                    "authorId": "3422038",
                    "name": "Su Lin Blodgett"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                },
                {
                    "authorId": "2058607401",
                    "name": "Hanna M. Wallach"
                },
                {
                    "authorId": "2261388192",
                    "name": "Alexandra Olteanu"
                }
            ]
        },
        {
            "paperId": "706c6b3781374b0b11f98f204a4ddd05b26ed009",
            "title": "Knowledge Infused Decoding",
            "abstract": "Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context. Hence, they tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. Recent remedies to this problem focus on modifying either the pre-training or task fine-tuning objectives to incorporate knowledge, which normally require additional costly training or architecture modification of LMs for practical applications. We present Knowledge Infused Decoding (KID) -- a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding. Specifically, we maintain a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning. On six diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART) armed with KID outperform many task-optimized state-of-the-art models, and show particularly strong performance in few-shot scenarios over seven related knowledge-infusion techniques. Human evaluation confirms KID's ability to generate more relevant and factual language for the input context when compared with multiple baselines. Finally, KID also alleviates exposure bias and provides stable generation quality when generating longer sequences. Code for KID is available at https://github.com/microsoft/KID.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7247867",
                    "name": "Ruibo Liu"
                },
                {
                    "authorId": "2250250",
                    "name": "Guoqing Zheng"
                },
                {
                    "authorId": "2152953535",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "117962087",
                    "name": "Radhika Gaonkar"
                },
                {
                    "authorId": "118565563",
                    "name": "Chongyang Gao"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ]
        },
        {
            "paperId": "fd76561584ceeec407707b207f16cf734ebf2f5a",
            "title": "PREME: Preference-based Meeting Exploration through an Interactive Questionnaire",
            "abstract": "The recent increase in the volume of online meetings necessitates automated tools for organizing the material, especially when an attendee has missed the discussion and needs assistance in quickly exploring it. In this work, we propose a novel end-to-end framework for generating interactive questionnaires for preference-based meeting exploration. As a result, users are supplied with a list of suggested questions reflecting their preferences. Since the task is new, we introduce an automatic evaluation strategy by measuring how much the generated questions via questionnaire are answerable to ensure factual correctness and covers the source meeting for the depth of possible exploration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2269050250",
                    "name": "Negar Arabzadeh"
                },
                {
                    "authorId": "30967674",
                    "name": "Ali Ahmadvand"
                },
                {
                    "authorId": "1755651",
                    "name": "Julia Kiseleva"
                },
                {
                    "authorId": "2164249573",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1977489",
                    "name": "Ahmed Hassan Awadallah"
                },
                {
                    "authorId": "2112679941",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                }
            ]
        },
        {
            "paperId": "1838cbd2eee0a555ab7e850eff1fce69d62acb95",
            "title": "MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning",
            "abstract": "The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an understudied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages \u2013 without access to large-scale monolingual corpora or large amounts of labeled data \u2013 for tasks like cross-lingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2250250",
                    "name": "Guoqing Zheng"
                },
                {
                    "authorId": "1777140",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ]
        },
        {
            "paperId": "20e7f3b52be431724efa953d1d65a7dbd171a817",
            "title": "Language Scaling for Universal Suggested Replies Model",
            "abstract": "We consider the problem of scaling automated suggested replies for a commercial email application to multiple languages. Faced with increased compute requirements and low language resources for language expansion, we build a single universal model for improving the quality and reducing run-time costs of our production system. However, restricted data movement across regional centers prevents joint training across languages. To this end, we propose a multi-lingual multi-task continual learning framework, with auxiliary tasks and language adapters to train universal language representation across regions. The experimental results show positive cross-lingual transfer across languages while reducing catastrophic forgetting across regions. Our online results on real user traffic show significant CTR and Char-saved gain as well as 65% training cost reduction compared with per-language models. As a consequence, we have scaled the feature in multiple languages including low-resource markets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1914937",
                    "name": "Qianlan Ying"
                },
                {
                    "authorId": "34765717",
                    "name": "Payal Bajaj"
                },
                {
                    "authorId": "48717082",
                    "name": "Budhaditya Deb"
                },
                {
                    "authorId": "2116465515",
                    "name": "Yu Yang"
                },
                {
                    "authorId": "145200778",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "118114973",
                    "name": "Bojia Lin"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                },
                {
                    "authorId": "50706785",
                    "name": "Xia Song"
                },
                {
                    "authorId": "2152916528",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "71790825",
                    "name": "Daxin Jiang"
                }
            ]
        },
        {
            "paperId": "305204fc77037c9b8f25eb00b91fbc8b526af2b8",
            "title": "A Conditional Generative Matching Model for Multi-lingual Reply Suggestion",
            "abstract": "We study the problem of multilingual automated reply suggestions (RS) model serving many languages simultaneously. Multilingual models are often challenged by model capacity and severe data distribution skew across languages. While prior works largely focus on monolingual models, we propose Conditional Generative Matching models (CGM), optimized within a Variational Autoencoder framework to address challenges arising from multi-lingual RS. CGM does so with expressive message conditional priors, mixture densities to enhance multi-lingual data representation, latent alignment for language discrimination, and effective variational optimization techniques for training multi-lingual RS. The enhancements result in performance that exceed competitive baselines in relevance (ROUGE score) by more than 10\\% on average, and 16\\% for low resource languages. CGM also shows remarkable improvements in diversity (80\\%) illustrating its expressiveness in representation of multi-lingual data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48717082",
                    "name": "Budhaditya Deb"
                },
                {
                    "authorId": "2250250",
                    "name": "Guoqing Zheng"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ]
        },
        {
            "paperId": "426989945b8feb5a3882371104af0b5c737bd4f9",
            "title": "When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages",
            "abstract": "Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical service-oriented text prediction metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2863266",
                    "name": "S. Trajanovski"
                },
                {
                    "authorId": "13212680",
                    "name": "Chad Atalla"
                },
                {
                    "authorId": "2109296722",
                    "name": "Kunho Kim"
                },
                {
                    "authorId": "2058910",
                    "name": "Vipul Agarwal"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                },
                {
                    "authorId": "2596310",
                    "name": "Chris Quirk"
                }
            ]
        },
        {
            "paperId": "49a328730d3c6397820b733bbac903545568cd9c",
            "title": "UserIdentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis",
            "abstract": "Global models are typically trained to be as generalizable as possible. Invariance to the specific user is considered desirable since models are shared across multitudes of users. However, these models are often unable to produce personalized responses for individual users, based on their data. Contrary to widely-used personalization techniques based on few-shot and meta-learning, we propose UserIdentifier, a novel scheme for training a single shared model for all users. Our approach produces personalized responses by prepending a fixed, user-specific non-trainable string (called \u201cuser identifier\u201d) to each user\u2019s input text. Unlike prior work, this method doesn\u2019t need any additional model parameters, any extra rounds of personal few-shot learning or any change made to the vocabulary. We empirically study different types of user identifiers (numeric, alphanumeric, and also randomly generated) and demonstrate that, surprisingly, randomly generated user identifiers outperform the prefix-tuning based state-of-the-art approach by up to 13, on a suite of sentiment analysis datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115471757",
                    "name": "Fatemehsadat Mireshghallah"
                },
                {
                    "authorId": "2130460479",
                    "name": "Vaishnavi Shrivastava"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                },
                {
                    "authorId": "1562202621",
                    "name": "Robert Sim"
                },
                {
                    "authorId": "1780137",
                    "name": "D. Dimitriadis"
                }
            ]
        },
        {
            "paperId": "8ffe4d395d6b02d65a2301e0104c8c91c2187928",
            "title": "Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding",
            "abstract": "Recent work has focused on compressing pre-trained language models (PLMs) like BERT where the major focus has been to improve the in-distribution performance for downstream tasks. However, very few of these studies have analyzed the impact of compression on the generalizability and robustness of compressed models for out-of-distribution (OOD) data. Towards this end, we study two popular model compression techniques including knowledge distillation and pruning and show that the compressed models are significantly less robust than their PLM counterparts on OOD test sets although they obtain similar performance on in-distribution development sets for a task. Further analysis indicates that the compressed models overfit on the shortcut samples and generalize poorly on the hard ones. We further leverage this observation to develop a regularization strategy for robust model compression based on sample uncertainty.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "153655416",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                },
                {
                    "authorId": "2123553641",
                    "name": "Xia Hu"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ]
        },
        {
            "paperId": "d2bdf5f022b9539ef421b468854dbae754eb86b3",
            "title": "What do Compressed Large Language Models Forget? Robustness Challenges in Model Compression",
            "abstract": "Recent works have focused on compressing pre-trained language models (PLMs) like BERT where the major focus has been to improve the compressed model performance for downstream tasks. However, there has been no study in analyzing the impact of compression on the generalizability and robustness of these models. Towards this end, we study two popular model compression techniques including knowledge distillation and pruning and show that compressed models are signi\ufb01cantly less robust than their PLM counterparts on adversarial test sets although they obtain similar performance on in-distribution development sets for a task. Further analysis indicates that the compressed models over\ufb01t on the easy samples and generalize poorly on the hard ones. We further leverage this observation to develop a regularization strategy for model compression based on sample uncertainty. Experimental results on several natural language understanding tasks demonstrate our mitigation framework to improve both the adversarial generalization as well as in-distribution task performance of the compressed models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "153655416",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "1703337",
                    "name": "Milad Shokouhi"
                },
                {
                    "authorId": "2123553641",
                    "name": "Xia Hu"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ]
        }
    ]
}