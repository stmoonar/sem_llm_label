{
    "authorId": "2281398403",
    "papers": [
        {
            "paperId": "b27152219dcd97b3c6b4ffa84b135adff0af983a",
            "title": "Dynamic Traceback Learning for Medical Report Generation",
            "abstract": "Automated medical report generation has the potential to significantly reduce the workload associated with the time-consuming process of medical reporting. Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation. However, when trained end-to-end and applied directly to medical image-to-text generation, they face two significant challenges: i) difficulty in accurately capturing subtle yet crucial pathological details, and ii) reliance on both visual and textual inputs during inference, leading to performance degradation in zero-shot inference when only images are available. To address these challenges, this study proposes a novel multi-modal dynamic traceback learning framework (DTrace). Specifically, we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of image and text input, enabling text generation without strong reliance on the input from both modalities during inference. The learning of cross-modal knowledge is enhanced by supervising the model to recover masked semantic information from a complementary counterpart. Extensive experiments conducted on two benchmark datasets, IU-Xray and MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms state-of-the-art methods for medical report generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "da8103f9c09f20db5433b917e0a3adc9e0556cdc",
            "title": "SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance",
            "abstract": "Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "6f5beb87e5da0df8fd1ddf9fdd5c0a35f200b295",
            "title": "Semantic-Driven Global-Local Cooperative Contrastive Learning for Medical Report Generation",
            "abstract": "As the demand for radiology continues to increase, the shortage of specialized professionals becomes a challenging issue. Such shortage highlights the need to utilize advancements in artificial intelligence (AI) to automatically generate draft medical reports from radiology images. Recently, the application of contrastive learning has been leveraged in image-to-text generation because it allows the model to learn meaningful representations in latent space by contrasting similar and dissimilar image-text pairs. However, existing approaches to applying contrastive learning in medical report generation are limited by the following: 1) they are performed as an independent pretraining step, which hinders the cooperation between contrastive learning and the subsequent report generation step; 2) these methods are contingent on pairing images with their corresponding reports, thus establishing similarity based solely on this association. Such contingency inadvertently overlooks the situation where unpaired reports could also be relevant to a given image, thereby failing to accurately capture and understand the semantic relationships within the data; and 3) existing contrastive learning in medical report generation only utilizes the global representation, which cannot capture subtle but crucial local visual information. To address these limitations, we propose a Semantic-Driven Global-Local Cooperative Contrastive Learning Network (SGLCCNet), which integrates contrastive learning into the training process of report generation, enriched with semantic information extracted from reports and enhanced by the inclusion of local feature exploration. Extensive experiments on the IU-Xray dataset demonstrate that our method achieved the state-of-the-art. Further, we demonstrate how each of our proposed steps adds to the overall performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2281343537",
                    "name": "David Dagan Feng"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        }
    ]
}