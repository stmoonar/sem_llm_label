{
    "authorId": "3068001",
    "papers": [
        {
            "paperId": "09f61bf7b7d02112166620571fa8d958ba6cd7b5",
            "title": "Self-Supervised Audio-and-Text Pre-training with Extremely Low-Resource Parallel Data",
            "abstract": "Multimodal pre-training for audio-and-text has recently been proved to be effective and has significantly improved the performance of many downstream speech understanding tasks. However, these state-of-the-art pre-training audio-text models work well only when provided with large amount of parallel audio-and-text data, which brings challenges on many languages that are rich in unimodal corpora but scarce of parallel cross-modal corpus. In this paper, we investigate whether it is possible to pre-train an audio-text multimodal model with extremely low-resource parallel data and extra non-parallel unimodal data. Our pre-training framework consists of the following components: (1) Intra-modal Denoising Auto-Encoding (IDAE), which is able to reconstruct input text (audio) representations from a noisy version of itself. (2) Cross-modal Denoising Auto-Encoding (CDAE), which is pre-trained to reconstruct the input text (audio), given both a noisy version of the input text (audio) and the corresponding translated noisy audio features (text embeddings). (3) Iterative Denoising Process (IDP), which iteratively translates raw audio (text) and the corresponding text embeddings (audio features) translated from previous iteration into the new less-noisy text embeddings (audio features). We adapt a dual cross-modal Transformer as our backbone model which consists of two unimodal encoders for IDAE and two cross-modal encoders for CDAE and IDP. Our method achieves comparable performance on multiple downstream speech understanding tasks compared with the model pre-trained on fully parallel data, demonstrating the great potential of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1657521317",
                    "name": "Yunxing Kang"
                },
                {
                    "authorId": "2115347033",
                    "name": "Tianqiao Liu"
                },
                {
                    "authorId": "2145572545",
                    "name": "Hang Li"
                },
                {
                    "authorId": "144853279",
                    "name": "Y. Hao"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                }
            ]
        },
        {
            "paperId": "47b43c2c24c0305861bb6ba89ff32c8a41814294",
            "title": "Towards the Memorization Effect of Neural Networks in Adversarial Training",
            "abstract": "Recent studies suggest that ``memorization'' is one important factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: (a) Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose Benign Adversarial Training (BAT) which can facilitate adversarial training to avoid fitting ``harmful'' atypical samples and fit as more ``benign'' atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets such as CIFAR100 and Tiny~ImageNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2018756699",
                    "name": "Han Xu"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "2108329255",
                    "name": "Wentao Wang"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "1739705",
                    "name": "Anil K. Jain"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "47b882a77ba3ea521e54846387599a92f7834135",
            "title": "Temporal-aware Language Representation Learning From Crowdsourced Labels",
            "abstract": "Learning effective language representations from crowdsourced labels is crucial for many real-world machine learning tasks. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability. Since the high-capacity deep neural networks can easily memorize all disagreements among crowdsourced labels, directly applying existing supervised language representation learning algorithms may yield suboptimal solutions. In this paper, we propose TACMA, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators. The proposed approach (1) explicitly models the intra-observer variability with attention mechanism; (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements. The proposed heuristic is extremely easy to implement in around 5 lines of code. The proposed heuristic is evaluated on four synthetic and four real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC. To encourage the reproducible results, we make our code publicly available at https://github.com/CrowdsourcingMining/TACMA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144853279",
                    "name": "Y. Hao"
                },
                {
                    "authorId": "152475512",
                    "name": "X. Zhai"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                }
            ]
        },
        {
            "paperId": "798c61b2b985e918a74b9aa154e6bc3f01040763",
            "title": "Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence",
            "abstract": "Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145902734",
                    "name": "Jian Guan"
                },
                {
                    "authorId": "29422474",
                    "name": "Xiaoxi Mao"
                },
                {
                    "authorId": "3120655",
                    "name": "Changjie Fan"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "1730108",
                    "name": "Minlie Huang"
                }
            ]
        },
        {
            "paperId": "7ffc1b425026e916cd6db37c79df3e08e8f47ee6",
            "title": "OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics",
            "abstract": "Automatic metrics are essential for developing natural language generation (NLG) models, particularly for open-ended language generation tasks such as story generation. However, existing automatic metrics are observed to correlate poorly with human evaluation. The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples. We evaluate existing metrics on OpenMEVA and observe that they have poor correlation with human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge (e.g., causal order between events), the generalization ability and robustness. Our study presents insights for developing NLG models and metrics in further research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145902734",
                    "name": "Jian Guan"
                },
                {
                    "authorId": "101371510",
                    "name": "Zhexin Zhang"
                },
                {
                    "authorId": "2108500252",
                    "name": "Zhuoer Feng"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "29422474",
                    "name": "Xiaoxi Mao"
                },
                {
                    "authorId": "3120655",
                    "name": "Changjie Fan"
                },
                {
                    "authorId": "1730108",
                    "name": "Minlie Huang"
                }
            ]
        },
        {
            "paperId": "bdf1cfedb9f0867d755589b9eea226d10643fb90",
            "title": "CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations",
            "abstract": "Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these models are facing challenges of overfitting with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra-modality and inter-modality connections between audio and language through two proxy tasks on a large amount of audio-and-language pairs: masked language modeling and masked cross-modal acoustic modeling. After fine-tuning our pre-trained model on multiple downstream audio-and-language tasks, we observe significant improvements across various tasks, such as, emotion classification, sentiment analysis, and speaker verification. On this basis, we further propose a specially-designed fusion mechanism that can be used in fine-tuning phase, which allows our pre-trained model to achieve better performance. Lastly, we demonstrate detailed ablation studies to prove that both our novel cross-modality fusion component and audio-language pre-training methods significantly contribute to the promising results. The code and pre-trained models are available at https://github.com/tal-ai/CTAL_EMNLP2021.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145572545",
                    "name": "Hang Li"
                },
                {
                    "authorId": "1657521317",
                    "name": "Yunxing Kang"
                },
                {
                    "authorId": "2115347033",
                    "name": "Tianqiao Liu"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                }
            ]
        },
        {
            "paperId": "0c19221c7b41865030cb277373e4861d7ee00e63",
            "title": "Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations",
            "abstract": "There is an increasing interest in the use of mathematical word problem (MWP) generation in educational assessment. Different from standard natural question generation, MWP generation needs to maintain the underlying mathematical operations between quantities and variables, while at the same time ensuring the relevance between the output and the given topic. To address above problem, we develop an end-to-end neural model to generate diverse MWPs in real-world scenarios from commonsense knowledge graph and equations. The proposed model (1) learns both representations from edge-enhanced Levi graphs of symbolic equations and commonsense knowledge; (2) automatically fuses equation and commonsense knowledge information via a self-planning module when generating the MWPs. Experiments on an educational gold-standard set and a large-scale generated MWP set show that our approach is superior on the MWP generation task, and it outperforms the SOTA models in terms of both automatic evaluation metrics, i.e., BLEU-4, ROUGE-L, Self-BLEU, and human evaluation metrics, i.e., equation relevance, topic relevance, and language coherence. To encourage reproducible results, we make our code and MWP dataset public available at https://github.com/tal-ai/MaKE_EMNLP2021.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115347033",
                    "name": "Tianqiao Liu"
                },
                {
                    "authorId": "2111828193",
                    "name": "Qian Fang"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                }
            ]
        },
        {
            "paperId": "61f2c91be7cb927fbf3f6f19d712a6f38f8a253f",
            "title": "Representation Learning From Limited Educational Data With Crowdsourced Labels",
            "abstract": "Representation learning has been proven to play an important role in the unprecedented success of machine learning models in numerous tasks, such as machine translation, face recognition and recommendation. The majority of existing representation learning approaches often require a large number of consistent and noise-free labels. However, due to various reasons such as budget constraints and privacy concerns, labels are very limited in many real-world scenarios. Directly applying standard representation learning approaches on small labeled data sets will easily run into over-fitting problems and lead to sub-optimal solutions. Even worse, in some domains such as education, the limited labels are usually annotated by multiple workers with diverse expertise, which yields noises and inconsistency in such crowdsourcing settings. In this paper, we propose a novel framework which aims to learn effective representations from limited data with crowdsourced labels. Specifically, we design a grouping based deep neural network to learn embeddings from a limited number of training samples and present a Bayesian confidence estimator to capture the inconsistency among crowdsourced labels. Furthermore, to expedite the training process, we develop a hard example selection procedure to adaptively pick up training examples that are misclassified by the model. Extensive experiments conducted on three real-world data sets demonstrate the superiority of our framework on learning representations from limited data with crowdsourced labels, comparing with various state-of-the-art baselines. In addition, we provide a comprehensive analysis on each of the main components of our proposed framework and also introduce the promising results it achieved in our real production to fully understand the proposed framework. To encourage reproducible results, we make our code available online at https://github.com/tal-ai/RECLE.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2108329255",
                    "name": "Wentao Wang"
                },
                {
                    "authorId": "2115724245",
                    "name": "Guowei Xu"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "119530748",
                    "name": "Gale Yan Huang"
                },
                {
                    "authorId": "2108490933",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                }
            ]
        },
        {
            "paperId": "63ebb3cb79fe9d91f52b4ab24c4280ee567173a6",
            "title": "Learning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition",
            "abstract": "Speech emotion recognition is a challenging task because the emotion expression is complex, multimodal and fine-grained. In this paper, we propose a novel multimodal deep learning approach to perform fine-grained emotion recognition from real-life speeches. We design a temporal alignment mean-max pooling mechanism to capture the subtle and fine-grained emotions implied in every utterance. In addition, we propose a cross modality excitement module to conduct sample-specific adjustment on cross modality embeddings and adaptively recalibrate the corresponding values by its aligned latent features from the other modality. Our proposed model is evaluated on two well-known real-world speech emotion recognition datasets. The results demonstrate that our approach is superior on the prediction tasks for multimodal speech utterances, and it outperforms a wide range of baselines in terms of prediction accuracy. Further more, we conduct detailed ablation studies to show that our temporal alignment mean-max pooling mechanism and cross modality excitement significantly contribute to the promising results. In order to encourage the research reproducibility, we make the code publicly available at \\url{https://github.com/tal-ai/FG_CME.git}.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2145572545",
                    "name": "Hang Li"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                }
            ]
        },
        {
            "paperId": "b2576344bd6ed59b2f17e385eef75da95de61af5",
            "title": "Learning Fine-Grained Multimodal Alignment for Speech Emotion Recognition",
            "abstract": "Speech emotion recognition is a challenging task because the emotion expression is complex, multimodal and fine-grained. In this paper, we propose a novel multimodal deep learning approach to perform fine-grained emotion recognition from real-life speeches. We design a temporal alignment pooling mechanism to capture the subtle and fine-grained emotions implied in every utterance. In addition, we propose a cross modality excitation module to conduct sample-specific activations on acoustic embedding dimensions and adaptively recalibrate the corresponding values by latent semantic features. The proposed model is evaluated on two well-known real-world speech emotion recognition datasets. The results demonstrate that our approach is superior on the prediction tasks for multimodal speech utterances, and it outperforms a wide range of baselines in terms of prediction accuracy. In order to encourage the research reproducibility, we make the code publicly available at https://github.com/hzlihang99/icassp2021_CME.git.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145572545",
                    "name": "Hang Li"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                }
            ]
        }
    ]
}