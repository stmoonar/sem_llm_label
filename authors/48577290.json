{
    "authorId": "48577290",
    "papers": [
        {
            "paperId": "05db3d0f88615ac5edb5499f7ea5bdebe2c149bc",
            "title": "Predictive Performance Comparison of Decision Policies Under Confounding",
            "abstract": "Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3430706",
                    "name": "Luke M. Guerdan"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "2257955034",
                    "name": "Kenneth Holstein"
                },
                {
                    "authorId": "2272702885",
                    "name": "Zhiwei Steven Wu"
                }
            ]
        },
        {
            "paperId": "1610d01227fcd123fb8931ec004010264d5c2531",
            "title": "The Situate AI Guidebook: Co-Designing a Toolkit to Support Multi-Stakeholder, Early-stage Deliberations Around Public Sector AI Proposals",
            "abstract": "Public sector agencies are rapidly deploying AI systems to augment or automate critical decisions in real-world contexts like child welfare, criminal justice, and public health. A growing body of work documents how these AI systems often fail to improve services in practice. These failures can often be traced to decisions made during the early stages of AI ideation and design, such as problem formulation. However, today, we lack systematic processes to support effective, early-stage decision-making about whether and under what conditions to move forward with a proposed AI project. To understand how to scaffold such processes in real-world settings, we worked with public sector agency leaders, AI developers, frontline workers, and community advocates across four public sector agencies and three community advocacy groups in the United States. Through an iterative co-design process, we created the Situate AI Guidebook: a structured process centered around a set of deliberation questions to scaffold conversations around (1) goals and intended use for a proposed AI system, (2) societal and legal considerations, (3) data and modeling constraints, and (4) organizational governance factors. We discuss how the guidebook\u2019s design is informed by participants\u2019 challenges, needs, and desires for improved deliberation processes. We further elaborate on implications for designing responsible AI toolkits in collaboration with public sector agency stakeholders and opportunities for future work to expand upon the guidebook. This design approach can be more broadly adopted to support the co-creation of responsible AI toolkits that scaffold key decision-making processes surrounding the use of AI in the public sector and beyond.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1467645111",
                    "name": "Anna Kawakami"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "2257905527",
                    "name": "Haiyi Zhu"
                },
                {
                    "authorId": "2265427376",
                    "name": "Hoda Heidari"
                },
                {
                    "authorId": "2257955034",
                    "name": "Kenneth Holstein"
                }
            ]
        },
        {
            "paperId": "55fe44aef12866205e5e4e8cecb74679c3ca2ad3",
            "title": "Studying Up Public Sector AI: How Networks of Power Relations Shape Agency Decisions Around AI Design and Use",
            "abstract": "As public sector agencies rapidly introduce new AI tools in high-stakes domains like social services, it becomes critical to understand how decisions to adopt these tools are made in practice. We borrow from the anthropological practice to ``study up'' those in positions of power, and reorient our study of public sector AI around those who have the power and responsibility to make decisions about the role that AI tools will play in their agency. Through semi-structured interviews and design activities with 16 agency decision-makers, we examine how decisions about AI design and adoption are influenced by their interactions with and assumptions about other actors within these agencies (e.g., frontline workers and agency leaders), as well as those above (legal systems and contracted companies), and below (impacted communities). By centering these networks of power relations, our findings shed light on how infrastructural, legal, and social factors create barriers and disincentives to the involvement of a broader range of stakeholders in decisions about AI design and adoption. Agency decision-makers desired more practical support for stakeholder involvement around public sector AI to help overcome the knowledge and power differentials they perceived between them and other stakeholders (e.g., frontline workers and impacted community members). Building on these findings, we discuss implications for future research and policy around actualizing participatory AI approaches in public sector contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1467645111",
                    "name": "Anna Kawakami"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "2265427376",
                    "name": "Hoda Heidari"
                },
                {
                    "authorId": "2257955034",
                    "name": "Kenneth Holstein"
                },
                {
                    "authorId": "2257905527",
                    "name": "Haiyi Zhu"
                }
            ]
        },
        {
            "paperId": "346e4f35a5a81ef893792133ec1fec18f23c1768",
            "title": "Examining risks of racial biases in NLP tools for child protective services",
            "abstract": "Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49713890",
                    "name": "Anjalie Field"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "47404598",
                    "name": "Nupoor Gandhi"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                },
                {
                    "authorId": "1398859796",
                    "name": "Emily Putnam-Hornstein"
                },
                {
                    "authorId": "2082303978",
                    "name": "David Steier"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "6b3933397606168f65741c2fd2c2ea07d90b4a19",
            "title": "Counterfactual Prediction Under Outcome Measurement Error",
            "abstract": "Across domains such as medicine, employment, and criminal justice, predictive models often target labels that imperfectly reflect the outcomes of interest to experts and policymakers. For example, clinical risk assessments deployed to inform physician decision-making often predict measures of healthcare utilization (e.g., costs, hospitalization) as a proxy for patient medical need. These proxies can be subject to outcome measurement error when they systematically differ from the target outcome they are intended to measure. However, prior modeling efforts to characterize and mitigate outcome measurement error overlook the fact that the decision being informed by a model often serves as a risk-mitigating intervention that impacts the target outcome of interest and its recorded proxy. Thus, in these settings, addressing measurement error requires counterfactual modeling of treatment effects on outcomes. In this work, we study intersectional threats to model reliability introduced by outcome measurement error, treatment effects, and selection bias from historical decision-making policies. We develop an unbiased risk minimization method which, given knowledge of proxy measurement error properties, corrects for the combined effects of these challenges. We also develop a method for estimating treatment-dependent measurement error parameters when these are unknown in advance. We demonstrate the utility of our approach theoretically and via experiments on real-world data from randomized controlled trials conducted in healthcare and employment domains. As importantly, we demonstrate that models correcting for outcome measurement error or treatment effects alone suffer from considerable reliability limitations. Our work underscores the importance of considering intersectional threats to model validity during the design and evaluation of predictive models for decision support.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3430706",
                    "name": "Luke M. Guerdan"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "2257955034",
                    "name": "Kenneth Holstein"
                },
                {
                    "authorId": "1768074",
                    "name": "Zhiwei Steven Wu"
                }
            ]
        },
        {
            "paperId": "a8c684ba3f53382884f90981cae040649942f714",
            "title": "Recentering Validity Considerations through Early-Stage Deliberations Around AI and Policy Design",
            "abstract": "AI-based decision-making tools are rapidly spreading across a range of real-world, complex domains like healthcare, criminal justice, and child welfare. A growing body of research has called for increased scrutiny around the validity of AI system designs. However, in real-world settings, it is often not possible to fully address questions around the validity of an AI tool without also considering the design of associated organizational and public policies. Yet, considerations around how an AI tool may interface with policy are often only discussed retrospectively, after the tool is designed or deployed. In this short position paper, we discuss opportunities to promote multi-stakeholder deliberations around the design of AI-based technologies and associated policies, at the earliest stages of a new project.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1467645111",
                    "name": "Anna Kawakami"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "1742431",
                    "name": "Haiyi Zhu"
                },
                {
                    "authorId": "2253600",
                    "name": "Hoda Heidari"
                },
                {
                    "authorId": "2257955034",
                    "name": "Kenneth Holstein"
                }
            ]
        },
        {
            "paperId": "cffb72b4d815461c8122a03f5d9b5b62d80e294b",
            "title": "Ground(less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making",
            "abstract": "A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on \u201cground truth\u2019\u2019 labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision \u2013 including latent constructs that are not directly observable, such as disease status, the \u201ctoxicity\u201d of online comments, or future \u201cjob performance\u201d \u2013 predictive models target proxy labels that are readily available in existing datasets. Predictive models\u2019 reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3430706",
                    "name": "Luke M. Guerdan"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "1768074",
                    "name": "Zhiwei Steven Wu"
                },
                {
                    "authorId": "2257955034",
                    "name": "Kenneth Holstein"
                }
            ]
        },
        {
            "paperId": "0705ddb754ad0a7cf43c54617a18c9ff342f639d",
            "title": "A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms",
            "abstract": "Recent research increasingly brings to question the appropriateness of using predictive tools in complex, real-world tasks. While a growing body of work has explored ways to improve value alignment in these tools, comparatively less work has centered concerns around the fundamental justifiability of using these tools. This work seeks to center validity considerations in deliberations around whether and how to build data-driven algorithms in high-stakes domains. Toward this end, we translate key concepts from validity theory to predictive algorithms. We apply the lens of validity to re-examine common challenges in problem formulation and data issues that jeopardize the justifiability of using predictive algorithms and connect these challenges to the social science discourse around validity. Our interdisciplinary exposition clarifies how these concepts apply to algorithmic decision making contexts. We demonstrate how these validity considerations could distill into a series of high-level questions intended to promote and document reflections on the legitimacy of the predictive task and the suitability of the data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "1467645111",
                    "name": "Anna Kawakami"
                },
                {
                    "authorId": "1742431",
                    "name": "Haiyi Zhu"
                },
                {
                    "authorId": "2320027147",
                    "name": "Kenneth Holstein"
                },
                {
                    "authorId": "2253600",
                    "name": "Hoda Heidari"
                }
            ]
        },
        {
            "paperId": "0c799e9999c8de278d12b0c96d5d2f7fc544d5c8",
            "title": "Distributionally Robust Survival Analysis: A Novel Fairness Loss Without Demographics",
            "abstract": "We propose a general approach for training survival analysis models that minimizes a worst-case error across all subpopulations that are large enough (occurring with at least a user-specified minimum probability). This approach uses a training loss function that does not know any demographic information to treat as sensitive. Despite this, we demonstrate that our proposed approach often scores better on recently established fairness metrics (without a significant drop in prediction accuracy) compared to various baselines, including ones which directly use sensitive demographic information in their training loss. Our code is available at: https://github.com/discovershu/DRO_COX",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2122826299",
                    "name": "Shu Hu"
                },
                {
                    "authorId": "27694416",
                    "name": "George H. Chen"
                },
                {
                    "authorId": "51928199",
                    "name": "Shahriar Noroozizadeh"
                },
                {
                    "authorId": "1898068",
                    "name": "Jeremy C. Weiss"
                },
                {
                    "authorId": "2107917677",
                    "name": "Linhong Li"
                },
                {
                    "authorId": "2059428269",
                    "name": "Ren Zuo"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "11486955",
                    "name": "Helen Zhou"
                },
                {
                    "authorId": "2027359212",
                    "name": "Cheng Cheng"
                }
            ]
        },
        {
            "paperId": "72cf64fe6c8c5941ec767410ecf39300b847e6a1",
            "title": "Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding",
            "abstract": "Predictive algorithms inform consequential decisions in settings where the outcome is selectively observed given choices made by human decision makers. We propose a unified framework for the robust design and evaluation of predictive algorithms in selectively observed data. We impose general assumptions on how much the outcome may vary on average between unselected and selected units conditional on observed covariates and identified nuisance parameters, formalizing popular empirical strategies for imputing missing data such as proxy outcomes and instrumental variables. We develop debiased machine learning estimators for the bounds on a large class of predictive performance estimands, such as the conditional likelihood of the outcome, a predictive algorithm's mean square error, true/false positive rate, and many others, under these assumptions. In an administrative dataset from a large Australian financial institution, we illustrate how varying assumptions on unobserved confounding leads to meaningful changes in default risk predictions and evaluations of credit scores across sensitive groups.",
            "fieldsOfStudy": [
                "Economics",
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "88738815",
                    "name": "Ashesh Rambachan"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "48224429",
                    "name": "Edward H. Kennedy"
                }
            ]
        }
    ]
}