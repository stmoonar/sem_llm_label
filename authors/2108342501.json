{
    "authorId": "2108342501",
    "papers": [
        {
            "paperId": "87489be98980c08edd1a90937bbb1927db42f528",
            "title": "What, When, and Where? Self-Supervised Spatio- Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions",
            "abstract": "Spatio-temporal grounding describes the task of localizing events in space and time, e.g., in video data, based on verbal descriptions only. Models for this task are usually trained with human-annotated sentences and bounding box supervision. This work addresses this task from a multimodal supervision perspective, proposing a framework for spatio-temporal action grounding trained on loose video and subtitle supervision only, without human annotation. To this end, we combine local representation learning, which focuses on leveraging fine-grained spatial information, with a global representation encoding that captures higher-level representations and incorporates both in a joint approach. To evaluate this challenging task in a real-life setting, a new benchmark dataset is proposed, providing dense spatio-temporal grounding annotations in long, untrimmed, multi-action instructional videos for over 5K events. We evaluate the proposed approach and other methods on the proposed and standard downstream tasks, showing that our method improves over current baselines in various settings, including spatial, temporal, and untrimmed multi-action spatio-temporal grounding.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "2125336203",
                    "name": "Nina Shvetsova"
                },
                {
                    "authorId": "2110769738",
                    "name": "Andrew Rouditchenko"
                },
                {
                    "authorId": "1793546",
                    "name": "D. Kondermann"
                },
                {
                    "authorId": "2148303141",
                    "name": "Samuel Thomas"
                },
                {
                    "authorId": "2122374530",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "1723233",
                    "name": "R. Feris"
                },
                {
                    "authorId": "2070216353",
                    "name": "James Glass"
                },
                {
                    "authorId": "2077580009",
                    "name": "Hilde Kuehne"
                }
            ]
        },
        {
            "paperId": "aaea3b1c481098686c34c093d2b36156689db115",
            "title": "Weakly-Supervised Temporal Article Grounding",
            "abstract": "Given a long untrimmed video and natural language queries, video grounding (VG) aims to temporally localize the semantically-aligned video segments. Almost all existing VG work holds two simple but unrealistic assumptions: 1) All query sentences can be grounded in the corresponding video. 2) All query sentences for the same video are always at the same semantic scale. Unfortunately, both assumptions make today\u2019s VG models fail to work in practice. For example, in real-world multimodal assets (eg, news articles), most of the sentences in the article can not be grounded in their affiliated videos, and they typically have rich hierarchical relations (ie, at different semantic scales). To this end, we propose a new challenging grounding task: Weakly-Supervised temporal Article Grounding (WSAG). Specifically, given an article and a relevant video, WSAG aims to localize all \u201cgroundable\u201d sentences to the video, and these sentences are possibly at different semantic scales. Accordingly, we collect the first WSAG dataset to facilitate this task: YouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow articles and plentiful YouTube videos. In addition, we propose a simple but effective method DualMIL for WSAG, which consists of a two-level MIL loss and a single-/cross- sentence constraint loss. These training objectives are carefully designed for these relaxed assumptions. Extensive ablations have verified the effectiveness of DualMIL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143891667",
                    "name": "Long Chen"
                },
                {
                    "authorId": "2520427",
                    "name": "Yulei Niu"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "48030192",
                    "name": "Xudong Lin"
                },
                {
                    "authorId": "2067641876",
                    "name": "G. Han"
                },
                {
                    "authorId": "2150796972",
                    "name": "Christopher Thomas"
                },
                {
                    "authorId": "1381855534",
                    "name": "Hammad A. Ayyubi"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2122374530",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "039ce73659332c12168de439e3f79e7039b636af",
            "title": "RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System",
            "abstract": "We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4428136",
                    "name": "Haoyang Wen"
                },
                {
                    "authorId": "2117032681",
                    "name": "Ying Lin"
                },
                {
                    "authorId": "145242558",
                    "name": "T. Lai"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "2109154767",
                    "name": "Sha Li"
                },
                {
                    "authorId": "48030192",
                    "name": "Xudong Lin"
                },
                {
                    "authorId": "2108536188",
                    "name": "Ben Zhou"
                },
                {
                    "authorId": "2118482058",
                    "name": "Manling Li"
                },
                {
                    "authorId": "34269118",
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "2111112132",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "3099583",
                    "name": "Xiaodong Yu"
                },
                {
                    "authorId": "2101316346",
                    "name": "Alexander Dong"
                },
                {
                    "authorId": "2108330537",
                    "name": "Zhenhailong Wang"
                },
                {
                    "authorId": "51135899",
                    "name": "Y. Fung"
                },
                {
                    "authorId": "51234098",
                    "name": "Piyush Mishra"
                },
                {
                    "authorId": "1904906987",
                    "name": "Qing Lyu"
                },
                {
                    "authorId": "35399640",
                    "name": "D\u00eddac Sur\u00eds"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "1783500",
                    "name": "S. Brown"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                },
                {
                    "authorId": "1856025",
                    "name": "Carl Vondrick"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "144590225",
                    "name": "D. Roth"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "220f7abcc7cc0b71c252de49209d8db39a7ca3a5",
            "title": "Routing with Self-Attention for Multimodal Capsule Networks",
            "abstract": "The task of multimodal learning has seen a growing interest recently as it allows for training neural architectures based on different modalities such as vision, text, and audio. One challenge in training such models is that they need to jointly learn semantic concepts and their relationships across different input representations. Capsule networks have been shown to perform well in context of capturing the relation between low-level input features and higher-level concepts. However, capsules have so far mainly been used only in small-scale fully supervised settings due to the resource demand of conventional routing algorithms. We present a new multimodal capsule network that allows us to leverage the strength of capsules in the context of a multimodal learning framework on large amounts of video data. To adapt the capsules to large-scale input data, we propose a novel routing by self-attention mechanism that selects relevant capsules which are then used to generate a final joint multimodal feature representation. This allows not only for robust training with noisy video data, but also to scale up the size of the capsule network compared to traditional routing methods while still being computationally efficient. We evaluate the proposed architecture by pretraining it on a large-scale multimodal video dataset and applying it on four datasets in two challenging downstream tasks. Results show that the proposed multimodal capsule network is not only able to improve results compared to other routing techniques, but also achieves competitive performance on the task of multimodal learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064921899",
                    "name": "Kevin Duarte"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "2125336203",
                    "name": "Nina Shvetsova"
                },
                {
                    "authorId": "2110769738",
                    "name": "Andrew Rouditchenko"
                },
                {
                    "authorId": "2148303141",
                    "name": "Samuel Thomas"
                },
                {
                    "authorId": "49285584",
                    "name": "Alexander H. Liu"
                },
                {
                    "authorId": "30507748",
                    "name": "David F. Harwath"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                },
                {
                    "authorId": "2077580009",
                    "name": "Hilde Kuehne"
                },
                {
                    "authorId": "145103012",
                    "name": "M. Shah"
                }
            ]
        },
        {
            "paperId": "66c5a67baf4f1c3302dcff05febfc19c16e7f457",
            "title": "Joint Multimedia Event Extraction from Video and Article",
            "abstract": "Visual and textual modalities contribute complementary information about events described in multimedia documents. Videos contain rich dynamics and detailed unfoldings of events, while text describes more high-level and abstract concepts. However, existing event extraction methods either do not handle video or solely target video while ignoring other modalities. In contrast, we propose the first approach to jointly extract events from video and text articles. We introduce the new task of Video MultiMedia Event Extraction (Video M2E2) and propose two novel components to build the first system towards this task. First, we propose the first self-supervised multimodal event coreference model that can determine coreference between video events and text events without any manually annotated pairs. Second, we introduce the first multimodal transformer which extracts structured event information jointly from both videos and text documents. We also construct and will publicly release a new benchmark of video-article pairs, consisting of 860 video-article pairs with extensive annotations for evaluating methods on this task. Our experimental results demonstrate the effectiveness of our proposed method on our new benchmark dataset. We achieve 6.0% and 5.8% absolute F-score gain on multimodal event coreference resolution and multimedia event extraction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "48030192",
                    "name": "Xudong Lin"
                },
                {
                    "authorId": "2150796972",
                    "name": "Christopher Thomas"
                },
                {
                    "authorId": "2118482058",
                    "name": "Manling Li"
                },
                {
                    "authorId": "119361260",
                    "name": "Shoya Yoshida"
                },
                {
                    "authorId": "88999446",
                    "name": "Lovish Chum"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "7739764dbc65d13ca8c5e8825c7cf4f98f309175",
            "title": "Cascaded Multilingual Audio-Visual Learning from Videos",
            "abstract": "In this paper, we explore self-supervised audio-visual models that learn from instructional videos. Prior work has shown that these models can relate spoken words and sounds to visual content after training on a large-scale dataset of videos, but they were only trained and evaluated on videos in English. To learn multilingual audio-visual representations, we propose a cascaded approach that leverages a model trained on English videos and applies it to audio-visual data in other languages, such as Japanese videos. With our cascaded approach, we show an improvement in retrieval performance of nearly 10x compared to training on the Japanese videos solely. We also apply the model trained on English videos to Japanese and Hindi spoken captions of images, achieving state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2110769738",
                    "name": "Andrew Rouditchenko"
                },
                {
                    "authorId": "1394839535",
                    "name": "Angie Boggust"
                },
                {
                    "authorId": "30507748",
                    "name": "David F. Harwath"
                },
                {
                    "authorId": "152809214",
                    "name": "Samuel Thomas"
                },
                {
                    "authorId": "2077580009",
                    "name": "Hilde Kuehne"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "1819152",
                    "name": "Rameswar Panda"
                },
                {
                    "authorId": "1723233",
                    "name": "R. Feris"
                },
                {
                    "authorId": "144707379",
                    "name": "Brian Kingsbury"
                },
                {
                    "authorId": "1774515",
                    "name": "M. Picheny"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                }
            ]
        },
        {
            "paperId": "c2c9f1f9f37f8ea8583a6c69400b8ef20123c95f",
            "title": "PreViTS: Contrastive Pretraining with Video Tracking Supervision",
            "abstract": "Videos are a rich source for self-supervised learning (SSL) of visual representations due to the presence of natural temporal transformations of objects. However, current methods typically randomly sample video clips for learning, which results in an imperfect supervisory signal. In this work, we propose PreViTS, an SSL framework that utilizes an unsupervised tracking signal for selecting clips containing the same object, which helps better utilize temporal transformations of objects. PreViTS further uses the tracking signal to spatially constrain the frame regions to learn from and trains the model to locate meaningful objects by providing supervision on Grad-CAM attention maps. To evaluate our approach, we train a momentum contrastive (MoCo) encoder on VGG-Sound and Kinetics-400 datasets with PreViTS. Training with PreViTS outperforms representations learnt by contrastive strategy alone on video downstream tasks, obtaining state-of-the-art performance on action classification. PreViTS helps learn feature representations that are more robust to changes in background and context, as seen by experiments on datasets with background changes. Our experiment also demonstrates various visual transformation invariance captured by our model. Learning from large-scale videos with PreViTS could lead to more accurate and robust visual feature representations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "35100058",
                    "name": "Ramprasaath R. Selvaraju"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2047256670",
                    "name": "N. Naik"
                }
            ]
        },
        {
            "paperId": "c5c3ad98547202f120aaae4007cc665bdff0f447",
            "title": "Everything at Once \u2013 Multi-modal Fusion Transformer for Video Retrieval",
            "abstract": "Multi-modal learning from video data has seen increased attention recently as it allows training of semantically meaningful embeddings without human annotation, enabling tasks like zero-shot retrieval and action localization. In this work, we present a multi-modal, modality agnostic fusion transformer that learns to exchange information between multiple modalities, such as video, audio, and text, and integrate them into a fused representation in a joined multi-modal embedding space. We propose to train the system with a combinatorial loss on everything at once \u2013 any combination of input modalities, such as single modalities as well as pairs of modalities, explicitly leaving out any add-ons such as position or modality encoding. At test time, the resulting model can process and fuse any number of input modalities. Moreover, the implicit properties of the transformer allow to process inputs of different lengths. To evaluate the proposed approach, we train the model on the large scale HowTo100M dataset and evaluate the resulting embedding space on four challenging benchmark datasets obtaining state-of-the-art results in zero-shot video retrieval and zero-shot video action localization. Our code for this work is also available.11https://github.com/ninatu/everything_at_once",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2125336203",
                    "name": "Nina Shvetsova"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "2110769738",
                    "name": "Andrew Rouditchenko"
                },
                {
                    "authorId": "2148303141",
                    "name": "Samuel Thomas"
                },
                {
                    "authorId": "144707379",
                    "name": "Brian Kingsbury"
                },
                {
                    "authorId": "1723233",
                    "name": "R. Feris"
                },
                {
                    "authorId": "30507748",
                    "name": "David F. Harwath"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                },
                {
                    "authorId": "2077580009",
                    "name": "Hilde Kuehne"
                }
            ]
        },
        {
            "paperId": "d9b1bb8053f32c6da9bbbec564d750d55b486f00",
            "title": "Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos",
            "abstract": "Multimodal self-supervised learning is getting more and more attention as it allows not only to train large networks without human supervision but also to search and retrieve data across various modalities. In this context, this paper proposes a framework that, starting from a pre-trained backbone, learns a common multimodal embedding space that, in addition to sharing representations across different modalities, enforces a grouping of semantically similar instances. To this end, we extend the concept of instance-level contrastive learning with a multimodal clustering step in the training pipeline to capture semantic similarities across modalities. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains. To evaluate our approach, we train our model on the HowTo100M dataset and evaluate its zero-shot retrieval capabilities in two challenging domains, namely text-to-video retrieval, and temporal action localization, showing state-of-the-art results on four different datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "41020711",
                    "name": "Andrew Rouditchenko"
                },
                {
                    "authorId": "2064921899",
                    "name": "Kevin Duarte"
                },
                {
                    "authorId": "2077580009",
                    "name": "Hilde Kuehne"
                },
                {
                    "authorId": "152809214",
                    "name": "Samuel Thomas"
                },
                {
                    "authorId": "1394839535",
                    "name": "Angie Boggust"
                },
                {
                    "authorId": "1819152",
                    "name": "Rameswar Panda"
                },
                {
                    "authorId": "144707379",
                    "name": "Brian Kingsbury"
                },
                {
                    "authorId": "1723233",
                    "name": "R. Feris"
                },
                {
                    "authorId": "30507748",
                    "name": "David F. Harwath"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                },
                {
                    "authorId": "1774515",
                    "name": "M. Picheny"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "33558aa3311b256880346524981408449b58bc58",
            "title": "General Partial Label Learning via Dual Bipartite Graph Autoencoder",
            "abstract": "We formulate a practical yet challenging problem: General Partial Label Learning (GPLL). Compared to the traditional Partial Label Learning (PLL) problem, GPLL relaxes the supervision assumption from instance-level \u2014 a label set partially labels an instance \u2014 to group-level: 1) a label set partially labels a group of instances, where the within-group instance-label link annotations are missing, and 2) cross-group links are allowed \u2014 instances in a group may be partially linked to the label set from another group. Such ambiguous group-level supervision is more practical in real-world scenarios as additional annotation on the instance-level is no longer required, e.g., face-naming in videos where the group consists of faces in a frame, labeled by a name set in the corresponding caption. In this paper, we propose a novel graph convolutional network (GCN) called Dual Bipartite Graph Autoencoder (DB-GAE) to tackle the label ambiguity challenge of GPLL. First, we exploit the cross-group correlations to represent the instance groups as dual bipartite graphs: within-group and cross-group, which reciprocally complements each other to resolve the linking ambiguities. Second, we design a GCN autoencoder to encode and decode them, where the decodings are considered as the refined results. It is worth noting that DB-GAE is self-supervised and transductive, as it only uses the group-level supervision without a separate offline training stage. Extensive experiments on two real-world datasets demonstrate that DB-GAE significantly outperforms the best baseline over absolute 0.159 F1-score and 24.8% accuracy. We further offer analysis on various levels of label ambiguities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "2778637",
                    "name": "Alireza Zareian"
                },
                {
                    "authorId": "5462268",
                    "name": "Hanwang Zhang"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                }
            ]
        }
    ]
}