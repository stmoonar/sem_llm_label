{
    "authorId": "2290248265",
    "papers": [
        {
            "paperId": "450a0d2ba0d689537230e774219a7695ede1533e",
            "title": "Retrieval-Oriented Knowledge for Click-Through Rate Prediction",
            "abstract": "Click-through rate (CTR) prediction is crucial for personalized online services. Sample-level retrieval-based models, such as RIM, have demonstrated remarkable performance. However, they face challenges including inference inefficiency and high resource consumption due to the retrieval process, which hinder their practical application in industrial settings. To address this, we propose a universal plug-and-play \\underline{r}etrieval-\\underline{o}riented \\underline{k}nowledge (\\textbf{\\name}) framework that bypasses the real retrieval process. The framework features a knowledge base that preserves and imitates the retrieved \\&aggregated representations using a decomposition-reconstruction paradigm. Knowledge distillation and contrastive learning optimize the knowledge base, enabling the integration of retrieval-enhanced representations with various CTR models. Experiments on three large-scale datasets demonstrate \\name's exceptional compatibility and performance, with the neural knowledge base serving as an effective surrogate for the retrieval pool. \\name surpasses the teacher model while maintaining superior inference efficiency and demonstrates the feasibility of distilling knowledge from non-parametric methods using a parametric approach. These results highlight \\name's strong potential for real-world applications and its ability to transform retrieval-based methods into practical solutions. Our implementation code is available to support reproducibility in \\url{https://github.com/HSLiu-Initial/ROK.git}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167046956",
                    "name": "Huanshuo Liu"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2238203237",
                    "name": "Menghui Zhu"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "79494403",
                    "name": "Jiarui Qin"
                },
                {
                    "authorId": "2290248265",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2298987734",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "52dc538cf0f0b979105712ae073ca125602cda25",
            "title": "AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising",
            "abstract": "Click-Through Rate (CTR) prediction is a fundamental technique for online advertising recommendation and the complex online competitive auction process also brings many difficulties to CTR optimization. Recent studies have shown that introducing posterior auction information contributes to the performance of CTR prediction. However, existing work doesn't fully capitalize on the benefits of auction information and overlooks the data bias brought by the auction, leading to biased and suboptimal results. To address these limitations, we propose Auction Information Enhanced Framework (AIE) for CTR prediction in online advertising, which delves into the problem of insufficient utilization of auction signals and first reveals the auction bias. Specifically, AIE introduces two pluggable modules, namely Adaptive Market-price Auxiliary Module (AM2) and Bid Calibration Module (BCM), which work collaboratively to excavate the posterior auction signals better and enhance the performance of CTR prediction. Furthermore, the two proposed modules are lightweight, model-agnostic, and friendly to inference latency. Extensive experiments are conducted on a public dataset and an industrial dataset to demonstrate the effectiveness and compatibility of AIE. Besides, a one-month online A/B test in a large-scale advertising platform shows that AIE improves the base model by 5.76% and 2.44% in terms of eCPM and CTR, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290248265",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2115802321",
                    "name": "Chenxu Zhu"
                },
                {
                    "authorId": "2238203237",
                    "name": "Menghui Zhu"
                },
                {
                    "authorId": "2105646417",
                    "name": "Xinyi Dai"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2287873067",
                    "name": "Muyu Zhang"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2262216857",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "8a439444d888202a711b8b8a195934cdb138342e",
            "title": "Towards Efficient and Effective Unlearning of Large Language Models for Recommendation",
            "abstract": "The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \\textit{inefficiency} and \\textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \\textbf{E2URec}, the first \\underline{E}fficient and \\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process. Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets. Specifically, E2URec can efficiently forget specific data without affecting recommendation performance. The source code is at \\url{https://github.com/justarter/E2URec}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283141638",
                    "name": "Hangyu Wang"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2290248265",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                }
            ]
        }
    ]
}