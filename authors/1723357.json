{
    "authorId": "1723357",
    "papers": [
        {
            "paperId": "1604c2854370b9c6c9558b360c566f058ef33e7f",
            "title": "Learning Topology-Specific Experts for Molecular Property Prediction",
            "abstract": "Recently, graph neural networks (GNNs) have been successfully applied to predicting molecular properties, which is one of the most classical cheminformatics tasks with various applications. Despite their effectiveness, we empirically observe that training a single GNN model for diverse molecules with distinct structural patterns limits its prediction performance. In this paper, motivated by this observation, we propose TopExpert to leverage topology-specific prediction models (referred to as experts), each of which is responsible for each molecular group sharing similar topological semantics. That is, each expert learns topology-specific discriminative features while being trained with its corresponding topological group. To tackle the key challenge of grouping molecules by their topological patterns, we introduce a clustering-based gating module that assigns an input molecule into one of the clusters and further optimizes the gating module with two different types of self-supervision: topological semantics induced by GNNs and molecular scaffolds, respectively. Extensive experiments demonstrate that TopExpert has boosted the performance for molecular property prediction and also achieved better generalization for new molecules with unseen scaffolds than baselines. The code is available at https://github.com/kimsu55/ToxExpert.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2143061094",
                    "name": "S. Kim"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "72490748",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "1c10f092ee7cddfcef08fb5eb4e55acc6b71a95e",
            "title": "Distillation from Heterogeneous Models for Top-K Recommendation",
            "abstract": "Recent recommender systems have shown remarkable performance by using an ensemble of heterogeneous models. However, it is exceedingly costly because it requires resources and inference latency proportional to the number of models, which remains the bottleneck for production. Our work aims to transfer the ensemble knowledge of heterogeneous teachers to a lightweight student model using knowledge distillation (KD), to reduce the huge inference costs while retaining high accuracy. Through an empirical study, we find that the efficacy of distillation severely drops when transferring knowledge from heterogeneous teachers. Nevertheless, we show that an important signal to ease the difficulty can be obtained from the teacher\u2019s training trajectory. This paper proposes a new KD framework, named HetComp, that guides the student model by transferring easy-to-hard sequences of knowledge generated from the teachers\u2019 trajectories. To provide guidance according to the student\u2019s learning state, HetComp uses dynamic knowledge construction to provide progressively difficult ranking knowledge and adaptive knowledge transfer to gradually transfer finer-grained ranking information. Our comprehensive experiments show that HetComp significantly improves the distillation quality and the generalization of the student model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "1643930327",
                    "name": "Wonbin Kweon"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "2813328",
                    "name": "Jianxun Lian"
                },
                {
                    "authorId": "2110972323",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "b838508bcc1f591b7db00dabe19678fe8b0b3ce0",
            "title": "Dynamic Multi-Behavior Sequence Modeling for Next Item Recommendation",
            "abstract": "Sequential Recommender Systems (SRSs) aim to predict the next item that users will consume, by modeling the user interests within their item sequences. While most existing SRSs focus on a single type of user behavior, only a few pay attention to multi-behavior sequences, although they are very common in real-world scenarios. It is challenging to effectively capture the user interests within multi-behavior sequences, because the information about user interests is entangled throughout the sequences in complex relationships. To this end, we first address the characteristics of multi-behavior sequences that should be considered in SRSs, and then propose novel methods for Dynamic Multi-behavior Sequence modeling named DyMuS, which is a light version, and DyMuS+, which is an improved version, considering the characteristics. DyMuS first encodes each behavior sequence independently, and then combines the encoded sequences using dynamic routing, which dynamically integrates information required in the final result from among many candidates, based on correlations between the sequences. DyMuS+, furthermore, applies the dynamic routing even to encoding each behavior sequence to further capture the correlations at item-level. Moreover, we release a new, large and up-to-date dataset for multi-behavior recommendation. Our experiments on DyMuS and DyMuS+ show their superiority and the significance of capturing the characteristics of multi-behavior sequences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1929729355",
                    "name": "Junsu Cho"
                },
                {
                    "authorId": "108148310",
                    "name": "Dongmin Hyun"
                },
                {
                    "authorId": "2203359343",
                    "name": "Dong won Lim"
                },
                {
                    "authorId": "1986810",
                    "name": "H. Cheon"
                },
                {
                    "authorId": "2203425498",
                    "name": "Hyoung-iel Park"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "03cefe16cf6abcabe4f702b69f84f30dd6c2cd6e",
            "title": "Generating Multiple-Length Summaries via Reinforcement Learning for Unsupervised Sentence Summarization",
            "abstract": "Sentence summarization shortens given texts while maintaining core contents of the texts. Unsupervised approaches have been studied to summarize texts without human-written summaries. However, recent unsupervised models are extractive, which remove words from texts and thus they are less flexible than abstractive summarization. In this work, we devise an abstractive model based on reinforcement learning without ground-truth summaries. We formulate the unsupervised summarization based on the Markov decision process with rewards representing the summary quality. To further enhance the summary quality, we develop a multi-summary learning mechanism that generates multiple summaries with varying lengths for a given text, while making the summaries mutually enhance each other. Experimental results show that the proposed model substantially outperforms both abstractive and extractive models, yet frequently generating new words not contained in input texts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "108148310",
                    "name": "Dongmin Hyun"
                },
                {
                    "authorId": "2108045320",
                    "name": "Xiting Wang"
                },
                {
                    "authorId": "2109120259",
                    "name": "Chanyoung Park"
                },
                {
                    "authorId": "2110971997",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "510b5c8a5e2e813c1a485a27edfb2ea24eeb3af0",
            "title": "Beyond Learning from Next Item: Sequential Recommendation via Personalized Interest Sustainability",
            "abstract": "Sequential recommender systems have shown effective suggestions by capturing users' interest drift. There have been two groups of existing sequential models: user- and item-centric models. The user-centric models capture personalized interest drift based on each user's sequential consumption history, but do not explicitly consider whether users' interest in items sustains beyond the training time, i.e., interest sustainability. On the other hand, the item-centric models consider whether users' general interest sustains after the training time, but it is not personalized. In this work, we propose a recommender system taking advantages of the models in both categories. Our proposed model captures personalized interest sustainability, indicating whether each user's interest in items will sustain beyond the training time or not. We first formulate a task that requires to predict which items each user will consume in the recent period of the training time based on users' consumption history. We then propose simple yet effective schemes to augment users' sparse consumption history. Extensive experiments show that the proposed model outperforms 10 baseline models on 11 real-world datasets. The codes are available at: https://github.com/dmhyun/PERIS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "108148310",
                    "name": "Dongmin Hyun"
                },
                {
                    "authorId": "2109120259",
                    "name": "Chanyoung Park"
                },
                {
                    "authorId": "1929729355",
                    "name": "Junsu Cho"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "91ce8ce7e7b91121c3393b218bd679c417aff819",
            "title": "Consensus Learning from Heterogeneous Objectives for One-Class Collaborative Filtering",
            "abstract": "Over the past decades, for One-Class Collaborative Filtering (OCCF), many learning objectives have been researched based on a variety of underlying probabilistic models. From our analysis, we observe that models trained with different OCCF objectives capture distinct aspects of user-item relationships, which in turn produces complementary recommendations. This paper proposes a novel OCCF framework, named as ConCF, that exploits the complementarity from heterogeneous objectives throughout the training process, generating a more generalizable model. ConCF constructs a multi-branch variant of a given target model by adding auxiliary heads, each of which is trained with heterogeneous objectives. Then, it generates consensus by consolidating the various views from the heads, and guides the heads based on the consensus. The heads are collaboratively evolved based on their complementarity throughout the training, which again results in generating more accurate consensus iteratively. After training, we convert the multi-branch architecture back to the original target model by removing the auxiliary heads, thus there is no extra inference cost for the deployment. Our extensive experiments on real-world datasets demonstrate that ConCF significantly improves the generalization of the model by exploiting the complementarity from heterogeneous objectives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "1643930327",
                    "name": "Wonbin Kweon"
                },
                {
                    "authorId": "98573135",
                    "name": "Junyoung Hwang"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "9a73d00559c301a1dbb24bfbb4f5f4156ae23496",
            "title": "TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters",
            "abstract": "Topic taxonomies, which represent the latent topic (or category) structure of document collections, provide valuable knowledge of contents in many applications such as web search and information filtering. Recently, several unsupervised methods have been developed to automatically construct the topic taxonomy from a text corpus, but it is challenging to generate the desired taxonomy without any prior knowledge. In this paper, we study how to leverage the partial (or incomplete) information about the topic structure as guidance to find out the complete topic taxonomy. We propose a novel framework for topic taxonomy completion, named TaxoCom, which recursively expands the topic taxonomy by discovering novel sub-topic clusters of terms and documents. To effectively identify novel topics within a hierarchical topic structure, TaxoCom devises its embedding and clustering techniques to be closely-linked with each other: (i) locally discriminative embedding optimizes the text embedding space to be discriminative among known (i.e., given) sub-topics, and (ii) novelty adaptive clustering assigns terms into either one of the known sub-topics or novel sub-topics. Our comprehensive experiments on two real-world datasets demonstrate that TaxoCom not only generates the high-quality topic taxonomy in terms of term coherency and topic coverage but also outperforms all other baselines for a downstream task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "3396235",
                    "name": "Susik Yoon"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "cb2249f14414c31bd8617dfc76c20c8874ba0284",
            "title": "Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation",
            "abstract": "Topic taxonomies display hierarchical topic structures of a text corpus and provide topical knowledge to enhance various NLP applications. To dynamically incorporate new topic information, several recent studies have tried to expand (or complete) a topic taxonomy by inserting emerging topics identified in a set of new documents. However, existing methods focus only on frequent terms in documents and the local topic-subtopic relations in a taxonomy, which leads to limited topic term coverage and fails to model the global topic hierarchy. In this work, we propose a novel framework for topic taxonomy expansion, named TopicExpan, which directly generates topic-related terms belonging to new topics. Specifically, TopicExpan leverages the hierarchical relation structure surrounding a new topic and the textual content of an input document for topic term generation. This approach encourages newly-inserted topics to further cover important but less frequent terms as well as to keep their relation consistency within the taxonomy. Experimental results on two real-world text corpora show that TopicExpan significantly outperforms other baseline methods in terms of the quality of output taxonomies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "72490748",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "3396235",
                    "name": "Susik Yoon"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "e5872aa1bb24ee3497d233f9f242d1f1fd888431",
            "title": "Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning",
            "abstract": "Recently, finetuning a pretrained language model to capture the similarity between sentence embeddings has shown the state-of-the-art performance on the semantic textual similarity (STS) task. However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output. In this work, we explicitly describe the sentence distance as the weighted sum of contextualized token distances on the basis of a transportation problem, and then present the optimal transport-based distance measure, named RCMD; it identifies and leverages semantically-aligned token pairs. In the end, we propose CLRCMD, a contrastive learning framework that optimizes RCMD of sentence pairs, which enhances the quality of sentence similarity and their interpretation. Extensive experiments demonstrate that our learning framework outperforms other baselines on both STS and interpretable-STS benchmarks, indicating that it computes effective sentence similarity and also provides interpretation consistent with human judgement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72490748",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "1523619467",
                    "name": "Seongbo Jang"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "fa28e75fc93267021ecd7ee7f8a36b60261243b0",
            "title": "A Light Bug Triage Framework for Applying Large Pre-trained Language Model",
            "abstract": "Assigning appropriate developers to the bugs is one of the main challenges in bug triage. Demands for automatic bug triage are increasing in the industry, as manual bug triage is labor-intensive and time-consuming in large projects. The key to the bug triage task is extracting semantic information from a bug report. In recent years, large Pre-trained Language Models (PLMs) including BERT [4] have achieved dramatic progress in the natural language processing (NLP) domain. However, applying large PLMs to the bug triage task for extracting semantic information has several challenges. In this paper, we address the challenges and propose a novel framework for bug triage named LBT-P, standing for Light Bug Triage framework with a Pre-trained language model. It compresses a large PLM into small and fast models using knowledge distillation techniques and also prevents catastrophic forgetting of PLM by introducing knowledge preservation fine-tuning. We also develop a new loss function exploiting representations of earlier layers as well as deeper layers in order to handle the overthinking problem. We demonstrate our proposed framework on the real-world private dataset and three public real-world datasets [11]: Google Chromium, Mozilla Core, and Mozilla Firefox. The result of the experiments shows the superiority of LBT-P.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5352673",
                    "name": "Jaehyung Lee"
                },
                {
                    "authorId": "2199390986",
                    "name": "Kisun Han"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        }
    ]
}