{
    "authorId": "2281902096",
    "papers": [
        {
            "paperId": "5cdba098f7b91106333008244fd8286d83af229b",
            "title": "Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models",
            "abstract": "Knowledge graph completion (KGC) is a widely used method to tackle incompleteness in knowledge graphs (KGs) by making predictions for missing links. Description-based KGC leverages pre-trained language models to learn entity and relation representations with their names or descriptions, which shows promising results. However, the performance of description-based KGC is still limited by the quality of text and the incomplete structure, as it lacks sufficient entity descriptions and relies solely on relation names, leading to sub-optimal results. To address this issue, we propose MPIKGC, a general framework to compensate for the deficiency of contextualized knowledge and improve KGC by querying large language models (LLMs) from various perspectives, which involves leveraging the reasoning, explanation, and summarization capabilities of LLMs to expand entity descriptions, understand relations, and extract structures, respectively. We conducted extensive evaluation of the effectiveness and improvement of our framework based on four description-based KGC models, for both link prediction and triplet classification tasks. All codes and generated data will be publicly available after review.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262514619",
                    "name": "Derong Xu"
                },
                {
                    "authorId": "2030976630",
                    "name": "Ziheng Zhang"
                },
                {
                    "authorId": "2258562926",
                    "name": "Zhenxi Lin"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2288043255",
                    "name": "Zhihong Zhu"
                },
                {
                    "authorId": "2277237058",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2265580543",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "7c8609b93871c49e2e0cef2a0e11f9ec9b1ce921",
            "title": "Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item",
            "abstract": "Sequential recommendation systems (SRS) serve the purpose of predicting users' subsequent preferences based on their past interactions and have been applied across various domains such as e-commerce and social networking platforms. However, practical SRS encounters challenges due to the fact that most users engage with only a limited number of items, while the majority of items are seldom consumed. These challenges, termed as the long-tail user and long-tail item dilemmas, often create obstacles for traditional SRS methods. Mitigating these challenges is crucial as they can significantly impact user satisfaction and business profitability. While some research endeavors have alleviated these issues, they still grapple with issues such as seesaw or noise stemming from the scarcity of interactions. The emergence of large language models (LLMs) presents a promising avenue to address these challenges from a semantic standpoint. In this study, we introduce the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR), which leverages semantic embeddings from LLMs to enhance SRS performance without increasing computational overhead. To combat the long-tail item challenge, we propose a dual-view modeling approach that fuses semantic information from LLMs with collaborative signals from traditional SRS. To address the long-tail user challenge, we introduce a retrieval augmented self-distillation technique to refine user preference representations by incorporating richer interaction data from similar users. Through comprehensive experiments conducted on three authentic datasets using three widely used SRS models, our proposed enhancement framework demonstrates superior performance compared to existing methodologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2269463602",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2244621655",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "b0633ccf235e467c35b963ad012f6b8c54aba19f",
            "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
            "abstract": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262514619",
                    "name": "Derong Xu"
                },
                {
                    "authorId": "2030976630",
                    "name": "Ziheng Zhang"
                },
                {
                    "authorId": "2288043255",
                    "name": "Zhihong Zhu"
                },
                {
                    "authorId": "2258562926",
                    "name": "Zhenxi Lin"
                },
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2258675923",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2277237058",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2265580543",
                    "name": "Enhong Chen"
                }
            ]
        }
    ]
}