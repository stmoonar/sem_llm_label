{
    "authorId": "1845917958",
    "papers": [
        {
            "paperId": "f1d22f767a27ff8ca3886c556788080032fa79cf",
            "title": "Collecting Consistently High Quality Object Tracks with Minimal Human Involvement by Using Self-Supervised Learning to Detect Tracker Errors",
            "abstract": "We propose a hybrid framework for consistently producing high-quality object tracks by combining an automated object tracker with little human input. The key idea is to tailor a module for each dataset to intelligently decide when an object tracker is failing and so humans should be brought in to re-localize an object for continued tracking. Our approach leverages self-supervised learning on unlabeled videos to learn a tailored representation for a target object that is then used to actively monitor its tracked region and decide when the tracker fails. Since labeled data is not needed, our approach can be applied to novel object categories. Experiments on three datasets demonstrate our method outperforms existing approaches, especially for small, fast moving, or occluded objects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1845917958",
                    "name": "Samreen Anjum"
                },
                {
                    "authorId": "2300339200",
                    "name": "Suyog Jain"
                },
                {
                    "authorId": "2028946",
                    "name": "D. Gurari"
                }
            ]
        },
        {
            "paperId": "fea843d015b68a1e4743799274fc4e48816e328e",
            "title": "VQA Therapy: Exploring Answer Differences by Visually Grounding Answers",
            "abstract": "Visual question answering is a task of predicting the answer to a question about an image. Given that different people can provide different answers to a visual question, we aim to better understand why with answer groundings. We introduce the first dataset that visually grounds each unique answer to each visual question, which we call VQA-AnswerTherapy. We then propose two novel problems of predicting whether a visual question has a single answer grounding and localizing all answer groundings. We benchmark modern algorithms for these novel problems to show where they succeed and struggle. The dataset and evaluation server can be found publicly at https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110239221",
                    "name": "Chongyan Chen"
                },
                {
                    "authorId": "1845917958",
                    "name": "Samreen Anjum"
                },
                {
                    "authorId": "2028946",
                    "name": "D. Gurari"
                }
            ]
        },
        {
            "paperId": "455869f88df82b07ef7d5ab0dab5c28c6620daa1",
            "title": "Grounding Answers for Visual Questions Asked by Visually Impaired People",
            "abstract": "Visual question answering is the task of answering questions about images. We introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually grounds answers to visual questions asked by people with visual impairments. We analyze our dataset and compare it with five VQA-Grounding datasets to demonstrate what makes it similar and different. We then evaluate the SOTA VQA and VQA-Grounding models and demonstrate that current SOTA algorithms often fail to identify the correct visual evidence where the answer is located. These models regularly struggle when the visual evidence occupies a small fraction of the image, for images that are higher quality, as well as for visual questions that require skills in text recognition. The dataset, evaluation server, and leader-board all can be found at the following link: https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110239221",
                    "name": "Chongyan Chen"
                },
                {
                    "authorId": "1845917958",
                    "name": "Samreen Anjum"
                },
                {
                    "authorId": "2028946",
                    "name": "D. Gurari"
                }
            ]
        },
        {
            "paperId": "91f6e7d8ef42526d9be0e365a5f781393a4302f0",
            "title": "Exploring the Use of Deep Learning with Crowdsourcing to Annotate Images",
            "abstract": "We investigate what, if any, benefits arise from employing hybrid algorithm-crowdsourcing approaches over conventional approaches of relying exclusively on algorithms or crowds to annotate images.\u00a0 We introduce a framework that enables users to investigate different hybrid workflows for three popular image analysis tasks: image classification, object detection, and image captioning.\u00a0 \u00a0Three hybrid approaches are included that are based on having workers: (i) verify predicted labels, (ii) correct predicted labels, and (iii) annotate images for which algorithms have low confidence in their predictions.\u00a0 Deep learning algorithms are employed in these workflows since they offer high performance for image annotation tasks.\u00a0 Each workflow is evaluated with respect to annotation quality and worker time to completion on images coming from three diverse datasets (i.e., VOC, MSCOCO, VizWiz). Inspired by our findings, we offer recommendations regarding when and how to employ deep learning with crowdsourcing to achieve desired quality and efficiency for image annotation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1845917958",
                    "name": "Samreen Anjum"
                },
                {
                    "authorId": "2113855761",
                    "name": "Ambika Verma"
                },
                {
                    "authorId": "40629713",
                    "name": "B. Dang"
                },
                {
                    "authorId": "2028946",
                    "name": "D. Gurari"
                }
            ]
        },
        {
            "paperId": "41dbefcbce4bc0f358c633948c7b410c2dd23120",
            "title": "CTMC: Cell Tracking with Mitosis Detection Dataset Challenge",
            "abstract": "While significant developments have been made in cell tracking algorithms, current datasets are still limited in size and diversity, especially for data-hungry generalized deep learning models. We introduce a new larger and more diverse cell tracking dataset in terms of number of sequences, length of sequences, and cell lines, accompanied with a public evaluation server and leaderboard to accelerate progress on this new challenging dataset. Our benchmarking of four top performing tracking algorithms highlights new challenges and opportunities to improve the state-of-the-art in cell tracking.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1845917958",
                    "name": "Samreen Anjum"
                },
                {
                    "authorId": "2028946",
                    "name": "D. Gurari"
                }
            ]
        },
        {
            "paperId": "c87206ce4bcdcb53d1283cd92d3f384051e4c781",
            "title": "CrowdMOT",
            "abstract": "Crowdsourcing is a valuable approach for tracking objects in videos in a more scalable manner than possible with domain experts. However, existing frameworks do not produce high quality results with non-expert crowdworkers, especially for scenarios where objects split. To address this shortcoming, we introduce a crowdsourcing platform called CrowdMOT, and investigate two micro-task design decisions: (1) whether to decompose the task so that each worker is in charge of annotating all objects in a sub-segment of the video versus annotating a single object across the entire video, and (2) whether to show annotations from previous workers to the next individuals working on the task. We conduct experiments on a diversity of videos which show both familiar objects (aka - people) and unfamiliar objects (aka - cells). Our results highlight strategies for efficiently collecting higher quality annotations than observed when using strategies employed by today's state-of-art crowdsourcing system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1845917958",
                    "name": "Samreen Anjum"
                },
                {
                    "authorId": "2145322568",
                    "name": "Chi Lin"
                },
                {
                    "authorId": "2028946",
                    "name": "D. Gurari"
                }
            ]
        },
        {
            "paperId": "4dfd466f46e6e63368444c1adc95e258a9ff67a3",
            "title": "Dataset bias: A case study for visual question answering",
            "abstract": "We examine the issue of bias in datasets designed to train visual question answering (VQA) algorithms. These datasets include a collection of natural language questions about images (aka \u2010 visual questions). We consider three popular datasets that are captured by people with sight, people who are blind, and generated by computers. We first demonstrate that machine learning algorithms can be trained to recognize each dataset's bias, and so determine the source of a novel visual question. We then discuss potential risks and benefits of biased VQA datasets and corresponding machine learning algorithms that can identify the source of a visual question; e.g., whether it comes from a person with sight, a person who is blind, or bot (aka \u2010 computer). Our ultimate aim is to inspire the development of more inclusive VQA systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47295297",
                    "name": "Anubrata Das"
                },
                {
                    "authorId": "1845917958",
                    "name": "Samreen Anjum"
                },
                {
                    "authorId": "2028946",
                    "name": "D. Gurari"
                }
            ]
        },
        {
            "paperId": "0814210a135e052e036bdf40fd0c63a54541217a",
            "title": "MAQSA: a system for social analytics on news",
            "abstract": "We present MAQSA, a system for social analytics on news. MAQSA provides an interactive topic-centric dashboard that summarizes news articles and social activity (e.g., comments and tweets) around them. MAQSA helps editors and publishers in newsrooms understand user engagement and audience sentiment evolution on various topics of interest. It also helps news consumers explore public reaction on articles relevant to a topic and refine their exploration via related entities, topics, articles and tweets. Given a topic, e.g., \"Gulf Oil Spill,\" or \"The Arab Spring\", MAQSA combines three key dimensions: time, geographic location, and topic to generate a detailed activity dashboard around relevant articles. The dashboard contains an annotated comment timeline and a social graph of comments. It utilizes commenters' locations to build maps of comment sentiment and topics by region of the world. Finally, to facilitate exploration, MAQSA provides listings of related entities, articles, and tweets. It algorithmically processes large collections of articles and tweets, and enables the dynamic specification of topics and dates for exploration. In this demo, participants will be invited to explore the social dynamics around articles on oil spills, the Libyan revolution, and the Arab Spring. In addition, participants will be able to define and explore their own topics dynamically.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1845917958",
                    "name": "Samreen Anjum"
                },
                {
                    "authorId": "2009092",
                    "name": "Amira Ghenai"
                },
                {
                    "authorId": "40588385",
                    "name": "Aysha Siddique"
                },
                {
                    "authorId": "3009678",
                    "name": "Sofiane Abbar"
                },
                {
                    "authorId": "144478906",
                    "name": "S. Madden"
                },
                {
                    "authorId": "145632757",
                    "name": "Adam Marcus"
                },
                {
                    "authorId": "102697193",
                    "name": "Mohammed El-Haddad"
                }
            ]
        }
    ]
}