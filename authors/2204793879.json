{
    "authorId": "2204793879",
    "papers": [
        {
            "paperId": "28f03963ed4d081b671fb056b5a8aee1c5bbf31f",
            "title": "Continual Multimodal Knowledge Graph Construction",
            "abstract": "Current Multimodal Knowledge Graph Construction (MKGC) models struggle with the real-world dynamism of continuously emerging entities and relations, often succumbing to catastrophic forgetting\u2014loss of previously acquired knowledge. This study introduces benchmarks aimed at fostering the development of the continual MKGC domain. We further introduce the MSPT framework, designed to surmount the shortcomings of existing MKGC approaches during multimedia data processing. MSPT harmonizes the retention of learned knowledge (stability) and the integration of new data (plasticity), outperforming current continual learning and multimodal methods. Our results confirm MSPT's superior performance in evolving knowledge environments, showcasing its capacity to navigate the balance between stability and plasticity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143735911",
                    "name": "Xiang Chen"
                },
                {
                    "authorId": "2253784578",
                    "name": "Jintian Zhang"
                },
                {
                    "authorId": "2141660877",
                    "name": "Xiaohan Wang"
                },
                {
                    "authorId": "3357184",
                    "name": "Tongtong Wu"
                },
                {
                    "authorId": "152931849",
                    "name": "Shumin Deng"
                },
                {
                    "authorId": "2204793879",
                    "name": "Yongheng Wang"
                },
                {
                    "authorId": "2059080424",
                    "name": "Luo Si"
                },
                {
                    "authorId": "2144200945",
                    "name": "Huajun Chen"
                },
                {
                    "authorId": "2608639",
                    "name": "Ningyu Zhang"
                }
            ]
        },
        {
            "paperId": "45d9f9494b13dfcf353bec69f1afa01b1d6c753d",
            "title": "Improving Knowledge Distillation via Regularizing Feature Norm and Direction",
            "abstract": "Knowledge distillation (KD) exploits a large well-trained model (i.e., teacher) to train a small student model on the same dataset for the same task. Treating teacher features as knowledge, prevailing methods of knowledge distillation train student by aligning its features with the teacher's, e.g., by minimizing the KL-divergence between their logits or L2 distance between their intermediate features. While it is natural to believe that better alignment of student features to the teacher better distills teacher knowledge, simply forcing this alignment does not directly contribute to the student's performance, e.g., classification accuracy. In this work, we propose to align student features with class-mean of teacher features, where class-mean naturally serves as a strong classifier. To this end, we explore baseline techniques such as adopting the cosine distance based loss to encourage the similarity between student features and their corresponding class-means of the teacher. Moreover, we train the student to produce large-norm features, inspired by other lines of work (e.g., model pruning and domain adaptation), which find the large-norm features to be more significant. Finally, we propose a rather simple loss term (dubbed ND loss) to simultaneously (1) encourage student to produce large-\\emph{norm} features, and (2) align the \\emph{direction} of student features and teacher class-means. Experiments on standard benchmarks demonstrate that our explored techniques help existing KD methods achieve better performance, i.e., higher classification accuracy on ImageNet and CIFAR100 datasets, and higher detection precision on COCO dataset. Importantly, our proposed ND loss helps the most, leading to the state-of-the-art performance on these benchmarks. The source code is available at \\url{https://github.com/WangYZ1608/Knowledge-Distillation-via-ND}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162616659",
                    "name": "Yuzhu Wang"
                },
                {
                    "authorId": "26953623",
                    "name": "Lechao Cheng"
                },
                {
                    "authorId": "2448387",
                    "name": "Manni Duan"
                },
                {
                    "authorId": "2204793879",
                    "name": "Yongheng Wang"
                },
                {
                    "authorId": "7357719",
                    "name": "Zunlei Feng"
                },
                {
                    "authorId": "34362536",
                    "name": "Shu Kong"
                }
            ]
        },
        {
            "paperId": "5f0e22bbdb63e98c19d9af54d3d795ea1d256020",
            "title": "SimCGNN: Simple Contrastive Graph Neural Network for Session-based Recommendation",
            "abstract": "Session-based recommendation (SBR) problem, which focuses on next-item prediction for anonymous users, has received increasingly more attention from researchers. Existing graph-based SBR methods all lack the ability to differentiate between sessions with the same last item, and suffer from severe popularity bias. Inspired by nowadays emerging contrastive learning methods, this paper presents a Simple Contrastive Graph Neural Network for Session-based Recommendation (SimCGNN). In SimCGNN, we first obtain normalized session embeddings on constructed session graphs. We next construct positive and negative samples of the sessions by two forward propagation and a novel negative sample selection strategy, and then calculate the constructive loss. Finally, session embeddings are used to give prediction. Extensive experiments conducted on two real-word datasets show our SimCGNN achieves a significant improvement over state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Yuan Cao"
                },
                {
                    "authorId": "2121383794",
                    "name": "Xudong Zhang"
                },
                {
                    "authorId": "2153307623",
                    "name": "Fan Zhang"
                },
                {
                    "authorId": "51123910",
                    "name": "Feifei Kou"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                },
                {
                    "authorId": "2096526",
                    "name": "Xiongnan Jin"
                },
                {
                    "authorId": "2204793879",
                    "name": "Yongheng Wang"
                },
                {
                    "authorId": "1698521",
                    "name": "Jinpeng Chen"
                }
            ]
        }
    ]
}