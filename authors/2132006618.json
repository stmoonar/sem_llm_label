{
    "authorId": "2132006618",
    "papers": [
        {
            "paperId": "74961683f653bdb685578c3341ec64905435b352",
            "title": "D3CODE: Disentangling Disagreements in Data across Cultures on Offensiveness Detection and Evaluation",
            "abstract": "While human annotations play a crucial role in language technologies, annotator subjectivity has long been overlooked in data collection. Recent studies that have critically examined this issue are often situated in the Western context, and solely document differences across age, gender, or racial groups. As a result, NLP research on subjectivity have overlooked the fact that individuals within demographic groups may hold diverse values, which can influence their perceptions beyond their group norms. To effectively incorporate these considerations into NLP pipelines, we need datasets with extensive parallel annotations from various social and cultural groups. In this paper we introduce the \\dataset dataset: a large-scale cross-cultural dataset of parallel annotations for offensive language in over 4.5K sentences annotated by a pool of over 4k annotators, balanced across gender and age, from across 21 countries, representing eight geo-cultural regions. The dataset contains annotators' moral values captured along six moral foundations: care, equality, proportionality, authority, loyalty, and purity. Our analyses reveal substantial regional variations in annotators' perceptions that are shaped by individual moral values, offering crucial insights for building pluralistic, culturally sensitive NLP models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "2146515892",
                    "name": "M. D'iaz"
                },
                {
                    "authorId": "2273652866",
                    "name": "Dylan K. Baker"
                },
                {
                    "authorId": "2265650491",
                    "name": "Vinodkumar Prabhakaran"
                }
            ]
        },
        {
            "paperId": "97db70178d57ddafce90b1bff69a2684b42fd40f",
            "title": "GeniL: A Multilingual Dataset on Generalizing Language",
            "abstract": "Generative language models are transforming our digital ecosystem, but they often inherit societal biases, for instance stereotypes associating certain attributes with specific identity groups. While whether and how these biases are mitigated may depend on the specific use cases, being able to effectively detect instances of stereotype perpetuation is a crucial first step. Current methods to assess presence of stereotypes in generated language rely on simple template or co-occurrence based measures, without accounting for the variety of sentential contexts they manifest in. We argue that understanding the sentential context is crucial for detecting instances of generalization. We distinguish two types of generalizations: (1) language that merely mentions the presence of a generalization (\"people think the French are very rude\"), and (2) language that reinforces such a generalization (\"as French they must be rude\"), from non-generalizing context (\"My French friends think I am rude\"). For meaningful stereotype evaluations, we need to reliably distinguish such instances of generalizations. We introduce the new task of detecting generalization in language, and build GeniL, a multilingual dataset of over 50K sentences from 9 languages (English, Arabic, Bengali, Spanish, French, Hindi, Indonesian, Malay, and Portuguese) annotated for instances of generalizations. We demonstrate that the likelihood of a co-occurrence being an instance of generalization is usually low, and varies across different languages, identity groups, and attributes. We build classifiers to detect generalization in language with an overall PR-AUC of 58.7, with varying degrees of performance across languages. Our research provides data and tools to enable a nuanced understanding of stereotype perpetuation, a crucial step towards more inclusive and responsible language technologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "2183758",
                    "name": "S. Gubbi"
                },
                {
                    "authorId": "50991767",
                    "name": "Sunipa Dev"
                },
                {
                    "authorId": "2160404",
                    "name": "Shachi Dave"
                },
                {
                    "authorId": "2265650491",
                    "name": "Vinodkumar Prabhakaran"
                }
            ]
        },
        {
            "paperId": "14673ee549a757fbd62c16dedb0d1dce59bdb194",
            "title": "Distinguishing Address vs. Reference Mentions of Personal Names in Text",
            "abstract": "Detecting named entities in text has long been a core NLP task. However, not much work has gone into distinguishing whether an entity mention is addressing the entity vs. referring to the entity; e.g., John, would you turn the light off? vs. John turned the light off . While this distinction is marked by a vocative case marker in some languages, many modern Indo-European languages such as English do not use such explicit vocative markers, and the distinction is left to be interpreted in context. In this paper, we present a new annotated dataset that captures the address vs. reference distinction in English, 1 an automatic tagger that performs at 85% accuracy in making this distinction, and demonstrate how this distinction is important in NLP and computational social science applications in English language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "39829408",
                    "name": "M. Ferguson"
                },
                {
                    "authorId": "7180227",
                    "name": "Stav Atir"
                }
            ]
        },
        {
            "paperId": "af3388b61dc642b1d0be0c7147948fe800abb4c9",
            "title": "GRASP: A Disagreement Analysis Framework to Assess Group Associations in Perspectives",
            "abstract": "Human annotation plays a core role in machine learning \u2014 annotations for supervised models, safety guardrails for generative models, and human feedback for reinforcement learning, to cite a few avenues. However, the fact that many of these human annotations are inherently subjective is often overlooked. Recent work has demonstrated that ignoring rater subjectivity (typically resulting in rater disagreement) is problematic within specific tasks and for specific subgroups. Generalizable methods to harness rater disagreement and thus understand the socio-cultural leanings of subjective tasks remain elusive. In this paper, we propose GRASP, a comprehensive disagreement analysis framework to measure group association in perspectives among different rater subgroups, and demonstrate its utility in assessing the extent of systematic disagreements in two datasets: (1) safety annotations of human-chatbot conversations, and (2) offensiveness annotations of social media posts, both annotated by diverse rater pools across different socio-demographic axes. Our framework (based on disagreement metrics) reveals specific rater groups that have significantly different perspectives than others on certain tasks, and helps identify demographic axes that are crucial to consider in specific task contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265650491",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "2257291986",
                    "name": "Christopher Homan"
                },
                {
                    "authorId": "2257256357",
                    "name": "Lora Aroyo"
                },
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "2265756309",
                    "name": "Alicia Parrish"
                },
                {
                    "authorId": "2266021190",
                    "name": "Alex S. Taylor"
                },
                {
                    "authorId": "2146515892",
                    "name": "M. D'iaz"
                },
                {
                    "authorId": "2202541709",
                    "name": "Ding Wang"
                },
                {
                    "authorId": "2118924379",
                    "name": "Greg Serapio-Garc\u00eda"
                }
            ]
        },
        {
            "paperId": "e4c309c6e533d3ecbf1affac83936188e4a9cbf1",
            "title": "Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates",
            "abstract": "Recent years have seen substantial investments in AI-based tools designed to detect offensive language at scale, aiming to moderate social media platforms, and ensure safety of conversational AI technologies such as ChatGPT and Bard. These efforts largely treat this task as a technical endeavor, relying on data annotated for offensiveness by a global crowd workforce, without considering crowd workers\u2019 socio-cultural backgrounds or the values their perceptions reflect. Existing research that examines systematic variations in annotators\u2019 judgments often reduces these differences to socio-demographic categories along racial, or gender dimensions, overlooking the diversity of perspectives within such groups. On the other hand, social psychology literature highlights the crucial role that both cultural and psychological factors play in human perceptions and judgments. Through a large-scale cross-cultural study of 4309 participants from 21 countries across eight cultural regions, we demonstrate substantial cross-cultural and individual moral value-based differences in interpretations of offensiveness. Our study reveals specific regions that are significantly more sensitive to offensive language. Furthermore, using the Moral Foundations Theory, we study the underlying moral values that contribute to these cross-cultural differences. Notably, we find that participants\u2019 moral values play a far more important role in shaping their perceptions of offensiveness than geo-cultural distinctions. Our investigation, using a non-monolithic framework to understand cross-cultural moral concerns, reveals crucial insights that can be extrapolated to building AI models for the pluralistic world. Our results call for more extensive consideration of diverse human moral values when deploying AI models across diverse geo-cultural contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "2146515892",
                    "name": "M. D'iaz"
                },
                {
                    "authorId": "2273652866",
                    "name": "Dylan K. Baker"
                },
                {
                    "authorId": "2265650491",
                    "name": "Vinodkumar Prabhakaran"
                }
            ]
        },
        {
            "paperId": "f4f154892800008894ebbf57add31fcaac4f27ca",
            "title": "SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models",
            "abstract": "Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36701727",
                    "name": "Akshita Jha"
                },
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                },
                {
                    "authorId": "2160404",
                    "name": "Shachi Dave"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "50991767",
                    "name": "Sunipa Dev"
                }
            ]
        },
        {
            "paperId": "567067ad6c2f7b53eaf967a27effb109d235d87a",
            "title": "The Moral Foundations Reddit Corpus",
            "abstract": "Moral framing and sentiment can affect a variety of online and offline behaviors, including donation, pro-environmental action, political engagement, and even participation in violent protests. Various computational methods in Natural Language Processing (NLP) have been used to detect moral sentiment from textual data, but in order to achieve better performances in such subjective tasks, large sets of hand-annotated training data are needed. Previous corpora annotated for moral sentiment have proven valuable, and have generated new insights both within NLP and across the social sciences, but have been limited to Twitter. To facilitate improving our understanding of the role of moral rhetoric, we present the Moral Foundations Reddit Corpus, a collection of 16,123 Reddit comments that have been curated from 12 distinct subreddits, hand-annotated by at least three trained annotators for 8 categories of moral sentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty, Thin Morality, Implicit/Explicit Morality) based on the updated Moral Foundations Theory (MFT) framework. We use a range of methodologies to provide baseline moral-sentiment classification results for this new corpus, e.g., cross-domain classification and knowledge transfer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2133743149",
                    "name": "Jackson Trager"
                },
                {
                    "authorId": "2181122081",
                    "name": "Alireza S. Ziabari"
                },
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "2181122077",
                    "name": "Preni Golazazian"
                },
                {
                    "authorId": "1445095031",
                    "name": "Farzan Karimi-Malekabadi"
                },
                {
                    "authorId": "2070422394",
                    "name": "Ali Omrani"
                },
                {
                    "authorId": "49969909",
                    "name": "Zhihe Li"
                },
                {
                    "authorId": "2057247331",
                    "name": "Brendan Kennedy"
                },
                {
                    "authorId": "32990012",
                    "name": "N. K. Reimer"
                },
                {
                    "authorId": "1964745",
                    "name": "M. Reyes"
                },
                {
                    "authorId": "2181124079",
                    "name": "Kelsey Cheng"
                },
                {
                    "authorId": "2181123914",
                    "name": "Mellow Wei"
                },
                {
                    "authorId": "2181122585",
                    "name": "Christina Merrifield"
                },
                {
                    "authorId": "2181122698",
                    "name": "Arta Khosravi"
                },
                {
                    "authorId": "1571929933",
                    "name": "E. \u00c1lvarez"
                },
                {
                    "authorId": "2111849697",
                    "name": "Morteza Dehghani"
                }
            ]
        },
        {
            "paperId": "766912db878d4ca14680600e982faff44e508a8d",
            "title": "On Releasing Annotator-Level Labels and Information in Datasets",
            "abstract": "A common practice in building NLP datasets, especially using crowd-sourced annotations, involves obtaining multiple annotator judgements on the same data instances, which are then flattened to produce a single \u201cground truth\u201d label or score, through majority voting, averaging, or adjudication. While these approaches may be appropriate in certain annotation tasks, such aggregations overlook the socially constructed nature of human perceptions that annotations for relatively more subjective tasks are meant to capture. In particular, systematic disagreements between annotators owing to their socio-cultural backgrounds and/or lived experiences are often obfuscated through such aggregations. In this paper, we empirically demonstrate that label aggregation may introduce representational biases of individual and group perspectives. Based on this finding, we propose a set of recommendations for increased utility and transparency of datasets for downstream use cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "2146515892",
                    "name": "M. D'iaz"
                }
            ]
        },
        {
            "paperId": "8b245254996160ff3885c15e0bdb6bc2e5dd01bd",
            "title": "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
            "abstract": "Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators\u2019 judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "2146515892",
                    "name": "M. D'iaz"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                }
            ]
        }
    ]
}