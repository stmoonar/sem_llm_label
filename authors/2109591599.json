{
    "authorId": "2109591599",
    "papers": [
        {
            "paperId": "0340c850e033abbf71c7214e403c8fe2be5ef91f",
            "title": "Visual Tuning",
            "abstract": "Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: fine-tuning, prompt tuning, adapter tuning, parameter tuning, and remapping tuning. Meanwhile, it offers some exciting research directions for prospective pre-training and various interactions in visual tuning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48317098",
                    "name": "Bruce X. B. Yu"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2146017850",
                    "name": "Lin Liu"
                },
                {
                    "authorId": "2108620644",
                    "name": "Shijie Wang"
                },
                {
                    "authorId": "2216713990",
                    "name": "Zhiyu Wang"
                },
                {
                    "authorId": "2110810106",
                    "name": "Junfan Lin"
                },
                {
                    "authorId": "3041937",
                    "name": "Lingxi Xie"
                },
                {
                    "authorId": "2198533822",
                    "name": "Haojie Li"
                },
                {
                    "authorId": "33383055",
                    "name": "Zhouchen Lin"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                },
                {
                    "authorId": "2118428500",
                    "name": "Chang Wen Chen"
                }
            ]
        },
        {
            "paperId": "0c7ce5898dab92da540457b754254d72b8592fc2",
            "title": "Parameter-efficient Tuning of Large-scale Multimodal Foundation Model",
            "abstract": "Driven by the progress of large-scale pre-training, parameter-efficient transfer learning has gained immense popularity across different subfields of Artificial Intelligence. The core is to adapt the model to downstream tasks with only a small set of parameters. Recently, researchers have leveraged such proven techniques in multimodal tasks and achieve promising results. However, two critical issues remain unresolved: how to further reduce the complexity with lightweight design and how to boost alignment between modalities under extremely low parameters. In this paper, we propose A graceful prompt framework for cross-modal transfer (Aurora) to overcome these challenges. Considering the redundancy in existing architectures, we first utilize the mode approximation to generate 0.1M trainable parameters to implement the multimodal prompt tuning, which explores the low intrinsic dimension with only 0.04% parameters of the pre-trained model. Then, for better modality alignment, we propose the Informative Context Enhancement and Gated Query Transformation module under extremely few parameters scenes. A thorough evaluation on six cross-modal benchmarks shows that it not only outperforms the state-of-the-art but even outperforms the full fine-tuning approach. Our code is available at: https://github.com/WillDreamer/Aurora.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2150441370",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "29a7644583d7042c4476af126f3fe0a372897abe",
            "title": "LION: Implicit Vision Prompt Tuning",
            "abstract": "Despite recent promising performances across a range of vision tasks, vision Transformers still have an issue of high computational costs.\nRecently, vision prompt learning has provided an economical solution to this problem without fine-tuning the whole large-scale model. \nHowever, the efficiency and effectiveness of existing models are still far from satisfactory due to the parameter cost of extensive prompt blocks and tricky prompt framework designs. \nIn this paper, we propose a light-weight prompt framework named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep implicit models with stable low memory costs for various complex tasks.\nIn particular, we merely insect two equilibrium implicit layers in two ends of the pre-trained backbone with parameters frozen. Moreover, according to the lottery hypothesis, we further prune the parameters to relieve the computation burden in implicit layers. Various experiments have validated that our LION obtains promising performances on a wide range of datasets. Most importantly, LION reduces up to 11.5 % of training parameter numbers while obtaining higher performance than the state-of-the-art VPT, especially under challenging scenes. Furthermore, we find that our proposed LION has an excellent generalization performance, making it an easy way to boost transfer learning in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "33383055",
                    "name": "Zhouchen Lin"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "852ab98a42204ea905ba41b3b1354820be4f201c",
            "title": "When Parameter-efficient Tuning Meets General-purpose Vision-language Models",
            "abstract": "Instruction tuning has shown promising potential for developing general-purpose AI capabilities by using large-scale pre-trained models and boosts growing research to integrate multimodal information for creative applications. However, existing works still face two main limitations: the high training costs and heavy computing resource dependence of full model fine-tuning, and the lack of semantic information in instructions, which hinders multimodal alignment. Addressing these challenges, this paper proposes a novel approach to utilize Parameter-Efficient Tuning for generAl-purpose vision-Language models, namely PETAL. PETAL revolutionizes the training process by requiring only 0.5% of the total parameters, achieved through a unique mode approximation technique, which significantly reduces the training costs and reliance on heavy computing resources. Furthermore, PETAL enhances the semantic depth of instructions in two innovative ways: 1) by introducing adaptive instruction mixture-of-experts(MOEs), and 2) by fortifying the score-based linkage between parameter-efficient tuning and mutual information. Our extensive experiments across five multimodal downstream benchmarks reveal that PETAL not only outperforms current state-of-the-art methods in most scenarios but also surpasses full fine-tuning models in effectiveness. Additionally, our approach demonstrates remarkable advantages in few-shot settings, backed by comprehensive visualization analyses. Our source code is available at: https://github. com/melonking32/PETAL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275115509",
                    "name": "Yihang Zhai"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2265575121",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "b1e6fd86e81dba75626abd163d8c6bc6dd7b859e",
            "title": "DANCE: Learning A Domain Adaptive Framework for Deep Hashing",
            "abstract": "This paper studies unsupervised domain adaptive hashing, which aims to transfer a hashing model from a label-rich source domain to a label-scarce target domain. Current state-of-the-art approaches generally resolve the problem by integrating pseudo-labeling and domain adaptation techniques into deep hashing paradigms. Nevertheless, they usually suffer from serious class imbalance in pseudo-labels and suboptimal domain alignment caused by the neglection of the intrinsic structures of two domains. To address this issue, we propose a novel method named unbiaseD duAl hashiNg Contrastive lEarning (DANCE) for domain adaptive image retrieval. The core of our DANCE is to perform contrastive learning on hash codes from both instance level and prototype level. To begin, DANCE utilizes label information to guide instance-level hashing contrastive learning in the source domain. To generate unbiased and reliable pseudo-labels for semantic learning in the target domain, we uniformly select samples around each label embedding in the Hamming space. A momentum-update scheme is also utilized to smooth the optimization process. Additionally, we measure the semantic prototype representations in both source and target domains and incorporate them into a domain-aware prototype-level contrastive learning paradigm, which enhances domain alignment in the Hamming space while maximizing the model capacity. Experimental results on a number of well-known domain adaptive retrieval benchmarks validate the effectiveness of our proposed DANCE compared to a variety of competing baselines in different settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2215550446",
                    "name": "Xiang Wei"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "48240541",
                    "name": "C. Chen"
                },
                {
                    "authorId": "143863244",
                    "name": "Xiansheng Hua"
                },
                {
                    "authorId": "2115826917",
                    "name": "Xiao Luo"
                }
            ]
        },
        {
            "paperId": "ba15f6b2af45eeb7508ea5ca6d5c244154b890e6",
            "title": "Toward Effective Domain Adaptive Retrieval",
            "abstract": "This paper studies the problem of unsupervised domain adaptive hashing, which is less-explored but emerging for efficient image retrieval, particularly for cross-domain retrieval. This problem is typically tackled by learning hashing networks with pseudo-labeling and domain alignment techniques. Nevertheless, these approaches usually suffer from overconfident and biased pseudo-labels and inefficient domain alignment without sufficiently exploring semantics, thus failing to achieve satisfactory retrieval performance. To tackle this issue, we present PEACE, a principled framework which holistically explores semantic information in both source and target data and extensively incorporates it for effective domain alignment. For comprehensive semantic learning, PEACE leverages label embeddings to guide the optimization of hash codes for source data. More importantly, to mitigate the effects of noisy pseudo-labels, we propose a novel method to holistically measure the uncertainty of pseudo-labels for unlabeled target data and progressively minimize them through alternative optimization under the guidance of the domain discrepancy. Additionally, PEACE effectively removes domain discrepancy in the Hamming space from two views. In particular, it not only introduces composite adversarial learning to implicitly explore semantic information embedded in hash codes, but also aligns cluster semantic centroids across domains to explicitly exploit label information. Experimental results on several popular domain adaptive retrieval benchmarks demonstrate the superiority of our proposed PEACE compared with various state-of-the-art methods on both single-domain and cross-domain retrieval tasks. Our source codes are available at https://github.com/WillDreamer/PEACE.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2115826917",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2187310139",
                    "name": "Wei Xiang"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "48240541",
                    "name": "C. Chen"
                },
                {
                    "authorId": "143863244",
                    "name": "Xiansheng Hua"
                }
            ]
        },
        {
            "paperId": "cb3ddc59c3d56d5d6c83649080abd8a18e0fe93a",
            "title": "Prototypical Mixing and Retrieval-based Refinement for Label Noise-resistant Image Retrieval",
            "abstract": "Label noise is pervasive in real-world applications, which influences the optimization of neural network models. This paper investigates a realistic but understudied problem of image retrieval under label noise, which could lead to severe overfitting or memorization of noisy samples during optimization. Moreover, identifying noisy samples correctly is still a challenging problem for retrieval models. In this paper, we propose a novel approach called Prototypical Mixing and Retrieval-based Refinement (TITAN) for label noise-resistant image retrieval, which corrects label noise and mitigates the effects of the memorization simultaneously. Specifically, we first characterize numerous prototypes with Gaussian distributions in the hidden space, which would direct the Mixing procedure in providing synthesized samples. These samples are fed into a similarity learning framework with varying emphasis based on the prototypical structure to learn semantics with reduced overfitting. In addition, we retrieve comparable samples for each prototype from simple to complex, which refine noisy samples in an accurate and class-balanced manner. Comprehensive experiments on five benchmark datasets demonstrate the superiority of our proposed TITAN compared with various competing baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265575121",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2257378991",
                    "name": "Chong Chen"
                },
                {
                    "authorId": "2238119871",
                    "name": "Xian-Sheng Hua"
                },
                {
                    "authorId": "2241568233",
                    "name": "Xiao Luo"
                }
            ]
        },
        {
            "paperId": "d73ccaa9cedff36dc7a930319506d04c2fce6719",
            "title": "PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction",
            "abstract": "In this paper, we investigate the challenge of spatio-temporal video prediction, which involves generating future videos based on historical data streams. Existing approaches typically utilize external information such as semantic maps to enhance video prediction, which often neglect the inherent physical knowledge embedded within videos. Furthermore, their high computational demands could impede their applications for high-resolution videos. To address these constraints, we introduce a novel approach called Physics-assisted Spatio-temporal Network (PastNet) for generating high-quality video predictions. The core of our PastNet lies in incorporating a spectral convolution operator in the Fourier domain, which efficiently introduces inductive biases from the underlying physical laws. Additionally, we employ a memory bank with the estimated intrinsic dimensionality to discretize local features during the processing of complex spatio-temporal signals, thereby reducing computational costs and facilitating efficient high-resolution video prediction. Extensive experiments on various widely-used datasets demonstrate the effectiveness and efficiency of the proposed PastNet compared with state-of-the-art methods, particularly in high-resolution scenarios. Our code is available at https://github.com/easylearningscores/PastNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119798365",
                    "name": "Hao Wu"
                },
                {
                    "authorId": "71787357",
                    "name": "Wei Xion"
                },
                {
                    "authorId": "2217950059",
                    "name": "Fan Xu"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "48240541",
                    "name": "C. Chen"
                },
                {
                    "authorId": "2053903039",
                    "name": "Xiansheng Hua"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                }
            ]
        },
        {
            "paperId": "a8bd72ac4cdd8849b49766b7470b6d8d3cb58f1b",
            "title": "HEART: Towards Effective Hash Codes under Label Noise",
            "abstract": "Hashing, which encodes raw data into compact binary codes, has grown in popularity for large-scale image retrieval due to its storage and computation efficiency. Although deep supervised hashing has lately shown promising performance, they mostly assume that the semantic labels of training data are ideally noise-free, which is often unrealistic in real-world applications. In this paper, considering the practical application, we focus on the problem of learning to hash with label noise and propose a novel method called HEART to address the problem. HEART is a holistic framework which explores latent semantic distributions to select both clean samples and pairs of high confidence for mitigating the impacts of label noise. From a statistical perspective, our HEART characterizes each image by its multiple augmented views that can be considered as examples from its latent distribution and then calculates semantic distances between images using energy distances between their latent distributions. With semantic distances, we can select confident similar pairs to guide hashing contrastive learning for high-quality hash codes. Moreover, to prevent the memorization of noisy examples, we propose a novel strategy to identify clean samples which have small variations of losses on the latent distributions and train the network on clean samples using a pointwise loss. Experimental results on several popular benchmark datasets demonstrate the effectiveness of our HEART compared with a wide range of baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2115826917",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2187310139",
                    "name": "Wei Xiang"
                },
                {
                    "authorId": "48240541",
                    "name": "C. Chen"
                },
                {
                    "authorId": "143863244",
                    "name": "Xiansheng Hua"
                }
            ]
        },
        {
            "paperId": "dd53d8ad6dfc9df7af899c0b36b62feff64b8eb8",
            "title": "Obj2Seq: Formatting Objects as Sequences with Class Prompt for Visual Tasks",
            "abstract": "Visual tasks vary a lot in their output formats and concerned contents, therefore it is hard to process them with an identical structure. One main obstacle lies in the high-dimensional outputs in object-level visual tasks. In this paper, we propose an object-centric vision framework, Obj2Seq. Obj2Seq takes objects as basic units, and regards most object-level visual tasks as sequence generation problems of objects. Therefore, these visual tasks can be decoupled into two steps. First recognize objects of given categories, and then generate a sequence for each of these objects. The definition of the output sequences varies for different tasks, and the model is supervised by matching these sequences with ground-truth targets. Obj2Seq is able to flexibly determine input categories to satisfy customized requirements, and be easily extended to different visual tasks. When experimenting on MS COCO, Obj2Seq achieves 45.7% AP on object detection, 89.0% AP on multi-label classification and 65.0% AP on human pose estimation. These results demonstrate its potential to be generally applied to different visual tasks. Code has been made available at: https://github.com/CASIA-IVA-Lab/Obj2Seq.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111630665",
                    "name": "Zhiyang Chen"
                },
                {
                    "authorId": "9619542",
                    "name": "Yousong Zhu"
                },
                {
                    "authorId": "2145316146",
                    "name": "Zhaowen Li"
                },
                {
                    "authorId": "2158027167",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "48625175",
                    "name": "Wei Li"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "34127238",
                    "name": "Chaoyang Zhao"
                },
                {
                    "authorId": "49279300",
                    "name": "Liwei Wu"
                },
                {
                    "authorId": "2153291683",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "49606029",
                    "name": "Jinqiao Wang"
                },
                {
                    "authorId": "2113727378",
                    "name": "Ming Tang"
                }
            ]
        }
    ]
}