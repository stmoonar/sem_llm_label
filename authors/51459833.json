{
    "authorId": "51459833",
    "papers": [
        {
            "paperId": "5969eb1c326c62199da8e619dd13092e4a449027",
            "title": "Interpolation for Robust Learning: Data Augmentation on Wasserstein Geodesics",
            "abstract": "We propose to study and promote the robustness of a model as per its performance through the interpolation of training data distributions. Specifically, (1) we augment the data by finding the worst-case Wasserstein barycenter on the geodesic connecting subpopulation distributions of different categories. (2) We regularize the model for smoother performance on the continuous geodesic path connecting subpopulation distributions. (3) Additionally, we provide a theoretical guarantee of robustness improvement and investigate how the geodesic location and the sample size contribute, respectively. Experimental validations of the proposed strategy on \\textit{four} datasets, including CIFAR-100 and ImageNet, establish the efficacy of our method, e.g., our method improves the baselines' certifiable robustness on CIFAR10 up to $7.7\\%$, with $16.8\\%$ on empirical robustness on CIFAR-100. Our work provides a new perspective of model robustness through the lens of Wasserstein geodesic-based interpolation with a practical off-the-shelf strategy that can be combined with existing robust training methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "29829392",
                    "name": "Aritra Guha"
                },
                {
                    "authorId": "2168550",
                    "name": "Zhuolin Yang"
                },
                {
                    "authorId": "145796039",
                    "name": "X. Nguyen"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "685248df424b5b5748c53c93cfab93f7e35e2cff",
            "title": "Multimodal Representation Learning of Cardiovascular Magnetic Resonance Imaging",
            "abstract": "Self-supervised learning is crucial for clinical imaging applications, given the lack of explicit labels in healthcare. However, conventional approaches that rely on precise vision-language alignment are not always feasible in complex clinical imaging modalities, such as cardiac magnetic resonance (CMR). CMR provides a comprehensive visualization of cardiac anatomy, physiology, and microstructure, making it challenging to interpret. Additionally, CMR reports require synthesizing information from sequences of images and different views, resulting in potentially weak alignment between the study and diagnosis report pair. To overcome these challenges, we propose \\textbf{CMRformer}, a multimodal learning framework to jointly learn sequences of CMR images and associated cardiologist's reports. Moreover, one of the major obstacles to improving CMR study is the lack of large, publicly available datasets. To bridge this gap, we collected a large \\textbf{CMR dataset}, which consists of 13,787 studies from clinical cases. By utilizing our proposed CMRformer and our collected dataset, we achieved remarkable performance in real-world clinical tasks, such as CMR image retrieval and diagnosis report retrieval. Furthermore, the learned representations are evaluated to be practically helpful for downstream applications, such as disease classification. Our work could potentially expedite progress in the CMR study and lead to more accurate and effective diagnosis and treatment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "2119407102",
                    "name": "Peide Huang"
                },
                {
                    "authorId": "1389703562",
                    "name": "Makiya Nakashima"
                },
                {
                    "authorId": "2158008485",
                    "name": "Jae-Hyeok Lee"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "2214612170",
                    "name": "W. Tang"
                },
                {
                    "authorId": "2118465661",
                    "name": "Po-Heng Chen"
                },
                {
                    "authorId": "2204079759",
                    "name": "C. Nguyen"
                },
                {
                    "authorId": "50435752",
                    "name": "Byung-Hak Kim"
                },
                {
                    "authorId": "2179058130",
                    "name": "Debbie Kwon"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "103185169",
                    "name": "David Chen"
                }
            ]
        },
        {
            "paperId": "7ed237af793f43c442b3e8e1bc9ace906a276b2a",
            "title": "Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?",
            "abstract": "Recent advancements in Large Language Models (LLMs) have drawn increasing attention since the learned embeddings pretrained on large-scale datasets have shown powerful ability in various downstream applications. However, whether the learned knowledge by LLMs can be transferred to clinical cardiology remains unknown. In this work, we aim to bridge this gap by transferring the knowledge of LLMs to clinical Electrocardiography (ECG). We propose an approach for cardiovascular disease diagnosis and automatic ECG diagnosis report generation. We also introduce an additional loss function by Optimal Transport (OT) to align the distribution between ECG and language embedding. The learned embeddings are evaluated on two downstream tasks: (1) automatic ECG diagnosis report generation, and (2) zero-shot cardiovascular disease detection. Our approach is able to generate high-quality cardiac diagnosis reports and also achieves competitive zero-shot classification performance even compared with supervised baselines, which proves the feasibility of transferring knowledge from LLMs to the cardiac domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2151860122",
                    "name": "Michael Rosenberg"
                },
                {
                    "authorId": "1418480916",
                    "name": "E. Liu"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "82da02137bae421a3f7a89c3bf2ab662037f4dfa",
            "title": "Embodied Executable Policy Learning with Language-based Scene Summarization",
            "abstract": "Large Language models (LLMs) have shown remarkable success in assisting robot learning tasks, i.e., complex household planning.However, the performance of pretrained LLMs heavily relies on domain-specific templated text data, which may be infeasible in real-world robot learning tasks with image-based observations. Moreover, existing LLMs with text inputs lack the capability to evolve with non-expert interactions with environments.In this work, we introduce a novel learning paradigm that generates robots\u2019 executable actions in the form of text, derived solely from visual observations. Our proposed paradigm stands apart from previous works, which utilized either language instructions or a combination of language and visual data as inputs. We demonstrate that our proposed method can employ two fine-tuning strategies, including imitation learning and reinforcement learning approaches, to adapt to the target test tasks effectively.We conduct extensive experiments involving various model selections, environments, and tasks across 7 house layouts in the VirtualHome environment. Our experimental results demonstrate that our method surpasses existing baselines, confirming the effectiveness of this novel learning paradigm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "87c9ac281bb606fb348acfb70348ce744a761d68",
            "title": "MultiSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos",
            "abstract": "Multimodal summarization with multimodal output (MSMO) has emerged as a promising research direction. Nonetheless, numerous limitations exist within existing public MSMO datasets, including insufficient upkeep, data inaccessibility, limited size, and the absence of proper categorization, which pose significant challenges to effective research. To address these challenges and provide a comprehensive dataset for this new direction, we have meticulously curated the MultiSum dataset. Our new dataset features (1) Human-validated summaries for both video and textual content, providing superior human instruction and labels for multimodal learning. (2) Comprehensively and meticulously arranged categorization, spanning 17 principal categories and 170 subcategories to encapsulate a diverse array of real-world scenarios. (3) Benchmark tests performed on the proposed dataset to assess varied tasks and methods, including video temporal segmentation , video summarization , text summarization , and multimodal summarization . To champion accessibility and collaboration, we release the MultiSum dataset and the data collection tool as fully open-source resources, fostering trans-parency and accelerating future developments. Our project website can be found at https://multisum-dataset.github.io/ .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2219660996",
                    "name": "Aditesh Kumar"
                },
                {
                    "authorId": "2220631291",
                    "name": "Karthik Mittal"
                },
                {
                    "authorId": "2219563428",
                    "name": "Claire Jin"
                },
                {
                    "authorId": "2149231840",
                    "name": "Zhengyuan Yang"
                },
                {
                    "authorId": "50703697",
                    "name": "Linjie Li"
                },
                {
                    "authorId": "2124948371",
                    "name": "Jianfeng Wang"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "29957038",
                    "name": "Lijuan Wang"
                }
            ]
        },
        {
            "paperId": "8f9ea5ed79c7996be594b08a0fc8d5471ecdf0f5",
            "title": "Offline Reinforcement Learning with Imbalanced Datasets",
            "abstract": "The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the context of imbalanced datasets with varying levels of imbalance, utilizing the variant of D4RL. Empirical results demonstrate the superiority of our method over other baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188128317",
                    "name": "Li Jiang"
                },
                {
                    "authorId": "2110844331",
                    "name": "Sijie Cheng"
                },
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "49507262",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "2185568838",
                    "name": "Wai Kin Victor Chan"
                },
                {
                    "authorId": "2221203321",
                    "name": "Zhao Ding"
                }
            ]
        },
        {
            "paperId": "af07c2b3ec06b31568f9842f71185f2cfc7d9efc",
            "title": "SCCS: Semantics-Consistent Cross-domain Summarization via Optimal Transport Alignment",
            "abstract": "Multimedia summarization with multimodal output (MSMO) is a recently explored application in language grounding. It plays an essential role in real-world applications, i.e., automatically generating cover images and titles for news articles or providing introductions to online videos. However, existing meth-ods extract features from the whole video and article and use fusion methods to select the representative one, thus usually ignoring the critical structure and varying semantics with video/document. In this work, we propose a Semantics-Consistent Cross-domain Summa-rization (SCCS) model based on optimal transport alignment with visual and textual segmentation. Our method first decomposes both videos and articles into segments in order to capture the structural semantics, and then follows a cross-domain alignment objective with optimal transport distance, which leverages multimodal interaction to match and select the visual and textual summary. We evaluated our method on three MSMO datasets, and achieved performance improvement by 8% & 6% of textual and 6.6% &5.7% of video summarization, respectively, which demonstrated the effectiveness of our method in producing high-quality multimodal summaries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "145262461",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "8056043",
                    "name": "Zhaowen Wang"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "41151701",
                    "name": "Hailin Jin"
                }
            ]
        },
        {
            "paperId": "e5d31a57c28225eec753e2174fe8c3fbbeca20fa",
            "title": "Converting ECG Signals to Images for Efficient Image-text Retrieval via Encoding",
            "abstract": "Automated interpretation of electrocardiograms (ECG) has garnered significant attention with the advancements in machine learning methodologies. Despite the growing interest in automated ECG interpretation using machine learning, most current studies focus solely on classification or regression tasks and overlook a crucial aspect of clinical cardio-disease diagnosis: the diagnostic report generated by experienced human clinicians. In this paper, we introduce a novel approach to ECG interpretation, leveraging recent breakthroughs in Large Language Models (LLMs) and Vision-Transformer (ViT) models. Rather than treating ECG diagnosis as a classification or regression task, we propose an alternative method of automatically identifying the most similar clinical cases based on the input ECG data. Also, since interpreting ECG as images are more affordable and accessible, we process ECG as encoded images and adopt a vision-language learning paradigm to jointly learn vision-language alignment between encoded ECG images and ECG diagnosis reports. Encoding ECG into images can result in an efficient ECG retrieval system, which will be highly practical and useful in clinical applications. More importantly, our findings could serve as a crucial resource for providing diagnostic services in regions where only paper-printed ECG images are accessible due to past underdevelopment.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "2131159786",
                    "name": "Shiqi Liu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2157140664",
                    "name": "Jingqi Zhang"
                },
                {
                    "authorId": "1722611",
                    "name": "Chaojing Duan"
                },
                {
                    "authorId": "2151860122",
                    "name": "Michael Rosenberg"
                },
                {
                    "authorId": "1418480916",
                    "name": "E. Liu"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "e8b7b212d448ef3e08423e26bd224aa7ccf6dec1",
            "title": "Align and Attend: Multimodal Summarization with Dual Contrastive Losses",
            "abstract": "The goal of multimodal summarization is to extract the most important information from different modalities to form summaries. Unlike unimodal summarization, the multimodal summarization task explicitly leverages cross-modal information to help generate more reliable and high-quality summaries. However, existing methods fail to lever-age the temporal correspondence between different modal-ities and ignore the intrinsic correlation between different samples. To address this issue, we introduce Align and Attend Multimodal Summarization (A2Summ), a unified multimodal transformer-based model which can effectively align and attend the multimodal input. In addition, we propose two novel contrastive losses to model both inter-sample and intra-sample correlations. Extensive experiments on two standard video summarization datasets (TVSum and SumMe) and two multimodal summarization datasets (Daily Mail and CNN) demonstrate the superiority of A2Summ, achieving state-of-the-art performances on all datasets. Moreover, we collected a large-scale multimodal summarization dataset BLiSS, which contains livestream videos and transcribed texts with annotated summaries. Our code and dataset are publicly available at https://boheumd.github.io/A2Summ/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2082464935",
                    "name": "Bo He"
                },
                {
                    "authorId": "2152812028",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "145262461",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "1781242",
                    "name": "Abhinav Shrivastava"
                },
                {
                    "authorId": "8056043",
                    "name": "Zhaowen Wang"
                }
            ]
        },
        {
            "paperId": "f2d1f783d593cfd1ff3113ddcde86a19e4d684c9",
            "title": "Interpolation for Robust Learning: Data Augmentation on Geodesics",
            "abstract": "We propose to study and promote the robustness of a model as per its performance through the interpolation of training data distributions. Speci\ufb01cally, (1) we augment the data by \ufb01nding the worst-case Wasserstein barycenter on the geodesic connecting subpopulation distributions of di\ufb00erent categories. (2) We regularize the model for smoother performance on the continuous geodesic path connecting subpopulation distributions. (3) Additionally, we provide a theoretical guarantee of robustness improvement and investigate how the geodesic location and the sample size contribute, respectively. Experimental validations of the proposed strategy on four datasets, including CIFAR-100 and ImageNet, establish the e\ufb03cacy of our method, e.g., our method improves the baselines\u2019 certi\ufb01able robustness on CIFAR10 up to 7 . 7%, with 16 . 8% on empirical robustness on CIFAR-100. Our work provides a new perspective of model robustness through the lens of Wasserstein geodesic-based interpolation with a practical o\ufb00-the-shelf strategy that can be combined with existing robust training methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "29829392",
                    "name": "Aritra Guha"
                },
                {
                    "authorId": "2119399524",
                    "name": "Zhuolin Yang"
                },
                {
                    "authorId": "145796039",
                    "name": "X. Nguyen"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                }
            ]
        }
    ]
}