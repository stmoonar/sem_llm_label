{
    "authorId": "3331141",
    "papers": [
        {
            "paperId": "14673ee549a757fbd62c16dedb0d1dce59bdb194",
            "title": "Distinguishing Address vs. Reference Mentions of Personal Names in Text",
            "abstract": "Detecting named entities in text has long been a core NLP task. However, not much work has gone into distinguishing whether an entity mention is addressing the entity vs. referring to the entity; e.g., John, would you turn the light off? vs. John turned the light off . While this distinction is marked by a vocative case marker in some languages, many modern Indo-European languages such as English do not use such explicit vocative markers, and the distinction is left to be interpreted in context. In this paper, we present a new annotated dataset that captures the address vs. reference distinction in English, 1 an automatic tagger that performs at 85% accuracy in making this distinction, and demonstrate how this distinction is important in NLP and computational social science applications in English language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "39829408",
                    "name": "M. Ferguson"
                },
                {
                    "authorId": "7180227",
                    "name": "Stav Atir"
                }
            ]
        },
        {
            "paperId": "19bd9ff0e8843b3fc18cf204d29c61d53ed2fdb3",
            "title": "MD3: The Multi-Dialect Dataset of Dialogues",
            "abstract": "We introduce a new dataset of conversational speech representing English from India, Nigeria, and the United States. The Multi-Dialect Dataset of Dialogues (MD3) strikes a new balance between open-ended conversational speech and task-oriented dialogue by prompting participants to perform a series of short information-sharing tasks. This facilitates quantitative cross-dialectal comparison, while avoiding the imposition of a restrictive task structure that might inhibit the expression of dialect features. Preliminary analysis of the dataset reveals significant differences in syntax and in the use of discourse markers. The dataset, which will be made publicly available with the publication of this paper, includes more than 20 hours of audio and more than 200,000 orthographically-transcribed tokens.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065533890",
                    "name": "Jacob Eisenstein"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "2059157845",
                    "name": "Clara Rivera"
                },
                {
                    "authorId": "81551000",
                    "name": "Dorottya Demszky"
                },
                {
                    "authorId": "46362239",
                    "name": "D. Sharma"
                }
            ]
        },
        {
            "paperId": "38e4ee78d41d0f910e8a5263e58e7b35cacb87c6",
            "title": "Intersectionality in Conversational AI Safety: How Bayesian Multilevel Models Help Understand Diverse Perceptions of Safety",
            "abstract": "Conversational AI systems exhibit a level of human-like behavior that promises to have profound impacts on many aspects of daily life -- how people access information, create content, and seek social support. Yet these models have also shown a propensity for biases, offensive language, and conveying false information. Consequently, understanding and moderating safety risks in these models is a critical technical and social challenge. Perception of safety is intrinsically subjective, where many factors -- often intersecting -- could determine why one person may consider a conversation with a chatbot safe and another person could consider the same conversation unsafe. In this work, we focus on demographic factors that could influence such diverse perceptions. To this end, we contribute an analysis using Bayesian multilevel modeling to explore the connection between rater demographics and how raters report safety of conversational AI systems. We study a sample of 252 human raters stratified by gender, age group, race/ethnicity group, and locale. This rater pool provided safety labels for 1,340 human-chatbot conversations. Our results show that intersectional effects involving demographic characteristics such as race/ethnicity, gender, and age, as well as content characteristics, such as degree of harm, all play significant roles in determining the safety of conversational AI systems. For example, race/ethnicity and gender show strong intersectional effects, particularly among South Asian and East Asian women. We also find that conversational degree of harm impacts raters of all race/ethnicity groups, but that Indigenous and South Asian raters are particularly sensitive to this harm. Finally, we observe the effect of education is uniquely intersectional for Indigenous raters, highlighting the utility of multilevel frameworks for uncovering underrepresented social perspectives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2300795",
                    "name": "Christopher Homan"
                },
                {
                    "authorId": "2118924379",
                    "name": "Greg Serapio-Garc\u00eda"
                },
                {
                    "authorId": "1745337",
                    "name": "Lora Aroyo"
                },
                {
                    "authorId": "2152965375",
                    "name": "Mark D\u00edaz"
                },
                {
                    "authorId": "119389860",
                    "name": "Alicia Parrish"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "2110306510",
                    "name": "Alex S Taylor"
                },
                {
                    "authorId": "2202541709",
                    "name": "Ding Wang"
                }
            ]
        },
        {
            "paperId": "6e30a511242cd48a1394d87ce8d2b682978014a0",
            "title": "DICES Dataset: Diversity in Conversational AI Evaluation for Safety",
            "abstract": "Machine learning approaches often require training and evaluation datasets with a clear separation between positive and negative examples. This risks simplifying and even obscuring the inherent subjectivity present in many tasks. Preserving such variance in content and diversity in datasets is often expensive and laborious. This is especially troubling when building safety datasets for conversational AI systems, as safety is both socially and culturally situated. To demonstrate this crucial aspect of conversational AI safety, and to facilitate in-depth model performance analyses, we introduce the DICES (Diversity In Conversational AI Evaluation for Safety) dataset that contains fine-grained demographic information about raters, high replication of ratings per item to ensure statistical power for analyses, and encodes rater votes as distributions across different demographics to allow for in-depth explorations of different aggregation strategies. In short, the DICES dataset enables the observation and measurement of variance, ambiguity, and diversity in the context of conversational AI safety. We also illustrate how the dataset offers a basis for establishing metrics to show how raters' ratings can intersects with demographic categories such as racial/ethnic groups, age groups, and genders. The goal of DICES is to be used as a shared resource and benchmark that respects diverse perspectives during safety evaluation of conversational AI systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1745337",
                    "name": "Lora Aroyo"
                },
                {
                    "authorId": "2110307119",
                    "name": "Alex S. Taylor"
                },
                {
                    "authorId": "2152965375",
                    "name": "Mark D\u00edaz"
                },
                {
                    "authorId": "49004787",
                    "name": "C. Homan"
                },
                {
                    "authorId": "119389860",
                    "name": "Alicia Parrish"
                },
                {
                    "authorId": "2118924379",
                    "name": "Greg Serapio-Garc\u00eda"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "2202541709",
                    "name": "Ding Wang"
                }
            ]
        },
        {
            "paperId": "7dafe15855bf7a7899bf255b97025f2b46e56847",
            "title": "The Reasonable Effectiveness of Diverse Evaluation Data",
            "abstract": "In this paper, we present findings from an semi-experimental exploration of rater diversity and its influence on safety annotations of conversations generated by humans talking to a generative AI-chat bot. We find significant differences in judgments produced by raters from different geographic regions and annotation platforms, and correlate these perspectives with demographic sub-groups. Our work helps define best practices in model development -- specifically human evaluation of generative models -- on the backdrop of growing work on sociotechnical AI evaluations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1745337",
                    "name": "Lora Aroyo"
                },
                {
                    "authorId": "2152965375",
                    "name": "Mark D\u00edaz"
                },
                {
                    "authorId": "2300795",
                    "name": "Christopher Homan"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "2110306510",
                    "name": "Alex S Taylor"
                },
                {
                    "authorId": "2202541709",
                    "name": "Ding Wang"
                }
            ]
        },
        {
            "paperId": "dee265d98d409664dc892335a40a76129bcc6b73",
            "title": "Building Socio-culturally Inclusive Stereotype Resources with Community Engagement",
            "abstract": "With rapid development and deployment of generative language models in global settings, there is an urgent need to also scale our measurements of harm, not just in the number and types of harms covered, but also how well they account for local cultural contexts, including marginalized identities and the social biases experienced by them. Current evaluation paradigms are limited in their abilities to address this, as they are not representative of diverse, locally situated but global, socio-cultural perspectives. It is imperative that our evaluation resources are enhanced and calibrated by including people and experiences from different cultures and societies worldwide, in order to prevent gross underestimations or skews in measurements of harm. In this work, we demonstrate a socio-culturally aware expansion of evaluation resources in the Indian societal context, specifically for the harm of stereotyping. We devise a community engaged effort to build a resource which contains stereotypes for axes of disparity that are uniquely present in India. The resultant resource increases the number of stereotypes known for and in the Indian context by over 1000 stereotypes across many unique identities. We also demonstrate the utility and effectiveness of such expanded resources for evaluations of language models. CONTENT WARNING: This paper contains examples of stereotypes that may be offensive.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50991767",
                    "name": "Sunipa Dev"
                },
                {
                    "authorId": "81464242",
                    "name": "J. Goyal"
                },
                {
                    "authorId": "2075016270",
                    "name": "Dinesh Tewari"
                },
                {
                    "authorId": "2160404",
                    "name": "Shachi Dave"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                }
            ]
        },
        {
            "paperId": "f4f154892800008894ebbf57add31fcaac4f27ca",
            "title": "SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models",
            "abstract": "Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36701727",
                    "name": "Akshita Jha"
                },
                {
                    "authorId": "2132006618",
                    "name": "A. Davani"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                },
                {
                    "authorId": "2160404",
                    "name": "Shachi Dave"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "50991767",
                    "name": "Sunipa Dev"
                }
            ]
        },
        {
            "paperId": "047796039425ad33e57c0e92141c8321cf708197",
            "title": "Power to the People? Opportunities and Challenges for Participatory AI",
            "abstract": "Participatory approaches to artificial intelligence (AI) and machine learning (ML) are gaining momentum: the increased attention comes partly with the view that participation opens the gateway to an inclusive, equitable, robust, responsible and trustworthy AI. Among other benefits, participatory approaches are essential to understanding and adequately representing the needs, desires and perspectives of historically marginalized communities. However, there currently exists lack of clarity on what meaningful participation entails and what it is expected to do. In this paper we first review participatory approaches as situated in historical contexts as well as participatory methods and practices within the AI and ML pipeline. We then introduce three case studies in participatory AI. Participation holds the potential for beneficial, emancipatory and empowering technology design, development and deployment while also being at risk for concerns such as cooptation and conflation with other activities. We lay out these limitations and concerns and argue that as participatory AI/ML becomes in vogue, a contextual and nuanced understanding of the term as well as consideration of who the primary beneficiaries of participatory activities ought to be constitute crucial factors to realizing the benefits and opportunities that participation brings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8318698",
                    "name": "Abeba Birhane"
                },
                {
                    "authorId": "103087275",
                    "name": "William S. Isaac"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "2146515892",
                    "name": "M. D'iaz"
                },
                {
                    "authorId": "21817215",
                    "name": "M. C. Elish"
                },
                {
                    "authorId": "116589025",
                    "name": "Iason Gabriel"
                },
                {
                    "authorId": "1701708450",
                    "name": "Shakir Mohamed"
                }
            ]
        },
        {
            "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2841893",
                    "name": "Aakanksha Chowdhery"
                },
                {
                    "authorId": "46617804",
                    "name": "Sharan Narang"
                },
                {
                    "authorId": "39172707",
                    "name": "Jacob Devlin"
                },
                {
                    "authorId": "40377863",
                    "name": "Maarten Bosma"
                },
                {
                    "authorId": "2159632445",
                    "name": "Gaurav Mishra"
                },
                {
                    "authorId": "145625142",
                    "name": "Adam Roberts"
                },
                {
                    "authorId": "152399055",
                    "name": "P. Barham"
                },
                {
                    "authorId": "3351938",
                    "name": "Hyung Won Chung"
                },
                {
                    "authorId": "152549864",
                    "name": "Charles Sutton"
                },
                {
                    "authorId": "3159346",
                    "name": "Sebastian Gehrmann"
                },
                {
                    "authorId": "2620528",
                    "name": "Parker Schuh"
                },
                {
                    "authorId": "2362367",
                    "name": "Kensen Shi"
                },
                {
                    "authorId": "2160888237",
                    "name": "Sasha Tsvyashchenko"
                },
                {
                    "authorId": "2124977868",
                    "name": "Joshua Maynez"
                },
                {
                    "authorId": "1484043592",
                    "name": "Abhishek Rao"
                },
                {
                    "authorId": "80940648",
                    "name": "Parker Barnes"
                },
                {
                    "authorId": "144447820",
                    "name": "Yi Tay"
                },
                {
                    "authorId": "1846258",
                    "name": "Noam M. Shazeer"
                },
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "49849144",
                    "name": "Emily Reif"
                },
                {
                    "authorId": "2140321952",
                    "name": "Nan Du"
                },
                {
                    "authorId": "2044655623",
                    "name": "Ben Hutchinson"
                },
                {
                    "authorId": "2161431901",
                    "name": "Reiner Pope"
                },
                {
                    "authorId": "2065251344",
                    "name": "James Bradbury"
                },
                {
                    "authorId": "2058365883",
                    "name": "Jacob Austin"
                },
                {
                    "authorId": "2090818",
                    "name": "M. Isard"
                },
                {
                    "authorId": "2284681044",
                    "name": "Guy Gur-Ari"
                },
                {
                    "authorId": "38253388",
                    "name": "Pengcheng Yin"
                },
                {
                    "authorId": "2145151992",
                    "name": "Toju Duke"
                },
                {
                    "authorId": "6639036",
                    "name": "Anselm Levskaya"
                },
                {
                    "authorId": "1780892",
                    "name": "Sanjay Ghemawat"
                },
                {
                    "authorId": "50991767",
                    "name": "Sunipa Dev"
                },
                {
                    "authorId": "47407464",
                    "name": "H. Michalewski"
                },
                {
                    "authorId": "143936294",
                    "name": "Xavier Garc\u00eda"
                },
                {
                    "authorId": "40055795",
                    "name": "Vedant Misra"
                },
                {
                    "authorId": "2148473059",
                    "name": "Kevin Robinson"
                },
                {
                    "authorId": "2096916416",
                    "name": "Liam Fedus"
                },
                {
                    "authorId": "65855107",
                    "name": "Denny Zhou"
                },
                {
                    "authorId": "7975935",
                    "name": "Daphne Ippolito"
                },
                {
                    "authorId": "150970919",
                    "name": "D. Luan"
                },
                {
                    "authorId": "8939217",
                    "name": "Hyeontaek Lim"
                },
                {
                    "authorId": "2368067",
                    "name": "Barret Zoph"
                },
                {
                    "authorId": "1572884723",
                    "name": "A. Spiridonov"
                },
                {
                    "authorId": "35474601",
                    "name": "Ryan Sepassi"
                },
                {
                    "authorId": "35363891",
                    "name": "David Dohan"
                },
                {
                    "authorId": "3504647",
                    "name": "Shivani Agrawal"
                },
                {
                    "authorId": "3175815",
                    "name": "Mark Omernick"
                },
                {
                    "authorId": "2555924",
                    "name": "Andrew M. Dai"
                },
                {
                    "authorId": "2598683",
                    "name": "Thanumalayan Sankaranarayana Pillai"
                },
                {
                    "authorId": "97905921",
                    "name": "Marie Pellat"
                },
                {
                    "authorId": "102549875",
                    "name": "Aitor Lewkowycz"
                },
                {
                    "authorId": "2057453483",
                    "name": "Erica Moreira"
                },
                {
                    "authorId": "48422824",
                    "name": "R. Child"
                },
                {
                    "authorId": "2636739",
                    "name": "Oleksandr Polozov"
                },
                {
                    "authorId": "3844009",
                    "name": "Katherine Lee"
                },
                {
                    "authorId": "2198519",
                    "name": "Zongwei Zhou"
                },
                {
                    "authorId": "1524732527",
                    "name": "Xuezhi Wang"
                },
                {
                    "authorId": "4125424",
                    "name": "Brennan Saeta"
                },
                {
                    "authorId": "2152965375",
                    "name": "Mark D\u00edaz"
                },
                {
                    "authorId": "2345617",
                    "name": "Orhan Firat"
                },
                {
                    "authorId": "1754926",
                    "name": "Michele Catasta"
                },
                {
                    "authorId": "119640649",
                    "name": "Jason Wei"
                },
                {
                    "authorId": "1398655031",
                    "name": "K. Meier-Hellstern"
                },
                {
                    "authorId": "2396681",
                    "name": "D. Eck"
                },
                {
                    "authorId": "48448318",
                    "name": "J. Dean"
                },
                {
                    "authorId": "1754497",
                    "name": "Slav Petrov"
                },
                {
                    "authorId": "22640071",
                    "name": "Noah Fiedel"
                }
            ]
        },
        {
            "paperId": "0d2480eb44deb3e1dfdf3686705975c04f584b73",
            "title": "A Human Rights-Based Approach to Responsible AI",
            "abstract": "Research on fairness, accountability, transparency and ethics of AI-based interventions in society has gained much-needed momen-tum in recent years. However it lacks an explicit alignment with a set of normative values and principles that guide this research and interventions. Rather, an implicit consensus is often assumed to hold for the values we impart into our models \u2013 something that is at odds with the pluralistic world we live in. In this paper, we put forth the doctrine of universal human rights as a set of globally salient and cross-culturally recognized set of values that can serve as a grounding framework for explicit value alignment in responsible AI \u2013 and discuss its e\ufb03cacy as a framework for civil society partnership and participation. We argue that a human rights framework orients the research in this space away from the machines and the risks of their biases, and towards humans and the risks to their rights, essentially helping to center the conversation around who is harmed, what harms they face, and how those harms may be mitigated.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3331141",
                    "name": "Vinodkumar Prabhakaran"
                },
                {
                    "authorId": "118707418",
                    "name": "Margaret Mitchell"
                },
                {
                    "authorId": "2076288",
                    "name": "Timnit Gebru"
                },
                {
                    "authorId": "116589025",
                    "name": "Iason Gabriel"
                }
            ]
        }
    ]
}