{
    "authorId": "2281612988",
    "papers": [
        {
            "paperId": "2aef0925afe4888580147fefd820b98ce25481f4",
            "title": "Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements",
            "abstract": "With the rapid proliferation of textual data, predicting long texts has emerged as a significant challenge in the domain of natural language processing. Traditional text prediction methods encounter substantial difficulties when grappling with long texts, primarily due to the presence of redundant and irrelevant information, which impedes the model's capacity to capture pivotal insights from the text. To address this issue, we introduce a novel approach to long-text classification and prediction. Initially, we employ embedding techniques to condense the long texts, aiming to diminish the redundancy therein. Subsequently,the Bidirectional Encoder Representations from Transformers (BERT) embedding method is utilized for text classification training. Experimental outcomes indicate that our method realizes considerable performance enhancements in classifying long texts of Preferential Trade Agreements. Furthermore, the condensation of text through embedding methods not only augments prediction accuracy but also substantially reduces computational complexity. Overall, this paper presents a strategy for long-text prediction, offering a valuable reference for researchers and engineers in the natural language processing sphere.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204629618",
                    "name": "Jiahui Zhao"
                },
                {
                    "authorId": "2280395029",
                    "name": "Ziyi Meng"
                },
                {
                    "authorId": "2280333415",
                    "name": "Stepan Gordeev"
                },
                {
                    "authorId": "2281612988",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "2276324326",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2280335006",
                    "name": "Sandro Steinbach"
                },
                {
                    "authorId": "2265671362",
                    "name": "Caiwen Ding"
                }
            ]
        },
        {
            "paperId": "42d2e852d9f3d7bcc499584fe0a8e5ad8a2df11e",
            "title": "Structural Knowledge Informed Continual Multivariate Time Series Forecasting",
            "abstract": "Recent studies in multivariate time series (MTS) forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations. However, modeling variable dependencies remains underexplored when MTS is continuously accumulated under different regimes (stages). Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance. To address this issue, we propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting within a continual learning paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects representative MTS samples from each regime for memory replay. Specifically, we develop a forecasting model based on graph structure learning, where a consistency regularization scheme is imposed between the learned variable dependencies and the structural knowledge while optimizing the forecasting objective over the MTS data. As such, MTS representations learned in each regime are associated with distinct structural knowledge, which helps the model memorize a variety of conceivable scenarios and results in accurate forecasts in the continual learning context. Meanwhile, we develop a representation-matching memory replay scheme that maximizes the temporal coverage of MTS data to efficiently preserve the underlying temporal dynamics and dependency structures of each regime. Thorough empirical studies on synthetic and real-world benchmarks validate SKI-CL's efficacy and advantages over the state-of-the-art for continual MTS forecasting tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281612988",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2276324326",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2257349440",
                    "name": "Sahil Garg"
                },
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2246897295",
                    "name": "Yuriy Nevmyvaka"
                }
            ]
        },
        {
            "paperId": "44f6cea2aa05620fef00d8dc9e566918dc6771c9",
            "title": "Empowering Time Series Analysis with Large Language Models: A Survey",
            "abstract": "Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (\\textit{i.e.}, direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2281612988",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "3358065",
                    "name": "Xikun Zhang"
                },
                {
                    "authorId": "2257349440",
                    "name": "Sahil Garg"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2246897295",
                    "name": "Yuriy Nevmyvaka"
                },
                {
                    "authorId": "2276324326",
                    "name": "Dongjin Song"
                }
            ]
        },
        {
            "paperId": "48e1128f64be63b438df8893f237328f35b14647",
            "title": "International Trade Flow Prediction with Bilateral Trade Provisions",
            "abstract": "This paper presents a novel methodology for predicting international bilateral trade flows, emphasizing the growing importance of Preferential Trade Agreements (PTAs) in the global trade landscape. Acknowledging the limitations of traditional models like the Gravity Model of Trade, this study introduces a two-stage approach combining explainable machine learning and factorization models. The first stage employs SHAP Explainer for effective variable selection, identifying key provisions in PTAs, while the second stage utilizes Factorization Machine models to analyze the pairwise interaction effects of these provisions on trade flows. By analyzing comprehensive datasets, the paper demonstrates the efficacy of this approach. The findings not only enhance the predictive accuracy of trade flow models but also offer deeper insights into the complex dynamics of international trade, influenced by specific bilateral trade provisions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "authors": [
                {
                    "authorId": "2281612988",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "2280333415",
                    "name": "Stepan Gordeev"
                },
                {
                    "authorId": "2204629618",
                    "name": "Jiahui Zhao"
                },
                {
                    "authorId": "2280395029",
                    "name": "Ziyi Meng"
                },
                {
                    "authorId": "2290726707",
                    "name": "Caiwen Ding"
                },
                {
                    "authorId": "2280335006",
                    "name": "Sandro Steinbach"
                },
                {
                    "authorId": "2244672906",
                    "name": "Dongjin Song"
                }
            ]
        },
        {
            "paperId": "a0e7f328781eda3e3dc919c77bd1eedb99b99612",
            "title": "S2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
            "abstract": "Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embeddings space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maximizing the cosine similarity in the joint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as prompts to provide strong indicators (context) for time series that exhibit different temporal dynamics. With thorough empirical studies on multiple benchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve superior forecasting performance over state-of-the-art baselines. Furthermore, our ablation studies and visualizations verify the necessity of prompt learning informed by semantic space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281612988",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2257349440",
                    "name": "Sahil Garg"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2246897295",
                    "name": "Yuriy Nevmyvaka"
                },
                {
                    "authorId": "2276324326",
                    "name": "Dongjin Song"
                }
            ]
        }
    ]
}