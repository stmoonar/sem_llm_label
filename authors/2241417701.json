{
    "authorId": "2241417701",
    "papers": [
        {
            "paperId": "2fb593ca4b6d2631832d6424e238c32db3db5434",
            "title": "Factuality of Large Language Models in the Year 2024",
            "abstract": "Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2282539097",
                    "name": "Muhammad Arslan Manzoor"
                },
                {
                    "authorId": "2284540712",
                    "name": "Fei Liu"
                },
                {
                    "authorId": "2282539112",
                    "name": "Georgi Georgiev"
                },
                {
                    "authorId": "2211732585",
                    "name": "Rocktim Jyoti Das"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "8f584931615043d23ed5a1351b0fca547fbe6f16",
            "title": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR",
            "abstract": "Recent advancements in multimodal large language models (MLLMs) have made significant progress in integrating information across various modalities, yet real-world applications in educational and scientific domains remain challenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task, which focuses on transcribing scientific conference videos by leveraging visual information from slides to enhance the accuracy of technical terminologies. Realized that traditional metrics like WER fall short in assessing performance accurately, prompting the proposal of severity-aware WER (SWER) that considers the content type and severity of ASR errors. We propose the Scientific Vision Augmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to improve transcript quality through post-editing. Evaluations of state-of-the-art MLLMs, including GPT-4o, show a 45% improvement over speech-only baselines, highlighting the importance of multimodal information integration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "122699890",
                    "name": "Thuy-Trang Vu"
                },
                {
                    "authorId": "2888926",
                    "name": "Ehsan Shareghi"
                },
                {
                    "authorId": "2561045",
                    "name": "Gholamreza Haffari"
                }
            ]
        },
        {
            "paperId": "9741eb61739f2221c186634663876e2c1024c746",
            "title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs",
            "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets. OpenFactCheck is publicly released at https://github.com/yuxiaw/OpenFactCheck.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2300555930",
                    "name": "Hasan Iqbal"
                },
                {
                    "authorId": "2282539112",
                    "name": "Georgi Georgiev"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "aa335755b411de74ca37536f636b76ab87bcda07",
            "title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs",
            "abstract": "Empathy plays a pivotal role in fostering prosocial behavior, often triggered by the sharing of personal experiences through narratives. However, modeling empathy using NLP approaches remains challenging due to its deep interconnection with human interaction dynamics. Previous approaches, which involve fine-tuning language models (LMs) on human-annotated empathic datasets, have had limited success. In our pursuit of improving empathy understanding in LMs, we propose several strategies, including contrastive learning with masked LMs and supervised fine-tuning with Large Language Models (LLMs). While these methods show improvements over previous methods, the overall results remain unsatisfactory. To better understand this trend, we performed an analysis which reveals a low agreement among annotators. This lack of consensus hinders training and highlights the subjective nature of the task. We also explore the cultural impact on annotations. To study this, we meticulously collected story pairs in Urdu language and find that subjectivity in interpreting empathy among annotators appears to be independent of cultural background. The insights from our systematic exploration of LMs' understanding of empathy suggest that there is considerable room for exploration in both task formulation and modeling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282539097",
                    "name": "Muhammad Arslan Manzoor"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "c56eab12bd00e2fe28868af21d518044d66df00d",
            "title": "SemEval-2024 Task 8: Multidomain, Multimodel and Multilingual Machine-Generated Text Detection",
            "abstract": "We present the results and the main findings of SemEval-2024 Task 8: Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. The task featured three subtasks. Subtask A is a binary classification task determining whether a text is written by a human or generated by a machine. This subtask has two tracks: a monolingual track focused solely on English texts and a multilingual track. Subtask B is to detect the exact source of a text, discerning whether it is written by a human or generated by a specific LLM. Subtask C aims to identify the changing point within a text, at which the authorship transitions from human to machine. The task attracted a large number of participants: subtask A monolingual (126), subtask A multilingual (59), subtask B (70), and subtask C (30). In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used. For all subtasks, the best systems used LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2058455381",
                    "name": "Petar Ivanov"
                },
                {
                    "authorId": "2265989879",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "2284686859",
                    "name": "Giovanni Puccetti"
                },
                {
                    "authorId": "2284687590",
                    "name": "Thomas Arnold"
                },
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "c59628de894a4aa7f91548bad5b4103b747256e8",
            "title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
            "abstract": "The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain, and multi-generator corpus of MGTs -- M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github.com/mbzuai-nlp/M4GT-Bench.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2058455381",
                    "name": "Petar Ivanov"
                },
                {
                    "authorId": "2265989879",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "2284688236",
                    "name": "Osama Mohanned Afzal"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "2284686859",
                    "name": "Giovanni Puccetti"
                },
                {
                    "authorId": "2284687590",
                    "name": "Thomas Arnold"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "ea4c0ab66529cac83f0b2b50eaef305da6a297e1",
            "title": "LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection",
            "abstract": "The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs). Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated). This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains. In this paper, we present $\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection. It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished. Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation. This might be useful in some domains like education, where any LLM intervention is usually prohibited. Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains. LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video describing our system is available at https://youtu.be/E8eT_bE7k8c.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2315296727",
                    "name": "Mervat Abassy"
                },
                {
                    "authorId": "2315302985",
                    "name": "Kareem Elozeiri"
                },
                {
                    "authorId": "2315840157",
                    "name": "Alexander Aziz"
                },
                {
                    "authorId": "2315297170",
                    "name": "Minh Ngoc Ta"
                },
                {
                    "authorId": "2309163441",
                    "name": "Raj Vardhan Tomar"
                },
                {
                    "authorId": "2315301202",
                    "name": "Bimarsha Adhikari"
                },
                {
                    "authorId": "2315642904",
                    "name": "Saad El Dine Ahmed"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2315671993",
                    "name": "Zhuohan Xie"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2300660029",
                    "name": "Ekaterina Artemova"
                },
                {
                    "authorId": "51259225",
                    "name": "V. Mikhailov"
                },
                {
                    "authorId": "2308041454",
                    "name": "Rui Xing"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "2300555930",
                    "name": "Hasan Iqbal"
                },
                {
                    "authorId": "2266755049",
                    "name": "Zain Muhammad Mujahid"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "40dccaaf6f5a563437f26c75e92279ba415df392",
            "title": "Rethinking STS and NLI in Large Language Models",
            "abstract": "Recent years, have seen the rise of large language models (LLMs), where practitioners use task-specific prompts; this was shown to be effective for a variety of tasks. However, when applied to semantic textual similarity (STS) and natural language inference (NLI), the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements. With this in mind, here we try to rethink STS and NLI in the era of LLMs. We first evaluate the performance of STS and NLI in the clinical/biomedical domain, and then we assess LLMs\u2019 predictive confidence and their capability of capturing collective human opinions. We find that these old problems are still to be properly addressed in the era of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "6aa6003c7d7b3d275ae981aa6200014968c32430",
            "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impressive performance, they can be unreliable due to factual errors in their generations. Assessing their confidence and calibrating them across different tasks can help mitigate risks and enable LLMs to produce better generations. There has been a lot of recent research aiming to address this, but there has been no comprehensive overview to organize it and to outline the main lessons learned. The present survey aims to bridge this gap. In particular, we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration. We further discuss their applications and suggest promising directions for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "2266465656",
                    "name": "Fengyu Cai"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2266465052",
                    "name": "Heinz Koeppl"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "72c62b3a2280e66499d5918fadc3c31474425768",
            "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
            "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document, aiming to facilitate the evaluation of automatic fact-checking systems. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims, with the best F1=0.63 by this annotation solution based on GPT-4. Annotation tool, benchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2266755026",
                    "name": "Revanth Gangi Reddy"
                },
                {
                    "authorId": "2266755049",
                    "name": "Zain Muhammad Mujahid"
                },
                {
                    "authorId": "1943255906",
                    "name": "Arnav Arora"
                },
                {
                    "authorId": "2266754085",
                    "name": "Aleksandr Rubashevskii"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2266841994",
                    "name": "Liangming Pan"
                },
                {
                    "authorId": "2107059981",
                    "name": "Nadav Borenstein"
                },
                {
                    "authorId": "2266754790",
                    "name": "Aditya Pillai"
                },
                {
                    "authorId": "2256988818",
                    "name": "Isabelle Augenstein"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        }
    ]
}