{
    "authorId": "7565696",
    "papers": [
        {
            "paperId": "30dad15dfd627f37a461ede4f658c7c647947f30",
            "title": "When is the consistent prediction likely to be a correct prediction?",
            "abstract": "Self-consistency (Wang et al., 2023) suggests that the most consistent answer obtained through large language models (LLMs) is more likely to be correct. In this paper, we challenge this argument and propose a nuanced correction. Our observations indicate that consistent answers derived through more computation i.e. longer reasoning texts, rather than simply the most consistent answer across all outputs, are more likely to be correct. This is predominantly because we demonstrate that LLMs can autonomously produce chain-of-thought (CoT) style reasoning with no custom prompts merely while generating longer responses, which lead to consistent predictions that are more accurate. In the zero-shot setting, by sampling Mixtral-8x7B model multiple times and considering longer responses, we achieve 86% of its self-consistency performance obtained through zero-shot CoT prompting on the GSM8K and MultiArith datasets. Finally, we demonstrate that the probability of LLMs generating a longer response is quite low, highlighting the need for decoding strategies conditioned on output length.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284673632",
                    "name": "Alex Nguyen"
                },
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2113540861",
                    "name": "Chengyu Dong"
                },
                {
                    "authorId": "2284595153",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "564bffb42edb6a24be8b144f22eec97e0579028b",
            "title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization",
            "abstract": "RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each using three competing reward functions. Our experiments demonstrate that multi-objective methods that directly optimize volume perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284695180",
                    "name": "Yasaman Jafari"
                },
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2284939720",
                    "name": "Rose Yu"
                },
                {
                    "authorId": "2257289790",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "57cbb0578326ed792d03981ca701214844462d22",
            "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification",
            "abstract": "Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2267341626",
                    "name": "Jason Weston"
                },
                {
                    "authorId": "3369052",
                    "name": "Jack Lanchantin"
                },
                {
                    "authorId": "48647153",
                    "name": "Roberta Raileanu"
                },
                {
                    "authorId": "3376175",
                    "name": "M. Lomeli"
                },
                {
                    "authorId": "2284595153",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "2173509991",
                    "name": "Jane Dwivedi-Yu"
                }
            ]
        },
        {
            "paperId": "a3e44cc712a5480189cc6dca3381ff4ea1ec551d",
            "title": "DOCMASTER: A Unified Platform for Annotation, Training, & Inference in Document Question-Answering",
            "abstract": "The application of natural language processing models to PDF documents is pivotal for various business applications yet the challenge of training models for this purpose persists in businesses due to specific hurdles. These include the complexity of working with PDF formats that necessitate parsing text and layout information for curating training data and the lack of privacy-preserving annotation tools. This paper introduces DOCMASTER, a unified platform designed for annotating PDF documents, model training, and inference, tailored to document question-answering. The annotation interface enables users to input questions and highlight text spans within the PDF file as answers, saving layout information and text spans accordingly. Furthermore, DOCMASTER supports both state-of-the-art layout-aware and text models for comprehensive training purposes. Importantly, as annotations, training, and inference occur on-device, it also safeguards privacy. The platform has been instrumental in driving several research prototypes concerning document analysis such as the AI assistant utilized by University of California San Diego\u2019s (UCSD) International Services and Engagement Office (ISEO) for processing a substantial volume of PDF documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284673632",
                    "name": "Alex Nguyen"
                },
                {
                    "authorId": "1762478",
                    "name": "Zilong Wang"
                },
                {
                    "authorId": "2254284383",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                }
            ]
        },
        {
            "paperId": "d244b2ba0fd5f573d19df0c7b362692c018748a1",
            "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
            "abstract": "Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics&humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2284673632",
                    "name": "Alex Nguyen"
                },
                {
                    "authorId": "2284595153",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "4d295abfdaee6a34dd9004724e0381c3d1dbb4a7",
            "title": "SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank",
            "abstract": "Deep neural classifiers trained with cross-entropy loss (CE loss) often suffer from poor calibration, necessitating the task of out-of-distribution (OOD) detection. Traditional supervised OOD detection methods require expensive manual annotation of in-distribution and OOD samples. To address the annotation bottleneck, we introduce SELFOOD, a self-supervised OOD detection method that requires only in-distribution samples as supervision. We cast OOD detection as an inter-document intra-label (IDIL) ranking problem and train the classifier with our pairwise ranking loss, referred to as IDIL loss. Specifically, given a set of in-distribution documents and their labels, for each label, we train the classifier to rank the softmax scores of documents belonging to that label to be higher than the scores of documents that belong to other labels. Unlike CE loss, our IDIL loss function reaches zero when the desired confidence ranking is achieved and gradients are backpropagated to decrease probabilities associated with incorrect labels rather than continuously increasing the probability of the correct label. Extensive experiments with several classifiers on multiple classification datasets demonstrate the effectiveness of our method in both coarse- and fine-grained settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "2113540861",
                    "name": "Chengyu Dong"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "76e287f89bcfa45389293c007e86c46f878a7f9e",
            "title": "DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase",
            "abstract": "In-Context Learning (ICL) combined with pre-trained large language models has achieved promising results on various NLP tasks. However, ICL requires high-quality annotated demonstrations which might not be available in real-world scenarios. To overcome this limitation, we propose \\textbf{D}ata \\textbf{A}ugmentation for \\textbf{I}n-Context \\textbf{L}earning (\\textbf{DAIL}). DAIL leverages the intuition that large language models are more familiar with the content generated by themselves. It first utilizes the language model to generate paraphrases of the test sample and employs majority voting to determine the final result based on individual predictions. Our extensive empirical evaluation shows that DAIL outperforms the standard ICL method and other ensemble-based methods in the low-resource scenario. Additionally, we explore the use of voting consistency as a confidence score of the model when the logits of predictions are inaccessible. We believe our work will stimulate further research on ICL in low-resource settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161635474",
                    "name": "Dawei Li"
                },
                {
                    "authorId": "2265594725",
                    "name": "Yaxuan Li"
                },
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2265619790",
                    "name": "Shuyao Li"
                },
                {
                    "authorId": "2265620708",
                    "name": "Yulin wang"
                },
                {
                    "authorId": "2265608993",
                    "name": "Xueqi Wang"
                },
                {
                    "authorId": "2265581759",
                    "name": "William Hogan"
                },
                {
                    "authorId": "2257005005",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "927de483f345652d777ead90182b6a284e91c8d0",
            "title": "A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches",
            "abstract": "Etremely Weakly Supervised Text Classification (XWS-TC) refers to text classification based on minimal high-level human guidance, such as a few label-indicative seed words or classification instructions. There are two mainstream approaches for XWS-TC, however, never being rigorously compared: (1) training classifiers based on pseudo-labels generated by (softly) matching seed words (SEED) and (2) prompting (and calibrating) language models using classification instruction (and raw texts) to decode label words (PROMPT). This paper presents the first XWS-TC benchmark to compare the two approaches on fair grounds, where the datasets, supervisions, and hyperparameter choices are standardized across methods. Our benchmarking results suggest that (1) Both SEED and PROMPT approaches are competitive and there is no clear winner; (2) SEED is empirically more tolerant than PROMPT to human guidance (e.g., seed words, classification instructions, and label words) changes; (3) SEED is empirically more selective than PROMPT to the pre-trained language models; (4) Recent SEED and PROMPT methods have close connections and a clustering post-processing step based on raw in-domain texts is a strong performance booster to both. We hope this benchmark serves as a guideline in selecting XWS-TC methods in different scenarios and stimulate interest in developing guidance- and model-robust XWS-TC methods. We release the repo at https://github.com/ZihanWangKi/x-TC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240689",
                    "name": "Zihan Wang"
                },
                {
                    "authorId": "2155412834",
                    "name": "Tianle Wang"
                },
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "1df431133711c5e2ba65ef790ebb588b105f2dc3",
            "title": "Intermediate Training on Question Answering Datasets Improves Generative Data Augmentation",
            "abstract": "Manually annotating datasets requires domain experts to read through many documents and carefully label them, which is often expensive. Recently, pre-trained generative language models (GLMs) have demonstrated exceptional abilities in generating text which motivates to leverage them for generative data augmentation. We improve generative data augmentation by formulating the data generation as context generation task and use question answering (QA) datasets for intermediate training. Speci\ufb01cally, we view QA to be more as a format than of a task and train GLMs as context generators for a given question and its respective answer. Then, we cast down-stream tasks into question answering format and adapt the \ufb01ne-tuned context generators to the target task domain. Finally, we use the \ufb01ne-tuned GLM to generate relevant contexts, which is further used as synthetic training data for their corresponding tasks. We perform extensive experiments, case studies, and ablation studies on multiple sentiment and topic clas-si\ufb01cation datasets and demonstrate substantial improvements in performance in few-shot, zero-shot settings. Remarkably, on the SST-2 dataset, intermediate training on SocialIQA dataset achieves an improvement of 40% on Macro-F1 score. Through thorough analyses, we observe that QA datasets that requires high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "144244743",
                    "name": "Tu Vu"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "889045714af6c2e9c187dd5347f1b308447edcc9",
            "title": "Leveraging QA Datasets to Improve Generative Data Augmentation",
            "abstract": "The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLM\u2019s ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "144244743",
                    "name": "Tu Vu"
                },
                {
                    "authorId": "32246932",
                    "name": "Timo Schick"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        }
    ]
}