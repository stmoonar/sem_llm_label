{
    "authorId": "51180669",
    "papers": [
        {
            "paperId": "5d80cf68ed66a0769d3ec2f2550d2fa95d9044c8",
            "title": "Neurosymbolic AI approach to Attribution in Large Language Models",
            "abstract": "Attribution in large language models (LLMs) remains a significant challenge, particularly in ensuring the factual accuracy and reliability of the generated outputs. Current methods for citation or attribution, such as those employed by tools like Perplexity.ai and Bing Search-integrated LLMs, attempt to ground responses by providing real-time search results and citations. However, so far, these approaches suffer from issues such as hallucinations, biases, surface-level relevance matching, and the complexity of managing vast, unfiltered knowledge sources. While tools like Perplexity.ai dynamically integrate web-based information and citations, they often rely on inconsistent sources such as blog posts or unreliable sources, which limits their overall reliability. We present that these challenges can be mitigated by integrating Neurosymbolic AI (NesyAI), which combines the strengths of neural networks with structured symbolic reasoning. NesyAI offers transparent, interpretable, and dynamic reasoning processes, addressing the limitations of current attribution methods by incorporating structured symbolic knowledge with flexible, neural-based learning. This paper explores how NesyAI frameworks can enhance existing attribution models, offering more reliable, interpretable, and adaptable systems for LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2141126407",
                    "name": "Deepa Tilwani"
                },
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "2324782133",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "b2bd75de4f3b5a97c097680c4c015c33914e82e8",
            "title": "AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing Pipelines",
            "abstract": "Anomaly detection in manufacturing pipelines remains a critical challenge, intensified by the complexity and variability of industrial environments. This paper introduces AssemAI, an interpretable image-based anomaly detection system tailored for smart manufacturing pipelines. Our primary contributions include the creation of a tailored image dataset and the development of a custom object detection model, YOLO-FF, designed explicitly for anomaly detection in manufacturing assembly environments. Utilizing the preprocessed image dataset derived from an industry-focused rocket assembly pipeline, we address the challenge of imbalanced image data and demonstrate the importance of image-based methods in anomaly detection. The proposed approach leverages domain knowledge in data preparation, model development and reasoning. We compare our method against several baselines, including simple CNN and custom Visual Transformer (ViT) models, showcasing the effectiveness of our custom data preparation and pretrained CNN integration. Additionally, we incorporate explainability techniques at both user and model levels, utilizing ontology for user-friendly explanations and SCORE-CAM for in-depth feature and model analysis. Finally, the model was also deployed in a real-time setting. Our results include ablation studies on the baselines, providing a comprehensive evaluation of the proposed system. This work highlights the broader impact of advanced image-based anomaly detection in enhancing the reliability and efficiency of smart manufacturing processes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35029095",
                    "name": "Renjith Prasad"
                },
                {
                    "authorId": "2003371371",
                    "name": "Chathurangi Shyalika"
                },
                {
                    "authorId": "2806817",
                    "name": "Ramtin Zand"
                },
                {
                    "authorId": "2210930051",
                    "name": "Fadi El Kalach"
                },
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "48836198",
                    "name": "R. Harik"
                },
                {
                    "authorId": "2307708453",
                    "name": "Amit Sheth"
                }
            ]
        },
        {
            "paperId": "42f3552410e51045c14f9a1834d97880a53bbb58",
            "title": "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes",
            "abstract": "As people become more aware of their food choices, food computation models have become increasingly popular in assisting people in maintaining healthy eating habits. For example, food recommendation systems analyze recipe instructions to assess nutritional contents and provide recipe recommendations. The recent and remarkable successes of generative AI methods, such as auto-regressive Large Language Models, can enable robust methods for a more comprehensive understanding of recipes for healthy food recommendations beyond surface-level nutrition content assessments. In this study, we investigate the use of generative AI methods to extend current food computation models, primarily involving the analysis of nutrition and ingredients, to also incorporate cooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.), Cooking actions are notoriously hard to model using statistical learning methods due to irregular data patterns - significantly varying natural language descriptions for the same action (e.g., marinate the meat vs. marinate the meat and leave overnight) and infrequently occurring patterns (e.g., add salt occurs far more frequently than marinating the meat). The prototypical approach to handling irregular data patterns is to increase the volume of data that the model ingests by orders of magnitude. Unfortunately, in the cooking domain, these problems are further compounded with larger data volumes presenting a unique challenge that is not easily handled by simply scaling up. In this work, we propose novel aggregation-based generative AI methods, Cook-Gen, that reliably generate cooking actions from recipes, despite difficulties with irregular data patterns, while also outperforming Large Language Models and other strong baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2214695852",
                    "name": "Kanak Raj"
                },
                {
                    "authorId": "35029095",
                    "name": "Renjith Prasad"
                },
                {
                    "authorId": "2187301061",
                    "name": "Yuxin Zi"
                },
                {
                    "authorId": "2068096390",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "6b1c1bafe67521f0fddd8ffb73700c88978e0a44",
            "title": "Knowledge Graph Empowered Machine Learning Pipelines for Improved Efficiency, Reusability, and Explainability",
            "abstract": "Artificial intelligence (AI) pipelines are complex, heavily parameterized, and expensive to execute in terms of time and computational resources. Consequently, it is onerous to run experiments with all possible parameter combinations to achieve an optimal solution. However, these AI experiments can be optimized by recommending relevant parameters to commence the experiments, reducing search space significantly, which can be fine tuned further. The relevant parameters can be identified by observing the metadata of pipelines executed in the past, and the relevant pipeline with relevant parameters can be recommended to the user. Currently, there are various metadata frameworks that automatically record the metadata of AI pipelines. Developing a recommendation system requires understanding pipeline metadata components and their interactions. There is a need to represent the metadata generated by these AI pipelines that capture the relationship among these pipeline entities. This article presents a knowledge-infused recommender that utilizes prior knowledge and metadata of already executed pipelines represented using the proposed metadata schema to recommend a relevant pipeline per user queries. Unlike black-box models, the use of knowledge graphs makes recommendations explainable, improving transparency and trustworthiness for the users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "2643562",
                    "name": "Aalap Tripathy"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "35655815",
                    "name": "H. Y. Yip"
                },
                {
                    "authorId": "2205970689",
                    "name": "Annmary Justine"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "76f20cde68a7720079266ab02f3ce66b35716fb6",
            "title": "Ki-Cook: clustering multimodal cooking representations through knowledge-infused learning",
            "abstract": "Cross-modal recipe retrieval has gained prominence due to its ability to retrieve a text representation given an image representation and vice versa. Clustering these recipe representations based on similarity is essential to retrieve relevant information about unknown food images. Existing studies cluster similar recipe representations in the latent space based on class names. Due to inter-class similarity and intraclass variation, associating a recipe with a class name does not provide sufficient knowledge about recipes to determine similarity. However, recipe title, ingredients, and cooking actions provide detailed knowledge about recipes and are a better determinant of similar recipes. In this study, we utilized this additional knowledge of recipes, such as ingredients and recipe title, to identify similar recipes, emphasizing attention especially on rare ingredients. To incorporate this knowledge, we propose a knowledge-infused multimodal cooking representation learning network, Ki-Cook, built on the procedural attribute of the cooking process. To the best of our knowledge, this is the first study to adopt a comprehensive recipe similarity determinant to identify and cluster similar recipe representations. The proposed network also incorporates ingredient images to learn multimodal cooking representation. Since the motivation for clustering similar recipes is to retrieve relevant information for an unknown food image, we evaluated the ingredient retrieval task. We performed an empirical analysis to establish that our proposed model improves the Coverage of Ground Truth by 12% and the Intersection Over Union by 10% compared to the baseline models. On average, the representations learned by our model contain an additional 15.33% of rare ingredients compared to the baseline models. Owing to this difference, our qualitative evaluation shows a 39% improvement in clustering similar recipes in the latent space compared to the baseline models, with an inter-annotator agreement of the Fleiss kappa score of 0.35.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "40977746",
                    "name": "Swati Padhee"
                },
                {
                    "authorId": "2224857413",
                    "name": "Saini Rohan Rao"
                },
                {
                    "authorId": "2042283024",
                    "name": "Ronak Kaoshik"
                },
                {
                    "authorId": "2309490506",
                    "name": "Anirudh Sundara Rajan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "c8af4ae553e106b923369083bd7163ec0d1f9d2b",
            "title": "A Rich Recipe Representation as Plan to Support Expressive Multi Modal Queries on Recipe Content and Preparation Process",
            "abstract": "Food is not only a basic human necessity but also a key factor driving a society's health and economic well-being. As a result, the cooking domain is a popular use-case to demonstrate decision-support (AI) capabilities in service of benefits like precision health with tools ranging from information retrieval interfaces to task-oriented chatbots. An AI here should understand concepts in the food domain (e.g., recipes, ingredients), be tolerant to failures encountered while cooking (e.g., browning of butter), handle allergy-based substitutions, and work with multiple data modalities (e.g. text and images). However, the recipes today are handled as textual documents which makes it difficult for machines to read, reason and handle ambiguity. This demands a need for better representation of the recipes, overcoming the ambiguity and sparseness that exists in the current textual documents. In this paper, we discuss the construction of a machine-understandable rich recipe representation (R3), in the form of plans, from the recipes available in natural language. R3 is infused with additional knowledge such as information about allergens and images of ingredients, possible failures and tips for each atomic cooking step. To show the benefits of R3, we also present TREAT, a tool for recipe retrieval which uses R3 to perform multi-modal reasoning on the recipe's content (plan objects - ingredients and cooking tools), food preparation process (plan actions and time), and media type (image, text). R3 leads to improved retrieval efficiency and new capabilities that were hither-to not possible in textual representation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50847178",
                    "name": "Vishal Pallagani"
                },
                {
                    "authorId": "2160881556",
                    "name": "Priyadharsini Ramamurthy"
                },
                {
                    "authorId": "50840713",
                    "name": "Vedant Khandelwal"
                },
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "2160882985",
                    "name": "Kausik Lakkaraju"
                },
                {
                    "authorId": "24057502",
                    "name": "Sathyanarayanan N. Aakur"
                },
                {
                    "authorId": "50784532",
                    "name": "Biplav Srivastava"
                }
            ]
        },
        {
            "paperId": "51714314ad41d0780f25579981c4c6ebb6ee3bad",
            "title": "kBot: Knowledge-Enabled Personalized Chatbot for Asthma Self-Management",
            "abstract": "There is a well-recognized need for a shift to proactive asthma care given the impact asthma has on overall healthcare costs. The demand for continuous monitoring of patient's adherence to the medication care plan, assessment of environmental triggers, and management of asthma can be challenging in traditional clinical settings and taxing on clinical professionals. Recent years have seen a robust growth of general purpose conversational systems. However, they lack the capabilities to support applications such an individual's health, which requires the ability to contextualize, learn interactively, and provide the proper hyper-personalization needed to hold meaningful conversations. In this paper, we present kBot, a knowledge-enabled personalized chatbot system designed for health applications and adapted to help pediatric asthmatic patients (age 8 to 15) to better control their asthma. Its core functionalities include continuous monitoring of the patient's medication adherence and tracking of relevant health signals and environment data. kBot takes the form of an Android application with a frontend chat interface capable of conversing in both text and voice, and a backend cloud-based server application that handles data collection, processing, and dialogue management. It achieves contextualization by piecing together domain knowledge from online sources and inputs from our clinical partners. The personalization aspect is derived from patient answering questionnaires and day-to-day conversations. kBot's preliminary evaluation focused on chatbot quality, technology acceptance, and system usability involved eight asthma clinicians and eight researchers. For both groups, kBot achieved an overall technology acceptance value of greater than 8 on the 11-point Likert scale and a mean System Usability Score (SUS) greater than 80. A demo of our kBot application is available at this URL: https://bit.ly/kBot\\_demo.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "51977561",
                    "name": "Dipesh Kadariya"
                },
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "35655815",
                    "name": "H. Y. Yip"
                },
                {
                    "authorId": "37508164",
                    "name": "M. Kalra"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                }
            ]
        },
        {
            "paperId": "7d6975d3a5d121324b55e853bb934f353463bc93",
            "title": "Determination of Personalized Asthma Triggers from Evidence based on Multimodal Sensing and Mobile Application",
            "abstract": "Objective: Asthma is a chronic pulmonary disease with multiple triggers manifesting as symptoms with various intensities. This paper evaluates the suitability of long-term monitoring of pediatric asthma using diverse data to qualify and quantify triggers that contribute to the asthma symptoms and control to enable a personalized management plan. \nMaterials and Methods: Asthma condition, environment, and adherence to the prescribed care plan were continuously tracked for 97 pediatric patients using kHealth-Asthma technology for one or three months. \nResult: At the cohort level, among 21% of the patients deployed in spring, 63% and 19% indicated pollen and Particulate Matter (PM2.5), respectively, as the major asthma contributors. Of the 18% of the patients deployed in fall, 29% and 21% found pollen and PM2.5 respectively, to be the contributors. For the 28% of the patients deployed in winter, PM2.5 was identified as the major contributor for 80% of them. One patient across each season has been chosen to explain the determination of personalized triggers by observing correlations between triggers and asthma symptoms gathered from anecdotal evidence. \nDiscussion and Conclusion: Both public and personal health signals including compliance to prescribed care plan have been captured through continuous monitoring using the kHealth-Asthma technology which generated insights on causes of asthma symptoms across different seasons. Collectively, they can form the underlying basis for personalized management plan and intervention. \nKEYWORDS: Personalized Digital Health, Medical Internet of Things, Pediatric Asthma Management, Patient Generated Health Data, Personalized Triggers, Telehealth,",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "51977561",
                    "name": "Dipesh Kadariya"
                },
                {
                    "authorId": "35655815",
                    "name": "H. Y. Yip"
                },
                {
                    "authorId": "47247243",
                    "name": "Utkarshani Jaimini"
                },
                {
                    "authorId": "3195814",
                    "name": "K. Thirunarayan"
                },
                {
                    "authorId": "37508164",
                    "name": "M. Kalra"
                },
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                }
            ]
        }
    ]
}