{
    "authorId": "1746959",
    "papers": [
        {
            "paperId": "1784fdf3290c1ff86cf04218aaeee5f8fb19fb9f",
            "title": "The 12th International Workshop on Natural Language Processing for Social Media (SocialNLP 2024)",
            "abstract": "The field of SocialNLP stands at the confluence of two pivotal domains: natural language processing (NLP) and social computing, offering a multidisciplinary framework for exploring the intersections between these areas. This domain is characterized by its tri-directional focus. First, it endeavors to tackle prevalent challenges within the realm of social computing by leveraging advanced NLP methodologies. Second, it aims to address traditional NLP problems by harnessing the vast and dynamic data available from social networks and social media platforms. Third, SocialNLP is dedicated to identifying and solving emerging issues that lie at the nexus of social computing and NLP. The 12th iteration of the SocialNLP workshop, a hallmark event in this vibrant field, is scheduled to convene at TheWebConf (WWW) 2024. This edition has witnessed the acceptance of nine rigorously peer-reviewed papers, chosen from a competitive pool, reflecting an acceptance ratio of 53.8%. This achievement underscores the high caliber of research and the innovative approaches that the submitted works propose to advance the field of SocialNLP. The organizing committee extends its heartfelt gratitude to all contributing authors for their submissions, to the members of the program committee for their indispensable role in the rigorous review process, and to the workshop chairs for their visionary leadership and dedication. Their collective efforts have been instrumental in fostering an environment of academic excellence and collaboration that continues to drive forward the frontiers of knowledge in the interdisciplinary area of SocialNLP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2277810141",
                    "name": "Cheng-Te Li"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                },
                {
                    "authorId": "1896151979",
                    "name": "Yu-Che Tsai"
                }
            ]
        },
        {
            "paperId": "7e760ed4ed69be02f7cca3770bbf4df439242c60",
            "title": "SocialNLP Fake-EmoReact 2021 Challenge Overview: Predicting Fake Tweets from Their Replies and GIFs",
            "abstract": "This paper provides an overview of the Fake-EmoReact 2021 Challenge, held at the 9th SocialNLP Workshop, in conjunction with NAACL 2021. The challenge requires predicting the authenticity of tweets using reply context and augmented GIF categories from EmotionGIF dataset. We offer the Fake-EmoReact dataset with more than 453k as the experimental materials, where every tweet is labeled with authenticity. Twenty-four teams registered to participate in this challenge, and 5 submitted their results successfully in the evaluation phase. The best team achieves 93.9 on Fake-EmoReact 2021 dataset using F1 score. In addition, we show the definition of share task, data collection, and the teams' performance that joined this challenge and their approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118805165",
                    "name": "Chien-Kun Huang"
                },
                {
                    "authorId": "2185273",
                    "name": "Yi-Ting Chang"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                },
                {
                    "authorId": "2305597403",
                    "name": "Cheng-Te Li"
                },
                {
                    "authorId": "2426757",
                    "name": "Hong-Han Shuai"
                }
            ]
        },
        {
            "paperId": "c31ff2763f212906965676614d74254ae8835e19",
            "title": "Enhancing Perception: Refining Explanations of News Claims with LLM Conversations",
            "abstract": "We introduce E NHANCING P ERCEPTION , a framework for Large Language Models (LLMs) designed to streamline the time-intensive task typically undertaken by professional fact-checkers of crafting explanations for fake news. This study investigates the effectiveness of enhancing LLM explanations through conversational refinement. We compare various questioner agents, including state-of-the-art LLMs like GPT-4, Claude 2, PaLM 2, and 193 American participants acting as human questioners. Based on the histories of these refinement conversations, we further generate comprehensive summary explanations. We evaluated the effectiveness of these initial, refined, and summary explanations across 40 news claims by involving 2,797 American participants, measuring their self-reported belief change regarding both real and fake claims after receiving the explanations. Our findings reveal that, in the context of fake news, explanations that have undergone conversational refinement\u2014whether by GPT-4 or human questioners, who ask more diverse and detail-oriented questions\u2014were significantly more effective than both the initial unrefined explanations and the summary explanations. Moreover, these refined explanations achieved a level of effectiveness comparable to that of expert-written explanations. The results highlight the potential of automatic explanation refinement by LLMs in debunking fake news claims.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2069602444",
                    "name": "Yi-Li Hsu"
                },
                {
                    "authorId": "2311825125",
                    "name": "Jui-Ning Chen"
                },
                {
                    "authorId": "2311819753",
                    "name": "Shang-Chien Liu"
                },
                {
                    "authorId": "2311877954",
                    "name": "Chiang Fan Yang"
                },
                {
                    "authorId": "2261362789",
                    "name": "Aiping Xiong"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                }
            ]
        },
        {
            "paperId": "5f63f05eaabf2b7d815858dd5814257568407de0",
            "title": "Label-Aware Hyperbolic Embeddings for Fine-grained Emotion Classification",
            "abstract": "Fine-grained emotion classification (FEC) is a challenging task. Specifically, FEC needs to handle subtle nuance between labels, which can be complex and confusing. Most existing models only address text classification problem in the euclidean space, which we believe may not be the optimal solution as labels of close semantic (e.g., afraid and terrified) may not be differentiated in such space, which harms the performance. In this paper, we propose HypEmo, a novel framework that can integrate hyperbolic embeddings to improve the FEC task. First, we learn label embeddings in the hyperbolic space to better capture their hierarchical structure, and then our model projects contextualized representations to the hyperbolic space to compute the distance between samples and labels. Experimental results show that incorporating such distance to weight cross entropy loss substantially improve the performance on two benchmark datasets, with around 3% improvement compared to previous state-of-the-art, and could even improve up to 8.6% when the labels are hard to distinguish. Code is available at https://github.com/dinobby/HypEmo.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109675427",
                    "name": "Chih-Yao Chen"
                },
                {
                    "authorId": "2122375061",
                    "name": "Tun-Min Hung"
                },
                {
                    "authorId": "2069602444",
                    "name": "Yi-Li Hsu"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                }
            ]
        },
        {
            "paperId": "62e0a6b33643802542db2c46b582ec1e8e5a8111",
            "title": "HonestBait: Forward References for Attractive but Faithful Headline Generation",
            "abstract": "Current methods for generating attractive headlines often learn directly from data, which bases attractiveness on the number of user clicks and views. Although clicks or views do reflect user interest, they can fail to reveal how much interest is raised by the writing style and how much is due to the event or topic itself. Also, such approaches can lead to harmful inventions by over-exaggerating the content, aggravating the spread of false information. In this work, we propose HonestBait, a novel framework for solving these issues from another aspect: generating headlines using forward references (FRs), a writing technique often used for clickbait. A self-verification process is included during training to avoid spurious inventions. We begin with a preliminary user study to understand how FRs affect user interest, after which we present PANCO1, an innovative dataset containing pairs of fake news with verified news for attractive but faithful news headline generation. Automatic metrics and human evaluations show that our framework yields more attractive results (+11.25% compared to human-written verified news headlines) while maintaining high veracity, which helps promote real information to fight against fake news.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109675427",
                    "name": "Chih-Yao Chen"
                },
                {
                    "authorId": "2220702148",
                    "name": "Dennis Wu"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                }
            ]
        },
        {
            "paperId": "9df29853b62c4c141ad13dfc47d9de4adaa3ee72",
            "title": "Location-Aware Visual Question Generation with Lightweight Models",
            "abstract": "This work introduces a novel task, location-aware visual question generation (LocaVQG), which aims to generate engaging questions from data relevant to a particular geographical location. Specifically, we represent such location-aware information with surrounding images and a GPS coordinate. To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions. Then, we aim to learn a lightweight model that can address the LocaVQG task and fit on an edge device, such as a mobile phone. To this end, we propose a method which can reliably generate engaging questions from location-aware information. Our proposed method outperforms baselines regarding human evaluation (e.g., engagement, grounding, coherence) and automatic evaluation metrics (e.g., BERTScore, ROUGE-2). Moreover, we conduct extensive ablation studies to justify our proposed techniques for both generating the dataset and solving the task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261363681",
                    "name": "Nicholas Collin Suwono"
                },
                {
                    "authorId": "2261393356",
                    "name": "Justin Chih-Yao Chen"
                },
                {
                    "authorId": "2122375061",
                    "name": "Tun-Min Hung"
                },
                {
                    "authorId": "2261394416",
                    "name": "T. Huang"
                },
                {
                    "authorId": "2261363891",
                    "name": "I-Bin Liao"
                },
                {
                    "authorId": "2261371045",
                    "name": "Yung-Hui Li"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                },
                {
                    "authorId": "2261364774",
                    "name": "Shao-Hua Sun"
                }
            ]
        },
        {
            "paperId": "c0617b8aadb1cdce4ba0a0b51014b8275f8b17c4",
            "title": "SocialNLP\u201923: 11th International Workshop on Natural Language Processing for Social Media",
            "abstract": "SocialNLP is an inter-disciplinary area of natural language processing (NLP) and social computing. SocialNLP has three directions: (1) addressing issues in social computing using NLP techniques; (2) solving NLP problems using information from social networks or social media; and (3) handling new problems related to both social computing and natural language processing. The 11th SocialNLP workshop is held at TheWebConf 2023. We accepted nine papers with acceptance ratio 56%. We sincerely thank to all authors, program committee members, and workshop chairs, for their great contributions and help in this edition of SocialNLP workshop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2169355",
                    "name": "Cheng-te Li"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                }
            ]
        },
        {
            "paperId": "e1770838ec0667cad48729a81764ed9964d6a8e6",
            "title": "LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis",
            "abstract": "Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA's labor and time demands.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3400291",
                    "name": "Shih-Chieh Dai"
                },
                {
                    "authorId": "2261362789",
                    "name": "Aiping Xiong"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                }
            ]
        },
        {
            "paperId": "ff4113bcddab4470fff70f06fb0e3860dddab6ac",
            "title": "Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term",
            "abstract": "With advancements in natural language processing (NLP) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. While many researchers have focused on generating good explanations, how these explanations can really help humans combat fake news is under-explored. In this study, we compare the effectiveness of a warning label and the state-of-the-art counterfactual explanations generated by GPT-4 in debunking misinformation. In a two-wave, online human-subject study, participants (N = 215) were randomly assigned to a control group in which false contents are shown without any intervention, a warning tag group in which the false claims were labeled, or an explanation group in which the false contents were accompanied by GPT-4 generated explanations. Our results show that both interventions significantly decrease participants' self-reported belief in fake claims in an equivalent manner for the short-term and long-term. We discuss the implications of our findings and directions for future NLP-based misinformation debunking strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2069602444",
                    "name": "Yi-Li Hsu"
                },
                {
                    "authorId": "3400291",
                    "name": "Shih-Chieh Dai"
                },
                {
                    "authorId": "2261362789",
                    "name": "Aiping Xiong"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                }
            ]
        },
        {
            "paperId": "2e2147ca34e726f568fb28089424d2d97cb8b7f5",
            "title": "Learning to Rank Visual Stories From Human Ranking Data",
            "abstract": "Visual storytelling (VIST) is a typical vision and language task that has seen extensive development in the natural language generation research domain. However, it remains unclear whether conventional automatic evaluation metrics for text generation are applicable on VIST. In this paper, we present the VHED (VIST Human Evaluation Data) dataset, which first re-purposes human evaluation results for automatic evaluation; hence we develop Vrank (VIST Ranker), a novel reference-free VIST metric for story evaluation. We first show that the results from commonly adopted automatic metrics for text generation have little correlation with those obtained from human evaluation, which motivates us to directly utilize human evaluation results to learn the automatic evaluation model. In the experiments, we evaluate the generated texts to predict story ranks using our model as well as other reference-based and reference-free metrics. Results show that Vrank prediction is significantly more aligned to human evaluation than other metrics with almost 30% higher accuracy when ranking story pairs. Moreover, we demonstrate that only Vrank shows human-like behavior in its strong ability to find better stories when the quality gap between two stories is high. Finally, we show the superiority of Vrank by its generalizability to pure textual stories, and conclude that this reuse of human evaluation results puts Vrank in a strong position for continued future advances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48162772",
                    "name": "Chi-Yang Hsu"
                },
                {
                    "authorId": "9628638",
                    "name": "Yun-Wei Chu"
                },
                {
                    "authorId": "2165225005",
                    "name": "Vincent Chen"
                },
                {
                    "authorId": "2051973162",
                    "name": "Kuan-Chieh Lo"
                },
                {
                    "authorId": "2131059444",
                    "name": "Chacha Chen"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                }
            ]
        }
    ]
}