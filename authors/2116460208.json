{
    "authorId": "2116460208",
    "papers": [
        {
            "paperId": "0b18bb8858a6e437b007cadea507a09946061383",
            "title": "Multimodal Graph Neural Architecture Search under Distribution Shifts",
            "abstract": "Multimodal graph neural architecture search (MGNAS) has shown great success for automatically designing the optimal multimodal graph neural network (MGNN) architecture by leveraging multimodal representation, crossmodal information and graph structure in one unified framework. However, existing MGNAS fails to handle distribution shifts that naturally exist in multimodal graph data, since the searched architectures inevitably capture spurious statistical correlations under distribution shifts. To solve this problem, we propose a novel Out-of-distribution Generalized Multimodal Graph Neural Architecture Search (OMG-NAS) method which optimizes the MGNN architecture with respect to its performance on decorrelated OOD data. Specifically, we propose a multimodal graph representation decorrelation strategy, which encourages the searched MGNN model to output representations that eliminate spurious correlations through iteratively optimizing the feature weights and controller. In addition, we propose a global sample weight estimator that facilitates the sharing of optimal sample weights learned from existing architectures. This design promotes the effective estimation of the sample weights for candidate MGNN architectures to generate decorrelated multimodal graph representations, concentrating more on the truly predictive relations between invariant features and ground-truth labels. Extensive experiments on real-world multimodal graph datasets demonstrate the superiority of our proposed method over SOTA baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293898555",
                    "name": "Jie Cai"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2145538097",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "1e80802a2fef00f2e10dfd4ab0ecce18ee2af82b",
            "title": "Exploring the Potential of Large Language Models in Graph Generation",
            "abstract": "Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification. However, the abilities of LLMs for graph generation remain unexplored in the literature. Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation. Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation. We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance. Besides, LLMs show potential in generating molecules with specific properties. These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2186253494",
                    "name": "Yang Yao"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2118690469",
                    "name": "Zeyang Zhang"
                },
                {
                    "authorId": "2112481185",
                    "name": "Yi Qin"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "2292399289",
                    "name": "Xu Chu"
                },
                {
                    "authorId": "2292430960",
                    "name": "Yuekui Yang"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                },
                {
                    "authorId": "2292410833",
                    "name": "Hong Mei"
                }
            ]
        },
        {
            "paperId": "35c07f14c5edd6dc87d0ad9ab9849bb6ab6055aa",
            "title": "Unsupervised Graph Neural Architecture Search with Disentangled Self-Supervision",
            "abstract": "The existing graph neural architecture search (GNAS) methods heavily rely on supervised labels during the search process, failing to handle ubiquitous scenarios where supervisions are not available. In this paper, we study the problem of unsupervised graph neural architecture search, which remains unexplored in the literature. The key problem is to discover the latent graph factors that drive the formation of graph data as well as the underlying relations between the factors and the optimal neural architectures. Handling this problem is challenging given that the latent graph factors together with architectures are highly entangled due to the nature of the graph and the complexity of the neural architecture search process. To address the challenge, we propose a novel Disentangled Self-supervised Graph Neural Architecture Search (DSGAS) model, which is able to discover the optimal architectures capturing various latent graph factors in a self-supervised fashion based on unlabeled graph data. Specifically, we first design a disentangled graph super-network capable of incorporating multiple architectures with factor-wise disentanglement, which are optimized simultaneously. Then, we estimate the performance of architectures under different factors by our proposed self-supervised training with joint architecture-graph disentanglement. Finally, we propose a contrastive search with architecture augmentations to discover architectures with factor-specific expertise. Extensive experiments on 11 real-world datasets demonstrate that the proposed model is able to achieve state-of-the-art performance against several baseline methods in an unsupervised manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118690469",
                    "name": "Zeyang Zhang"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "2279330465",
                    "name": "Shiqi Shen"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "7448e270ccfab5a2b4838129f78136731c496ab6",
            "title": "Data-Augmented Curriculum Graph Neural Architecture Search under Distribution Shifts",
            "abstract": "Graph neural architecture search (NAS) has achieved great success in designing architectures for graph data processing.However, distribution shifts pose great challenges for graph NAS, since the optimal searched architectures for the training graph data may fail to generalize to the unseen test graph data. The sole prior work tackles this problem by customizing architectures for each graph instance through learning graph structural information, but failed to consider data augmentation during training, which has been proven by existing works to be able to improve generalization.In this paper, we propose Data-augmented Curriculum Graph Neural Architecture Search (DCGAS), which learns an architecture customizer with good generalizability to data under distribution shifts. Specifically, we design an embedding-guided data generator, which can generate sufficient graphs for training to help the model better capture graph structural information. In addition, we design a two-factor uncertainty-based curriculum weighting strategy, which can evaluate the importance of data in enabling the model to learn key information in real-world distribution and reweight them during training. Experimental results on synthetic datasets and real datasets with distribution shifts demonstrate that our proposed method learns generalizable mappings and outperforms existing methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186253494",
                    "name": "Yang Yao"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2112481185",
                    "name": "Yi Qin"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                },
                {
                    "authorId": "2292410833",
                    "name": "Hong Mei"
                }
            ]
        },
        {
            "paperId": "7ede5d34d3e418c102fae9677554cdefd56497aa",
            "title": "Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification",
            "abstract": "Graph Neural Architecture Search (GNAS) has achieved superior performance on various graph-structured tasks. However, existing GNAS studies overlook the applications of GNAS in resource-constraint scenarios. This paper proposes to design a joint graph data and architecture mechanism, which identifies important sub-architectures via the valuable graph data. To search for optimal lightweight Graph Neural Networks (GNNs), we propose a Lightweight Graph Neural Architecture Search with Graph SparsIfication and Network Pruning (GASSIP) method. In particular, GASSIP comprises an operation-pruned architecture search module to enable efficient lightweight GNN search. Meanwhile, we design a novel curriculum graph data sparsification module with an architecture-aware edge-removing difficulty measurement to help select optimal sub-architectures. With the aid of two differentiable masks, we iteratively optimize these two modules to efficiently search for the optimal lightweight architecture. Extensive experiments on five benchmarks demonstrate the effectiveness of GASSIP. Particularly, our method achieves on-par or even higher node classification performance with half or fewer model parameters of searched GNNs and a sparser graph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181504062",
                    "name": "Beini Xie"
                },
                {
                    "authorId": "2140312584",
                    "name": "Heng Chang"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "2118690469",
                    "name": "Zeyang Zhang"
                },
                {
                    "authorId": "2308044035",
                    "name": "Simin Wu"
                },
                {
                    "authorId": "2298094088",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2260668917",
                    "name": "Yuan Meng"
                },
                {
                    "authorId": "2277687596",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "854b58d5126fd536e9e512069452370641e346a2",
            "title": "Causal-Aware Graph Neural Architecture Search under Distribution Shifts",
            "abstract": "Graph NAS has emerged as a promising approach for autonomously designing GNN architectures by leveraging the correlations between graphs and architectures. Existing methods fail to generalize under distribution shifts that are ubiquitous in real-world graph scenarios, mainly because the graph-architecture correlations they exploit might be spurious and varying across distributions. We propose to handle the distribution shifts in the graph architecture search process by discovering and exploiting the causal relationship between graphs and architectures to search for the optimal architectures that can generalize under distribution shifts. The problem remains unexplored with following challenges: how to discover the causal graph-architecture relationship that has stable predictive abilities across distributions, and how to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures. To address these challenges, we propose Causal-aware Graph Neural Architecture Search (CARNAS), which is able to capture the causal graph-architecture relationship during the architecture search process and discover the generalized graph architecture under distribution shifts. Specifically, we propose Disentangled Causal Subgraph Identification to capture the causal subgraphs that have stable prediction abilities across distributions. Then, we propose Graph Embedding Intervention to intervene on causal subgraphs within the latent space, ensuring that these subgraphs encapsulate essential features for prediction while excluding non-causal elements. Additionally, we propose Invariant Architecture Customization to reinforce the causal invariant nature of the causal subgraphs, which are utilized to tailor generalized graph architectures. Extensive experiments demonstrate that CARNAS achieves advanced out-of-distribution generalization ability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260834123",
                    "name": "Peiwen Li"
                },
                {
                    "authorId": "2298094088",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2118690469",
                    "name": "Zeyang Zhang"
                },
                {
                    "authorId": "2112481185",
                    "name": "Yi Qin"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "2249121211",
                    "name": "Jialong Wang"
                },
                {
                    "authorId": "2298224490",
                    "name": "Yang Li"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "9a0e9ee432acf902c297916622778f1e3cd18261",
            "title": "OOD-GNN: Out-of-Distribution Generalized Graph Neural Network: (Extended Abstract)",
            "abstract": "Graph neural networks (GNNs) have achieved impressive performance when testing and training graph data come from identical distribution. However, existing GNNs lack out-of-distribution generalization abilities so that their performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this problem, we propose an out-of-distribution generalized graph neural network (OOD-GNN) for achieving satisfactory performance on unseen testing graphs that have different distributions with training graphs. OOD-GNN employs a novel nonlinear graph representation decorrelation method utilizing random Fourier features, which encourages the model to eliminate the statistical dependence between relevant and irrelevant graph representations through iteratively optimizing the sample graph weights and graph encoder. The learned weights help the graph encoder to get rid of spurious correlations and, in turn, concentrate more on the true connection between learned discriminative graph representations and their ground-truth labels. We conduct extensive experiments to validate the out-of-distribution generalization abilities on two synthetic and 12 real-world datasets with distribution shifts. The results demonstrate that our proposed OOD-GNN significantly outperforms state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145538097",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "b2e2e808181be431bcfeef57bd30cdf93dadfd30",
            "title": "Customized Cross-device Neural Architecture Search with Images",
            "abstract": "Cross-device scenarios have become increasingly common, where non-independently and identically distributed (non-IID) data is generated and stored in different devices. However, the existing cross-device NAS methods only search for a fixed architecture for different devices, neglecting that different devices have varying hardware characteristics and data distributions. In this paper, we propose a novel NAS framework that can customize the most suitable architecture for each device and its associated dataset. Specifically, we propose a decoupled data feature extractor and a device feature extractor to characterize the complex distributions of the different datasets and diverse hardware features. Then, we propose a prototype matcher to customize the operators and shape selection parameters of architectures. Experiments on ImageNet and CIFAR-10 show that our method can discover more efficient and effective architectures in cross-device scenarios than the existing approaches. To the best of our knowledge, this is the first exploration on customized cross-device NAS problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186253494",
                    "name": "Yang Yao"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2112481185",
                    "name": "Yi Qin"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                },
                {
                    "authorId": "2292410833",
                    "name": "Hong Mei"
                }
            ]
        },
        {
            "paperId": "b9d5e237ca163c719aed189d3fc8efc3748bc509",
            "title": "Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts",
            "abstract": "Dynamic graph neural networks (DyGNNs) currently struggle with handling distribution shifts that are inherent in dynamic graphs. Existing work on DyGNNs with out-of-distribution settings only focuses on the time domain, failing to handle cases involving distribution shifts in the spectral domain. In this paper, we discover that there exist cases with distribution shifts unobservable in the time domain while observable in the spectral domain, and propose to study distribution shifts on dynamic graphs in the spectral domain for the first time. However, this investigation poses two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain; and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns. To address these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD), which can handle distribution shifts on dynamic graphs by capturing and utilizing invariant and variant spectral patterns. Specifically, we first design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums, allowing the mixed dynamic graph patterns to be transformed into separate frequency components. We then develop a disentangled spectrum mask to filter graph dynamics from various frequency components and discover the invariant and variant spectral patterns. Finally, we propose invariant spectral filtering, which encourages the model to rely on invariant patterns for generalization under distribution shifts. Experimental results on synthetic and real-world dynamic graph datasets demonstrate the superiority of our method for both node classification and link prediction tasks under distribution shifts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118690469",
                    "name": "Zeyang Zhang"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "2273698217",
                    "name": "Zhou Qin"
                },
                {
                    "authorId": "2070076938",
                    "name": "Weigao Wen"
                },
                {
                    "authorId": "2273682025",
                    "name": "Hui Xue"
                },
                {
                    "authorId": "2145538097",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "0e87f4c721c2a5302e9cf7e2b3a6ceacfaceb469",
            "title": "LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?",
            "abstract": "In an era marked by the increasing adoption of Large Language Models (LLMs) for various tasks, there is a growing focus on exploring LLMs\u2019 capabilities in handling web data, particularly graph data. Dynamic graphs, which capture temporal network evolution patterns, are ubiquitous in real-world web data. Evaluating LLMs\u2019 competence in understanding spatial-temporal information on dynamic graphs is essential for their adoption in web applications, which remains unexplored in the literature. In this paper, we bridge the gap via proposing to evaluate LLMs\u2019 spatial-temporal understanding abilities on dynamic graphs, to the best of our knowledge, for the first time. Specifically, we propose the LLM4DyG benchmark, which includes nine specially designed tasks considering the capability evaluation of LLMs from both temporal and spatial dimensions. Then, we conduct extensive experiments to analyze the impacts of different data generators, data statistics, prompting techniques, and LLMs on the model performance. Finally, we propose Disentangled Spatial-Temporal Thoughts ( DST2 ) for LLMs on dynamic graphs to enhance LLMs\u2019 spatial-temporal understanding abilities. Our main observations are: 1) LLMs have prelim-inary spatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph tasks show increasing difficulties for LLMs as the graph size and density increase, while not sensitive to the time span and data generation mechanism, 3) the proposed DST2 prompting method can help to improve LLMs\u2019 spatial-temporal understanding abilities on dynamic graphs for most tasks. The data and codes will be open-sourced at publication time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118690469",
                    "name": "Zeyang Zhang"
                },
                {
                    "authorId": "2298094088",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "2145538097",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2112481185",
                    "name": "Yi Qin"
                },
                {
                    "authorId": "2308044035",
                    "name": "Simin Wu"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        }
    ]
}