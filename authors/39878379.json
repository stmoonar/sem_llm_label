{
    "authorId": "39878379",
    "papers": [
        {
            "paperId": "144a6584af37eb750e637a96fd80668cbe0c4a44",
            "title": "Self-Cleaning: Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances",
            "abstract": "To achieve state-of-the-art performance, one 001 still needs to train NER models on large-scale, 002 high-quality annotated data, an asset that is 003 both costly and time-intensive to accumulate. 004 In contrast, real-world applications often resort 005 to massive low-quality labeled data through 006 non-expert annotators via crowdsourcing and 007 external knowledge bases via distant supervi-008 sion as a cost-effective alternative. However, 009 these annotation methods result in noisy labels, 010 which in turn lead to a notable decline in per-011 formance. Hence, we propose to denoise the 012 noisy NER data with guidance from a small set 013 of clean instances. Along with the main NER 014 model we train a discriminator model and use 015 its outputs to recalibrate the sample weights. 016 The discriminator is capable of detecting both 017 span and category errors with different discrim-018 inative prompts. Results on public crowdsourc-019 ing and distant supervision datasets show that 020 the proposed method can consistently improve 021 performance with a small guidance set. 022",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261494597",
                    "name": "Zhendong Chu"
                },
                {
                    "authorId": "2283147661",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2307905181",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2313548885",
                    "name": "Vlad Morariu"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                }
            ]
        },
        {
            "paperId": "49e9352f4b87efdbf09a5f3048d11cf83016d37d",
            "title": "Chain of Logic: Rule-Based Reasoning with Large Language Models",
            "abstract": "Rule-based reasoning, a fundamental type of legal reasoning, enables us to draw conclusions by accurately applying a rule to a set of facts. We explore causal language models as rule-based reasoners, specifically with respect to compositional rules - rules consisting of multiple elements which form a complex logical expression. Reasoning about compositional rules is challenging because it requires multiple reasoning steps, and attending to the logical relationships between elements. We introduce a new prompting method, Chain of Logic, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression). This method was inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers. We evaluate chain of logic across eight rule-based reasoning tasks involving three distinct compositional rules from the LegalBench benchmark and demonstrate it consistently outperforms other prompting methods, including chain of thought and self-ask, using open-source and commercial language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121292636",
                    "name": "Sergio Servantez"
                },
                {
                    "authorId": "2284591479",
                    "name": "Joe Barrow"
                },
                {
                    "authorId": "2238213596",
                    "name": "Kristian Hammond"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "846768c90137ecbf08ab6630bb894bb349096bc3",
            "title": "DocSynthv2: A Practical Autoregressive Modeling for Document Generation",
            "abstract": "While the generation of document layouts has been extensively explored, comprehensive document generation encompassing both layout and content presents a more complex challenge. This paper delves into this advanced domain, proposing a novel approach called DocSynthv2 through the development of a simple yet effective autoregressive structured model. Our model, distinct in its integration of both layout and textual cues, marks a step beyond existing layout-generation approaches. By focusing on the relationship between the structural elements and the textual content within documents, we aim to generate cohesive and contextually relevant documents without any reliance on visual components. Through experimental studies on our curated benchmark for the new task, we demonstrate the ability of our model combining layout and textual information in enhancing the generation quality and relevance of documents, opening new pathways for research in document creation and automated design. Our findings emphasize the effectiveness of autoregressive models in handling complex document generation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150473007",
                    "name": "Sanket Biswas"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                },
                {
                    "authorId": "2305681954",
                    "name": "Josep Llad'os"
                }
            ]
        },
        {
            "paperId": "b3795d73cadfacfb6aa0f723978915012070a3d5",
            "title": "DocScript: Document-level Script Event Prediction",
            "abstract": "We present a novel task of document-level script event prediction, which aims to predict the next event given a candidate list of narrative events in long-form documents. To enable this, we introduce DocSEP, a challenging dataset in two new domains - contractual documents and Wikipedia articles, where timeline events may be paragraphs apart and may require multi-hop temporal and causal reasoning. We benchmark existing baselines and present a novel architecture called DocScript to learn sequential ordering between events at the document scale. Our experimental results on the DocSEP dataset demonstrate that learning longer-range dependencies between events is a key challenge and show that contemporary LLMs such as ChatGPT and FlanT5 struggle to solve this task, indicating their lack of reasoning abilities for understanding causal relationships and temporal sequences within long texts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "31099365",
                    "name": "Aparna Garimella"
                },
                {
                    "authorId": "2301580599",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2273677165",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "066090ac30fa17d6e5fd327481da768b922ae4b8",
            "title": "Characteristics of Deep and Skim Reading on Smartphones vs. Desktop: A Comparative Study",
            "abstract": "Deep reading fosters text comprehension, memory, and critical thinking. The growing prevalance of digital reading on mobile interfaces raises concerns that deep reading is being replaced by skimming and sifting through information, but this is currently unmeasured. Traditionally, reading quality is assessed using comprehension tests, which require readers to explicitly answer a set of carefully composed questions. To quantify and understand reading behaviour in natural settings and at scale, however, implicit measures are needed of deep versus skim reading across desktop and mobile devices, the most prominent digital reading platforms. In this paper, we present an approach to systematically induce deep and skim reading and subsequently train classifiers to discriminate these two reading styles based on eye movement patterns and interaction data. Based on a user study with 29 participants, we created models that detect deep reading on both devices with up to 0.82 AUC. We present the characteristics of deep reading and discuss how our models can be used to measure the effect of reading UI design and monitor long-term changes in reading behaviours.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109244239",
                    "name": "Xiuge Chen"
                },
                {
                    "authorId": "2072389004",
                    "name": "Namrata Srivastava"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2058280979",
                    "name": "Jennifer Healey"
                },
                {
                    "authorId": "1726630",
                    "name": "Tilman Dingler"
                }
            ]
        },
        {
            "paperId": "383db93b039c2f7d743a06dc62a8db2ff1ea33f7",
            "title": "DocEdit: Language-Guided Document Editing",
            "abstract": "Professional document editing tools require a certain level of expertise to perform complex edit operations. To make editing tools accessible to increasingly novice users, we investigate intelligent document assistant systems that can make or suggest edits based on a user's natural language request. Such a system should be able to understand the user's ambiguous requests and contextualize them to the visual cues and textual content found in a document image to edit localized unstructured text and structured layouts. To this end, we propose a new task of language-guided localized document editing, where the user provides a document and an open vocabulary editing request, and the intelligent system produces a command that can be used to automate edits in real-world document editing software. In support of this task, we curate the DocEdit dataset, a collection of approximately 28K instances of user edit requests over PDF and design templates along with their corresponding ground truth software executable commands. To our knowledge, this is the first dataset that provides a diverse mix of edit operations with direct and indirect references to the embedded text and visual objects such as paragraphs, lists, tables, etc. We also propose DocEditor, a Transformer-based localization-aware multimodal (textual, spatial, and visual) model that performs the new task. The model attends to both document objects and related text contents which may be referred to in a user edit request, generating a multimodal embedding that is used to predict an edit command and associated bounding box localizing it. Our proposed model empirically outperforms other baseline deep learning approaches by 15-18%, providing a strong starting point for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2301580599",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2172597446",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                }
            ]
        },
        {
            "paperId": "91ac4c094a2936c292b5dec56fd44a913f183083",
            "title": "LayerDoc: Layer-wise Extraction of Spatial Hierarchical Structure in Visually-Rich Documents",
            "abstract": "Digital documents often contain images and scanned text. Parsing such visually-rich documents is a core task for work-flow automation, but it remains challenging since most documents do not encode explicit layout information, e.g., how characters and words are grouped into boxes and ordered into larger semantic entities. Current state-of-the-art layout extraction methods are challenged by such documents as they rely on word sequences to have correct reading order and do not exploit their hierarchical structure. We propose LayerDoc, an approach that uses visual features, textual semantics, and spatial coordinates along with constraint inference to extract the hierarchical layout structure of documents in a bottom-up layer-wise fashion. LayerDoc recursively groups smaller regions into larger semantic elements in 2D to infer complex nested hierarchies. Experiments show that our approach outperforms competitive baselines by 10-15% on three diverse datasets of forms and mobile app screen layouts for the tasks of spatial region classification, higher-order group identification, layout hierarchy extraction, reading order detection, and word grouping.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2064021145",
                    "name": "Ashutosh Mehra"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2301580599",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2101319527",
                    "name": "Anandhavelu N"
                },
                {
                    "authorId": "2536742",
                    "name": "Quan Hung Tran"
                },
                {
                    "authorId": "1419671559",
                    "name": "Verena Kaynig-Fittkau"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                },
                {
                    "authorId": "2172597446",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                }
            ]
        },
        {
            "paperId": "a273c40cc1f1d7096efe40d62fe28befa524245c",
            "title": "Exposing Model Theft: A Robust and Transferable Watermark for Thwarting Model Extraction Attacks",
            "abstract": "The increasing prevalence of Deep Neural Networks (DNNs) in cloud-based services has led to their widespread use through various APIs. However, recent studies reveal the susceptibility of these public APIs to model extraction attacks, where adversaries attempt to create a local duplicate of the private model using data and API-generated predictions. Existing defense methods often involve perturbing prediction distributions to hinder an attacker's training goals, inadvertently affecting API utility. In this study, we extend the concept of digital watermarking to protect DNNs' APIs. We suggest embedding a watermark into the safeguarded APIs; thus, any model attempting to copy will inherently carry the watermark, allowing the defender to verify any suspicious models. We propose a simple yet effective framework to increase watermark transferability. By requiring the model to memorize the preset watermarks in the final decision layers, we significantly enhance the transferability of watermarks. Comprehensive experiments show that our proposed framework not only successfully watermarks APIs but also maintains their utility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057059798",
                    "name": "Ruixiang Tang"
                },
                {
                    "authorId": "1791983892",
                    "name": "Hongye Jin"
                },
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2243656383",
                    "name": "Xia Hu"
                }
            ]
        },
        {
            "paperId": "bc5f13e846cb4a2b80475b642a9b93d8aa1ccc1e",
            "title": "Computable Contracts by Extracting Obligation Logic Graphs",
            "abstract": "The emergence of contract specific programming languages has struggled to translate into widespread adoption of computable contracts due largely to high conversion costs. In this work, we present the first system for converting natural language contracts into code through the extraction of key entities, relationships, and formulas into a graph representation called the Obligation Logic Graph (OLG). This approach allows the semantic meaning of contract obligations, including dependencies between obligations, to be captured through the OLG and mapped to code downstream. We also introduce OLG extraction as a new joint entity and relation prediction task for legal contracts, and present the Contract-OLG dataset, consisting of 1,876 contract provisions, 18,597 entities and 18,170 relationships. We perform detailed experiments to understand the capabilities of state-of-the-art Transformer and graph-based models at completing these tasks, and identify where there is currently a significant gap between human expert and machine performance, particularly for relation extraction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121292636",
                    "name": "Sergio Servantez"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2233085914",
                    "name": "Alexa F. Siu"
                },
                {
                    "authorId": "6657914",
                    "name": "Milan Aggarwal"
                },
                {
                    "authorId": "145846953",
                    "name": "Balaji Krishnamurthy"
                },
                {
                    "authorId": "31099365",
                    "name": "Aparna Garimella"
                },
                {
                    "authorId": "2238213596",
                    "name": "Kristian Hammond"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "fc1400e02aa3b1ffb07628dc152a9157feac4782",
            "title": "Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances",
            "abstract": "To achieve state-of-the-art performance, one still needs to train NER models on large-scale, high-quality annotated data, an asset that is both costly and time-intensive to accumulate. In contrast, real-world applications often resort to massive low-quality labeled data through non-expert annotators via crowdsourcing and external knowledge bases via distant supervision as a cost-effective alternative. However, these annotation methods result in noisy labels, which in turn lead to a notable decline in performance. Hence, we propose to denoise the noisy NER data with guidance from a small set of clean instances. Along with the main NER model we train a discriminator model and use its outputs to recalibrate the sample weights. The discriminator is capable of detecting both span and category errors with different discriminative prompts. Results on public crowdsourcing and distant supervision datasets show that the proposed method can consistently improve performance with a small guidance set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261494597",
                    "name": "Zhendong Chu"
                },
                {
                    "authorId": "2261751659",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                }
            ]
        }
    ]
}