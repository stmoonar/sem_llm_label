{
    "authorId": "2145751547",
    "papers": [
        {
            "paperId": "6c620d914336793fde50c97eb5bd795c4b071440",
            "title": "Cross-Domain Low-Dose CT Image Denoising With Semantic Preservation and Noise Alignment",
            "abstract": "Deep learning (DL)-based Low-dose CT (LDCT) image denoising methods may face domain shift problem, where data from different domains (i.e., hospitals) may have similar anatomical regions but exhibit different intrinsic noise characteristics. Therefore, we propose a plug-and-play model called Low- and High-frequency Alignment (LHFA) to address this issue by leveraging semantic features and aligning noise distributions of different CT datasets, while maintaining diagnostic image quality and suppressing noise. Specifically, the LHFA model consists of a Low-frequency Alignment (LFA) module that preserves semantic features (i.e., low-frequency components) with fewer perturbations from both domains for reconstruction. Notably, a High-frequency Alignment (HFA) module is proposed to quantify the discrepancy between noise representations (i.e., high-frequency components) in a latent space mapped by an auto-encoder. Experimental results demonstrate that the LHFA model effectively alleviates the domain shift problem and significantly improves the performance of DL-based methods on cross-domain LDCT image denoising task, outperforming other domain adaptation-based methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189917492",
                    "name": "Jiaxin Huang"
                },
                {
                    "authorId": "2000512096",
                    "name": "Kecheng Chen"
                },
                {
                    "authorId": "2261908032",
                    "name": "Yazhou Ren"
                },
                {
                    "authorId": "2145751547",
                    "name": "Jiayu Sun"
                },
                {
                    "authorId": "1783239",
                    "name": "X. Pu"
                },
                {
                    "authorId": "2291996342",
                    "name": "Ce Zhu"
                }
            ]
        },
        {
            "paperId": "72d187f3cb3516f381342949c21912b7bf2a5702",
            "title": "Cross Domain Low-Dose CT Image Denoising With Semantic Information Alignment",
            "abstract": "Recently, cross domain adaptation has been applied into quite a few image restoration tasks. While promising performance has been achieved, the domain shift problem between the training set (a.k.a., source domain) and the testing set (a.k.a., target domain) in Low-dose Computed Tomography (LDCT) image denoising tasks is typically ignored by most existing methods. This is prone to the degradation of the denoising performance due to large discrepancy of feature distribution in each dataset from various vendors. Therefore, a simple yet effective LDCT denoising approach has been proposed in this paper to alleviate the domain shift between source and target domains through a novel semantic information alignment. Specifically, we first propose an adaptive version of random frequency mask (RFM) to extract the shared semantic information of cross domains. Then, we incorporate the mask into the existing denoiser to construct a semantic-information-guided objective. Experiments on synthetic and real datasets show our proposed method achieves impressive performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189917492",
                    "name": "Jiaxin Huang"
                },
                {
                    "authorId": "2000512096",
                    "name": "Kecheng Chen"
                },
                {
                    "authorId": "2145751547",
                    "name": "Jiayu Sun"
                },
                {
                    "authorId": "1783239",
                    "name": "X. Pu"
                },
                {
                    "authorId": "2041980",
                    "name": "Yazhou Ren"
                }
            ]
        },
        {
            "paperId": "10d9c1f66742ac68ea8cd13f5c0a3968b3771c54",
            "title": "GCN-MIF: Graph Convolutional Network with Multi-Information Fusion for Low-dose CT Denoising (preprint)",
            "abstract": "Being low-level radiation exposure and less harmful to health, low-dose computed tomography (LDCT) has been widely adopted in the early screening of lung cancer and COVID-19. LDCT images inevitably suffer from the degradation problem caused by complex noises. It was reported that deep learning (DL)-based LDCT denoising methods using convolutional neural network (CNN) achieved impressive denoising performance. Although most existing DL-based methods (e.g., encoder-decoder framework) can implicitly utilize non-local and contextual information via downsampling operator and 3D CNN, the explicit multi-information (i.e., local, non-local, and contextual) integration may not be explored enough. To address this issue, we propose a novel graph convolutional network-based LDCT denoising model, namely GCN-MIF, to explicitly perform multi-information fusion for denoising purpose. Concretely, by constructing intra- and inter-slice graph, the graph convolutional network is introduced to leverage the non-local and contextual relationships among pixels. The traditional CNN is adopted for the extraction of local information. Finally, the proposed GCN-MIF model fuses all the extracted local, non-local, and contextual information. Extensive experiments show the effectiveness of our proposed GCN-MIF model by quantitative and visualized results. Furthermore, a double-blind reader study on a public clinical dataset is also performed to validate the usability of denoising results in terms of the structural fidelity, the noise suppression, and the overall score. Models and code are available at https://github.com/tonyckc/GCN-MIF_demo.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2000512096",
                    "name": "Kecheng Chen"
                },
                {
                    "authorId": "2145751547",
                    "name": "Jiayu Sun"
                },
                {
                    "authorId": "2115733423",
                    "name": "Jiang Shen"
                },
                {
                    "authorId": "26980093",
                    "name": "Jixiang Luo"
                },
                {
                    "authorId": "2125496873",
                    "name": "Xinyu Zhang"
                },
                {
                    "authorId": "12491130",
                    "name": "Xuelin Pan"
                },
                {
                    "authorId": "48198574",
                    "name": "Dongsheng Wu"
                },
                {
                    "authorId": "152621421",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2093350390",
                    "name": "Miguel Bento"
                },
                {
                    "authorId": "2041980",
                    "name": "Yazhou Ren"
                },
                {
                    "authorId": "1783239",
                    "name": "X. Pu"
                }
            ]
        },
        {
            "paperId": "2b5acde0d838ae3a9b1359b48eeedd654373176e",
            "title": "Bidirectional Relationship Inferring Network for Referring Image Localization and Segmentation",
            "abstract": "Recently, referring image localization and segmentation has aroused widespread interest. However, the existing methods lack a clear description of the interdependence between language and vision. To this end, we present a bidirectional relationship inferring network (BRINet) to effectively address the challenging tasks. Specifically, we first employ a vision-guided linguistic attention module to perceive the keywords corresponding to each image region. Then, language-guided visual attention adopts the learned adaptive language to guide the update of the visual features. Together, they form a bidirectional cross-modal attention module (BCAM) to achieve the mutual guidance between language and vision. They can help the network align the cross-modal features better. Based on the vanilla language-guided visual attention, we further design an asymmetric language-guided visual attention, which significantly reduces the computational cost by modeling the relationship between each pixel and each pooled subregion. In addition, a segmentation-guided bottom-up augmentation module (SBAM) is utilized to selectively combine multilevel information flow for object localization. Experiments show that our method outperforms other state-of-the-art methods on three referring image localization datasets and four referring image segmentation datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "49081953",
                    "name": "Guang Feng"
                },
                {
                    "authorId": "2111296659",
                    "name": "Zhiwei Hu"
                },
                {
                    "authorId": "2108065801",
                    "name": "Lihe Zhang"
                },
                {
                    "authorId": "2145751547",
                    "name": "Jiayu Sun"
                },
                {
                    "authorId": "153176123",
                    "name": "Huchuan Lu"
                }
            ]
        },
        {
            "paperId": "666ec8c70465718285953dd5e04cb9f9379b4a37",
            "title": "RIDnet: Radiologist-Inspired Deep Neural Network for Low-dose CT Denoising",
            "abstract": "Being low-level radiation exposure and less harmful to health, low-dose computed tomography (LDCT) has been widely adopted in the early screening of lung cancer and COVID-19. LDCT images inevitably suffer from the degradation problem caused by complex noises. It was reported that, compared with commercial iterative reconstruction methods, deep learning (DL)-based LDCT denoising methods using convolutional neural network (CNN) achieved competitive performance. Most existing DL-based methods focus on the local information extracted by CNN, while ignoring both explicit non-local and context information (which are leveraged by radiologists). To address this issue, we propose a novel deep learning model named radiologist-inspired deep denoising network (RIDnet) to imitate the workflow of a radiologist reading LDCT images. Concretely, the proposed model explicitly integrates all the local, non-local and context information rather than local information only. Our radiologist-inspired model is potentially favoured by radiologists as a familiar workflow. A double-blind reader study on a public clinical dataset shows that, compared with state-of-the-art methods, our proposed model achieves the most impressive performance in terms of the structural fidelity, the noise suppression and the overall score. As a physicians-inspired model, RIDnet gives a new research roadmap that takes into account the behavior of physicians when designing decision support tools for assisting clinical diagnosis. Models and code are available at https://github.com/tonyckc/RIDnet_demo.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15495073",
                    "name": "Kecheng Chen"
                },
                {
                    "authorId": "2145751547",
                    "name": "Jiayu Sun"
                },
                {
                    "authorId": "2115733423",
                    "name": "Jiang Shen"
                },
                {
                    "authorId": "26980093",
                    "name": "Jixiang Luo"
                },
                {
                    "authorId": null,
                    "name": "Xinyu Zhang"
                },
                {
                    "authorId": "12491130",
                    "name": "Xuelin Pan"
                },
                {
                    "authorId": "48198574",
                    "name": "Dongsheng Wu"
                },
                {
                    "authorId": "2116811326",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2093350390",
                    "name": "Miguel Bento"
                },
                {
                    "authorId": "2041980",
                    "name": "Yazhou Ren"
                },
                {
                    "authorId": "1783239",
                    "name": "X. Pu"
                }
            ]
        },
        {
            "paperId": "94c557b169809520c2cde245d791b0dbf4cbe85f",
            "title": "Lesion-Inspired Denoising Network: Connecting Medical Image Denoising and Lesion Detection",
            "abstract": "Deep learning has achieved notable performance in the denoising task of low-quality medical images and the detection task of lesions, respectively. However, existing low-quality medical image denoising approaches are disconnected from the detection task of lesions. Intuitively, the quality of denoised images will influence the lesion detection accuracy that in turn can be used to affect the denoising performance. To this end, we propose a play-and-plug medical image denoising framework, namely Lesion-Inspired Denoising Network (LIDnet), to collaboratively improve both denoising performance and detection accuracy of denoised medical images. Specifically, we propose to insert the feedback of downstream detection task into existing denoising framework by jointly learning a multi-loss objective. Instead of using perceptual loss calculated on the entire feature map, a novel region-of-interest (ROI) perceptual loss induced by the lesion detection task is proposed to further connect these two tasks. To achieve better optimization for overall framework, we propose a customized collaborative training strategy for LIDnet. On consideration of clinical usability and imaging characteristics, three low-dose CT images datasets are used to evaluate the effectiveness of the proposed LIDnet. Experiments show that, by equipping with LIDnet, both of the denoising and lesion detection performance of baseline methods can be significantly improved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15495073",
                    "name": "Kecheng Chen"
                },
                {
                    "authorId": "2061752691",
                    "name": "Kun Long"
                },
                {
                    "authorId": "2041980",
                    "name": "Yazhou Ren"
                },
                {
                    "authorId": "2145751547",
                    "name": "Jiayu Sun"
                },
                {
                    "authorId": "1783239",
                    "name": "X. Pu"
                }
            ]
        },
        {
            "paperId": "9bdd27e81b406ae894059eb91a75c278d0591af8",
            "title": "MODNet-V: Improving Portrait Video Matting via Background Restoration",
            "abstract": "To address the challenging portrait video matting problem more precisely, existing works typically apply some matting priors that require additional user efforts to obtain, such as annotated trimaps or background images. In this work, we observe that instead of asking the user to explicitly provide a background image, we may recover it from the input video itself. To this end, we first propose a novel background restoration module (BRM) to recover the background image dynamically from the input video. BRM is extremely lightweight and can be easily integrated into existing matting models. By combining BRM with a recent image matting model, MODNet, we then present MODNet-V for portrait video matting. Benefited from the strong background prior provided by BRM, MODNet-V has only 1/3 of the parameters of MODNet but achieves comparable or even better performances. Our design allows MODNet-V to be trained in an end-to-end manner on a single NVIDIA 3090 GPU. Finally, we introduce a new patch refinement module (PRM) to adapt MODNet-V for high-resolution videos while keeping MODNet-V lightweight and fast.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145751547",
                    "name": "Jiayu Sun"
                },
                {
                    "authorId": "152984116",
                    "name": "Zhanghan Ke"
                },
                {
                    "authorId": "2108065801",
                    "name": "Lihe Zhang"
                },
                {
                    "authorId": "153176123",
                    "name": "Huchuan Lu"
                },
                {
                    "authorId": "1726262",
                    "name": "Rynson W. H. Lau"
                }
            ]
        },
        {
            "paperId": "139bdc53d0aec55092733c20a72edc1fecb98d91",
            "title": "Task-Driven Deep Learning for LDCT Image Denoising",
            "abstract": "Compared with normal-dose computed tomography (NDCT), low-dose CT (LDCT) images have lower potential radiation risk for patients while suffering from the degradation problem by noise. In the past decades, deep learning-based (DL-based) methods have achieved impressive denoising performances in comparison to traditional methods. However, most existing DL-based methods typically preform training on a specific pairs of LDCT/NDCT images and aim to generalize well on clinical scenarios with LDCT images only. It is a difficult task and challenge, denoising LDCT images with various noise characteristics due to different imaging protocols. We propose a task-driven deep learning framework for LDCT image denoising. Specifically, the variational autoencoder (VAE) is leveraged to learn noise distribution. By utilizing abundant open-source NDCT images as the latent references, we then construct pairs of induced-LDCT (namely pseudo-LDCT)/NDCT images rather than simply using pairs of non-induced-LDCT/NDCT images. Thus, the denoising model can perceive the noise within LDCT images directly. Extensive experiments on LDCT datasets (without NDCT references) show that our proposed framework achieves competitive performances compared with existing DL-based LDCT denoising methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15495073",
                    "name": "Kecheng Chen"
                },
                {
                    "authorId": "2139299903",
                    "name": "Jiaxin Huang"
                },
                {
                    "authorId": "2145751547",
                    "name": "Jiayu Sun"
                },
                {
                    "authorId": "2041980",
                    "name": "Yazhou Ren"
                },
                {
                    "authorId": "1783239",
                    "name": "X. Pu"
                }
            ]
        },
        {
            "paperId": "9d953680e9231c1bf88b890f6276169859349d28",
            "title": "MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition",
            "abstract": "Existing portrait matting methods either require auxiliary inputs that are costly to obtain or involve multiple stages that are computationally expensive, making them less suitable for real-time applications. In this work, we present a light-weight matting objective decomposition network (MODNet) for portrait matting in real-time with a single input image. The key idea behind our efficient design is by optimizing a series of sub-objectives simultaneously via explicit constraints. In addition, MODNet includes two novel techniques for improving model efficiency and robustness. First, an Efficient Atrous Spatial Pyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for semantic estimation. Second, a self-supervised sub-objectives consistency (SOC) strategy is proposed to adapt MODNet to real-world data to address the domain shift problem common to trimap-free methods. MODNet is easy to be trained in an end-to-end manner. It is much faster than contemporaneous methods and runs at 67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms prior trimap-free methods by a large margin on both Adobe Matting Dataset and a carefully designed photographic portrait matting (PPM-100) benchmark proposed by us. Further, MODNet achieves remarkable results on daily photos and videos.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152984116",
                    "name": "Zhanghan Ke"
                },
                {
                    "authorId": "2145751547",
                    "name": "Jiayu Sun"
                },
                {
                    "authorId": "3249631",
                    "name": "Kaican Li"
                },
                {
                    "authorId": "144479026",
                    "name": "Qiong Yan"
                },
                {
                    "authorId": "1726262",
                    "name": "Rynson W. H. Lau"
                }
            ]
        }
    ]
}