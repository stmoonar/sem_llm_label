{
    "authorId": "144684248",
    "papers": [
        {
            "paperId": "2a5890d18f83e96634c5dbc927a0039394c8f394",
            "title": "Improving Speech Prosody of Audiobook Text-To-Speech Synthesis with Acoustic and Textual Contexts",
            "abstract": "We present a multi-speaker Japanese audiobook text-to-speech (TTS) system that leverages multimodal context information of preceding acoustic context and bilateral textual context to improve the prosody of synthetic speech. Previous work either uses unilateral or single-modality context, which does not fully represent the context information. The proposed method uses an acoustic context encoder and a textual context encoder to aggregate context information and feeds it to the TTS model, which enables the model to predict context-dependent prosody. We conducted comprehensive objective and subjective evaluations on a multi-speaker Japanese audiobook dataset. Experimental results demonstrate that the proposed method significantly outperforms two previous works. Additionally, we present insights about the different choices of context - modalities, lateral information and length - for audiobook TTS that have never been discussed in the literature before.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2007596395",
                    "name": "Detai Xin"
                },
                {
                    "authorId": "9918923",
                    "name": "Sharath Adavanne"
                },
                {
                    "authorId": "31597556",
                    "name": "F. Ang"
                },
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                },
                {
                    "authorId": "2424104",
                    "name": "Shinnosuke Takamichi"
                },
                {
                    "authorId": "1685827",
                    "name": "H. Saruwatari"
                }
            ]
        },
        {
            "paperId": "ce203ed9d93151a1b8c182c7737b72758660b8af",
            "title": "Extraction of Product Specifications from the Web - Going Beyond Tables and Lists",
            "abstract": "E-commerce product pages on the web often present product specification data in structured tabular blocks. Extraction of these product attribute-value specifications has benefited applications like product catalogue curation, search, question answering, and others. However, across different Websites, there is a wide variety of HTML elements (like , , , , etc.) typically used to render these blocks that makes their automatic extraction a challenge. Most of the current research has focused on extracting product specifications from tables and lists and, therefore, suffers from recall when applied to a large-scale extraction setting. In this paper, we present a product specification extraction approach that goes beyond tables or lists and generalizes across the diverse HTML elements used for rendering specification blocks. Using a combination of hand-coded features and deep learned spatial and token features, we first identify the specification blocks on a product page. We then extract the product attribute-value pairs from these blocks following an approach inspired by wrapper induction. We created a labeled dataset of product specifications extracted from 14,111 diverse specification blocks taken from a range of different product websites. Our experiments show the efficacy of our approach compared to the current specification extraction models and support our claim about its application to large-scale product specification extraction.,",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149081611",
                    "name": "G. Gangadhar"
                },
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                }
            ]
        },
        {
            "paperId": "575f47035494362cadd9fb22daca33bbdcab038b",
            "title": "ProductQnA: Answering User Questions on E-Commerce Product Pages",
            "abstract": "Product pages on e-commerce websites often overwhelm their customers with a wealth of data, making discovery of relevant information a challenge. Motivated by this, here, we present a novel framework to answer both factoid and non-factoid user questions on product pages. We propose several question-answer matching models leveraging both deep learned distributional semantics and semantics imposed by a structured resource like a domain specific ontology. The proposed framework supports the use of a combination of these models and we show, through empirical evaluation, that a cascade of these models does much better in meeting the high precision requirements of such a question-answering system. Evaluation on user asked questions shows that the proposed system achieves 66% higher precision1 as compared to IDF-weighted average of word vectors baseline [1].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                },
                {
                    "authorId": "2054297550",
                    "name": "Kartik Mehta"
                },
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "118738038",
                    "name": "Vidit Bansal"
                },
                {
                    "authorId": "1793182",
                    "name": "Nikhil Rasiwasia"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                }
            ]
        },
        {
            "paperId": "17e157ea88e441a717ab58745c8d6cd5750d3e26",
            "title": "An Interactive Multi-Label Consensus Labeling Model for Multiple Labeler Judgments",
            "abstract": "\n \n Multi-label classification is crucial to several practical applications including document categorization, video tagging, targeted advertising etc. Training a multi-label classifier requires a large amount of labeled data which is often unavailable or scarce. Labeled data is then acquired by consulting multiple labelers---both human and machine. Inspired by ensemble methods, our premise is that labels inferred with high consensus among labelers, might be closer to the ground truth. We propose strategies based on interaction and active learning to obtain higher quality labels that potentially lead to greater consensus. We propose a novel formulation that aims to collectively optimize the cost of labeling, labeler reliability, label-label correlation and inter-labeler consensus. Evaluation on data labeled by multiple labelers (both human and machine) shows that our consensus output is closer to the ground truth when compared to the \"majority\" baseline. We present illustrative cases where it even improves over the existing ground truth. We also present active learning strategies to leverage our consensus model in interactive learning settings. Experiments on several real-world datasets (publicly available) demonstrate the efficacy of our approach in achieving promising classification results with fewer labeled data.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                },
                {
                    "authorId": "41030077",
                    "name": "Narasimha Raju Uppalapati"
                },
                {
                    "authorId": "144980189",
                    "name": "Pankaj Singh"
                },
                {
                    "authorId": "145799547",
                    "name": "Ganesh Ramakrishnan"
                }
            ]
        },
        {
            "paperId": "94ae678e8910e9802eeff25ce9c0985da2b96d14",
            "title": "Interactive Martingale Boosting",
            "abstract": "We present an approach and a system that explores the application of interactive machine learning to a branching program-based boosting algorithm-- Martingale Boosting. Typically, its performance is based on the ability of a learner to meet a fixed objective and does not account for preferences (e.g. low false positives) arising from an underlying classification problem. We use user preferences gathered on holdout data to guide the two-sided advantages of individual weak learners and tune them to meet these preferences. Extensive experiments show that while arbitrary preferences might be difficult to meet for a single classifier, a non-linear ensemble of classifiers as the one constructed by martingale boosting, performs better.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                },
                {
                    "authorId": "3431214",
                    "name": "Pushpak Burange"
                },
                {
                    "authorId": "145799547",
                    "name": "Ganesh Ramakrishnan"
                }
            ]
        },
        {
            "paperId": "ae1565ab95e384e4c204383f349135846d6424d7",
            "title": "Learning to Collectively Link Entities",
            "abstract": "Recently Kulkarni et al. [20] proposed an approach for collective disambiguation of entity mentions occurring in natural language text. Their model achieves disambiguation by efficiently computing exact MAP inference in a binary labeled Markov Random Field. Here, we build on their disambiguation model and propose an approach to jointly learn the node and edge parameters of such a model. We use a max margin framework, which is efficiently implemented using projected subgradient, for collective learning. We leverage this in an online and interactive annotation system which incrementally trains the model as data gets curated progressively. We demonstrate the usefulness of our system by manually completing annotations for a subset of the Wikipedia collection. We have made this data publicly available. Evaluation shows that learning helps and our system performs better than several other systems including that of Kulkarni et al.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                },
                {
                    "authorId": "2061025840",
                    "name": "Kanika Agarwal"
                },
                {
                    "authorId": "2162189",
                    "name": "Pararth Shah"
                },
                {
                    "authorId": "2629222",
                    "name": "Sunny Raj Rathod"
                },
                {
                    "authorId": "145799547",
                    "name": "Ganesh Ramakrishnan"
                }
            ]
        },
        {
            "paperId": "cfb1db4fe0139691f7221d72db1df1b88be16887",
            "title": "A machine-assisted human translation system for technical documents",
            "abstract": "Translation systems are known to benefit from the availability of a bilingual lexicon for a domain of interest. A system, aiming to build such a lexicon from source language corpus, often requires human assistance and is confronted by conflicting requirements of minimizing human translation effort while improving the translation quality. We present an approach that exploits redundancy in the source corpus and extracts recurring patterns which are: frequent, syntactically well-formed, and provide maximum corpus coverage. The patterns generalize over phrases and word types and our approach finds a succinct set of good patterns with high coverage. Our interactive system leverages these patterns in multiple iterations of translation and post-editing, thereby progressively generating a high quality bilingual lexicon.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112128933",
                    "name": "Vishwajeet Kumar"
                },
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                },
                {
                    "authorId": "144980189",
                    "name": "Pankaj Singh"
                },
                {
                    "authorId": "145799547",
                    "name": "Ganesh Ramakrishnan"
                },
                {
                    "authorId": "3129478",
                    "name": "Ganesh Arnaal"
                }
            ]
        },
        {
            "paperId": "d8147ae5499a1c908dd86d3cec1be06cb047b70b",
            "title": "An Approach to Collective Entity Linking",
            "abstract": "Entity linking is the task of disambiguating entities in unstructured text by linking them to an entity in a catalog. Several collective entity linking approaches exist that attempt to collectively disam-biguate all mentions in the text by leveraging both local mention-entity context and global entity-entity relatedness. However, the complexity of these models makes it unfeasible to employ exact inference techniques and jointly train the local and global feature weights. In this work we present a collective disambiguation model, that, under suitable assumptions makes ef-\ufb01cient implementation of exact MAP inference possible. We also present an ef\ufb01-cient approach to train the local and global features of this model and implement it in an interactive entity linking system. The system receives human feedback on a document collection and progressively trains the underlying disambiguation model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                },
                {
                    "authorId": "2061025840",
                    "name": "Kanika Agarwal"
                },
                {
                    "authorId": "2162189",
                    "name": "Pararth Shah"
                },
                {
                    "authorId": "2629222",
                    "name": "Sunny Raj Rathod"
                },
                {
                    "authorId": "145799547",
                    "name": "Ganesh Ramakrishnan"
                }
            ]
        },
        {
            "paperId": "53acbf573816167d6e84ae3894b6341b534cf3f9",
            "title": "System for collective entity disambiguation",
            "abstract": "We present an approach and a system for collective disambiguation of entity mentions occurring in natural language text. Given an input text, the system spots mentions and their candidate entities. Candidate entities across all mentions are jointly modeled as binary nodes in a Markov Random Field. Their edges correspond to the joint signal between pairs of entities. This facilitates collective disambiguation of the mentions achieved by performing MAP inference on the MRF in a binary label space. Our model also allows for a natural treatment of mentions that either have no entity attached or have more than one attachments. By restricting cliques to nodes and edges and with a submodularity assumption on their potentials, we get an inference problem that is efficiently solved using graph min cut.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                },
                {
                    "authorId": "2061025840",
                    "name": "Kanika Agarwal"
                },
                {
                    "authorId": "2162189",
                    "name": "Pararth Shah"
                },
                {
                    "authorId": "2629222",
                    "name": "Sunny Raj Rathod"
                },
                {
                    "authorId": "145799547",
                    "name": "Ganesh Ramakrishnan"
                }
            ]
        },
        {
            "paperId": "ed4e3bb262a4bd02564135c7024be6df5ed0c0ed",
            "title": "Efficient Reuse of Structured and Unstructured Resources for Ontology Population",
            "abstract": "We study the problem of ontology population for a domain ontology and present solutions based on semi-automatic techniques. A domain ontology for an organization, often consists of classes whose instances are either specific to, or independent of the organization. E.g. in an academic domain ontology, classes like Professor, Department could be organization (university) specific, while Conference, Programming languages are organization independent. This distinction allows us to leverage data sources both\u2015within the organization and those in the Internet \u2015 to extract entities and populate an ontology. We propose techniques that build on those for open domain IE. Together with user input, we show through comprehensive evaluation, how these semi-automatic techniques achieve high precision. We experimented with the academic domain and built an ontology comprising of over 220 classes. Intranet documents from five universities formed our organization specific corpora and we used open domain knowledge bases like Wikipedia, Linked Open Data, and web pages from the Internet as the organization independent data sources. The populated ontology that we built for one of the universities comprised of over 75,000 instances. We adhere to the semantic web standards and tools and make the resources available in the OWL format. These could be useful for applications such as information extraction, text annotation, and information retrieval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2653544",
                    "name": "Chetana Gavankar"
                },
                {
                    "authorId": "144684248",
                    "name": "Ashish Kulkarni"
                },
                {
                    "authorId": "145799547",
                    "name": "Ganesh Ramakrishnan"
                }
            ]
        }
    ]
}