{
    "authorId": "2159011228",
    "papers": [
        {
            "paperId": "84f5f9e77aab23c5ca8d68b97c62233867a4a4e1",
            "title": "Noise-Free Audio Signal Processing in Noisy Environment: A Hardware and Algorithm Solution",
            "abstract": "Dealing with background noise is a challenging task in audio signal processing, negatively impacting algorithm performance and system robustness. In this paper, we propose a simple solution that combines recording hardware modification and algorithm improvement to tackle the challenge. The proposed solution could produce clean and noise-free high quality audio recording even in noisy recording environment. Experiment results show that the proposed solution leads to better sound event detection accuracy and speech recognition results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2288325330",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "2288059395",
                    "name": "Zongyi Liu"
                },
                {
                    "authorId": "2288010213",
                    "name": "Shunyan Luo"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "2288045141",
                    "name": "Shuyi Wang"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        },
        {
            "paperId": "8596588b6b9a705520404a9cfb51449d8d7e8a39",
            "title": "Detecting Content Segments from Online Sports Streaming Events: Challenges and Solutions",
            "abstract": "Developing a client-side segmentation algorithm for on-line sports streaming holds significant importance. For instance, in order to assess the video quality from an end-user perspective such as artifact detection, it is important to initially segment the content within the streaming playback. The challenge lies in localizing the content due to the intricate scene changes between content and non-content sections in popular sports like football, tennis, baseball, and more. Client-side content detection can be implemented in two ways: intrusively, involving the interception of network traffic and parsing service provider data and logs, or non-intrusively, which entails capturing streamed videos from content providers and subjecting them to analysis using computer vision technologies. In this paper, we introduce a non-intrusive framework that leverages a combination of traditional machine learning algorithms and deep neural networks (DNN) to distinguish content sections from noncontent sections across various online sports streaming services. Our algorithm has demonstrated a remarkable level of accuracy and effectiveness in sports broadcasting events, effectively overcoming the complexities introduced by intricate non-content insertion methods during the games.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2288059395",
                    "name": "Zongyi Liu"
                },
                {
                    "authorId": "2288325330",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "2288010213",
                    "name": "Shunyan Luo"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "2288045141",
                    "name": "Shuyi Wang"
                }
            ]
        },
        {
            "paperId": "7d3f0f83fa943529d1516030c6abebd5a89aa44c",
            "title": "High Precision Sound Event Detection based on Transfer Learning using Transposed Convolutions and Feature Pyramid Network",
            "abstract": "We introduce two models for high precision sound event detection leveraging transfer learning. The sound events we detect include \u201cspeech\u201d, \u201cmusic\u201d, and \u201cchime\u201d. Both models consist of a CNN backbone pre-trained using AudioSet for audio classification. To get high precision detection results, the first model employs transposed convolutional layers as the detection head, while the second model uses Feature Pyramid Network(FPN) as the detection head. Experimental results show 98.8% accuracy and 98.6% F1 score on a private test set, from the one using FPN. Both models outperform a two-stage model using LSTM, various model ensembles, and a pre-trained neural network model for audio classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114031488",
                    "name": "S. Luo"
                },
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "152613893",
                    "name": "Z. Liu"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        },
        {
            "paperId": "ee210be856f8f782308808775e3dbd64a3e3df45",
            "title": "International Workshop on Multimodal Learning - 2023 Theme: Multimodal Learning with Foundation Models",
            "abstract": "The recent advancements in machine learning and artificial intelligence (particularly foundation models such as BERT, GPT-3, T5, ResNet, etc.) have demonstrated remarkable capabilities and driven significant revolutionary changes to the way we make inferences from complex data. These models represent a fundamental shift in the way data are approached and offer exciting new research directions and opportunities for multimodal learning and data fusion. Given the potential of foundation models to transform the field of multimodal learning, there is a need to bring together experts and researchers to discuss the latest developments in this area, exchange ideas, and identify key research questions and challenges that need to be addressed. By hosting this workshop, we aim to create a forum for researchers to share their insights and expertise on multimodal data fusion and learning using foundation models, and to explore potential new research directions and applications in the rapidly evolving field. We expect contributions from interdisciplinary researchers to study and model interactions between (but not limited to) modalities of language, graphs, time-series, vision, tabular data, sensors, and more. Our workshop will emphasize interdisciplinary work and aim at seeding cross-team collaborations around new tasks, datasets, and models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "49604787",
                    "name": "Fanyou Wu"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "dc55a213cf37862af7f8d1675c732a0dd0f3bc76",
            "title": "Detect Audio-Video Temporal Synchronization Errors in Advertisements (Ads)",
            "abstract": "Detecting audio-video (A/V) synchronization error is important to measure end user experience. Today, researches in this domain are mainly focused on contents such as movies or sports. The state of art algorithms usually first detect a specific type of events and then correlate the A/V data within during these events, e.g., find the human chatting events and then correlate the vocals with the lip shapes. Detecting A/V sync errors during Ads, on the other hand, has not received a lot of attentions. Compared with contents, an Ads section do not contain a particular type of events that can be used to detect A/V sync error. For example, many vocals in Ads are either from background narrators or have a very short period of time, so that the popular lip-sync based algorithms won\u2019t work accurately. In this paper, we present a novel algorithm that uses the scene change time features: we first segment out individual Ad from a playback. Then for each pair of temporal adjacent Ads, we compute the scene change time for the video data and the audio data separately, and then build their time difference histogram. Next, we aggregate the histograms from all Ads pairs within one Ads section. Finally, we combine the aggregated histogram to compute the A/V off-sync time values. We show that compared with the traditional lip-sync based algorithms, the new algorithm not only significantly improves the prediction rate, but also increases the prediction accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152613893",
                    "name": "Z. Liu"
                },
                {
                    "authorId": "2193282077",
                    "name": "Devin Chen"
                },
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2114031488",
                    "name": "S. Luo"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        },
        {
            "paperId": "e32ba95825c343a6acb1031f26ee4e547ea1bac7",
            "title": "A Two-Stage LSTM Based Approach for Voice Activity Detection with Sound Event Classification",
            "abstract": "We introduce a two-stage approach using LSTM for voice activity detection with sound event classification. This approach proves to be effective when training data is limited. Moreover, it achieves better performance than pre-trained model using large-scale data set (AudioSet). Apart from clip-level accuracy, we also introduce two metrics for evaluating overall audio segmentation accuracy: mean $\\mathbf{IoU}$),and mean front miss. On test set, our method achieves 98 % accuracy, 0.95 mean $\\mathbf{IoU}$ for speech and 0.99 mean $\\mathbf{IoU}$ for music, and 0.03 mean front miss for both speech and music.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "152613893",
                    "name": "Z. Liu"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        },
        {
            "paperId": "e8df39abe02fd781b2775a9f7ecc8cceb9f724a6",
            "title": "The self-organizing feature map used for speaker-independent speech recognition",
            "abstract": "Kohonen's self-organizing feature map (SOFM) is an effective neural network for unsupervised learning. It is expected to produce a topologically correct mapping between input and output space. This paper describes a speaker-independent isolated word speech recognition system that uses a self-organizing feature map. Many experiments indicated that the self-organizing feature map algorithm shows some defects. Our speech recognition research focuses on improving the algorithm. After the improved algorithm was adopted, experimental results show that the recognition rate of this system rises significantly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2202617893",
                    "name": "Liqing Zhou"
                },
                {
                    "authorId": "145038577",
                    "name": "Liu Zemin"
                }
            ]
        }
    ]
}