{
    "authorId": "2164704207",
    "papers": [
        {
            "paperId": "1b70fe4dc8d39ebc08b9472e6e90a1f21eddb6a3",
            "title": "Open Assistant Toolkit - version 2",
            "abstract": "We present the second version of the Open Assistant Toolkit (OAT-v2), an open-source task-oriented conversational system for composing generative neural models. OAT-v2 is a scalable and flexible assistant platform supporting multiple domains and modalities of user interaction. It splits processing a user utterance into modular system components, including submodules such as action code generation, multimodal content retrieval, and knowledge-augmented response generation. Developed over multiple years of the Alexa TaskBot challenge, OAT-v2 is a proven system that enables scalable and robust experimentation in experimental and real-world deployment. OAT-v2 provides open models and software for research and commercial applications to enable the future of multimodal virtual assistants across diverse applications and types of rich interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "48890086",
                    "name": "Federico Rossetto"
                },
                {
                    "authorId": "1796270950",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "2289842043",
                    "name": "Andrew Ramsay"
                },
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2289842340",
                    "name": "Philip Zubel"
                },
                {
                    "authorId": "2283847249",
                    "name": "Niklas Tecklenburg"
                },
                {
                    "authorId": "2261782190",
                    "name": "Jeffrey Dalton"
                }
            ]
        },
        {
            "paperId": "20025b2ef0cececcbc646a66093468da0c43769b",
            "title": "GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants",
            "abstract": "We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "1796270950",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "2283847249",
                    "name": "Niklas Tecklenburg"
                },
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "48890086",
                    "name": "Federico Rossetto"
                },
                {
                    "authorId": "2261782190",
                    "name": "Jeffrey Dalton"
                }
            ]
        },
        {
            "paperId": "21f7c8135396ee19e78fe6c9002bddf3ac6c056d",
            "title": "CODEC: Complex Document and Entity Collection",
            "abstract": "CODEC is a document and entity ranking benchmark that focuses on complex research topics. We target essay-style information needs of social science researchers, i.e. \"How has the UK's Open Banking Regulation benefited Challenger Banks\". CODEC includes 42 topics developed by researchers and a new focused web corpus with semantic annotations including entity links. This resource includes expert judgments on 17,509 documents and entities (416.9 per topic) from diverse automatic and interactive manual runs. The manual runs include 387 query reformulations, providing data for query performance prediction and automatic rewriting evaluation. CODEC includes analysis of state-of-the-art systems, including dense retrieval and neural re-ranking. The results show the topics are challenging with headroom for document and entity ranking improvement. Query expansion with entity information shows significant gains on document ranking, demonstrating the resource's value for evaluating and improving entity-oriented search. We also show that the manual query reformulations significantly improve document ranking and entity ranking performance. Overall, CODEC provides challenging research topics to support the development and evaluation of entity-centric search methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2105439683",
                    "name": "Paul Owoicho"
                },
                {
                    "authorId": "1796270950",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                }
            ]
        },
        {
            "paperId": "36342da8ef61828be9c756f225e13c21616e41d3",
            "title": "VILT: Video Instructions Linking for Complex Tasks",
            "abstract": "This work addresses challenges in developing conversational assistants that support rich multimodal video interactions to accomplish real-world tasks interactively. We introduce the task of automatically linking instructional videos to task steps as \"Video Instructions Linking for Complex Tasks\" (VILT). Specifically, we focus on the domain of cooking and empowering users to cook meals interactively with a video-enabled Alexa skill. We create a reusable benchmark with 61 queries from recipe tasks and curate a collection of 2,133 instructional \"How-To\" cooking videos. Studying VILT with state-of-the-art retrieval methods, we find that dense retrieval with ANCE is the most effective, achieving an NDCG@3 of 0.566 and P@1 of 0.644. We also conduct a user study that measures the effect of incorporating videos in a real-world task setting, where 10 participants perform several cooking tasks with varying multimodal experimental conditions using a state-of-the-art Alexa TaskBot system. The users interacting with manually linked videos said they learned something new 64% of the time, which is a 9% increase compared to the automatically linked videos (55%), indicating that linked video relevance is important for task learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "1796270950",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                }
            ]
        },
        {
            "paperId": "8bc6162766b4e6cd616ad508ea488ecc628cf4ac",
            "title": "Conversational Information Seeking: Theory and Application",
            "abstract": "Conversational information seeking (CIS) involves interaction sequences between one or more users and an information system. Interactions in CIS are primarily based on natural language dialogue, while they may include other types of interactions, such as click, touch, and body gestures. CIS recently attracted significant attention and advancements continue to be made. This tutorial follows the content of the recent Conversational Information Seeking book authored by several of the tutorial presenters. The tutorial aims to be an introduction to CIS for newcomers to CIS in addition to the recent advanced topics and state-of-the-art approaches for students and researchers with moderate knowledge of the topic. A significant part of the tutorial is dedicated to hands-on experiences based on toolkits developed by the presenters for conversational passage retrieval and multi-modal task-oriented dialogues. The outcomes of this tutorial include theoretical and practical knowledge, including a forum to meet researchers interested in CIS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                },
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "2105439683",
                    "name": "Paul Owoicho"
                },
                {
                    "authorId": "2065812052",
                    "name": "Filip Radlinski"
                },
                {
                    "authorId": "48890086",
                    "name": "Federico Rossetto"
                },
                {
                    "authorId": "2528063",
                    "name": "Johanne R. Trippas"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                }
            ]
        },
        {
            "paperId": "bb7503d9f97bb684775afa6c1f05eda784b36ed3",
            "title": "GRILLBot: A multi-modal conversational agent for complex real-world tasks",
            "abstract": "We present GRILLBot, an open-source multi-modal task-oriented voice assistant to help users perform complex tasks, focusing on the domains of cooking and home improvement. GRILLBot curates and leverages web information extraction to build coverage over a broad range of tasks for which a user can receive guidance. To represent each task, we propose TaskGraphs as a dynamic graph unifying steps, requirements, and curated domain knowledge enabling contextual question answering, and detailed explanations. Multi-modal elements play a key role in GRILLBot both helping the user navigate through the task and enriching the experience with helpful videos and images that are automatically linked throughout the task. We leverage a contextual neural semantic parser to enable flexible navigation when interacting with the system by jointly encoding stateful information with the conversation history. GRILLBot enables dynamic and adaptable task planning and assistance for complex tasks by combining elements of task representations that incorporate text and structure, combined with neural models for search, question answering, and dialogue state management. GRILLBot competed in the Alexa prize TaskBot Challenge as one of the finalists.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1796270950",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2105439683",
                    "name": "Paul Owoicho"
                },
                {
                    "authorId": "48890086",
                    "name": "Federico Rossetto"
                },
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                }
            ]
        }
    ]
}