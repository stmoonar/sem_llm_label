{
    "authorId": "2109964198",
    "papers": [
        {
            "paperId": "0f01e48b322659e017faeaa76211c209bd15b108",
            "title": "MMSci: A Multimodal Multi-Discipline Dataset for PhD-Level Scientific Comprehension",
            "abstract": "The rapid advancement of Large Language Models (LLMs) and Large Multimodal Models (LMMs) has heightened the demand for AI-based scientific assistants capable of understanding scientific articles and figures. Despite progress, there remains a significant gap in evaluating models' comprehension of professional, graduate-level, and even PhD-level scientific content. Current datasets and benchmarks primarily focus on relatively simple scientific tasks and figures, lacking comprehensive assessments across diverse advanced scientific disciplines. To bridge this gap, we collected a multimodal, multidisciplinary dataset from open-access scientific articles published in Nature Communications journals. This dataset spans 72 scientific disciplines, ensuring both diversity and quality. We created benchmarks with various tasks and settings to comprehensively evaluate LMMs' capabilities in understanding scientific figures and content. Our evaluation revealed that these tasks are highly challenging: many open-source models struggled significantly, and even GPT-4V and GPT-4o faced difficulties. We also explored using our dataset as training resources by constructing visual instruction-following data, enabling the 7B LLaVA model to achieve performance comparable to GPT-4V/o on our benchmark. Additionally, we investigated the use of our interleaved article texts and figure images for pre-training LMMs, resulting in improvements on the material generation task. The source dataset, including articles, figures, constructed benchmarks, and visual instruction-following data, is open-sourced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2261357619",
                    "name": "Xianjun Yang"
                },
                {
                    "authorId": "2312170621",
                    "name": "Kyuri Choi"
                },
                {
                    "authorId": "51439692",
                    "name": "Wanrong Zhu"
                },
                {
                    "authorId": "2310337940",
                    "name": "Ryan Hsieh"
                },
                {
                    "authorId": "2310433603",
                    "name": "HyeonJung Kim"
                },
                {
                    "authorId": "2310394887",
                    "name": "Jin Hyuk Lim"
                },
                {
                    "authorId": "2310418771",
                    "name": "Sungyoung Ji"
                },
                {
                    "authorId": "2310587270",
                    "name": "Byungju Lee"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                },
                {
                    "authorId": "2253654665",
                    "name": "L. Petzold"
                },
                {
                    "authorId": "2310407665",
                    "name": "Stephen D. Wilson"
                },
                {
                    "authorId": "2310341461",
                    "name": "Woosang Lim"
                },
                {
                    "authorId": "2297934952",
                    "name": "William Yang Wang"
                }
            ]
        },
        {
            "paperId": "723a6340ee641e190b22bb47455d05e3b4237179",
            "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
            "abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at https://github.com/facebookresearch/FnCTOD",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2284639678",
                    "name": "Zhiyu Zoey Chen"
                },
                {
                    "authorId": "2284594951",
                    "name": "Mike Ross"
                },
                {
                    "authorId": "2287921432",
                    "name": "Patrick Huber"
                },
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "2215596266",
                    "name": "Xin Luna Dong"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                }
            ]
        },
        {
            "paperId": "a68f9ad83eab98d2cb6445093c834c9500d5e841",
            "title": "FKA-Owl: Advancing Multimodal Fake News Detection through Knowledge-Augmented LVLMs",
            "abstract": "The massive generation of multimodal fake news involving both text and images exhibits substantial distribution discrepancies, prompting the need for generalized detectors. However, the insulated nature of training restricts the capability of classical detectors to obtain open-world facts. While Large Vision-Language Models (LVLMs) have encoded rich world knowledge, they are not inherently tailored for combating fake news and struggle to comprehend local forgery details. In this paper, we propose FKA-Owl, a novel framework that leverages forgery-specific knowledge to augment LVLMs, enabling them to reason about manipulations effectively. The augmented forgery-specific knowledge includes semantic correlation between text and images, and artifact trace in image manipulation. To inject these two kinds of knowledge into the LVLM, we design two specialized modules to establish their representations, respectively. The encoded knowledge embeddings are then incorporated into LVLMs. Extensive experiments on the public benchmark demonstrate that FKA-Owl achieves superior cross-domain performance compared to previous methods. Code is publicly available at https://liuxuannan.github.io/FKA_Owl.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2263703053",
                    "name": "Xuannan Liu"
                },
                {
                    "authorId": "2253583840",
                    "name": "Peipei Li"
                },
                {
                    "authorId": "2268583100",
                    "name": "Huaibo Huang"
                },
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2212020693",
                    "name": "Xing Cui"
                },
                {
                    "authorId": "2290472235",
                    "name": "Jiahao Liang"
                },
                {
                    "authorId": "2228651380",
                    "name": "Lixiong Qin"
                },
                {
                    "authorId": "2263663309",
                    "name": "Weihong Deng"
                },
                {
                    "authorId": "2262195320",
                    "name": "Zhaofeng He"
                }
            ]
        },
        {
            "paperId": "0ce1e3ba7cb96dab6db15ee7b4add29cab5d0efb",
            "title": "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, becoming increasingly crucial across various applications. However, this capability brings with it the risk of prompt injection attacks, where attackers inject instructions into LLMs' input to elicit undesirable actions or content. Understanding the robustness of LLMs against such attacks is vital for their safe implementation. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks. Our objective is to determine the extent to which LLMs can be influenced by injected instructions and their ability to differentiate between these injected and original target instructions. Through extensive experiments with leading instruction-following LLMs, we uncover significant vulnerabilities in their robustness to such attacks. Our results indicate that some models are overly tuned to follow any embedded instructions in the prompt, overly focusing on the latter parts of the prompt without fully grasping the entire context. By contrast, models with a better grasp of the context and instruction-following capabilities will potentially be more susceptible to compromise by injected instructions. This underscores the need to shift the focus from merely enhancing LLMs' instruction-following capabilities to improving their overall comprehension of prompts and discernment of instructions that are appropriate to follow. We hope our in-depth analysis offers insights into the underlying causes of these vulnerabilities, aiding in the development of future solutions. Code and data are available at https://github.com/Leezekun/instruction-following-robustness-eval",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "1780690",
                    "name": "Baolin Peng"
                },
                {
                    "authorId": "50462546",
                    "name": "Pengcheng He"
                },
                {
                    "authorId": "1740249",
                    "name": "Xifeng Yan"
                }
            ]
        },
        {
            "paperId": "15a2682ba1b479dea284062dd097a9a349a2eceb",
            "title": "AlpaCare: Instruction-tuned Large Language Models for Medical Application",
            "abstract": "Instruction-finetuning (IFT) has become crucial in aligning Large Language Models (LLMs) with diverse human needs and has shown great potential in medical applications. However, previous studies mainly fine-tune LLMs on biomedical datasets with limited diversity, which often rely on benchmarks or narrow task scopes, and hence significantly limit the effectiveness on their medical instruction-following ability and generalizability. To bridge this gap, we propose creating a diverse, machine-generated medical IFT dataset, MedInstruct-52k, using GPT-4 and ChatGPT with a high-quality expert-curated seed set. We then fine-tune LLaMA-series models on the dataset to develop AlpaCare. Despite using a smaller domain-specific dataset than previous medical LLMs, AlpaCare not only demonstrates superior performance on medical applications, with up to 38.1% absolute gain over best baselines in medical free-form instruction evaluations, but also achieves 6.7% absolute gains averaged over multiple general domain benchmarks. Human evaluation further shows that AlpaCare consistently outperforms best baselines in terms of both correctness and helpfulness. We offer public access to our data, model, and codebase in https://github.com/XZhang97666/AlpaCare.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108030191",
                    "name": "Xinlu Zhang"
                },
                {
                    "authorId": "2218035941",
                    "name": "Chenxin Tian"
                },
                {
                    "authorId": "2261357619",
                    "name": "Xianjun Yang"
                },
                {
                    "authorId": "2261358309",
                    "name": "Lichang Chen"
                },
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2253654665",
                    "name": "L. Petzold"
                }
            ]
        },
        {
            "paperId": "3f4bd6e35eca865b0226f1bc0da9fc5f0dc948a8",
            "title": "Time Series as Images: Vision Transformer for Irregularly Sampled Time Series",
            "abstract": "Irregularly sampled time series are increasingly prevalent, particularly in medical domains. While various specialized methods have been developed to handle these irregularities, effectively modeling their complex dynamics and pronounced sparsity remains a challenge. This paper introduces a novel perspective by converting irregularly sampled time series into line graph images, then utilizing powerful pre-trained vision transformers for time series classification in the same way as image classification. This method not only largely simplifies specialized algorithm designs but also presents the potential to serve as a universal framework for time series modeling. Remarkably, despite its simplicity, our approach outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the rigorous leave-sensors-out setting where a portion of variables is omitted during testing, our method exhibits strong robustness against varying degrees of missing observations, achieving an impressive improvement of 42.8% in absolute F1 score points over leading specialized baselines even with half the variables masked. Code and data are available at https://github.com/Leezekun/ViTST",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "50341591",
                    "name": "SHIYANG LI"
                },
                {
                    "authorId": "1740249",
                    "name": "Xifeng Yan"
                }
            ]
        },
        {
            "paperId": "5a185965ad1e87367d044b47043706d00b85b007",
            "title": "ChatEdit: Towards Multi-turn Interactive Facial Image Editing via Dialogue",
            "abstract": "This paper explores interactive facial image editing via dialogue and introduces the ChatEdit benchmark dataset for evaluating image editing and conversation abilities in this context. ChatEdit is constructed from the CelebA-HQ dataset, incorporating annotated multi-turn dialogues corresponding to user edit requests on the images. The dataset is challenging, as it requires the system to dynamically track user requests, edit images, and generate appropriate responses. Accordingly, we propose three benchmark tasks: (i) user edit request tracking, (ii) image editing, and (iii) response generation. We present a novel baseline framework that integrates a dialogue module for both tracking user requests and generating responses and an image editing module for image editing. Unlike previous approaches, our framework directly tracks user edit requests from the entire dialogue history up to the current turn and modifies the original image rather than adjusting the previous turn's output, thereby reducing error accumulation and preventing attribute forgetfulness. Extensive experiments on the ChatEdit dataset underline our framework's superior performance against prior models, while also highlighting potential room for further research. We will release the code and data publicly to facilitate advancements in complex interactive facial image editing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212020693",
                    "name": "Xing Cui"
                },
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "1390793985",
                    "name": "Peipei Li"
                },
                {
                    "authorId": "2149297987",
                    "name": "Yibo Hu"
                },
                {
                    "authorId": "1704812",
                    "name": "Hailin Shi"
                },
                {
                    "authorId": "2510474",
                    "name": "Zhaofeng He"
                }
            ]
        },
        {
            "paperId": "b0435af3063195e8ae880489e64ccde64e6d7563",
            "title": "Guiding Large Language Models via Directional Stimulus Prompting",
            "abstract": "We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "1780690",
                    "name": "Baolin Peng"
                },
                {
                    "authorId": "50462546",
                    "name": "Pengcheng He"
                },
                {
                    "authorId": "1947267",
                    "name": "Michel Galley"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "145026971",
                    "name": "Xi Yan"
                }
            ]
        },
        {
            "paperId": "c060976628a0a27489f2c1268818eac5991ad52f",
            "title": "Do you really follow me? Adversarial Instructions for Evaluating the Robustness of Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. However, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate LLMs\u2019 original instructions and prompt unintended actions and content. Therefore, it is crucial to understand LLMs\u2019 ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. In this paper, we propose a pioneering benchmark for automatically evaluating the robustness of LLMs against adversarial instructions. The objective of this benchmark is to quantify the extent to which LLMs are influenced by injected adversarial instructions and assess their ability to differentiate between these adversarial instructions and original user instructions. Through experiments conducted with state-of-the-art instruction-following LLMs, we uncover significant limitations in their robustness against adversarial instruction attacks. Furthermore, our findings indicate that prevalent instruction-tuned models are prone to being \u201coverfitted\u201d to follow any instruction phrase in the prompt without truly understanding which instructions should be followed. This highlights the need to address the challenge of training models to comprehend prompts instead of merely following instruction phrases and completing the text.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2257158371",
                    "name": "Baolin Peng"
                },
                {
                    "authorId": "2257269680",
                    "name": "Pengcheng He"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                }
            ]
        },
        {
            "paperId": "d948b7c90556ba4409f480532ad888d12b8f2d08",
            "title": "Enhancing Large Language Model Induced Task-Oriented Dialogue Systems Through Look-Forward Motivated Goals",
            "abstract": "Recently, the development of large language models (LLMs) has been significantly enhanced the question answering and dialogue generation, and makes them become increasingly popular in current practical scenarios. While unlike the general dialogue system which emphasizes the semantic performance, the task-oriented dialogue (ToD) systems aim to achieve the dialogue goal efficiently and successfully in multiple turns. Unfortunately, existing LLM-induced ToD systems lack the direct reward toward the final goal and do not take account of the dialogue proactivity that can strengthen the dialogue efficiency. To fill these gaps, we introduce the ProToD (Proactively Goal-Driven LLM-Induced ToD) approach, which anticipates the future dialogue actions and incorporates the goal-oriented reward signal to enhance ToD systems. Additionally, we present a novel evaluation method that assesses ToD systems based on goal-driven dialogue simulations. This method allows us to gauge user satisfaction, system efficiency and successful rate while overcoming the limitations of current Information and Success metrics. Empirical experiments conducted on the MultiWoZ 2.1 dataset demonstrate that our model can achieve superior performance using only 10% of the data compared to previous end-to-end fully supervised models. This improvement is accompanied by enhanced user satisfaction and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48430820",
                    "name": "Zhiyuan Hu"
                },
                {
                    "authorId": "2241533083",
                    "name": "Yue Feng"
                },
                {
                    "authorId": "145843537",
                    "name": "Yang Deng"
                },
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2241348826",
                    "name": "See-kiong Ng"
                },
                {
                    "authorId": "1755919",
                    "name": "A. Luu"
                },
                {
                    "authorId": "2258715976",
                    "name": "Bryan Hooi"
                }
            ]
        }
    ]
}