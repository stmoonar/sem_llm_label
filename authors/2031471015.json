{
    "authorId": "2031471015",
    "papers": [
        {
            "paperId": "04492af6276e59935b4dcaffc34f72c20adbe909",
            "title": "MSSM: A Multiple-level Sparse Sharing Model for Efficient Multi-Task Learning",
            "abstract": "Multi-task learning(MTL) is an open and challenging problem in various real-world applications. The typical way of conducting multi-task learning is establishing some global parameter sharing mechanism across all tasks or assigning each task an individual set of parameters with cross-connections between tasks. However, for most existing approaches, all tasks just thoroughly or proportionally share all the features without distinguishing the helpfulness of them. By that, some tasks would be intervened by the unhelpful features that are useful for other tasks, leading to undesired negative transfer between tasks. In this paper, we design a novel architecture named the Multiple-level Sparse Sharing Model (MSSM), which can learn features selectively and share knowledge across all tasks efficiently. MSSM first employs a field-level sparse connection module (FSCM) to enable much more expressive combinations of feature fields to be learned for generalization across tasks while still allowing for task-specific features to be customized for each task. Furthermore, a cell-level sparse sharing module (CSSM) can recognize the sharing pattern through a set of coding variables that selectively choose which cells to route for a given task. Extensive experimental results on several real-world datasets show that MSSM outperforms SOTA models significantly in terms of AUC and LogLoss metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118893038",
                    "name": "Ke Ding"
                },
                {
                    "authorId": "2031471015",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "2145948819",
                    "name": "Yong He"
                },
                {
                    "authorId": "2110384161",
                    "name": "Lei Cheng"
                },
                {
                    "authorId": "49314886",
                    "name": "Chilin Fu"
                },
                {
                    "authorId": "50815987",
                    "name": "Zhaoxin Huan"
                },
                {
                    "authorId": "2119001129",
                    "name": "Hai Li"
                },
                {
                    "authorId": "2118891495",
                    "name": "Tan Yan"
                },
                {
                    "authorId": "2146644035",
                    "name": "Liang Zhang"
                },
                {
                    "authorId": "2118044462",
                    "name": "Xiaolu Zhang"
                },
                {
                    "authorId": "2118895602",
                    "name": "Linjian Mo"
                }
            ]
        },
        {
            "paperId": "0ce24e44400f91d86ba337377a5093d5ff8923c8",
            "title": "Data Augmentation with Adversarial Training for Cross-Lingual NLI",
            "abstract": "Due to recent pretrained multilingual representation models, it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages. In practice, however, we still face the problem of scarce labeled data, leading to subpar results. In this paper, we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way. To this end, we propose two methods of training a generative model to induce synthesized examples, and then leverage the resulting data using an adversarial training regimen for more robustness. In a series of detailed experiments, we show that this fruitful combination leads to substantial gains in cross-lingual inference.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2031471015",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "2153095256",
                    "name": "Yaxin Zhu"
                },
                {
                    "authorId": "2011378",
                    "name": "Zuohui Fu"
                },
                {
                    "authorId": "2116459424",
                    "name": "Dongkuan Xu"
                },
                {
                    "authorId": "144608002",
                    "name": "Gerard de Melo"
                }
            ]
        },
        {
            "paperId": "18bb3bbeffe2b00378342a876d3de4ed695c57b4",
            "title": "Multi-Task Recurrent Modular Networks",
            "abstract": "We consider the models of deep multi-task learning with recurrent architectures that exploit regularities across tasks to improve the performance of multiple sequence processing tasks jointly. Most existing architectures are painstakingly customized to learn task relationships for different problems, which is not flexible enough to model the dynamic task relationships and lacks generalization abilities to novel test-time scenarios. We propose multi-task recurrent modular networks (MT-RMN) that can be incorporated in any multi-task recurrent models to address the above drawbacks. MT-RMN consists of a shared encoder and multiple task-specific decoders, and recurrently operates over time. For better flexibility, it modularizes the encoder into multiple layers of sub-networks and dynamically controls the connection between these sub-networks and the decoders at different time steps, which provides the recurrent networks with varying degrees of parameter sharing for tasks with dynamic relatedness. For the generalization ability, MT-RMN aims to discover a set of generalizable sub-networks in the encoder that are assembled in different ways for different tasks. The policy networks augmented with the differentiable routers are utilized to make the binary connection decisions between the sub-networks. The experimental results on three multi-task sequence processing datasets consistently demonstrate the effectiveness of MT-RMN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116459424",
                    "name": "Dongkuan Xu"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "2031471015",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "2991105",
                    "name": "Bo Zong"
                },
                {
                    "authorId": "3007026",
                    "name": "Wenchao Yu"
                },
                {
                    "authorId": "2090567",
                    "name": "Jingchao Ni"
                },
                {
                    "authorId": "2451800",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "48505793",
                    "name": "Xiang Zhang"
                }
            ]
        },
        {
            "paperId": "a223becf074a6d9ff6f1bada0e0f6eb15fd69341",
            "title": "Minimally-Supervised Structure-Rich Text Categorization via Learning on Text-Rich Networks",
            "abstract": "Text categorization is an essential task in Web content analysis. Considering the ever-evolving Web data and new emerging categories, instead of the laborious supervised setting, in this paper, we focus on the minimally-supervised setting that aims to categorize documents effectively, with a couple of seed documents annotated per category. We recognize that texts collected from the Web are often structure-rich, i.e., accompanied by various metadata. One can easily organize the corpus into a text-rich network, joining raw text documents with document attributes, high-quality phrases, label surface names as nodes, and their associations as edges. Such a network provides a holistic view of the corpus\u2019 heterogeneous data sources and enables a joint optimization for network-based analysis and deep textual model training. We therefore propose a novel framework for minimally supervised categorization by learning from the text-rich network. Specifically, we jointly train two modules with different inductive biases \u2013 a text analysis module for text understanding and a network learning module for class-discriminative, scalable network learning. Each module generates pseudo training labels from the unlabeled document set, and both modules mutually enhance each other by co-training using pooled pseudo labels. We test our model on two real-world datasets. On the challenging e-commerce product categorization dataset with 683 categories, our experiments show that given only three seed documents per category, our framework can achieve an accuracy of about 92%, significantly outperforming all compared methods; our accuracy is only less than 2% away from the supervised BERT model trained on about 50K labeled documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108262811",
                    "name": "Xinyang Zhang"
                },
                {
                    "authorId": "2047145237",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "2031471015",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "54e11c854d9d04ab6b074ebca59d7a464f97f55a",
            "title": "Leveraging Adversarial Training in Self-Learning for Cross-Lingual Text Classification",
            "abstract": "In cross-lingual text classification, one seeks to exploit labeled data from one language to train a text classification model that can then be applied to a completely different language. Recent multilingual representation models have made it much easier to achieve this. Still, there may still be subtle differences between languages that are neglected when doing so. To address this, we present a semi- supervised adversarial training process that minimizes the maximal loss for label-preserving input perturbations. The resulting model then serves as a teacher to induce labels for unlabeled target lan- guage samples that can be used during further adversarial training, allowing us to gradually adapt our model to the target language. Compared with a number of strong baselines, we observe signifi- cant gains in effectiveness on document and intent classification for a diverse set of languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2031471015",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "1830448647",
                    "name": "Yaxin Zhu"
                },
                {
                    "authorId": "2196187063",
                    "name": "Yupeng Zhang"
                },
                {
                    "authorId": "2011378",
                    "name": "Zuohui Fu"
                },
                {
                    "authorId": "2250320",
                    "name": "Dongkuan Xu"
                },
                {
                    "authorId": null,
                    "name": "Sen Yang"
                },
                {
                    "authorId": "144608002",
                    "name": "Gerard de Melo"
                }
            ]
        },
        {
            "paperId": "70bf54d1ba78c2046e0c558294d3f228417ef7ff",
            "title": "Domain-Specific Sentiment Lexicons Induced from Labeled Documents",
            "abstract": "Sentiment analysis is an area of substantial relevance both in industry and in academia, including for instance in social studies. Although supervised learning algorithms have advanced considerably in recent years, in many settings it remains more practical to apply an unsupervised technique. The latter are oftentimes based on sentiment lexicons. However, existing sentiment lexicons reflect an abstract notion of polarity and do not do justice to the substantial differences of word polarities between different domains. In this work, we draw on a collection of domain-specific data to induce a set of 24 domain-specific sentiment lexicons. We rely on initial linear models to induce initial word intensity scores, and then train new deep models based on word vector representations to overcome the scarcity of the original seed data. Our analysis shows substantial differences between domains, which make domain-specific sentiment lexicons a promising form of lexical resource in downstream tasks, and the predicted lexicons indeed perform effectively on tasks such as review classification and cross-lingual word sentiment prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112527451",
                    "name": "Sm Mazharul Islam"
                },
                {
                    "authorId": "2031471015",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "144608002",
                    "name": "Gerard de Melo"
                }
            ]
        },
        {
            "paperId": "773d5ddc414424a8948446ddaa5275b944f50891",
            "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon",
            "abstract": "How to develop slim and accurate deep neural networks has become crucial for real- world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2031471015",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "7159786",
                    "name": "Shangyu Chen"
                },
                {
                    "authorId": "1746914",
                    "name": "Sinno Jialin Pan"
                }
            ]
        }
    ]
}