{
    "authorId": "2116713871",
    "papers": [
        {
            "paperId": "03013e291fb3192b286147f5bdb5770e434f91b2",
            "title": "Do Language Models Plagiarize?",
            "abstract": "Past literature has illustrated that language models (LMs) often memorize parts of training instances and reproduce them in natural language generation (NLG) processes. However, it is unclear to what extent LMs \u201creuse\u201d a training corpus. For instance, models can generate paraphrased sentences that are contextually similar to training samples. In this work, therefore, we study three types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2 generated texts, in comparison to its training data, and further analyze the plagiarism patterns of fine-tuned LMs with domain-specific corpora which are extensively used in practice. Our results suggest that (1) three types of plagiarism widely exist in LMs beyond memorization, (2) both size and decoding methods of LMs are strongly associated with the degrees of plagiarism they exhibit, and (3) fine-tuned LMs\u2019 plagiarism patterns vary based on their corpus similarity and homogeneity. Given that a majority of LMs\u2019 training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indiscriminately pursuing larger models with larger training corpora. Plagiarized content can also contain individuals\u2019 personal and sensitive information. These findings overall cast doubt on the practicality of current LMs in mission-critical writing tasks and urge more discussions around the observed phenomena. Data and source code are available at https://github.com/Brit7777/LM-plagiarism.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116713871",
                    "name": "Jooyoung Lee"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "a88912c1989e2ec325ce8e3e3f060294147eaca5",
            "title": "Korean Dialect Identification Based on Intonation Modeling",
            "abstract": "Korean dialect identification (K-DID) is a challenging task due to its relatively unexplored field of study, mutual comprehensibility between the dialects, and lack of sufficient Korean dialect datasets available in the past. With large-scaled dialect datasets now available, this paper proposes intonational modeling of the Korean dialects by feeding frame-wise acoustic features on sequential modeling of a neural network. Compared to previous prosodic labeling with syllable-based pitch marking, our approach of intonation modeling is realized with the combination of a set of spectral features, including fundamental frequency, trained on a bidirectional LSTM network with attention mechanism. We believe the attention mechanism enables the detection of dialect-rich segments hidden among the dominant non-dialect segments within the same utterance. We test the networks on different combinations of speaker ages and speech styles. The best performance of the K-DID is achieved with 68.51 % in utterance-level accuracy, which surpasses our previous work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116713871",
                    "name": "Jooyoung Lee"
                },
                {
                    "authorId": "2155074754",
                    "name": "K. Kim"
                },
                {
                    "authorId": "1727628",
                    "name": "Minhwa Chung"
                }
            ]
        },
        {
            "paperId": "6e59f68b41b2edbed7099e45288c4bd7c838154a",
            "title": "Review of Language-Learner Computer Interactions: Theory, Methodology, and CALL Applications",
            "abstract": "Interdisciplinarity has become a buzzword in academia (Callard & Fitzgerald; Dzeng, 2013), and it is highly valued for helping to advance scientific research, approach and solve problems in innovative ways, as well as generate crucial research ideas (National Science Foundation, n.d.). Given that computer-assisted language learning (CALL), a field spanning two different disciplines (language learning and teaching and computer technology), is a relatively young field usually housed in applied linguistics and lacking in its own research methods, collaboration with other disciplines dealing with the development and use of technology is more than welcome to compensate for methodological shortcomings. In this sense, the edited volume, Language-learner computer interactions: Theory, methodology, and CALL applications, is a timely publication, as it is intended to bring together research methodologies and questions in the fields of applied linguistics, science, technology, and engineering. Due to the extraordinarily integrated nature of this volume, readers may realize that the relationship among these fields is closer than presumed and may notice that such interdisciplinary efforts make it possible to answer previously unanswered questions on the nature of learner\u2013computer interactions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116713871",
                    "name": "Jooyoung Lee"
                }
            ]
        }
    ]
}