{
    "authorId": "2156713249",
    "papers": [
        {
            "paperId": "2902a67f9aebb115fc2b6cdf611910e72e896bdd",
            "title": "GOODAT: Towards Test-time Graph Out-of-Distribution Detection",
            "abstract": "Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solution that operates independently of training data and modifications of GNN architecture. With a lightweight graph masker, GOODAT can learn informative subgraphs from test samples, enabling the capture of distinct graph patterns between OOD and ID samples. To optimize the graph masker, we meticulously design three unsupervised objective functions based on the graph information bottleneck principle, motivating the masker to capture compact yet informative subgraphs for OOD detection. Comprehensive evaluations confirm that our GOODAT method outperforms state-of-the-art benchmarks across a variety of real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2278588129",
                    "name": "Luzhi Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "49336556",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2277958230",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "4a339e9e6b8879f785e9af607517c17b31a06154",
            "title": "Gradient Transformation: Towards Efficient and Model-Agnostic Unlearning for Dynamic Graph Neural Networks",
            "abstract": "Graph unlearning has emerged as an essential tool for safeguarding user privacy and mitigating the negative impacts of undesirable data. Meanwhile, the advent of dynamic graph neural networks (DGNNs) marks a significant advancement due to their superior capability in learning from dynamic graphs, which encapsulate spatial-temporal variations in diverse real-world applications (e.g., traffic forecasting). With the increasing prevalence of DGNNs, it becomes imperative to investigate the implementation of dynamic graph unlearning. However, current graph unlearning methodologies are designed for GNNs operating on static graphs and exhibit limitations including their serving in a pre-processing manner and impractical resource demands. Furthermore, the adaptation of these methods to DGNNs presents non-trivial challenges, owing to the distinctive nature of dynamic graphs. To this end, we propose an effective, efficient, model-agnostic, and post-processing method to implement DGNN unlearning. Specifically, we first define the unlearning requests and formulate dynamic graph unlearning in the context of continuous-time dynamic graphs. After conducting a role analysis on the unlearning data, the remaining data, and the target DGNN model, we propose a method called Gradient Transformation and a loss function to map the unlearning request to the desired parameter update. Evaluations on six real-world datasets and state-of-the-art DGNN backbones demonstrate its effectiveness (e.g., limited performance drop even obvious improvement) and efficiency (e.g., at most 7.23$\\times$ speed-up) outperformance, and potential advantages in handling future unlearning requests (e.g., at most 32.59$\\times$ speed-up).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2112166824",
                    "name": "Xiangwen Yang"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2302859288",
                    "name": "Chengqi Zhang"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "3beedc05ca956bd39dc84173f3f5b4660535e814",
            "title": "Demystifying Uneven Vulnerability of Link Stealing Attacks against Graph Neural Networks",
            "abstract": "While graph neural networks (GNNs) dominate the state-of-the-art for exploring graphs in real-world applications, they have been shown to be vulnerable to a growing number of privacy attacks. For instance, link stealing is a well-known membership inference attack (MIA) on edges that infers the presence of an edge in a GNN\u2019s training graph. Recent studies on independent and identically distributed data (e.g., images) have empirically demonstrated that individuals from different groups suffer from different levels of privacy risks to MIAs, i.e., uneven vulnerability. However, theoretical evidence for such uneven vulnerability is missing. In this paper, we first present theoretical evidence of the uneven vulnerability of GNNs to link stealing attacks, which lays the foundation for demystifying such uneven risks among different groups of edges. We further demonstrate a group-based attack paradigm to expose the practical privacy harm to GNN users derived from the uneven vulnerability of edges. Finally, we empirically validate the existence of obvious uneven vulnerability on ten real-world datasets (e.g., about 25% AUC difference between different groups in the Credit graph). Compared with existing methods, the outperformance of our group-based attack paradigm confirms that cus-tomising different strategies for different groups results in more effective privacy attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2117010331",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2112166824",
                    "name": "Xiangwen Yang"
                },
                {
                    "authorId": "2837434",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                }
            ]
        },
        {
            "paperId": "459f7110dc6381b70c354b79857a1a717887be5c",
            "title": "Finding the Missing-half: Graph Complementary Learning for Homophily-prone and Heterophily-prone Graphs",
            "abstract": "Real-world graphs generally have only one kind of tendency in their connections. These connections are either homophily-prone or heterophily-prone. While graphs with homophily-prone edges tend to connect nodes with the same class (i.e., intra-class nodes), heterophily-prone edges tend to build relationships between nodes with different classes (i.e., inter-class nodes). Existing GNNs only take the original graph during training. The problem with this approach is that it forgets to take into consideration the ``missing-half\"structural information, that is, heterophily-prone topology for homophily-prone graphs and homophily-prone topology for heterophily-prone graphs. In our paper, we introduce Graph cOmplementAry Learning, namely GOAL, which consists of two components: graph complementation and complemented graph convolution. The first component finds the missing-half structural information for a given graph to complement it. The complemented graph has two sets of graphs including both homophily- and heterophily-prone topology. In the latter component, to handle complemented graphs, we design a new graph convolution from the perspective of optimisation. The experiment results show that GOAL consistently outperforms all baselines in eight real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153697517",
                    "name": "Y. Zheng"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2187919914",
                    "name": "V. Lee"
                },
                {
                    "authorId": "2149514422",
                    "name": "Yu Zheng"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "47d819f79d44d9cb5145b92e438f7b2a773451e8",
            "title": "A Survey on Fairness-aware Recommender Systems",
            "abstract": "As information filtering services, recommender systems have extremely enriched our daily life by providing personalized suggestions and facilitating people in decision-making, which makes them vital and indispensable to human society in the information era. However, as people become more dependent on them, recent studies show that recommender systems potentially own unintentional impacts on society and individuals because of their unfairness (e.g., gender discrimination in job recommendations). To develop trustworthy services, it is crucial to devise fairness-aware recommender systems that can mitigate these bias issues. In this survey, we summarise existing methodologies and practices of fairness in recommender systems. Firstly, we present concepts of fairness in different recommendation scenarios, comprehensively categorize current advances, and introduce typical methods to promote fairness in different stages of recommender systems. Next, after introducing datasets and evaluation metrics applied to assess the fairness of recommender systems, we will delve into the significant influence that fairness-aware recommender systems exert on real-world industrial applications. Subsequently, we highlight the connection between fairness and other principles of trustworthy recommender systems, aiming to consider trustworthiness principles holistically while advocating for fairness. Finally, we summarize this review, spotlighting promising opportunities in comprehending concepts, frameworks, the balance between accuracy and fairness, and the ties with trustworthiness, with the ultimate goal of fostering the development of fairness-aware recommender systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "82527705",
                    "name": "Luzhi Wang"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2218582225",
                    "name": "Weiping Ding"
                },
                {
                    "authorId": "2218637525",
                    "name": "Feng Xia"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "4fca71ebaeb89a5918bfd944d0645eec50eb647b",
            "title": "On the Interaction between Node Fairness and Edge Privacy in Graph Neural Networks",
            "abstract": "Due to the emergence of graph neural networks (GNNs) and their widespread implementation in real-world scenarios, the fairness and privacy of GNNs have attracted considerable interest since they are two essential social concerns in the era of building trustworthy GNNs. Existing studies have respectively explored the fairness and privacy of GNNs and exhibited that both fairness and privacy are at the cost of GNN performance. However, the interaction between them is yet to be explored and understood. In this paper, we investigate the interaction between the fairness of a GNN and its privacy for the \ufb01rst time. We empirically identify that edge privacy risks increase when the individual fairness of nodes is improved. Next, we present the intuition behind such a trade-off and employ the in\ufb02uence function and Pearson correlation to measure it theoretically. To take the performance, fairness, and privacy of GNNs into account simultaneously, we propose implementing fairness-aware re-weighting and privacy-aware graph structure perturbation modules in a retraining mechanism. Experimental results demonstrate that our method is effective in implementing GNN fairness with limited performance cost and restricted privacy risks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2300198381",
                    "name": "Quoc Viet Hung Nguyen"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "93b5f8111957a5b2ba36e1b4377d49b0309f3ae3",
            "title": "GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks",
            "abstract": "The emergence of Graph Neural Networks (GNNs) in graph data analysis and their deployment on Machine Learning as a Service platforms have raised critical concerns about data misuse during model training. This situation is further exacerbated due to the lack of transparency in local training processes, potentially leading to the unauthorized accumulation of large volumes of graph data, thereby infringing on the intellectual property rights of data owners. Existing methodologies often address either data misuse detection or mitigation, and are primarily designed for local GNN models rather than cloud-based MLaaS platforms. These limitations call for an effective and comprehensive solution that detects and mitigates data misuse without requiring exact training data while respecting the proprietary nature of such data. This paper introduces a pioneering approach called GraphGuard, to tackle these challenges. We propose a training-data-free method that not only detects graph data misuse but also mitigates its impact via targeted unlearning, all without relying on the original training data. Our innovative misuse detection technique employs membership inference with radioactive data, enhancing the distinguishability between member and non-member data distributions. For mitigation, we utilize synthetic graphs that emulate the characteristics previously learned by the target model, enabling effective unlearning even in the absence of exact graph data. We conduct comprehensive experiments utilizing four real-world graph datasets to demonstrate the efficacy of GraphGuard in both detection and unlearning. We show that GraphGuard attains a near-perfect detection rate of approximately 100% across these datasets with various GNN models. In addition, it performs unlearning by eliminating the impact of the unlearned graph with a marginal decrease in accuracy (less than 5%).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2112166824",
                    "name": "Xiangwen Yang"
                },
                {
                    "authorId": "2274079627",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2273925304",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                }
            ]
        },
        {
            "paperId": "cd401841bb095fc8e17fc443cbcc9e53e807ae4d",
            "title": "RD-Suite: A Benchmark for Ranking Distillation",
            "abstract": "The distillation of ranking models has become an important topic in both academia and industry. In recent years, several advanced methods have been proposed to tackle this problem, often leveraging ranking information from teacher rankers that is absent in traditional classification settings. To date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide range of tasks and datasets make it difficult to assess or invigorate advances in this field. This paper first examines representative prior arts on ranking distillation, and raises three questions to be answered around methodology and reproducibility. To that end, we propose a systematic and unified benchmark, Ranking Distillation Suite (RD-Suite), which is a suite of tasks with 4 large real-world datasets, encompassing two major modalities (textual and numeric) and two applications (standard distillation and distillation transfer). RD-Suite consists of benchmark results that challenge some of the common wisdom in the field, and the release of datasets with teacher scores and evaluation scripts for future research. RD-Suite paves the way towards better understanding of ranking distillation, facilities more research in this direction, and presents new challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2099586642",
                    "name": "Zhen Qin"
                },
                {
                    "authorId": "1886219",
                    "name": "R. Jagerman"
                },
                {
                    "authorId": "2096732",
                    "name": "Rama Kumar Pasumarthi"
                },
                {
                    "authorId": "39371343",
                    "name": "Honglei Zhuang"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "39934673",
                    "name": "Aijun Bai"
                },
                {
                    "authorId": "47214884",
                    "name": "Kai Hui"
                },
                {
                    "authorId": "2110134250",
                    "name": "Le Yan"
                },
                {
                    "authorId": "1526973500",
                    "name": "Xuanhui Wang"
                }
            ]
        },
        {
            "paperId": "edf2a9a2ee3c82756288366ce9d6592823399d53",
            "title": "Unraveling Privacy Risks of Individual Fairness in Graph Neural Networks",
            "abstract": "Graph neural networks (GNNs) have gained significant attraction due to their expansive real-world applications. To build trustworthy GNNs, two aspects - fairness and privacy - have emerged as critical considerations. Previous studies have separately examined the fairness and privacy aspects of GNNs, revealing their tradeoff with GNN performance. Yet, the inter-play between these two aspects remains unexplored. In this paper, we pioneer the exploration of the interaction between the privacy risks of edge leakage and the individual fairness of a GNN. Our theoretical analysis unravels that edge privacy risks unfortunately escalate when the nodes' individual fairness improves. Such an issue hinders the accomplishment of privacy and fairness of GNNs at the same time. To balance fairness and privacy, we carefully introduce fairness-aware loss reweighting based on in-fluence function and privacy-aware graph structure perturbation modules within a fine-tuning mechanism. Experimental results underscore the effectiveness of our approach in achieving GNN fairness with limited performance compromise and controlled privacy risks. This work contributes to the comprehensively developing trustworthy GNNs by simultaneously addressing both fairness and privacy aspects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
            "title": "Trustworthy Graph Neural Networks: Aspects, Methods, and Trends",
            "abstract": "Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications such as recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects, such as vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterized by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarize existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. In addition, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialization of trustworthy GNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "8163721",
                    "name": "Hanghang Tong"
                },
                {
                    "authorId": "2112496348",
                    "name": "Jian Pei"
                }
            ]
        }
    ]
}