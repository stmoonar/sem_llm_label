{
    "authorId": "2116734046",
    "papers": [
        {
            "paperId": "60531497fd812ae4712bb91cc54de049edffe5c0",
            "title": "RAPTA: A Hierarchical Representation Learning Solution For Real-Time Prediction of Path-Based Static Timing Analysis",
            "abstract": "This paper presents RAPTA, a customized Representation-learning Architecture for automation of feature engineering and predicting the result of Path-based Timing-Analysis early in the physical design cycle. RAPTA offers multiple advantages compared to prior work: 1) It has superior accuracy with errors std ranges 3.9ps~16.05ps in 32nm technology. 2) RAPTA's architecture does not change with feature-set size, 3) RAPTA does not require manual input feature engineering. To the best of our knowledge, this is the first work, in which Bidirectional Long Short-Term Memory (Bi-LSTM) representation learning is used to digest raw information for feature engineering, where generation of latent features and Multilayer Perceptron (MLP) based regression for timing prediction can be trained end-to-end.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2123930262",
                    "name": "Tanmoy Chowdhury"
                },
                {
                    "authorId": "66097019",
                    "name": "Ashka Vakil"
                },
                {
                    "authorId": "2161966302",
                    "name": "B. S. Latibari"
                },
                {
                    "authorId": "2113627697",
                    "name": "Sayed Aresh Beheshti-Shirazi"
                },
                {
                    "authorId": "1380629762",
                    "name": "Ali Mirzaeian"
                },
                {
                    "authorId": "46909769",
                    "name": "Xiaojie Guo"
                },
                {
                    "authorId": "2389278",
                    "name": "Sai Manoj Pudukotai Dinakarrao"
                },
                {
                    "authorId": "1747542",
                    "name": "H. Homayoun"
                },
                {
                    "authorId": "1740536",
                    "name": "I. Savidis"
                },
                {
                    "authorId": "2116734046",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "1928425942",
                    "name": "Avesta Sasan"
                }
            ]
        },
        {
            "paperId": "5d06e9c42d36a9fd623c003bc8a6387d63a8a3f5",
            "title": "Deep Graph Learning for Circuit Deobfuscation",
            "abstract": "Circuit obfuscation is a recently proposed defense mechanism to protect the intellectual property (IP) of digital integrated circuits (ICs) from reverse engineering. There have been effective schemes, such as satisfiability (SAT)-checking based attacks that can potentially decrypt obfuscated circuits, which is called deobfuscation. Deobfuscation runtime could be days or years, depending on the layouts of the obfuscated ICs. Hence, accurately pre-estimating the deobfuscation runtime within a reasonable amount of time is crucial for IC designers to optimize their defense. However, it is challenging due to (1) the complexity of graph-structured circuit; (2) the varying-size topology of obfuscated circuits; (3) requirement on efficiency for deobfuscation method. This study proposes a framework that predicts the deobfuscation runtime based on graph deep learning techniques to address the challenges mentioned above. A conjunctive normal form (CNF) bipartite graph is utilized to characterize the complexity of this SAT problem by analyzing the SAT attack method. Multi-order information of the graph matrix is designed to identify the essential features and reduce the computational cost. To overcome the difficulty in capturing the dynamic size of the CNF graph, an energy-based kernel is proposed to aggregate dynamic features into an identical vector space. Then, we designed a framework, Deep Survival Analysis with Graph (DSAG), which integrates energy-based layers and predicts runtime inspired by censored regression in survival analysis. Integrating uncensored data with censored data, the proposed model improves the standard regression significantly. DSAG is an end-to-end framework that can automatically extract the determinant features for deobfuscation runtime. Extensive experiments on benchmarks demonstrate its effectiveness and efficiency.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111361041",
                    "name": "Zhiqian Chen"
                },
                {
                    "authorId": "2152829032",
                    "name": "Lei Zhang"
                },
                {
                    "authorId": "116773482",
                    "name": "Gaurav Kolhe"
                },
                {
                    "authorId": "32568058",
                    "name": "Hadi Mardani Kamali"
                },
                {
                    "authorId": "2951001",
                    "name": "S. Rafatirad"
                },
                {
                    "authorId": "1430780489",
                    "name": "Sai Manoj Pudukotai Dinakarrao"
                },
                {
                    "authorId": "1747542",
                    "name": "H. Homayoun"
                },
                {
                    "authorId": "2110142089",
                    "name": "Chang-Tien Lu"
                },
                {
                    "authorId": "2116734046",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "126925a7125d3725ee277a4605a3692f1a7ca9a4",
            "title": "Mitigating Cache-Based Side-Channel Attacks through Randomization: A Comprehensive System and Architecture Level Analysis",
            "abstract": "Cache hierarchy was designed to allow CPU cores to process instructions faster by bridging the significant latency gap between the main memory and processor. In addition, various cache replacement algorithms are proposed to predict future data and instructions to boost the performance of the computer systems. However, recently proposed cache-based Side-Channel Attacks (SCAs) have shown to effectively exploiting such a hierarchical cache design. The cache-based SCAs are exploiting the hardware vulnerabilities to steal secret information from users by observing cache access patterns of cryptographic applications and thus are emerging as a serious threat to the security of the computer systems. Prior works on mitigating the cache-based SCAs have mainly focused on cache partitioning techniques and/or randomization of mapping between main memory. However, such solutions though effective, require modification in the processor hardware which increases the complexity of architecture design and are not applicable to current as well as legacy architectures. In response, this paper proposes a lightweight system and architecture level randomization technique to effectively mitigate the impact of side-channel attacks on last-level caches with no hardware redesign overhead for current as well as legacy architectures. To this aim, by carefully adapting the processor frequency and prefetchers operation and adding proper level of noise to the attackers\u2019 cache observations we attempt to protect the critical information from being leaked. The experimental results indicate that the concurrent randomization of frequency and prefetchers can significantly prevent cache-based side-channel attacks with no need for a new cache design. In addition, the proposed randomization and adaptation methodology outperforms the stat-of-the-art solutions in terms of the performance and execution time by reducing the performance overhead from 32.66% to nearly 20%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113289942",
                    "name": "Han Wang"
                },
                {
                    "authorId": "34223410",
                    "name": "H. Sayadi"
                },
                {
                    "authorId": "2393902",
                    "name": "T. Mohsenin"
                },
                {
                    "authorId": "2116734046",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "1798722",
                    "name": "Avesta Sasan"
                },
                {
                    "authorId": "2951001",
                    "name": "S. Rafatirad"
                },
                {
                    "authorId": "1747542",
                    "name": "H. Homayoun"
                }
            ]
        },
        {
            "paperId": "ef340dea5115674ff5ef2a9544d9684584502be5",
            "title": "Cognitive and Scalable Technique for Securing IoT Networks Against Malware Epidemics",
            "abstract": "The sheer volume of IoT networks being deployed today presents a major \u201cattack surface\u201d and poses significant security risks at a scale never encountered before. In other words, a single IoT device/node that gets infected with malware has the potential to spread the malicious activities across the network, eventually ceasing the network functionality or compromising the network. Simply detecting and quarantining the malware in IoT networks does not guarantee preventing malware propagation. On the other hand, use of traditional control theory for malware confinement is not effective, as most of the existing works do not consider real-time malware control strategies that can be implemented using uncertain infection information from the nodes in the network or have the containment problem decoupled from network performance. In response, in this work, we propose a two-pronged approach with malware detection at node-level, and confinement of malware at network-level. We deploy a recently proposed lightweight runtime malware detector at the node-level that employs Hardware Performance Counter (HPC) values for malware detection. This node-level malware information is combined with the malware propagation information and then fed during runtime to a stochastic predictive controller to confine the malware propagation without hampering the network performance. Synthesizing the node-level malware information with the model predictive containment strategy leads to achieving an average network throughput of nearly 200% of that of IoT network without any defense, and up to 160% of that of network with commonly employed state-of-the-art heuristic approaches for malware confinement. Furthermore, to scale with ever-increasing network topology sizes, we introduce a novel multi-attribute graph translation that can predict the network topology and node state information when provided with a snapshot of topology and node-level malware infection. The proposed multi-attribute graph translation has <5.88 Root Mean Square Error (RMSE) compared to the model predictive containment strategy and has shown nearly constant graph translation time and limited resource utilization independent of the network size.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2389278",
                    "name": "Sai Manoj Pudukotai Dinakarrao"
                },
                {
                    "authorId": "33465926",
                    "name": "Xiaojie Guo"
                },
                {
                    "authorId": "34223410",
                    "name": "H. Sayadi"
                },
                {
                    "authorId": "2646915",
                    "name": "Cameron Nowzari"
                },
                {
                    "authorId": "1798722",
                    "name": "Avesta Sasan"
                },
                {
                    "authorId": "2951001",
                    "name": "S. Rafatirad"
                },
                {
                    "authorId": "2116734046",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "1747542",
                    "name": "H. Homayoun"
                }
            ]
        },
        {
            "paperId": "690f681e19cafa4938f9246b79ac9192eb971bce",
            "title": "Pyramid: Machine Learning Framework to Estimate the Optimal Timing and Resource Usage of a High-Level Synthesis Design",
            "abstract": "The emergence of High-Level Synthesis (HLS) tools shifted the paradigm of hardware design by making the process of mapping high-level programming languages to hardware design such as C to VHDL/Verilog feasible. HLS tools offer a plethora of techniques to optimize designs for both area and performance, but resource usage and timing reports of HLS tools mostly deviate from the post-implementation results. In addition, to evaluate a hardware design performance, it is critical to determine the maximum achievable clock frequency. Obtaining such information using static timing analysis provided by CAD tools is difficult, due to the multitude of tool options. Moreover, a binary search to find the maximum frequency is tedious, time-consuming, and often does not obtain the optimal result. To address these challenges, we propose a framework, called Pyramid, that uses machine learning to accurately estimate the optimal performance and resource utilization of an HLS design. For this purpose, we first create a database of C-to- FPGA results from a diverse set of benchmarks. To find the achievable maximum clock frequency, we use Minerva, which is an automated hardware optimization tool. Minerva determines the close-to-optimal settings of tools, using static timing analysis and a heuristic algorithm, and targets either optimal throughput or throughput-to-area. Pyramid uses the database to train an ensemble machine learning model to map the HLS-reported features to the results of Minerva. To this end, Pyramid recalibrates the results of HLS to bridge the accuracy gap, and enable developers to estimate the throughput or throughputto- area of hardware design with more than 95% accuracy and alleviates the need to perform actual implementation for estimation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30521811",
                    "name": "Hosein Mohammadi Makrani"
                },
                {
                    "authorId": "2150702",
                    "name": "Farnoud Farahmand"
                },
                {
                    "authorId": "34223410",
                    "name": "H. Sayadi"
                },
                {
                    "authorId": "2068391284",
                    "name": "Sara Bondi"
                },
                {
                    "authorId": "2389278",
                    "name": "Sai Manoj Pudukotai Dinakarrao"
                },
                {
                    "authorId": "2116734046",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "1798722",
                    "name": "Avesta Sasan"
                },
                {
                    "authorId": "1747542",
                    "name": "H. Homayoun"
                },
                {
                    "authorId": "2951001",
                    "name": "S. Rafatirad"
                }
            ]
        }
    ]
}