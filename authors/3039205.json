{
    "authorId": "3039205",
    "papers": [
        {
            "paperId": "eb5c33ebf2852911842199e13f1ad53458fe105b",
            "title": "3DKD - An Effective Knowledge Distillation-Based Model for Human Fall Detection",
            "abstract": "Accidental falls are a commonly encountered occurrence in various groups of people, encompassing children, the elderly, and adults alike. The prompt detection of human falls represents a paramount method for mitigating the substantial risks associated with loss of self-control, fatality, or physical harm. Moreover, such proactive measures hold the potential to curtail healthcare expenses. Consequently, there exists a pressing need for research and development of systems aimed at detecting and facilitating the rescue of individuals involved in falls. In this regard, the proposed model, namely 3DKD, is constructed by employing the technique of knowledge distillation in conjunction with the attention mechanism, thereby yielding significant improvements in computer vision tasks within the same model layer. This innovative approach is anticipated to contribute to the creation of more precise deployment models capable of operating efficiently on weak devices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146734517",
                    "name": "T. Hoa"
                },
                {
                    "authorId": "2292765726",
                    "name": "Tran Thanh Thuong"
                },
                {
                    "authorId": "72222912",
                    "name": "M. Clari\u00f1o"
                },
                {
                    "authorId": "2292753975",
                    "name": "Vladimir Y. Mariano"
                },
                {
                    "authorId": "46535486",
                    "name": "Val Randolf M. Madrid"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                }
            ]
        },
        {
            "paperId": "7ee48cd70f6f7103decc3835c7e0bcb8a9716360",
            "title": "Task Assignment Strategies for Crowd Worker Ability Improvement",
            "abstract": "Workers are the most important resource in crowdsourcing. However, only investing in worker-centric needs, such as skill improvement, often conflicts with short-term platform-centric needs, such as task throughput. This paper studies learning strategies in task assignment in crowdsourcing and their impact on platform-centric needs. We formalize learning potential of individual tasks and collaborative tasks, and devise an iterative task assignment and completion approach that implements strategies grounded in learning theories. We conduct experiments to compare several learning strategies in terms of skill improvement, and in terms of task throughput and contribution quality. We discuss how our findings open new research directions in learning and collaboration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066598689",
                    "name": "Masaki Matsubara"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "cda187be8316892eecd1f9cfbfa70972f165ecc7",
            "title": "Multi-Session Diversity to Improve User Satisfaction in Web Applications",
            "abstract": "In various Web applications, users consume content in a series of sessions. That is prevalent in online music listening, where a session is a channel and channels are listened to in sequence, or in crowdsourcing, where a session is a set of tasks and task sets are completed in sequence. Content diversity can be defined in more than one way, e.g., based on artists or genres for music, or on requesters or rewards in crowdsourcing. A user may prefer to experience diversity within or across sessions. Naturally, intra-session diversity is set-based, whereas, inter-session diversity is sequence-based. This novel multi-session diversity gives rise to four bi-objective problems with the goal of minimizing or maximizing inter and intra diversities. Given the hardness of those problems, we propose to formulate a constrained optimization problem that optimizes inter diversity, subject to the constraint of intra diversity. We develop an efficient algorithm to solve our problem. Our experiments with human subjects on two real datasets, music and crowdsourcing, show our diversity formulations do serve different user needs, and yield high user satisfaction. Our large data experiments on real and synthetic data empirically demonstrate that our solution satisfy the theoretical bounds and is highly scalable, compared to baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47224252",
                    "name": "M. Esfandiari"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "2106769934",
                    "name": "Sepideh Nikookar"
                },
                {
                    "authorId": "2106777173",
                    "name": "Paras Sakharkar"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                }
            ]
        },
        {
            "paperId": "6aced7d7303fd82f8c4218aea2f3387efdf1d3c1",
            "title": "Making AI Machines Work for Humans in FoW",
            "abstract": "The Future of Work (FoW) is witnessing an evolution where AI systems (broadly machines or businesses) are used to the benefit of humans. Work here refers to all forms of paid and unpaid labor in both physical and virtual workplaces and that is enabled by AI systems. This covers crowdsourcing platforms such as Amazon Mechanical Turk, online labor marketplaces such as TaskRabbit and Qapa, but also regular jobs in physical workplaces. Bringing humans back to the frontier of FoW will increase their trust in AI systems and shift their perception to use them as a source of self-improvement, ensure better work performance, and positively shape social and economic outcomes of a society and a nation. To enable that, physical and virtual workplaces will need to capture human traits, behavior, evolving needs, and provide jobs to all. Attitudes, values, opinions regarding the processes and policies will need to be assessed and considered in the design of FoW ecosystems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2034201368",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "2146072233",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                },
                {
                    "authorId": "2034202207",
                    "name": "James Abello Monedero"
                },
                {
                    "authorId": "1807924",
                    "name": "P. Bourhis"
                },
                {
                    "authorId": "2038513",
                    "name": "F. Charoy"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1403157540",
                    "name": "D. Gross-Amblard"
                },
                {
                    "authorId": "51493818",
                    "name": "Emilie Hoareau"
                },
                {
                    "authorId": "2808778",
                    "name": "M. Inoguchi"
                },
                {
                    "authorId": "2034202198",
                    "name": "Jared Kenworthy"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                },
                {
                    "authorId": "2124213925",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1573872877",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "1802817",
                    "name": "Paolo Papotti"
                },
                {
                    "authorId": "2069605832",
                    "name": "Raghav Rao"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                },
                {
                    "authorId": "2792621",
                    "name": "Keishi Tajima"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "1825255212",
                    "name": "M. Tommasi"
                },
                {
                    "authorId": "49697753",
                    "name": "Kazutoshi Umemoto"
                },
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                },
                {
                    "authorId": "2034178749",
                    "name": "Koichiro Yoshida"
                }
            ]
        },
        {
            "paperId": "804f4cfede2f08501e15e5f9af7062489e924660",
            "title": "Fairness in Online Jobs: A Case Study on TaskRabbit and Google",
            "abstract": "Online job marketplaces are becoming very popular. Either jobs or people are ranked by algorithms. For example, Google and Facebook job search return a ranked list of jobs given a search query. TaskRabbit and Fiverr, on the other hand, produce rank-ings of workers for a given query. Qapa, an online marketplace, can be used to rank both workers and jobs. In this paper, we develop a unified framework for fairness to study ranking workers and jobs. We case study two particular sites: Google job search and TaskRabbit. Our framework addresses group fairness where groups are obtained with any combination of protected attributes. We define a measure for unfairness for a given group, query and location. We also define two generic fairness problems that we address in our framework: quantification, such as finding the k groups (resp., queries, locations) for which the site is most or least unfair, and comparison, such as finding the locations at which fairness between two groups differs from all locations, or finding the queries for which fairness at two locations differ from all queries. Since the number of groups, queries and locations can be arbitrarily large, we adapt Fagin top-k algorithms to address our fairness problems. To evaluate our framework, we run extensive experiments on two datasets crawled from TaskRabbit and Google job search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1410761195",
                    "name": "Ahmad Ghizzawi"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "51493818",
                    "name": "Emilie Hoareau"
                },
                {
                    "authorId": "1714010",
                    "name": "P. Mulhem"
                }
            ]
        },
        {
            "paperId": "e9702bead5f6df50862e4e3b4256d5014d55f2f9",
            "title": "Interactive Generation and Customization of Travel Packages for Individuals and Groups",
            "abstract": "We demonstrate SIMURGH, an interactive framework for generating customized travel packages (TPs) for individuals or for groups of travelers. This is beneficial in various use cases such as tourism planning and advertisement. Simurgh relies on gathering preferences of travelers and solving an optimization problem to generate personalized travel packages. SIMURGH goes beyond personalization by allowing travelers to customize travel packages via simple-yet-powerful interaction operators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "1678382",
                    "name": "Sruthi Viswanathan"
                }
            ]
        },
        {
            "paperId": "6abf8f373ada3e32b9f21ac5d032a6cb8833a153",
            "title": "On Benchmarking for Crowdsourcing and Future of Work Platforms",
            "abstract": "Online crowdsourcing platforms have proliferated over the last few years and cover a number of important domains, these platforms include from worker-task platforms such Amazon Mechanical Turk, worker-for-hire platforms such as TaskRabbit to specialized platforms with speci\ufb01c tasks such as ridesharing like Uber, Lyft, Ola etc. An increasing proportion of human workforce will be employed by these platforms in the near future. The crowdsourcing community has done yeoman\u2019s work in designing effective algorithms for various key components, such as incentive design, task assignment and quality control. Given the increasing importance of these crowdsourcing platforms, it is now time to design mechanisms so that it is easier to evaluate the effectiveness of these platforms. Speci\ufb01cally, we advocate developing benchmarks for crowdsourcing research. Benchmarks often identify important issues for the community to focus and improve upon. This has played a key role in the development of research domains as diverse as databases and deep learning. We believe that developing appropriate benchmarks for crowdsourcing will ignite further innovations. However, crowdsourcing \u2013 and future of work, in general \u2013 is a very diverse \ufb01eld that makes developing benchmarks much more challenging. Substantial effort is needed that spans across developing benchmarks for datasets, metrics, algorithms, platforms and so on. In this article, we initiate some discussion into this important problem and issue a call-to-arms for the community to work on this important initiative.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "2146072233",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144775006",
                    "name": "A. Dubey"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                }
            ]
        },
        {
            "paperId": "b11e165b8b6cec3bd07d1c53b17517d0a3da4ce3",
            "title": "GroupTravel: Customizing Travel Packages for Groups",
            "abstract": "We present GroupTravel, a framework that generates cus-tomized travel packages (TPs) for a group of individuals. GroupTravel implements different consensus functions proposed in group recommendation to reach agreement among members. Given a group whose members provide a travel query, GroupTravel returns k Composite Items (CIs) of Points Of Interest (POIs) that are valid, representative, cohesive and personalized. Validity is achieved by satisfying the query expressed by the group. Representativity ensures good coverage of a city. Cohesiveness reflects geographic proximity of POIs forming a CI. Personalization is achieved by choosing POIs that best match the travel preferences of group members. Additionally, group members can interact with generated TPs to customize them. With extensive synthetic experiments and user studies, we examine the benefit of personalization and the impact of different group consensus on user satisfaction. We also show that providing the ability to interact with TPs and reflecting that in the consensus yields better TPs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "1411354337",
                    "name": "Mehrdad Farokhnejad"
                }
            ]
        },
        {
            "paperId": "843b86e94ee75f40f458b73de6efb4b125e8e4ee",
            "title": "Personalized and Diverse Task Composition in Crowdsourcing",
            "abstract": "We study task composition in crowdsourcing and the effect of personalization and diversity on performance. A central process in crowdsourcing is task assignment, the mechanism through which workers find tasks. On popular platforms such as Amazon Mechanical Turk, task assignment is facilitated by the ability to sort tasks by dimensions such as creation date or reward amount. Task composition improves task assignment by producing for each worker, a personalized summary of tasks, referred to as a Composite Task (CT). We propose different ways of producing CTs and formulate an optimization problem that finds for a worker, the most relevant and diverse CTs. We show empirically that workers\u2019 experience is greatly improved due to personalization that enforces an adequation of CTs with workers\u2019 skills and preferences. We also study and formalize various ways of diversifying tasks in each CT. Task diversity is grounded in organization studies that have shown its impact on worker motivation\u00a0 [33] . Our experiments show that diverse CTs contribute to improving outcome quality. More specifically, we show that while task throughput and worker retention are best with ranked lists, crowdwork quality reaches its best with CTs diversified by requesters, thereby confirming that workers look to expose their \u201cgood\u201d work to many requesters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27052776",
                    "name": "Maha Alsayasneh"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "3416184",
                    "name": "Julien Pilourdault"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "48449806",
                    "name": "Motomichi Toyama"
                },
                {
                    "authorId": "2822140",
                    "name": "J. Renders"
                }
            ]
        },
        {
            "paperId": "4b199fb07e262c954e684fa5c127d48984393839",
            "title": "Process-based quality control techniques in crowdsourcing(\u5be9\u67fb\u5831\u544a)",
            "abstract": "Crowdsourcing has become a popular approach to complete tasks that are tedious using manual methods or difficult for automatic ones. As crowdsourcing taps the capacities of humans, its possibilities are endless. However, the unpredictability of human behavior and the actuality of human error make it difficult to consistently achieve high-quality outcomes, posing quality control as a major challenge in crowdsourcing. Although many strategies have been proposed to optimize data quality, their effectiveness is dependent on each particular crowdsourcing application. To solve this, more techniques and experiments from which humans and algorithms can learn from are needed to be able to build a recommendation system that proposes quality management techniques depending on the attributes of the crowdsourcing application. In this dissertation, I approach quality management in crowdsourcing based on the sub-processes involved, specifically: task design, task deployment, and task assignment. I first experimented on factors affecting task design. In particular, I tested the effect of task complexity on a data extraction task and crowd type on a sentiment analysis task. Experiments show that there is no significant difference in the quality achieved from simple and more complex versions of a data extraction task and that the performance of paid unpaid workers are comparable in a sentiment analysis task. For task deployment, task deployment strategies were proposed along three dimensions: work structure, workforce organization, and work style. To semi-automatically implement these strategies in a crowdsourcing platform, a deployment tool was designed and developed. The effectiveness of the strategies when applied to text creation tasks were then studied and recommendations were drafted for both crowdsourcing researchers and practitioners. Finally, for task assignment, a fuzzy clustering-based method for building a personalized summary of tasks, also known as composite tasks, for crowd workers was validated. As observed from the experiments, personalization improves the workers\u2019 overall experience and that diversifying tasks can improve the workers\u2019 output quality. Author: Ria Mae Harina Borromeo Supervisor: Motomichi Toyama",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                }
            ]
        }
    ]
}