{
    "authorId": "2134759582",
    "papers": [
        {
            "paperId": "19443d48399d4fe89a4b0a96917c50c6fd9c5af1",
            "title": "FLIRT: Feedback Loop In-context Red Teaming",
            "abstract": "Warning: this paper contains content that may be inappropriate or offensive. As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models, resulting in significantly higher toxic response generation rate compared to previously reported numbers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "34866641",
                    "name": "Christophe Dupuy"
                },
                {
                    "authorId": "2192042848",
                    "name": "Qian Hu"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "1804104",
                    "name": "R. Zemel"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                }
            ]
        },
        {
            "paperId": "318c6ef80e149340405d75ae73577208d6e30027",
            "title": "Domain Adaptation with External Off-Policy Acoustic Catalogs for Scalable Contextual End-to-End Automated Speech Recognition",
            "abstract": "Despite improvements to the generalization performance of automated speech recognition (ASR) models, specializing ASR models for downstream tasks remains a challenging task, primarily due to reduced data availability (necessitating increased data collection), and rapidly shifting data distributions (requiring more frequent model fine-tuning). In this work, we investigate the potential of leveraging external knowledge, particularly through off-policy generated text-to-speech key-value stores, to allow for flexible post-training adaptation to new data distributions. In our approach, audio embeddings captured from text-to-speech are used, along with semantic text embeddings, to bias ASR via an approximate k-nearest-neighbor (KNN) based attentive fusion step. Our experiments on LibiriSpeech and in-house voice assistant/search datasets show that the proposed approach can reduce domain adaptation time by up to 1K GPU-hours while providing up to 3% WER improvement compared to a fine-tuning baseline, suggesting a promising approach for adapting production ASR systems in challenging zero and few-shot scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152502885",
                    "name": "David Chan"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "3070896",
                    "name": "A. Rastrow"
                },
                {
                    "authorId": "145878402",
                    "name": "Bj\u00f6rn Hoffmeister"
                }
            ]
        },
        {
            "paperId": "56677526b80771dc553eebad3681ec2434cc9d93",
            "title": "Prune Then Distill: Dataset Distillation with Importance Sampling",
            "abstract": "The development of large datasets for various tasks has driven the success of deep learning models but at the cost of increased label noise, duplication, collection challenges, storage capabilities, and training requirements. In this work, we investigate whether all samples in large datasets contribute equally to better model accuracy. We study statistical and mathematical techniques to reduce redundancies in datasets by directly optimizing data samples for the generalization accuracy of deep learning models. Existing dataset optimization approaches include analytic methods that remove unimportant samples and synthetic methods that generate new datasets to maximize the generalization accuracy. We develop Prune then distill, a combination of analytic and synthetic dataset optimization algorithms, and demonstrate up to 15% relative improvement in generalization accuracy over either approach used independently on standard image and audio classification tasks. Additionally, we demonstrate up to 38% improvement in generalization accuracy of dataset pruning algorithms by maintaining class balance while pruning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161133253",
                    "name": "Anirudh S. Sundar"
                },
                {
                    "authorId": "2265391295",
                    "name": "G\u00f6k\u00e7e Keskin"
                },
                {
                    "authorId": "2979773",
                    "name": "Chander Chandak"
                },
                {
                    "authorId": "145905461",
                    "name": "I-Fan Chen"
                },
                {
                    "authorId": "2835722",
                    "name": "Pegah Ghahremani"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                }
            ]
        },
        {
            "paperId": "6975638e6a7ee1e403b3d76242b3ec7f127d91f6",
            "title": "Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data",
            "abstract": "Scaling up weakly-supervised datasets has shown to be highly effective in the image-text domain and has contributed to most of the recent state-of-the-art computer vision and multimodal neural networks. However, existing large-scale video-text datasets and mining techniques suffer from several limitations, such as the scarcity of aligned data, the lack of diversity in the data, and the difficulty of collecting aligned data. Currently popular video-text data mining approach via automatic speech recognition (ASR) used in HowTo100M provides low-quality captions that often do not refer to the video content. Other mining approaches do not provide proper language descriptions (video tags) and are biased toward short clips (alt text). In this work, we show how recent advances in image captioning allow us to pre-train high-quality video models without any parallel video-text data. We pre-train several video captioning models that are based on an OPT language model and a TimeSformer visual backbone. We fine-tune these networks on several video captioning datasets. First, we demonstrate that image captioning pseudolabels work better for pre-training than the existing HowTo100M ASR captions. Second, we show that pre-training on both images and videos produces a significantly better network (+4 CIDER on MSR- VTT) than pre-training on a single modality. Our methods are complementary to the existing pre-training or data mining approaches and can be used in a variety of settings. Given the efficacy of the pseudolabeling method, we are planning to publicly release the generated captions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1380253954",
                    "name": "Vladislav Lialin"
                },
                {
                    "authorId": "38696444",
                    "name": "Stephen Rawls"
                },
                {
                    "authorId": "152502885",
                    "name": "David Chan"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "1681193",
                    "name": "Anna Rumshisky"
                },
                {
                    "authorId": "1836135",
                    "name": "Wael Hamza"
                }
            ]
        },
        {
            "paperId": "d7b6b56c84b0005ac3b5d9c530b2a6f8def73145",
            "title": "Using External Off-Policy Speech-To-Text Mappings in Contextual End-To-End Automated Speech Recognition",
            "abstract": "Despite improvements to the generalization performance of automated speech recognition (ASR) models, specializing ASR models for downstream tasks remains a challenging task, primarily due to reduced data availability (necessitating increased data collection), and rapidly shifting data distributions (requiring more frequent model fine-tuning). In this work, we investigate the potential of leveraging external knowledge, particularly through off-policy key-value stores generated with text-to-speech methods, to allow for flexible post-training adaptation to new data distributions. In our approach, audio embeddings captured from text-to-speech, along with semantic text embeddings, are used to bias ASR via an approximate k-nearest-neighbor (KNN) based attentive fusion step. Our experiments on LibiriSpeech and in-house voice assistant/search datasets show that the proposed approach can reduce domain adaptation time by up to 1K GPU-hours while providing up to 3% WER improvement compared to a fine-tuning baseline, suggesting a promising approach for adapting production ASR systems in challenging zero and few-shot scenarios.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "152502885",
                    "name": "David Chan"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "3070896",
                    "name": "A. Rastrow"
                },
                {
                    "authorId": "145878402",
                    "name": "Bj\u00f6rn Hoffmeister"
                }
            ]
        },
        {
            "paperId": "10b59b888f3a9dd53b5ae393b5fd8bd9f82b215a",
            "title": "One-Stage Object Referring with Gaze Estimation",
            "abstract": "The classic object referring task aims at localizing the referred object in the image and requires a reference image and a natural language description as inputs. Given the facts that gaze signal can be easily obtained by a modern human-computer interaction system with a camera and that human tends to look at the object when referring to it, we propose a novel gaze-assisted object referring framework. The formulation not only simplifies the state-of-the-art gaze-assisted object referring system requiring many input signals besides gaze, but also incorporates the one-stage object detection idea to improve the inference efficiency. More importantly, it implicitly considers all object candidates and thus resolves the main pain point of existing two-stage object referring solutions for proposing an appropriate number of candidates \u2013 it cannot be too large, otherwise the computational cost can be prohibitive; it cannot be too small, otherwise the chance of missing a referred object can be significant. To utilize the gaze information, we propose to build a gaze heatmap by using the anchor position encoding map and the gaze prediction result. The gaze heatmap and the language feature are then merged into the feature pyramid in the object detection as the final one-stage referring system. In the CityScapes-OR dataset, the proposed method outperforms the state-of-the-art by 7.8% for Acc@1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30482318",
                    "name": "Jianhang Chen"
                },
                {
                    "authorId": "2115462987",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "2122374530",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                }
            ]
        },
        {
            "paperId": "1f61de3157dd73f5e73a9de57b7db14185c12358",
            "title": "Unified Modeling of Multi-Domain Multi-Device ASR Systems",
            "abstract": "Modern Automatic Speech Recognition (ASR) systems often use a portfolio of domain-specific models in order to get high accuracy for distinct user utterance types across different devices. In this paper, we propose an innovative approach that integrates the different per-domain per-device models into a unified model, using a combination of domain embedding, domain experts, mixture of experts and adversarial training. We run careful ablation studies to show the benefit of each of these innovations in contributing to the accuracy of the overall unified model. Experiments show that our proposed unified modeling approach actually outperforms the carefully tuned per-domain models, giving relative gains of up to 10% over a baseline model with negligible increase in the number of parameters.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "46231241",
                    "name": "Soumyajit Mitra"
                },
                {
                    "authorId": "51124292",
                    "name": "Swayambhu Nath Ray"
                },
                {
                    "authorId": "100727669",
                    "name": "Bharat Padi"
                },
                {
                    "authorId": "3012222",
                    "name": "Raghavendra Bilgi"
                },
                {
                    "authorId": "1801996",
                    "name": "Harish Arsikere"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "1863914",
                    "name": "A. Srinivasamurthy"
                },
                {
                    "authorId": "145400624",
                    "name": "S. Garimella"
                }
            ]
        },
        {
            "paperId": "8aa6b0341c46a31b92427f7448dbf0d804806715",
            "title": "Disentangled Action Recognition with Knowledge Bases",
            "abstract": "Action in video usually involves the interaction of human with objects. Action labels are typically composed of various combinations of verbs and nouns, but we may not have training data for all possible combinations. In this paper, we aim to improve the generalization ability of the compositional action recognition model to novel verbs or novel nouns that are unseen during training time, by leveraging the power of knowledge graphs. Previous work utilizes verb-noun compositional action nodes in the knowledge graph, making it inefficient to scale since the number of compositional action nodes grows quadratically with respect to the number of verbs and nouns. To address this issue, we propose our approach: Disentangled Action Recognition with Knowledge-bases (DARK), which leverages the inherent compositionality of actions. DARK trains a factorized model by first extracting disentangled feature representations for verbs and nouns, and then predicting classification weights using relations in external knowledge graphs. The type constraint between verb and noun is extracted from external knowledge bases and finally applied when composing actions. DARK has better scalability in the number of objects and verbs, and achieves state-of-the-art performance on the Charades dataset. We further propose a new benchmark split based on the Epic-kitchen dataset which is an order of magnitude bigger in the numbers of classes and samples, and benchmark various models on this benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1603002784",
                    "name": "Zhekun Luo"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "3493957",
                    "name": "Devin Guillory"
                },
                {
                    "authorId": "2110285393",
                    "name": "Keizo Kato"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "46485395",
                    "name": "Huijuan Xu"
                }
            ]
        },
        {
            "paperId": "8e565cc901b25e3ccafb758178f925b98b849299",
            "title": "Content-Context Factorized Representations for Automated Speech Recognition",
            "abstract": "Deep neural networks have largely demonstrated their ability to perform automated speech recognition (ASR) by extracting meaningful features from input audio frames. Such features, however, may consist not only of information about the spoken language content, but also may contain information about unnecessary contexts such as background noise and sounds or speaker identity, accent, or protected attributes. Such information can directly harm generalization performance, by introducing spurious correlations between the spoken words and the context in which such words were spoken. In this work, we introduce an unsupervised, encoder-agnostic method for factoring speech-encoder representations into explicit content-encoding representations and spurious context-encoding representations. By doing so, we demonstrate improved performance on standard ASR benchmarks, as well as improved performance in both real-world and artificially noisy ASR scenarios.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152502885",
                    "name": "David Chan"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                }
            ]
        },
        {
            "paperId": "a3afdb4bcbd86874acdda1f0ff84dfafd8cfad58",
            "title": "Multi-Modal Pre-Training for Automated Speech Recognition",
            "abstract": "Traditionally, research in automated speech recognition has focused on local-first encoding of audio representations to predict the spoken phonemes in an utterance. Unfortunately, approaches relying on such hyper-local information tend to be vulnerable to both local-level corruption (such as audio-frame drops, or loud noises) and global-level noise (such as environmental noise, or background noise) that has not been seen during training. In this work, we introduce a novel approach that leverages a self-supervised learning technique based on masked language modeling to compute a global, multi-modal encoding of the environment in which the utterance occurs. We then use a new deep-fusion framework to integrate this global context into a traditional ASR method, and demonstrate that the resulting method can outperform baseline methods by up to 7% on Librispeech; gains on internal datasets range from 6% (on larger models) to 45% (on smaller models).",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "152502885",
                    "name": "David Chan"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "1923031",
                    "name": "D. Chakrabarty"
                },
                {
                    "authorId": "145878402",
                    "name": "Bj\u00f6rn Hoffmeister"
                }
            ]
        }
    ]
}