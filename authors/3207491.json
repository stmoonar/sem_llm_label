{
    "authorId": "3207491",
    "papers": [
        {
            "paperId": "981691abcfc3ad075f38bb86946cf8d4bca8d050",
            "title": "Cross-Domain Character Recognition through Latent Space Alignment",
            "abstract": "Deep neural networks have proved its capability in many machine learning tasks. The effectiveness of deep neural networks in real-world applications, however, is greatly affected by the distribution discrepancy between the training and testing data. To address the issue, domain adaptation methods have been studied. In this work, we propose a novel unsupervised domain adaptation method which combines the feature learning and the distribution estimation into one learning framework, enabling automatic update of feature representations through fine-tuning parameterized distributions. As such, our model can produce an unified distribution to represent both source and target samples. Furthermore, two new regularizers are integrated into the optimization objective to minimize the divergence of the unified distribution from those of source and target domains. Experiments on character reconstruction show that our method demonstrates much better learning ability compared to the existing variational autoencoder. More importantly, our method improves recognition accuracy by more than 5% from that of the state-of-the-art methods in domain adaptation tasks built upon popular character datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2152122353",
                    "name": "Zexu Du"
                },
                {
                    "authorId": "2055055020",
                    "name": "Jinrui Gan"
                },
                {
                    "authorId": "2116019811",
                    "name": "Guoliang Zhang"
                },
                {
                    "authorId": "2153604287",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2153691345",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "47561503",
                    "name": "Peng Wu"
                },
                {
                    "authorId": "2190851126",
                    "name": "Fei Zhou"
                }
            ]
        },
        {
            "paperId": "9b687f677165a8a0d49a533c45baf17c607d1648",
            "title": "Cycle Self-Training for Semi-Supervised Object Detection with Distribution Consistency Reweighting",
            "abstract": "Recently, many semi-supervised object detection (SSOD) methods adopt teacher-student framework and have achieved state-of-the-art results. However, the teacher network is tightly coupled with the student network since the teacher is an exponential moving average (EMA) of the student, which causes a performance bottleneck. To address the coupling problem, we propose a Cycle Self-Training (CST) framework for SSOD, which consists of two teachers T1 and T2, two students S1 and S2. Based on these networks, a cycle self-training mechanism is built, i.e., S1$\\rightarrow $T1$\\rightarrow $S2$\\rightarrow $T2$\\rightarrow $S1. For S$\\rightarrow $T, we also utilize the EMA weights of the students to update the teachers. For T$\\rightarrow $S, instead of providing supervision for its own student S1(S2) directly, the teacher T1(T2) generates pseudo-labels for the student S2(S1), which looses the coupling effect. Moreover, owing to the property of EMA, the teacher is most likely to accumulate the biases from the student and make the mistakes irreversible. To mitigate the problem, we also propose a distribution consistency reweighting strategy, where pseudo-labels are reweighted based on distribution consistency across the teachers T1 and T2. With the strategy, the two students S2 and S1 can be trained robustly with noisy pseudo labels to avoid confirmation biases. Extensive experiments prove the superiority of CST by consistently improving the AP over the baseline and outperforming state-of-the-art methods by 2.1% absolute AP improvements with scarce labeled data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143855875",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "2146711404",
                    "name": "Bin Chen"
                },
                {
                    "authorId": "2153215531",
                    "name": "Bo Wang"
                },
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2175787349",
                    "name": "Feng Dai"
                },
                {
                    "authorId": "2111193177",
                    "name": "Peng Wu"
                }
            ]
        },
        {
            "paperId": "1bb958e0d6ce9ddedaebe71e42bf627729a80afb",
            "title": "Adversarial Attack: A New Threat to Smart Devices and How to Defend It",
            "abstract": "This article introduces adversarial attack, a recently-unveiled security threat to consumer electronics, especially those utilizing machine learning techniques. We start with the fundamental knowledge including what are adversarial examples, how to realize such attacks, and common defense methods. Adversarial training enhances models\u2019 resilience to adversarial attacks by taking both regular and adversarial examples for training. However, applying adversarial examples under a single adversarial strength provide defense in a very limited effective range. We propose a multiple-strength adversarial training method. A random walk algorithm is adopted to optimize the selection of adversarial strengths, which is closely related to the design cost and training time. We also analyze the hardware cost and quantization loss to guide future consumer electronics designs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143774361",
                    "name": "Chang Song"
                },
                {
                    "authorId": "3461946",
                    "name": "Hsin-Pai Cheng"
                },
                {
                    "authorId": "47912363",
                    "name": "Huanrui Yang"
                },
                {
                    "authorId": "3285742",
                    "name": "Sicheng Li"
                },
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2245553048",
                    "name": "Qing Wu"
                },
                {
                    "authorId": "51208601",
                    "name": "H. Li"
                }
            ]
        },
        {
            "paperId": "259adc07652e0c0d31f7d60fa5c6d1a9f040f780",
            "title": "Encoding Text Information with Graph Convolutional Networks for Personality Recognition",
            "abstract": "Personality recognition is a classic and important problem in social engineering. Due to the small number and particularity of personality recognition databases, only limited research has explored convolutional neural networks for this task. In this paper, we explore the use of graph convolutional network techniques for inferring a user\u2019s personality traits from their Facebook status updates or essay information. Since the basic five personality traits (such as openness) and their aspects (such as status information) are related to a wide range of text features, this work takes the Big Five personality model as the core of the study. We construct a single user personality graph for the corpus based on user-document relations, document-word relations, and word co-occurrence and then learn the personality graph convolutional networks (personality GCN) for the user. The parameters or the inputs of our personality GCN are initialized with a one-hot representation for users, words and documents; then, under the supervision of users and documents with known class labels, it jointly learns the embeddings for users, words, and documents. We used feature information sharing to incorporate the correlation between the five personality traits into personality recognition to perfect the personality GCN. Our experimental results on two public and authoritative benchmark datasets show that the general personality GCN without any external word embeddings or knowledge is superior to the state-of-the-art methods for personality recognition. The personality GCN method is efficient on small datasets, and the average F1-score and accuracy of personality recognition are improved by up to approximately 3.6% and 2.4\u20132.57%, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Zhe Wang"
                },
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2108053899",
                    "name": "Qingbiao Li"
                },
                {
                    "authorId": "2075431363",
                    "name": "Bo Yan"
                },
                {
                    "authorId": "3156914",
                    "name": "K. Zheng"
                }
            ]
        },
        {
            "paperId": "39afc0ef66f9853e897bd2c67685190f4ae27689",
            "title": "Redistributing and Re-Stylizing Features for Training a Fast Photorealistic Stylizer",
            "abstract": "Style transfer studies can be categorized into two types\u2014artistic and photorealistic. The high-speed transfer has been well-studied for artistic styles but remains challenging for photorealistic styles. To guarantee semantic accuracy and style faithfulness, prior photorealistic style transfer techniques often rely on intensive feature matching, hierarchical stylization, and complex auxiliary smoothing. Such high design complexity severely limits the space of transfer speed improvement. In this paper, we propose to accelerate the transfer through a single-level stylization without complex auxiliary smoothing. We design a two-stage \"stylization and re-stylization\" training pipeline to enhance style faithfulness. The stylization/re-stylization stage consists of two core steps: feature aggregation and redistribution. A new type of layers, Feature Aggregation (FA) layers, is proposed to gradually aggregate multi-scale style features into content features at each spatial location. A Spatially coherent Content-style Preserving (SCP) loss at feature map level is then used to preserve semantic accuracy. The SCP loss provides effective guidance on redistributing the aggregated features between locations to enforce spatial coherence of style-sensitive content semantic. Experimental results show that compared to previous competitive methods, our method reduces at least 72% run time while achieving better image synthesis quality based on both subjective and objective evaluation metrics. Ablation studies validate the major contribution of our proposed SCP loss and re-stylization to the quality of our synthesized images.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2064407915",
                    "name": "Bin Ni"
                },
                {
                    "authorId": "51208601",
                    "name": "H. Li"
                }
            ]
        },
        {
            "paperId": "639bf103501714dd00476b3b0e71c4e273aceeb9",
            "title": "Regularized Training and Tight Certification for Randomized Smoothed Classifier with Provable Robustness",
            "abstract": "Recently smoothing deep neural network based classifiers via isotropic Gaussian perturbation is shown to be an effective and scalable way to provide state-of-the-art probabilistic robustness guarantee against \u21132 norm bounded adversarial perturbations. However, how to train a good base classifier that is accurate and robust when smoothed has not been fully investigated. In this work, we derive a new regularized risk, in which the regularizer can adaptively encourage the accuracy and robustness of the smoothed counterpart when training the base classifier. It is computationally efficient and can be implemented in parallel with other empirical defense methods. We discuss how to implement it under both standard (non-adversarial) and adversarial training scheme. At the same time, we also design a new certification algorithm, which can leverage the regularization effect to provide tighter robustness lower bound that holds with high probability. Our extensive experimentation demonstrates the effectiveness of the proposed training and certification approaches on CIFAR-10 and ImageNet datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "49876941",
                    "name": "Huijie Feng"
                },
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2321965",
                    "name": "Guoyang Chen"
                },
                {
                    "authorId": "49039449",
                    "name": "Weifeng Zhang"
                },
                {
                    "authorId": "50318386",
                    "name": "Y. Ning"
                }
            ]
        },
        {
            "paperId": "830f88fcd1cb0ca3779ae02d421a2c2328281410",
            "title": "MVStylizer: an efficient edge-assisted video photorealistic style transfer system for mobile phones",
            "abstract": "Recent research has made great progress in realizing neural style transfer of images, which denotes transforming an image to a desired style. Many users start to use their mobile phones to record their daily life, and then edit and share the captured images and videos with other users. However, directly applying existing style transfer approaches on videos, i.e., transferring the style of a video frame by frame, requires an extremely large amount of computation resources. It is still technically unaffordable to perform style transfer of videos on mobile phones. To address this challenge, we propose MVStylizer, an efficient edge-assisted photorealistic video style transfer system for mobile phones. Instead of performing stylization frame by frame, only key frames in the original video are processed by a pre-trained deep neural network (DNN) on edge servers, while the rest of stylized intermediate frames are generated by our designed optical-flow-based frame interpolation algorithm on mobile phones. A meta-smoothing module is also proposed to simultaneously upscale a stylized frame to arbitrary resolution and remove style transfer related distortions in these upscaled frames. In addition, for the sake of continuously enhancing the performance of the DNN model on the edge server, we adopt a federated learning scheme to keep retraining each DNN model on the edge server with collected data from mobile clients and syncing with a global DNN model on the cloud server. Such a scheme effectively leverages the diversity of collected data from various mobile clients and efficiently improves the system performance. Our experiments demonstrate that MVStylizer can generate stylized videos with an even better visual quality compared to the state-of-the-art method while achieving 75.5\u00d7 speedup for 1920\u00d71080 videos.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2108667778",
                    "name": "Ang Li"
                },
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2108952137",
                    "name": "Yiran Chen"
                },
                {
                    "authorId": "2064407915",
                    "name": "Bin Ni"
                }
            ]
        },
        {
            "paperId": "3e3f3a89043e304612977a15dfa025a3e67f812d",
            "title": "An efficient mobile-edge collaborative system for video photorealistic style transfer",
            "abstract": "In the past decade, convolutional neural networks (CNNs) have achieved great practical success in image transformation tasks, including style transfer, semantic segmentation, etc. CNN-based style transfer, which denotes transforming an image into a desired output image according to a user-specified style image, is one of the most popular techniques in image transformation. It has led to to many successful industrial applications with significant commercial impacts, such as Prisma and DeepArt. Figure 1 shows the general workflow of the CNN-based style transfer. Given a content image and a user-specified style image, the content features and style features can be extracted using a pre-trained CNN, and then be merged to generate the stylized image. The CNN model is trained for generating a stylized image that has similar content features as the content image's and similar style features as the style image's. In this example, we can see the content image is captured at a lake in the daytime, while the style image is another similar scene captured at dusk. After performing style transfer, the content image is successfully transformed to the dusky scene while keeping the content unchanged as the content image.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108667778",
                    "name": "Ang Li"
                },
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2108952137",
                    "name": "Yiran Chen"
                },
                {
                    "authorId": "2064407915",
                    "name": "Bin Ni"
                }
            ]
        },
        {
            "paperId": "99bf7734663031a0c00a545cc53075f8bb160b28",
            "title": "Efficiently Learning a Robust Self-Driving Model with Neuron Coverage Aware Adaptive Filter Reuse",
            "abstract": "Human drivers learn driving skills from both regular (non-accidental) and accidental driving experiences, while most of current self-driving research focuses on regular driving only. We argue that learning from accidental driving data is necessary for robustly modeling driving behavior. A main challenge, however, is how accident data can be effectively used together with regular data to learn vehicle motion, since manually labeling accident data without expertise is significantly difficult. Our proposed solution for robust vehicle motion learning, in this paper, is to integrate layer-level discriminability and neuron coverage(neuron-level robustness) regulariziers into an unsupervised generative network for video prediction. Layer-level discriminability increases divergence of feature distribution between the regular data and accident data at network layers. Neuron coverage regulariziers enlarge interval span of neuron activation adopted by training samples, to reduce probability that a sample falls into untested interval regions. To accelerate training process, we propose adaptive filter reuse based on neuron coverage. Our strategies of filter reuse reduce structural network parameters, encourage memory reuse, and guarantee effectiveness of robust vehicle motion learning. Experiments results show that our model improves the inference accuracy by 1.1% compared to FCMLSTM, and cut down 10.2% training time over the traditional method with negligible accuracy loss.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2108667778",
                    "name": "Ang Li"
                },
                {
                    "authorId": "2152691751",
                    "name": "Bing Li"
                },
                {
                    "authorId": "2108952137",
                    "name": "Yiran Chen"
                }
            ]
        },
        {
            "paperId": "f96c87d5c6b60b7a9d5b75d79c2f3d6b46ba5da0",
            "title": "Conditional Transferring Features: Scaling GANs to Thousands of Classes with 30% Less High-Quality Data for Training",
            "abstract": "Generative adversarial network (GAN) can greatly improve the quality of unsupervised image generation. Previous GAN-based methods often require a large amount of high-quality training data. This work aims to reduce the use of high-quality data in training, meanwhile scaling up GANs to thousands of classes. We propose an image generation method based on conditional transferring features, which can capture pixel-level semantic changes when transforming low-quality images into high-quality ones. Self-supervision learning is then integrated into our GAN architecture to provide more label-free semantic supervisory information observed from the training data. As such, training our GAN architecture requires much fewer high-quality images with a small number of additional low-quality images. Experiments show that even removing 30% high-quality images from the training set, our method can still achieve better image synthesis quality on CIFAR-10, STL-10, ImageNet, and CASIA-HWDB1.0, compared to previous competitive methods. Experiments on ImageNet with 1,000 classes of images and CASIA-HWDB1.0 with 3,755 classes of Chinese handwriting characters also validate the scalability of our method on object classes. Ablation studies further validate the contribution of our conditional transferring features and self-supervision learning to the quality of our synthesized images.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "145225262",
                    "name": "W. Wen"
                },
                {
                    "authorId": "5442167",
                    "name": "Yiran Chen"
                },
                {
                    "authorId": "51208601",
                    "name": "H. Li"
                }
            ]
        }
    ]
}