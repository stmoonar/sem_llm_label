{
    "authorId": "144255847",
    "papers": [
        {
            "paperId": "5f918647e612b7b0fbeb95b588c51bc662f5e1c2",
            "title": "StreamEnsemble: Predictive Queries over Spatiotemporal Streaming Data",
            "abstract": "Predictive queries over spatiotemporal (ST) stream data pose significant data processing and analysis challenges. ST data streams involve a set of time series whose data distributions may vary in space and time, exhibiting multiple distinct patterns. In this context, assuming a single machine learning model would adequately handle such variations is likely to lead to failure. To address this challenge, we propose StreamEnsemble, a novel approach to predictive queries over ST data that dynamically selects and allocates Machine Learning models according to the underlying time series distributions and model characteristics. Our experimental evaluation reveals that this method markedly outperforms traditional ensemble methods and single model approaches in terms of accuracy and time, demonstrating a significant reduction in prediction error of more than 10 times compared to traditional approaches.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2323785665",
                    "name": "Anderson Chaves"
                },
                {
                    "authorId": "2176421587",
                    "name": "Eduardo S. Ogasawara"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "2321932089",
                    "name": "Fabio Porto"
                }
            ]
        },
        {
            "paperId": "871b47aca63b0e7a0e9828f1f55ecf6b1537a6df",
            "title": "Subset Models for Multivariate Time Series Forecast",
            "abstract": "Multivariate time series find extensive applications in conjunction with machine learning methodologies for scenario forecasting across various domains. Nevertheless, certain domains exhibit inherent complexities and diversities, which detrimentally impact the predictive efficacy of global models. This ongoing study introduces a Subset Modeling Framework designed to acknowledge the inherent diversity within a domain\u2019s multivariate space. Comparative assessments between subset models and global models are conducted in terms of performance, revealing compelling findings and suggesting the potential for further exploration and refinement of this novel framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2255436137",
                    "name": "Raphael Saldanha"
                },
                {
                    "authorId": "2283565076",
                    "name": "Victor Ribeiro"
                },
                {
                    "authorId": "2255545324",
                    "name": "Eduardo H. M. Pena"
                },
                {
                    "authorId": "2283563757",
                    "name": "Marcel Pedroso"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "2255565502",
                    "name": "F\u00e1bio Porto"
                }
            ]
        },
        {
            "paperId": "0385d39a5f32175b9007bd8034d25f9207f87857",
            "title": "Subset Modelling: A Domain Partitioning Strategy for Data-efficient Machine-Learning",
            "abstract": "The success of machine learning (ML) systems depends on data availability, volume, quality, and efficient computing resources. A challenge in this context is to reduce computational costs while maintaining adequate accuracy of the models. This paper presents a framework to address this challenge. The idea is to identify \u201csubdomains\u201d within the input space, train local models that produce better predictions for samples from that specific subdomain, instead of training a single global model on the full dataset. We experimentally evaluate our approach on two real-world datasets. Our results indicate that subset modelling (i) improves the predictive performance compared to a single global model and (ii) allows data-efficient training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2255558225",
                    "name": "Vitor Ribeiro"
                },
                {
                    "authorId": "2255545324",
                    "name": "Eduardo H. M. Pena"
                },
                {
                    "authorId": "2255436137",
                    "name": "Raphael Saldanha"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1410017982",
                    "name": "Falaah Arif Khan"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "2255565502",
                    "name": "F\u00e1bio Porto"
                }
            ]
        },
        {
            "paperId": "0a4f7757e60ca0b8e52ff9df7f0663e5c37eb937",
            "title": "ProvLight: Efficient Workflow Provenance Capture on the Edge-to-Cloud Continuum",
            "abstract": "Modern scientific workflows require hybrid infrastructures combining numerous decentralized resources on the IoT/Edge interconnected to Cloud/HPC systems (aka the Computing Continuum) to enable their optimized execution. Understanding and optimizing the performance of such complex Edge-to-Cloud workflows is challenging. Capturing the provenance of key performance indicators, with their related data and processes, may assist in understanding and optimizing workflow executions. However, the capture overhead can be prohibitive, particularly in resource-constrained devices, such as the ones on the IoT/Edge.To address this challenge, based on a performance analysis of existing systems, we propose ProvLight, a tool to enable efficient provenance capture on the IoT/Edge. We leverage simplified data models, data compression and grouping, and lightweight transmission protocols to reduce overheads. We further integrate ProvLight into the E2Clab framework to enable workflow provenance capture across the Edge-to-Cloud Continuum. This integration makes E2Clab a promising platform for the performance optimization of applications through reproducible experiments.We validate ProvLight at a large scale with synthetic workloads on 64 real-life IoT/Edge devices in the FIT IoT LAB testbed. Evaluations show that ProvLight outperforms state-of-the-art systems like ProvLake and DfAnalyzer in resource-constrained devices. ProvLight is 26\u201437x faster to capture and transmit provenance data; uses 5\u20147x less CPU; 2x less memory; transmits 2x less data; and consumes 2\u20142.5x less energy. ProvLight [1] and E2Clab [2] are available as open-source tools.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21168032",
                    "name": "Daniel Rosendo"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                },
                {
                    "authorId": "1726606",
                    "name": "Alexandru Costan"
                },
                {
                    "authorId": "144712722",
                    "name": "Renan Souza"
                },
                {
                    "authorId": "2120208943",
                    "name": "D\u00e9bora B. Pina"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1779139",
                    "name": "Gabriel Antoniu"
                }
            ]
        },
        {
            "paperId": "5a5a8cfd442ecc160ed85b4c8cf478f1789d23a7",
            "title": "KheOps: Cost-effective Repeatability, Reproducibility, and Replicability of Edge-to-Cloud Experiments",
            "abstract": "Distributed infrastructures for computation and analytics are now evolving towards an interconnected ecosystem allowing complex scientific workflows to be executed across hybrid systems spanning from IoT Edge devices to Clouds, and sometimes to supercomputers (the Computing Continuum). Understanding the performance trade-offs of large-scale workflows deployed on such complex Edge-to-Cloud Continuum is challenging. To achieve this, one needs to systematically perform experiments, to enable their reproducibility and allow other researchers to replicate the study and the obtained conclusions on different infrastructures. This breaks down to the tedious process of reconciling the numerous experimental requirements and constraints with low-level infrastructure design choices. To address the limitations of the main state-of-the-art approaches for distributed, collaborative experimentation, such as Google Colab, Kaggle, and Code Ocean, we propose KheOps, a collaborative environment specifically designed to enable cost-effective reproducibility and replicability of Edge-to-Cloud experiments. KheOps is composed of three core elements: (1) an experiment repository; (2) a notebook environment; and (3) a multi-platform experiment methodology. We illustrate KheOps with a real-life Edge-to-Cloud application. The evaluations explore the point of view of the authors of an experiment described in an article (who aim to make their experiments reproducible) and the perspective of their readers (who aim to replicate the experiment). The results show how KheOps helps authors to systematically perform repeatable and reproducible experiments on the Grid5000 + FIT IoT LAB testbeds. Furthermore, KheOps helps readers to cost-effectively replicate authors experiments in different infrastructures such as Chameleon Cloud + CHI@Edge testbeds, and obtain the same conclusions with high accuracies (> 88% for all performance metrics).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21168032",
                    "name": "Daniel Rosendo"
                },
                {
                    "authorId": "50110206",
                    "name": "K. Keahey"
                },
                {
                    "authorId": "1726606",
                    "name": "Alexandru Costan"
                },
                {
                    "authorId": "2122228",
                    "name": "Matthieu Simonin"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1779139",
                    "name": "Gabriel Antoniu"
                }
            ]
        },
        {
            "paperId": "dd249ccbdab518dc9c02579dd3f5b0a44dd3d558",
            "title": "AEDFL: Efficient Asynchronous Decentralized Federated Learning with Heterogeneous Devices",
            "abstract": "Federated Learning (FL) has achieved significant achievements recently, enabling collaborative model training on distributed data over edge devices. Iterative gradient or model exchanges between devices and the centralized server in the standard FL paradigm suffer from severe efficiency bottlenecks on the server. While enabling collaborative training without a central server, existing decentralized FL approaches either focus on the synchronous mechanism that deteriorates FL convergence or ignore device staleness with an asynchronous mechanism, resulting in inferior FL accuracy. In this paper, we propose an Asynchronous Efficient Decentralized FL framework, i.e., AEDFL, in heterogeneous environments with three unique contributions. First, we propose an asynchronous FL system model with an efficient model aggregation method for improving the FL convergence. Second, we propose a dynamic staleness-aware model update approach to achieve superior accuracy. Third, we propose an adaptive sparse training method to reduce communication and computation costs without significant accuracy degradation. Extensive experimentation on four public datasets and four models demonstrates the strength of AEDFL in terms of accuracy (up to 16.3% higher), efficiency (up to 92.9% faster), and computation costs (up to 42.3% lower).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118971193",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "2119145833",
                    "name": "Tianshi Che"
                },
                {
                    "authorId": "2261386556",
                    "name": "Yang Zhou"
                },
                {
                    "authorId": "2275201759",
                    "name": "Ruoming Jin"
                },
                {
                    "authorId": "2261363858",
                    "name": "H. Dai"
                },
                {
                    "authorId": "2246391421",
                    "name": "Dejing Dou"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "233042f31b64f2ace7bf5196d03f185ccddf4677",
            "title": "Two-Phase Scheduling for Efficient Vehicle Sharing",
            "abstract": "Cooperative Intelligent Transport Systems (C-ITS) is a promising technology to make transportation safer and more efficient. Ridesharing for long-distance is becoming a key means of transportation in C-ITS. In this paper, we focus on private long-distance ridesharing, which reduces the total cost of vehicle utilization for long-distance journeys. In this context, we investigate journey scheduling problem with shared vehicles to reduce the total cost of vehicle utilization. Most of the existing works directly schedule journeys to vehicles with long scheduling time and only consider the cost of driving travellers instead of the total cost. In contrast, to reduce the total cost and scheduling time, we propose a comprehensive cost model and a two-phase journey scheduling approach, which includes path generation and path scheduling. On this basis, we propose two path generation methods: a simple near optimal method and a reset near optimal method as well as a greedy based path scheduling method. Finally, we present an experimental evaluation with different path generation and path scheduling methods with synthetic data generated based on real-world data. The results reveal that the proposed scheduling approach significantly outperforms baseline methods in terms of total cost (up to 69.8%) and scheduling time (up to 84.0%) and the scheduling time is reasonable (up to 0.16s). The results also show that our approach has higher efficiency (up to 141.7%) than baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108410330",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "2015216",
                    "name": "Carlyna Bondiombouy"
                },
                {
                    "authorId": "144137533",
                    "name": "L. Mo"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                }
            ]
        },
        {
            "paperId": "74b9d344cecaa9c3d7fd10ad22aa52479a89ac87",
            "title": "A Data-Driven Model Selection Approach to Spatio-Temporal Prediction",
            "abstract": "Spatio-temporal Predictive Queries encompass a spatio-temporal constraint, defining a region, a target variable, and an evaluation metric. The output of such queries presents the future values for the target variable computed by predictive models at each point of the spatio-temporal region. Unfortunately, especially for large spatio-temporal domains with millions of points, training temporal models at each spatial domain point is prohibitive. In this work, we propose a data-driven approach for selecting pre-trained temporal models to be applied at each query point. The chosen approach applies a model to a point according to the training and input time series similarity. The approach avoids training a different model for each domain point, saving model training time. Moreover, it provides a technique to decide on the best-trained model to be applied to a point for prediction. In order to assess the applicability of the proposed strategy, we evaluate a case study for temperature forecasting using historical data and auto-regressive models. Computational experiments show that the proposed approach, compared to the baseline, achieves equivalent predictive performance using a composition of pre-trained models at a fraction of the total computational cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065983908",
                    "name": "Roc\u00edo Zorrilla"
                },
                {
                    "authorId": "5628621",
                    "name": "Eduardo Ogasawara"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "2145364345",
                    "name": "F\u00e1bio Porto"
                }
            ]
        },
        {
            "paperId": "f23433216ad07ce8ef7401f26d24cf6dee8c5f22",
            "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources",
            "abstract": "Although more layers and more parameters generally improve the accuracy of the models, such big models generally have high computational complexity and require big memory, which exceed the capacity of small devices for inference and incurs long training time. In addition, it is difficult to afford long training time and inference time of big models even in high performance servers, as well. As an efficient approach to compress a large deep model (a teacher model) to a compact model (a student model), knowledge distillation emerges as a promising approach to deal with the big models. Existing knowledge distillation methods cannot exploit the elastic available computing resources and correspond to low efficiency. In this paper, we propose an Elastic Deep Learning framework for knowledge Distillation, that is, EDL\u2010Dist. The advantages of EDL\u2010Dist are threefold. First, the inference and the training process is separated. Second, elastic available computing resources can be utilized to improve the efficiency. Third, fault\u2010tolerance of the training and inference processes is supported. We take extensive experimentation to show that the throughput of EDL\u2010Dist is up to 3.125 times faster than the baseline method (online knowledge distillation) while the accuracy is similar or higher.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118971193",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "9532787",
                    "name": "Daxiang Dong"
                },
                {
                    "authorId": "2108249583",
                    "name": "Xi Wang"
                },
                {
                    "authorId": "2140532375",
                    "name": "An Qin"
                },
                {
                    "authorId": "2155445773",
                    "name": "Xingjian Li"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1721158",
                    "name": "D. Dou"
                },
                {
                    "authorId": "3046102",
                    "name": "Dianhai Yu"
                }
            ]
        },
        {
            "paperId": "35820f5e138372de915e1170fb7136e1cb955fc0",
            "title": "Online Deep Learning Hyperparameter Tuning based on Provenance Analysis",
            "abstract": "Training Deep Learning (DL) models require adjusting a series of hyperparameters. Although there are several tools to automatically choose the best hyperparameter configuration, the user is still the main actor to take the final decision. To decide whether the training should continue or try different configurations, the user needs to analyze online the hyperparameters most adequate to the training dataset, observing metrics such as accuracy and loss values. Provenance naturally represents data derivation relationships (i.e., transformations, parameter values, etc.), which provide important support in this data analysis. Most of the existing provenance solutions define their own and proprietary data representations to support DL users in choosing the best hyperparameter configuration, which makes data analysis and interoperability difficult. We present Keras-Prov and its extension, named Keras-Prov++, which provides an analytical dashboard to support online hyperparameter fine-tuning. Different from the current mainstream solutions, Keras-Prov automatically captures the provenance data of DL applications using the W3C PROV recommendation, allowing for hyperparameter online analysis to help the user deciding on changing hyperparameters\u2019 values after observing the performance of the models on a validation set. We provide an experimental evaluation of Keras-Prov++ using AlexNet and a real case study, named DenseED, that acts as a surrogate model for solving equations. During the online analysis, the users identify scenarios that suggest reducing the number of epochs to avoid unnecessary executions and fine-tuning the learning rate to improve the model accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2098334296",
                    "name": "L. Kunstmann"
                },
                {
                    "authorId": "2120208943",
                    "name": "D\u00e9bora B. Pina"
                },
                {
                    "authorId": "2148235427",
                    "name": "Filipe Silva"
                },
                {
                    "authorId": "144707454",
                    "name": "Aline Paes"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "2315401825",
                    "name": "Daniel de Oliveira"
                },
                {
                    "authorId": "1763201",
                    "name": "M. Mattoso"
                }
            ]
        }
    ]
}