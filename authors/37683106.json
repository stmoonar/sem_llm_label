{
    "authorId": "37683106",
    "papers": [
        {
            "paperId": "38d37fc0e471b7e7ade321bc839c04f4ab28a65a",
            "title": "MMoOn Core - the Multilingual Morpheme Ontology",
            "abstract": "In the last years a rapid emergence of lexical resources has evolved in the Semantic Web. Whereas most of the linguistic information is already machine-readable, we found that morphological information is mostly absent or only contained in semi-structured strings. An integration of morphemic data has not yet been undertaken due to the lack of existing domain-specific ontologies and explicit morphemic data. In this paper, we present the Multilingual Morpheme Ontology called MMoOn Core which can be regarded as the first comprehensive ontology for the linguistic domain of morphological language data. It will be described how crucial concepts like morphs, morphemes, word forms and meanings are represented and interrelated and how language-specific morpheme inventories can be created as a new possibility of morphological datasets. The aim of the MMoOn Core ontology is to serve as a shared semantic model for linguists and NLP researchers alike to enable the creation, conversion, exchange, reuse and enrichment of morphological language data across different data-dependent language sciences. Therefore, various use cases are illustrated to draw attention to the cross-disciplinary potential which can be realized with the MMoOn Core ontology in the context of the existing Linguistic Linked Data research landscape.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3448570",
                    "name": "Bettina Klimek"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "30596da17f14943eb98f428844e100724e28456d",
            "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus",
            "abstract": "In the past decade, the DBpedia community has put significant amount of effort on developing technical infrastructure and methods for efficient extraction of structured information from Wikipedia. These efforts have been primarily focused on harvesting, refinement and publishing semi-structured information found in Wikipedia articles, such as information from infoboxes, categorization information, images, wikilinks and citations. Nevertheless, still vast amount of valuable information is contained in the unstructured Wikipedia article texts. In this paper, we present DBpedia NIF - a large-scale and multilingual knowledge extraction corpus. The aim of the dataset is two-fold: to dramatically broaden and deepen the amount of structured information in DBpedia, and to provide large-scale and multilingual language resource for development of various NLP and IR task. The dataset provides the content of all articles for 128 Wikipedia languages. We describe the dataset creation process and the NLP Interchange Format (NIF) used to model the content, links and the structure the information of the Wikipedia articles. The dataset has been further enriched with about 25% more links and selected partitions published as Linked Data. Finally, we describe the maintenance and sustainability plans, and selected use cases of the dataset from the TextExt knowledge extraction challenge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "123017598",
                    "name": "Julio Hernandez"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "478a6120d2bc84466da48d93241f384b1bf2bb38",
            "title": "FREME: Multilingual Semantic Enrichment with Linked Data and Language Technologies",
            "abstract": "In the recent years, Linked Data and Language Technology solutions gained popularity. Nevertheless, their coupling in real-world business is limited due to several issues. Existing products and services are developed for a particular domain, can be used only in combination with already integrated datasets or their language coverage is limited. In this paper, we present an innovative solution FREME - an open framework of e-Services for multilingual and semantic enrichment of digital content. The framework integrates six interoperable e-Services. We describe the core features of each e-Service and illustrate their usage in the context of four business cases: i) authoring and publishing; ii) translation and localisation; iii) cross-lingual access to data; and iv) personalised Web content recommendations. Business cases drive the design and development of the framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "1708763",
                    "name": "F. Sasaki"
                },
                {
                    "authorId": "3398446",
                    "name": "Tatjana Gornostaja"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "1691320",
                    "name": "E. Mannens"
                },
                {
                    "authorId": "1876933",
                    "name": "Frank Salliau"
                },
                {
                    "authorId": "40551568",
                    "name": "Michele Osella"
                },
                {
                    "authorId": "2073845606",
                    "name": "Phil Ritchie"
                },
                {
                    "authorId": "1805133",
                    "name": "Giannis Stoitsis"
                },
                {
                    "authorId": "2705240",
                    "name": "Kevin Koidl"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "2067025636",
                    "name": "Nilesh Chakraborty"
                }
            ]
        },
        {
            "paperId": "04dc1e72d8382cc3aa4fbbd5595e664b816d4981",
            "title": "Knowledge base shipping to the linked open data cloud",
            "abstract": "Popular knowledge bases that provide SPARQL endpoints for the web are usually experiencing a high number of requests, which often results in low availability of their interfaces. A common approach to counter the availability issue is to run a local mirror of the knowledge base. Running a SPARQL endpoint is currently a complex task which requires a lot of effort and technical support for domain experts who just want to use the SPARQL interface. With our approach of containerised knowledge base shipping we are introducing a simple to setup methodology for running a local mirror of an RDF knowledge base and SPARQL endpoint with interchangeable exploration components. The flexibility of the presented approach further helps maintaining the publication infrastructure for dataset projects. We are demonstrating and evaluating the presented methodology at the example of the dataset projects DBpedia, Catalogus Professorum Lipsiensium and S\u00e4chsisches Pfarrerbuch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "29356676",
                    "name": "Thomas Riechert"
                }
            ]
        },
        {
            "paperId": "3b61603d43c820816c63b57b13bfbb584342708e",
            "title": "MEX vocabulary: a lightweight interchange format for machine learning experiments",
            "abstract": "Over the last decades many machine learning experiments have been published, giving benefit to the scientific progress. In order to compare machine-learning experiment results with each other and collaborate positively, they need to be performed thoroughly on the same computing environment, using the same sample datasets and algorithm configurations. Besides this, practical experience shows that scientists and engineers tend to have large output data in their experiments, which is both difficult to analyze and archive properly without provenance metadata. However, the Linked Data community still misses a lightweight specification for interchanging machine-learning metadata over different architectures to achieve a higher level of interoperability. In this paper, we address this gap by presenting a novel vocabulary dubbed MEX. We show that MEX provides a prompt method to describe experiments with a special focus on data provenance and fulfills the requirements for a long-term maintenance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145538480",
                    "name": "Diego Esteves"
                },
                {
                    "authorId": "2705621",
                    "name": "Diego Moussallem"
                },
                {
                    "authorId": "32124073",
                    "name": "C. Baron"
                },
                {
                    "authorId": "1869895",
                    "name": "Tommaso Soru"
                },
                {
                    "authorId": "2370666",
                    "name": "Ricardo Usbeck"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "144568027",
                    "name": "Jens Lehmann"
                }
            ]
        }
    ]
}