{
    "authorId": "1862782",
    "papers": [
        {
            "paperId": "0350636522997217df53553ddf3e472338bca97b",
            "title": "ToolACE: Winning the Points of LLM Function Calling",
            "abstract": "Function calling significantly extends the application boundary of large language models, where high-quality and diverse training data is critical for unlocking this capability. However, real function-calling data is quite challenging to collect and annotate, while synthetic data generated by existing pipelines tends to lack coverage and accuracy. In this paper, we present ToolACE, an automatic agentic pipeline designed to generate accurate, complex, and diverse tool-learning data. ToolACE leverages a novel self-evolution synthesis process to curate a comprehensive API pool of 26,507 diverse APIs. Dialogs are further generated through the interplay among multiple agents, guided by a formalized thinking process. To ensure data accuracy, we implement a dual-layer verification system combining rule-based and model-based checks. We demonstrate that models trained on our synthesized data, even with only 8B parameters, achieve state-of-the-art performance on the Berkeley Function-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a subset of the data are publicly available at https://huggingface.co/Team-ACE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281903918",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "2236672137",
                    "name": "Xu Huang"
                },
                {
                    "authorId": "46180553",
                    "name": "Xingshan Zeng"
                },
                {
                    "authorId": "2319803268",
                    "name": "Xinlong Hao"
                },
                {
                    "authorId": "2319400570",
                    "name": "Shuai Yu"
                },
                {
                    "authorId": "2284449061",
                    "name": "Dexun Li"
                },
                {
                    "authorId": "2319348454",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "2319373060",
                    "name": "Weinan Gan"
                },
                {
                    "authorId": "2319925953",
                    "name": "Zhengying Liu"
                },
                {
                    "authorId": "2319955791",
                    "name": "Yuanqing Yu"
                },
                {
                    "authorId": "2137432660",
                    "name": "Zezhong Wang"
                },
                {
                    "authorId": "2319403694",
                    "name": "Yuxian Wang"
                },
                {
                    "authorId": "2319409534",
                    "name": "Wu Ning"
                },
                {
                    "authorId": "2282135504",
                    "name": "Yutai Hou"
                },
                {
                    "authorId": "2319537684",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2276003321",
                    "name": "Chuhan Wu"
                },
                {
                    "authorId": "2319420767",
                    "name": "Xinzhi Wang"
                },
                {
                    "authorId": "2282190778",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2315458611",
                    "name": "Duyu Tang"
                },
                {
                    "authorId": "2315299495",
                    "name": "Dandan Tu"
                },
                {
                    "authorId": "2238661808",
                    "name": "Lifeng Shang"
                },
                {
                    "authorId": "2257942536",
                    "name": "Xin Jiang"
                },
                {
                    "authorId": "2284295184",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2249841180",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "0c3d8ea68ed9114d74eceaffc5c32cd0ee31d993",
            "title": "Understanding Privacy Risks of Embeddings Induced by Large Language Models",
            "abstract": "Large language models (LLMs) show early signs of artificial general intelligence but struggle with hallucinations. One promising solution to mitigate these hallucinations is to store external knowledge as embeddings, aiding LLMs in retrieval-augmented generation. However, such a solution risks compromising privacy, as recent studies experimentally showed that the original text can be partially reconstructed from text embeddings by pre-trained language models. The significant advantage of LLMs over traditional pre-trained models may exacerbate these concerns. To this end, we investigate the effectiveness of reconstructing original knowledge and predicting entity attributes from these embeddings when LLMs are employed. Empirical findings indicate that LLMs significantly improve the accuracy of two evaluated tasks over those from pre-trained models, regardless of whether the texts are in-distribution or out-of-distribution. This underscores a heightened potential for LLMs to jeopardize user privacy, highlighting the negative consequences of their widespread use. We further discuss preliminary strategies to mitigate this risk.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118353327",
                    "name": "Zhihao Zhu"
                },
                {
                    "authorId": "2278431511",
                    "name": "Ninglu Shao"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2000383268",
                    "name": "Chenwang Wu"
                },
                {
                    "authorId": "2240687341",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2275334006",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "2797dbc899193810c3bd5848653682131e4bc125",
            "title": "A Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation",
            "abstract": "Cross-domain recommendation (CDR), aiming to extract and transfer knowledge across domains, has attracted wide attention for its efficacy in addressing data sparsity and cold-start problems. Despite significant advances in representation disentanglement to capture diverse user preferences, existing methods usually neglect representation enhancement and lack rigorous decoupling constraints, thereby limiting the transfer of relevant information. To this end, we propose a Unified Framework for Adaptive Representation Enhancement and Inversed Learning in Cross-Domain Recommendation (AREIL). Specifically, we first divide user embeddings into domain-shared and domain-specific components to disentangle mixed user preferences. Then, we incorporate intra-domain and inter-domain information to adaptively enhance the ability of user representations. In particular, we propose a graph convolution module to capture high-order information, and a self-attention module to reveal inter-domain correlations and accomplish adaptive fusion. Next, we adopt domain classifiers and gradient reversal layers to achieve inversed representation learning in a unified framework. Finally, we employ a cross-entropy loss for measuring recommendation performance and jointly optimize the entire framework via multi-task learning. Extensive experiments on multiple datasets validate the substantial improvement in the recommendation performance of AREIL. Moreover, ablation studies and representation visualizations further illustrate the effectiveness of adaptive enhancement and inversed learning in CDR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2294685051",
                    "name": "Luankang Zhang"
                },
                {
                    "authorId": "2256768674",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2293142009",
                    "name": "Suojuan Zhang"
                },
                {
                    "authorId": "2260649859",
                    "name": "Mingjia Yin"
                },
                {
                    "authorId": "2210397106",
                    "name": "Yongqiang Han"
                },
                {
                    "authorId": "2294507671",
                    "name": "Jiaqing Zhang"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "279a3f8672fae617839c6a00e29ceef51b50a086",
            "title": "Dataset Regeneration for Sequential Recommendation",
            "abstract": "The sequential recommender (SR) system is a crucial component of modern recommender systems, as it aims to capture the evolving preferences of users. Significant efforts have been made to enhance the capabilities of SR systems. These methods typically follow the model-centric paradigm, which involves developing effective models based on fixed datasets. However, this approach often overlooks potential quality issues and flaws inherent in the data. Driven by the potential of data-centric AI, we propose a novel data-centric paradigm for developing an ideal training dataset using a model-agnostic dataset regeneration framework called DR4SR. This framework enables the regeneration of a dataset with exceptional cross-architecture generalizability. Additionally, we introduce the DR4SR+ framework, which incorporates a model-aware dataset personalizer to tailor the regenerated dataset specifically for a target model. To demonstrate the effectiveness of the data-centric paradigm, we integrate our framework with various model-centric methods and observe significant performance improvements across four widely adopted datasets. Furthermore, we conduct in-depth analyses to explore the potential of the data-centric paradigm and provide valuable insights. The code can be found at https://github.com/USTC-StarTeam/DR4SR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260649859",
                    "name": "Mingjia Yin"
                },
                {
                    "authorId": "2256768854",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2292344659",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "2293683997",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "2293142009",
                    "name": "Suojuan Zhang"
                },
                {
                    "authorId": "2111005730",
                    "name": "Sirui Zhao"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "330fd503ef6924c6972813ca8dbc75d48973ec11",
            "title": "Entropy Law: The Story Behind Data Compression and LLM Performance",
            "abstract": "Data is the cornerstone of large language models (LLMs), but not all data is useful for model learning. Carefully selected data can better elicit the capabilities of LLMs with much less computational overhead. Most methods concentrate on evaluating the quality of individual samples in data selection, while the combinatorial effects among samples are neglected. Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim to uncover the underlying relationships between LLM performance and data selection. Inspired by the information compression nature of LLMs, we uncover an ``entropy law'' that connects LLM performance with data compression ratio and first-epoch training loss, which reflect the information redundancy of a dataset and the mastery of inherent knowledge encoded in this dataset, respectively. Through both theoretical deduction and empirical evaluation, we find that model performance is negatively correlated to the compression ratio of training data, which usually yields a lower training loss. Based on the findings of the entropy law, we propose a quite efficient and universal data selection method named \\textbf{ZIP} for training LLMs, which aim to prioritize data subsets exhibiting a low compression ratio. Based on a multi-stage algorithm that selects diverse data in a greedy manner, we can obtain a good data subset with satisfactory diversity. Extensive experiments have been conducted to validate the entropy law and the superiority of ZIP across different LLM backbones and alignment stages. We also present an interesting application of entropy law that can detect potential performance risks at the beginning of model training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260649859",
                    "name": "Mingjia Yin"
                },
                {
                    "authorId": "2276003321",
                    "name": "Chuhan Wu"
                },
                {
                    "authorId": "2283529247",
                    "name": "Yufei Wang"
                },
                {
                    "authorId": "2256768674",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2260810851",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2293683997",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "2284295184",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "34b6038c761cb754c8e3f4637281427ef69e67b0",
            "title": "Learning Partially Aligned Item Representation for Cross-Domain Sequential Recommendation",
            "abstract": "Cross-domain sequential recommendation (CDSR) aims to uncover and transfer users' sequential preferences across multiple recommendation domains. While significant endeavors have been made, they primarily concentrated on developing advanced transfer modules and aligning user representations using self-supervised learning techniques. However, the problem of aligning item representations has received limited attention, and misaligned item representations can potentially lead to sub-optimal sequential modeling and user representation alignment. To this end, we propose a model-agnostic framework called \\textbf{C}ross-domain item representation \\textbf{A}lignment for \\textbf{C}ross-\\textbf{D}omain \\textbf{S}equential \\textbf{R}ecommendation (\\textbf{CA-CDSR}), which achieves sequence-aware generation and adaptively partial alignment for item representations. Specifically, we first develop a sequence-aware feature augmentation strategy, which captures both collaborative and sequential item correlations, thus facilitating holistic item representation generation. Next, we conduct an empirical study to investigate the partial representation alignment problem from a spectrum perspective. It motivates us to devise an adaptive spectrum filter, achieving partial alignment adaptively. Furthermore, the aligned item representations can be fed into different sequential encoders to obtain user representations. The entire framework is optimized in a multi-task learning paradigm with an annealing strategy. Extensive experiments have demonstrated that CA-CDSR can surpass state-of-the-art baselines by a significant margin and can effectively align items in representation spaces to enhance performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260649859",
                    "name": "Mingjia Yin"
                },
                {
                    "authorId": "2256768674",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2292344659",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "2293683997",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "2293557930",
                    "name": "Zhi Li"
                },
                {
                    "authorId": "2111005730",
                    "name": "Sirui Zhao"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "46a4a77a9eb04fcccc4fe25ddbece8baa1a0eeee",
            "title": "WESE: Weak Exploration to Strong Exploitation for LLM Agents",
            "abstract": "Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent's reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge. A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2236672137",
                    "name": "Xu Huang"
                },
                {
                    "authorId": "2281903918",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "2223761014",
                    "name": "Xiaolong Chen"
                },
                {
                    "authorId": "2223459253",
                    "name": "Xingmei Wang"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2240536007",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "4b733e52a22502190c6ec6e3eac6591234a8b103",
            "title": "Average User-Side Counterfactual Fairness for Collaborative Filtering",
            "abstract": "Recently, the user-side fairness issue in Collaborative Filtering (CF) algorithms has gained considerable attention, arguing that results should not discriminate an individual or a sub-user group based on users\u2019 sensitive attributes (e.g., gender). Researchers have proposed fairness-aware CF models by decreasing statistical associations between predictions and sensitive attributes. A more natural idea is to achieve model fairness from a causal perspective. The remaining challenge is that we have no access to interventions, i.e., the counterfactual world that produces recommendations when each user has changed the sensitive attribute value. To this end, we first borrow the Rubin-Neyman potential outcome framework to define average causal effects of sensitive attributes. Next, we show that removing causal effects of sensitive attributes is equal to average counterfactual fairness in CF. Then, we use the propensity re-weighting paradigm to estimate the average causal effects of sensitive attributes and formulate the estimated causal effects as an additional regularization term. To the best of our knowledge, we are one of the first few attempts to achieve counterfactual fairness from the causal effect estimation perspective in CF, which frees us from building sophisticated causal graphs. Finally, experiments on three real-world datasets show the superiority of our proposed model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051349636",
                    "name": "Pengyang Shao"
                },
                {
                    "authorId": "2175545570",
                    "name": "Le Wu"
                },
                {
                    "authorId": "2156563711",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2287821406",
                    "name": "Richang Hong"
                },
                {
                    "authorId": "2267870550",
                    "name": "Yong Li"
                },
                {
                    "authorId": "2146058717",
                    "name": "Meng Wang"
                }
            ]
        },
        {
            "paperId": "5ab7e332f1151c3d62cfb181e06c80dd9d66b504",
            "title": "Deep Tree-based Retrieval for Efficient Recommendation: Theory and Method",
            "abstract": "With the development of deep learning techniques, deep recommendation models also achieve remarkable improvements in terms of recommendation accuracy. However, due to the large number of candidate items in practice and the high cost of preference computation, these methods also suffer from low efficiency of recommendation. The recently proposed tree-based deep recommendation models alleviate the problem by directly learning tree structure and representations under the guidance of recommendation objectives. However, such models have shortcomings. The max-heap assumption in the hierarchical tree, in which the preference for a parent node should be the maximum between the preferences for its children, is difficult to satisfy in their binary classification objectives. To this end, we propose Tree-based Deep Retrieval (TDR for short) for efficient recommendation. In TDR, all the trees generated during the training process are retained to form the forest. When learning the node representation of each tree, we have to satisfy the max-heap assumption as much as possible and mimic beam search behavior over the tree in the training stage. This is achieved by TDR to regard the training task as multi-classification over tree nodes at the same level. However, the number of tree nodes grows exponentially with levels, making us train the preference model with the guidance of the sampled-softmax technique. The experiments are conducted on real-world datasets, validating the effectiveness of the proposed preference model learning method and tree learning method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316667358",
                    "name": "Ze Liu"
                },
                {
                    "authorId": "2117170420",
                    "name": "Jin Zhang"
                },
                {
                    "authorId": "2153286417",
                    "name": "Chao Feng"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2309727900",
                    "name": "Jie Wang"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "6561212f9572d5201f2d657063cccc09ca519412",
            "title": "Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation",
            "abstract": "In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to cart, and purchasing. Multi-behavior sequential recommendation aims to jointly consider multiple behaviors to improve the target behavior's performance. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to contrast the valid information with the noisy signal in the data, and seamlessly integrate the two denoising processes to achieve a high degree of decoupling of the noisy signal. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our approach in dealing with multi-behavior sequential recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2210397106",
                    "name": "Yongqiang Han"
                },
                {
                    "authorId": "2256768854",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2293923694",
                    "name": "Kefan Wang"
                },
                {
                    "authorId": "12892739",
                    "name": "Likang Wu"
                },
                {
                    "authorId": "2293557930",
                    "name": "Zhi Li"
                },
                {
                    "authorId": "2292344659",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "2293683997",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2113754294",
                    "name": "Enhong Chen"
                }
            ]
        }
    ]
}