{
    "authorId": "2259931859",
    "papers": [
        {
            "paperId": "3f619f7d474b7a61ffe9f8e4a05ba75a9e35adbe",
            "title": "RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking",
            "abstract": "The rapid progress of Large Language Models (LLMs) has opened up new opportunities across various domains and applications; yet it also presents challenges related to potential misuse. To mitigate such risks, red teaming has been employed as a proactive security measure to probe language models for harmful outputs via jailbreak attacks. However, current jailbreak attack approaches are single-turn with explicit malicious queries that do not fully capture the complexity of real-world interactions. In reality, users can engage in multi-turn interactions with LLM-based chat assistants, allowing them to conceal their true intentions in a more covert manner. To bridge this gap, we, first, propose a new jailbreak approach, RED QUEEN ATTACK. This method constructs a multi-turn scenario, concealing the malicious intent under the guise of preventing harm. We craft 40 scenarios that vary in turns and select 14 harmful categories to generate 56k multi-turn attack data points. We conduct comprehensive experiments on the RED QUEEN ATTACK with four representative LLM families of different sizes. Our experiments reveal that all LLMs are vulnerable to RED QUEEN ATTACK, reaching 87.62% attack success rate on GPT-4o and 75.4% on Llama3-70B. Further analysis reveals that larger models are more susceptible to the RED QUEEN ATTACK, with multi-turn structures and concealment strategies contributing to its success. To prioritize safety, we introduce a straightforward mitigation strategy called RED QUEEN GUARD, which aligns LLMs to effectively counter adversarial attacks. This approach reduces the attack success rate to below 1% while maintaining the model's performance across standard benchmarks. Full implementation and dataset are publicly accessible at https://github.com/kriti-hippo/red_queen.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2323027122",
                    "name": "Yifan Jiang"
                },
                {
                    "authorId": "2292259353",
                    "name": "Kriti Aggarwal"
                },
                {
                    "authorId": "103242455",
                    "name": "Tanmay Laud"
                },
                {
                    "authorId": "2322990096",
                    "name": "Kashif Munir"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                },
                {
                    "authorId": "2293143373",
                    "name": "Subhabrata Mukherjee"
                }
            ]
        },
        {
            "paperId": "59424220241289ba6b693bf419b17d8cf31a70ff",
            "title": "On the Adaptation of Unlimiformer for Decoder-Only Transformers",
            "abstract": "One of the prominent issues stifling the current generation of large language models is their limited context length. Recent proprietary models such as GPT-4 and Claude 2 have introduced longer context lengths, 8k/32k and 100k, respectively; however, despite the efforts in the community, most common models, such as LLama-2, have a context length of 4k or less. Unlimiformer (Bertsch et al., 2023) is a recently popular vector-retrieval augmentation method that offloads cross-attention computations to a kNN index. However, its main limitation is incompatibility with decoder-only transformers out of the box. In this work, we explore practical considerations of adapting Unlimiformer to decoder-only transformers and introduce a series of modifications to overcome this limitation. Moreover, we expand the original experimental setup on summarization to include a new task (i.e., free-form Q&A) and an instruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase the effectiveness of these modifications on summarization, performing on par with a model with 2x the context length. Moreover, we discuss limitations and future directions for free-form Q&A and instruction-tuned models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "102222453",
                    "name": "Alon Benhaim"
                },
                {
                    "authorId": "27419446",
                    "name": "Barun Patra"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                },
                {
                    "authorId": "14839052",
                    "name": "Saksham Singhal"
                },
                {
                    "authorId": "2301764251",
                    "name": "Xia Song"
                }
            ]
        },
        {
            "paperId": "8bc08a3fa45a25822f5926fbdc306f192dcfbd20",
            "title": "Efficient and Accurate Contextual Re-Ranking for Knowledge Graph Question Answering",
            "abstract": "The efficacy of neural \u201cretrieve and generate\u201d systems is well established for question answering (QA) over unstructured text. Recent efforts seek to extend this approach to knowledge graph (KG) QA by converting structured triples to unstructured text. However, the relevance of KG triples retrieved by these systems limits their accuracy. In this paper, we improve the relevance of retrieved triples using a carefully designed re-ranker. Specifically, our pipeline (i) retrieves over documents of triples grouped by entity, (ii) re-ranks triples from these documents with context: triples in the 1-hop neighborhood of the documents\u2019 subject entity, and (iii) generates an answer from highly relevant re-ranked triples. To train our re-ranker, we propose a novel \u201ctriple-level\u201d labeling strategy that infers fine-grained labels and shows that these significantly improve the relevance of retrieved information. We show that the resulting \u201cretrieve, re-rank, and generate\u201d pipeline significantly improves upon prior KGQA systems, achieving a new state-of-the-art on FreebaseQA by 5.56% Exact Match. We perform multiple ablations that reveal the distinct benefits of our contextual re-ranker and labeling strategy and conclude with a case study that highlights opportunities for future works.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280276151",
                    "name": "Kexuan Sun"
                },
                {
                    "authorId": "2139649061",
                    "name": "Nic Jedema"
                },
                {
                    "authorId": "2303002552",
                    "name": "Karishma Sharma"
                },
                {
                    "authorId": "2301582504",
                    "name": "Ruben Janssen"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                },
                {
                    "authorId": "2278903766",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "1719404",
                    "name": "Alessandro Moschitti"
                }
            ]
        },
        {
            "paperId": "a557a3fad6220dde7ce17797f2b8ad5fc61c682f",
            "title": "Knowledge-Powered Recommendation for an Improved Diet Water Footprint",
            "abstract": "According to WWF, 1.1 billion people lack access to water, and 2.7 billion experience water scarcity at least one month a year. By 2025, two-thirds of the world's population may be facing water shortages. This highlights the urgency of managing water usage efficiently, especially in water-intensive sectors like food. This paper proposes a recommendation engine, powered by knowledge graphs, aiming to facilitate sustainable and healthy food consumption. The engine recommends ingredient substitutes in user recipes that improve nutritional value and reduce environmental impact, particularly water footprint. The system architecture includes source identification, information extraction, schema alignment, knowledge graph construction, and user interface development. The research offers a promising tool for promoting healthier eating habits and contributing to water conservation efforts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267697165",
                    "name": "Saurav Joshi"
                },
                {
                    "authorId": "2267335678",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                }
            ]
        },
        {
            "paperId": "cdce4525bc94b8b72d7330f4e26775142edd0018",
            "title": "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models",
            "abstract": "While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments reveal the challenging nature of such problems for MLLMs while showcasing the immense gap between open-source and closed-source models. We also uncover critical shortcomings of visual and textual perceptions, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experiment with different methods, such as Chain-of-Thought prompting, leading to a significant (up to 100%) boost in performance. Our code and datasets are available at https://github.com/usc-isi-i2/isi-mmlm-rpm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "2196148017",
                    "name": "Zhivar Sourati"
                },
                {
                    "authorId": "2280276151",
                    "name": "Kexuan Sun"
                },
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "2280252269",
                    "name": "Yifan Jiang"
                },
                {
                    "authorId": "2258550405",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                }
            ]
        },
        {
            "paperId": "ebdb35c3a7efb5e0d4d79bdcfd167051446aba7a",
            "title": "The Hitchhiker's Guide to Human Alignment with *PO",
            "abstract": "With the growing utilization of large language models (LLMs) across domains, alignment towards human preferences has become one of the most critical aspects of training models. At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). However, prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we aim to identify the algorithm that, while being performant, is simultaneously more robust to varying hyperparameters, thereby increasing the likelihood of achieving better results. We focus on a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment, offering practical insights into the strengths and weaknesses of these methods. Furthermore, to better understand the shortcomings of generations from the different methods, we analyze the model generations through the lens of KL divergence of the SFT model and the response length statistics. Our analysis reveals that the widely adopted DPO method consistently produces lengthy responses of inferior quality that are very close to the SFT responses. Motivated by these findings, we propose an embarrassingly simple extension to the DPO algorithm, LN-DPO, resulting in more concise responses without sacrificing quality compared to the policy obtained by vanilla DPO.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "2312344360",
                    "name": "Xihui Lin"
                },
                {
                    "authorId": "27419446",
                    "name": "Barun Patra"
                },
                {
                    "authorId": "113810201",
                    "name": "Vishrav Chaudhary"
                },
                {
                    "authorId": "102222453",
                    "name": "Alon Benhaim"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                },
                {
                    "authorId": "2301764251",
                    "name": "Xia Song"
                }
            ]
        },
        {
            "paperId": "fdc7c55eeb2151b3bb5f54c0982883bcaa56fe21",
            "title": "Surprising Resilience of Science During a Global Pandemic: A Large-Scale Descriptive Analysis",
            "abstract": "The COVID-19 pandemic profoundly impacted people globally, yet its effect on scientists and research institutions has yet to be fully examined. To address this knowledge gap, we use a newly available bibliographic dataset covering tens of millions of papers and authors to investigate changes in research activity and collaboration during this period. Employing statistical methods, we analyze the pandemic's disruptions on the participation, productivity, and collaborations of researchers at the top 1,000 institutions worldwide based on historical productivity, taking into account variables such as geography, researcher seniority and gender, and field of study. Our findings reveal an unexpected trend: research activity and output significantly increased in the early stages of the pandemic, indicating a surprising resilience in the scientific community. However, by the end of 2022, there was a notable reversion to historical trends in research participation and productivity. This reversion suggests that the initial spike in research activity was a short-lived disruption rather than a permanent shift. As such, monitoring scientific outputs in 2023 and beyond becomes crucial. There may be a delayed negative effect of the pandemic on research, given the long time horizon for many research fields and the temporary closure of wet labs. Further analysis is needed to fully comprehend the factors that underpin the resilience of scientific innovation in the face of global crises. Our study provides an initial comprehensive exploration up to the end of 2022, offering valuable insights into how the scientific community has adapted and responded over the course of the pandemic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "2214035571",
                    "name": "Casandra Rusti"
                },
                {
                    "authorId": "2320815585",
                    "name": "Ziao Wang"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                },
                {
                    "authorId": "2320800305",
                    "name": "Kristina Lerman"
                }
            ]
        },
        {
            "paperId": "ff05de1241fd5a5a2e05f7ed298e5ba9a123d7ba",
            "title": "MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning",
            "abstract": "While multi-modal large language models (MLLMs) have shown significant progress on many popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question. Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding high-level patterns (e.g., repetition constraints) that control the input shapes (e.g., digits) in a specific task configuration (e.g., matrix). However, existing AVR benchmarks only considered a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (3 by 3 matrices). To evaluate MLLMs' reasoning abilities comprehensively, we introduce MARVEL, a multidimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations. To inspect whether the model accuracy is grounded in perception and reasoning, MARVEL complements the general AVR question with perception questions in a hierarchical evaluation framework. We conduct comprehensive experiments on MARVEL with nine representative MLLMs in zero-shot and few-shot settings. Our experiments reveal that all models show near-random performance on the AVR question, with significant performance gaps (40%) compared to humans across all patterns and task configurations. Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance) and even count the panels in the puzzle (<45%), hindering their ability for abstract reasoning. We release our entire code and dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280252269",
                    "name": "Yifan Jiang"
                },
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "2280276151",
                    "name": "Kexuan Sun"
                },
                {
                    "authorId": "2196148017",
                    "name": "Zhivar Sourati"
                },
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "22244290",
                    "name": "Kaixin Ma"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                }
            ]
        },
        {
            "paperId": "04c6a828169c683b69064d10f4bca39fb83b18b1",
            "title": "Is Dynamicity All You Need?",
            "abstract": "Scientific domains are fluid entities that change and turn as time passes. Take machine learning as an example. Up until the \u201990s, most of the methods were expert-knowledge-driven. However, as time passed, more data-driven approaches appeared, finally leading to the advent of deep learning methods. As a result, in a span of 30 years, the field has gone through many changes and breakthroughs and is at a point where many novelties have a life span of shorter than five years. In parallel, a regular researcher\u2019s career span is around the same length. Consequently, being a researcher requires shifts in the field of study throughout one\u2019s career. Besides, researchers\u2019 scientific interests are inherently dynamic and change over time. Hence, there exists a dynamicity to authors\u2019 interests and fields of work over time. In this work, we study this phenomenon through systematic approaches for representing and tracking dynamicity in different epochs. Our representation approaches are based on the idea that each author could be represented as a distribution of other authors. Concurrently, our tracking approaches rely on established mathematical concepts for measuring the change between two distributions. We focus on the publications in the 2001-2020 range and present a set of analyses built on top of the introduced approaches to understanding the potential connection between dynamicity and success.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1850429085",
                    "name": "Richard Delwin Myloth"
                },
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "2204572135",
                    "name": "Arun Baalaaji Sankar Ananthan"
                },
                {
                    "authorId": "2152944834",
                    "name": "Xinwei Du"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                }
            ]
        },
        {
            "paperId": "59f4b2103d6a78ebad51db8bd603d4c5571c0cbf",
            "title": "Citation Intent Classification Through Weakly Supervised Knowledge Graphs",
            "abstract": "Citations are scientists\u2019 tools for grounding their innovations and findings in the existing collective knowledge. They are used for semantically distinct purposes as scientists utilize them at different parts of their work to convey specific information. As a result, a crucial aspect of scientific document understanding is recognizing the authorial intent associated with citations. Current state-of-the-art methods rely on contextual sentences surrounding each citation to classify the intent. However, in the absence of textual content, these approaches become unusable. In this work, we propose a text-free citation intent classification method built on relational information among scholarly works in this work. To this end, we introduce a large-scale knowledge graph built from the publications in the SciCite dataset and their multi-hop neighborhood extracted from The Semantic Scholar Open Research Corpus (S2ORC). We also augment this knowledge graph by adding weakly-labeled links based on the intent information available in the S2ORC. Finally, we cast the intent classification task as a link prediction problem on the newly created knowledge graph. We study this problem in both transductive and inductive settings. Our experimental results show that we can achieve a comparable macro F1 score to word embedding content-based methods by only relying on features and relations derived from this knowledge graph. Specifically, we achieve macro F1 scores of 62.16 and 59.81 in the transductive and inductive settings, respectively, on the link-level SciCite dataset. Moreover, by combining our method with the state-of-the-art NLP-based model, we achieve improvements across all metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152944834",
                    "name": "Xinwei Du"
                },
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "2204572135",
                    "name": "Arun Baalaaji Sankar Ananthan"
                },
                {
                    "authorId": "1850429085",
                    "name": "Richard Delwin Myloth"
                },
                {
                    "authorId": "2259931859",
                    "name": "Jay Pujara"
                }
            ]
        }
    ]
}