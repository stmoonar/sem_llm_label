{
    "authorId": "2304523047",
    "papers": [
        {
            "paperId": "81a953de95f4932b6342fa55b2040926b2590e9c",
            "title": "CodeR: Issue Resolving with Multi-Agent and Task Graphs",
            "abstract": "GitHub issue resolving recently has attracted significant attention from academia and industry. SWE-bench is proposed to measure the performance in resolving issues. In this paper, we propose CodeR, which adopts a multi-agent framework and pre-defined task graphs to Repair&Resolve reported bugs and add new features within code Repository. On SWE-bench lite, CodeR is able to solve 28.33% of issues, when submitting only once for each issue. We examine the performance impact of each design of CodeR and offer insights to advance this research direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2304895824",
                    "name": "Dong Chen"
                },
                {
                    "authorId": "2316780443",
                    "name": "Shaoxin Lin"
                },
                {
                    "authorId": "2304477776",
                    "name": "Muhan Zeng"
                },
                {
                    "authorId": "2134434187",
                    "name": "Daoguang Zan"
                },
                {
                    "authorId": "2304515651",
                    "name": "Jian-Gang Wang"
                },
                {
                    "authorId": "2304470645",
                    "name": "Anton Cheshkov"
                },
                {
                    "authorId": "2267508408",
                    "name": "Jun Sun"
                },
                {
                    "authorId": "2296050795",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2059261893",
                    "name": "Guoliang Dong"
                },
                {
                    "authorId": "2304471915",
                    "name": "Artem Aliev"
                },
                {
                    "authorId": "2307237447",
                    "name": "Jie Wang"
                },
                {
                    "authorId": "2304584825",
                    "name": "Xiao Cheng"
                },
                {
                    "authorId": "2084524",
                    "name": "Guangtai Liang"
                },
                {
                    "authorId": "2304523047",
                    "name": "Yuchi Ma"
                },
                {
                    "authorId": "2304471393",
                    "name": "Pan Bian"
                },
                {
                    "authorId": "2300201756",
                    "name": "Tao Xie"
                },
                {
                    "authorId": "2253829508",
                    "name": "Qianxiang Wang"
                }
            ]
        },
        {
            "paperId": "a727cc6abe52255e197f0e7056af66ed8a42b785",
            "title": "Iterative Knowledge Distillation through Feedback-Driven Learning Cycles",
            "abstract": "Large code models (LCMs) have remarkably advanced the field of code intelligence. Despite their impressive capabilities, they still face practical employment challenges, such as high costs, limited accessibility of proprietary LCMs, and adaptability issues of ultra-large LCMs. These challenges highlight the critical need for more accessible, lightweight yet effective LCMs. In this paper, we propose IterKD, an Iter Knowledge Distillation framework, which aims at continually transferring the programming capabilities of larger, advanced LCMs (Teacher) to smaller, less powerful LCMs (Student). IterKD consists of three stages in one cycle: (1) Correct-and-Fault Knowledge Delivery stage aims at improving the student models capability to recognize errors while ensuring its basic programming skill during the knowledge transferring, which involves correctness-aware supervised learning and fault-aware contrastive learning methods. (2) Multi-view Feedback stage aims at measuring the quality of results generated by the student model from two views, including model-based and static tool-based measurement; (3) Feedback-based Knowledge Update stage aims at updating the student model adaptively by generating new questions at different difficulty levels, in which the difficulty levels are categorized based on the feedback in the last stage. By performing the training cycle iteratively, the student model is continuously refined through learning more advanced programming skills from the teacher model. Finally, based on the proposed IterKD framework, we develop a lightweight yet effective LCM, named IterCoder, which is built upon CodeLlama-7B. Experimental results show that IterCoder achieves a Pass@1 score of 65.2 on the HumanEval benchmark, outperforming over-30B-sized LCMs by an average of 47.51% and surpassing comparable-sized LCMs by an average of 118.47%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280918261",
                    "name": "Yujia Chen"
                },
                {
                    "authorId": "2315878474",
                    "name": "Yang Ye"
                },
                {
                    "authorId": "2315274204",
                    "name": "Zhongqi Li"
                },
                {
                    "authorId": "2304523047",
                    "name": "Yuchi Ma"
                },
                {
                    "authorId": "2278987362",
                    "name": "Cuiyun Gao"
                }
            ]
        },
        {
            "paperId": "6ab8aca8f631f42760a86cc614dfd7208b3fe58e",
            "title": "Learning-based Widget Matching for Migrating GUI Test Cases",
            "abstract": "GUI test case migration is to migrate GUI test cases from a source app to a target app. The key of test case migration is widget matching. Recently, researchers have proposed various approaches by formulating widget matching as a matching task. However, since these matching approaches depend on static word embeddings without using contextual information to represent widgets and manually formulated matching functions, there are main limitations of these matching approaches when handling complex matching relations in apps. To address the limitations, we propose the first learning-based widget matching approach named TEMdroid ( TEst Migration) for test case migration. Unlike the existing approaches, TEMdroid uses BERT to capture contextual information and learns a matching model to match widgets. Additionally, to balance the significant imbalance between positive and negative samples in apps, we design a two-stage training strategy where we first train a hard-negative sample miner to mine hard-negative samples, and further train a matching model using positive samples and mined hard-negative samples. Our evaluation on 34 apps shows that TEM-droid is effective in event matching (i.e., widget matching and target event synthesis) and test case migration. For event matching, TEM-droid's Top1 accuracy is 76%, improving over 17% compared to baselines. For test case migration, TEMdroid's F1 score is 89%, also 7% improvement compared to the baseline approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110752255",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2089966950",
                    "name": "Bo Shen"
                },
                {
                    "authorId": "2038503437",
                    "name": "Dezhi Ran"
                },
                {
                    "authorId": null,
                    "name": "Jiaxin Zhang"
                },
                {
                    "authorId": "2145906426",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2304523047",
                    "name": "Yuchi Ma"
                },
                {
                    "authorId": "2084524",
                    "name": "Guangtai Liang"
                },
                {
                    "authorId": "2172444921",
                    "name": "Ying Li"
                },
                {
                    "authorId": "2057038049",
                    "name": "Tao Xie"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                }
            ]
        },
        {
            "paperId": "06ea568379211ffa07d9605f66f26f6f736ea5e0",
            "title": "PanGu-Coder: Program Synthesis with Function-Level Language Modeling",
            "abstract": "We present PanGu-Coder, a pretrained decoder-only language model adopting the PanGu-Alpha architecture for text-to-code generation, i.e. the synthesis of programming language solutions given a natural language problem description. We train PanGu-Coder using a two-stage strategy: the first stage employs Causal Language Modelling (CLM) to pre-train on raw programming language data, while the second stage uses a combination of Causal Language Modelling and Masked Language Modelling (MLM) training objectives that focus on the downstream task of text-to-code generation and train on loosely curated pairs of natural language program definitions and code functions. Finally, we discuss PanGu-Coder-FT, which is fine-tuned on a combination of competitive programming problems and code with continuous integration tests. We evaluate PanGu-Coder with a focus on whether it generates functionally correct programs and demonstrate that it achieves equivalent or better performance than similarly sized models, such as CodeX, while attending a smaller context window and training on less data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48810605",
                    "name": "Fenia Christopoulou"
                },
                {
                    "authorId": "2346538",
                    "name": "Gerasimos Lampouras"
                },
                {
                    "authorId": "22168669",
                    "name": "Milan Gritta"
                },
                {
                    "authorId": "2044459",
                    "name": "Guchun Zhang"
                },
                {
                    "authorId": "121083081",
                    "name": "Yinpeng Guo"
                },
                {
                    "authorId": "2145415560",
                    "name": "Zhong-yi Li"
                },
                {
                    "authorId": "2145908386",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "6250798",
                    "name": "M. Xiao"
                },
                {
                    "authorId": "2089966950",
                    "name": "Bo Shen"
                },
                {
                    "authorId": "2155689998",
                    "name": "Lin Li"
                },
                {
                    "authorId": "145772431",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "14906343",
                    "name": "Li-yu Yan"
                },
                {
                    "authorId": "8157288",
                    "name": "Pingyi Zhou"
                },
                {
                    "authorId": "2153689079",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2304523047",
                    "name": "Yuchi Ma"
                },
                {
                    "authorId": "2676143",
                    "name": "Ignacio Iacobacci"
                },
                {
                    "authorId": "2136912252",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2084524",
                    "name": "Guangtai Liang"
                },
                {
                    "authorId": "2111613690",
                    "name": "Jia Wei"
                },
                {
                    "authorId": "2110310493",
                    "name": "Xin Jiang"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                },
                {
                    "authorId": "30738758",
                    "name": "Qun Liu"
                }
            ]
        }
    ]
}