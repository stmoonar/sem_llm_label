{
    "authorId": "2156009696",
    "papers": [
        {
            "paperId": "5bde14509d9be0cd751194c6035a02799e97605d",
            "title": "Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation",
            "abstract": "Sequential recommender systems (SRS) are designed to predict users' future behaviors based on their historical interaction data. Recent research has increasingly utilized contrastive learning (CL) to leverage unsupervised signals to alleviate the data sparsity issue in SRS. In general, CL-based SRS first augments the raw sequential interaction data by using data augmentation strategies and employs a contrastive training scheme to enforce the representations of those sequences from the same raw interaction data to be similar. Despite the growing popularity of CL, data augmentation, as a basic component of CL, has not received sufficient attention. This raises the question: Is it possible to achieve superior recommendation results solely through data augmentation? To answer this question, we benchmark eight widely used data augmentation strategies, as well as state-of-the-art CL-based SRS methods, on four real-world datasets under both warm- and cold-start settings. Intriguingly, the conclusion drawn from our study is that, certain data augmentation strategies can achieve similar or even superior performance compared with some CL-based methods, demonstrating the potential to significantly alleviate the data sparsity issue with fewer computational overhead. We hope that our study can further inspire more fundamental studies on the key functional components of complex CL techniques. Our processed datasets and codes are available at https://github.com/AIM-SE/DA4Rec.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293137193",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2265951415",
                    "name": "You-Liang Huang"
                },
                {
                    "authorId": "2154871075",
                    "name": "Yueqi Xie"
                },
                {
                    "authorId": "2292122107",
                    "name": "Jingqi Gao"
                },
                {
                    "authorId": "2292055270",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2156009696",
                    "name": "Jae Boum Kim"
                },
                {
                    "authorId": "2118021616",
                    "name": "Sunghun Kim"
                }
            ]
        },
        {
            "paperId": "a44d904432cdda9d188bfff8e31619e02f2a4d89",
            "title": "Attention Calibration for Transformer-based Sequential Recommendation",
            "abstract": "Transformer-based sequential recommendation (SR) has been booming in recent years, with the self-attention mechanism as its key component. Self-attention has been widely believed to be able to effectively select those informative and relevant items from a sequence of interacted items for next-item prediction via learning larger attention weights for these items. However, this may not always be true in reality. Our empirical analysis of some representative Transformer-based SR models reveals that it is not uncommon for large attention weights to be assigned to less relevant items, which can result in inaccurate recommendations. Through further in-depth analysis, we find two factors that may contribute to such inaccurate assignment of attention weights:sub-optimal position encoding andnoisy input. To this end, in this paper, we aim to address this significant yet challenging gap in existing works. To be specific, we propose a simple yet effective framework called Attention Calibration for Transformer-based Sequential Recommendation (AC-TSR). In AC-TSR, a novel spatial calibrator and adversarial calibrator are designed respectively to directly calibrates those incorrectly assigned attention weights. The former is devised to explicitly capture the spatial relationships (i.e., order and distance) among items for more precise calculation of attention weights. The latter aims to redistribute the attention weights based on each item's contribution to the next-item prediction. AC-TSR is readily adaptable and can be seamlessly integrated into various existing transformer-based SR models. Extensive experimental results on four benchmark real-world datasets demonstrate the superiority of our proposed AC-TSR via significant recommendation performance enhancements. The source code is available at https://github.com/AIM-SE/AC-TSR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2190432576",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "2154871075",
                    "name": "Yueqi Xie"
                },
                {
                    "authorId": "2118389668",
                    "name": "Jingqi Gao"
                },
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2156009696",
                    "name": "Jae Boum Kim"
                },
                {
                    "authorId": "2061592207",
                    "name": "Chenyu You"
                },
                {
                    "authorId": "2118021616",
                    "name": "Sunghun Kim"
                }
            ]
        },
        {
            "paperId": "68309384f7a8e96d98ede4271e7c04425c23f3f2",
            "title": "AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential Recommendation",
            "abstract": "Sequential recommendation (SR) aims to model users' dynamic preferences from a series of interactions. A pivotal challenge in user modeling for SR lies in the inherent variability of user preferences. An effective SR model is expected to capture both the long-term and short-term preferences exhibited by users, wherein the former can offer a comprehensive understanding of stable interests that impact the latter. To more effectively capture such information, we incorporate locality inductive bias into the Transformer by amalgamating its global attention mechanism with a local convolutional filter, and adaptively ascertain the mixing importance on a personalized basis through layer-aware adaptive mixture units, termed as AdaMCT. Moreover, as users may repeatedly browse potential purchases, it is expected to consider multiple relevant items concurrently in long-/short-term preferences modeling. Given that softmax-based attention may promote unimodal activation, we propose the Squeeze-Excitation Attention (with sigmoid activation) into SR models to capture multiple pertinent items (keys) simultaneously. Extensive experiments on three widely employed benchmarks substantiate the effectiveness and efficiency of our proposed approach. Source code is available at https://github.com/juyongjiang/AdaMCT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116325689",
                    "name": "Juyong Jiang"
                },
                {
                    "authorId": "2156009696",
                    "name": "Jae Boum Kim"
                },
                {
                    "authorId": "152908559",
                    "name": "Yingtao Luo"
                },
                {
                    "authorId": "2158520757",
                    "name": "Kai Zhang"
                },
                {
                    "authorId": "2118021616",
                    "name": "Sunghun Kim"
                }
            ]
        },
        {
            "paperId": "7e8ed8d7ed0bfa725ae14a1fa534fc67bc757b74",
            "title": "Improving Sequential Recommendations via Bidirectional Temporal Data Augmentation with Pre-training",
            "abstract": "Sequential recommendation systems are integral to discerning temporal user preferences. Yet, the task of learning from abbreviated user interaction sequences poses a notable challenge. Data augmentation has been identified as a potent strategy to enhance the informational richness of these sequences. Traditional augmentation techniques, such as item randomization, may disrupt the inherent temporal dynamics. Although recent advancements in reverse chronological pseudo-item generation have shown promise, they can introduce temporal discrepancies when assessed in a natural chronological context. In response, we introduce a sophisticated approach, Bidirectional temporal data Augmentation with pre-training (BARec). Our approach leverages bidirectional temporal augmentation and knowledge-enhanced fine-tuning to synthesize authentic pseudo-prior items that \\emph{retain user preferences and capture deeper item semantic correlations}, thus boosting the model's expressive power. Our comprehensive experimental analysis confirms the superiority of BARec across both short and elongated sequence contexts. Moreover, theoretical examination and visual representation of item embeddings offer further insight into the model's logical processes and interpretability. The source code for our study is available at \\textcolor{blue}{\\href{https://github.com/juyongjiang/BARec}{https://github.com/juyongjiang/BARec}}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116325689",
                    "name": "Juyong Jiang"
                },
                {
                    "authorId": "2156009696",
                    "name": "Jae Boum Kim"
                },
                {
                    "authorId": "152908559",
                    "name": "Yingtao Luo"
                },
                {
                    "authorId": "2158520757",
                    "name": "Kai Zhang"
                },
                {
                    "authorId": "2118021616",
                    "name": "Sunghun Kim"
                }
            ]
        },
        {
            "paperId": "d61762591500714b31896016831bdaca2ecef4d4",
            "title": "Sequential Recommendation with Bidirectional Chronological Augmentation of Transformer",
            "abstract": "Sequential recommendation can capture user chronological preferences from their historical behaviors, yet the learning of short sequences (cold start problem) is still an open challenge. Recently, data augmentation with pseudo-prior items generated by transformers has drawn considerable attention. These methods can generate pseudo-prior items sequentially in reverse chronological order to extend the training sequence. However, the performance may still degrade in very short sequences; most notably, the generation of pseudo-prior items does not take into account the forward direction (from the past to the future), and so the underlying temporal correlations are not preserved in terms of conditional probabilities. Motivated by this, we propose a Bidirectional Chronological Augmentation of Transformer (BiCAT) that uses a forward learning constraint in the reverse generative process to capture contextual information more effectively. Such self-knowledge distillation can bridge the gap between data augmentation and model representation, which enhances the robustness of sequence encoder. Moreover, an informative positive and negative sampling strategy is proposed to accelerate optimization and prevent overfitting. Extensive experiments on two popular real-world datasets demonstrate the efficacy of our method: on very short sequences ( \ud835\udc3f \u2264 3) and long sequences (20 < \ud835\udc3f \u2264 50), our approach can improve state-of-the-art performance by an average of 35.04% and 8.76% respectively, in terms of Recall@5. Source code is available at https://github.com/juyongjiang/BiCAT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116325689",
                    "name": "Juyong Jiang"
                },
                {
                    "authorId": "152908559",
                    "name": "Yingtao Luo"
                },
                {
                    "authorId": "2156009696",
                    "name": "Jae Boum Kim"
                },
                {
                    "authorId": null,
                    "name": "Kai Zhang"
                },
                {
                    "authorId": "1787729",
                    "name": "Sunghun Kim"
                }
            ]
        }
    ]
}