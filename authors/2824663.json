{
    "authorId": "2824663",
    "papers": [
        {
            "paperId": "10f8562d188da3bf8598ace15a06558c987bb899",
            "title": "Optimizing Hyperparameters with Conformal Quantile Regression",
            "abstract": "Many state-of-the-art hyperparameter optimization (HPO) algorithms rely on model-based optimizers that learn surrogate models of the target function to guide the search. Gaussian processes are the de facto surrogate model due to their ability to capture uncertainty but they make strong assumptions about the observation noise, which might not be warranted in practice. In this work, we propose to leverage conformalized quantile regression which makes minimal assumptions about the observation noise and, as a result, models the target function in a more realistic and robust fashion which translates to quicker HPO convergence on empirical benchmarks. To apply our method in a multi-fidelity setting, we propose a simple, yet effective, technique that aggregates observed results across different resource levels and outperforms conventional methods across many empirical tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144607961",
                    "name": "David Salinas"
                },
                {
                    "authorId": "1411129857",
                    "name": "Jacek Golebiowski"
                },
                {
                    "authorId": "145227684",
                    "name": "Aaron Klein"
                },
                {
                    "authorId": "1680574",
                    "name": "M. Seeger"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "6196920a11ebc04f35c049e4c0184085f044821e",
            "title": "Renate: A Library for Real-World Continual Learning",
            "abstract": "Continual learning enables the incremental training of machine learning models on non-stationary data streams.While academic interest in the topic is high, there is little indication of the use of state-of-the-art continual learning algorithms in practical machine learning deployment. This paper presents Renate, a continual learning library designed to build real-world updating pipelines for PyTorch models. We discuss requirements for the use of continual learning algorithms in practice, from which we derive design principles for Renate. We give a high-level description of the library components and interfaces. Finally, we showcase the strengths of the library by presenting experimental results. Renate may be found at https://github.com/awslabs/renate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2794970",
                    "name": "Martin Wistuba"
                },
                {
                    "authorId": "100998451",
                    "name": "Martin Ferianc"
                },
                {
                    "authorId": "8604693",
                    "name": "Lukas Balles"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                },
                {
                    "authorId": "3024156",
                    "name": "Giovanni Zappella"
                }
            ]
        },
        {
            "paperId": "ee8212720d7c5b3cfacc1abea993756b0adb4a94",
            "title": "Fortuna: A Library for Uncertainty Quantification in Deep Learning",
            "abstract": "We present Fortuna, an open-source library for uncertainty quantification in deep learning. Fortuna supports a range of calibration techniques, such as conformal prediction that can be applied to any trained neural network to generate reliable uncertainty estimates, and scalable Bayesian inference methods that can be applied to Flax-based deep neural networks trained from scratch for improved uncertainty quantification and accuracy. By providing a coherent framework for advanced uncertainty quantification methods, Fortuna simplifies the process of benchmarking and helps practitioners build robust AI systems.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "51039593",
                    "name": "Gianluca Detommaso"
                },
                {
                    "authorId": "32264326",
                    "name": "Alberto Gasparin"
                },
                {
                    "authorId": "2192704",
                    "name": "Michele Donini"
                },
                {
                    "authorId": "1680574",
                    "name": "M. Seeger"
                },
                {
                    "authorId": "145771261",
                    "name": "A. Wilson"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "08f14bc8734253ba886b91531c68ff88393a5a84",
            "title": "Syne Tune: A Library for Large Scale Hyperparameter Tuning and Reproducible Research",
            "abstract": "We present Syne Tune, a library for large-scale distributed hyperparameter optimization (HPO). 1 Syne Tune\u2019s modular architecture allows users to easily switch between different execution backends to facilitate experimentation and makes it easy to contribute new optimization algorithms. To foster reproducible benchmarking, Syne Tune provides an efficient simulator backend and a benchmarking suite, which are essential for large-scale evaluations of distributed asynchronous HPO algorithms on tabulated and surrogate benchmarks. We showcase these functionalities with a range of state-of-the-art gradient-free optimizers, including multi-fidelity and transfer learning approaches on popular benchmarks from the literature. Additionally, we demonstrate the benefits of Syne Tune for constrained and multiobjective HPO applications through two use cases: the former considers hyperparameters that induce fair solutions and the latter automatically selects machine types along with the conventional hyperparameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144607961",
                    "name": "David Salinas"
                },
                {
                    "authorId": "1680574",
                    "name": "M. Seeger"
                },
                {
                    "authorId": "145227684",
                    "name": "Aaron Klein"
                },
                {
                    "authorId": "19174917",
                    "name": "Valerio Perrone"
                },
                {
                    "authorId": "2794970",
                    "name": "Martin Wistuba"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "3145014e65861f3981a652222daf964bca3bbe62",
            "title": "Diverse Counterfactual Explanations for Anomaly Detection in Time Series",
            "abstract": "Data-driven methods that detect anomalies in times series data are ubiquitous in practice, but they are in general unable to provide helpful explanations for the predictions they make. In this work we propose a model-agnostic algorithm that generates counterfactual ensemble explanations for time series anomaly detection models. Our method generates a set of diverse counterfactual examples, i.e, multiple perturbed versions of the original time series that are not considered anomalous by the detection model. Since the magnitude of the perturbations is limited, these counterfactuals represent an ensemble of inputs similar to the original time series that the model would deem normal. Our algorithm is applicable to any differentiable anomaly detection model. We investigate the value of our method on univariate and multivariate real-world datasets and two deep-learning-based anomaly detection models, under several explainability criteria previously proposed in other data domains such as Validity, Plausibility, Closeness and Diversity. We show that our algorithm can produce ensembles of counterfactual examples that satisfy these criteria and thanks to a novel type of visualisation, can convey a richer interpretation of a model's internal mechanism than existing methods. Moreover, we design a sparse variant of our method to improve the interpretability of counterfactual explanations for high-dimensional time series anomalies. In this setting, our explanation is localised on only a few dimensions and can therefore be communicated more efficiently to the model's user.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2285351482",
                    "name": "Deborah Sulem"
                },
                {
                    "authorId": "2192704",
                    "name": "Michele Donini"
                },
                {
                    "authorId": "2625318",
                    "name": "M. B. Zafar"
                },
                {
                    "authorId": "51055581",
                    "name": "Fran\u00e7ois-Xavier Aubet"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "2155897297",
                    "name": "Sanjiv Das"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "393fd928d39d067f865b6ebe2a97b33604ca02cf",
            "title": "Memory Efficient Continual Learning with Transformers",
            "abstract": "In many real-world scenarios, data to train machine learning models becomes available over time. Unfortunately, these models struggle to continually learn new concepts without forgetting what has been learnt in the past. This phenomenon is known as catastrophic forgetting and it is difficult to prevent due to practical constraints. For instance, the amount of data that can be stored or the computational resources that can be used might be limited. Moreover, applications increasingly rely on large pre-trained neural networks, such as pre-trained Transformers, since the resources or data might not be available in sufficiently large quantities to practitioners to train the model from scratch. In this paper, we devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters. Different than the existing approaches, our method is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks. On both image and text classification tasks, we empirically demonstrate that our method maintains a good predictive performance without retraining the model or increasing the number of model parameters over time. The resulting model is also significantly faster at inference time compared to Adapter-based state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2445273",
                    "name": "B. Ermi\u015f"
                },
                {
                    "authorId": "3024156",
                    "name": "Giovanni Zappella"
                },
                {
                    "authorId": "2794970",
                    "name": "Martin Wistuba"
                },
                {
                    "authorId": "2064309559",
                    "name": "Aditya Rawal"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "42588c5feaaf189f2d7584a0562264a8c0662ede",
            "title": "Uncertainty Calibration in Bayesian Neural Networks via Distance-Aware Priors",
            "abstract": "As we move away from the data, the predictive uncertainty should increase, since a great variety of explanations are consistent with the little available information. We introduce Distance-Aware Prior (DAP) calibration, a method to correct overconfidence of Bayesian deep learning models outside of the training domain. We define DAPs as prior distributions over the model parameters that depend on the inputs through a measure of their distance from the training set. DAP calibration is agnostic to the posterior inference method, and it can be performed as a post-processing step. We demonstrate its effectiveness against several baselines in a variety of classification and regression problems, including benchmarks designed to test the quality of predictive distributions away from the data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "51039593",
                    "name": "Gianluca Detommaso"
                },
                {
                    "authorId": "32264326",
                    "name": "Alberto Gasparin"
                },
                {
                    "authorId": "145771261",
                    "name": "A. Wilson"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "516f19c1cdf2dfd130b262fa53ce876fd015b86f",
            "title": "Gradient-Matching Coresets for Rehearsal-Based Continual Learning",
            "abstract": "The goal of continual learning (CL) is to efficiently update a machine learning model with new data without forgetting previously-learned knowledge. Most widely-used CL methods rely on a rehearsal memory of data points to be reused while training on new data. Curating such a rehearsal memory to maintain a small, informative subset of all the data seen so far is crucial to the success of these methods. We devise a coreset selection method for rehearsal-based continual learning. Our method is based on the idea of gradient matching: The gradients induced by the coreset should match, as closely as possible, those induced by the original training dataset. Inspired by the neural tangent kernel theory, we perform this gradient matching across the model's initialization distribution, allowing us to extract a coreset without having to train the model first. We evaluate the method on a wide range of continual learning scenarios and demonstrate that it improves the performance of rehearsal-based CL methods compared to competing memory management strategies such as reservoir sampling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8604693",
                    "name": "Lukas Balles"
                },
                {
                    "authorId": "3024156",
                    "name": "Giovanni Zappella"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "7658bf257dbe1ee2455d80b95be072c6f5135ffd",
            "title": "Continual Learning with Transformers for Image Classification",
            "abstract": "In many real-world scenarios, data to train machine learning models become available over time. However, neural network models struggle to continually learn new concepts without forgetting what has been learnt in the past. This phenomenon is known as catastrophic forgetting and it is often difficult to prevent due to practical constraints, such as the amount of data that can be stored or the limited computation sources that can be used. Moreover, training large neural networks, such as Transformers, from scratch is very costly and requires a vast amount of training data, which might not be available in the application domain of interest. A recent trend indicates that dynamic architectures based on an expansion of the parameters can reduce catastrophic forgetting efficiently in continual learning, but this needs complex tuning to balance the growing number of parameters and barely share any information across tasks. As a result, they struggle to scale to a large number of tasks without significant overhead. In this paper, we validate in the computer vision domain a recent solution called Adaptive Distillation of Adapters (ADA), which is developed to perform continual learning using pre-trained Transformers and Adapters on text classification tasks. We empirically demonstrate on different classification tasks that this method maintains a good predictive performance without retraining the model or increasing the number of model parameters over the time. Besides it is significantly faster at inference time compared to the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2445273",
                    "name": "B. Ermi\u015f"
                },
                {
                    "authorId": "3024156",
                    "name": "Giovanni Zappella"
                },
                {
                    "authorId": "2794970",
                    "name": "Martin Wistuba"
                },
                {
                    "authorId": "2064309559",
                    "name": "Aditya Rawal"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "ae00d8f00c2c0b18ad7046f55fce99a47ae10eea",
            "title": "Memory Efficient Continual Learning for Neural Text Classification",
            "abstract": "Learning text classi\ufb01ers based on pre-trained language models has become the standard practice in natural language processing applications. Unfortunately, training large neural language models, such as transformers, from scratch is very costly and requires a vast amount of training data, which might not be available in the application domain of interest. Moreover, in many real-world scenarios, classes are uncovered as more data is seen, calling for class-incremental modelling approaches. In this work we devise a method to perform text classi\ufb01cation using pre-trained models on a sequence of classi\ufb01cation tasks provided in sequence. We formalize the problem as a continual learning problem where the algorithm learns new tasks without performance degradation on the previous ones and without re-training the model from scratch. We empirically demonstrate that our method requires signi\ufb01cantly less model parameters compared to other state of the art meth-ods and that it is signi\ufb01cantly faster at inference time. The tight control on the number of model parameters, and so the memory, is not only improving ef\ufb01ciency. It is making possible the usage of the algorithm in real-world applications where deploying a solution with a constantly increasing memory consumption is just unrealistic. While our method suffers little forgetting, it retains a predictive performance on-par with state of the art but less memory ef\ufb01cient methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2445273",
                    "name": "B. Ermi\u015f"
                },
                {
                    "authorId": "3024156",
                    "name": "Giovanni Zappella"
                },
                {
                    "authorId": "2794970",
                    "name": "Martin Wistuba"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        }
    ]
}