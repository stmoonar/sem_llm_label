{
    "authorId": "152662346",
    "papers": [
        {
            "paperId": "0925cb188cb1863b2e2d672a0788dc9687ec7694",
            "title": "MGEL: Multigrained Representation Analysis and Ensemble Learning for Text Moderation",
            "abstract": "In this work, we describe our efforts in addressing two typical challenges involved in the popular text classification methods when they are applied to text moderation: the representation of multibyte characters and word obfuscations. Specifically, a multihot byte-level scheme is developed to significantly reduce the dimension of one-hot character-level encoding caused by the multiplicity of instance-scarce non-ASCII characters. In addition, we introduce a simple yet effective weighting approach for fusing n-gram features to empower the classical logistic regression. Surprisingly, it outperforms well-tuned representative neural networks greatly. As a continual effort toward text moderation, we endeavor to analyze the current state-of-the-art (SOTA) algorithm bidirectional encoder representations from transformers (BERT), which works well in context understanding but performs poorly on intentional word obfuscations. To resolve this crux, we then develop an enhanced variant and remedy this drawback by integrating byte and character decomposition. It advances the SOTA performance on the largest abusive language datasets as demonstrated by our comprehensive experiments. Our work offers a feasible and effective framework to tackle word obfuscations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1491233580",
                    "name": "Fei Tan"
                },
                {
                    "authorId": "46622514",
                    "name": "Changwei Hu"
                },
                {
                    "authorId": "1809492",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "2153551136",
                    "name": "Zhi Wei"
                },
                {
                    "authorId": "35164325",
                    "name": "Aasish Pappu"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "40662871",
                    "name": "Keqian Li"
                }
            ]
        },
        {
            "paperId": "d74f2399adb29ec8bfde74d4d5e8be4843e4888c",
            "title": "TwHIN: Embedding the Twitter Heterogeneous Information Network for Personalized Recommendation",
            "abstract": "Social networks, such as Twitter, form a heterogeneous information network (HIN) where nodes represent domain entities (e.g., user, content, advertiser, etc.) and edges represent one of many entity interactions (e.g, a user re-sharing content or \"following\" another). Interactions from multiple relation types can encode valuable information about social network entities not fully captured by a single relation; for instance, a user's preference for accounts to follow may depend on both user-content engagement interactions and the other users they follow. In this work, we investigate knowledge-graph embeddings for entities in the Twitter HIN (TwHIN); we show that these pretrained representations yield significant offline and online improvement for a diverse range of downstream recommendation and classification tasks: personalized ads rankings, account follow-recommendation, offensive content detection, and search ranking. We discuss design choices and practical challenges of deploying industry-scale HIN embeddings, including compressing them to reduce end-to-end model latency and handling parameter drift across versions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                },
                {
                    "authorId": "47966426",
                    "name": "Thomas Markovich"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "34777224",
                    "name": "C. Verma"
                },
                {
                    "authorId": "94100042",
                    "name": "Baekjin Kim"
                },
                {
                    "authorId": "37558091",
                    "name": "R. Eskander"
                },
                {
                    "authorId": "2104662",
                    "name": "Yury Malkov"
                },
                {
                    "authorId": "1661216672",
                    "name": "Frank Portman"
                },
                {
                    "authorId": "151367676",
                    "name": "Sofia Samaniego"
                },
                {
                    "authorId": "2122804028",
                    "name": "Ying Xiao"
                },
                {
                    "authorId": "1761880",
                    "name": "A. Haghighi"
                }
            ]
        },
        {
            "paperId": "f5888d776f122f53292973bd3693628ebd265bc6",
            "title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter",
            "abstract": "Pre-trained language models (PLMs) are fundamental for natural language processing applications. Most existing PLMs are not tailored to the noisy user-generated text on social media, and the pre-training does not factor in the valuable social engagement logs available in a social network. We present TwHIN-BERT, a multilingual language model productionized at Twitter, trained on in-domain data from the popular social network. TwHIN-BERT differs from prior pre-trained language models as it is trained with not only text-based self-supervision but also with a social objective based on the rich social engagements within a Twitter heterogeneous information network (TwHIN). Our model is trained on 7 billion tweets covering over 100 distinct languages, providing a valuable representation to model short, noisy, user-generated text. We evaluate our model on various multilingual social recommendation and semantic understanding tasks and demonstrate significant metric improvement over established pre-trained language models. We open-source TwHIN-BERT and our curated hashtag prediction and social engagement benchmark datasets to the research community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108262811",
                    "name": "Xinyang Zhang"
                },
                {
                    "authorId": "2104662",
                    "name": "Yury Malkov"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "2953855",
                    "name": "B. McWilliams"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1398503968",
                    "name": "Ahmed El-Kishky"
                }
            ]
        },
        {
            "paperId": "3ae6450d4d1181b72d1a93aab832afec1b258c03",
            "title": "HABERTOR: An Efficient and Effective Deep Hatespeech Detector",
            "abstract": "We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. HABERTOR inherits BERT's architecture, but is different in four aspects: (i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; (ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; (iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our HABERTOR is 4~5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1% of the number of words. Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072580889",
                    "name": "T. Tran"
                },
                {
                    "authorId": "1809492",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "46622514",
                    "name": "Changwei Hu"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "1491233580",
                    "name": "Fei Tan"
                },
                {
                    "authorId": "2848353",
                    "name": "Kyumin Lee"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                }
            ]
        },
        {
            "paperId": "db16b52c4cf2c110e31f1606c1a2e8ca7d8e4f38",
            "title": "Transport-based model for turbulence-corrupted imagery.",
            "abstract": "A new model for turbulence-corrupted imagery is proposed based on the theory of optimal mass transport. By describing the relationship between photon density and the phase of the traveling wave, and combining it with a least action principle, the model suggests a new class of methods for approximately recovering the solution of the photon density flow created by a turbulent atmosphere. Both coherent and incoherent imagery are used to validate and compare the model to other methods typically used to describe this type of data. Given its superior performance in describing experimental data, the new model suggests new algorithms for a variety of atmospheric imaging and wave propagation applications.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2034683",
                    "name": "J. Nichols"
                },
                {
                    "authorId": "47341214",
                    "name": "T. Emerson"
                },
                {
                    "authorId": "27526930",
                    "name": "L. Cattell"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "90842119",
                    "name": "A. Kanaev"
                },
                {
                    "authorId": "3316260",
                    "name": "F. Bucholtz"
                },
                {
                    "authorId": "13103553",
                    "name": "A. Watnik"
                },
                {
                    "authorId": "2886830",
                    "name": "T. Doster"
                },
                {
                    "authorId": "144703687",
                    "name": "G. Rohde"
                }
            ]
        },
        {
            "paperId": "f395606853a78d6f64b1f13be8938293fdc114ef",
            "title": "Representing and Learning High Dimensional Data with the Optimal Transport Map from a Probabilistic Viewpoint",
            "abstract": "In this paper, we propose a generative model in the space of diffeomorphic deformation maps. More precisely, we utilize the Kantarovich-Wasserstein metric and accompanying geometry to represent an image as a deformation from templates. Moreover, we incorporate a probabilistic viewpoint by assuming that each image is locally generated from a reference image. We capture the local structure by modelling the tangent planes at reference images. Once basis vectors for each tangent plane are learned via probabilistic PCA, we can sample a local coordinate, that can be inverted back to image space exactly. With experiments using 4 different datasets, we show that the generative tangent plane model in the optimal transport (OT) manifold can be learned with small numbers of images and can be used to create infinitely many 'unseen' images. In addition, the Bayesian classification accompanied with the probabilist modeling of the tangent planes shows improved accuracy over that done in the image space. Combining the results of our experiments supports our claim that certain datasets can be better represented with the Kantarovich-Wasserstein metric. We envision that the proposed method could be a practical solution to learning and representing data that is generated with templates in situatons where only limited numbers of data points are available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "2999959",
                    "name": "Matthew Thorpe"
                }
            ]
        },
        {
            "paperId": "abb5cdfb79f21b37e358a89dc0556bbb927fdc99",
            "title": "Transport-based analysis, modeling, and learning from signal and data distributions",
            "abstract": "Transport-based techniques for signal and data analysis have received increased attention recently. Given their abilities to provide accurate generative models for signal intensities and other data distributions, they have been used in a variety of applications including content-based retrieval, cancer detection, image super-resolution, and statistical machine learning, to name a few, and shown to produce state of the art in several applications. Moreover, the geometric characteristics of transport-related metrics have inspired new kinds of algorithms for interpreting the meaning of data distributions. Here we provide an overview of the mathematical underpinnings of mass transport-related methods, including numerical implementation, as well as a review, with demonstrations, of several applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2062432",
                    "name": "Soheil Kolouri"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "2999959",
                    "name": "Matthew Thorpe"
                },
                {
                    "authorId": "2318292",
                    "name": "D. Slep\u010dev"
                },
                {
                    "authorId": "144703687",
                    "name": "G. Rohde"
                }
            ]
        }
    ]
}