{
    "authorId": "33285410",
    "papers": [
        {
            "paperId": "01e338c4026c475dc22a30507002c7061a99986d",
            "title": "Thompson Sampling with Less Exploration is Fast and Optimal",
            "abstract": "We propose \u03f5 -Exploring Thompson Sampling ( \u03f5 - TS), a modified version of the Thompson Sampling (TS) algorithm (Agrawal & Goyal, 2017) for multi-armed bandits. In \u03f5 -TS, arms are selected greedily based on empirical mean rewards with probability 1 \u2212 \u03f5 , and based on posterior samples obtained from TS with probability \u03f5 . Here, \u03f5 \u2208 (0 , 1) is a user-defined constant. By reducing exploration, \u03f5 -TS improves computational efficiency compared to TS while achieving better regret bounds. We establish that \u03f5 -TS is both minimax optimal and asymptotically optimal for various popular reward distributions, including Gaussian, Bernoulli, Poisson, and Gamma. A key technical advancement in our analysis is the relaxation of the requirement for a stringent anti-concentration bound of the posterior distribution, which was necessary in recent analyses that achieved similar bounds (Jin et al., 2021b; 2022). As a result, \u03f5 -TS maintains the posterior update structure of TS while minimizing alterations, such as clipping the sampling distribution or solving the inverse of the Kullback-Leibler (KL) divergence between reward distributions, as done in previous work. Furthermore, our algorithm is as easy to implement as TS, but operates significantly faster due to reduced exploration. Empirical evaluations confirm the efficiency and optimality of \u03f5 -TS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28449813",
                    "name": "Tianyuan Jin"
                },
                {
                    "authorId": "2046911289",
                    "name": "Xiangli Yang"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "47568847",
                    "name": "Pan Xu"
                }
            ]
        },
        {
            "paperId": "8cc3e781dab8bfdb0063d9a4237d5443be3d70e9",
            "title": "Capacity Constrained Influence Maximization in Social Networks",
            "abstract": "Influence maximization (IM) aims to identify a small number of influential individuals to maximize the information spread and finds applications in various fields. It was first introduced in the context of viral marketing, where a company pays a few influencers to promote the product. However, apart from the cost factor, the capacity of individuals to consume content poses challenges for implementing IM in real-world scenarios. For example, players on online gaming platforms can only interact with a limited number of friends. In addition, we observe that in these scenarios, (i) the initial adopters of promotion are likely to be the friends of influencers rather than the influencers themselves, and (ii) existing IM solutions produce sub-par results with high computational demands. Motivated by these observations, we propose a new IM variant called capacity constrained influence maximization (CIM), which aims to select a limited number of influential friends for each initial adopter such that the promotion can reach more users. To solve CIM effectively, we design two greedy algorithms, MG-Greedy and RR-Greedy, ensuring the 1/2-approximation ratio. To improve the efficiency, we devise the scalable implementation named RR-OPIM+ with (1/2-\u03b5)-approximation and near-linear running time. We extensively evaluate the performance of 9 approaches on 6 real-world networks, and our solutions outperform all competitors in terms of result quality and running time. Additionally, we deploy RR-OPIM+ to online game scenarios, which improves the baseline considerably.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2601786",
                    "name": "Shiqi Zhang"
                },
                {
                    "authorId": "2127733390",
                    "name": "Yiqi Huang"
                },
                {
                    "authorId": "2301268329",
                    "name": "Jiachen Sun"
                },
                {
                    "authorId": "1814276",
                    "name": "Wenqing Lin"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "2105635181",
                    "name": "Bo Tang"
                }
            ]
        },
        {
            "paperId": "9028fb9f46e423b037660de15a0fe10526a2c7a8",
            "title": "Multi-Task Processing in Vertex-Centric Graph Systems: Evaluations and Insights",
            "abstract": "Vertex-centric (VC) graph systems are at the core of large-scale distributed graph processing. For such systems, a common usage pattern is the concurrent processing of multiple tasks ( multi-processing for short), which aims to execute a large number of unit tasks in parallel. In this paper, we point out that multi-processing has not been sufficiently studied or evaluated in previous work; hence, we fill this critical gap with three major contributions. First, we examine the tradeoff between two important measures in VC-systems: the number of communication rounds and message congestion . We show that this tradeoff is crucial to system performance; yet, existing approaches fail to achieve an optimal tradeoff, leading to poor performance. Second, based on extensive experimental evaluations on mainstream VC systems (e.g., Giraph, Pregel+, GraphD) and benchmark multi-processing tasks (e.g., Batch Personalized PageRanks, Multiple Source Shortest Paths), we present several important insights on the correlation between system performance and configurations, which is valuable to practitioners in optimizing system performance. Third, based on the insights drawn from our experimental evaluations, we present a cost-based tuning framework that optimizes the performance of a representative VC-system. This demonstrates the usefulness of the insights.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "2140564220",
                    "name": "Zichen Zhu"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "2118771474",
                    "name": "Y. Yang"
                },
                {
                    "authorId": "2109336214",
                    "name": "Chunbo Li"
                },
                {
                    "authorId": "2064074014",
                    "name": "B. Kao"
                }
            ]
        },
        {
            "paperId": "a207e3b032c9d79cd2b5cd5123426a6411733ae3",
            "title": "PrivLava: Synthesizing Relational Data with Foreign Keys under Differential Privacy",
            "abstract": "Answering database queries while preserving privacy is an important problem that has attracted considerable research attention in recent years. A canonical approach to this problem is to use synthetic data. That is, we replace the input database R with a synthetic database R* that preserves the characteristics of R, and use R* to answer queries. Existing solutions for relational data synthesis, however, either fail to provide strong privacy protection, or assume that R contains a single relation. In addition, it is challenging to extend the existing single-relation solutions to the case of multiple relations, because they are unable to model the complex correlations induced by the foreign keys. Therefore, multi-relational data synthesis with strong privacy guarantees is an open problem. In this paper, we address the above open problem by proposing PrivLava, the first solution for synthesizing relational data with foreign keys under differential privacy, a rigorous privacy framework widely adopted in both academia and industry. The key idea of PrivLava is to model the data distribution in R using graphical models, with latent variables included to capture the inter-relational correlations caused by foreign keys. We show that PrivLava supports arbitrary foreign key references that form a directed acyclic graph, and is able to tackle the common case when R contains a mixture of public and private relations. Extensive experiments on census data sets and the TPC-H benchmark demonstrate that PrivLava significantly outperforms its competitors in terms of the accuracy of aggregate queries processed on the synthetic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2124455504",
                    "name": "Kuntai Cai"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                }
            ]
        },
        {
            "paperId": "d2f2e94379b117341202d99f5382b3fe824cf093",
            "title": "Efficient and Effective Edge-wise Graph Representation Learning",
            "abstract": "Graph representation learning (GRL) is a powerful tool for graph analysis, which has gained massive attention from both academia and industry due to its superior performance in various real-world applications. However, the majority of existing works for GRL are dedicated to node-based tasks and thus focus on producing node representations. Despite such methods can be used to derive edge representations by regarding edges as nodes, they suffer from sub-par result utility in practical edge-wise applications, such as financial fraud detection and review spam combating, due to neglecting the unique properties of edges and their inherent drawbacks. Moreover, to our knowledge, there is a paucity of research devoted to edge representation learning. These methods either require high computational costs in sampling random walks or yield severely compromised representation quality because of falling short of capturing high-order information between edges. To address these challenges, we present TER and AER, which generate high-quality edge representation vectors based on the graph structure surrounding edges and edge attributes, respectively. In particular, TER can accurately encode high-order proximities of edges into low-dimensional vectors in a practically efficient and theoretically sound way, while AER augments edge attributes through a carefully-designed feature aggregation scheme. Our extensive experimental study demonstrates that the combined edge representations of TER and AER can achieve significantly superior performance in terms of edge classification on 8 real-life datasets, while being up to one order of magnitude faster than 16 baselines on large graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2202841029",
                    "name": "Hewen Wang"
                },
                {
                    "authorId": "2115421976",
                    "name": "Renchi Yang"
                },
                {
                    "authorId": "2112440523",
                    "name": "Keke Huang"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                }
            ]
        },
        {
            "paperId": "f8730e36518370ac19821e12570a8044c79e5489",
            "title": "Efficient Approximation Algorithms for Spanning Centrality",
            "abstract": "Given a graph \\mathcalG , the spanning centrality (SC) of an edge e measures the importance of e for \\mathcalG to be connected. In practice, SC has seen extensive applications in computational biology, electrical networks, and combinatorial optimization. However, it is highly challenging to compute the SC of all edges (AESC) on large graphs. Existing techniques fail to deal with such graphs, as they either suffer from expensive matrix operations or require sampling numerous long random walks. To circumvent these issues, this paper proposes TGT and its enhanced version TGT+, two algorithms for AESC computation that offers rigorous theoretical approximation guarantees. In particular, TGT remedies the deficiencies of previous solutions by conducting deterministic graph traversals with carefully-crafted truncated lengths. TGT+ further advances TGT in terms of both empirical efficiency and asymptotic performance while retaining result quality, based on the combination of TGT with random walks and several additional heuristic optimizations. We experimentally evaluate TGT+ against recent competitors for AESC using a variety of real datasets. The experimental outcomes authenticate that TGT+ outperforms state of the arts often by over one order of magnitude speedup without degrading the accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2601786",
                    "name": "Shiqi Zhang"
                },
                {
                    "authorId": "2115421976",
                    "name": "Renchi Yang"
                },
                {
                    "authorId": "144066821",
                    "name": "Jing Tang"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "2105635181",
                    "name": "Bo Tang"
                }
            ]
        },
        {
            "paperId": "45ee667ed362f03da3701eed8d5fde438356910f",
            "title": "Scalable and Effective Bipartite Network Embedding",
            "abstract": "Given a bipartite graph G consisting of inter-set weighted edges connecting the nodes in two disjoint sets U and V, bipartite network embedding (BNE) maps each node ui in U and vj in V to compact embedding vectors that capture the hidden topological features surrounding the nodes, to facilitate downstream tasks. Effective BNE should preserve not only the direct connections between nodes but also the multi-hop relationships formed alternately by the two types of nodes in G, which can incur prohibitive overheads, especially on massive bipartite graphs with millions of nodes and billions of edges. Existing solutions are hardly scalable to massive bipartite graphs, and often produce low-quality results. This paper proposes GEBE, a generic BNE framework achieving state-of-the-art performance on massive bipartite graphs, via four main algorithmic designs. First, we present two generic measures to capture the multi-hop similarity/proximity between homogeneous/heterogeneous nodes respectively, and the measures can be instantiated with three popular probability distributions, including Poisson, Geometric, and Uniform distributions. Second, GEBE formulates a novel and unified BNE objective to preserve the two measures of all possible node pairs. Third, GEBE includes several efficiency designs to get high-quality embeddings on massive graphs. Finally, we observe that GEBE achieves the best performance when instantiating MHS and MHP using a Poisson distribution, and thus, we further develop GEBEp based on Poisson-instantiated MHS and MHP, with non-trivial efficiency optimizations. Extensive experiments, comparing 15 competitors on 10 real datasets, demonstrate that our solutions, especially GEBEp, obtain superior result utility than all competitors for top-N recommendation and link prediction, while being up to orders of magnitude faster.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115421976",
                    "name": "Renchi Yang"
                },
                {
                    "authorId": "35774994",
                    "name": "Jieming Shi"
                },
                {
                    "authorId": "2112440523",
                    "name": "Keke Huang"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                }
            ]
        },
        {
            "paperId": "b11d011e47782cb9791720555c76acc6864d86ca",
            "title": "VC-Tune: Tuning and Exploring Distributed Vertex-Centric Graph Systems",
            "abstract": "Distributed vertex-centric graph systems, or VC-systems, have achieved tremendous success in the industry. A common usage pattern of VC-systems is multi-processing, or the concurrent processing of multiple unit tasks. Example multi-processing includes answering multiple single-source shortest path queries on a graph. However, concurrent processing of all the unit tasks may overload the system with excessive memory usage, leading to intolerable system delays. To ad-dress the important challenge, we present V C- Tune, a system with a convenient interface to help practitioners orchestrate the unit tasks for improving the overall performance within the system limit. This demonstration allows the audience to interact with our system to explore the configuration space of multi-processing in VC-systems and compare different sys-tem configurations. In addition, we embed into the system an automatic configuration search algorithm, which can directly recommend to the practitioners a suitable configuration that gives a satisfactory system performance. An introduction video is at (https://sites.google.com/view/vc-tune-video).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140564220",
                    "name": "Zichen Zhu"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "2118771474",
                    "name": "Y. Yang"
                },
                {
                    "authorId": "2135922552",
                    "name": "Dingheng Mo"
                },
                {
                    "authorId": "2180287037",
                    "name": "Yufei Han"
                }
            ]
        },
        {
            "paperId": "cce9e3a74cb38622f1b083a0a76a0ae12b53d04a",
            "title": "I/O-Efficient Algorithms for Degeneracy Computation on Massive Networks",
            "abstract": "Degeneracy is an important concept to measure the sparsity of a graph which has been widely used in many network analysis applications. Many network analysis algorithms, such as clique enumeration and truss decomposition, perform very well in graphs having small degeneracies. In this paper, we propose an I/O-efficient algorithm to compute the degeneracy of the massive graph that cannot be fully kept in the main memory. The proposed algorithm only uses <inline-formula><tex-math notation=\"LaTeX\">$O(n)$</tex-math> <alternatives><mml:math><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"wang-ieq1-3021484.gif\"/></alternatives></inline-formula> memory, where <inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math><alternatives><mml:math><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href=\"wang-ieq2-3021484.gif\"/></alternatives></inline-formula> denotes the number of nodes of the graph. We also develop an I/O-efficient algorithm to incrementally maintain the degeneracy on dynamic graphs. Extensive experiments show that our algorithms significantly outperform the state-of-the-art degeneracy computation algorithms in terms of both running time and I/O costs. The results also demonstrate high scalability of the proposed algorithms. For example, in a real-world web graph with 930 million nodes and 13.3 billion edges, the proposed algorithm takes only 633 seconds and uses less than 4.5GB memory to compute the degeneracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121775",
                    "name": "Ronghua Li"
                },
                {
                    "authorId": "2112703203",
                    "name": "Qiushuo Song"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "2316506750",
                    "name": "Lu Qin"
                },
                {
                    "authorId": "2240263835",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "2143666425",
                    "name": "Rui Mao"
                }
            ]
        },
        {
            "paperId": "e4b720544adbc1c088d61e2eea57965497c5455b",
            "title": "Approximating Probabilistic Group Steiner Trees in Graphs",
            "abstract": "\n Consider an edge-weighted graph, and a number of properties of interests (PoIs). Each vertex has a probability of exhibiting each PoI. The joint probability that a set of vertices exhibits a PoI is the probability that this set contains at least one vertex that exhibits this PoI. The\n probabilistic group Steiner tree\n problem is to find a tree such that (i) for each PoI, the joint probability that the set of vertices in this tree exhibits this PoI is no smaller than a threshold value,\n e.g.\n , 0.97; and (ii) the total weight of edges in this tree is the minimum. Solving this problem is useful for mining various graphs with uncertain vertex properties, but is NP-hard. The existing work focuses on certain cases, and cannot perform this task. To meet this challenge, we propose 3 approximation algorithms for solving the above problem. Let |\u0393| be the number of PoIs, and \u03be be an upper bound of the number of vertices for satisfying the threshold value of exhibiting each PoI. Algorithms 1 and 2 have tight approximation guarantees proportional to |\u0393| and \u03be, and exponential time complexities with respect to \u03be and |\u0393|, respectively. In comparison, Algorithm 3 has a looser approximation guarantee proportional to, and a polynomial time complexity with respect to, both |\u0393| and \u03be. Experiments on real and large datasets show that the proposed algorithms considerably outperform the state-of-the-art related work for finding probabilistic group Steiner trees in various cases.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2190977970",
                    "name": "Shuang Yang"
                },
                {
                    "authorId": "2108587727",
                    "name": "Yahui Sun"
                },
                {
                    "authorId": "2167513185",
                    "name": "Jiesong Liu"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "2121775",
                    "name": "Ronghua Li"
                },
                {
                    "authorId": "12457830",
                    "name": "Zhewei Wei"
                }
            ]
        }
    ]
}