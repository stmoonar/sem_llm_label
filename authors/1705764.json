{
    "authorId": "1705764",
    "papers": [
        {
            "paperId": "80e0b9019b5b0a5eab3ec00a35572a08a15b8691",
            "title": "Rebalancing Social Feed to Minimize Polarization and Disagreement",
            "abstract": "Social media have great potential for enabling public discourse on important societal issues. However, adverse effects, such as polarization and echo chambers, greatly impact the benefits of social media and call for algorithms that mitigate these effects. In this paper, we propose a novel problem formulation aimed at slightly nudging users' social feeds in order to strike a balance between relevance and diversity, thus mitigating the emergence of polarization, without lowering the quality of the feed. Our approach is based on re-weighting the relative importance of the accounts that a user follows, so as to calibrate the frequency with which the content produced by various accounts is shown to the user. We analyze the convexity properties of the problem, demonstrating the non-matrix convexity of the objective function and the convexity of the feasible set. To efficiently address the problem, we develop a scalable algorithm based on projected gradient descent. We also prove that our problem statement is a proper generalization of the undirected-case problem so that our method can also be adopted for undirected social networks. As a baseline for comparison in the undirected case, we develop a semidefinite programming approach, which provides the optimal solution. Through extensive experiments on synthetic and real-world datasets, we validate the effectiveness of our approach, which outperforms non-trivial baselines, underscoring its ability to foster healthier and more cohesive online communities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742226337",
                    "name": "Federico Cinus"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "b739ec5306307f3575547f83c5ab2cf7c65c4a17",
            "title": "Hyper-distance Oracles in Hypergraphs",
            "abstract": "We study point-to-point distance estimation in hypergraphs, where the query is parameterized by a positive integer s, which defines the required level of overlap for two hyperedges to be considered adjacent. To answer s-distance queries, we first explore an oracle based on the line graph of the given hypergraph and discuss its limitations: the main one is that the line graph is typically orders of magnitude larger than the original hypergraph. We then introduce HypED, a landmark-based oracle with a predefined size, built directly on the hypergraph, thus avoiding constructing the line graph. Our framework allows to approximately answer vertex-to-vertex, vertex-to-hyperedge, and hyperedge-to-hyperedge s-distance queries for any value of s. A key observation at the basis of our framework is that, as s increases, the hypergraph becomes more fragmented. We show how this can be exploited to improve the placement of landmarks, by identifying the s-connected components of the hypergraph. For this task, we devise an efficient algorithm based on the union-find technique and a dynamic inverted index. We experimentally evaluate HypED on several real-world hypergraphs and prove its versatility in answering s-distance queries for different values of s. Our framework allows answering such queries in fractions of a millisecond, while allowing fine-grained control of the trade-off between index size and approximation error at creation time. Finally, we prove the usefulness of the s-distance oracle in two applications, namely, hypergraph-based recommendation and the approximation of the s-closeness centrality of vertices and hyper-edges in the context of protein-to-protein interactions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39046274",
                    "name": "Giulia Preti"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "0a063a94b3c1c81e505fab18b45429909e067bf1",
            "title": "FreSCo: Mining Frequent Patterns in Simplicial Complexes",
            "abstract": "Simplicial complexes are a generalization of graphs that model higher-order relations. In this paper, we introduce simplicial patterns \u2014that we call simplets\u2014 and generalize the task of frequent pattern mining from the realm of graphs to that of simplicial complexes. Our task is particularly challenging due to the enormous search space and the need for higher-order isomorphism. We show that finding the occurrences of simplets in a complex can be reduced to a bipartite graph isomorphism problem, in linear time and at most quadratic space. We then propose an anti-monotonic frequency measure that allows us to start the exploration from small simplets and stop expanding a simplet as soon as its frequency falls below the minimum frequency threshold. Equipped with these ideas and a clever data structure, we develop a memory-conscious algorithm that, by carefully exploiting the relationships among the simplices in the complex and among the simplets, achieves efficiency and scalability for our complex mining task. Our algorithm, FreSCo, comes in two flavors: it can compute the exact frequency of the simplets or, more quickly, it can determine whether a simplet is frequent, without having to compute the exact frequency. Experimental results prove the ability of FreSCo to mine frequent simplets in complexes of various size and dimension, and the significance of the simplets with respect to the traditional graph patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39046274",
                    "name": "Giulia Preti"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "33037ce7a4024973a0203e7ac718c14068b18241",
            "title": "GRAPHSHAP: Motif-based Explanations for Black-box Graph Classifiers",
            "abstract": ". Most methods for explaining black-box classi\ufb01ers (e.g., on tabular data, images, or time series) rely on measuring the impact that the removal/perturbation of features has on the model output. This forces the explanation language to match the classi\ufb01er features space. However, when dealing with graph data, in which the basic features correspond essentially to the adjacency information describing the graph structure (i.e., the edges), this matching between features space and explanation language might not be appropriate. In this regard, we argue that ( i ) a good explanation method for graph classi\ufb01cation should be fully agnostic with respect to the internal representation used by the black-box; and ( ii ) a good explanation language for graph classi\ufb01cation tasks should be represented by higher-order structures, such as motifs. The need to decouple the feature space (edges) from the explanation space (motifs) is thus a major challenge towards developing actionable explanations for graph classi\ufb01cation tasks. In this paper we introduce G RAPH S HAP , a Shapley-based approach able to provide motif-based explanations for black-box graph classi\ufb01ers, assuming no knowledge whatsoever about the model or its training data: the only requirement is that the black-box can be queried at will. For the sake of computational e\ufb03ciency we explore a progressive approximation strategy and show how a simple kernel can e\ufb03ciently approximate explanation scores, thus allowing G RAPH S HAP to scale on scenarios with a large explanation space (i.e., large number of motifs). We devise a synthetic dataset generator with arti\ufb01cially injected motifs in order to empirically compare di\ufb00erent masking approaches and to demonstrate that the proposed kernel is able to approximate the exact Shapley values with a computational complexity that is linear with respect to the number of explained features. Furthermore, we introduce additional auxiliary components such as a custom graph convolutional layer and algorithms for motif mining and ranking. Finally, we test G RAPH S HAP on a real-world brain-network dataset consisting of patients affected by Autism Spectrum Disorder and a control group. Our experiments highlight how the classi\ufb01cation provided by a black-box model can be e\ufb00ectively explained by few connectomics patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26582424",
                    "name": "A. Perotti"
                },
                {
                    "authorId": "46726748",
                    "name": "P. Bajardi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2735649",
                    "name": "A. Panisson"
                }
            ]
        },
        {
            "paperId": "6223114eb0a64680e43f7ed41503980cd5f922a4",
            "title": "Rewiring What-to-Watch-Next Recommendations to Reduce Radicalization Pathways",
            "abstract": "Recommender systems typically suggest to users content similar to what they consumed in the past. If a user happens to be exposed to strongly polarized content, she might subsequently receive recommendations which may steer her towards more and more radicalized content, eventually being trapped in what we call a \u201cradicalization pathway\u201d. In this paper, we study the problem of mitigating radicalization pathways using a graph-based approach. Specifically, we model the set of recommendations of a \u201cwhat-to-watch-next\u201d recommender as a d-regular directed graph where nodes correspond to content items, links to recommendations, and paths to possible user sessions. We measure the \u201csegregation\u201d score of a node representing radicalized content as the expected length of a random walk from that node to any node representing non-radicalized content. High segregation scores are associated to larger chances to get users trapped in radicalization pathways. Hence, we define the problem of reducing the prevalence of radicalization pathways by selecting a small number of edges to \u201crewire\u201d, so to minimize the maximum of segregation scores among all radicalized nodes, while maintaining the relevance of the recommendations. We prove that the problem of finding the optimal set of recommendations to rewire is NP-hard and NP-hard to approximate within any factor. Therefore, we turn our attention to heuristics, and propose an efficient yet effective greedy algorithm based on the absorbing random walk theory. Our experiments on real-world datasets in the context of video and news recommendations confirm the effectiveness of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120088794",
                    "name": "Francesco Fabbri"
                },
                {
                    "authorId": "2220456",
                    "name": "Yanhao Wang"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2065333595",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "49105984",
                    "name": "M. Mathioudakis"
                }
            ]
        },
        {
            "paperId": "691bf1d8f7ee062f370ce2ced217255ff08ff5c3",
            "title": "On the Relation Between Opinion Change and Information Consumption on Reddit",
            "abstract": "While much attention has been devoted to the causes of opinion change, little is known about its consequences. Our study moves a first step in this direction by looking at Reddit, and in particular to the subreddit r/ChangeMyView, a community dedicated to debating one\u2019s own opinions on a wide array of topics. We analyze changes in online information consumption behavior that arise after a self-reported opinion change, by looking at the participation to a set of sociopolitical communities. We find that people who self-report an opinion change are significantly more likely to change their future participation in a specific subset of those communities. Specifically, there is a significant association (Pearson r = 0.46) between using propaganda-like language in a community and the increase in chances of leaving it. Comparable results (Pearson r = 0.39) hold for the opposite direction, i.e., joining these same communities. In addition, the textual content of the post associated with opinion change is indicative of which communities will be joined or left: a predictive model based only on the text of this post can pinpoint these communities with an average precision@5 of 0.20. Our results establish a link between opinion change and information consumption, and highlight how online propagandistic communities act as a first gateway to internalize a shift in one\u2019s sociopolitical opinion.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171081238",
                    "name": "Flavio Petruzzellis"
                },
                {
                    "authorId": "47286020",
                    "name": "Corrado Monti"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "6eeaaa666e1ad26e43628a3bc92baf1631c6a9dc",
            "title": "Balancing Utility and Fairness in Submodular Maximization",
            "abstract": "Submodular function maximization is a fundamental combinatorial optimization problem with plenty of applications -- including data summarization, influence maximization, and recommendation. In many of these problems, the goal is to find a solution that maximizes the average utility over all users, for each of whom the utility is defined by a monotone submodular function. However, when the population of users is composed of several demographic groups, another critical problem is whether the utility is fairly distributed across different groups. Although the \\emph{utility} and \\emph{fairness} objectives are both desirable, they might contradict each other, and, to the best of our knowledge, little attention has been paid to optimizing them jointly. To fill this gap, we propose a new problem called \\emph{Bicriteria Submodular Maximization} (BSM) to balance utility and fairness. Specifically, it requires finding a fixed-size solution to maximize the utility function, subject to the value of the fairness function not being below a threshold. Since BSM is inapproximable within any constant factor, we focus on designing efficient instance-dependent approximation schemes. Our algorithmic proposal comprises two methods, with different approximation factors, obtained by converting a BSM instance into other submodular optimization problem instances. Using real-world and synthetic datasets, we showcase applications of our proposed methods in three submodular maximization problems: maximum coverage, influence maximization, and facility location.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220456",
                    "name": "Yanhao Wang"
                },
                {
                    "authorId": "1823512389",
                    "name": "Yuchen Li"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "94751361",
                    "name": "Ying Wang"
                }
            ]
        },
        {
            "paperId": "7706ef7e213cbb099dfeb1861dae82971de50780",
            "title": "Learning Multiscale Non-stationary Causal Structures",
            "abstract": "This paper addresses a gap in the current state of the art by providing a solution for modeling causal relationships that evolve over time and occur at different time scales. Specifically, we introduce the multiscale non-stationary directed acyclic graph (MN-DAG), a framework for modeling multivariate time series data. Our contribution is twofold. Firstly, we expose a probabilistic generative model by leveraging results from spectral and causality theories. Our model allows sampling an MN-DAG according to user-specified priors on the time-dependence and multiscale properties of the causal graph. Secondly, we devise a Bayesian method named Multiscale Non-stationary Causal Structure Learner (MN-CASTLE) that uses stochastic variational inference to estimate MN-DAGs. The method also exploits information from the local partial correlation between time series over different time resolutions. The data generated from an MN-DAG reproduces well-known features of time series in different domains, such as volatility clustering and serial correlation. Additionally, we show the superior performance of MN-CASTLE on synthetic data with different multiscale and non-stationary properties compared to baseline models. Finally, we apply MN-CASTLE to identify the drivers of the natural gas prices in the US market. Causal relationships have strengthened during the COVID-19 outbreak and the Russian invasion of Ukraine, a fact that baseline methods fail to capture. MN-CASTLE identifies the causal impact of critical economic drivers on natural gas prices, such as seasonal factors, economic uncertainty, oil prices, and gas storage deviations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2139739203",
                    "name": "Gabriele D'Acunto"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "46726748",
                    "name": "P. Bajardi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "7df0faabdee41aa5a5c314a0ca4bc851a92eac62",
            "title": "Social Norms on Reddit: A Demographic Analysis",
            "abstract": "Social norms, the shared informal rules of acceptable behavior, drive and reflect the evolution of societies. As an increasingly large part of social interactions happens online, social media data offers an unprecedented opportunity to assess the perception of social norm boundaries in-the-wild. In this regard, Reddit\u2019s r/AITA represents an invaluable source of codified social norms. This subreddit is an online forum where individuals describe how they acted in a specific situation in the past, and ask for the feedback of the community about whether their behavior was deviant or socially acceptable. Other users in the community share their views and express a judgment codified by a tag. This study focuses on assessing which factors are associated with judgements expressed by the community. Specifically, we investigate two main factors: the demographics of the author of the submission and the topic of the submission. Our analysis shows a clear gender imbalance in the judgements, with submissions by male authors receiving negative judgements with a 62% higher likelihood. Older authors (\u2265 28) also have a higher chance of receiving negative judgements (+21%). Regarding topics, submissions about romantic relationships and work tend to be judged more positively (+69% and +70%, respectively), thus hinting towards a role of the community as a support group, especially for female participants. We then focus on controversial submissions which garner heterogeneous judgements. We find that these submissions are clearly separable from those ones that are unanimously judged, and that male and older (\u2265 28) authors are more likely to describe controversial situations that split the community (+26% and +22%, respectively). Finally, we focus on the characteristics of the evaluators. We find that their judgements are associated with the other communities they belong to (signifying other interests and experiences), with an effect size comparable to the demographic group of the author. By combining all these variables\u2014demographics of the author and communities of the evaluator\u2014we are able to build a classifier that predicts a deviance judgement on a submission with AUC = 0.85.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2173930462",
                    "name": "Sara De Candia"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "47286020",
                    "name": "Corrado Monti"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "c1189af2293e320cbcee769d2fc2298a175c30c0",
            "title": "Cascade-based Echo Chamber Detection",
            "abstract": "Despite echo chambers in social media have been under considerable scrutiny, general models for their detection and analysis are missing. In this work, we aim to fill this gap by proposing a probabilistic generative model that explains social media footprints---i.e., social network structure and propagations of information---through a set of latent communities, characterized by a degree of echo-chamber behavior and by an opinion polarity. Specifically, echo chambers are modeled as communities that are permeable to pieces of information with similar ideological polarity, and impermeable to information of opposed leaning: this allows discriminating echo chambers from communities that lack a clear ideological alignment. To learn the model parameters we propose a scalable, stochastic adaptation of the Generalized Expectation Maximization algorithm, that optimizes the joint likelihood of observing social connections and information propagation. Experiments on synthetic data show that our algorithm is able to correctly reconstruct ground-truth latent communities with their degree of echo-chamber behavior and opinion polarity. Experiments on real-world data about polarized social and political debates, such as the Brexit referendum or the COVID-19 vaccine campaign, confirm the effectiveness of our proposal in detecting echo chambers. Finally, we show how our model can improve accuracy in auxiliary predictive tasks, such as stance detection and prediction of future propagations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2067355150",
                    "name": "Marco Minici"
                },
                {
                    "authorId": "1742226337",
                    "name": "Federico Cinus"
                },
                {
                    "authorId": "47286020",
                    "name": "Corrado Monti"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2062659381",
                    "name": "G. Manco"
                }
            ]
        },
        {
            "paperId": "0287dfba85a684ace5e3bce06e4b87b21ab04f9e",
            "title": "STruD: Truss Decomposition of Simplicial Complexes",
            "abstract": "A simplicial complex is a generalization of a graph: a collection of n-ary relationships (instead of binary as the edges of a graph), named simplices. In this paper, we develop a new tool to study the structure of simplicial complexes: we generalize the graph notion of truss decomposition to complexes, and show that this more powerful representation gives rise to different properties compared to the graph-based one. This power, however, comes with important computational challenges derived from the combinatorial explosion caused by the downward closure property of complexes. Drawing upon ideas from itemset mining and similarity search, we design a memory-aware algorithm, dubbed STruD, which is able to efficiently compute the truss decomposition of a simplicial complex. STruDadapts its behavior to the amount of available memory by storing intermediate data in a compact way. We then devise a variant that computes directly the n simplices of maximum trussness. By applying STruDto several datasets, we prove its scalability, and provide an analysis of their structure. Finally, we show that the truss decomposition can be seen as a filtration, and as such it can be used to study the persistent homology of a dataset, a method for computing topological features at different spatial resolutions, prominent in Topological Data Analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39046274",
                    "name": "Giulia Preti"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "1bc1e8b646dd5b33194243f4b541fcf89851834a",
            "title": "Comparing Equity and Effectiveness of Different Algorithms in an Application for the Room Rental Market",
            "abstract": "Machine Learning (ML) techniques have been increasingly adopted by the real estate market in the last few years. Applications include, among many others, predicting the market value of a property or an area, advanced systems for managing marketing and ads campaigns, and recommendation systems based on user preferences. While these techniques can provide important benefits to the business owners and the users of the platforms, algorithmic biases can result in inequalities and loss of opportunities for groups of people who are already disadvantaged in their access to housing. In this work, we present a comprehensive and independent algorithmic evaluation of a recommender system for the real estate market, designed specifically for finding shared apartments in metropolitan areas. We were granted full access to the internals of the platform, including details on algorithms and usage data during a period of 2 years. We analyze the performance of the various algorithms which are deployed for the recommender system and asses their effect across different population groups. Our analysis reveals that introducing a recommender system algorithm facilitates finding an appropriate tenant or a desirable room to rent, but at the same time, it strengthen performance inequalities between groups, further reducing opportunities of finding a rental for certain minorities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145872970",
                    "name": "David Solans"
                },
                {
                    "authorId": "120088794",
                    "name": "Francesco Fabbri"
                },
                {
                    "authorId": "1902736",
                    "name": "Caterina Calsamiglia"
                },
                {
                    "authorId": "2082412452",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "22818ba16b0cf32462f26b3d24087b92c9de5b26",
            "title": "Shortest Paths and Centrality in Uncertain Networks",
            "abstract": "\n Computing the shortest path between a pair of nodes is a fundamental graph primitive, which has critical applications in vehicle routing, finding functional pathways in biological networks, survivable network design, among many others. In this work, we study shortest-path queries over uncertain networks, i.e., graphs where every edge is associated with a probability of existence. We show that, for a given path, it is #\n P\n -hard to compute the probability of it being the shortest path, and we also derive other interesting properties highlighting the complexity of computing the Most Probable Shortest Paths (MPSPs). We thus devise sampling-based efficient algorithms, with end-to-end accuracy guarantees, to compute the MPSP. As a concrete application, we show how to compute a novel concept of betweenness centrality in an uncertain graph using MPSPs. Our thorough experimental results and rich real-world case studies on sensor networks and brain networks validate the effectiveness, efficiency, scalability, and usefulness of our solution.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150029298",
                    "name": "Arkaprava Saha"
                },
                {
                    "authorId": "2082359498",
                    "name": "Ruben Brokkelkamp"
                },
                {
                    "authorId": "2472355",
                    "name": "Yllka Velaj"
                },
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "2c6023d273c410807131d6ed40f11c07d80229f2",
            "title": "Multi-relation Graph Summarization",
            "abstract": "Graph summarization is beneficial in a wide range of applications, such as visualization, interactive and exploratory analysis, approximate query processing, reducing the on-disk storage footprint, and graph processing in modern hardware. However, the bulk of the literature on graph summarization surprisingly overlooks the possibility of having edges of different types. In this article, we study the novel problem of producing summaries of multi-relation networks, i.e., graphs where multiple edges of different types may exist between any pair of nodes. Multi-relation graphs are an expressive model of real-world activities, in which a relation can be a topic in social networks, an interaction type in genetic networks, or a snapshot in temporal graphs. The first approach that we consider for multi-relation graph summarization is a two-step method based on summarizing each relation in isolation, and then aggregating the resulting summaries in some clever way to produce a final unique summary. In doing this, as a side contribution, we provide the first polynomial-time approximation algorithm based on the k-Median clustering for the classic problem of lossless single-relation graph summarization. Then, we demonstrate the shortcomings of these two-step methods, and propose holistic approaches, both approximate and heuristic algorithms, to compute a summary directly for multi-relation graphs. In particular, we prove that the approximation bound of k-Median clustering for the single relation solution can be maintained in a multi-relation graph with proper aggregation operation over adjacency matrices corresponding to its multiple relations. Experimental results and case studies (on co-authorship networks and brain networks) validate the effectiveness and efficiency of the proposed algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8766005",
                    "name": "Xiangyu Ke"
                },
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "322b44b346f921158e3ec64199e32f61adc4ea14",
            "title": "The Effect of People Recommenders on Echo Chambers and Polarization",
            "abstract": "The effects of online social media on critical issues, such as polarization and misinformation, are under scrutiny due to the disruptive consequences that these phenomena can have on our societies. Among the algorithms routinely used by social media platforms, people-recommender systems are of special interest, as they directly contribute to the evolution of the social network structure, affecting the information and the opinions users are exposed to.\n\nIn this paper, we propose a novel framework to assess the effect of people recommenders on the evolution of opinions. Our proposal is based on Monte Carlo simulations combining link recommendation and opinion-dynamics models. In order to control initial conditions, we define a random network model to generate graphs with opinions, with tunable amounts of modularity and homophily. Finally, we join these elements into a methodology able to study the causal relationship between the recommender system and the echo chamber effect. Our method can also assess if such relationships are statistically significant. We also show how such a framework can be used to measure, by means of simulations, the impact of different intervention strategies.\n\nOur thorough experimentation shows that people recommenders can in fact lead to a significant increase in echo chambers. However, this happens only if there is considerable initial homophily in the network. Also, we find that if the network already contains echo chambers, the effect of the recommendation algorithm is negligible. Such findings are robust to two very different opinion dynamics models, a bounded confidence model and an epistemological model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "1742226337",
                    "name": "Federico Cinus"
                },
                {
                    "authorId": "2067355150",
                    "name": "Marco Minici"
                },
                {
                    "authorId": "47286020",
                    "name": "Corrado Monti"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "6c26f91480e7b281e4123c533ad6f123f9519e6c",
            "title": "Counterfactual Graphs for Explainable Classification of Brain Networks",
            "abstract": "Training graph classifiers able to distinguish between healthy brains and dysfunctional ones, can help identifying substructures associated to specific cognitive phenotypes. However, the mere predictive power of the graph classifier is of limited interest to the neuroscientists, which have plenty of tools for the diagnosis of specific mental disorders. What matters is the interpretation of the model, as it can provide novel insights and new hypotheses. In this paper we propose counterfactual graphs as a way to produce local post-hoc explanations of any black-box graph classifier. Given a graph and a black-box, a counterfactual is a graph which, while having high structural similarity with the original graph, is classified by the black-box in a different class. We propose and empirically compare several strategies for counterfactual graph search. Our experiments against a white-box classifier with known optimal counterfactual, show that our methods, although heuristic, can produce counterfactuals very close to the optimal one. Finally, we show how to use counterfactual graphs to build global explanations correctly capturing the behaviour of different black-box classifiers and providing interesting insights for the neuroscientists.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "89449460",
                    "name": "Carlo Abrate"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "71a57c324d6ac6d7e164e8f58fcac60bea37dbcf",
            "title": "Maxmin-Fair Ranking: Individual Fairness under Group-Fairness Constraints",
            "abstract": "We study a novel problem of fairness in ranking aimed at minimizing the amount of individual unfairness introduced when enforcing group-fairness constraints. Our proposal is rooted in the distributional maxmin fairness theory, which uses randomization to maximize the expected satisfaction of the worst-off individuals. We devise an exact polynomial-time algorithm to find maxmin-fair distributions of general search problems (including, but not limited to, ranking), and show that our algorithm can produce rankings which, while satisfying the given group-fairness constraints, ensure that the maximum possible value is to individuals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398620943",
                    "name": "David Garc\u00eda-Soriano"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "9774f4ab2bdcd3949aecfe3903f04ac2aa3d1f14",
            "title": "The evolving causal structure of equity risk factors",
            "abstract": "In recent years, multi-factor strategies have gained increasing popularity in the financial industry, as they allow investors to have a better understanding of the risk drivers underlying their portfolios. Moreover, such strategies promise to promote diversification and thus limit losses in times of financial turmoil. However, recent studies have reported a significant level of redundancy between these factors, which might enhance risk contagion among multi-factor portfolios during financial crises. Therefore, it is of fundamental importance to better understand the relationships among factors. Empowered by recent advances in causal structure learning methods, this paper presents a study of the causal structure of financial risk factors and its evolution over time. In particular, the data we analyze covers 11 risk factors concerning the US equity market, spanning a period of 29 years at daily frequency. Our results show a statistically significant sparsifying trend of the underlying causal structure. However, this trend breaks down during periods of financial stress, in which we can observe a densification of the causal network driven by a growth of the out-degree of the market factor node. Finally, we present a comparison with the analysis of factors cross-correlations, which further confirms the importance of causal analysis for gaining deeper insights in the dynamics of the factor system, particularly during economic downturns. Our findings are especially significant from a risk-management perspective. They link the evolution of the causal structure of equity risk factors with market volatility and a worsening macroeconomic environment, and show that, in times of financial crisis, exposure to different factors boils down to exposure to the market risk factor.",
            "fieldsOfStudy": [
                "Economics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2139739203",
                    "name": "Gabriele D'Acunto"
                },
                {
                    "authorId": "46726748",
                    "name": "P. Bajardi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                }
            ]
        },
        {
            "paperId": "a9fe71f6693306c5d4593ffa1e3c524f86b8cf18",
            "title": "Learning Ideological Embeddings from Information Cascades",
            "abstract": "Modeling information cascades in a social network through the lenses of the ideological leaning of its users can help understanding phenomena such as misinformation propagation and confirmation bias, and devising techniques for mitigating their toxic effects. In this paper we propose a stochastic model to learn the ideological leaning of each user in a multidimensional ideological space, by analyzing the way politically salient content propagates. In particular, our model assumes that information propagates from one user to another if both users are interested in the topic and ideologically aligned with each other. To infer the parameters of our model, we devise a gradient-based optimization procedure maximizing the likelihood of an observed set of information cascades. Our experiments on real-world political discussions on Twitter and Reddit confirm that our model is able to learn the political stance of the social media users in a multidimensional ideological space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47286020",
                    "name": "Corrado Monti"
                },
                {
                    "authorId": "1718485",
                    "name": "G. Manco"
                },
                {
                    "authorId": "3222408",
                    "name": "\u00c7igdem Aslay"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "ba35a826e355b4c8a431aee7b20770bf5b4e5e86",
            "title": "Efficient Probabilistic Truss Indexing on Uncertain Graphs",
            "abstract": "Networks in many real-world applications come with an inherent uncertainty in their structure, due to e.g., noisy measurements, inference and prediction models, or for privacy purposes. Modeling and analyzing uncertain graphs has attracted a great deal of attention. Among the various graph analytic tasks studied, the extraction of dense substructures, such as cores or trusses, has a central role. In this paper, we study the problem of (k, \u03b3)-truss indexing and querying over an uncertain graph . A (k, \u03b3)-truss is the largest subgraph of , such that the probability of each edge being contained in at least k \u2212 2 triangles is no less than \u03b3. Our first proposal, CPT-index, keeps all the (k, \u03b3)-trusses: retrieval for any given k and \u03b3 can be executed in an optimal linear time w.r.t. the graph size of the queried (k, \u03b3)-truss. We develop a bottom-up CPT-indexconstruction scheme and an improved algorithm for fast CPT-indexconstruction using top-down graph partitions. For trading off between (k, \u03b3)-truss offline indexing and online querying, we further develop an approximate indexing approach (\u03f5, \u0394r)-APXequipped with two parameters, \u03f5 and \u0394r, that govern tolerated errors. Extensive experiments using large-scale uncertain graphs with 261 million edges validate the efficiency of our proposed indexing and querying algorithms against state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108886003",
                    "name": "Zitang Sun"
                },
                {
                    "authorId": "39261035",
                    "name": "Xin Huang"
                },
                {
                    "authorId": "9097705",
                    "name": "Jianliang Xu"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "d8931cc2997f7f611eb60fd509cc7ba4b90a3d1d",
            "title": "Dense and well-connected subgraph detection in dual networks",
            "abstract": "Dense subgraph discovery is a fundamental problem in graph mining with a wide range of applications \\cite{gionis2015dense}. Despite a large number of applications ranging from computational neuroscience to social network analysis, that take as input a {\\em dual} graph, namely a pair of graphs on the same set of nodes, dense subgraph discovery methods focus on a single graph input with few notable exceptions \\cite{semertzidis2019finding,charikar2018finding,reinthal2016finding,jethava2015finding}. In this work, we focus the following problem: given a pair of graphs $G,H$ on the same set of nodes $V$, how do we find a subset of nodes $S \\subseteq V$ that induces a well-connected subgraph in $G$ and a dense subgraph in $H$? Our formulation generalizes previous research on dual graphs \\cite{Wu+15,WuZLFJZ16,Cui2018}, by enabling the {\\em control} of the connectivity constraint on $G$. We propose a novel mathematical formulation based on $k$-edge connectivity, and prove that it is solvable exactly in polynomial time. We compare our method to state-of-the-art competitors; we find empirically that ranging the connectivity constraint enables the practitioner to obtain insightful information that is otherwise inaccessible. Finally, we show that our proposed mining tool can be used to better understand how users interact on Twitter, and connectivity aspects of human brain networks with and without Autism Spectrum Disorder (ASD).",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2117179922",
                    "name": "Tianyi Chen"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1398620943",
                    "name": "David Garc\u00eda-Soriano"
                },
                {
                    "authorId": "2594516",
                    "name": "Atsushi Miyauchi"
                },
                {
                    "authorId": "2023899",
                    "name": "Charalampos E. Tsourakakis"
                }
            ]
        },
        {
            "paperId": "efbd1e852417a4f933f3b27048ce9e83a88883c8",
            "title": "WoMG: A Library for Word-of-Mouth Cascades Generation",
            "abstract": "Studying information propagation in social media is an important task with plenty of applications for business and science. Generating realistic synthetic information cascades can help the research community in developing new methods and applications, testing sociological hypotheses and different what-if scenarios by simply changing few parameters. We demonstrate womg, a synthetic data generator which combines topic modeling and a topic-aware propagation model to create realistic information-rich cascades, whose shape depends on many factors, including the topic of the item and its virality, the homophily of the social network, the interests of its users and their social influence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742226337",
                    "name": "Federico Cinus"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "47286020",
                    "name": "Corrado Monti"
                },
                {
                    "authorId": "2735649",
                    "name": "A. Panisson"
                }
            ]
        },
        {
            "paperId": "f53e1a275139760bfb273506200ffcf4587cf25d",
            "title": "Exposure Inequality in People Recommender Systems: The Long-Term Effects",
            "abstract": "People recommender systems may affect the exposure that users receive in social networking platforms, influencing attention dynamics and potentially strengthening pre-existing inequalities that disproportionately affect certain groups.\nIn this paper we introduce a model to simulate the feedback loop created by multiple rounds of interactions between users and a link recommender in a social network. This allows us to study the long-term consequences of those particular recommendation algorithms. Our model is equipped with several parameters to control: (i) the level of homophily in the network, (ii) the relative size of the groups, (iii) the choice among several state-of-the-art link recommenders, and (iv) the choice among three different stochastic user behavior models, that decide which recommendations are accepted or rejected.\nOur extensive experimentation with the proposed model shows that a minority group, if homophilic enough, can get a disproportionate advantage in exposure from all link recommenders. Instead, when it is heterophilic, it gets underexposed. Moreover, while the homophily level of the minority affects the speed of the growth of the disparate exposure, the relative size of the minority affects the magnitude of the effect. Finally, link recommenders strengthen exposure inequalities at the individual level, exacerbating the\u201crich-get-richer\u201d effect: this happens for both the minority and the majority class and independently of their level of homophily.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120088794",
                    "name": "Francesco Fabbri"
                },
                {
                    "authorId": "2145478330",
                    "name": "Maria Luisa Croci"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2065333595",
                    "name": "C. Castillo"
                }
            ]
        },
        {
            "paperId": "01b3b9fadc6502931ffd6e726320c7982d834f52",
            "title": "Query-Efficient Correlation Clustering",
            "abstract": "Correlation clustering is arguably the most natural formulation of clustering. Given n objects and a pairwise similarity measure, the goal is to cluster the objects so that, to the best possible extent, similar objects are put in the same cluster and dissimilar objects are put in different clusters. A main drawback of correlation clustering is that it requires as input the \u0398(n2) pairwise similarities. This is often infeasible to compute or even just to store. In this paper we study query-efficient algorithms for correlation clustering. Specifically, we devise a correlation clustering algorithm that, given a budget of Q queries, attains a solution whose expected number of disagreements is at most , where is the optimal cost for the instance. Its running time is O(Q), and can be easily made non-adaptive (meaning it can specify all its queries at the outset and make them in parallel) with the same guarantees. Up to constant factors, our algorithm yields a provably optimal trade-off between the number of queries Q and the worst-case error attained, even for adaptive algorithms. Finally, we perform an experimental study of our proposed method on both synthetic and real data, showing the scalability and the accuracy of our algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2081808012",
                    "name": "David Garc'ia-Soriano"
                },
                {
                    "authorId": "1712289",
                    "name": "Konstantin Kutzkov"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2023899",
                    "name": "Charalampos E. Tsourakakis"
                }
            ]
        },
        {
            "paperId": "2fb2cc02c54185fb6251198f8bec891e16d4cbde",
            "title": "Scalable Dynamic Graph Summarization",
            "abstract": "Large-scale dynamic interaction graphs can be challenging to process and store, due to their size and the continuous change of communication patterns between nodes. In this work, we address the problem of summarizing large-scale dynamic graphs, while maintaining the evolution of their structure and interactions. Our approach is based on grouping the nodes of the graph in supernodes according to their connectivity and communication patterns. The resulting summary graph preserves the information about the evolution of the graph within a time window. We propose two online algorithms for summarizing this type of graphs. Our baseline algorithm <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"bonchi-ieq1-2884471.gif\"/></alternatives></inline-formula>C based on clustering is fast but rather memory expensive. The second method we propose, named <inline-formula><tex-math notation=\"LaTeX\">$\\mu$</tex-math><alternatives><mml:math><mml:mi>\u03bc</mml:mi></mml:math><inline-graphic xlink:href=\"bonchi-ieq2-2884471.gif\"/></alternatives></inline-formula>C, reduces the memory requirements by introducing an intermediate step that keeps statistics of the clustering of the previous rounds. Our algorithms are distributed by design, and we implement them over the Apache Spark framework, so as to address the problem of scalability for large-scale graphs and massive streams. We apply our methods to several dynamic graphs, and show that we can efficiently use the summary graphs to answer temporal and probabilistic graph queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3266105",
                    "name": "Ioanna Tsalouchidou"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1389957009",
                    "name": "R. Baeza-Yates"
                }
            ]
        },
        {
            "paperId": "36a8498ec555324c6a2fcd8e3156c9bd75ec299b",
            "title": "Better Fewer but Better: Community Search with Outliers",
            "abstract": "Given a set of vertices in a network, that we believe are of interest for the application under analysis, community search is the problem of producing a subgraph potentially explaining the relationships existing among the vertices of interest. In practice this means that the solution should add some vertices to the query ones, so to create a connected subgraph that exhibits some \u201ccohesiveness\u201d property. This problem has received increasing attention in recent years: while several cohesiveness functions have been studied, the bulk of the literature looks for a solution subgraphs containing all the query vertices. However, in many exploratory analyses we might only have a reasonable belief about the vertices of interest: if only one of them is not really related to the others, forcing the solution to include all of them might hide the existence of much more cohesive and meaningful subgraphs, that we could have found by allowing the solution to detect and drop the outlier vertex. In this paper we study the problem of community search with outliers, where we are allowed to drop up to k query vertices, with k being an input parameter. We consider three of the most used measures of cohesiveness: the minimum degree, the diameter of the subgraph and the maximum distance with a query vertex. By optimizing one and using one of the others as a constraint we obtain three optimization problems: we study their hardness and we propose different exact and approximation algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "47819953",
                    "name": "Lorenzo Severini"
                },
                {
                    "authorId": "1816033",
                    "name": "Mauro Sozio"
                }
            ]
        },
        {
            "paperId": "3d70b9bf3d595201e2c2aec1afab5834668e90df",
            "title": "Generating Realistic Interest-Driven Information Cascades",
            "abstract": "We propose a model for the synthetic generation of information cascades in social media. In our model the information \u201cmemes\u201d propagating in the social network are characterized by a probability distribution in a topic space, accompanied by a textual description, i.e., a bag of keywords coherent with the topic distribution. Similarly, every user of the social media is described by a vector of interests defined over the same topic space. Information cascades are governed by the topic of the meme, its level of virality, the interests of each user, community pressure, and social influence.The main technical challenge we face towards our goal is the generation of realistic interest vectors, given a known network structure and a tunable level of homophily. We tackle this problem by means of a method based on non-negative matrix factorization, which is shown experimentally to outperform non-trivial baselines based on label propagation and random-walk-based graph embedding.As we showcase in our experiments, our model offers a small set of simple and easily interpretable \u201cknobs\u201d which allow to study, in vitro, how each set of assumptions affects the resulting propagations. Finally, we show how to generate synthetic cascades that have similar macro-statistics to the real-world cascades for a dataset containing both the network and the cascades.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742226337",
                    "name": "Federico Cinus"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "47286020",
                    "name": "Corrado Monti"
                },
                {
                    "authorId": "2735649",
                    "name": "A. Panisson"
                }
            ]
        },
        {
            "paperId": "3df04e94e9fe134c19af474e3c523660bc152a61",
            "title": "Cores matter? An analysis of graph decomposition effects on influence maximization problems",
            "abstract": "Estimating the spreading potential of nodes in a social network is an important problem which finds application in a variety of different contexts, ranging from viral marketing to spread of viruses and rumor blocking. Several studies have exploited both mesoscale structures and local centrality measures in order to estimate the spreading potential of nodes. To this end, one known result in the literature establishes a correlation between the spreading potential of a node and its coreness: i.e., in a core-decompostion of a network, nodes in higher cores have a stronger influence potential on the rest of the network. In this paper we show that the above result does not hold in general under common settings of propagation models with submodular activation function on directed networks, as those ones used in the influence maximization (IM) problem. Motivated by this finding, we extensively explore where the set of influential nodes extracted by state-of-the-art IM methods are located in a network w.r.t. different notions of graph decomposition. Our analysis on real-world networks provides evidence that, regardless of the particular IM method, the best spreaders are not always located within the inner-most subgraphs defined according to commonly used graph-decomposition methods. We identify the main reasons that explain this behavior, which can be ascribed to the inability of classic decomposition methods in incorporating higher-order degree of nodes. By contrast, we find that a distance-based generalization of the core-decomposition for directed networks can profitably be exploited to actually restrict the location of candidate solutions for IM to a single, well-defined portion of a network graph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47630927",
                    "name": "A. Calio"
                },
                {
                    "authorId": "1688359",
                    "name": "Andrea Tagarelli"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "528526cb82d2c2b2a4040a1a10168fc2ee1dcbfe",
            "title": "Explainable Classification of Brain Networks via Contrast Subgraphs",
            "abstract": "Mining human-brain networks to discover patterns that can be used to discriminate between healthy individuals and patients affected by some neurological disorder, is a fundamental task in neuro-science. Learning simple and interpretable models is as important as mere classification accuracy. In this paper we introduce a novel approach for classifying brain networks based on extracting contrast subgraphs, i.e., a set of vertices whose induced subgraphs are dense in one class of graphs and sparse in the other. We formally define the problem and present an algorithmic solution for extracting contrast subgraphs. We then apply our method to a brain-network dataset consisting of children affected by Autism Spectrum Disorder and children Typically Developed. Our analysis confirms the interestingness of the discovered patterns, which match background knowledge in the neuro-science literature. Further analysis on other classification tasks confirm the simplicity, soundness, and high explainability of our proposal, which also exhibits superior classification accuracy, to more complex state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "52523225",
                    "name": "Tommaso Lanciano"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                }
            ]
        },
        {
            "paperId": "79164aec34709012be2b7f02f49a29ebc0e363e3",
            "title": "Give more data, awareness and control to individual citizens, and they will help COVID-19 containment",
            "abstract": "The rapid dynamics of COVID-19 calls for quick and effective tracking of virus transmission chains and early detection of outbreaks, especially in the phase 2 of the pandemic, when lockdown and other restriction measures are progressively withdrawn, in order to avoid or minimize contagion resurgence. For this purpose, contact-tracing apps are being proposed for large scale adoption by many countries. A centralized approach, where data sensed by the app are all sent to a nation-wide server, raises concerns about citizens' privacy and needlessly strong digital surveillance, thus alerting us to the need to minimize personal data collection and avoiding location tracking. We advocate the conceptual advantage of a decentralized approach, where both contact and location data are collected exclusively in individual citizens' \"personal data stores\", to be shared separately and selectively, voluntarily, only when the citizen has tested positive for COVID-19, and with a privacy preserving level of granularity. This approach better protects the personal sphere of citizens and affords multiple benefits: it allows for detailed information gathering for infected people in a privacy-preserving fashion;and, in turn this enables both contact tracing, and, the early detection of outbreak hotspots on more finely-granulated geographic scale. Our recommendation is two-fold. First to extend existing decentralized architectures with a light touch, in order to manage the collection of location data locally on the device, and allow the user to share spatio-temporal aggregates - if and when they want, for specific aims - with health authorities, for instance. Second, we favour a longer-term pursuit of realizing a Personal Data Store vision, giving users the opportunity to contribute to collective good in the measure they want, enhancing self-awareness, and cultivating collective efforts for rebuilding society.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1717192",
                    "name": "M. Nanni"
                },
                {
                    "authorId": "50663909",
                    "name": "G. Andrienko"
                },
                {
                    "authorId": "2172195129",
                    "name": "Albert-L'aszl'o Barab'asi"
                },
                {
                    "authorId": "144386157",
                    "name": "C. Boldrini"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "144300761",
                    "name": "C. Cattuto"
                },
                {
                    "authorId": "3098280",
                    "name": "Francesca Chiaromonte"
                },
                {
                    "authorId": "2235275955",
                    "name": "Giovanni Comand'e"
                },
                {
                    "authorId": "2060071082",
                    "name": "M. Conti"
                },
                {
                    "authorId": "40638665",
                    "name": "Marc-Alexandre C\u00f4t\u00e9"
                },
                {
                    "authorId": "79774420",
                    "name": "F. Dignum"
                },
                {
                    "authorId": "1716665",
                    "name": "Virginia Dignum"
                },
                {
                    "authorId": "1393591007",
                    "name": "J. Domingo-Ferrer"
                },
                {
                    "authorId": "1681278",
                    "name": "P. Ferragina"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1704327",
                    "name": "Riccardo Guidotti"
                },
                {
                    "authorId": "1768825",
                    "name": "D. Helbing"
                },
                {
                    "authorId": "145670814",
                    "name": "K. Kaski"
                },
                {
                    "authorId": "2207387183",
                    "name": "J\u00e1nos Kert\u00e9sz"
                },
                {
                    "authorId": "31413949",
                    "name": "S. Lehmann"
                },
                {
                    "authorId": "1776476",
                    "name": "B. Lepri"
                },
                {
                    "authorId": "2199139669",
                    "name": "P. Lukowicz"
                },
                {
                    "authorId": "1749003",
                    "name": "S. Matwin"
                },
                {
                    "authorId": "2229020742",
                    "name": "D. Jim'enez"
                },
                {
                    "authorId": "2296962",
                    "name": "A. Monreale"
                },
                {
                    "authorId": "1752599",
                    "name": "K. Morik"
                },
                {
                    "authorId": "145709776",
                    "name": "Nuria Oliver"
                },
                {
                    "authorId": "2174176466",
                    "name": "A. Passarella"
                },
                {
                    "authorId": "1702610",
                    "name": "Andrea Passerini"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "153439805",
                    "name": "A. Pentland"
                },
                {
                    "authorId": "1760810",
                    "name": "F. Pianesi"
                },
                {
                    "authorId": "33769943",
                    "name": "Francesca Pratesi"
                },
                {
                    "authorId": "2120595",
                    "name": "S. Rinzivillo"
                },
                {
                    "authorId": "2325428",
                    "name": "S. Ruggieri"
                },
                {
                    "authorId": "2227342424",
                    "name": "A. Siebes"
                },
                {
                    "authorId": "3005971",
                    "name": "R. Trasarti"
                },
                {
                    "authorId": "7485612",
                    "name": "J. Hoven"
                },
                {
                    "authorId": "80273596",
                    "name": "A. Vespignani"
                }
            ]
        },
        {
            "paperId": "a249e3e4761135841d14ba663599205f55c46bb0",
            "title": "Learning Opinion Dynamics From Social Traces",
            "abstract": "Opinion dynamics the research field dealing with how people's opinions form and evolve in a social context? traditionally uses agent-based models to validate the implications of sociological theories. These models encode the causal mechanism that drives the opinion formation process, and have the advantage of being easy to interpret. However, as they do not exploit the availability of data, their predictive power is limited. Moreover, parameter calibration and model selection are manual and difficult tasks. In this work we propose an inference mechanism for fitting a generative, agent-like model of opinion dynamics to real-world social traces. Given a set of observables (e.g., actions and interactions between agents), our model can recover the most-likely latent opinion trajectories that are compatible with the assumptions about the process dynamics. This type of model retains the benefits of agent-based ones (i.e., causal interpretation), while adding the ability to perform model selection and hypothesis testing on real data. We showcase our proposal by translating a classical agent-based model of opinion dynamics into its generative counterpart. We then design an inference algorithm based on online expectation maximization to learn the latent parameters of the model. Such algorithm can recover the latent opinion trajectories from traces generated by the classical agent-based model. In addition, it can identify the most likely set of macro parameters used to generate a data trace, thus allowing testing of sociological hypotheses. Finally, we apply our model to real-world data from Reddit to explore the long-standing question about the impact of the backfire effect. Our results suggest a low prominence of the effect in Reddit's political conversation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47286020",
                    "name": "Corrado Monti"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "ad0491d52998aa42cf514ef8acbed1668371ede8",
            "title": "X2R2: a Tool for Explainable and Explorative Reidentification Risk Analysis",
            "abstract": "Reidentification-risk analysis and anonymity have received a great deal of attention in the last two decades. While the research community has been developing several privacy notions and the algorithms to achieve them, these tools have faced difficulties in being transferred to the wider audience of practitioners, for they require a considerable amount of data privacy technical knowledge.We demonstrate X2R2 (Explainable Explorative Reidentification Risk), a data anonymization tool for the laymen. X2R2 guides the user through a transparent explorative process, during which the existing reidentification risks are explained and quantified, possible data transformation options are recommended, and the consequences of these operations, in terms of privacy risk and data utility, are clearly shown.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26595666",
                    "name": "T. Hagedoorn"
                },
                {
                    "authorId": "2124972983",
                    "name": "Rohit Kumar"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "ba538bdde6dab7a589c6c05d8d982f633572fe8e",
            "title": "The Effect of Homophily on Disparate Visibility of Minorities in People Recommender Systems",
            "abstract": "Evaluating (and mitigating) the potential negative effects of algorithms has become a central issue in computer science. While research on algorithmic bias in ranking systems has dealt with disparate exposure of products or individuals, less attention has been devoted to the analysis of the disparate exposure of subgroups of online users.In this paper, we investigate the visibility of minorities in people recommender systems in social networks. Specifically, we consider a bi-populated social network, i.e., a graph where the nodes belong to two different groups (majority and minority) and, by applying state-of-the-art people recommenders, we analyze how disparate visibility can be amplified or mitigated by different levels of homophily within each subgroup.We start our analysis on real-world social graphs, where the two subgroups are defined by sensitive demographic attributes such as gender or age. Our findings suggest that the way and the extent to which people recommenders can produce disparate visibility on the two subgroups, might depend in large part on the level of homophily within the subgroups. % To verify these findings, we move our analysis to synthetic datasets, where we can control characteristics of the input social graph, such as the size of the minority and the level of homophily. Our results show that homophily plays a key role in promoting or reducing visibility for different subgroups under various combinations of dataset characteristics and recommendation algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120088794",
                    "name": "Francesco Fabbri"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1824224",
                    "name": "Ludovico Boratto"
                },
                {
                    "authorId": "2065333595",
                    "name": "C. Castillo"
                }
            ]
        },
        {
            "paperId": "ed8ea9380d8b10ccd72fe428e61ec34c0d5b3aa4",
            "title": "Roots of Trumpism: Homophily and Social Feedback in Donald Trump Support on Reddit",
            "abstract": "We study the emergence of support for Donald Trump in Reddit\u2019s political discussion. With almost 800k subscribers, \u201cr/The_Donald\u201d is one of the largest communities on Reddit, and one of the main hubs for Trump supporters. It was created in 2015, shortly after Donald Trump began his presidential campaign. By using only data from 2012, we predict the likelihood of being a supporter of Donald Trump in 2016, the year of the last US presidential elections. To characterize the behavior of Trump supporters, we draw from three different sociological hypotheses: homophily, social influence, and social feedback. We operationalize each hypothesis as a set of features for each user, and train classifiers to predict their participation in r/The_Donald. We find that homophily-based and social feedback-based features are the most predictive signals. Conversely, we do not observe a strong impact of social influence mechanisms. We also perform an introspection of the best-performing model to build a \u201cpersona\u201d of the typical supporter of Donald Trump on Reddit. We find evidence that the most prominent traits include a predominance of masculine interests, a conservative and libertarian political leaning, and links with politically incorrect and conspiratorial content.",
            "fieldsOfStudy": [
                "Sociology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1672633023",
                    "name": "Joan Massachs"
                },
                {
                    "authorId": "47286020",
                    "name": "Corrado Monti"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "f500bb235d05ed56318784a00f4a1c00f9e86ec3",
            "title": "Adaptive Community Search in Dynamic Networks",
            "abstract": "Community search is a well-studied problem which, given a static graph and a query set of vertices, requires to find a cohesive (or dense) subgraph containing the query vertices. In this paper we study the problem of community search in temporal dynamic networks. We adapt to the temporal setting the notion of network inefficiency which is based on the pairwise shortest-path distance among all the vertices in a solution. For this purpose we define the notion of shortest-fastest-path distance: a linear combination of the temporal and spatial dimensions governed by a user-defined parameter. We thus define the MINIMUM TEMPORAL-INEFFICIENCY SUBGRAPH problem and show that it is NP-hard. We develop an algorithm which exploits a careful transformation of the temporal network to a static directed and weighted graph, and some recent approximation algorithm for finding the minimum Directed Steiner Tree. We finally generalize our framework to the streaming setting in which new snapshots of the temporal graph keep arriving continuously and our goal is to produce a community search solution for the temporal graph corresponding to a sliding time window.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3266105",
                    "name": "Ioanna Tsalouchidou"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1389957009",
                    "name": "R. Baeza-Yates"
                }
            ]
        },
        {
            "paperId": "021b3224179788869b0f2bcebf94b0850f6ad65e",
            "title": "The Responsibility Challenge for Data",
            "abstract": "As data science and artificial intelligence become ubiquitous, they have an increasing impact on society. While many of these impacts are beneficial, others may not be. So understanding and managing these impacts is required of every responsible data scientist. Nevertheless, most human decision-makers use algorithms for efficiency purposes and not to make a better (i.e., fairer) decisions. Even the task of risk assessment in the criminal justice system enables efficiency instead of (and often at the expense of) fairness. So we need to frame the problem with fairness, and other societal impacts, as primary objectives. In this context, most attention has been paid to the machine learning of a model for a task, such as recognition, prediction, or classification. However, issues arise in all parts of the data eco-system, from data acquisition to data presentation. For example, the majority of the population is not white and male, yet this demographic is over-represented in the training data. It is challenging for a data scientist to satisfactorily discharge this broad responsibility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1397398770",
                    "name": "Tina Eliassi-Rad"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "18d5b2f02079acf191977de09dfbe6f7bc75f474",
            "title": "Span-core Decomposition for Temporal Networks",
            "abstract": "When analyzing temporal networks, a fundamental task is the identification of dense structures (i.e., groups of vertices that exhibit a large number of links), together with their temporal span (i.e., the period of time for which the high density holds). In this article, we tackle this task by introducing a notion of temporal core decomposition where each core is associated with two quantities, its coreness, which quantifies how densely it is connected, and its span, which is a temporal interval: we call such cores span-cores. For a temporal network defined on a discrete temporal domain T, the total number of time intervals included in T is quadratic in |T|, so that the total number of span-cores is potentially quadratic in |T| as well. Our first main contribution is an algorithm that, by exploiting containment properties among span-cores, computes all the span-cores efficiently. Then, we focus on the problem of finding only the maximal span-cores, i.e., span-cores that are not dominated by any other span-core by both their coreness property and their span. We devise a very efficient algorithm that exploits theoretical findings on the maximality condition to directly extract the maximal ones without computing all span-cores. Finally, as a third contribution, we introduce the problem of temporal community search, where a set of query vertices is given as input, and the goal is to find a set of densely-connected subgraphs containing the query vertices and covering the whole underlying temporal domain T. We derive a connection between this problem and the problem of finding (maximal) span-cores. Based on this connection, we show how temporal community search can be solved in polynomial-time via dynamic programming, and how the maximal span-cores can be profitably exploited to significantly speed-up the basic algorithm. We provide an extensive experimentation on several real-world temporal networks of widely different origins and characteristics. Our results confirm the efficiency and scalability of the proposed methods. Moreover, we showcase the practical relevance of our techniques in a number of applications on temporal networks, describing face-to-face contacts between individuals in schools. Our experiments highlight the relevance of the notion of (maximal) span-core in analyzing social dynamics, detecting/correcting anomalies in the data, and graph-embedding-based network classification.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "32524198",
                    "name": "Edoardo Galimberti"
                },
                {
                    "authorId": "1389551664",
                    "name": "Martino Ciaperoni"
                },
                {
                    "authorId": "8977167",
                    "name": "A. Barrat"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "144300761",
                    "name": "C. Cattuto"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                }
            ]
        },
        {
            "paperId": "2619065716623dc241565b2ef63cfbd47cdecc52",
            "title": "The importance of unexpectedness: Discovering buzzing stories in anomalous temporal graphs",
            "abstract": ". The real-time nature and massive volume of social-media data has converted news portals and micro-blogging platforms into social sensors, causing a \ufb02ourishing of research on story or event detection in online user-generated content and social-media text streams. Existing approaches to story identi\ufb01cation broadly fall into two categories. Approaches in the \ufb01rst category extract stories as cohesive substructures in a graph representing the strength of association between terms. The latter category includes approaches that analyze the temporal evolution of individual terms and identify stories by grouping terms with similar anomalous temporal behavior. Both categories have their own limitations. Approaches in the \ufb01rst category are unable to distinguish ever-popular concepts from stories that buzz in a time interval of interest, i.e., attract an amount of attention that deviates signi\ufb01cantly from the typical level observed. The second category ignores term co-associations and the wealth of information captured by them. In this work we advance the literature on story identi\ufb01cation by pro\ufb01tably combining the peculiarities of the two main state-of-the-art approaches. We propose a novel method that characterizes abnormal association between terms in a certain time window and leverages the graph structure induced by such anomalous associations so as to identify stories as subsets of terms that are cohesively associated in this graph. Experiments performed on two datasets extracted from a real-world web-search query log and a news corpus, respectively, attest the superiority of the proposed method over the two main existing story-identi\ufb01cation approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "3033855",
                    "name": "Ilaria Bordino"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1765155",
                    "name": "G. Stilo"
                }
            ]
        },
        {
            "paperId": "95f12a112ee7a3bb252f003c1ceab0b45c654b94",
            "title": "Distance-generalized Core Decomposition",
            "abstract": "The k-core of a graph is defined as the maximal subgraph in which every vertex is connected to at least k other vertices within that subgraph. In this work we introduce a distance-based generalization of the notion of k-core, which we refer to as the $(k,h)$-core, i.e., the maximal subgraph in which every vertex has at least k other vertices at distance $\u0142eq h$ within that subgraph. We study the properties of the $(k,h)$-core showing that it preserves many of the nice features of the classic core decomposition (e.g., its connection with the notion of distance-generalized chromatic number ) and it preserves its usefulness to speed-up or approximate distance-generalized notions of dense structures, such as h-club. Computing the distance-generalized core decomposition over large networks is intrinsically complex. However, by exploiting clever upper and lower bounds we can partition the computation in a set of totally independent subcomputations, opening the door to top-down exploration and to multithreading, and thus achieving an efficient algorithm.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "47819953",
                    "name": "Lorenzo Severini"
                }
            ]
        },
        {
            "paperId": "f77053919dd1225245cfb7daf1f10ea28e58dc2e",
            "title": "Uncertain Graph Sparsification (Extended Abstract)",
            "abstract": "Uncertain graphs are prevalent in several applications including communications systems, biological databases and social networks. The ever increasing size of the underlying data renders both graph storage and query processing extremely expensive. Sparsification has often been used to reduce the size of deterministic graphs by maintaining only the important edges. However, adaptation of deterministic sparsification methods fails in the uncertain setting. To overcome this problem, we introduce the first sparsification techniques aimed explicitly at uncertain graphs. The proposed methods reduce the number of edges and redistribute their probabilities in order to decrease the graph size, while preserving its underlying structure. The resulting graph can be used to efficiently and accurately approximate any query and mining tasks on the original graph, including clustering coefficient, page rank, reliability and shortest path distance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047271",
                    "name": "Panos Parchas"
                },
                {
                    "authorId": "2559413",
                    "name": "Nikolaos Papailiou"
                },
                {
                    "authorId": "1746338",
                    "name": "Dimitris Papadias"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "19300aff1439f395c86d2ea5483ac95bd443c401",
            "title": "Summarizing Graphs at Multiple Scales: New Trends",
            "abstract": "Recent advances in computing resources have made it possible to collect enormous amounts of interconnected data, such as social media interactions, web activity, knowledge bases, product and service purchases, autonomous vehicle routing, smart home sensor data, and more. The massive scale and complexity of this data, however, not only vastly surpasses human processing power, but also goes beyond limitations with regard to computation and storage. That is, there is an urgent need for methods and tools that summarize large interconnected data to enable faster computations, storage reduction, interactive large-scale visualization and understanding, and pattern discovery. Network summarization-which aims to find a small representation of an original, larger graph-features a variety of methods with different goals and for different input data representations (e.g., attributed graphs, time-evolving or streaming graphs, heterogeneous graphs). The objective of this tutorial is to give a systematic overview of methods for summarizing and explaining graphs at different scales: the node-group level, the network level, and the multi-network level. We emphasize the current challenges, present real-world applications, and highlight the open research problems in this vibrant research area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "3183025",
                    "name": "Jilles Vreeken"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "4e7d50d4a9eca1fcaedf72c62dbec6716253ebd4",
            "title": "Core Decomposition in Multilayer Networks",
            "abstract": "Multilayer networks are a powerful paradigm to model complex systems, where multiple relations occur between the same entities. Despite the keen interest in a variety of tasks, algorithms, and analyses in this type of network, the problem of extracting dense subgraphs has remained largely unexplored so far. As a first step in this direction, in this work, we study the problem of core decomposition of a multilayer network. Unlike the single-layer counterpart in which cores are all nested into one another and can be computed in linear time, the multilayer context is much more challenging as no total order exists among multilayer cores; rather, they form a lattice whose size is exponential in the number of layers. In this setting, we devise three algorithms, which differ in the way they visit the core lattice and in their pruning techniques. We assess time and space efficiency of the three algorithms on a large variety of real-world multilayer networks. We then move a step forward and study the problem of extracting the inner-most (also known as maximal) cores, i.e., the cores that are not dominated by any other core in terms of their core index in all the layers. inner-most cores are typically orders of magnitude less than all the cores. Motivated by this, we devise an algorithm that effectively exploits the maximality property and extracts inner-most cores directly, without first computing a complete decomposition. This allows for a consistent speed up over a na\u00efve method that simply filters out non-inner-most ones from all the cores. Finally, we showcase the multilayer core-decomposition tool in a variety of scenarios and problems. We start by considering the problem of densest-subgraph extraction in multilayer networks. We introduce a definition of multilayer densest subgraph that tradesoff between high density and number of layers in which the high density holds, and exploit multilayer core decomposition to approximate this problem with quality guarantees. As further applications, we show how to utilize multilayer core decomposition to speed-up the extraction of frequent cross-graph quasi-cliques and to generalize the community-search problem to the multilayer setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32524198",
                    "name": "Edoardo Galimberti"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "52523225",
                    "name": "Tommaso Lanciano"
                }
            ]
        },
        {
            "paperId": "777dab18c6bb0fe65fcbc5aec569d5a68a79f738",
            "title": "Fair-by-design algorithms: matching problems and beyond",
            "abstract": "In discrete search and optimization problems where the elements that may or not be included in a solution correspond to humans, individual fairness needs to be expressed in terms of each individual's satisfaction probability (the probability of being included in the solution). In this paper we introduce the maxmin fairness framework which provides, on any given input instance, the strongest guarantee possible for all individuals, in terms of satisfaction probability. \nA probability distribution over valid solutions is maxmin-fair if it is not possible to improve the satisfaction probability of any individual without decreasing it for some other individual which is no better off. We provide an efficient exact algorithm for maxmin-fair bipartite matching combining flow-based methods with algorithms for edge-coloring regular bipartite graphs. As shown in our experimental evaluation, our algorithm scales to graphs with millions of vertices and hundreds of millions of edges, taking only a few minutes on a simple architecture. \nWe generalize our method to the case where the structure of valid solutions forms a matroid, in which case the price of fairness is zero, and show that then maxmin-fair distributions mini- mize social inequality among Pareto-efficient distributions. More generally, we prove that a maxmin-fair distribution of solutions to any combinatorial search problem may be found efficiently under the sole assumption that, given an arbitrary assignment of non-negative weights to individuals, a maximum-weight solution may be found in polynomial time. This class of problems extends beyond matchings and matroids, and includes the vast majority of search problems for which exact algorithms are known.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398620943",
                    "name": "David Garc\u00eda-Soriano"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "7898e22277880670f1c5b26b8060b45a587d6f31",
            "title": "CIKM 2018 Co-Located Workshops Summary",
            "abstract": "This paper provides an overview of the workshops co-located with the 27th ACM International Conference on Information and Knowledge Management (CIKM 2018), held during October 22-26, 2018 in Turin, Italy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145046124",
                    "name": "A. Cuzzocrea"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                }
            ]
        },
        {
            "paperId": "cfe2aba6c56b64c47d39647d5c92df297335a283",
            "title": "CIKM 2018 Workshops: Preface",
            "abstract": "This paper briefly introduces the workshops that have been co-located with the 27th ACM International Conference on Information and Knowledge Management (CIKM 2018), held during October 22-26, 2018 in Turin, Italy. CIKM 2018 selected 9 workshops, which have been focused on several topics, all concerned with information and knowledge management in the big data era. The CIKM 2018 workshop are listed in the following:",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145046124",
                    "name": "A. Cuzzocrea"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                }
            ]
        },
        {
            "paperId": "d39bf6c9e94c5258becabe430d7d83335f462587",
            "title": "Invited papers from the ACM conference on hypertext and social media",
            "abstract": "This Special Issue presents three invited papers, selected from among the best contributions that were presented at the 2017 ACM International Conference on Hypertext and Social Media (HT 2017) held in Prague, Czech Republic on 4\u20137th July 2017. Since 1987, HT has successfully brought together leading researchers and developers from the Hypertext community. It is concerned with all aspects of modern hypertext research, including social media, adaptation, personalisation, recommendations, user modelling, linked data and semantic web, dynamic and computed hypertext, and its application in digital humanities, as well as with interplay between those aspects such as linking stories with data or linking people with resources. The call for papers of HT 2017 was organised into four technical tracks: Social Networks and Digital Humanities (Linking people), Semantic Web and Linked Data (Linking data), Adaptive Hypertext and Recommendations (Linking resources), News and Storytelling (Linking stories). The Program Committee of HT 2017 accepted 19 papers (acceptance rate 27%) for regular presentation, and an additional 12 short-presentation papers. In addition, the conference featured four demonstrations and two keynotes: Kristina Lerman and Peter Mika. The three papers selected for this Special Issue cover a diverse set of topics, well representing the spectrum of topics that were discussed at HT 2017. The first paper, entitled \u201cImplicit Negative Link Detection on Online Political Networks via Matrix Tri-Factorizations\u201d (Ozer, Yildirim and Davulcu), deals with the prediction of negative connections between users of online political networks. Currently, the majority of social media sites do not support explicit negative links between participating users. However, the very nature of the political discourse often involves users in discussing controversial political issues, which results in a series of agreements and disagreements. The authors present a technically sound approach to extracting negative links from a variety of online political platforms by using a matrix factorisation approach. Matrix factorisation is extended in multiple ways to reflect the information that can be found in the sentiment of the written comments as well as the social balance theory known from the social sciences. The paper concludes with a range of experiments on the real datasets using the Twitter accounts of the politicians of the major UK political parties. The experiments show an improved accuracy of the community detection methods applied on the networks with the extracted negative interaction links as compared to the application of these methods on the networks having only positive links. The second paper, entitled \u201cHybrid Recommendations by Content-Aligned Bayesian Personalized Ranking\u201d (Peska) focuses on recommender systems that seek to predict the \"rating\" or \"preference\" a user would give to an item and hence enabling to display items in order the user might find interesting. A special problem is cold-start recommendation, i.e. for a new user or of a new item. The author proposes a hybrid recommendation technique \u201cContentAligned Bayesian Personalized Ranking\u201d (CABPR) with several variants. This is based on an existing Bayesian Personalized Ranking matrix factorization (BPR) by Rendle et al. CABPR",
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1805892",
                    "name": "Peter Dolog"
                },
                {
                    "authorId": "1747800",
                    "name": "D. Helic"
                },
                {
                    "authorId": "1712852",
                    "name": "P. Vojt\u00e1s"
                }
            ]
        },
        {
            "paperId": "e7dd232958d73e414cea53f571a39ac101994b0a",
            "title": "Probabilistic Causal Analysis of Social Influence",
            "abstract": "Mastering the dynamics of social influence requires separating, in a database of information propagation traces, the genuine causal processes from temporal correlation, i.e., homophily and other spurious causes. However, most studies to characterize social influence, and, in general, most data-science analyses focus on correlations, statistical independence, or conditional independence. Only recently, there has been a resurgence of interest in \"causal data science,'' e.g., grounded on causality theories. In this paper we adopt a principled causal approach to the analysis of social influence from information-propagation data, rooted in the theory of probabilistic causation. Our approach consists of two phases. In the first one, in order to avoid the pitfalls of misinterpreting causation when the data spans a mixture of several subtypes (\"Simpson's paradox''), we partition the set of propagation traces into groups, in such a way that each group is as less contradictory as possible in terms of the hierarchical structure of information propagation. To achieve this goal, we borrow the notion of \"agony'' and define the Agony-bounded Partitioning problem, which we prove being hard, and for which we develop two efficient algorithms with approximation guarantees. In the second phase, for each group from the first phase, we apply a constrained MLE approach to ultimately learn a minimal causal topology. Experiments on synthetic data show that our method is able to retrieve the genuine causal arcs w.r.t. a ground-truth generative model. Experiments on real data show that, by focusing only on the extracted causal structures instead of the whole social graph, the effectiveness of predicting influence spread is significantly improved.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1689299",
                    "name": "B. Mishra"
                },
                {
                    "authorId": "3279191",
                    "name": "Daniele Ramazzotti"
                }
            ]
        },
        {
            "paperId": "fe69028f2c7b32d745082a8dd5005cf3611f72dc",
            "title": "Mining (maximal) Span-cores from Temporal Networks",
            "abstract": "When analyzing temporal networks, a fundamental task is the identification of dense structures (i.e., groups of vertices that exhibit a large number of links), together with their temporal span (i.e., the period of time for which the high density holds). We tackle this task by introducing a notion of temporal core decomposition where each core is associated with its span: we call such cores span-cores. As the total number of time intervals is quadratic in the size of the temporal domain T under analysis, the total number of span-cores is quadratic in $|T|$ as well. Our first contribution is an algorithm that, by exploiting containment properties among span-cores, computes all the span-cores efficiently. Then, we focus on the problem of finding only the maximal span-cores, i.e., span-cores that are not dominated by any other span-core by both the coreness property and the span. We devise a very efficient algorithm that exploits theoretical findings on the maximality condition to directly compute the maximal ones without computing all span-cores. Experimentation on several real-world temporal networks confirms the efficiency and scalability of our methods. Applications on temporal networks, gathered by a proximity-sensing infrastructure recording face-to-face interactions in schools, highlight the relevance of the notion of (maximal) span-core in analyzing social dynamics and detecting/correcting anomalies in the data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32524198",
                    "name": "Edoardo Galimberti"
                },
                {
                    "authorId": "8977167",
                    "name": "A. Barrat"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "144300761",
                    "name": "C. Cattuto"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                }
            ]
        },
        {
            "paperId": "10415225672f1b79669cfb445bb3aa8220f8de1d",
            "title": "Proceedings of the 28th ACM Conference on Hypertext and Social Media",
            "abstract": "It is our great pleasure to welcome you to the 2017 ACM International Conference on Hypertext and Social Media (HT 2017) in Prague, Czech Republic on 4th -- 7th July. \n \nHT is a top-tier ACM conference in the areas of Hypertext and Social Media. Since 1987, it has successfully brought together leading researchers and developers from the community. It is concerned with all aspects of modern hypertext research, including social media, adaptation, personalization, recommendations, user modeling, linked data and semantic web, dynamic and computed hypertext, and its application in digital humanities, as well as with interplay between those aspects such as linking stories with data or linking people with resources. \n \nHT 2017 continues to create an outstanding technical program consisting of research and demo paper presentations. This year we organized the call for papers in four technical tracks: Social Networks and Digital Humanities (Linking people), Semantic Web and Linked Data (Linking data), Adaptive Hypertext and Recommendations (Linking resources), News and Storytelling (Linking stories). In total, we have received 69 regular paper (10 pages) submissions reviewed by a group of 86 program committee (PC) members. In the research track the PC accepted 19 regular papers (acceptance rate 27%), and 12 short-presentation papers (with the same number of pages in the proceeding as the regular ones). In addition, the conference will feature 4 demonstrations, which will appear as demo papers in the main conference proceedings. The conference keynote speakers will be Kristina Lerman and Peter Mika.",
            "fieldsOfStudy": [
                "Sociology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1805892",
                    "name": "Peter Dolog"
                },
                {
                    "authorId": "1712852",
                    "name": "P. Vojt\u00e1s"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1747800",
                    "name": "D. Helic"
                }
            ]
        },
        {
            "paperId": "1d0b61503222191fe85c7bd112f91036f6a5028e",
            "title": "FA*IR: A Fair Top-k Ranking Algorithm",
            "abstract": "In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n \u00bb k candidates, maximizing utility (i.e., select the \"best\" candidates) subject to group fairness criteria. Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-k should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above. An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2065333595",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "2075582",
                    "name": "S. Hajian"
                },
                {
                    "authorId": null,
                    "name": "Mohamed Megahed"
                },
                {
                    "authorId": "1389957009",
                    "name": "R. Baeza-Yates"
                }
            ]
        },
        {
            "paperId": "268fbcfc6cc09d9f95598afc57b900d8c21d584f",
            "title": "To Be Connected, or Not to Be Connected: That is the Minimum Inefficiency Subgraph Problem",
            "abstract": "We study the problem of extracting a selective connector for a given set of query vertices Q subset of V in a graph G = (V,E). A selective connector is a subgraph of G which exhibits some cohesiveness property, and contains the query vertices but does not necessarily connect them all. Relaxing the connectedness requirement allows the connector to detect multiple communities and to be tolerant to outliers. We achieve this by introducing the new measure of network inefficiency and by instantiating our search for a selective connector as the problem of finding the minimum inefficiency subgraph. We show that the minimum inefficiency subgraph problem is NP-hard, and devise efficient algorithms to approximate it. By means of several case studies in a variety of application domains (such as human brain, cancer, and food networks), we show that our minimum inefficiency subgraph produces high-quality solutions, exhibiting all the desired behaviors of a selective connector.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2820106",
                    "name": "Natali Ruchansky"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1398620943",
                    "name": "David Garc\u00eda-Soriano"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1946641",
                    "name": "N. Kourtellis"
                }
            ]
        },
        {
            "paperId": "403d577b81770749f37ee4e0c190a4cb399251f7",
            "title": "28th ACM International Conference on Hypertext and Social Media",
            "abstract": "The 28th 2017 ACM international conference on Hypertext and Social Media will be held in Prague, Czech Republic, from July 4 to 7. This newsletter article briefly introduces the conference and its venue. We hope to meet you all at Hypertext 2017! https://ht.acm.org/ht2017/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1805892",
                    "name": "Peter Dolog"
                },
                {
                    "authorId": "1712852",
                    "name": "P. Vojt\u00e1s"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1747800",
                    "name": "D. Helic"
                }
            ]
        },
        {
            "paperId": "40d33511b150b8aee53de32341bba61dab79593e",
            "title": "Summarizing Static and Dynamic Big Graphs",
            "abstract": "Large-scale, highly-interconnected networks pervade our society and the natural world around us, including the World Wide Web, social networks, knowledge graphs, genome and scientific databases, medical and government records. The massive scale of graph data often surpasses the available computation and storage resources. Besides, users get overwhelmed by the daunting task of understanding and using such graphs due to their sheer volume and complexity. Hence, there is a critical need to summarize large graphs into concise forms that can be more easily visualized, processed, and managed. Graph summarization has indeed attracted a lot of interests from various research communities, such as sociology, physics, chemistry, bioinformatics, and computer science. Different ways of summarizing graphs have been invented that are often complementary to each other. In this tutorial, we discuss algorithmic advances on graph summarization in the context of both classical (e.g., static graphs) and emerging (e.g., dynamic and stream graphs) applications. We emphasize the current challenges and highlight some future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "8b295d802af087991333abbd49edd98059f7980b",
            "title": "Secure Centrality Computation Over Multiple Networks",
            "abstract": "Consider a multi-layered graph, where the different layers correspond to different proprietary social networks on the same ground set of users. Suppose that the owners of the different networks (called hosts) are mutually non-trusting parties: how can they compute a centrality score for each of the users using all the layers, but without disclosing information about their private graphs? Under this setting we study a suite of three centrality measures whose algebraic structure allows performing that computation with provable security and efficiency. The first measure counts the nodes reachable from a node within a given radius. The second measure extends the first one by counting the number of paths between any two nodes. The final one is a generalization to the multi-layered graph case: not only the number of paths is counted, but also the multiplicity of these paths in the different layers is considered. We devise a suite of multiparty protocols to compute those centrality measures, which are all provably secure in the information-theoretic sense. One typical challenge and limitation of secure multiparty computation protocols is their scalability. We tackle this problem and devise a protocol which is highly scalable and still provably secure. We test our protocols on several real-world multi-layered graphs: interestingly, the protocol to compute the most sensitive measure (i.e., the multi-layered centrality) is also the most scalable one and can be efficiently run on very large networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2297426",
                    "name": "Gilad Asharov"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1398620943",
                    "name": "David Garc\u00eda-Soriano"
                },
                {
                    "authorId": "1687574",
                    "name": "Tamir Tassa"
                }
            ]
        },
        {
            "paperId": "a1b517e6d539dde2b208593cf39f25673c10f25a",
            "title": "Core Decomposition and Densest Subgraph in Multilayer Networks",
            "abstract": "Multilayer networks are a powerful paradigm to model complex systems, where various relations might occur among the same set of entities. Despite the keen interest in a variety of problems, algorithms, and analysis methods in this type of network, the problem of extracting dense subgraphs has remained largely unexplored. As a first step in this direction, in this work we study the problem of core decomposition of a multilayer network. Unlike the single-layer counterpart in which cores are all nested into one another and can be computed in linear time, the multilayer context is much more challenging as no total order exists among multilayer cores; rather, they form a lattice whose size is exponential in the number of layers. In this setting we devise three algorithms which differ in the way they visit the core lattice and in their pruning techniques. We assess time and space efficiency of the three algorithms on a large variety of real-world multilayer networks. We then move a step forward and showcase an application of the multilayer core-decomposition tool to the problem of densest-subgraph extraction from multilayer networks. We introduce a definition of multilayer densest subgraph that trades-off between high density and number of layers in which the high density holds, and show how multilayer core decomposition can be exploited to approximate this problem with quality guarantees.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32524198",
                    "name": "Edoardo Galimberti"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                }
            ]
        },
        {
            "paperId": "e2dfcf45718ec9d65c7df1108bada9f0f41cad15",
            "title": "On Information Propagation, Social Influence, and Communities",
            "abstract": "With the success of online social networks and microblogging platforms such as Facebook, Tumblr, and Twitter, the phenomenon of influence-driven propagations, has recently attracted the interest of computer scientists, sociologists , information technologists, and marketing specialists. In this talk we will take a data mining perspective, discussing what (and how) can be learned from a social network and a database of traces of past propagations over the social network. Starting from one of the key problems in this area, i.e. the identification of influential users, we will provide a brief overview of our recent contributions in this area. We will expose the connection between the phenomenon of information propagation and the existence of communities in social network, and we will go deeper in this new research topic arising at the overlap of information propagation analysis and community detection. where he leades the \" Algorithmic Data Analytics \" group. He is also Scientific Director for Data Mining at Eurecat (Technological Center of Catalunya), Barcelona. Before he was Director of Research at Yahoo Labs in Barcelona, Spain, where he was leading the Web Mining Research group. His recent research interests include mining query-logs, social networks, and social media, as well as the privacy issues related to mining these kinds of sensible data. In the past he has been interested in data mining query languages, constrained pattern mining, mining spatiotemporal and mobility data, and privacy preserving data mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "06bf54ac36d4d4a6bfef6b2994d93974b2613180",
            "title": "Efficient Methods for Influence-Based Network-Oblivious Community Detection",
            "abstract": "We study the problem of detecting social communities when the social graph is not available but instead we have access to a log of user activity, that is, a dataset of tuples (u, i, t) recording the fact that user u \u201cadopted\u201d item i at time t. We propose a stochastic framework that assumes that the adoption of items is governed by an underlying diffusion process over the unobserved social network and that such a diffusion model is based on community-level influence. That is, we aim at modeling communities through the lenses of social contagion. By fitting the model parameters to the user activity log, we learn the community membership and the level of influence of each user in each community. The general framework is instantiated with two different diffusion models, one with discrete time and one with continuous time, and we show that the computational complexity of both approaches is linear in the number of users and in the size of the propagation log. Experiments on synthetic data with planted community structure show that our methods outperform non-trivial baselines. The effectiveness of the proposed techniques is further validated on real-word data, on which our methods are able to detect high-quality communities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092322825",
                    "name": "Nicola Barbieri"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1718485",
                    "name": "G. Manco"
                }
            ]
        },
        {
            "paperId": "0a1323a6ac0e78e0712ab0614e4ff560191d5f5e",
            "title": "Identifying Buzzing Stories via Anomalous Temporal Subgraph Discovery",
            "abstract": "Story identification from online user-generated content has recently raised increasing attention. Existing approaches fall into two categories. Approaches in the first category extract stories as cohesive substructures in a graph representing the strength of association between terms. The latter category includes approaches that analyze the temporal evolution of individual terms and identify stories by grouping terms with similar anomalous temporal behavior. Both categories have limitations. In this work we advance the literature on story identification by devising a novel method that profitably combines the peculiarities of the two main existing approaches, thus also addressing their weaknesses. Experiments on a dataset extracted from a real-world web-search log demonstrate the superiority of the proposed method over the state of the art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "3033855",
                    "name": "Ilaria Bordino"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1765155",
                    "name": "G. Stilo"
                }
            ]
        },
        {
            "paperId": "1167d42fd0d0d233185883b9fe94a7b33b4d8476",
            "title": "Uncertain Graph Sparsification",
            "abstract": "Uncertain graphs are prevalent in several applications including communications systems, biological databases, and social networks. The ever increasing size of the underlying data renders both graph storage and query processing extremely expensive. Sparsification has often been used to reduce the size of deterministic graphs by maintaining only the important edges. However, adaptation of deterministic sparsification methods fails in the uncertain setting. To overcome this problem, we introduce the first sparsification techniques aimed explicitly at uncertain graphs. The proposed methods reduce the number of edges and redistribute their probabilities in order to decrease the graph size, while preserving its underlying structure. The resulting graph can be used to efficiently and accurately approximate any query and mining tasks on the original graph. An extensive experimental evaluation with real and synthetic datasets illustrates the effectiveness of our techniques on several common graph tasks, including clustering coefficient, page rank, reliability, and shortest path distance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047271",
                    "name": "Panos Parchas"
                },
                {
                    "authorId": "2559413",
                    "name": "Nikolaos Papailiou"
                },
                {
                    "authorId": "1746338",
                    "name": "Dimitris Papadias"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "452d3d9bbf434da476897ee8a909f713be57b7ae",
            "title": "Algorithmic Bias: From Discrimination Discovery to Fairness-aware Data Mining",
            "abstract": "Algorithms and decision making based on Big Data have become pervasive in all aspects of our daily lives lives (offline and online), as they have become essential tools in personal finance, health care, hiring, housing, education, and policies. It is therefore of societal and ethical importance to ask whether these algorithms can be discriminative on grounds such as gender, ethnicity, or health status. It turns out that the answer is positive: for instance, recent studies in the context of online advertising show that ads for high-income jobs are presented to men much more often than to women [Datta et al., 2015]; and ads for arrest records are significantly more likely to show up on searches for distinctively black names [Sweeney, 2013]. This algorithmic bias exists even when there is no discrimination intention in the developer of the algorithm. Sometimes it may be inherent to the data sources used (software making decisions based on data can reflect, or even amplify, the results of historical discrimination), but even when the sensitive attributes have been suppressed from the input, a well trained machine learning algorithm may still discriminate on the basis of such sensitive attributes because of correlations existing in the data. These considerations call for the development of data mining systems which are discrimination-conscious by-design. This is a novel and challenging research area for the data mining community. The aim of this tutorial is to survey algorithmic bias, presenting its most common variants, with an emphasis on the algorithmic techniques and key ideas developed to derive efficient solutions. The tutorial covers two main complementary approaches: algorithms for discrimination discovery and discrimination prevention by means of fairness-aware data mining. We conclude by summarizing promising paths for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2075582",
                    "name": "S. Hajian"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                }
            ]
        },
        {
            "paperId": "5b33395f610698b55e93c2dcfa8247c389e22eea",
            "title": "Spheres of Influence for More Effective Viral Marketing",
            "abstract": "What is the set of nodes of a social network that, under a probabilistic contagion model, would get infected if a given node $s$ gets infected? We call this set the sphere of influence of s. Due to the stochastic nature of the contagion model we need to define a notion of \"expected\" or \"typical\" cascade: this is a set of nodes which is the closest to all the possible cascades starting from s. We thus formalize the Typical Cascade problem which requires, for a given source node s, to find the set of nodes minimizing the expected Jaccard distance to all the possible cascades from s. The expected cost of a typical cascade also provides us a measure of the stability of cascade propagation, i.e., how much random cascades from a source node s deviate from the \"typical\" cascade. In this sense source nodes with lower expected costs are more reliable. We show that, while computing the quality of a candidate solution is SPhard, a method based on (1) sampling random cascades and (2) computing their Jaccard Median, can obtain a multiplicative approximation with just O(1) samples. We then devise an index that allows to efficiently compute the sphere of influence for any node in the network. Finally, we propose to approach the influence maximization problem as an instance of set cover on the spheres of influence. Through exhaustive evaluation using real-world networks and different methods of assigning the influence probability to each edge, we show that our approach outperforms in quality the theoretically optimal greedy algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144180859",
                    "name": "Y. Mehmood"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1398620943",
                    "name": "David Garc\u00eda-Soriano"
                }
            ]
        },
        {
            "paperId": "a224b890d9b8c116b46eea0aea64400a2e2f0620",
            "title": "Centrality Measures on Big Graphs: Exact, Approximated, and Distributed Algorithms",
            "abstract": "Centrality measures allow to measure the relative importance of a node or an edge in a graph w.r.t.~other nodes or edges. Several measures of centrality have been developed in the literature to capture different aspects of the informal concept of importance, and algorithms for these different measures have been proposed. In this tutorial, we survey the different definitions of centrality measures and the algorithms to compute them. We start from the most common measures, such as closeness centrality and betweenness centrality, and move to more complex ones such as spanning-edge centrality. In our presentation, we begin from exact algorithms and then progress to approximation algorithms, including sampling-based ones, and to highly-scalable MapReduce algorithms for huge graphs, both for exact computation and for keeping the measures up-to-date on dynamic graphs where edges are inserted or removed over time. Our goal is to show how advanced algorithmic techniques and scalable systems can be used to obtain efficient algorithms for an important graph mining task, and to encourage research in the area by highlighting open problems and possible directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "3019872",
                    "name": "Matteo Riondato"
                }
            ]
        },
        {
            "paperId": "ac05093b651cd21241ac03a399727c6829a2050f",
            "title": "Revenue Maximization in Incentivized Social Advertising",
            "abstract": "Incentivized social advertising, an emerging marketing model, provides monetization opportunities not only to the owners of the social networking platforms but also to their influential users by offering a \"cut\" on the advertising revenue. We consider a social network (the host) that sells ad-engagements to advertisers by inserting their ads, in the form of promoted posts, into the feeds of carefully selected \"initial endorsers\" or seed users: these users receive monetary incentives in exchange for their endorsements. The endorsements help propagate the ads to the feeds of their followers. Whenever any user engages with an ad, the host is paid some fixed amount by the advertiser, and the ad further propagates to the feed of her followers, potentially recursively. In this context, the problem for the host is is to allocate ads to influential users, taking into account the propensity of ads for viral propagation, and carefully apportioning the monetary budget of each of the advertisers between incentives to influential users and ad-engagement costs, with the rational goal of maximizing its own revenue.We show that, taking all important factors into account, the problem of revenue maximization in incentivized social advertising corresponds to the problem of monotone submodular function maximization, subject to a partition matroid constraint on the ads-to-seeds allocation, and submodular knapsack constraints on the advertisers' budgets. We show that this problem is NP-hard and devise two greedy algorithms with provable approximation guarantees, which differ in their sensitivity to seed user incentive costs.Our approximation algorithms require repeatedly estimating the expected marginal gain in revenue as well as in advertiser payment. By exploiting a connection to the recent advances made in scalable estimation of expected influence spread, we devise efficient and scalable versions of our two greedy algorithms. An extensive experimental assessment confirms the high quality of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3222408",
                    "name": "\u00c7igdem Aslay"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                }
            ]
        },
        {
            "paperId": "cfa0c739607e812d7854bc6e9696e289aa8958a4",
            "title": "Conditional Reliability in Uncertain Graphs",
            "abstract": "Network reliability is a well-studied problem that requires to measure the probability that a target node is reachable from a source node in a probabilistic (or uncertain) graph, i.e., a graph where every edge is assigned a probability of existence. Many approaches and problem variants have been considered in the literature, with the majority of them assuming that edge-existence probabilities are fixed. Nevertheless, in real-world graphs, edge probabilities typically depend on external conditions. In metabolic networks, a protein can be converted into another protein with some probability depending on the presence of certain enzymes. In social influence networks, the probability that a tweet of some user will be re-tweeted by her followers depends on whether the tweet contains specific hashtags. In transportation networks, the probability that a network segment will work properly or not, might depend on external conditions such as weather or time of the day. In this paper, we overcome this limitation and focus on <italic>conditional reliability</italic>, that is, assessing reliability when edge-existence probabilities depend on a set of conditions. In particular, we study the problem of determining the top-<inline-formula> <tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><inline-graphic xlink:href=\"khan-ieq1-2816653.gif\"/> </alternatives></inline-formula> conditions that maximize the reliability between two nodes. We deeply characterize our problem and show that, even employing polynomial-time reliability-estimation methods, it is <inline-formula> <tex-math notation=\"LaTeX\">$\\mathbf {NP}$</tex-math><alternatives><inline-graphic xlink:href=\"khan-ieq2-2816653.gif\"/> </alternatives></inline-formula>-hard, does not admit any <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {PTAS}$ </tex-math><alternatives><inline-graphic xlink:href=\"khan-ieq3-2816653.gif\"/></alternatives></inline-formula>, and the underlying objective function is non-submodular. We then devise a practical method that targets both accuracy and efficiency. We also study natural generalizations of the problem with multiple source and target nodes. An extensive empirical evaluation on several large, real-life graphs demonstrates effectiveness and scalability of our methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "3451938",
                    "name": "A. Nufer"
                }
            ]
        },
        {
            "paperId": "0328f5ebd02477c414a57c013fd880b7f41eb287",
            "title": "Privacy concerns vs. user behavior in community question answering",
            "abstract": "Community-based question answering (CQA) platforms are crowd-sourced services for sharing user expertise on various topics, from mechanical repairs to parenting. While they naturally build-in an online social network infrastructure, they carry a very different purpose from Facebook-like social networks, where users \"hang-out\" with their friends and tend to share more personal information. It is unclear, thus, how the privacy concerns and their correlation with user behavior in an online social network translate into a CQA platform. This study analyzes one year of recorded traces from a mature CQA platform to understand the association between users' privacy concerns as manifested by their account settings and their activity in the platform. The results show that privacy preference is correlated with behavior in the community in terms of engagement, retention, accomplishments and deviance from the norm. We find privacy-concerned users have higher qualitative and quantitative contributions, show higher retention, report more abuses, have higher perception on answer quality and have larger social circles. However, at the same time, these users also exhibit more deviant behavior than the users with public profiles.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "49352157",
                    "name": "Md. Imrul Kayes"
                },
                {
                    "authorId": "1946641",
                    "name": "N. Kourtellis"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1986996",
                    "name": "Adriana Iamnitchi"
                }
            ]
        },
        {
            "paperId": "08463b56fb7e8be5395e71087d4a712dc2b272fa",
            "title": "Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part III",
            "abstract": "The three volume set LNAI 9284, 9285, and 9286 constitutes the refereed proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD 2015, held in Porto, Portugal, in September 2015. The 131 papers presented in these proceedings were carefully reviewed and selected from a total of 483 submissions. These include 89 research papers, 11 industrial papers, 14 nectar papers, 17 demo papers. They were organized in topical sections named: classification, regression and supervised learning; clustering and unsupervised learning; data preprocessing; data streams and online learning; deep learning; distance and metric learning; large scale learning and big data; matrix and tensor analysis; pattern and sequence mining; preference learning and label ranking; probabilistic, statistical, and graphical approaches; rich data; and social and graphs. Part III is structured in industrial track, nectar track, and demo track.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                },
                {
                    "authorId": "46517947",
                    "name": "M. May"
                },
                {
                    "authorId": "1735228",
                    "name": "B. Zadrozny"
                },
                {
                    "authorId": "1745702",
                    "name": "Ricard Gavald\u00e0"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "3698192",
                    "name": "Jaime S. Cardoso"
                },
                {
                    "authorId": "1685612",
                    "name": "M. Spiliopoulou"
                }
            ]
        },
        {
            "paperId": "29654c700c8c9c9b3d44804b595bd808fa64a362",
            "title": "Taxonomy-Based Discovery and Annotation of Functional Areas in the City",
            "abstract": "\n \n Mapping the functional use of city areas (e.g., mapping clusters of hotels or of electronic shops) enables a variety of applications (e.g., innovative way-finding tools). To do that mapping, researchers have recently processed geo-referenced data with spatial clustering algorithms. These algorithms usually perform two consecutive steps: they cluster nearby points on the map, and then assign labels (e.g., 'electronics') to the resulting clusters. When applied in the city context, these algorithms do not fully work, not least because they consider the two steps of clustering and labeling as separate. Since there is no reason to keep those two steps separate, we propose a framework that clusters points based not only on their density but also on their semantic relatedness. We evaluate this framework upon Foursquare data in the cities of Barcelona, Milan, and London. We find that it is more effective than the baseline method of DBSCAN in discovering functional areas. We complement that quantitative evaluation with a user study involving 111 participants in the three cities. Finally, to illustrate the generalizability of our framework, we process temporal data with it and successfully discover seasonal uses of the city.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145459863",
                    "name": "Carmen Vaca"
                },
                {
                    "authorId": "144041798",
                    "name": "D. Quercia"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                }
            ]
        },
        {
            "paperId": "358e8bd763db4dd4376b0fa2b5f1456b1b466be1",
            "title": "Cultures in Community Question Answering",
            "abstract": "CQA services are collaborative platforms where users ask and answer questions. We investigate the influence of national culture on people's online questioning and answering behavior. For this, we analyzed a sample of 200 thousand users in Yahoo Answers from 67 countries. We measure empirically a set of cultural metrics defined in Geert Hofstede's cultural dimensions and Robert Levine's Pace of Life and show that behavioral cultural differences exist in community question answering platforms. We find that national cultures differ in Yahoo Answers along a number of dimensions such as temporal predictability of activities, contribution-related behavioral patterns, privacy concerns, and power inequality.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "49352157",
                    "name": "Md. Imrul Kayes"
                },
                {
                    "authorId": "1946641",
                    "name": "N. Kourtellis"
                },
                {
                    "authorId": "144041798",
                    "name": "D. Quercia"
                },
                {
                    "authorId": "1986996",
                    "name": "Adriana Iamnitchi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "449b14b07df6264d75dad8698baca09016555777",
            "title": "The Minimum Wiener Connector Problem",
            "abstract": "The Wiener index of a graph is the sum of all pairwise shortest-path distances between its vertices. In this paper we study the novel problem of finding a minimum Wiener connector: given a connected graph G=(V,E) and a set Q \u2286 V of query vertices, find a subgraph of G that connects all query vertices and has minimum Wiener index. We show that MIN WIENER CONNECTOR admits a polynomial-time (albeit impractical) exact algorithm for the special case where the number of query vertices is bounded. We show that in general the problem is NP-hard, and has no PTAS unless P = NP. Our main contribution is a constant-factor approximation algorithm running in time \u00d5(|Q||E|). A thorough experimentation on a large variety of real-world graphs confirms that our method returns smaller and denser solutions than other methods, and does so by adding to the query set Q a small number of ``important'' vertices (i.e., vertices with high centrality).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2820106",
                    "name": "Natali Ruchansky"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1398620943",
                    "name": "David Garc\u00eda-Soriano"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1946641",
                    "name": "N. Kourtellis"
                }
            ]
        },
        {
            "paperId": "56ca4d9202232688942892685e1c7218d05ee020",
            "title": "Finding Subgraphs with Maximum Total Density and Limited Overlap",
            "abstract": "Finding dense subgraphs in large graphs is a key primitive in a variety of real-world application domains, encompassing social network analytics, event detection, biology, and finance. In most such applications, one typically aims at finding several (possibly overlapping) dense subgraphs which might correspond to communities in social networks or interesting events. While a large amount of work is devoted to finding a single densest subgraph, perhaps surprisingly, the problem of finding several dense subgraphs with limited overlap has not been studied in a principled way, to the best of our knowledge. In this work we define and study a natural generalization of the densest subgraph problem, where the main goal is to find at most $k$ subgraphs with maximum total aggregate density, while satisfying an upper bound on the pairwise Jaccard coefficient between the sets of nodes of the subgraphs. After showing that such a problem is NP-Hard, we devise an efficient algorithm that comes with provable guarantees in some cases of interest, as well as, an efficient practical heuristic. Our extensive evaluation on large real-world graphs confirms the efficiency and effectiveness of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1908403",
                    "name": "Oana Balalau"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2172722926",
                    "name": "T-H. Hubert Chan"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1816033",
                    "name": "Mauro Sozio"
                }
            ]
        },
        {
            "paperId": "56d3fe127e3b7df2de22eb532d27ceedad1449a2",
            "title": "Top-k Reliable Edge Colors in Uncertain Graphs",
            "abstract": "We study the fundamental problem of finding the set of top-k edge colors that maximizes the reliability between a source node and a destination node in an uncertain and edge-colored graph. Our top-k reliable color set problem naturally arises in a variety of real-world applications including pathway finding in biological networks, topic-aware influence maximization, and team formation in social networks, among many others. In addition to the #P-completeness of the classical reliability finding problem between a source and a destination node over an uncertain graph, we prove that our problem is also NP-hard, and neither sub-modular, nor super-modular. To this end, we aim at designing effective and scalable solutions for the top-k reliable color set problem. We first introduce two baselines following the idea of repetitive inclusion of the next best edge colors, and we later develop a more efficient and effective algorithm that directly finds the highly-reliable paths while maintaining the budget on the number of edge-colors. An extensive empirical evaluation on various large-scale and real-world graph datasets illustrates that our proposed techniques are both scalable and highly accurate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "2006929",
                    "name": "Thomas Wohler"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "c94d51b0f7b7510cf0c9fe4ac04055339648045e",
            "title": "The Social World of Content Abusers in Community Question Answering",
            "abstract": "Community-based question answering platforms can be rich sources of information on a variety of specialized topics, from finance to cooking. The usefulness of such platforms depends heavily on user contributions (questions and answers), but also on respecting the community rules. As a crowd-sourced service, such platforms rely on their users for monitoring and flagging content that violates community rules. Common wisdom is to eliminate the users who receive many flags. Our analysis of a year of traces from a mature Q&A site shows that the number of flags does not tell the full story: on one hand, users with many flags may still contribute positively to the community. On the other hand, users who never get flagged are found to violate community rules and get their accounts suspended. This analysis, however, also shows that abusive users are betrayed by their network properties: we find strong evidence of homophilous behavior and use this finding to detect abusive users who go under the community radar. Based on our empirical observations, we build a classifier that is able to detect abusive users with an accuracy as high as 83%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49352157",
                    "name": "Md. Imrul Kayes"
                },
                {
                    "authorId": "1946641",
                    "name": "N. Kourtellis"
                },
                {
                    "authorId": "144041798",
                    "name": "D. Quercia"
                },
                {
                    "authorId": "1986996",
                    "name": "Adriana Iamnitchi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "f69e4e866b13cbfd6ddc74c44373ca5219f84122",
            "title": "Uncertain Graph Processing through Representative Instances",
            "abstract": "Data in several applications can be represented as an uncertain graph whose edges are labeled with a probability of existence. Exact query processing on uncertain graphs is prohibitive for most applications, as it involves evaluation over an exponential number of instantiations. Thus, typical approaches employ Monte-Carlo sampling, which (i) draws a number of possible graphs (samples), (ii) evaluates the query on each of them, and (iii) aggregates the individual answers to generate the final result. However, this approach can also be extremely time consuming for large uncertain graphs commonly found in practice. To facilitate efficiency, we study the problem of extracting a single representative instance from an uncertain graph. Conventional processing techniques can then be applied on this representative to closely approximate the result on the original graph.\n In order to maintain data utility, the representative instance should preserve structural characteristics of the uncertain graph. We start with representatives that capture the expected vertex degrees, as this is a fundamental property of the graph topology. We then generalize the notion of vertex degree to the concept of n-clique cardinality, that is, the number of cliques of size n that contain a vertex. For the first problem, we propose two methods: Average Degree Rewiring (ADR), which is based on random edge rewiring, and Approximate B-Matching (ABM), which applies graph matching techniques. For the second problem, we develop a greedy approach and a game-theoretic framework. We experimentally demonstrate, with real uncertain graphs, that indeed the representative instances can be used to answer, efficiently and accurately, queries based on several metrics such as shortest path distance, clustering coefficient, and betweenness centrality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047271",
                    "name": "Panos Parchas"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1746338",
                    "name": "Dimitris Papadias"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "fb2a72554d71111c39559f431e419d59350d61d4",
            "title": "Graph Query Reformulation with Diversity",
            "abstract": "We study a problem of graph-query reformulation enabling explorative query-driven discovery in graph databases. Given a query issued by the user, the system, apart from returning the result patterns, also proposes a number of specializations (i.e., supergraphs) of the original query to facilitate the exploration of the results. We formalize the problem of finding a set of reformulations of the input query by maximizing a linear combination of coverage (of the original query's answer set) and diversity among the specializations. We prove that our problem is hard, but also that a simple greedy algorithm achieves a (1/2)-approximation guarantee. The most challenging step of the greedy algorithm is the computation of the specialization that brings the maximum increment to the objective function. To efficiently solve this step, we show how to compute the objective-function increment of a specialization linearly in the number of its results and derive an upper bound that we exploit to devise an efficient search-space visiting strategy. An extensive evaluation on real and synthetic databases attests high efficiency and accuracy of our proposal.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                }
            ]
        },
        {
            "paperId": "01e5e42b666e11930fea6f243b48eaae5519c45c",
            "title": "Scalable Online Betweenness Centrality in Evolving Graphs",
            "abstract": "Betweenness centrality is a classic measure that quantifies the importance of a graph element (vertex or edge) according to the fraction of shortest paths passing through it. This measure is notoriously expensive to compute, and the best known algorithm runs in O(nm) time. The problems of efficiency and scalability are exacerbated in a dynamic setting, where the input is an evolving graph seen edge by edge, and the goal is to keep the betweenness centrality up to date. In this paper, we propose the first truly scalable algorithm for online computation of betweenness centrality of both vertices and edges in an evolving graph where new edges are added and existing edges are removed. Our algorithm is carefully engineered with out-of-core techniques and tailored for modern parallel stream processing engines that run on clusters of shared-nothing commodity hardware. Hence, it is amenable to real-world deployment. We experiment on graphs that are two orders of magnitude larger than previous studies. Our method is able to keep the betweenness centrality measures up-to-date online, i.e., the time to update the measures is smaller than the inter-arrival time between two consecutive updates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1946641",
                    "name": "N. Kourtellis"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "0afdee2c68dabc1207fd1ddf81a25d84fef87abc",
            "title": "Fast Reliability Search in Uncertain Graphs",
            "abstract": "Uncertain, or probabilistic, graphs have been increasingl y used to represent noisy linked data in many emerging application scenarios, and have recently attracted the attention of the databa se research community. A fundamental problem on uncertain graphs is reliability, which deals with the probability of nodes being reachable one from another. Existing literature has exclusively focused on reliability detection, which asks to compute the probability that two given nodes are connected. In this paper we study reliability search on uncertain graphs, which we define as the problem of computing all nodes reachable from a set of query nodes with probability no less than a given threshold. Existing reliability-detection approac hes are not well-suited to efficiently handle the reliability-search p roblem. We propose RQ-tree, a novel index which is based on a hierarchical clustering of the nodes in the graph, and further optimized using a balanced-minimum-cut criterion. Based on RQ-tree, we define a fast filtering-and-verification online query-evaluation s trategy that relies on a maximum-flow-based candidate-generation phase , followed by a verification phase consisting of either a lower-bo unding method or a sampling technique. The first verification method returns no incorrect nodes, thus guaranteeing perfect precis ion, completely avoids sampling, and is more efficient. The second ve rification method ensures instead better recall. Extensive experiments on real-world uncertain graphs show that our methods are very efficient\u2014over state-of-the-art relia bilitydetection methods, we obtain a speed-up up to five orders of ma gnitude; as well as accurate\u2014our techniques achieve precision > 0.95 and recall usually higher than 0.75.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                }
            ]
        },
        {
            "paperId": "6f6ee28dfb1ba341dd2b74b5bdcd276357182bab",
            "title": "Composite Retrieval of Diverse and Complementary Bundles",
            "abstract": "Users are often faced with the problem of finding complementary items that together achieve a single common goal (e.g., a starter kit for a novice astronomer, a collection of question/answers related to low-carb nutrition, a set of places to visit on holidays). In this paper, we argue that for some application scenarios returning item bundles is more appropriate than ranked lists. Thus we define composite retrieval as the problem of finding k bundles of complementary items. Beyond complementarity of items, the bundles must be valid w.r.t. a given budget, and the answer set of k bundles must exhibit diversity. We formally define the problem and show that in its general form is NP-hard and that also the special cases in which each bundle is formed by only one item, or only one bundle is sought, are hard. Our characterization however suggests how to adopt a two-phase approach (Produce-and-Choose, or PAC) in which we first produce many valid bundles, and then we choose k among them. For the first phase we devise two ad-hoc clustering algorithms, while for the second phase we adapt heuristics with approximation guarantees for a related problem. We also devise another approach which is based on first finding a k-clustering and then selecting a valid bundle from each of the produced clusters (Cluster-and-Pick, or CAP). We compare experimentally the proposed methods on two real-world data sets: the first data set is given by a sample of touristic attractions in 10 large European cities, while the second is a large database of user-generated restaurant reviews from Yahoo! Local. Our experiments show that when diversity is highly important, CAP is the best option, while when diversity is less important, a PAC approach constructing bundles around randomly chosen pivots, is better.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2722051",
                    "name": "E. Feuerstein"
                },
                {
                    "authorId": "1398937781",
                    "name": "I. M\u00e9ndez-D\u00edaz"
                },
                {
                    "authorId": "144535064",
                    "name": "Paula Zabala"
                }
            ]
        },
        {
            "paperId": "833e97b28941eb31e8804c94af668098d5790b90",
            "title": "The pursuit of a good possible world: extracting representative instances of uncertain graphs",
            "abstract": "Data in several applications can be represented as an uncertain graph, whose edges are labeled with a probability of existence. Exact query processing on uncertain graphs is prohibitive for most applications, as it involves evaluation over an exponential number of instantiations. Even approximate processing based on sampling is usually extremely expensive since it requires a vast number of samples to achieve reasonable quality guarantees. To overcome these problems, we propose algorithms for creating deterministic representative instances of uncertain graphs that maintain the underlying graph properties. Specifically, our algorithms aim at preserving the expected vertex degrees because they capture well the graph topology. Conventional processing techniques can then be applied on these instances to closely approximate the result on the uncertain graph. We experimentally demonstrate, with real and synthetic uncertain graphs, that indeed the representative instances can be used to answer, efficiently and accurately, queries based on several properties such as shortest path distance, clustering coefficient and betweenness centrality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047271",
                    "name": "Panos Parchas"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1746338",
                    "name": "Dimitris Papadias"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "85c24f0f0f99ba36649673f10c9d4d2a5076c339",
            "title": "Correlation clustering: from theory to practice",
            "abstract": "Correlation clustering is arguably the most natural formulation of clustering. Given a set of objects and a pairwise similarity measure between them, the goal is to cluster the objects so that, to the best possible extent, similar objects are put in the same cluster and dissimilar objects are put in different clusters. As it just needs a definition of similarity, its broad generality makes it applicable to a wide range of problems in different contexts, and in particular makes it naturally suitable to clustering structured objects for which feature vectors can be difficult to obtain. Despite its simplicity, generality and wide applicability, correlation clustering has so far received much more attention from the algorithmic theory community than from the data mining community. The goal of this tutorial is to show how correlation clustering can be a powerful addition to the toolkit of the data mining researcher and practitioner, and to encourage discussions and further research in the area. In the tutorial we will survey the problem and its most common variants, with an emphasis on the algorithmic techniques and key ideas developed to derive efficient solutions. We will motivate the problems and discuss real-world applications, the scalability issues that may arise, and the existing approaches to handle them.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1398620943",
                    "name": "David Garc\u00eda-Soriano"
                },
                {
                    "authorId": "2941680",
                    "name": "Edo Liberty"
                }
            ]
        },
        {
            "paperId": "8cedc418ded7c4f5b3a10d6e551478c7084940af",
            "title": "Who to follow and why: link prediction with explanations",
            "abstract": "User recommender systems are a key component in any on-line social networking platform: they help the users growing their network faster, thus driving engagement and loyalty. In this paper we study link prediction with explanations for user recommendation in social networks. For this problem we propose WTFW (\"Who to Follow and Why\"), a stochastic topic model for link prediction over directed and nodes-attributed graphs. Our model not only predicts links, but for each predicted link it decides whether it is a \"topical\" or a \"social\" link, and depending on this decision it produces a different type of explanation. A topical link is recommended between a user interested in a topic and a user authoritative in that topic: the explanation in this case is a set of binary features describing the topic responsible of the link creation. A social link is recommended between users which share a large social neighborhood: in this case the explanation is the set of neighbors which are more likely to be responsible for the link creation. Our experimental assessment on real-world data confirms the accuracy of WTFW in the link prediction and the quality of the associated explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092322825",
                    "name": "Nicola Barbieri"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1718485",
                    "name": "G. Manco"
                }
            ]
        },
        {
            "paperId": "92440b94f792c397a802787894a7d4da778f7931",
            "title": "Viral Marketing Meets Social Advertising: Ad Allocation with Minimum Regret",
            "abstract": "Social advertisement is one of the fastest growing sectors in the digital advertisement landscape: ads in the form of promoted posts are shown in the feed of users of a social networking platform, along with normal social posts; if a user clicks on a promoted post, the host (social network owner) is paid a fixed amount from the advertiser. In this context, allocating ads to users is typically performed by maximizing click-through-rate, i.e., the likelihood that the user will click on the ad. However, this simple strategy fails to leverage the fact the ads can propagate virally through the network, from endorsing users to their followers. \n \nIn this paper, we study the problem of allocating ads to users through the viral-marketing lenses. We show that allocation that takes into account the propensity of ads for viral propagation can achieve significantly better performance. However, uncontrolled virality could be undesirable for the host as it creates room for exploitation by the advertisers: hoping to tap uncontrolled virality, an advertiser might declare a lower budget for its marketing campaign, aiming at the same large outcome with a smaller cost. \n \nThis creates a challenging trade-off: on the one hand, the host aims at leveraging virality and the network effect to improve advertising efficacy, while on the other hand the host wants to avoid giving away free service due to uncontrolled virality. We formalize this as the problem of ad allocation with minimum regret, which we show is NP-hard and inapproximable w.r.t. any factor. However, we devise an algorithm that provides approximation guarantees w.r.t. the total budget of all advertisers. We develop a scalable version of our approximation algorithm, which we extensively test on four real-world data sets, confirming that our algorithm delivers high quality solutions, is scalable, and significantly outperforms several natural baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3222408",
                    "name": "\u00c7igdem Aslay"
                },
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "93b8c7b39c2e7c5d7ea3949a308a40e9eb62b83d",
            "title": "Distance oracles in edge-labeled graphs",
            "abstract": "A fundamental operation over edge-labeled graphs is the computation of shortest-path distances subject to a constraint on the set of permissible edge labels. Applying exact algorithms for such an operation is not a viable option, especially for massive graphs, or in scenarios where the distance computation is used as a primitive for more complex computations. In this paper we study the problem of efficient approximation of shortest-path queries with edge-label constraints, for which we devise two indexes based on the idea of landmarks: distances from all vertices of the graph to a selected subset of landmark vertices are pre-computed and then used at query time to efficiently approximate distance queries. The major challenge to face is that, in principle, an exponential number of constraint label sets needs to be stored for each vertex-landmark pair, which makes the index pre-computation and storage far from trivial. We tackle this challenge from two different perspectives, which lead to indexes with different characteristics: one index is faster and more accurate, but it requires more space than the other. We extensively evaluate our techniques on real and synthetic datasets, showing that our indexes can efficiently and accurately estimate label-constrained distance queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1693307",
                    "name": "Antti Ukkonen"
                }
            ]
        },
        {
            "paperId": "c3920c6c06a70b11cbec596b22f3462ab5cb9d0b",
            "title": "Influence Maximization with Viral Product Design",
            "abstract": "Product design and viral marketing are two popular concepts in the marketing literature that, although following different paths, aim at the same goal: maximizing the adoption of a new product. While the effect of the social network is nowadays kept in great consideration in any marketingrelated activity, the interplay between product design and social influence is surprisingly still largely unexplored. In this paper we move a first step in this direction and study the problem of designing the features of a novel product such that its adoption, fueled by peer influence and \u201cword-of-mouth\u201d effect, is maximized. We model the viral process of product adoption on the basis of social influence and the features of the product, and devise an improved iterative scaling procedure to learn the parameters that maximize the likelihood of our novel feature-aware propagation model. In order to design an effective algorithm for our problem, we study the property of the underlying propagation model. In particular we show that the expected spread, i.e., the objective function to maximize, is monotone and submodular when we fix the features of the product and seek for the set of users to target in the viral marketing campaign. Instead, when we fix the set of users and try to find the optimal features for the product, then the expected spread is neither submodular nor monotone (as it is the case, in general, for product design). Therefore, we develop an algorithm based on an alternating optimization between selecting the features of the product, and the set of users to target in the campaign. Our experimental evaluation on realworld data from the domain of social music consumption (LastFM) and social movie consumption (Flixster) confirms the effectiveness of the proposed framework in integrating product design in viral marketing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092322825",
                    "name": "Nicola Barbieri"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "cf8b130e30483baf2756262b9369fc3864c1244d",
            "title": "Core decomposition of uncertain graphs",
            "abstract": "Core decomposition has proven to be a useful primitive for a wide range of graph analyses. One of its most appealing features is that, unlike other notions of dense subgraphs, it can be computed linearly in the size of the input graph. In this paper we provide an analogous tool for uncertain graphs, i.e., graphs whose edges are assigned a probability of existence. The fact that core decomposition can be computed efficiently in deterministic graphs does not guarantee efficiency in uncertain graphs, where even the simplest graph operations may become computationally intensive. Here we show that core decomposition of uncertain graphs can be carried out efficiently as well. We extensively evaluate our definitions and methods on a number of real-world datasets and applications, such as influence maximization and task-driven team formation.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "1889171",
                    "name": "Andreas Kaltenbrunner"
                },
                {
                    "authorId": "1782794",
                    "name": "Yana Volkovich"
                }
            ]
        },
        {
            "paperId": "d39537004fc8205eb612299728ba67d0e7565b16",
            "title": "Privacy Preserving Estimation of Social Influence",
            "abstract": "Exploiting word-of-mouth effect to create viral cascades in social networks is a very appealing possibility from the marketing standpoint. However, in order to set up an effective viral marketing campaign, one has first to accurately estimate social influence. This is usually done by analyzing user activity data. As we point out in this paper, the data analysis and sharing that is needed to estimate social influence raises important privacy issues that may jeopardize the legal, ethical and societal acceptability of such practice, and in turn, the concrete applicability of viral marketing in the real world. In this paper we devise secure multiparty protocols that allow a group of service providers and a social networking platform to jointly compute social influence, in a privacy preserving manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1687574",
                    "name": "Tamir Tassa"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "e2ebacad5f5b39b2bae6436551adcad8001d489a",
            "title": "Description-Driven Community Detection",
            "abstract": "Traditional approaches to community detection, as studied by physicists, sociologists, and more recently computer scientists, aim at simply partitioning the social network graph. However, with the advent of online social networking sites, richer data has become available: beyond the link information, each user in the network is annotated with additional information, for example, demographics, shopping behavior, or interests. In this context, it is therefore important to develop mining methods which can take advantage of all available information. In the case of community detection, this means finding good communities (a set of nodes cohesive in the social graph) which are associated with good descriptions in terms of user information (node attributes).\n Having good descriptions associated to our models make them understandable by domain experts and thus more useful in real-world applications. Another requirement dictated by real-world applications, is to develop methods that can use, when available, any domain-specific background knowledge. In the case of community detection the background knowledge could be a vague description of the communities sought in a specific application, or some prototypical nodes (e.g., good customers in the past), that represent what the analyst is looking for (a community of similar users).\n Towards this goal, in this article, we define and study the problem of finding a diverse set of cohesive communities with concise descriptions. We propose an effective algorithm that alternates between two phases: a hill-climbing phase producing (possibly overlapping) communities, and a description induction phase which uses techniques from supervised pattern set mining. Our framework has the nice feature of being able to build well-described cohesive communities starting from any given description or seed set of nodes, which makes it very flexible and easily applicable in real-world applications.\n Our experimental evaluation confirms that the proposed method discovers cohesive communities with concise descriptions in realistic and large online social networks such as Delicious, Flickr, and LastFM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "18133094",
                    "name": "Simon Pool"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "144453519",
                    "name": "M. Leeuwen"
                }
            ]
        },
        {
            "paperId": "ed19749c63da0c86490d2a770d815de7ccba05ce",
            "title": "Online Topic-aware Influence Maximization Queries",
            "abstract": "Influence maximization is the key algorithmic problem behind viral marketing: it requires to identify a set of influential users in a social network, who, when convinced to adopt a product, shall influence other users in the network, leading to a large number of adoptions. Although real world users evidently have di\u21b5erent degrees of interest and authoritativeness on di\u21b5erent topics, the bulk of the literature on influence maximization is topic-blind, in the sense that it treats all items as they were the same. In this paper we study Topic-aware Influence Maximization (TIM) queries: given a directed social graph, where the arcs are associated with a topic-dependent user-to-user social influence strength, and given a budget k, the problem requires to find a set of k users (named seed set) that we shall target in a viral marketing campaign for a given new item (described as a distribution over topics) in order to maximize its adoption. Our goal is to answer such queries in milliseconds, thus enabling online social influence analytics, what-if simulation, and marketing decision making. The main challenge here is the enormous number of potential queries: any possible distribution over the topic space (i.e., any possible item) induces a di\u21b5erent probabilistic graph, and thus a di\u21b5erent instance of the standard influence maximization problem, for which eciency and scalability are still unsolved problems. Given these computational challenges, we propose to build an index over pre-computed solutions for a limited number of possible queries. Our proposal, INFLEX, employs a treebased index for similarity search with Bregman divergences, to eciently retrieve a good-enough set of neighbor points for the query item. Then it performs rank aggregation on their seed sets to produce the final answer to the query. Experimental results on real data show that INFLEX can provide in few milliseconds a solution very similar (Kendall\u2327 distance < 0.1) to the one produced by the best known o\u270fine computation (which usually takes several days).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3222408",
                    "name": "\u00c7igdem Aslay"
                },
                {
                    "authorId": "2092322825",
                    "name": "Nicola Barbieri"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1389957009",
                    "name": "R. Baeza-Yates"
                }
            ]
        },
        {
            "paperId": "0ff65e58e16844021fc8a9e9259b21a01f979148",
            "title": "Influence-Based Network-Oblivious Community Detection",
            "abstract": "How can we detect communities when the social graphs is not available? We tackle this problem by modeling social contagion from a log of user activity, that is a dataset of tuples (u, i, t) recording the fact that user u \"adopted\" item i at time t. This is the only input to our problem. We propose a stochastic framework which assumes that item adoptions are governed by un underlying diffusion process over the unobserved social network, and that such diffusion model is based on community-level influence. By fitting the model parameters to the user activity log, we learn the community membership and the level of influence of each user in each community. This allows to identify for each community the \"key\" users, i.e., the leaders which are most likely to influence the rest of the community to adopt a certain item. The general framework can be instantiated with different diffusion models. In this paper we define two models: the extension to the community level of the classic (discrete time) Independent Cascade model, and a model that focuses on the time delay between adoptions. To the best of our knowledge, this is the first work studying community detection without the network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092322825",
                    "name": "Nicola Barbieri"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1718485",
                    "name": "G. Manco"
                }
            ]
        },
        {
            "paperId": "212e81b65a743042a45defb555e9c452e2155b27",
            "title": "The role of information diffusion in the evolution of social networks",
            "abstract": "Every day millions of users are connected through online social networks, generating a rich trove of data that allows us to study the mechanisms behind human interactions. Triadic closure has been treated as the major mechanism for creating social links: if Alice follows Bob and Bob follows Charlie, Alice will follow Charlie. Here we present an analysis of longitudinal micro-blogging data, revealing a more nuanced view of the strategies employed by users when expanding their social circles. While the network structure affects the spread of information among users, the network is in turn shaped by this communication activity. This suggests a link creation mechanism whereby Alice is more likely to follow Charlie after seeing many messages by Charlie. We characterize users with a set of parameters associated with different link creation strategies, estimated by a Maximum-Likelihood approach. Triadic closure does have a strong effect on link formation, but shortcuts based on traffic are another key factor in interpreting network evolution. However, individual strategies for following other users are highly heterogeneous. Link creation behaviors can be summarized by classifying users in different categories with distinct structural and behavioral characteristics. Users who are popular, active, and influential tend to create traffic-based shortcuts, making the information diffusion process more efficient in the network.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2617057",
                    "name": "L. Weng"
                },
                {
                    "authorId": "2951268",
                    "name": "Jacob Ratkiewicz"
                },
                {
                    "authorId": "1826736",
                    "name": "N. Perra"
                },
                {
                    "authorId": "2054864984",
                    "name": "B. Gon\u00e7alves"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2251027",
                    "name": "Rossano Schifanella"
                },
                {
                    "authorId": "143653472",
                    "name": "F. Menczer"
                },
                {
                    "authorId": "1769960",
                    "name": "A. Flammini"
                }
            ]
        },
        {
            "paperId": "28cd61c2f5cbd51c91d89898f2e41a7b5e5ed015",
            "title": "Introduction to the special issue on social web mining",
            "abstract": "Social web platforms such as Facebook, Twitter, and Flickr are used by millions of users daily, leading to a vast volume of semantically rich and dynamically updated social data. This data richness represents an unprecedented opportunity for any researcher interested in modeling human interactions. Making sense of the data generated from social web platforms can also be important for developing new technologies and applications including sentiment analysis for elections, ad serving, and social recommender systems. Various methods have been developed at commercial and academic laboratories for these purposes. This special issue consists of eight articles accepted by ACM Transactions on Intelligent Systems and Technology Special Issue on Social Web Mining. The articles have been selected according to a strict review process. There were in total 33 submissions to this special issue, and each submission received peer reviews by three reviewers. The accepted articles cover topics in mining the social web, including new approaches for prediction tasks (e.g., link predictions, tag recommendations, user preferences) and community detection for social networks. They also cover new approaches for topic modeling in social media including dynamically modeling sentiments and topics for social networks considering the underlying dynamics and personalized emerging topic detection. The complex, highly dynamic temporal behavior of topics is clearly one of the distinguishing aspects of social media. Not only it is important to determine topics and users' views at any given moment, but also to track and describe their evolution over time in an interpretable manner. Four of the contributions to the special issue deal with this temporal aspect. The first such article, \" Dynamic Joint Sentiment-Topic Model \" , by He, Lin, Gao, and Wong, describes methods for dynamically building and incrementally adjusting the joint sentiment-topic distributions, allowing for a multiscale perspective. The second article, \" Personalized Emerging Topic Detection Based on a Term Aging Model \" , by Cataldi, Di Caro, and Schifanella, proposes a method for retrieving emerging topics, built on aging models for terms and topics, in real time and recommending them at the user level. Arias, Arratia, and Xuriguera in their article, \" Forecasting with Twitter Data \" , study how much mood analysis in Twitter helps to improve prediction in time series when used as a complement to predictors that use standard information, in particular for predicting volatility in stock markets and movie box office revenues. Finally, Lee, Caverlee, Cheng, and Sui in \" Campaign \u2026",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "70219052",
                    "name": "Wray L. Buntine"
                },
                {
                    "authorId": "1745702",
                    "name": "Ricard Gavald\u00e0"
                },
                {
                    "authorId": "3095943",
                    "name": "Shengbo Guo"
                }
            ]
        },
        {
            "paperId": "2cb3a981015780609a8ad67908b08023ff6567c2",
            "title": "Mining Summaries of Propagations",
            "abstract": "Analyzing the traces left by a meme of information propagating through a social network or by a user browsing a website can help to unveil the structure and dynamics of such complex networks. This may in turn open the door to concrete applications, such as finding influential users for a topic in a social network, or detecting the typical structure of a web browsing session that leads to a product purchase. In this paper we define the problem of mining summaries of propagations as a constrained pattern-mining problem. A propagation is a DAG where an entity (e.g., information exchanged in a social network, or a user browsing a website) flows following the underlying hierarchical structure of the nodes. A summary is a set of propagations that (i) involve a similar population of nodes, and (ii) exhibit a coherent hierarchical structure when merged altogether to form a single graph. The first constraint is defined based on the Jaccard coefficient, while the definition of the second one relies on the graph-theoretic concept of \"agony\" of a graph. It turns out that both constraints satisfy the downward closure property, thus enabling Apriori-like algorithms. However, motivated by the fact that computing agony is much more expensive than computing Jaccard, we devise two algorithms that explore the search space differently. The first algorithm is an Apriori-like, bottom-up method that checks both the constraints level-by-level. The second algorithm consists of a first phase where the search space is pruned as much as possible by exploiting the Jaccard constraint only, while involving the second constraint only afterwards, in a subsequent phase. We test our algorithms on four real-world datasets. Quantitative results reveal that the choice of the most efficient algorithm depends on the selectivity of the two constraints. Qualitative results show the relevance of the extracted summaries in a number of real-world scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2774204",
                    "name": "L. Macchia"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "3315710",
                    "name": "Luca Chiarandini"
                }
            ]
        },
        {
            "paperId": "6755a4bebea02e536e244a4ebffc059f6c66bba3",
            "title": "STRIP: stream learning of influence probabilities",
            "abstract": "Influence-driven diffusion of information is a fundamental process in social networks. Learning the latent variables of such process, i.e., the influence strength along each link, is a central question towards understanding the structure and function of complex networks, modeling information cascades, and developing applications such as viral marketing. Motivated by modern microblogging platforms, such as twitter, in this paper we study the problem of learning influence probabilities in a data-stream scenario, in which the network topology is relatively stable and the challenge of a learning algorithm is to keep up with a continuous stream of tweets using a small amount of time and memory. Our contribution is a number of randomized approximation algorithms, categorized according to the available space (superlinear, linear, and sublinear in the number of nodes n) and according to different models (landmark and sliding window). Among several results, we show that we can learn influence probabilities with one pass over the data, using O(nlog n) space, in both the landmark model and the sliding-window model, and we further show that our algorithm is within a logarithmic factor of optimal. For truly large graphs, when one needs to operate with sublinear space, we show that we can still learn influence probabilities in one pass, assuming that we restrict our attention to the most active users. Our thorough experimental evaluation on large social graph demonstrates that the empirical performance of our algorithms agrees with that predicted by the theory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1712289",
                    "name": "Konstantin Kutzkov"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                }
            ]
        },
        {
            "paperId": "6f325ac1385050e17110dbf1b0ff01fec22ca7b1",
            "title": "Denser than the densest subgraph: extracting optimal quasi-cliques with quality guarantees",
            "abstract": "Finding dense subgraphs is an important graph-mining task with many applications. Given that the direct optimization of edge density is not meaningful, as even a single edge achieves maximum density, research has focused on optimizing alternative density functions. A very popular among such functions is the average degree, whose maximization leads to the well-known densest-subgraph notion. Surprisingly enough, however, densest subgraphs are typically large graphs, with small edge density and large diameter. In this paper, we define a novel density function, which gives subgraphs of much higher quality than densest subgraphs: the graphs found by our method are compact, dense, and with smaller diameter. We show that the proposed function can be derived from a general framework, which includes other important density functions as subcases and for which we show interesting general theoretical properties. To optimize the proposed function we provide an additive approximation algorithm and a local-search heuristic. Both algorithms are very efficient and scale well to large graphs. We evaluate our algorithms on real and synthetic datasets, and we also devise several application studies as variants of our original problem. When compared with the method that finds the subgraph of the largest average degree, our algorithms return denser subgraphs with smaller diameter. Finally, we discuss new interesting research directions that our problem leaves open.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2023899",
                    "name": "Charalampos E. Tsourakakis"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "2645426",
                    "name": "M. A. Tsiarli"
                }
            ]
        },
        {
            "paperId": "92aec6fa5754b503b53158ff304cdcd6ce1c7792",
            "title": "Complexity and algorithms for composite retrieval",
            "abstract": "Online search has become a daily activity and a source of a variety of valuable information, from the finest granularity such as finding the address of a specific restaurant, to more complex tasks like looking for accessories compatible with an iPhone or planning a trip. The latter typically involves running multiple search queries to gather information about different places, reading online reviews to find out about hotels, and checking geographic proximity of places to visit. We refer to this information seeking activity as composite retrieval and propose to organize results into item bundles that together constitute an improved exploratory experience over ranked lists. As a first step towards composite retrieval definition, we need to formalize intuitive desirable properties of item bundles. We distinguish between properties of each bundle in the answer and properties of the answer as a whole. Consider the case of a user selecting the restaurants to try during a visit to a new city. The user has a limited budget which might be either financial, or simply the number of nights spent in the city. The user prefers suggested restaurants to serve different cuisines. The validity of a bundle of restaurants is given by the budget constraint and the complementarity of the restaurants in the bundle w.r.t. the cuisine they serve. Other restaurant attributes could be used for defining valid bundles. For example, instead of cuisines, different dress codes could be required to every restaurant in a single bundle. Moreover, in order to provide meaningful bundles, restaurants forming each bundle must be compatible, e.g., close geographically, or liked by similar reviewers. The degree of compatibility of the items forming a bundle defines the quality of the bundle. Intuitively, in the case geographic distance is used, the closer restaurants are from each other, the higher the quality of the bundle they belong to. Similarly, when common reviewers are used as the",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2722051",
                    "name": "E. Feuerstein"
                },
                {
                    "authorId": "1398937781",
                    "name": "I. M\u00e9ndez-D\u00edaz"
                },
                {
                    "authorId": "144535064",
                    "name": "Paula Zabala"
                }
            ]
        },
        {
            "paperId": "a64c5b87d268bf405a60be3b5d684b55502029bb",
            "title": "From machu_picchu to \"rafting the urubamba river\": anticipating information needs via the entity-query graph",
            "abstract": "We study the problem of anticipating user search needs, based on their browsing activity. Given the current web page p that a user is visiting we want to recommend a small and diverse set of search queries that are relevant to the content of p, but also non-obvious and serendipitous. We introduce a novel method that is based on the content of the page visited, rather than on past browsing patterns as in previous literature. Our content-based approach can be used even for previously unseen pages. We represent the topics of a page by the set of Wikipedia entities extracted from it. To obtain useful query suggestions for these entities, we exploit a novel graph model that we call EQGraph (Entity-Query Graph), containing entities, queries, and transitions between entities, between queries, as well as from entities to queries. We perform Personalized PageRank computation on such a graph to expand the set of entities extracted from a page into a richer set of entities, and to associate these entities with relevant query suggestions. We develop an efficient implementation to deal with large graph instances and suggest queries from a large and diverse pool. We perform a user study that shows that our method produces relevant and interesting recommendations, and outperforms an alternative method based on reverse IR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3033855",
                    "name": "Ilaria Bordino"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "b527f54dc237721af64b3d952ac1798b358d3bb5",
            "title": "Cascade-based community detection",
            "abstract": "Given a directed social graph and a set of past informa- tion cascades observed over the graph, we study the novel problem of detecting modules of the graph (communities of nodes), that also explain the cascades. Our key observation is that both information propagation and social ties forma- tion in a social network can be explained according to the same latent factor, which ultimately guide a user behavior within the network. Based on this observation, we propose the Community-Cascade Network (CCN) model, a stochas- tic mixture membership generative model that can fit, at the same time, the social graph and the observed set of cas- cades. Our model produces overlapping communities and for each node, its level of authority and passive interest in each community it belongs. For learning the parameters of the CCN model, we devise a Generalized Expectation Maximization procedure. We then apply our model to real-world social networks and in- formation cascades: the results witness the validity of the proposed CCN model, providing useful insights on its signif- icance for analyzing social behavior.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092322825",
                    "name": "Nicola Barbieri"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1718485",
                    "name": "G. Manco"
                }
            ]
        },
        {
            "paperId": "c0a19942af45dc129eac4b5b11020910a16c6ed3",
            "title": "Preface to the fourth IEEE workshop on privacy aspects of data mining",
            "abstract": "Introduction Machine learning and data mining algorithms have penetrated our everyday lives and play a central role in many application domains, including social networks, healthcare, location-based systems, and advertising. At the same time, 90\\% of today's data have been produced in the last two years! These data come from social networking sites, mobile phone applications, electronic medical record systems, ecommerce sites, and open data portals. The analysis of this wealth of data can lead to valuable insights that will benefit data recipients and the society at large, but may also lead to serious privacy breaches, unless privacy-enhancing technologies are in place.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1400458662",
                    "name": "A. Gkoulalas-Divanis"
                },
                {
                    "authorId": "1726972",
                    "name": "Grigorios Loukides"
                },
                {
                    "authorId": "1682418",
                    "name": "C. Aggarwal"
                },
                {
                    "authorId": "144082649",
                    "name": "Raghav Bhaskar"
                },
                {
                    "authorId": "1852261",
                    "name": "Daniel Kifer"
                },
                {
                    "authorId": "1686908",
                    "name": "S. Laxman"
                },
                {
                    "authorId": "7916525",
                    "name": "Xintao Wu"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1824832",
                    "name": "M. Damiani"
                },
                {
                    "authorId": "1393591007",
                    "name": "J. Domingo-Ferrer"
                },
                {
                    "authorId": "1760762",
                    "name": "B. Fung"
                },
                {
                    "authorId": "2152123121",
                    "name": "Wei Jiang"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "2084302753",
                    "name": "Guenter Karjoth"
                },
                {
                    "authorId": "1731655",
                    "name": "Panagiotis Karras"
                },
                {
                    "authorId": "31717144",
                    "name": "B. Malin"
                },
                {
                    "authorId": "144119773",
                    "name": "Kun Liu"
                },
                {
                    "authorId": "2869286",
                    "name": "Mehmet Sayal"
                },
                {
                    "authorId": "144035515",
                    "name": "J. Shao"
                },
                {
                    "authorId": "145976303",
                    "name": "L. Singh"
                },
                {
                    "authorId": "1688897",
                    "name": "Spiros Skiadopoulos"
                },
                {
                    "authorId": "40038267",
                    "name": "A. Slavkovic"
                },
                {
                    "authorId": "39765787",
                    "name": "Adam D. Smith"
                },
                {
                    "authorId": "2454554",
                    "name": "Manolis Terrovitis"
                },
                {
                    "authorId": "144019071",
                    "name": "Philip S. Yu"
                },
                {
                    "authorId": "2133842989",
                    "name": "Ke Wang"
                }
            ]
        },
        {
            "paperId": "c88c4d167da4a2c61668883dfa50552b2ecd60dc",
            "title": "Local correlation clustering",
            "abstract": "Correlation clustering is perhaps the most natural formulation of clustering. Given $n$ objects and a pairwise similarity measure, the goal is to cluster the objects so that, to the best possible extent, similar objects are put in the same cluster and dissimilar objects are put in different clusters. Despite its theoretical appeal, the practical relevance of correlation clustering still remains largely unexplored, mainly due to the fact that correlation clustering requires the $\\Theta(n^2)$ pairwise similarities as input. \nIn this paper we initiate the investigation into \\emph{local} algorithms for correlation clustering. In \\emph{local correlation clustering} we are given the identifier of a single object and we want to return the cluster to which it belongs in some globally consistent near-optimal clustering, using a small number of similarity queries. Local algorithms for correlation clustering open the door to \\emph{sublinear-time} algorithms, which are particularly useful when the similarity between items is costly to compute, as it is often the case in many practical application domains. They also imply $(i)$ distributed and streaming clustering algorithms, $(ii)$ constant-time estimators and testers for cluster edit distance, and $(iii)$ property-preserving parallel reconstruction algorithms for clusterability. \nSpecifically, we devise a local clustering algorithm attaining a $(3, \\varepsilon)$-approximation in time $O(1/\\varepsilon^2)$ independently of the dataset size. An explicit approximate clustering for all objects can be produced in time $O(n/\\varepsilon)$ (which is provably optimal). We also provide a fully additive $(1,\\varepsilon)$-approximation with local query complexity $poly(1/\\varepsilon)$ and time complexity $2^{poly(1/\\varepsilon)}$. The latter yields the fastest polynomial-time approximation scheme for correlation clustering known to date.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1398620943",
                    "name": "David Garc\u00eda-Soriano"
                },
                {
                    "authorId": "1712289",
                    "name": "Konstantin Kutzkov"
                }
            ]
        },
        {
            "paperId": "f5ef58926c2cc0e36b6206623184ac0a3db40ca7",
            "title": "The bang for the buck: fair competitive viral marketing from the host perspective",
            "abstract": "The key algorithmic problem in viral marketing is to identify a set of influential users (called seeds) in a social network, who, when convinced to adopt a product, shall influence other users in the network, leading to a large number of adoptions. When two or more players compete with similar products on the same network we talk about competitive viral marketing, which so far has been studied exclusively from the perspective of one of the competing players. In this paper we propose and study the novel problem of competitive viral marketing from the perspective of the host, i.e., the owner of the social network platform. The host sells viral marketing campaigns as a service to its customers, keeping control of the selection of seeds. Each company specifies its budget and the host allocates the seeds accordingly. From the host's perspective, it is important not only to choose the seeds to maximize the collective expected spread, but also to assign seeds to companies so that it guarantees the \"bang for the buck\" for all companies is nearly identical, which we formalize as the fair seed allocation problem. We propose a new propagation model capturing the competitive nature of viral marketing. Our model is intuitive and retains the desired properties of monotonicity and submodularity. We show that the fair seed allocation problem is NP-hard, and develop an efficient algorithm called Needy Greedy. We run experiments on three real-world social networks, showing that our algorithm is effective and scalable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "16f47e89d87f2e39aacce5f328d67258dc9189fc",
            "title": "Efficient query recommendations in the long tail via center-piece subgraphs",
            "abstract": "We present a recommendation method based on the well-known concept of center-piece subgraph, that allows for the time/space efficient generation of suggestions also for rare, i.e., long-tail queries. Our method is scalable with respect to both the size of datasets from which the model is computed and the heavy workloads that current web search engines have to deal with. Basically, we relate terms contained into queries with highly correlated queries in a query-flow graph. This enables a novel recommendation generation method able to produce recommendations for approximately 99% of the workload of a real-world search engine. The method is based on a graph having term nodes, query nodes, and two kinds of connections: term-query and query-query. The first connects a term to the queries in which it is contained, the second connects two query nodes if the likelihood that a user submits the second query after having issued the first one is sufficiently high. On such large graph we need to compute the center-piece subgraph induced by terms contained into queries. In order to reduce the cost of the above computation, we introduce a novel and efficient method based on an inverted index representation of the model. We experiment our solution on two real-world query logs and we show that its effectiveness is comparable (and in some case better) than state-of-the-art methods for head-queries. More importantly, the quality of the recommendations generated remains very high also for long-tail queries, where other methods fail even to produce any suggestion. Finally, we extensively investigate scalability and efficiency issues and we show the viability of our method in real world search engines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "46188036",
                    "name": "R. Perego"
                },
                {
                    "authorId": "144925193",
                    "name": "F. Silvestri"
                },
                {
                    "authorId": "2507979",
                    "name": "H. Vahabi"
                },
                {
                    "authorId": "1797933",
                    "name": "Rossano Venturini"
                }
            ]
        },
        {
            "paperId": "435add102af7176a8b55748ae4618ed4f1c58eea",
            "title": "Injecting Uncertainty in Graphs for Identity Obfuscation",
            "abstract": "Data collected nowadays by social-networking applications create fascinating opportunities for building novel services, as well as expanding our understanding about social structures and their dynamics. Unfortunately, publishing social-network graphs is considered an ill-advised practice due to privacy concerns. To alleviate this problem, several anonymization methods have been proposed, aiming at reducing the risk of a privacy breach on the published data, while still allowing to analyze them and draw relevant conclusions. \n \nIn this paper we introduce a new anonymization approach that is based on injecting uncertainty in social graphs and publishing the resulting uncertain graphs. While existing approaches obfuscate graph data by adding or removing edges entirely, we propose using a finer-grained perturbation that adds or removes edges partially: this way we can achieve the same desired level of obfuscation with smaller changes in the data, thus maintaining higher utility. Our experiments on real-world networks confirm that at the same level of identity obfuscation our method provides higher usefulness than existing randomized methods that publish standard graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1733343",
                    "name": "P. Boldi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1687574",
                    "name": "Tamir Tassa"
                }
            ]
        },
        {
            "paperId": "8966b3dd80f1eff973560a470160218b88da9d27",
            "title": "Top-10 Data Mining Case Studies",
            "abstract": "We report on the panel discussion held at the ICDM'10 conference on the top 10 data mining case studies in order to provide a snapshot of where and how data mining techniques have made significant real-world impact. The tasks covered by 10 case studies range from the detection of anomalies such as cancer, fraud, and system failures to the optimization of organizational operations, and include the automated extraction of information from unstructured sources. From the 10 cases we find that supervised methods prevail while unsupervised techniques play a supporting role. Further, significant domain knowledge is generally required to achieve a completed solution. Finally, we find that successful applications are more commonly associated with continual improvement rather than by single \"aha moments\" of knowledge (\"nugget\") discovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50641517",
                    "name": "Gabor Melli"
                },
                {
                    "authorId": "1748808",
                    "name": "Xindong Wu"
                },
                {
                    "authorId": "3026191",
                    "name": "P. Beinat"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2148761004",
                    "name": "Longbing Cao"
                },
                {
                    "authorId": "144487977",
                    "name": "Rong Duan"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "1791498",
                    "name": "R. Ghani"
                },
                {
                    "authorId": "2270765",
                    "name": "B. Kitts"
                },
                {
                    "authorId": "1722864",
                    "name": "Bart Goethals"
                },
                {
                    "authorId": "1690163",
                    "name": "G. McLachlan"
                },
                {
                    "authorId": "145525190",
                    "name": "J. Pei"
                },
                {
                    "authorId": "2192265427",
                    "name": "Ashok Srivastava"
                },
                {
                    "authorId": "1683380",
                    "name": "Osmar R Zaiane"
                }
            ]
        },
        {
            "paperId": "a7d69b494b4021c9af9a95c242959b239907f328",
            "title": "Chromatic correlation clustering",
            "abstract": "We study a novel clustering problem in which the pairwise relations between objects are categorical. This problem can be viewed as clustering the vertices of a graph whose edges are of different types (colors). We introduce an objective function that aims at partitioning the graph such that the edges within each cluster have, as much as possible, the same color. We show that the problem is NP-hard and propose a randomized algorithm with approximation guarantee proportional to the maximum degree of the input graph. The algorithm iteratively picks a random edge as pivot, builds a cluster around it, and removes the cluster from the graph. Although being fast, easy-to-implement, and parameter free, this algorithm tends to produce a relatively large number of clusters. To overcome this issue we introduce a variant algorithm, which modifies how the pivot is chosen and and how the cluster is built around the pivot. Finally, to address the case where a fixed number of output clusters is required, we devise a third algorithm that directly optimizes the objective function via a strategy based on the alternating minimization paradigm.\n We test our algorithms on synthetic and real data from the domains of protein-interaction networks, social media, and bibliometrics. Experimental evidence show that our algorithms outperform a baseline algorithm both in the task of reconstructing a ground-truth clustering and in terms of objective function value.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1794262",
                    "name": "Francesco Gullo"
                },
                {
                    "authorId": "2023899",
                    "name": "Charalampos E. Tsourakakis"
                },
                {
                    "authorId": "1693307",
                    "name": "Antti Ukkonen"
                }
            ]
        },
        {
            "paperId": "cff08c0bf8e68a7233046ff4ea209c63718bd085",
            "title": "The early-adopter graph and its application to web-page recommendation",
            "abstract": "In this paper we present a novel graph-based data abstraction for modeling the browsing behavior of web users. The objective is to identify users who discover interesting pages before others. We call these users early adopters. By tracking the browsing activity of early adopters we can identify new interesting pages early, and recommend these pages to similar users. We focus on news and blog pages, which are more dynamic in nature and more appropriate for recommendation. Our proposed model is called early-adopter graph. In this graph, nodes represent users and a directed arc between users u and v expresses the fact that u and v visit similar pages and, in particular, that user u tends to visit those pages before user v. The weight of the edge is the degree to which the temporal rule \"v visits a page before v\" holds. Based on the early-adopter graph, we build a recommendation system for news and blog pages, which outperforms other out-of-the-shelf recommendation systems based on collaborative filtering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2577635",
                    "name": "I. Mele"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                }
            ]
        },
        {
            "paperId": "e04533afca00b069085873fd7bf6b9a78426eed1",
            "title": "Interactive and context-aware tag spell check and correction",
            "abstract": "Collaborative content creation and annotation creates vast repositories of all sorts of media, and user-defined tags play a central role as they are a simple yet powerful tool for organizing, searching and exploring the available resources. We observe that when a user annotates a resource with a set of tags, those tags are introduced one at a time. Therefore, when the fourth tag is introduced, a knowledge represented by the previous three tags, i.e., the context in which the fourth tag is produced, is available and exploitable for generating potential correction of the current tag. This context, together with the \"wisdom of the crowd\" represented by the co-occurrences of tags in all the resources of the repository, can be exploited to provide interactive tag spell check and correction. We develop this idea in a framework, based on a weighted tag co-occurrence graph and on nodes relatedness measures defined on weighted neighborhoods. We test our proposal on a dataset coming from YouTube. The results show that our framework is effective as it outperforms two important baselines. We also show that it is efficient, thus enabling its use in modern tagging services.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1741208",
                    "name": "O. Frieder"
                },
                {
                    "authorId": "2763561",
                    "name": "F. M. Nardini"
                },
                {
                    "authorId": "144925193",
                    "name": "F. Silvestri"
                },
                {
                    "authorId": "2507979",
                    "name": "H. Vahabi"
                }
            ]
        },
        {
            "paperId": "1191d6a025e62eda79a9b52a682d538194477e4c",
            "title": "Diverse Dimension Decomposition of an Itemset Space",
            "abstract": "We introduce the problem of diverse dimension decomposition in transactional databases. A dimension is a set of mutually-exclusive item sets, and our problem is to find a decomposition of the item set space into dimensions, which are orthogonal to each other, and that provide high coverage of the input database. The mining framework we propose effectively represents a dimensionality-reducing transformation from the space of all items to the space of orthogonal dimensions. Our approach relies on information-theoretic concepts, and we are able to formulate the dimension-finding problem with a single objective function that simultaneously captures constraints on coverage, exclusivity and orthogonality. We describe an efficient greedy method for finding diverse dimensions from transactional databases. The experimental evaluation of the proposed approach using two real datasets, flickr and delicious, demonstrates the effectiveness of our solution. Although we are motivated by the applications in the collaborative tagging domain, we believe that the mining task we introduce in this paper is general enough to be useful in other application domains.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "26a28bbb0118acb629d9c1e68c5d87f1aee990e9",
            "title": "Influence Propagation in Social Networks: A Data Mining Perspective",
            "abstract": "With the success of online social networks and mi- croblogs such as Facebook, Flickr and Twitter, the phenomenon of influence exerted by users of such platforms on other users, and how it propagates in the network, has recently attracted the interest of computer scientists, information technologists, and marketing specialists. One of the key problems in this area is the identification of influential users, by targeting whom certain desirable marketing outcomes can be achieved. In this article we take a data mining perspective and we discuss what (and how) can be learned from the available traces of past propagations. While doing this we provide a brief overview of some recent progresses in this area and discuss some open problems. By no means this article must be intended as an exhaustive survey: it is instead (admittedly) a rather biased and personal perspective of the author on the topic of influence propagation in social networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "36c0b81a2ef2505e5c1c763c1abc25cdd72903f2",
            "title": "A Data-Based Approach to Social Influence Maximization",
            "abstract": "Influence maximization is the problem of finding a set of users in a social network, such that by targeting this set, one maximizes the expected spread of influence in the network. Most of the literature on this topic has focused exclusively on the social graph, overlooking historical data, i.e., traces of past action propagations. In this paper, we study influence maximization from a novel data-based perspective. In particular, we introduce a new model, which we call credit distribution, that directly leverages available propagation traces to learn how influence flows in the network and uses this to estimate expected influence spread. Our approach also learns the different levels of influence-ability of users, and it is time-aware in the sense that it takes the temporal nature of influence into account. \n \nWe show that influence maximization under the credit distribution model is NP-hard and that the function that defines expected spread under our model is submodular. Based on these, we develop an approximation algorithm for solving the influence maximization problem that at once enjoys high accuracy compared to the standard approach, while being several orders of magnitude faster and more scalable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "3adee022c70fde0301f3ee6a3b89f46fda6991d4",
            "title": "Recommendations for the long tail by term-query graph",
            "abstract": "We define a new approach to the query recommendation problem. In particular, our main goal is to design a model enabling the generation of query suggestions also for rare and previously unseen queries. In other words we are targeting queries in the long tail. The model is based on a graph having two sets of nodes: Term nodes, and Query nodes. The graph induces a Markov chain on which a generic random walker starts from a subset of Term nodes, moves along Query nodes, and restarts (with a given probability) only from the same initial subset of Term nodes. Computing the stationary distribution of such a Markov chain is equivalent to extracting the so-called Center-piece Subgraph from the graph associated with the Markov chain itself. Given a query, we extract its terms and we set the restart subset to this term set. Therefore, we do not require a query to have been previously observed for the recommending model to be able to generate suggestions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "46188036",
                    "name": "R. Perego"
                },
                {
                    "authorId": "144925193",
                    "name": "F. Silvestri"
                },
                {
                    "authorId": "2507979",
                    "name": "H. Vahabi"
                },
                {
                    "authorId": "1797933",
                    "name": "Rossano Venturini"
                }
            ]
        },
        {
            "paperId": "591a930fbe9ce42156b057b72bf8e6d18a3148bf",
            "title": "Characterizing Uncertain Data using Compression",
            "abstract": "Motivated by sensor networks, mobility data, biology and life sciences, the area of mining uncertain data has recently received a great deal of attention. While various papers have focused on efficiently mining frequent patterns from uncertain data, the problem of discovering a small set of interesting patterns that provide an accurate and condensed description of a probabilistic database is still unexplored. In this paper we study the problem of discovering characteristic patterns in uncertain data through information theoretic lenses. Adopting the possible worlds interpretation of probabilistic data and a compression scheme based on the MDL principle, we formalize the problem of mining patterns that compress the database well in expectation. Despite its huge search space, we show that this problem can be accurately approximated. In particular, we devise a sequence of three methods where each new method improves the memory requirements orders of magnitudes compared to its predecessor, while giving up only a little in terms of approximation accuracy. We empirically compare our methods on both synthetic data and real data from life science. Results show that from a probabilistic matrix with more than one million rows and columns, we can extract a small set of meaningful patterns that accurately characterize the data distribution of any probable world.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "144453519",
                    "name": "M. Leeuwen"
                },
                {
                    "authorId": "1693307",
                    "name": "Antti Ukkonen"
                }
            ]
        },
        {
            "paperId": "5b6300814123be1a413cf9b7103448771e598d5a",
            "title": "Viscous democracy for social networks",
            "abstract": "Decision-making procedures in online social networks should reflect participants' political influence within the network.",
            "fieldsOfStudy": [
                "Sociology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1733343",
                    "name": "P. Boldi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "1737624",
                    "name": "S. Vigna"
                }
            ]
        },
        {
            "paperId": "a66a983e5af2242a6ec5041c1b07f3fc1a6c9dd5",
            "title": "Trajectory anonymity in publishing personal mobility data",
            "abstract": "Recent years have witnessed pervasive use of location-aware devices such as GSM mobile phones, GPS-enabled PDAs, location sensors, and active RFID tags. The use of these devices generates a huge collection of spatio-temporal data, variously called moving object data, trajectory data, or moblity data. These data can be used for various data analysis purposes such as city traffic control, mobility management, urban planning, and location-based service advertisements. Clearly, the spatio-temporal data so collected may help an attacker to discover personal and sensitive information like user habits, social customs, religious and sexual preferences of individuals. Consequently, it raises serious concerns about privacy. Simply replacing users' real identifiers (name, SSN, etc.) with pseudonyms is insufficient to guarantee anonymity. The problem is that due to the existence of quasi-identifiers, i.e., spatio-temporal data points that can be linked to external information to re-identify individuals, the attacker may be able to trace the anonymous spatio-temporal data back to individuals.\n In this survey, we discuss recent advancement on anonymity preserving data publishing of moving object databases in an off-line fashion. We first introduce several anonymity models, then we describe in detail some of the proposed techniques to enforce trajectory anonymity, discussing their merits and limitations. We conclude by identifying challenging open problems that need attention.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2108465033",
                    "name": "Wendy Hui Wang"
                }
            ]
        },
        {
            "paperId": "b32ca7a3b395ef89ff01ca6929462d8c0ab66185",
            "title": "Social Network Analysis and Mining for Business Applications",
            "abstract": "Social network analysis has gained significant attention in recent years, largely due to the success of online social networking and media-sharing sites, and the consequent availability of a wealth of social network data. In spite of the growing interest, however, there is little understanding of the potential business applications of mining social networks. While there is a large body of research on different problems and methods for social network mining, there is a gap between the techniques developed by the research community and their deployment in real-world applications. Therefore the potential business impact of these techniques is still largely unexplored.\n In this article we use a business process classification framework to put the research topics in a business context and provide an overview of what we consider key problems and techniques in social network analysis and mining from the perspective of business applications. In particular, we discuss data acquisition and preparation, trust, expertise, community structure, network dynamics, and information propagation. In each case we present a brief overview of the problem, describe state-of-the art approaches, discuss business application examples, and map each of the topics to a business process classification framework. In addition, we provide insights on prospective business applications, challenges, and future research directions. The main contribution of this article is to provide a state-of-the-art overview of current techniques while providing a critical perspective on business applications of social network analysis and mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "144633617",
                    "name": "A. Jaimes"
                }
            ]
        },
        {
            "paperId": "cc2d7de07d57a489d3347dfd40eba502fb8d54e2",
            "title": "Sparsification of influence networks",
            "abstract": "We present Spine, an efficient algorithm for finding the \"backbone\" of an influence network. Given a social graph and a log of past propagations, we build an instance of the independent-cascade model that describes the propagations. We aim at reducing the complexity of that model, while preserving most of its accuracy in describing the data.\n We show that the problem is inapproximable and we present an optimal, dynamic-programming algorithm, whose search space, albeit exponential, is typically much smaller than that of the brute force, exhaustive-search approach. Seeking a practical, scalable approach to sparsification, we devise Spine, a greedy, efficient algorithm with practically little compromise in quality.\n We claim that sparsification is a fundamental data-reduction operation with many applications, ranging from visualization to exploratory and descriptive data analysis. As a proof of concept, we use Spine on real-world datasets, revealing the backbone of their influence-propagation networks. Moreover, we apply Spine as a pre-processing step for the influence-maximization problem, showing that computations on sparsified models give up little accuracy, but yield significant improvements in terms of scalability.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49105984",
                    "name": "M. Mathioudakis"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1693307",
                    "name": "Antti Ukkonen"
                }
            ]
        },
        {
            "paperId": "dd3c85737bbe4676aa33a67260b698725c8a0488",
            "title": "Fast Matrix Computations for Pairwise and Columnwise Commute Times and Katz Scores",
            "abstract": "Abstract We explore methods for approximating the commute time and Katz score between a pair of nodes. These methods are based on the approach of matrices, moments, and quadrature developed in the numerical linear algebra community. They rely on the Lanczos process and provide upper and lower bounds on an estimate of the pairwise scores. We also explore methods to approximate the commute times and Katz scores from a node to all other nodes in the graph. Here, our approach for the commute times is based on a variation of the conjugate gradient algorithm, and it provides an estimate of all the diagonals of the inverse of a matrix. Our technique for the Katz scores is based on exploiting an empirical localization property of the Katz matrix. We adapt algorithms used for personalized PageRank computing to these Katz scores and theoretically show that this approach is convergent. We evaluate these methods on 17 real-world graphs ranging in size from 1000 to 1,000,000 nodes. Our results show that our pairwise commute-time method and columnwise Katz algorithm both have attractive theoretical properties and empirical performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2467965",
                    "name": "Pooya Esfandiar"
                },
                {
                    "authorId": "1757913",
                    "name": "D. Gleich"
                },
                {
                    "authorId": "1805855",
                    "name": "C. Greif"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "f13540a4016bb706a4e6b0fb2de4d719f7d08e1b",
            "title": "Suggesting ghost edges for a smaller world",
            "abstract": "Small changes in the network topology can have dramatic effects on its capacity to disseminate information. In this paper, we consider the problem of adding a small number of ghost edges in the network in order to minimize the average shortest-path distance between nodes, towards a smaller-world network. We formalize the problem of suggesting ghost edges and we propose a novel method for quickly evaluating the importance of ghost edges in sparse graphs. Through experiments on real and synthetic data sets, we demonstrate that our approach performs very well, for a varying range of conditions, and it outperforms sensible baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1695845",
                    "name": "M. Papagelis"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                }
            ]
        },
        {
            "paperId": "21968ae000669eb4cf03718a0d97e23a6bf75926",
            "title": "Learning influence probabilities in social networks",
            "abstract": "Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "3547fb89f1f8dddb6d192ce1bb3eace5338f2d1e",
            "title": "Hiding Sequential and Spatiotemporal Patterns",
            "abstract": "The process of discovering relevant patterns holding in a database was first indicated as a threat to database security by O'Leary in. Since then, many different approaches for knowledge hiding have emerged over the years, mainly in the context of association rules and frequent item sets mining. Following many real-world data and application demands, in this paper, we shift the problem of knowledge hiding to contexts where both the data and the extracted knowledge have a sequential structure. We define the problem of hiding sequential patterns and show its NP-hardness. Thus, we devise heuristics and a polynomial sanitization algorithm. Starting from this framework, we specialize it to the more complex case of spatiotemporal patterns extracted from moving objects databases. Finally, we discuss a possible kind of attack to our model, which exploits the knowledge of the underlying road network, and enhance our model to protect from this kind of attack. An exhaustive experiential analysis on real-world data sets shows the effectiveness of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187601",
                    "name": "Osman Abul"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                }
            ]
        },
        {
            "paperId": "3ac9f79ef0790ee6257e66789acac03243480aa8",
            "title": "Do you want to take notes?: identifying research missions in Yahoo! search pad",
            "abstract": "Addressing user's information needs has been one of the main goals of Web search engines since their early days. In some cases, users cannot see their needs immediately answered by search results, simply because these needs are too complex and involve multiple aspects that are not covered by a single Web or search results page. This typically happens when users investigate a certain topic in domains such as education, travel or health, which often require collecting facts and information from many pages. We refer to this type of activities as \"research missions\". These research missions account for 10% of users' sessions and more than 25% of all query volume, as verified by a manual analysis that was conducted by Yahoo! editors.\n We demonstrate in this paper that such missions can be automatically identified on-the-fly, as the user interacts with the search engine, through careful runtime analysis of query flows and query sessions.\n The on-the-fly automatic identification of research missions has been implemented in Search Pad, a novel Yahoo! application that was launched in 2009, and that we present in this paper. Search Pad helps users keeping trace of results they have consulted. Its novelty however is that unlike previous notes taking products, it is automatically triggered only when the system decides, with a fair level of confidence, that the user is undertaking a research mission and thus is in the right context for gathering notes. Beyond the Search Pad specific application, we believe that changing the level of granularity of query modeling, from an isolated query to a list of queries pertaining to the same research missions, so as to better reflect a certain type of information needs, can be beneficial in a number of other Web search applications. Session-awareness is growing and it is likely to play, in the near future, a fundamental role in many on-line tasks: this paper presents a first step on this path.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145985025",
                    "name": "D. Donato"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "36851270",
                    "name": "Tom Chi"
                },
                {
                    "authorId": "1781257",
                    "name": "Y. Maarek"
                }
            ]
        },
        {
            "paperId": "3ad900778ba49b61e3d3b7c09ee374b84f5df17e",
            "title": "Privacy-Aware Knowledge Discovery: Novel Applications and New Techniques",
            "abstract": "Covering research at the frontier of this field, Privacy-Aware Knowledge Discovery: Novel Applications and New Techniques presents state-of-the-art privacy-preserving data mining techniques for application domains, such as medicine and social networks, that face the increasing heterogeneity and complexity of new forms of data. Renowned authorities from prominent organizations not only cover well-established resultsthey also explore complex domains where privacy issues are generally clear and well defined, but the solutions are still preliminary and in continuous development. Divided into seven parts, the book provides in-depth coverage of the most novel reference scenarios for privacy-preserving techniques. The first part gives general techniques that can be applied to various applications discussed in the rest of the book. The second section focuses on the sanitization of network traces and privacy in data stream mining. After the third part on privacy in spatio-temporal data mining and mobility data analysis, the book examines time series analysis in the fourth section, explaining how a perturbation method and a segment-based method can tackle privacy issues of time series data. The fifth section on biomedical data addresses genomic data as well as the problem of privacy-aware information sharing of health data. In the sixth section on web applications, the book deals with query log mining and web recommender systems. The final part on social networks analyzes privacy issues related to the management of social network data under different perspectives. While several new results have recently occurred in the privacy, database, and data mining research communities, a uniform presentation of up-to-date techniques and applications is lacking. Filling this void, Privacy-Aware Knowledge Discovery presents novel algorithms, patterns, and models, along with a significant collection of open problems for future investigation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "88162783",
                    "name": "E. Ferrari"
                }
            ]
        },
        {
            "paperId": "3be0580c083894327ebd45cc5c00fd356d390c30",
            "title": "On the high density of leadership nuclei in endorsement social networks",
            "abstract": "In this paper we study the community structure of <i>endorsement networks</i>, i.e., social networks in which a directed edge <i>u</i> \u2192 <i>v</i> is asserting an action of support from user <i>u</i> to user <i>v</i>. Examples include scenarios in which a user <i>u</i> is <i>favoring</i> a photo, <i>liking</i> a post, or <i>following</i> the microblog of user <i>v</i>.\n Starting from the hypothesis that the footprint of a community in an endorsement network is a bipartite directed clique from a set of followers to a set of leaders, we apply frequent itemset mining techniques to discover such bicliques. Our analysis of real networks discovers that an interesting phenomenon is taking place: the leaders of a community are endorsing each other forming a <i>very dense nucleus</i>.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143944406",
                    "name": "Guillermo Garrido"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                }
            ]
        },
        {
            "paperId": "4524672008dfac045b21758b1be9b5defacf8104",
            "title": "Cold start link prediction",
            "abstract": "In the traditional link prediction problem, a snapshot of a social network is used as a starting point to predict, by means of graph-theoretic measures, the links that are likely to appear in the future. In this paper, we introduce cold start link prediction as the problem of predicting the structure of a social network when the network itself is totally missing while some other information regarding the nodes is available. We propose a two-phase method based on the bootstrap probabilistic graph. The first phase generates an implicit social network under the form of a probabilistic graph. The second phase applies probabilistic graph-based measures to produce the final prediction. We assess our method empirically over a large data collection obtained from Flickr, using interest groups as the initial information. The experiments confirm the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1776940",
                    "name": "B. B. Cambazoglu"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "47ebc9a37ed91f2c047e74f247f5fa1c6cabf900",
            "title": "Learning and Predicting the Evolution of Social Networks",
            "abstract": "With the increasing availability of large social network data, there is also an increasing interest in analyzing how those networks evolve over time. Traditionally, the analysis of social networks has focused only on a single snapshot of a network. Researchers have already verified that social networks follow power-law degree distribution, have a small diameter, and exhibit small-world structure and community structure. Attempts to explain the properties of social networks have led to dynamic models inspired by the preferential attachment models which assumes that new network nodes have a higher probability of forming links with high-degree nodes, creating a rich-get-richer effect. Although some effort has been devoted to analyzing global properties of social network evolution, not much has been done to study graph evolution at a microscopic level. A first step in this direction investigated a variety of network formation strategies, showing that edge locality plays a critical role in network evolution. We propose a different approach. Following the paradigm of association rules and frequent-pattern mining, our work searches for typical patterns of structural changes in dynamic networks. Mining for such local patterns is a computationally challenging task that can provide further insight into the increasing amount of evolving network data. Beyond the notion of graph evolution rules (GERs), a concept that we introduced in an earlier work, we developed the Graph Evolution Rule Miner (GERM) software to extract such rules and applied these rules to predict future network evolution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710403",
                    "name": "Bj\u00f6rn Bringmann"
                },
                {
                    "authorId": "1749714",
                    "name": "M. Berlingerio"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                }
            ]
        },
        {
            "paperId": "74f6f46705994a6c8f118b9562201e8f0d24c2f4",
            "title": "k-nearest neighbors in uncertain graphs",
            "abstract": "Complex networks, such as biological, social, and communication networks, often entail uncertainty, and thus, can be modeled as probabilistic graphs. Similar to the problem of similarity search in standard graphs, a fundamental problem for probabilistic graphs is to efficiently answer k-nearest neighbor queries (k-NN), which is the problem of computing the k closest nodes to some specific node. In this paper we introduce a framework for processing k-NN queries in probabilistic graphs. We propose novel distance functions that extend well-known graph concepts, such as shortest paths. In order to compute them in probabilistic graphs, we design algorithms based on sampling. During k-NN query processing we efficiently prune the search space using novel techniques. Our experiments indicate that our distance functions outperform previously used alternatives in identifying true neighbors in real-world biological data. We also demonstrate that our algorithms scale for graphs with tens of millions of edges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2873279",
                    "name": "Michalis Potamias"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1715771",
                    "name": "G. Kollios"
                }
            ]
        },
        {
            "paperId": "8eb83dbaa17c9af917d1242f6acbcbf99d9fdcaf",
            "title": "The Meme Ranking Problem: Maximizing Microblogging Virality",
            "abstract": "Microblogging is a communication paradigm in which users post bits of information (brief text updates or micro media such as photos, video or audio clips) that are visible by their communities. When a user finds a \u201cmeme\u201d of another user interesting, she can eventually repost it, thus allowing memes to propagate virally trough a social network. In this paper we introduce the meme ranking problem, as the problem of selecting which k memes (among the ones posted their contacts) to show to users when they log into the system. The objective is to maximize the overall activity of the network, that is, the total number of reposts that occur. We deeply characterize the problem showing that not only exact solutions are unfeasible, but also approximated solutions are prohibitive to be adopted in an on-line setting. Therefore we devise a set of heuristics and we compare them trough an extensive simulation based on the real-world Yahoo! Meme social graph, and with parameters learnt from real logs of meme propagations. Our experimentation demonstrates the effectiveness and feasibility of these methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1789397",
                    "name": "D. Ienco"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                }
            ]
        },
        {
            "paperId": "af5a50e56c4b30a70cf6ee71cad1e98d789200a4",
            "title": "Approximation Analysis of Influence Spread in Social Networks",
            "abstract": "In the context of influence propagation in a social graph, we can identify three orthogonal dimensions - the number of seed nodes activated at the beginning (known as budget), the expected number of activated nodes at the end of the propagation (known as expected spread or coverage), and the time taken for the propagation. We can constrain one or two of these and try to optimize the third. In their seminal paper (12), Kempe, Kleinberg and Tardos (KKT) constrained the budget, left time unconstrained, and maximized the coverage. In this paper, we study alternative optimization problems: MINSEED and MINTIME. In MINSEED, a coverage thresholdis given and the task is to find the minimum size seed set such that by activating it, at leastnodes are eventually activated in the expected sense. In MINTIME, a coverage thresholdand a budget threshold k are given, and the task is to find a seed set of size at most k such that by activating it, at leastnodes are activated in the expected sense, in the minimum possible time. It turns out both these problems are NP-hard. Given the hardness of the problems, we naturally turn to the subject of approximation algo- rithms. For MINSEED, we develop a bicriteria approximation by exploiting its relationship to the problem of Real-valued Submodular Set Cover (RSSC). We prove a generic inapproximabil- ity result for RSSC suggesting that improving this approximation factor is likely to be hard. For MINTIME we show that even bicriteria and tricriteria approximations are hard under several conditions. Our proof exploits the relationship between MINTIME and the problem of Robust Asymmetric k-center (RAKC). We show, however, that if we allow the budget for number of seeds k to be boosted by a logarithmic factor and allow the coverage to fall short, then the problem can be solved exactly in PTIME, i.e., we can achieve the required coverage within the time achieved by the optimal solution to MINTIME with budget k and coverage threshold \ufffd. Finally, we show the value of the approximation algorithms, by conducting an experimental comparison of their quality against that achieved by various heuristics.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "161675ae07abfbd2ca63b5d03258dfaed11ed82f",
            "title": "Anonymizing moving objects: how to hide a MOB in a crowd?",
            "abstract": "Moving object databases (MOD) have gained much interest in recent years due to the advances in mobile communications and positioning technologies. Study of MOD can reveal useful information (e.g., traffic patterns and congestion trends) that can be used in applications for the common benefit. In order to mine and/or analyze the data, MOD must be published, which can pose a threat to the location privacy of a user. Indeed, based on prior knowledge of a user's location at several time points, an attacker can potentially associate that user to a specific moving object (MOB) in the published database and learn her position information at other time points.\n In this paper, we study the problem of privacy-preserving publishing of moving object database. Unlike in microdata, we argue that in MOD, there does not exist a fixed set of quasi-identifier (QID) attributes for all the MOBs. Consequently the anonymization groups of MOBs (i.e., the sets of other MOBs within which to hide) may not be disjoint. Thus, there may exist MOBs that can be identified explicitly by combining different anonymization groups. We illustrate the pitfalls of simple adaptations of classical k-anonymity and develop a notion which we prove is robust against privacy attacks. We propose two approaches, namely extreme-union and symmetric anonymization, to build anonymization groups that provably satisfy our proposed k-anonymity requirement, as well as yield low information loss. We ran an extensive set of experiments on large real-world and synthetic datasets of vehicular traffic. Our results demonstrate the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2106849",
                    "name": "Roman Yarovoy"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2108465033",
                    "name": "Wendy Hui Wang"
                }
            ]
        },
        {
            "paperId": "3e5b3f9d0957b6cad3fcbd8e4ae4fdede457dc4e",
            "title": "Query suggestions using query-flow graphs",
            "abstract": "The query-flow graph [Boldi et al., CIKM 2008] is an aggregated representation of the latent querying behavior contained in a query log. Intuitively, in the query-flow graph a directed edge from query <i>q</i><sub><i>i</i></sub> to query <i>q</i><sub><i>j</i></sub> means that the two queries are likely to be part of the same search mission. Any path over the query-flow graph may be seen as a possible search task, whose likelihood is given by the strength of the edges along the path. An edge (<i>q</i><sub><i>i</i></sub>, <i>q<sub>j</sub></i>) is also labelled with some information: e.g., the probability that user moves from <i>q</i><sub><i>i</i></sub> to <i>q</i><sub><i>j</i></sub>, or the type of the transition, for instance, the fact that <i>q</i><sub><i>j</i></sub> is a specialization of <i>q</i><sub><i>i</i></sub>.\n In this paper we propose, and experimentally study, query recommendations based on short random walks on the query-flow graph. Our experiments show that these methods can match in precision, and often improve, recommendations based on query-click graphs, without using users' clicks. Our experiments also show that it is important to consider transition-type labels on edges for having good quality recommendations.\n Finally, one feature that we had in mind while devising our methods was that of providing diverse sets of recommendations: the experimentation that we conducted provides encouraging results in this sense.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1733343",
                    "name": "P. Boldi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "145985025",
                    "name": "D. Donato"
                },
                {
                    "authorId": "1737624",
                    "name": "S. Vigna"
                }
            ]
        },
        {
            "paperId": "4bcca348c358a5aa382715e693aa9304158d881b",
            "title": "Fast shortest path distance estimation in large networks",
            "abstract": "In this paper we study approximate landmark-based methods for point-to-point distance estimation in very large networks. These methods involve selecting a subset of nodes as landmarks and computing offline the distances from each node in the graph to those landmarks. At runtime, when the distance between a pair of nodes is needed, it can be estimated quickly by combining the precomputed distances. We prove that selecting the optimal set of landmarks is an NP-hard problem, and thus heuristic solutions need to be employed. We therefore explore theoretical insights to devise a variety of simple methods that scale well in very large networks. The efficiency of the suggested techniques is tested experimentally using five real-world graphs having millions of edges. While theoretical bounds support the claim that random landmarks work well in practice, our extensive experimentation shows that smart landmark selection can yield dramatically more accurate results: for a given target accuracy, our methods require as much as 250 times less space than selecting landmarks at random. In addition, we demonstrate that at a very small accuracy loss our techniques are several orders of magnitude faster than the state-of-the-art exact methods. Finally, we study an application of our methods to the task of social search in large graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2873279",
                    "name": "Michalis Potamias"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                }
            ]
        },
        {
            "paperId": "556167b08b9bc1e5df1b6a4e8ac7fbe41542bf3c",
            "title": "Optimising topical query decomposition",
            "abstract": "Topical query decomposition (TQD) is a paradigm recently introduced in [1], which, given a query, returns to the user a set of queries that cover the answer set of the original query. The TQD problem was studied as a variant of the set-cover problem and solved by means of a greedy algorithm.\n This paper aims to strengthen the original formulation by introducing a new global objective function, and thus formalising the problem as a combinatorial optimisation one. Such a reformulation defines a common framework allowing a formal evaluation and comparison of different approaches to TQD. We apply simulated annealing, a sub-optimal meta-heuristic, to the problem of topical query decomposition and we show, through a large experimentation on a data sample extracted from an actual query log, that such meta-heuristic achieves better results than the greedy algorithm.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3329147",
                    "name": "M. Sydow"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "145985025",
                    "name": "D. Donato"
                }
            ]
        },
        {
            "paperId": "5ba1e23821a03921d57a7d605ce0a1a946a777d5",
            "title": "From \"Dango\" to \"Japanese Cakes\": Query Reformulation Models and Patterns",
            "abstract": "Understanding query reformulation patterns is a key step towards next generation web search engines: it can help improving users' web-search experience by predicting their intent, and thus helping them to locate information more effectively. As a step in this direction, we build an accurate model for classifying user query reformulations into broad classes (generalization, specialization, error correction or parallel move), achieving 92\\% accuracy. We apply the model to automatically label two large query logs, creating annotated query-flow graphs. We study the resulting reformulation patterns, finding results consistent with previous studies done on smaller manually annotated datasets, and discovering new interesting patterns, including connections between reformulation types and topical categories. Finally, applying our findings to a third query log that is publicly available for research purposes, we demonstrate that our reformulation classifier leads to improved recommendations in a query recommendation system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1733343",
                    "name": "P. Boldi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "1737624",
                    "name": "S. Vigna"
                }
            ]
        },
        {
            "paperId": "aa2c714ef15e69d1e21da8b04231915a7f78d9b0",
            "title": "Compressing tags to find interesting media groups",
            "abstract": "On photo sharing websites like Flickr and Zooomr, users are offered the possibility to assign tags to their uploaded pictures. Using these tags to find interesting groups of semantically related pictures in the result set of a given query is a problem with obvious applications. We analyse this problem from a Minimum Description Length (MDL) perspective and develop an algorithm that finds the most interesting groups. The method is based on Krimp, which finds small sets of patterns that characterise the data using compression. These patterns are sets of tags, often assignedtogether to photos. The better a database compresses, the more structure it contains and thus the more homogeneous it is. Following this observation we devise a compression-based measure. Our experiments on Flickr data show that the most interesting and homogeneous groups are found. We show extensive examples and compare to clusterings on the Flickr website.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144453519",
                    "name": "M. Leeuwen"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1780952",
                    "name": "B\u00f6rkur Sigurbj\u00f6rnsson"
                },
                {
                    "authorId": "1731332",
                    "name": "A. Siebes"
                }
            ]
        },
        {
            "paperId": "aaeb154342b3924ff8520fe4b2cf6785ad0395b2",
            "title": "Constraint-Based Pattern Discovery",
            "abstract": "Devising fast and scalable algorithms, able to crunch huge amount of data, was for many years one of the main goals of data mining research. But then we realized that this was not enough. It does not matter how efficient such algorithms can be, the results we obtain are often of limited use in practice. Typically, the knowledge we seek is in a small pool of local patterns hidden within an ocean of irrelevant patterns generated from a sea of data. Therefore, it is the volume of the results itself that creates a second order mining problem for the human expert. This is, typically, the case of association rules and frequent itemset mining (Agrawal & Srikant, 1994), to which, during the last decade a lot of researchers have dedicated their (mainly algorithmic) investigations. The computational problem is that of efficiently mining from a database of transactions, those itemsets which satisfy a user-defined constraint of minimum frequency. Recently the research community has turned its attention to more complex kinds of frequent patterns extracted from more structured data: sequences, trees, and graphs. All these different kinds of pattern have different peculiarities and application fields, but they all share the same computational aspects: a usually very large input, an exponential search space, and a too large solution set. This situation\u2014too many data yielding too many patterns\u2014is harmful for two reasons. First, performance degrades: mining generally becomes inefficient or, often, simply unfeasible. Second, the identification of the fragments of interesting knowledge, blurred within a huge quantity of mostly useless patterns, is difficult. The paradigm of constraintbased pattern mining was introduced as a solution to both these problems. In such paradigm, it is the user who specifies to the system what is interesting for the current application: constraints are a tool to drive the mining process towards potentially interesting patterns, moreover they can be pushed deep inside the mining algorithm in order to fight the exponential search space curse, and to achieve better performance (Srikant et al., 1997; Ng et al. 1998; Han et al., 1999; Grahne et al., 2000).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "b2db5ed9b073e4d9393e456a24ec4ee6ac29d13f",
            "title": "Voting in social networks",
            "abstract": "A voting system is a set of rules that a community adopts to take collective decisions. In this paper we study voting systems for a particular kind of community: electronically mediated social networks. In particular, we focus on delegative democracy (a.k.a. proxy voting) that has recently received increased interest for its ability to combine the benefits of direct and representative systems, and that seems also perfectly suited for electronically mediated social networks. In such a context, we consider a voting system in which users can only express their preference for one among the people they are explicitly connected with, and this preference can be propagated transitively, using an attenuation factor. We present this system and we study its properties. We also take into consideration the problem of missing votes, which is particularly relevant in online networks, as some recent case shows. Our experiments on real-world networks provide interesting insight into the significance and stability of the results obtained with the suggested voting system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1733343",
                    "name": "P. Boldi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "1737624",
                    "name": "S. Vigna"
                }
            ]
        },
        {
            "paperId": "d6fbee625aac1ca72056522b35104d6c8ebdc5e3",
            "title": "Nearest-neighbor Queries in Probabilistic Graphs",
            "abstract": "Large probabilistic graphs arise in various domains spanning from social networks to biological and communication networks. An important query in these graphs is the k nearestneighbor query, which involves finding and reporting the k closest nodes to a specific node. This query assumes the existence of a measure of the \u201cproximity\u201d or the \u201cdistance\u201d between any two nodes in the graph. To that end, we propose various novel distance functions that extend well known notions of classical graph theory, such as shortest paths and random walks. We argue that many meaningful distance functions are computationally intractable to compute exactly. Thus, in order to process nearest-neighbor queries, we resort to Monte Carlo sampling and exploit novel graph-transformation ideas and pruning opportunities. In our extensive experimental analysis, we explore the trade-offs of our approximation algorithms and demonstrate that they scale well on real-world probabilistic graphs with tens of millions of edges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2873279",
                    "name": "Michalis Potamias"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1715771",
                    "name": "G. Kollios"
                }
            ]
        },
        {
            "paperId": "df9459e06227f429465f3bcea3591ed13f5a0d77",
            "title": "GuruMine: A Pattern Mining System for Discovering Leaders and Tribes",
            "abstract": "In this demo we introduce GuruMine, a pattern mining system for the discovery of leaders, i.e., influential users in social networks, and their tribes, i.e., a set of users usually influenced by the same leader over several actions. GuruMine is built upon a novel pattern mining framework for leaders discovery, that we introduced in [1]. In particular, we consider social networks where users perform actions. Actions may be as simple as tagging resources (urls) as in del.icio.us, rating songs as in Yahoo! Music, or movies as in Yahoo! Movies, or users buying gadgets such as cameras, handholds, etc. and blogging a review on the gadgets. The assumption is that actions performed by a user can be seen by their network friends. Users seeing their friends actions are sometimes tempted to perform those actions. On the basis of the propagation of such influence, in [1] we provided various notion of leaders and developed algorithms for their efficient discovery. GuruMine provides users with a friendly graphical interface for selecting the actions of interest, and the kind of leaders to mine. The set of parameters driving the pattern discovery process can be iteratively refined, and the result is updated, if possible without incurring a completely new computation. Once a set of leaders has been extracted, GuruMine can easily validate them on a set of actions unseen during the pattern mining, by analyzing the portion of network reached by the influence of the selected leaders on the unseen actions. GuruMine also offers various visualizations over the social networks: the propagation of an action, the leaders, their tribes, and the interactions between different leaders and tribes. In this demo we will show: (i) how the pattern mining process can be driven towards the discovery of a good set of leaders, (ii) the ease of use of GuruMine system, and (iii) its outstanding performances on large real-world social networks and actions databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "05aa3b988a3f5ba743bb6b297a8ed41f2bfd81e0",
            "title": "Sequence Mining Automata: A New Technique for Mining Frequent Sequences under Regular Expressions",
            "abstract": "In this paper we study the problem of mining frequent sequences satisfying a given regular expression. Previous approaches to solve this problem were focusing on its search space, pushing (in some way) the given regular expression to prune unpromising candidate patterns. On the contrary, we focus completely on the given input data and regular expression. We introduce sequence mining automata (SMA), a specialized kind of Petri Net that while reading input sequences, it produces for each sequence all and only the patterns contained in the sequence and that satisfy the given regular expression. Based on this automaton, we develop a family of algorithms. Our thorough experimentation on different datasets and application domains confirms that in many cases our methods outperform the current state of the art of frequent sequence mining algorithms using regular expressions (in some cases of orders of magnitude).",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3005971",
                    "name": "R. Trasarti"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1722864",
                    "name": "Bart Goethals"
                }
            ]
        },
        {
            "paperId": "5105b69b51180a8f5067fad26780178913c7d740",
            "title": "The query-flow graph: model and applications",
            "abstract": "Query logs record the queries and the actions of the users of search engines, and as such they contain valuable information about the interests, the preferences, and the behavior of the users, as well as their implicit feedback to search engine results. Mining the wealth of information available in the query logs has many important applications including query-log analysis, user profiling and personalization, advertising, query recommendation, and more.\n In this paper we introduce the query-flow graph, a graph representation of the interesting knowledge about latent querying behavior. Intuitively, in the query-flow graph a directed edge from query qi to query qj means that the two queries are likely to be part of the same \"search mission\". Any path over the query-flow graph may be seen as a searching behavior, whose likelihood is given by the strength of the edges along the path.\n The query-flow graph is an outcome of query-log mining and, at the same time, a useful tool for it. We propose a methodology that builds such a graph by mining time and textual information as well as aggregating queries from different users. Using this approach we build a real-world query-flow graph from a large-scale query log and we demonstrate its utility in concrete applications, namely, finding logical sessions, and query recommendation. We believe, however, that the usefulness of the query-flow graph goes beyond these two applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1733343",
                    "name": "P. Boldi"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "145985025",
                    "name": "D. Donato"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1737624",
                    "name": "S. Vigna"
                }
            ]
        },
        {
            "paperId": "b69cedb77bdcc9804f8509e8659a7554a0c8011f",
            "title": "Discovering leaders from community actions",
            "abstract": "We introduce a novel frequent pattern mining approach to discover leaders and tribes in social networks. In particular, we consider social networks where users perform actions. Actions may be as simple as tagging resources (urls) as in del.icio.us, rating songs as in Yahoo! Music, or movies as in Yahoo! Movies, or users buying gadgets such as cameras, handhelds, etc. and blogging a review on the gadgets. The assumption is that actions performed by a user can be seen by their network friends. Users seeing their friends' actions are sometimes tempted to perform those actions. We are interested in the problem of studying the propagation of such \"influence\", and on this basis, identifying which users are leaders when it comes to setting the trend for performing various actions. We consider alternative definitions of leaders based on frequent patterns and develop algorithms for their efficient discovery. Our definitions are based on observing the way influence propagates in a time window, as the window is moved in time. Given a social graph and a table of user actions, our algorithms can discover leaders of various flavors by making one pass over the actions table. We run detailed experiments to evaluate the utility and scalability of our algorithms on real-life data. The results of our experiments confirm on the one hand, the efficiency of the proposed algorithm, and on the other hand, the effectiveness and relevance of the overall framework. To the best of our knowledge, this the first frequent pattern based approach to social network mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "deb9548e9e0ffae60411c95e8193e42881238b54",
            "title": "Topical query decomposition",
            "abstract": "We introduce the problem of query decomposition, where we are given a query and a document retrieval system, and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query. Ideally, these queries should represent coherent, conceptually well-separated topics.\n We provide an abstract formulation of the query decomposition problem, and we tackle it from two different perspectives. We first show how the problem can be instantiated as a specific variant of a set cover problem, for which we provide an efficient greedy algorithm. Next, we show how the same problem can be seen as a constrained clustering problem, with a very particular kind of constraint, i.e., clustering with predefined clusters. We develop a two-phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming. Our experiments, conducted on a set of actual queries in a Web scale search engine, confirm the effectiveness of the proposed solutions.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145668771",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "145985025",
                    "name": "D. Donato"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                }
            ]
        },
        {
            "paperId": "f7eca95f5c2c96618259810593e35103ad0cd59d",
            "title": "Never Walk Alone: Uncertainty for Anonymity in Moving Objects Databases",
            "abstract": "Preserving individual privacy when publishing data is a problem that is receiving increasing attention. According to the fc-anonymity principle, each release of data must be such that each individual is indistinguishable from at least k - 1 other individuals. In this paper we study the problem of anonymity preserving data publishing in moving objects databases. We propose a novel concept of k-anonymity based on co-localization that exploits the inherent uncertainty of the moving object's whereabouts. Due to sampling and positioning systems (e.g., GPS) imprecision, the trajectory of a moving object is no longer a polyline in a three-dimensional space, instead it is a cylindrical volume, where its radius delta represents the possible location imprecision: we know that the trajectory of the moving object is within this cylinder, but we do not know exactly where. If another object moves within the same cylinder they are indistinguishable from each other. This leads to the definition of (k,delta) -anonymity for moving objects databases. We first characterize the (k, delta)-anonymity problem and discuss techniques to solve it. Then we focus on the most promising technique by the point of view of information preservation, namely space translation. We develop a suitable measure of the information distortion introduced by space translation, and we prove that the problem of achieving (k,delta) -anonymity by space translation with minimum distortion is NP-hard. Faced with the hardness of our problem we propose a greedy algorithm based on clustering and enhanced with ad hoc pre-processing and outlier removal techniques. The resulting method, named NWA (Never Walk .Alone), is empirically evaluated in terms of data quality and efficiency. Data quality is assessed both by means of objective measures of information distortion, and by comparing the results of the same spatio-temporal range queries executed on the original database and on the (k, delta)-anonymized one. Experimental results show that for a wide range of values of delta and k, the relative error introduced is kept low, confirming that NWA produces high quality (k, delta)-anonymized data.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187601",
                    "name": "Osman Abul"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1717192",
                    "name": "M. Nanni"
                }
            ]
        },
        {
            "paperId": "fd0a170b926a7277d5ef13f6a318e40e36af2a73",
            "title": "PinKDD'08: privacy, security, and trust in KDD post workshop report",
            "abstract": "This report summarizes the events of the 2nd International Workshop on Privacy, Security, and Trust in KDD, at the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. The workshop was held on August 24, 2008 in Las Vegas, Nevada and brought together computer scientists working on how data protection issues factor into the context of data mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "88162783",
                    "name": "E. Ferrari"
                },
                {
                    "authorId": "30401303",
                    "name": "Wei Jiang"
                },
                {
                    "authorId": "31717144",
                    "name": "B. Malin"
                }
            ]
        },
        {
            "paperId": "125dae515940846458285bd806276bfcfa1c5ecf",
            "title": "Privacy-Aware Knowledge Discovery from Location Data",
            "abstract": "Spatio-temporal, geo-referenced datasets are growing rapidly, and will be more in the near future. This phenomenon is mostly due to the daily collection of telecommunication data from mobile phones and other location-aware devices and is expected to enable novel classes of applications based on the extraction of behavioral patterns from mobility data. Such patterns could be used for instance in traffic and sustainable mobility management (e.g., to study the accessibility to services), urban planning, environmental monitoring, and collaborative location-based services. Clearly, in these applications privacy is a concern, since some knowledge may be sensitive, or an over-specific pattern may reveal the behaviour of groups of few individual. In this paper we focus on automated privacy-preserving methods we developed for extracting and sharing user- consumable forms of knowledge from large amounts of raw data referenced in space and in time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2413538",
                    "name": "Maurizio Atzori"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "2187601",
                    "name": "Osman Abul"
                }
            ]
        },
        {
            "paperId": "2ac5e25bc14aa9e3223ac95c78a7823362f261f9",
            "title": "Hiding Sequences",
            "abstract": "The process of discovering relevant patterns holding in a database, was first indicated as a threat to database security by O' Leary in [20]. Since then, many different approaches for knowledge hiding have emerged over the years, mainly in the context of association rules and frequent itemsets mining. Following many real-world data and applications demands, in this paper we shift, the problem of knowledge hiding to contexts where both the data arid the extracted knowledge have a sequential structure. We provide problem statement, some theoretical issues including NP-hardness of the problem, a polynomial sanitization algorithm and an experimental evaluation. Finally we discuss possible extensions that will allow to use this work as a basic building block for more complex kinds of patterns and applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187601",
                    "name": "Osman Abul"
                },
                {
                    "authorId": "2413538",
                    "name": "Maurizio Atzori"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                }
            ]
        },
        {
            "paperId": "f2ace45d7a909a245de39dbeb449e9ace4b2e9ad",
            "title": "PinKDD'07: privacy, security, and trust in KDD post-workshop report",
            "abstract": "In this report, we summarize the events of the First International Workshop on Privacy, Security, and Trust in KDD (PinKDD), which was held in conjunction with the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. The workshop convened on August 12, 2007 in San Jose, California and brought together researchers, as well as practitioners, working on how privacy, security, and trust can be resolved or modeled within a data mining framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "88162783",
                    "name": "E. Ferrari"
                },
                {
                    "authorId": "31717144",
                    "name": "B. Malin"
                },
                {
                    "authorId": "1793443",
                    "name": "Y. Saygin"
                }
            ]
        },
        {
            "paperId": "31204ef0608162e2b58b7160f6c1b90d7f7e49c8",
            "title": "ConQueSt: a Constraint-based Querying System for Exploratory Pattern Discovery",
            "abstract": "ConQueSt is a constraint-based querying system devised with the aim of supporting the intrinsically exploratory nature of pattern discovery. It provides users with an expressive constraint-based query language which allows the discovery process to be effectively driven toward potentially interesting patterns. Constraints are also exploited to reduce the cost of pattern mining. The system is built around an efficient constraint-based mining engine which entails several data and search space reduction techniques, and allows new user-defined constraints to be easily added.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1730904",
                    "name": "C. Lucchese"
                },
                {
                    "authorId": "38477686",
                    "name": "S. Orlando"
                },
                {
                    "authorId": "46188036",
                    "name": "R. Perego"
                },
                {
                    "authorId": "3005971",
                    "name": "R. Trasarti"
                }
            ]
        },
        {
            "paperId": "8972898ad3a91c4652c0fcb3e130a90d1b898352",
            "title": "Mining HLA Patterns Explaining Liver Diseases",
            "abstract": "Human leukocyte antigens (HLA), also known as histocompatibility antigens, are molecules found on all nucleated cells in the body. Histocompatibility antigens help the immune system to recognize whether or not a cell is foreign to the body, hence the success of an organ transplantation is strongly connected to the HLA systems of the donor-recipient pair. Beyond this important role, the HLA system seems to influence also the clinical course of hepatic cirrhosis, both on viral and autoimmune basis. However, not only different antigens have different importance w.r.t. hepatic cirrhosis, but, to make things more complicated, other not yet well characterized factors could play an important role. It is thus important to assess the existence of associations between haplotypic settings (possibly mixed with clinical and demographical information) and hepatic cirrhosis. Algorithms developed for frequent patterns discovery can help us achieve this goal. Thanks to their ability in handling a very large number of variables and the associated exponential search space, such techniques can discover interesting haplotypic patterns beyond traditional analysis methods capabilities",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "1749714",
                    "name": "M. Berlingerio"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2114607",
                    "name": "Silvia Chelazzi"
                },
                {
                    "authorId": "3013094",
                    "name": "M. Curcio"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "2041959",
                    "name": "F. Scatena"
                }
            ]
        },
        {
            "paperId": "e5a18251a913295bcff42ae51a265628685d4f40",
            "title": "Towards low-perturbation anonymity preserving pattern discovery",
            "abstract": "It is generally believed that data mining results do not violate the anonymity of the individuals recorded in the source database. In fact, data mining models and patterns, in order to ensure a required statistical significance, represent a large number of individuals and thus conceal individual identities: this is the case of the minimum support threshold in association rule mining. We have recently shown [3], that the above belief is ill-founded: by shifting the concept of k-anonymity [8] from data to patterns, we have formally characterized the notion of a threat to anonymity in the context of frequent itemsets mining, and provided a methodology to efficiently and effectively identify such threats that might arise from the disclosure of a set of frequent itemsets. In our previous paper [2] we have introduced a first, na\u00efve strategy (named suppressive) to sanitize such threats. In this paper we develop a novel sanitization strategy, named additive, which outperforms the previous one in terms of the introduced distortion and has the interesting feature of maintaining the original set of frequent itemsets unchanged, while modifying only the corresponding support values.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2413538",
                    "name": "Maurizio Atzori"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        },
        {
            "paperId": "01a86a1284ae19a0037136f0db520f92c54a877b",
            "title": "Blocking anonymity threats raised by frequent itemset mining",
            "abstract": "In this paper we study when the disclosure of data mining results represents, per se, a threat to the anonymity of the individuals recorded in the analyzed database. The novelty of our approach is that we focus on an objective definition of privacy compliance of patterns without any reference to a preconceived knowledge of what is sensitive and what is not, on the basis of the rather intuitive and realistic constraint that the anonymity of individuals should be guaranteed. In particular, the problem addressed here arises from the possibility of inferring from the output of frequent itemset mining (i.e., a set of item-sets with support larger than a threshold a), the existence of patterns with very low support (smaller than an anonymity threshold k)[M. Atzori et. al, 2005]. In the following we develop a simple methodology to block such inference opportunities by introducing distortion on the dangerous patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2413538",
                    "name": "Maurizio Atzori"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        },
        {
            "paperId": "1c37428cf153fc046f0e1f9aeaab4fdfe0c6feac",
            "title": "ExAnte: a preprocessing method for frequent-pattern mining",
            "abstract": "Our main research objective is to define a data mining query language, supported by a system that can optimize constraint-based data mining queries. We have invented ExAnte, a simple yet effective preprocessing technique for frequent-pattern mining. ExAnte exploits constraints to dramatically reduce the analyzed data to those containing patterns of interest. This data reduction, in turn, induces a strong reduction of the candidate patterns' search space, thus supporting substantial performance improvements in subsequent mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "34590547",
                    "name": "Alessio Mazzanti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        },
        {
            "paperId": "8f61e7c40689be66eb21feecb27b0dd41ed14483",
            "title": "On closed constrained frequent pattern mining",
            "abstract": "Constrained frequent patterns and closed frequent patterns are two paradigms aimed at reducing the set of extracted patterns to a smaller, more interesting, subset. Although a lot of work has been done with both these paradigms, there is still confusion around the mining problem obtained by joining closed and constrained frequent patterns in a unique framework. In this paper, we shed light on this problem by providing a formal definition and a thorough characterization. We also study computational issues and show how to combine the most recent results in both paradigms, providing a very efficient algorithm which exploits the two requirements (satisfying constraints and being closed) together at mining time in order to reduce the computation as much as possible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1730904",
                    "name": "C. Lucchese"
                }
            ]
        },
        {
            "paperId": "bc2f07e1161679944fb3477f9987a64f37e45914",
            "title": "Pre-processing for Constrained Pattern Mining",
            "abstract": "Constraint pushing techniques have been proven to be effective in reducing the search space in the frequent pattern mining task, and thus in improving efficiency. But while pushing anti-monotone constraints in a level-wise computation of frequent itemsets has been recognized to be always profitable, the case is different for monotone constraints. In fact, monotone constraints have been considered harder to push in the computation and less effective in pruning the search space. In this paper, we show that this prejudice is ill founded and introduce ExAnte, a pre-processing data reduction algorithm which reduces dramatically both the search space and the input dataset in constrained frequent patterns mining. Experimental results show a reduction of orders of magnitude, thus enabling a much easier mining task. ExAnte can be used as a pre-processor with any constrained patterns mining algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "34590547",
                    "name": "Alessio Mazzanti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        },
        {
            "paperId": "defe4ae85579fd4856b0ff4b786072ce23b4b389",
            "title": "ExAMiner: optimized level-wise frequent pattern mining with monotone constraints",
            "abstract": "The key point is that, in frequent pattern mining, the most appropriate way of exploiting monotone constraints in conjunction with frequency is to use them in order to reduce the problem input together with the search space. Following this intuition, we introduce ExAMiner, a level-wise algorithm which exploits the real synergy of antimonotone and monotone constraints: the total benefit is greater than the sum of the two individual benefits. ExAMiner generalizes the basic idea of the preprocessing algorithm ExAnte [F. Bonchi et al., (2003)], embedding such ideas at all levels of an Apriori-like computation. The resulting algorithm is the generalization of the Apriori algorithm when a conjunction of monotone constraints is conjoined to the frequency antimonotone constraint. Experimental results confirm that this is, so far, the most efficient way of attacking the computational problem in analysis.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "34590547",
                    "name": "Alessio Mazzanti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        },
        {
            "paperId": "f9680f3410fe185c78d37c026251a200590f0130",
            "title": "Data mining for intelligent Web caching",
            "abstract": "Presents a vertical application of data warehousing and data mining technology: intelligent Web caching. We introduce several ways to construct intelligent Web caching algorithms that employ predictive models of Web requests; the general idea is to extend the LRU (least recently used) policy of Web and proxy servers by making it sensible to Web access models extracted from Web log data using data mining techniques. Two approaches have been studied, in particular one based on association rules and another based on decision trees. The experimental results of the new algorithms show substantial improvements over existing LRU-based caching techniques in terms of the hit rate, i.e. the fraction of Web documents directly retrieved in the cache. We designed and developed a prototypical system, which supports data warehousing of Web log data, extraction of data mining models and simulation of the Web caching algorithms, around an architecture that integrates the various phases in the knowledge discovery process. The system supports a systematic evaluation and benchmarking of the proposed algorithms with respect to existing caching strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1718485",
                    "name": "G. Manco"
                },
                {
                    "authorId": "1694224",
                    "name": "C. Renso"
                },
                {
                    "authorId": "1717192",
                    "name": "M. Nanni"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "2325428",
                    "name": "S. Ruggieri"
                }
            ]
        },
        {
            "paperId": "5174dd480228b68a9175d7afd95c04a5f0371474",
            "title": "A classification-based methodology for planning audit strategies in fraud detection",
            "abstract": "Planning adequate audit strategies is a key success factor in a posterion\u2019 fraud detection, e.g., in the fiscal and insurance domains, where audits are intended to detect tax evasion and fraudulent claims. A case study is presented in this paper, which illustrates how techniques based on classification can be used to support the task of planning audit strategies. The proposed approach is sensible to some conflicting issues of audit planning, e.g., the trade-off between maximizing audit benefits vs. minimizing audit costs. A methodological scenario, common to a whole class of similar applications, is then abstracted away from the case study. The limitations of available systems to support the identified overall KDD process, bring us to point out the key aspects of a logic-based database language, integrated with mining mechanisms, which is used to provide a uniform, highly expressive environment for the various steps in the construction of the considered case-study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "2726758",
                    "name": "G. Mainetto"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        }
    ]
}