{
    "authorId": "1737524",
    "papers": [
        {
            "paperId": "24440f165f72e88373a678fa696b9b2b2f9a8204",
            "title": "REIS: A Visual Analytics Tool for Rendering and Exploring Instance Segmentation of Point Clouds",
            "abstract": "3D Instance Segmentation (3DIS) of Point Clouds (PCs) is valuable for applications like autonomous vehicles, robotics, and Building Information Modeling (BIM). Current work on this topic is guided mainly by global metrics like mAP, which arguably do not support a deep, informed analysis of technique tradeoffs and, more importantly, directions for improvement. Qualitative analysis is widely adopted to provide such guidance, but it is generally implemented ad-hoc. This is true across many tasks in Deep Learning, but PC 3DIS is especially challenging to visually analyze due to the many variables involved: three spatial dimensions, colors, semantic labels, and instance IDs. We propose REIS, a visual analytics tool for Rendering and Exploring Instance Segmentation results. It supports qualitative analysis in two ways: first, through PC renderings targeted at efficient investigation of 3DIS results; second, by providing a systematic way to explore these results via the interactive Instance Detection Matrix- a confusion matrix analog that summarizes error and success cases, and allows the user to navigate through them. To show the efficacy of REIS, we use it to evaluate a state-of-the-art 3DIS approach on the S3DIS dataset. Our code is available at https://github.com/pedro-sidra/pcloud_explorer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2297763025",
                    "name": "Pedro S. de Freitas"
                },
                {
                    "authorId": "2866101",
                    "name": "Wilson Gavi\u00e3o"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "2275136017",
                    "name": "Cl\u00e1udio R. Jung"
                }
            ]
        },
        {
            "paperId": "317e524f5280e7cfdb52bddfa0536b9ee36bbfd1",
            "title": "SHEVA: A Visual Analytics System for Statistical Hypothesis Exploration",
            "abstract": "We demonstrate SHEVA, a System for Hypothesis Exploration with Visual Analytics. SHEVA adopts an Exploratory Data Analysis (EDA) approach to discovering statistically-sound insights from large datasets. The system addresses three longstanding challenges in Multiple Hypothesis Testing: (i) the likelihood of rejecting the null hypothesis by chance, (ii) the pitfall of not being representative of the input data, and (iii) the ability to navigate among many data regions while preserving the user's train of thought. To address (i) & (ii), SHEVA implements significance adjustment methods that account for data-informed properties such as coverage and novelty. To address (iii), SHEVA proposes to guide users by recommending one-sample and two-sample hypotheses in a stepwise fashion following a data hierarchy. Users may choose from a collection of pre-trained hypothesis exploration policies and let SHEVA guide them through the most significant hypotheses in the data, or intervene to override suggested hypotheses. Furthermore, SHEVA relies on data-to-visual element mappings to convey hypothesis testing results in an interpretable fashion, and allows hypothesis pipelines to be stored and retrieved later to be tested on new datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257369136",
                    "name": "Vicente Nejar de Almeida"
                },
                {
                    "authorId": "2239823093",
                    "name": "Eduardo Ribeiro"
                },
                {
                    "authorId": "2135814451",
                    "name": "Nassim Bouarour"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "7d715af35cdabea31920f151fe5e9476de55ed97",
            "title": "Data Visualization for Digital Twins",
            "abstract": "Visualization techniques are useful in the analysis and insight generation for applications in computing in science and engineering. In this article, we describe the importance of visualization to a digital twin (DT), a virtual representation of a physical object, process or system that can be applied for different tasks, such as data-driven simulation, analysis or monitoring. We illustrate tasks in DTs and give examples of how visualization techniques can be applied for DTs in different application areas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "2057494508",
                    "name": "N. Santos"
                },
                {
                    "authorId": "2233501994",
                    "name": "J. C. Rivera"
                },
                {
                    "authorId": "52478087",
                    "name": "R. Romeu"
                },
                {
                    "authorId": "2688905",
                    "name": "Mara Abel"
                },
                {
                    "authorId": "2093523",
                    "name": "R. Borgo"
                },
                {
                    "authorId": "1710352",
                    "name": "K. Gaither"
                }
            ]
        },
        {
            "paperId": "a729bbb09965ee65e2d0c7cb15314817a258e918",
            "title": "COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography",
            "abstract": "The COVID-19 pandemic presented numerous challenges to healthcare systems worldwide. Given that lung infections are prevalent among COVID-19 patients, chest Computer Tomography (CT) scans have frequently been utilized as an alternative method for identifying COVID-19 conditions and various other types of pulmonary diseases. Deep learning architectures have emerged to automate the identification of pulmonary disease types by leveraging CT scan slices as inputs for classification models. This paper introduces COVID-VR, a novel approach for classifying pulmonary diseases based on volume rendering images of the lungs captured from multiple angles, thereby providing a comprehensive view of the entire lung in each image. To assess the effectiveness of our proposal, we compared it against competing strategies utilizing both private data obtained from partner hospitals and a publicly available dataset. The results demonstrate that our approach effectively identifies pulmonary lesions and performs competitively when compared to slice-based methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "147704341",
                    "name": "N. M. L. Romero"
                },
                {
                    "authorId": "2226772306",
                    "name": "Ricco Vasconcellos"
                },
                {
                    "authorId": "2127723",
                    "name": "M. R. Mendoza"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                }
            ]
        },
        {
            "paperId": "edf3d52f83dfdbbb286ace27d009841b32bc3396",
            "title": "Team INF-UFRGS at SemEval-2023 Task 7: Supervised Contrastive Learning for Pair-level Sentence Classification and Evidence Retrieval",
            "abstract": "This paper describes the EvidenceSCL system submitted by our team (INF-UFRGS) to SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT). NLI4CT is divided into two tasks, one for determining the inference relation between a pair of statements in clinical trials and a second for retrieving a set of supporting facts from the premises necessary to justify the label predicted in the first task. Our approach uses pair-level supervised contrastive learning to classify pairs of sentences. We trained EvidenceSCL on two datasets created from NLI4CT and additional data from other NLI datasets. We show that our approach can address both goals of NLI4CT, and although it reached an intermediate position, there is room for improvement in the technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223257486",
                    "name": "Abel Corr\u00eaa Dias"
                },
                {
                    "authorId": "2221318608",
                    "name": "Filipe Dias"
                },
                {
                    "authorId": "2221318642",
                    "name": "Higor Moreira"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                }
            ]
        },
        {
            "paperId": "18a1d863a36718f98d7b79fd9f1dd8af4a0e736d",
            "title": "Improving Automated Lung Segmentation in Ct Images by Adding Anomalies Adjacent to the Pleura",
            "abstract": "Automated lung segmentation in anomalous CT images is of preeminent importance for computer-aided diagnosis systems. State-of-the-art methods tend to either miss consolidated anomalies adjacent to the pleura due to the lack of contrast with the surrounding tissues or include parts of the mediastinum in the segmentation mask. Here we improve a recent and fast segmentation approach based on a sequence of Image Foresting Transforms by adding anomalies adjacent to the pleura with mediastinum exclusion. By that, we expect to improve anomaly segmentation inside the lungs. We present the advantages of our method over the original one and another recent approach based on deep learning using different datasets of anomalous CT images. Our approach can be 1.5 times more accurate than both baselines on average.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "90160257",
                    "name": "A. M. Sousa"
                },
                {
                    "authorId": "2164119934",
                    "name": "I. F. Silva"
                },
                {
                    "authorId": "147704341",
                    "name": "N. M. L. Romero"
                },
                {
                    "authorId": "12401899",
                    "name": "R. Zerbini"
                },
                {
                    "authorId": "144929735",
                    "name": "F. Reis"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "1716806",
                    "name": "A. Falc\u00e3o"
                }
            ]
        },
        {
            "paperId": "24756a097b5dcacfba62ebdb9f21d3e2772fb32b",
            "title": "Guided Stable Dynamic Projections",
            "abstract": "Projections aim to convey the relationships and similarity of high\u2010dimensional data in a low\u2010dimensional representation. Most such techniques are designed for static data. When used for time\u2010dependent data, they usually fail to create a stable and suitable low dimensional representation. We propose two dynamic projection methods (PCD\u2010tSNE and LD\u2010tSNE) that use global guides to steer projection points. This avoids unstable movement that does not encode data dynamics while keeping t\u2010SNE's neighborhood preservation ability. PCD\u2010tSNE scores a good balance between stability, neighborhood preservation, and distance preservation, while LD\u2010tSNE allows creating stable and customizable projections. We compare our methods to 11 other techniques using quality metrics and datasets provided by a recent benchmark for dynamic projections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "6106266",
                    "name": "E. F. Vernier"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "1686665",
                    "name": "A. Telea"
                }
            ]
        },
        {
            "paperId": "2e61585ce7a62c810350f03550ce70b091e90c37",
            "title": "High-Performance Interactive Scientific Visualization With Datoviz via the Vulkan Low-Level GPU API",
            "abstract": "IntroductionMost scientific disciplines are facing exponentially increasing amounts of data, which require scalable interactive scientific visualization technology. The development of massively parallel graphics processing units (GPU), fostered by the video game industry and artificial intelligence, represents a remarkable opportunity in this respect.Real-time computer graphics technology has been used in scientific visualization for decades, mostly via OpenGL, a popular open-source graphics library created in 1992. It has been used by many video games, graphics applications, and scientific visualization software. During its first decade, OpenGL provided a fixed function pipeline that was simple to use, but not particularly powerful because of the lack of control of the rendering pipeline. In 2004, OpenGL 2.X introduced a programmable pipeline that gave the user a way to customize the various stages of rendering. Since then, there have been various open-source libraries providing OpenGL-based scientific visualization, mostly focused on 3D rendering. Mayavi is a popular example in Python\u00a0\\cite{Ramachandran_2011}.In 2013, Luke Campagnola, Almar Klein, Cyrille Rossant, and Nicolas P. Rougier wrote a new visualization library: VisPy\u00a0\\cite{np2015}. This library took full advantage of the GL ES 2.x API to achieve both fast and scalable rendering of the most common plotting objects, in both 2D and 3D (lines, scatters, images, colormaps, volumes, meshes, etc.). Within a few years, VisPy reached a large scientific audience and became the main real-time 2D/3D scientific rendering library in Python.Although usage of OpenGL is still widespread in the graphics and scientific communities for legacy reasons, the industry is steadily moving to newer low-level graphics APIs such as Vulkan (Khronos), WebGPU (W3C), Metal (Apple), and DirectX 12 (Microsoft). In this context, VisPy is now facing the same problem as Mayavi faced a few years back: it must decide on its future.The Khronos group introduced the Vulkan API in 2016 (https://www.khronos.org/news/press/khronos-releases-vulkan-1-0-specification). This has been a complete redesign from the ground up to give much more control (compared to OpenGL) and to support all features of the latest GPU hardware. However, Vulkan has a huge barrier to entry: drawing a simple triangle using the Vulkan API directly involves about a thousand lines of code. In particular, all the logic related to the presentation of images to the screen, using a swapchain for double- or triple-buffering, all the synchronization of the different GPU tasks and CPU-GPU interactions in the main rendering loop must be done manually. To leverage the power of Vulkan for applications such as scientific visualization, there is thus a crucial need for intermediate-level libraries that drastically simplify the access to Vulkan.One potential solution isbe to use existing rendering engines such as Ogre (https://www.ogre3d.org/), Unity (https://unity.com/), or Unreal (https://www.unrealengine.com/). However, although scientific visualization and video games do share many similarities, they are quite different in their ends. Games are generally highly dynamic and interactive, whereas scientific visualization is much more static and less interactive (to some extent) and can be indifferently 1D, 2D, or 3D. Furthermore, scientific visualization involves a number of concepts that are generally not present in game engines, such as colormaps, labeled axes, non-cartesian projections, image interpolations, etc. Scientific visualization must also be faithful to the data and this requires a highly precise rendering to achieve high representational accuracy. Besides, it is not unusual to render millions or even billions of points in a scientific visualization \u2014 while a modern GPU has no problem rendering such a large collection, the corresponding API must be aware of such extreme cases to ensure proper rendering. All these limitations disqualify typical video game rendering engines as a general purpose API for scientific visualization.This exposes a crucial need for a rendering engine that is to scientific visualization what game engines are to video games: an intermediate-level library that allows developers of custom scientific visualizations, or developers of high-level plotting libraries, to leverage Vulkan without delving into an incredibly complicated low-level API. We report here progress that we have made in the past couple of years towards a cross-platform, cross-language scientific visualization engine that leverages Vulkan for scalable, low-overhead, high-performance scientific visualization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39737360",
                    "name": "Cyrille Rossant"
                },
                {
                    "authorId": "143603708",
                    "name": "N. Rougier"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "1710352",
                    "name": "K. Gaither"
                }
            ]
        },
        {
            "paperId": "3b9fb3318b7c2c627b964f5f8a5a4e8d481e3b87",
            "title": "CNN Filter Learning from Drawn Markers for the Detection of Suggestive Signs of COVID-19 in CT Images",
            "abstract": "Early detection of COVID-19 is vital to control its spread. Deep learning methods have been presented to detect suggestive signs of COVID-19 from chest CT images. However, due to the novelty of the disease, annotated volumetric data are scarce. Here we propose a method that does not require either large annotated datasets or backpropagation to estimate the filters of a convolutional neural network (CNN). For a few CT images, the user draws markers at representative normal and abnormal regions. The method generates a feature extractor composed of a sequence of convolutional layers, whose kernels are specialized in enhancing regions similar to the marked ones, and the decision layer of our CNN is a support vector machine. As we have no control over the CT image acquisition, we also propose an intensity standardization approach. Our method can achieve mean accuracy and kappa values of 0.97 and 0.93, respectively, on a dataset with 117 CT images extracted from different sites, surpassing its counterpart in all scenarios.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "90160257",
                    "name": "A. M. Sousa"
                },
                {
                    "authorId": "144929735",
                    "name": "F. Reis"
                },
                {
                    "authorId": "12401899",
                    "name": "R. Zerbini"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "1716806",
                    "name": "A. Falc\u00e3o"
                }
            ]
        },
        {
            "paperId": "614733f3e799b2b0c4701d2e5c58c89dfd5f7a7d",
            "title": "Interactive Data Visualization in Jupyter Notebooks",
            "abstract": "Interactive visualizations are at the core of the exploratory data analysis process, enabling users to directly manipulate and gain insights from data. In this article, we present three different ways in which interactive visualizations can be included in Jupyter Notebooks: 1) matplotlib callbacks; 2) visualization toolkits; and 3) embedding HTML visualizations. We hope that this article will help developers to select the best tools to build their interactive charts in Jupyter Notebooks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066898372",
                    "name": "Jorge Piazentin Ono"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "143803711",
                    "name": "Cl\u00e1udio T. Silva"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "1710352",
                    "name": "K. Gaither"
                }
            ]
        }
    ]
}