{
    "authorId": "119119640",
    "papers": [
        {
            "paperId": "1e17e4259b9d89841a40199e2de0d31792dfee71",
            "title": "Interpretation of Intracardiac Electrograms Through Textual Representations",
            "abstract": "Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretability study to provide a multi-perspective intuition of the model's behavior, which could greatly benefit the clinical use.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2282468445",
                    "name": "Diana Gomez"
                },
                {
                    "authorId": "2282467791",
                    "name": "Avi Alok"
                },
                {
                    "authorId": "2217729610",
                    "name": "Chaojing Duan"
                },
                {
                    "authorId": "2282468233",
                    "name": "Michael A. Rosenberg"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "2267397386",
                    "name": "Emerson Liu"
                },
                {
                    "authorId": "2282499096",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "38d7d45a8e90ad2fdd0a33494fa3d7b57c11bcb2",
            "title": "Entity6K: A Large Open-Domain Evaluation Dataset for Real-World Entity Recognition",
            "abstract": "Open-domain real-world entity recognition is essential yet challenging, involving identifying various entities in diverse environments. The lack of a suitable evaluation dataset has been a major obstacle in this field due to the vast number of entities and the extensive human effort required for data curation. We introduce Entity6K, a comprehensive dataset for real-world entity recognition, featuring 5,700 entities across 26 categories, each supported by 5 human-verified images with annotations. Entity6K offers a diverse range of entity names and categorizations, addressing a gap in existing datasets. We conducted benchmarks with existing models on tasks like image captioning, object detection, zero-shot classification, and dense captioning to demonstrate Entity6K's effectiveness in evaluating models' entity recognition capabilities. We believe Entity6K will be a valuable resource for advancing accurate entity recognition in open-domain settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267397829",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2292207626",
                    "name": "Winfred Wang"
                },
                {
                    "authorId": "2149231840",
                    "name": "Zhengyuan Yang"
                },
                {
                    "authorId": "50703697",
                    "name": "Linjie Li"
                },
                {
                    "authorId": "2261062142",
                    "name": "Jianfeng Wang"
                },
                {
                    "authorId": "2290179598",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "2290642943",
                    "name": "Lei Li"
                },
                {
                    "authorId": "2273909761",
                    "name": "Lijuan Wang"
                }
            ]
        },
        {
            "paperId": "419fc40f4c9e929e90a3a6c7e7aca6d58ecace8e",
            "title": "Evaluating Durability: Benchmark Insights into Multimodal Watermarking",
            "abstract": "With the development of large models, watermarks are increasingly employed to assert copyright, verify authenticity, or monitor content distribution. As applications become more multimodal, the utility of watermarking techniques becomes even more critical. The effectiveness and reliability of these watermarks largely depend on their robustness to various disturbances. However, the robustness of these watermarks in real-world scenarios, particularly under perturbations and corruption, is not well understood. To highlight the significance of robustness in watermarking techniques, our study evaluated the robustness of watermarked content generated by image and text generation models against common real-world image corruptions and text perturbations. Our results could pave the way for the development of more robust watermarking techniques in the future. Our project website can be found at \\url{https://mmwatermark-robustness.github.io/}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267397829",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "150345512",
                    "name": "Xuandong Zhao"
                },
                {
                    "authorId": "2304955927",
                    "name": "Shangbang Long"
                },
                {
                    "authorId": "2290179598",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "2290642943",
                    "name": "Lei Li"
                }
            ]
        },
        {
            "paperId": "161eefb3b54ddb69d6bd82f1e255facb26988d37",
            "title": "MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos",
            "abstract": "Multimodal summarization with multimodal output (MSMO) has emerged as a promising research direction. Nonetheless, numerous limitations exist within existing public MSMO datasets, including insufficient maintenance, data inaccessibility, limited size, and the absence of proper categorization, which pose significant challenges. To address these challenges and provide a comprehensive dataset for this new direction, we have meticulously curated the MMSum dataset. Our new dataset features (1) Human-validated summaries for both video and textual content, providing superior human instruction and labels for mul-timodal learning. (2) Comprehensively and meticulously arranged categorization, spanning 17 principal categories and 170 subcategories to encapsulate a diverse array of real-world scenarios. (3) Benchmark tests performed on the proposed dataset to assess various tasks and methods, including video summarization, text summarization, and multimodal summarization. To champion accessibility and collaboration, we released the MMSum dataset and the data collection tool as fully open-source resources, fostering transparency and accelerating future developments, at https://mmsum-dataset.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267397829",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2219660996",
                    "name": "Aditesh Kumar"
                },
                {
                    "authorId": "2220631291",
                    "name": "Karthik Mittal"
                },
                {
                    "authorId": "2219563428",
                    "name": "Claire Jin"
                },
                {
                    "authorId": "2149231840",
                    "name": "Zhengyuan Yang"
                },
                {
                    "authorId": "50703697",
                    "name": "Linjie Li"
                },
                {
                    "authorId": "2261062142",
                    "name": "Jianfeng Wang"
                },
                {
                    "authorId": "2267388089",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "2267398404",
                    "name": "Bo Li"
                },
                {
                    "authorId": "29957038",
                    "name": "Lijuan Wang"
                }
            ]
        },
        {
            "paperId": "7ed237af793f43c442b3e8e1bc9ace906a276b2a",
            "title": "Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?",
            "abstract": "Recent advancements in Large Language Models (LLMs) have drawn increasing attention since the learned embeddings pretrained on large-scale datasets have shown powerful ability in various downstream applications. However, whether the learned knowledge by LLMs can be transferred to clinical cardiology remains unknown. In this work, we aim to bridge this gap by transferring the knowledge of LLMs to clinical Electrocardiography (ECG). We propose an approach for cardiovascular disease diagnosis and automatic ECG diagnosis report generation. We also introduce an additional loss function by Optimal Transport (OT) to align the distribution between ECG and language embedding. The learned embeddings are evaluated on two downstream tasks: (1) automatic ECG diagnosis report generation, and (2) zero-shot cardiovascular disease detection. Our approach is able to generate high-quality cardiac diagnosis reports and also achieves competitive zero-shot classification performance even compared with supervised baselines, which proves the feasibility of transferring knowledge from LLMs to the cardiac domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2151860122",
                    "name": "Michael Rosenberg"
                },
                {
                    "authorId": "1418480916",
                    "name": "E. Liu"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "82da02137bae421a3f7a89c3bf2ab662037f4dfa",
            "title": "Embodied Executable Policy Learning with Language-based Scene Summarization",
            "abstract": "Large Language models (LLMs) have shown remarkable success in assisting robot learning tasks, i.e., complex household planning.However, the performance of pretrained LLMs heavily relies on domain-specific templated text data, which may be infeasible in real-world robot learning tasks with image-based observations. Moreover, existing LLMs with text inputs lack the capability to evolve with non-expert interactions with environments.In this work, we introduce a novel learning paradigm that generates robots\u2019 executable actions in the form of text, derived solely from visual observations. Our proposed paradigm stands apart from previous works, which utilized either language instructions or a combination of language and visual data as inputs. We demonstrate that our proposed method can employ two fine-tuning strategies, including imitation learning and reinforcement learning approaches, to adapt to the target test tasks effectively.We conduct extensive experiments involving various model selections, environments, and tasks across 7 house layouts in the VirtualHome environment. Our experimental results demonstrate that our method surpasses existing baselines, confirming the effectiveness of this novel learning paradigm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "87c9ac281bb606fb348acfb70348ce744a761d68",
            "title": "MultiSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos",
            "abstract": "Multimodal summarization with multimodal output (MSMO) has emerged as a promising research direction. Nonetheless, numerous limitations exist within existing public MSMO datasets, including insufficient upkeep, data inaccessibility, limited size, and the absence of proper categorization, which pose significant challenges to effective research. To address these challenges and provide a comprehensive dataset for this new direction, we have meticulously curated the MultiSum dataset. Our new dataset features (1) Human-validated summaries for both video and textual content, providing superior human instruction and labels for multimodal learning. (2) Comprehensively and meticulously arranged categorization, spanning 17 principal categories and 170 subcategories to encapsulate a diverse array of real-world scenarios. (3) Benchmark tests performed on the proposed dataset to assess varied tasks and methods, including video temporal segmentation , video summarization , text summarization , and multimodal summarization . To champion accessibility and collaboration, we release the MultiSum dataset and the data collection tool as fully open-source resources, fostering trans-parency and accelerating future developments. Our project website can be found at https://multisum-dataset.github.io/ .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2219660996",
                    "name": "Aditesh Kumar"
                },
                {
                    "authorId": "2220631291",
                    "name": "Karthik Mittal"
                },
                {
                    "authorId": "2219563428",
                    "name": "Claire Jin"
                },
                {
                    "authorId": "2149231840",
                    "name": "Zhengyuan Yang"
                },
                {
                    "authorId": "50703697",
                    "name": "Linjie Li"
                },
                {
                    "authorId": "2124948371",
                    "name": "Jianfeng Wang"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "29957038",
                    "name": "Lijuan Wang"
                }
            ]
        },
        {
            "paperId": "e5d31a57c28225eec753e2174fe8c3fbbeca20fa",
            "title": "Converting ECG Signals to Images for Efficient Image-text Retrieval via Encoding",
            "abstract": "Automated interpretation of electrocardiograms (ECG) has garnered significant attention with the advancements in machine learning methodologies. Despite the growing interest in automated ECG interpretation using machine learning, most current studies focus solely on classification or regression tasks and overlook a crucial aspect of clinical cardio-disease diagnosis: the diagnostic report generated by experienced human clinicians. In this paper, we introduce a novel approach to ECG interpretation, leveraging recent breakthroughs in Large Language Models (LLMs) and Vision-Transformer (ViT) models. Rather than treating ECG diagnosis as a classification or regression task, we propose an alternative method of automatically identifying the most similar clinical cases based on the input ECG data. Also, since interpreting ECG as images are more affordable and accessible, we process ECG as encoded images and adopt a vision-language learning paradigm to jointly learn vision-language alignment between encoded ECG images and ECG diagnosis reports. Encoding ECG into images can result in an efficient ECG retrieval system, which will be highly practical and useful in clinical applications. More importantly, our findings could serve as a crucial resource for providing diagnostic services in regions where only paper-printed ECG images are accessible due to past underdevelopment.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "2131159786",
                    "name": "Shiqi Liu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2157140664",
                    "name": "Jingqi Zhang"
                },
                {
                    "authorId": "1722611",
                    "name": "Chaojing Duan"
                },
                {
                    "authorId": "2151860122",
                    "name": "Michael Rosenberg"
                },
                {
                    "authorId": "1418480916",
                    "name": "E. Liu"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "fc1eb6951dff338164ec9da03e39b4f78f15658e",
            "title": "Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report",
            "abstract": "Automated interpretation of electrocardiograms (ECG) has garnered significant attention with the advancements in machine learning methodologies. Despite the growing interest, most current studies focus solely on classification or regression tasks, which overlook a crucial aspect of clinical cardio-disease diagnosis: the diagnostic report generated by experienced human clinicians. In this paper, we introduce a novel approach to ECG interpretation, leveraging recent breakthroughs in Large Language Models (LLMs) and Vision-Transformer (ViT) models. Rather than treating ECG diagnosis as a classification or regression task, we propose an alternative method of automatically identifying the most similar clinical cases based on the input ECG data. Also, since interpreting ECG as images is more affordable and accessible, we process ECG as encoded images and adopt a vision-language learning paradigm to jointly learn vision-language alignment between encoded ECG images and ECG diagnosis reports. Encoding ECG into images can result in an efficient ECG retrieval system, which will be highly practical and useful in clinical applications. More importantly, our findings could serve as a crucial resource for providing diagnostic services in underdeveloped regions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2267397829",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "2267413882",
                    "name": "Shiqi Liu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "2157140664",
                    "name": "Jingqi Zhang"
                },
                {
                    "authorId": "2217729610",
                    "name": "Chaojing Duan"
                },
                {
                    "authorId": "2151860122",
                    "name": "Michael Rosenberg"
                },
                {
                    "authorId": "2267397386",
                    "name": "Emerson Liu"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "2267388089",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "f92c07ba9a3de29fb212a65b041f56bb11730c37",
            "title": "Can Brain Signals Reveal Inner Alignment with Human Languages?",
            "abstract": "Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \\textbf{MTAM}, a \\textbf{M}ultimodal \\textbf{T}ransformer \\textbf{A}lignment \\textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide interpretations of the performance improvement: (1) feature distribution shows the effectiveness of the alignment module for discovering and encoding the relationship between EEG and language; (2) alignment weights show the influence of different language semantics as well as EEG frequency features; (3) brain topographical maps provide an intuitive demonstration of the connectivity in the brain regions. Our code is available at \\url{https://github.com/Jason-Qiu/EEG_Language_Alignment}.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                }
            ]
        }
    ]
}