{
    "authorId": "2065702132",
    "papers": [
        {
            "paperId": "d377fd14dcabaad8955c9d89b2899e661472dd2b",
            "title": "Collaborative Cooking in VR: Effects of Network Distortion in Multi-User Virtual Environments",
            "abstract": "The future of human interaction is virtual. Thus it will require effective collaboration on tasks among users in remote settings. eXtended Reality (XR) is playing a leading role in this transition, offering a realm where virtual collaboration becomes not just possible but essential in situations where physical presence is limited by risk, cost, or complexity. However, while networks are continuously evolving, they can still introduce unexpected impairments that potentially degrade the user perception, i.e., the Quality-of-Experience (QoE), of such Collaborative Virtual Reality (CVR) scenarios. In response to this challenge, this paper presents a demonstrator designed to explicitly showcase the effects of network conditions on CVR. Our platform, centered around a pizza-making game, allows for exploration of the real-time impact of different network parameters, such as packet delay, loss, and throttling on the user engagement and perception in CVR. The framework employs a combination of subjective, objective, and physiological assessments, including the capture of heart rate and skin conductivity, to gain comprehensive insights into user experiences. Our platform not only allows users to directly experience the impact of network impairments on CVR interactions but also provides initial evidence of how such distortions affect both subjective perceptions and objective performance metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2239105746",
                    "name": "Javad Sameri"
                },
                {
                    "authorId": "2189807569",
                    "name": "Sam van Damme"
                },
                {
                    "authorId": "2267647305",
                    "name": "Susanna Schwarzmann"
                },
                {
                    "authorId": "2297435967",
                    "name": "Qing Wei"
                },
                {
                    "authorId": "2254067120",
                    "name": "Riccardo Trivisonno"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                },
                {
                    "authorId": "2197849132",
                    "name": "Maria Torres Vega"
                }
            ]
        },
        {
            "paperId": "e4fcd83de478fbaf7b84db627fcdacdaffc45262",
            "title": "Warrens: Decentralized Connectionless Tunnels for Edge Container Networks",
            "abstract": "In recent years, workload containerisation has been extended to the edge, bringing with it the need for flexible overlay networking. However, current container networking solutions are generally designed for the cloud, aimed at relatively static clusters with centralized generation of container subnet addresses and assigning them to nodes. Added to that existing tunneling solutions, such as Virtual Private Networks (VPN), also have centralized components. Conversely, the network edge is geo-dispersed and has a volatile topology,with edge nodes typically hidden behind routers, in private networks. To enable large-scale networking at the edge, there is need for decentralized self-management of container network addresses and overlay tunnels. This manuscript presents Warrens, a framework for fully decentralized and self-organizing cloud-edge container networks. Warrens enables communication between edge nodes in different private networks by enabling connectionless tunnels, supported by decentralized self-assignment of container IP addresses, with the assignment scheme minimizing address conflict to a negligible level. Warrens has been implemented in two variants using kernel-level eBPF for processing speed, and user-level Golang for wider compatibility. Warrens is shown to be highly scalable compared to a typical VPN solution, and performance evaluations demonstrate it can handle a full network load on both x64 devices and a Raspberry Pi with $\\approx 0.5\\%$ to 5% total CPU load, depending on traffic direction and protocols used.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52223901",
                    "name": "Tom Goethals"
                },
                {
                    "authorId": "1404810662",
                    "name": "Mays F. Al-Naday"
                },
                {
                    "authorId": "1803683",
                    "name": "B. Volckaert"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                }
            ]
        },
        {
            "paperId": "02c6926a190df8dbd0d8f7403e0ef8b0f7c59a40",
            "title": "Diktyo: Network-Aware Scheduling in Container-Based Clouds",
            "abstract": "Containers have revolutionized application deployment and life-cycle management in current cloud platforms. Applications have evolved from single monoliths to complex graphs of loosely-coupled microservices. However, the efficient allocation of microservice-based applications is challenging due to their complex inter-dependencies. Further, recent applications are becoming even more delay-sensitive, demanding lower latency between dependent microservices. Scheduling policies in popular container orchestration platforms mainly aim to increase the resource efficiency of the infrastructure, insufficient for latency-sensitive applications. Application domains such as the Internet of Things and multi-tier Web services would benefit from network-aware policies that consider network latency and bandwidth in the scheduling process. Previous works have studied network-aware scheduling via theoretical formulations or heuristic-based methods evaluated via simulations or small testbeds, making their full applicability in popular platforms difficult. This paper proposes a novel network-aware framework for the popular Kubernetes (K8s) platform named Diktyo that determines the placement of dependent microservices in long-running applications focused on reducing the application\u2019s end-to-end latency and guaranteeing bandwidth reservations. Simulations show that Diktyo can significantly reduce the network latency for various applications across different infrastructure topologies compared to default K8s scheduling plugins. Also, experiments in a K8s cluster with microservice benchmark applications show that Diktyo can increase database throughput by 22% and reduce application response time by 45%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115681922",
                    "name": "Jos\u00e9 Santos"
                },
                {
                    "authorId": "2157434199",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "145115855",
                    "name": "T. Wauters"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                }
            ]
        },
        {
            "paperId": "1f50e75fb4f2c244c496ead4992b18443b188102",
            "title": "Benchmarking Whole Knowledge Graph Embedding Techniques",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33122761",
                    "name": "P. Bonte"
                },
                {
                    "authorId": "51286999",
                    "name": "Sander Vanden Hautte"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                },
                {
                    "authorId": "134767416",
                    "name": "Sofie Van Hoecke"
                },
                {
                    "authorId": "1726557",
                    "name": "F. Ongenae"
                }
            ]
        },
        {
            "paperId": "26c7de9b97bd44fd0bc6de32d4560ede845e36e7",
            "title": "A Haptic-enabled, Distributed and Networked Immersive System for Multi-User Collaborative Virtual Reality",
            "abstract": "Virtual Reality (VR) is gaining attention in various domains such as entertainment, industry, mental healthcare and VR training. Al- though most of these use-cases are still limited to single-user tasks, a lot of applications are heavily depending on multi-user collaboration. Existing multi-user VR systems are most often created in a classic server-client architecture, however, which induces unpredictable network behaviour which can affect the end-user's Quality-of-Experience (QoE) and performance. In addition, the interaction methods in these systems are often constrained to either traditional VR controllers or very use-case specific interaction methods, such that general purpose haptic gloves form a somewhat under-explored part of literature. Therefore, we (i) present a networked, distributed multi-user VR system with synchronization of environments over a low-bandwidth networked connection. In addition, we (ii) enhance the experience by adding haptic gloves to the system, which we compare to the traditional VR controllers in a subjective experiment. As a proof-of-concept, a use case is implemented in which two users have to prepare and bake a virtual pizza. The results show that high framerates (> 90 Frames Per Second (FPS)) can be obtained while keeping network throughput to a minimum ( < 1 Mbps). The accompanying user study shows that haptic gloves are preferred when immersiveness is the main emphasis of the virtual environment, while controllers are more suited when performance is in the center of attention. In objective terms, the applicability of haptic feedback is highly dependent on the task at hand.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189807569",
                    "name": "Sam van Damme"
                },
                {
                    "authorId": "2259972288",
                    "name": "Fangio Van de Velde"
                },
                {
                    "authorId": "2259955549",
                    "name": "Mohammad Javad Sameri"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                },
                {
                    "authorId": "2197849132",
                    "name": "Maria Torres Vega"
                }
            ]
        },
        {
            "paperId": "28fb1719cfcabbadba894469e14c218aeb317b2c",
            "title": "Adaptive Partially Reliable Delivery of Immersive Media Over QUIC-HTTP/3",
            "abstract": "The increasing popularity of head-mounted displays (HMD) and depth cameras has encouraged content providers to offer interactive immersive media content over the internet. Traditionally, dynamic adaptive streaming over HTTP (DASH) is the go-to standard for video streaming. However, HTTP is built on top of protocols such as the transmission control protocol (TCP), which prioritize reliability over latency, thereby, inducing additional delay due to acknowledgments and retransmissions, especially on lossy networks. In addition, such reliable protocols suffer from head-of-line (HOL) blocking problem at various levels, leading to playout interruptions of the video streaming application. The third generation of HTTP, i.e., HTTP/3 was recently introduced to deal with the issues posed by TCP. As a major change, HTTP/3 replaces TCP with QUIC at the transport layer, which solves the HOL problem at the transport layer. Moreover, the datagram extension of QUIC allows for unreliable data delivery, just like UDP, which could substantially reduce the latency. Combining this feature of QUIC with the quality adaptation capabilities of DASH-based streaming could bring interactive immersive media delivery to the next level. This work proposes the integration of DASH with the concept of partial reliability of QUIC to reduce playout interruptions and increase the quality of the delivered immersive content on lossy networks. Here, the DASH scheme takes quality and prioritization decisions based on the changing network conditions and the user\u2019s viewport, respectively. Then, the part of the content with the highest priority, i.e., within the viewport, is delivered reliably and the rest unreliably. To the best of our knowledge, this is the first work to combine adaptive streaming with partial reliability. Herein, we provide an implementation of a headless player which supports HTTP/3 over partially reliable QUIC as well as state-of-the-art protocols like HTTP/3 over reliable QUIC and HTTP/2 over TCP. We performed an extensive evaluation of our proposed solution using real-world 5G throughput traces and bursty packet loss conditions, using point cloud streaming as the use case. Firstly, our evaluation shows that HTTP/2 is highly intolerant to loss and not suitable for streaming immersive media. Furthermore, even at a loss as high as 5%, the partially reliable framework achieves 46% higher throughput and delivers the content with 33% fewer playout interruptions compared to the reliable counterpart. Since current point cloud decoders are sensitive to loss, we applied the forward error correction mechanism to the data sent unreliably to ensure that the client decodes the content at a probability of 99.9%. Applying this overhead to our solution provides a significant gain of 25% in the throughput compared to the state of the art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1394580307",
                    "name": "H. Ravuri"
                },
                {
                    "authorId": "2197849132",
                    "name": "Maria Torres Vega"
                },
                {
                    "authorId": "2215407664",
                    "name": "Jeroen Der Van Hooft"
                },
                {
                    "authorId": "145115855",
                    "name": "T. Wauters"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                }
            ]
        },
        {
            "paperId": "5df603ba681d891103cecaa137555fcf56f09ae6",
            "title": "Characterization of the Quality of Experience and Immersion of Point Cloud Videos in Augmented Reality Through a Subjective Study",
            "abstract": "Point cloud streaming has recently attracted research attention as it has the potential to provide six degrees of freedom movement, which is essential for truly immersive media. The transmission of point clouds requires high-bandwidth connections, and adaptive streaming is a promising solution to cope with fluctuating bandwidth conditions. Thus, understanding the impact of different factors in adaptive streaming on the Quality of Experience (QoE) becomes fundamental. Point clouds have been evaluated in Virtual Reality (VR), where viewers are completely immersed in a virtual environment. Augmented Reality (AR) is a novel technology and has recently become popular, yet quality evaluations of point clouds in AR environments are still limited to static images. In this paper, we perform a subjective study of four impact factors on the QoE of point cloud video sequences in AR conditions, including encoding parameters (quantization parameters, QPs), quality switches, viewing distance, and content characteristics. The experimental results show that these factors significantly impact the QoE. The QoE decreases if the sequence is encoded at high QPs and/or switches to lower quality and/or is viewed at a shorter distance, and vice versa. Additionally, the results indicate that the end user is not able to distinguish the quality differences between two quality levels at a specific (high) viewing distance. An intermediate-quality point cloud encoded at geometry QP (G-QP) 24 and texture QP (T-QP) 32 and viewed at 2.5m can have a QoE (i.e., score 6.5 out of 10) comparable to a high-quality point cloud encoded at 16 and 22 for G-QP and T-QP, respectively, and viewed at a distance of 5m. Regarding content characteristics, objects with lower contrast can yield better quality scores. Participants\u2019 responses reveal that the visual quality of point clouds has not yet reached an immersion level as desired. The average QoE of the highest visual quality is less than 8 out of 10. There is also a good correlation between objective metrics (e.g., color Peak Signal-to-Noise Ratio (PSNR) and geometry PSNR) and the QoE score. Especially the Pearson correlation coefficients of color PSNR is 0.84. Finally, we found that machine learning models are able to accurately predict the QoE of point clouds in AR environments. The subjective test results and questionnaire responses are available on GitHub: https://github.com/minhkstn/QoE-and-Immersion-of-Dynamic-Point-Cloud.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114751619",
                    "name": "Minh Nguyen"
                },
                {
                    "authorId": "2202586618",
                    "name": "Shivi Vats"
                },
                {
                    "authorId": "2189807569",
                    "name": "Sam van Damme"
                },
                {
                    "authorId": "113568560",
                    "name": "Jeroen van der Hooft"
                },
                {
                    "authorId": "2197849132",
                    "name": "Maria Torres Vega"
                },
                {
                    "authorId": "145115855",
                    "name": "T. Wauters"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                },
                {
                    "authorId": "1757605",
                    "name": "C. Timmerer"
                },
                {
                    "authorId": "1737679",
                    "name": "H. Hellwagner"
                }
            ]
        },
        {
            "paperId": "8ebe9a097308b6e87e43f6006039851111a81cd6",
            "title": "Service-Based, Multi-Provider, Fog Ecosystem With Joint Optimization of Request Mapping and Response Routing",
            "abstract": "Digital transformation is increasingly reliant on service-based operations in fog networks. The latter is a geo-dispersed form of the cloud, extending resources closer to end-users for improved privacy and reduced latency. The dispersion leverages diversity of compute-network capacities and energy prices, while promotes the coexistence of multiple providers. This drives variation in operational cost, coupled with limited information sharing across providers. Consequently, there is a critical need for an orchestration solution that preserves autonomy and optimizes operational cost across domains, while meeting service requirements. This article proposes a novel service-based fog management and network orchestrator (sbMANO), which utilizes service metadata in enabling multi-provider resource management. The sbMANO is empowered with a novel optimization algorithm for service-based joint request mapping and response routing. The algorithm acts on partial information and preserves the edge for delay-critical services. The performance of the algorithm is evaluated analytically for delay-aware and delay-agnostic variants. The results show that both achieve near-optimal performance in maximizing user satisfaction with minimum operational cost. Furthermore, the delay-aware variant outperforms the agnostic counterpart, with higher user satisfaction and lower operational cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1404810662",
                    "name": "Mays F. Al-Naday"
                },
                {
                    "authorId": "1699717",
                    "name": "N. Thomos"
                },
                {
                    "authorId": "2585753",
                    "name": "Jiejun Hu"
                },
                {
                    "authorId": "1803683",
                    "name": "B. Volckaert"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                },
                {
                    "authorId": "3007089",
                    "name": "M. Reed"
                }
            ]
        },
        {
            "paperId": "c7e46f5c3a997965dc6f8ae915936899c61b7a89",
            "title": "Characterizing the Impact of Data-Damaged Models on Generalization Strength in Intrusion Detection",
            "abstract": "Generalization is a longstanding assumption in articles concerning network intrusion detection through machine learning. Novel techniques are frequently proposed and validated based on the improvement they attain when classifying one or more of the existing datasets. The necessary follow-up question of whether this increased performance in classification is meaningful outside of the dataset(s) is almost never investigated. This lacuna is in part due to the sparse dataset landscape in network intrusion detection and the complexity of creating new data. The introduction of two recent datasets, namely CIC-IDS2017 and CSE-CIC-IDS2018, opened up the possibility of testing generalization capability within similar academic datasets. This work investigates how well models from different algorithmic families, pretrained on CICIDS2017, are able to classify the samples in CSE-CIC-IDS2018 without retraining. Earlier work has shown how robust these models are to data reduction when classifying state-of-the-art datasets. This work experimentally demonstrates that the implicit assumption that strong generalized performance naturally follows from strong performance on a specific dataset is largely erroneous. The supervised machine learning algorithms suffered flat losses in classification performance ranging from 0 to 50% (depending on the attack class under test). For non-network-centric attack classes, this performance regression is most pronounced, but even the less affected models that classify the network-centric attack classes still show defects. Current implementations of intrusion detection systems (IDSs) with supervised machine learning (ML) as a core building block are thus very likely flawed if they have been validated on the academic datasets, without the consideration for their general performance on other academic or real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1414010294",
                    "name": "Laurens D\u2019hooge"
                },
                {
                    "authorId": "2029679358",
                    "name": "Miel Verkerken"
                },
                {
                    "authorId": "145115855",
                    "name": "T. Wauters"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                },
                {
                    "authorId": "1803683",
                    "name": "B. Volckaert"
                }
            ]
        },
        {
            "paperId": "cfffc17fc887e5859e6f67006126936d2eed4ceb",
            "title": "Investigating Generalized Performance of Data-Constrained Supervised Machine Learning Models on Novel, Related Samples in Intrusion Detection",
            "abstract": "Recently proposed methods in intrusion detection are iterating on machine learning methods as a potential solution. These novel methods are validated on one or more datasets from a sparse collection of academic intrusion detection datasets. Their recognition as improvements to the state-of-the-art is largely dependent on whether they can demonstrate a reliable increase in classification metrics compared to similar works validated on the same datasets. Whether these increases are meaningful outside of the training/testing datasets is rarely asked and never investigated. This work aims to demonstrate that strong general performance does not typically follow from strong classification on the current intrusion detection datasets. Binary classification models from a range of algorithmic families are trained on the attack classes of CSE-CIC-IDS2018, a state-of-the-art intrusion detection dataset. After establishing baselines for each class at various points of data access, the same trained models are tasked with classifying samples from the corresponding attack classes in CIC-IDS2017, CIC-DoS2017 and CIC-DDoS2019. Contrary to what the baseline results would suggest, the models have rarely learned a generally applicable representation of their attack class. Stability and predictability of generalized model performance are central issues for all methods on all attack classes. Focusing only on the three best-in-class models in terms of interdataset generalization, reveals that for network-centric attack classes (brute force, denial of service and distributed denial of service), general representations can be learned with flat losses in classification performance (precision and recall) below 5%. Other attack classes vary in generalized performance from stark losses in recall (\u221235%) with intact precision (98+%) for botnets to total degradation of precision and moderate recall loss for Web attack and infiltration models. The core conclusion of this article is a warning to researchers in the field. Expecting results of proposed methods on the test sets of state-of-the-art intrusion detection datasets to translate to generalized performance is likely a serious overestimation. Four proposals to reduce this overestimation are set out as future work directions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1414010294",
                    "name": "Laurens D\u2019hooge"
                },
                {
                    "authorId": "2029679358",
                    "name": "Miel Verkerken"
                },
                {
                    "authorId": "145115855",
                    "name": "T. Wauters"
                },
                {
                    "authorId": "2065702132",
                    "name": "F. de Turck"
                },
                {
                    "authorId": "1803683",
                    "name": "B. Volckaert"
                }
            ]
        }
    ]
}