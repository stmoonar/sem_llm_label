{
    "authorId": "2064342729",
    "papers": [
        {
            "paperId": "204b4b3e9a26004d1954fa1e2b14536bb3be1c29",
            "title": "\"Can We Detect Substance Use Disorder?\": Knowledge and Time Aware Classification on Social Media from Darkweb",
            "abstract": "Opioid and substance misuse is rampant in the United States today, with the phenomenon known as the\"opioid crisis\". The relationship between substance use and mental health has been extensively studied, with one possible relationship being: substance misuse causes poor mental health. However, the lack of evidence on the relationship has resulted in opioids being largely inaccessible through legal means. This study analyzes the substance use posts on social media with opioids being sold through crypto market listings. We use the Drug Abuse Ontology, state-of-the-art deep learning, and knowledge-aware BERT-based models to generate sentiment and emotion for the social media posts to understand users' perceptions on social media by investigating questions such as: which synthetic opioids people are optimistic, neutral, or negative about? or what kind of drugs induced fear and sorrow? or what kind of drugs people love or are thankful about? or which drugs people think negatively about? or which opioids cause little to no sentimental reaction. We discuss how we crawled crypto market data and its use in extracting posts for fentanyl, fentanyl analogs, and other novel synthetic opioids. We also perform topic analysis associated with the generated sentiments and emotions to understand which topics correlate with people's responses to various drugs. Additionally, we analyze time-aware neural models built on these features while considering historical sentiment and emotional activity of posts related to a drug. The most effective model performs well (statistically significant) with (macroF1=82.12, recall =83.58) to identify substance use disorder.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46189109",
                    "name": "Usha Lokala"
                },
                {
                    "authorId": "2206841649",
                    "name": "Orchid Chetia Phukan"
                },
                {
                    "authorId": "2061341707",
                    "name": "T. Dastidar"
                },
                {
                    "authorId": "37450171",
                    "name": "Francois R. Lamy"
                },
                {
                    "authorId": "2859627",
                    "name": "Raminta Daniulaityte"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "360e4cbd79690f75756900dcb4818cf177071995",
            "title": "Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust",
            "abstract": "A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs. This has significant application-level user trust implications as stochastic patterns without a strong sense of meaning cannot be trusted in high-stakes applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2066796687",
                    "name": "Tarun Garg"
                },
                {
                    "authorId": "2216605916",
                    "name": "Vedant Palit"
                },
                {
                    "authorId": "2187301061",
                    "name": "Yuxin Zi"
                },
                {
                    "authorId": "2068096390",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "42f3552410e51045c14f9a1834d97880a53bbb58",
            "title": "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes",
            "abstract": "As people become more aware of their food choices, food computation models have become increasingly popular in assisting people in maintaining healthy eating habits. For example, food recommendation systems analyze recipe instructions to assess nutritional contents and provide recipe recommendations. The recent and remarkable successes of generative AI methods, such as auto-regressive Large Language Models, can enable robust methods for a more comprehensive understanding of recipes for healthy food recommendations beyond surface-level nutrition content assessments. In this study, we investigate the use of generative AI methods to extend current food computation models, primarily involving the analysis of nutrition and ingredients, to also incorporate cooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.), Cooking actions are notoriously hard to model using statistical learning methods due to irregular data patterns - significantly varying natural language descriptions for the same action (e.g., marinate the meat vs. marinate the meat and leave overnight) and infrequently occurring patterns (e.g., add salt occurs far more frequently than marinating the meat). The prototypical approach to handling irregular data patterns is to increase the volume of data that the model ingests by orders of magnitude. Unfortunately, in the cooking domain, these problems are further compounded with larger data volumes presenting a unique challenge that is not easily handled by simply scaling up. In this work, we propose novel aggregation-based generative AI methods, Cook-Gen, that reliably generate cooking actions from recipes, despite difficulties with irregular data patterns, while also outperforming Large Language Models and other strong baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2214695852",
                    "name": "Kanak Raj"
                },
                {
                    "authorId": "35029095",
                    "name": "Renjith Prasad"
                },
                {
                    "authorId": "2187301061",
                    "name": "Yuxin Zi"
                },
                {
                    "authorId": "2068096390",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "43c05d2046d74a9536971a6f86b55420d214576e",
            "title": "L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models\u2731",
            "abstract": "Fine-tuning pre-trained foundational language models (FLM) for specific tasks is often impractical, especially for resource-constrained devices. This necessitates the development of a Lifelong Learning (L3) framework that continuously adapts to a stream of Natural Language Processing (NLP) tasks efficiently. We propose an approach that focuses on extracting meaningful representations from unseen data, constructing a structured knowledge base, and improving task performance incrementally. We conducted experiments on various NLP tasks to validate its effectiveness, including benchmarks like GLUE and SuperGLUE. We measured good performance across the accuracy, training efficiency, and knowledge transfer metrics. Initial experimental results show that the proposed L3 ensemble method increases the model accuracy 4% \u223c 36% compared to the fine-tuned FLM. Furthermore, L3 model outperforms naive fine-tuning approaches while maintaining competitive or superior performance (up to 15.4% increase in accuracy) compared to the state-of-the-art language model (T5) for the given task, STS benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151484341",
                    "name": "Aidin Shiri"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "1491238594",
                    "name": "Manas Gaur"
                }
            ]
        },
        {
            "paperId": "6b1c1bafe67521f0fddd8ffb73700c88978e0a44",
            "title": "Knowledge Graph Empowered Machine Learning Pipelines for Improved Efficiency, Reusability, and Explainability",
            "abstract": "Artificial intelligence (AI) pipelines are complex, heavily parameterized, and expensive to execute in terms of time and computational resources. Consequently, it is onerous to run experiments with all possible parameter combinations to achieve an optimal solution. However, these AI experiments can be optimized by recommending relevant parameters to commence the experiments, reducing search space significantly, which can be fine tuned further. The relevant parameters can be identified by observing the metadata of pipelines executed in the past, and the relevant pipeline with relevant parameters can be recommended to the user. Currently, there are various metadata frameworks that automatically record the metadata of AI pipelines. Developing a recommendation system requires understanding pipeline metadata components and their interactions. There is a need to represent the metadata generated by these AI pipelines that capture the relationship among these pipeline entities. This article presents a knowledge-infused recommender that utilizes prior knowledge and metadata of already executed pipelines represented using the proposed metadata schema to recommend a relevant pipeline per user queries. Unlike black-box models, the use of knowledge graphs makes recommendations explainable, improving transparency and trustworthiness for the users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "2643562",
                    "name": "Aalap Tripathy"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "35655815",
                    "name": "H. Y. Yip"
                },
                {
                    "authorId": "2205970689",
                    "name": "Annmary Justine"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "6d18dcd4b562ea15f3270551b71596b2a1980b07",
            "title": "Tutorials at The Web Conference 2023",
            "abstract": "This paper summarizes the content of the 28 tutorials that have been given at The Web Conference 2023.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "1705291",
                    "name": "Valeria Fionda"
                },
                {
                    "authorId": "2215622430",
                    "name": "Olaf Hartig"
                },
                {
                    "authorId": "1805958417",
                    "name": "Reyhaneh Abdolazimi"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2215690996",
                    "name": "Hongzhi Chen"
                },
                {
                    "authorId": "2117027107",
                    "name": "Xiao Chen"
                },
                {
                    "authorId": "2052469774",
                    "name": "P. Cui"
                },
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                },
                {
                    "authorId": "2215596266",
                    "name": "Xin Luna Dong"
                },
                {
                    "authorId": "2957808",
                    "name": "Lisette Espin Noboa"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2132398392",
                    "name": "Manuela Fritz"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2161309826",
                    "name": "Jingtong Gao"
                },
                {
                    "authorId": "46909769",
                    "name": "Xiaojie Guo"
                },
                {
                    "authorId": "2215622544",
                    "name": "Torsten Hahmann"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "1842532",
                    "name": "Estevam Hruschka"
                },
                {
                    "authorId": "2118850040",
                    "name": "Liang Hu"
                },
                {
                    "authorId": "2139299903",
                    "name": "Jiaxin Huang"
                },
                {
                    "authorId": "47247243",
                    "name": "Utkarshani Jaimini"
                },
                {
                    "authorId": "2299944027",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "51118506",
                    "name": "F. Karimi"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                },
                {
                    "authorId": "3309003",
                    "name": "Hady W. Lauw"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "1808423005",
                    "name": "Trung-Hoang Le"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2110855835",
                    "name": "Geon Lee"
                },
                {
                    "authorId": "19326298",
                    "name": "Liat Levontin"
                },
                {
                    "authorId": "2144231489",
                    "name": "Cheng-Te Li"
                },
                {
                    "authorId": "144911687",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2110471246",
                    "name": "Ying Li"
                },
                {
                    "authorId": "2030126978",
                    "name": "Jay Chiehen Liao"
                },
                {
                    "authorId": "2157067900",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "46189109",
                    "name": "Usha Lokala"
                },
                {
                    "authorId": "2085850",
                    "name": "Ben London"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "153612944",
                    "name": "H. Mcginty"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "2069543964",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "48331451",
                    "name": "Devesh Parekh"
                },
                {
                    "authorId": "2188744953",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2101664",
                    "name": "Tiago P. Peixoto"
                },
                {
                    "authorId": "144615425",
                    "name": "S. Pemberton"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                },
                {
                    "authorId": "2065812052",
                    "name": "Filip Radlinski"
                },
                {
                    "authorId": "48890086",
                    "name": "Federico Rossetto"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2911888",
                    "name": "Aghiles Salah"
                },
                {
                    "authorId": "2128305",
                    "name": "M. Sameki"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "28908689",
                    "name": "C. Shimizu"
                },
                {
                    "authorId": "40553270",
                    "name": "Kijung Shin"
                },
                {
                    "authorId": "2451800",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "2064266862",
                    "name": "Dacheng Tao"
                },
                {
                    "authorId": "2528063",
                    "name": "Johanne R. Trippas"
                },
                {
                    "authorId": "39603708",
                    "name": "Quoc-Tuan Truong"
                },
                {
                    "authorId": "1896151979",
                    "name": "Yu-Che Tsai"
                },
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "2215624802",
                    "name": "Bram Van Den Akker"
                },
                {
                    "authorId": "3265905",
                    "name": "Linshan Wang"
                },
                {
                    "authorId": "2144295736",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2153691630",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "69047048",
                    "name": "H. Weld"
                },
                {
                    "authorId": "2116666963",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2181385841",
                    "name": "D. Xu"
                },
                {
                    "authorId": "2138609128",
                    "name": "E. Xu"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2156653838",
                    "name": "Bo Yang"
                },
                {
                    "authorId": "2125559318",
                    "name": "Keyue Yang"
                },
                {
                    "authorId": "1388775854",
                    "name": "E. Yom-Tov"
                },
                {
                    "authorId": "31888223",
                    "name": "Jaemin Yoo"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2281410",
                    "name": "R. Zafarani"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                },
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                },
                {
                    "authorId": "2145906426",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "3358065",
                    "name": "Xikun Zhang"
                },
                {
                    "authorId": "1739818",
                    "name": "Yongfeng Zhang"
                },
                {
                    "authorId": "49891156",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2148904413",
                    "name": "Zhengqi Zhang"
                },
                {
                    "authorId": "144010790",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "2116710405",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "76f20cde68a7720079266ab02f3ce66b35716fb6",
            "title": "Ki-Cook: clustering multimodal cooking representations through knowledge-infused learning",
            "abstract": "Cross-modal recipe retrieval has gained prominence due to its ability to retrieve a text representation given an image representation and vice versa. Clustering these recipe representations based on similarity is essential to retrieve relevant information about unknown food images. Existing studies cluster similar recipe representations in the latent space based on class names. Due to inter-class similarity and intraclass variation, associating a recipe with a class name does not provide sufficient knowledge about recipes to determine similarity. However, recipe title, ingredients, and cooking actions provide detailed knowledge about recipes and are a better determinant of similar recipes. In this study, we utilized this additional knowledge of recipes, such as ingredients and recipe title, to identify similar recipes, emphasizing attention especially on rare ingredients. To incorporate this knowledge, we propose a knowledge-infused multimodal cooking representation learning network, Ki-Cook, built on the procedural attribute of the cooking process. To the best of our knowledge, this is the first study to adopt a comprehensive recipe similarity determinant to identify and cluster similar recipe representations. The proposed network also incorporates ingredient images to learn multimodal cooking representation. Since the motivation for clustering similar recipes is to retrieve relevant information for an unknown food image, we evaluated the ingredient retrieval task. We performed an empirical analysis to establish that our proposed model improves the Coverage of Ground Truth by 12% and the Intersection Over Union by 10% compared to the baseline models. On average, the representations learned by our model contain an additional 15.33% of rare ingredients compared to the baseline models. Owing to this difference, our qualitative evaluation shows a 39% improvement in clustering similar recipes in the latent space compared to the baseline models, with an inter-annotator agreement of the Fleiss kappa score of 0.35.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "40977746",
                    "name": "Swati Padhee"
                },
                {
                    "authorId": "2224857413",
                    "name": "Saini Rohan Rao"
                },
                {
                    "authorId": "2042283024",
                    "name": "Ronak Kaoshik"
                },
                {
                    "authorId": "2309490506",
                    "name": "Anirudh Sundara Rajan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "853fc07ed068a0e58ef6c80186eb080df73251ce",
            "title": "Process Knowledge-infused Learning for Clinician-friendly Explanations",
            "abstract": "Language models have the potential to assess mental health using social media data. By analyzing online posts and conversations, these models can detect patterns indicating mental health conditions like depression, anxiety, or suicidal thoughts. They examine keywords, language markers, and sentiment to gain insights into an individual\u2019s mental well-being. This information is crucial for early detection, intervention, and support, improving mental health care and prevention strategies. However, using language models for mental health assessments from social media has two limitations: (1) They do not compare posts against clinicians\u2019 diagnostic processes, and (2) It\u2019s challenging to explain language model outputs using concepts that the clinician can understand, i.e., clinician-friendly explanations. In this study, we introduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm that layers clinical process knowledge structures on language model outputs, enabling clinician-friendly explanations of the underlying language model predictions. We rigorously test our methods on existing benchmark datasets, augmented with such clinical process knowledge, and release a new dataset for assessing suicidality. PKiL performs competitively, achieving a 70% agreement with users, while other XAI methods only achieve 47% agreement (average inter-rater agreement of 0.72). Our evaluations demonstrate that PK-iL effectively explains model predictions to clinicians.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2187301061",
                    "name": "Yuxin Zi"
                },
                {
                    "authorId": "1491238594",
                    "name": "Manas Gaur"
                },
                {
                    "authorId": "2213301515",
                    "name": "Jinendra Malekar"
                },
                {
                    "authorId": "2145908831",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2068096390",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "8e92ca0de14fca53c71892c16a99c69bbe5cf169",
            "title": "Knowledge-Infused Self Attention Transformers",
            "abstract": "Transformer-based language models have achieved impressive success in various natural language processing tasks due to their ability to capture complex dependencies and contextual information using self-attention mechanisms. However, they are not without limitations. These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users. These limitations stem from the absence of implicit and missing context in the data alone. To address this, researchers have explored augmenting these models with external knowledge from knowledge graphs to provide the necessary additional context. However, the ad-hoc nature of existing methods makes it difficult to properly analyze the effects of knowledge infusion on the many moving parts or components of a transformer. This paper introduces a systematic method for infusing knowledge into different components of a transformer-based model. A modular framework is proposed to identify specific components within the transformer architecture, such as the self-attention mechanism, encoder layers, or the input embedding layer, where knowledge infusion can be applied. Additionally, extensive experiments are conducted on the General Language Understanding Evaluation (GLUE) benchmark tasks, and the findings are reported. This systematic approach aims to facilitate more principled approaches to incorporating knowledge into language model architectures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2187301061",
                    "name": "Yuxin Zi"
                },
                {
                    "authorId": "2068096390",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "1491238594",
                    "name": "Manas Gaur"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "9ceed28b20acfcc7e8e3ddee519eb11d7f2aef86",
            "title": "ProKnow: Process knowledge for safety constrained and explainable question generation for mental health diagnostic assistance",
            "abstract": "Virtual Mental Health Assistants (VMHAs) are utilized in health care to provide patient services such as counseling and suggestive care. They are not used for patient diagnostic assistance because they cannot adhere to safety constraints and specialized clinical process knowledge (ProKnow) used to obtain clinical diagnoses. In this work, we define ProKnow as an ordered set of information that maps to evidence-based guidelines or categories of conceptual understanding to experts in a domain. We also introduce a new dataset of diagnostic conversations guided by safety constraints and ProKnow that healthcare professionals use (ProKnow-data). We develop a method for natural language question generation (NLG) that collects diagnostic information from the patient interactively (ProKnow-algo). We demonstrate the limitations of using state-of-the-art large-scale language models (LMs) on this dataset. ProKnow-algo incorporates the process knowledge through explicitly modeling safety, knowledge capture, and explainability. As computational metrics for evaluation do not directly translate to clinical settings, we involve expert clinicians in designing evaluation metrics that test four properties: safety, logical coherence, and knowledge capture for explainability while minimizing the standard cross entropy loss to preserve distribution semantics-based similarity to the ground truth. LMs with ProKnow-algo generated 89% safer questions in the depression and anxiety domain (tested property: safety). Further, without ProKnow-algo generations question did not adhere to clinical process knowledge in ProKnow-data (tested property: knowledge capture). In comparison, ProKnow-algo-based generations yield a 96% reduction in our metrics to measure knowledge capture. The explainability of the generated question is assessed by computing similarity with concepts in depression and anxiety knowledge bases. Overall, irrespective of the type of LMs, ProKnow-algo achieved an averaged 82% improvement over simple pre-trained LMs on safety, explainability, and process-guided question generation. For reproducibility, we will make ProKnow-data and the code repository of ProKnow-algo publicly available upon acceptance.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "1491238594",
                    "name": "Manas Gaur"
                },
                {
                    "authorId": "2199859429",
                    "name": "Misagh Soltani"
                },
                {
                    "authorId": "9460529",
                    "name": "Vipula Rawte"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        }
    ]
}