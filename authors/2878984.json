{
    "authorId": "2878984",
    "papers": [
        {
            "paperId": "020054bdd2300805599471347f2e283d982c4b73",
            "title": "REAL Sampling: Boosting Factuality and Diversity of Open-Ended Generation via Asymptotic Entropy",
            "abstract": "Decoding methods for large language models (LLMs) usually struggle with the tradeoff between ensuring factuality and maintaining diversity. For example, a higher p threshold in the nucleus (top-p) sampling increases the diversity but decreases the factuality, and vice versa. In this paper, we propose REAL (Residual Entropy from Asymptotic Line) sampling, a decoding method that achieves improved factuality and diversity over nucleus sampling by predicting an adaptive threshold of $p$. Specifically, REAL sampling predicts the step-wise likelihood of an LLM to hallucinate, and lowers the p threshold when an LLM is likely to hallucinate. Otherwise, REAL sampling increases the p threshold to boost the diversity. To predict the step-wise hallucination likelihood without supervision, we construct a Token-level Hallucination Forecasting (THF) model to predict the asymptotic entropy (i.e., inherent uncertainty) of the next token by extrapolating the next-token entropies from a series of LLMs with different sizes. If a LLM's entropy is higher than the asymptotic entropy (i.e., the LLM is more uncertain than it should be), the THF model predicts a high hallucination hazard, which leads to a lower p threshold in REAL sampling. In the FactualityPrompts benchmark, we demonstrate that REAL sampling based on a 70M THF model can substantially improve the factuality and diversity of 7B LLMs simultaneously, judged by both retrieval-based metrics and human evaluation. After combined with contrastive decoding, REAL sampling outperforms 9 sampling methods, and generates texts that are more factual than the greedy sampling and more diverse than the nucleus sampling with $p=0.5$. Furthermore, the predicted asymptotic entropy is also a useful unsupervised signal for hallucination detection tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2306142084",
                    "name": "Haw-Shiuan Chang"
                },
                {
                    "authorId": "2300284348",
                    "name": "Nanyun Peng"
                },
                {
                    "authorId": "2300284478",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2305823305",
                    "name": "Anil Ramakrishna"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                }
            ]
        },
        {
            "paperId": "600f9864794d79b9801e3ebef90efb410776f606",
            "title": "Prompting Vision-Language Models For Aspect-Controlled Generation of Referring Expressions",
            "abstract": "Referring Expression Generation (REG) is the task of generating a description that unambiguously identifies a given target in the scene. Different from Image Captioning (IC), REG requires learning fine-grained characteristics of not only the scene objects but also their surrounding context. Referring expressions are usually not singular; an object can often be uniquely referenced in numerous ways, for instance, by color, by location, or by relationship with other objects. Most prior works, however, have not explored this \u2018aspect-based mul-tiplicity\u2019 of referring expressions. Hence, in this work, we focus on the Aspect-Controlled REG task, which requires generating a referring expression conditioned on the input aspect(s), where an aspect captures a style of reference. By changing the input aspect such as color, location, action etc., one can generate multiple distinct expressions per target region. To solve this new task, we first modify BLIP (Li et al., 2022a) for aligning image-regions and text-expressions. We achieve this through a novel approach for feeding the input by drawing a bounding box around the target image-region and prompting the model to generate the referring expression. Our base REG model already beats all prior works in CIDEr score. To tackle Aspect-Controlled REG, we append \u2018aspect tokens\u2019 to the prompt and show that distinct expressions can be generated by just changing the prompt. Finally, to prove the high-quality and diversity of the data generated by our proposed aspect-controlled REG model, we also perform data-augmentation-based evaluation",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308643996",
                    "name": "Danfeng Guo"
                },
                {
                    "authorId": "2308134588",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "2258670815",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "2308127367",
                    "name": "Emre Barut"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2217929400",
                    "name": "Jing Huang"
                },
                {
                    "authorId": "2300284478",
                    "name": "Mohit Bansal"
                }
            ]
        },
        {
            "paperId": "10243faeb3db176f23ecf24568875142b8382e8d",
            "title": "Unsupervised Melody-to-Lyrics Generation",
            "abstract": "Automatic melody-to-lyric generation is a task in which song lyrics are generated to go with a given melody. It is of significant practical interest and more challenging than unconstrained lyric generation as the music imposes additional constraints onto the lyrics. The training data is limited as most songs are copyrighted, resulting in models that underfit the complicated cross-modal relationship between melody and lyrics. In this work, we propose a method for generating high-quality lyrics without training on any aligned melody-lyric data. Specifically, we design a hierarchical lyric generation framework that first generates a song outline and second the complete lyrics. The framework enables disentanglement of training (based purely on text) from inference (melody-guided text generation) to circumvent the shortage of parallel data.We leverage the segmentation and rhythm alignment between melody and lyrics to compile the given melody into decoding constraints as guidance during inference. The two-step hierarchical design also enables content control via the lyric outline, a much-desired feature for democratizing collaborative song creation. Experimental results show that our model can generate high-quality lyrics that are more on-topic, singable, intelligible, and coherent than strong baselines, for example SongMASS, a SOTA model trained on a parallel dataset, with a 24% relative overall quality improvement based on human ratings. Our code is available at https://github.com/amazon-science/unsupervised-melody-to-lyrics-generation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "48391421",
                    "name": "Yufei Tian"
                },
                {
                    "authorId": "1414599717",
                    "name": "Anjali Narayan-Chen"
                },
                {
                    "authorId": "3114640",
                    "name": "Shereen Oraby"
                },
                {
                    "authorId": "32015706",
                    "name": "Alessandra Cervone"
                },
                {
                    "authorId": "34280810",
                    "name": "Gunnar A. Sigurdsson"
                },
                {
                    "authorId": "46387857",
                    "name": "Chenyang Tao"
                },
                {
                    "authorId": "1902008743",
                    "name": "Wenbo Zhao"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2217929400",
                    "name": "Jing Huang"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "7d3b96b370ab105c8c9302cbeeb0f5e94bdabbc8",
            "title": "SPC: Soft Prompt Construction for Cross Domain Generalization",
            "abstract": "Recent advances in prompt tuning have proven effective as a new language modeling paradigm for various natural language understanding tasks. However, it is challenging to adapt the soft prompt embeddings to different domains or generalize to low-data settings when learning soft prompts itself is unstable, task-specific, and bias-prone. This paper proposes a principled learning framework\u2014soft prompt construction (SPC)\u2014to facilitate learning domain-adaptable soft prompts. Derived from the SPC framework is a simple loss that can plug into various models and tuning approaches to improve their cross-domain performance. We show SPC can improve upon SOTA for contextual query rewriting, summarization, and paraphrase detection by up to 5%, 19%, and 16%, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1902008743",
                    "name": "Wenbo Zhao"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2217929400",
                    "name": "Jing Huang"
                }
            ]
        },
        {
            "paperId": "d71c20c56f0fb86dd9eb7eb34b7607ce286ec450",
            "title": "Unsupervised Melody-Guided Lyrics Generation",
            "abstract": "Automatic song writing is a topic of significant practical interest. However, its research is largely hindered by the lack of training data due to copyright concerns and challenged by its creative nature. Most noticeably, prior works often fall short of modeling the cross-modal correlation between melody and lyrics due to limited parallel data, hence generating lyrics that are less singable. Existing works also lack effective mechanisms for content control, a much desired feature for democratizing song creation for people with limited music background. In this work, we propose to generate pleasantly listenable lyrics without training on melody-lyric aligned data. Instead, we design a hierarchical lyric generation framework that disentangles training (based purely on text) from inference (melody-guided text generation). At inference time, we leverage the crucial alignments between melody and lyrics and compile the given melody into constraints to guide the generation process. Evaluation results show that our model can generate high-quality lyrics that are more singable, intelligible, coherent, and in rhyme than strong baselines including those supervised on parallel data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48391421",
                    "name": "Yufei Tian"
                },
                {
                    "authorId": "1414599717",
                    "name": "Anjali Narayan-Chen"
                },
                {
                    "authorId": "3114640",
                    "name": "Shereen Oraby"
                },
                {
                    "authorId": "32015706",
                    "name": "Alessandra Cervone"
                },
                {
                    "authorId": "34280810",
                    "name": "Gunnar A. Sigurdsson"
                },
                {
                    "authorId": "46387857",
                    "name": "Chenyang Tao"
                },
                {
                    "authorId": "50771481",
                    "name": "Wenbo Zhao"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2217929400",
                    "name": "Jing Huang"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "e65f6420f0876d4681dc11a0163e12a8a236dc5a",
            "title": "Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning",
            "abstract": "Dialogue state tracking (DST) is an important step in dialogue management to keep track of users' beliefs. Existing works fine-tune all language model (LM) parameters to tackle the DST task, which requires significant data and computing resources for training and hosting. The cost grows exponentially in the real-world deployment where dozens of fine-tuned LM are used for different domains and tasks. To reduce parameter size and better utilize cross-task shared information, we propose to use soft prompt token embeddings to learn task properties. Without tuning LM parameters, our method drastically reduces the number of parameters needed to less than 0.5% of prior works while achieves better low-resource DST performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "ef4bd399a66f6e2b4a0b6bfe2aa2063eaef10375",
            "title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models",
            "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form optimal solution with reasonable modeling capacity. Despite the success, several challenges arise when apply NADO to a wide range of scenarios. Vanilla NADO suffers from gradient vanishing for low-probability control signals and is highly reliant on a regularization to satisfy the stochastic version of Bellman equation. In addition, the vanilla implementation of NADO introduces a few additional transformer layers, suffering from a limited capacity especially compared to other finetune-based model adaptation methods like LoRA. In this paper, we propose a improved version of the NADO algorithm, namely DiNADO (norm-Disentangled NeurAlly-Decomposed Oracles), which improves the performance of the NADO algorithm through disentangling the step-wise global norm over the approximated oracle $R$-value for all potential next-tokens, allowing DiNADO to be combined with finetuning methods like LoRA. We discuss in depth how DiNADO achieves better capacity, stability and flexibility with both empirical and theoretical results. Experiments on formality control in machine translation and the lexically constrained generation task CommonGen demonstrates the significance of the improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167235968",
                    "name": "Sidi Lu"
                },
                {
                    "authorId": "1902008743",
                    "name": "Wenbo Zhao"
                },
                {
                    "authorId": "46387857",
                    "name": "Chenyang Tao"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "48914934",
                    "name": "Shan Wu"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "f925109f1c7591edbcd85f0df41d417bcecfc426",
            "title": "Mitigating Bias for Question Answering Models by Tracking Bias Influence",
            "abstract": "Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applied to multiple QA formulations across multiple bias categories. It can significantly reduce the bias level in all 9 bias categories in the BBQ dataset while maintaining comparable QA accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "2258670815",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "2258671169",
                    "name": "Yu-Hsiang Lin"
                },
                {
                    "authorId": "2258697911",
                    "name": "Wenbo Zhao"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2256611613",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2257127887",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2256996328",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "7e5712806ec37a554914dc8dae19fe57b9645763",
            "title": "GRAVL-BERT: Graphical Visual-Linguistic Representations for Multimodal Coreference Resolution",
            "abstract": "Learning from multimodal data has become a popular research topic in recent years. Multimodal coreference resolution (MCR) is an important task in this area. MCR involves resolving the references across different modalities, e.g., text and images, which is a crucial capability for building next-generation conversational agents. MCR is challenging as it requires encoding information from different modalities and modeling associations between them. Although significant progress has been made for visual-linguistic tasks such as visual grounding, most of the current works involve single turn utterances and focus on simple coreference resolutions. In this work, we propose an MCR model that resolves coreferences made in multi-turn dialogues with scene images. We present GRAVL-BERT, a unified MCR framework which combines visual relationships between objects, background scenes, dialogue, and metadata by integrating Graph Neural Networks with VL-BERT. We present results on the SIMMC 2.0 multimodal conversational dataset, achieving the rank-1 on the DSTC-10 SIMMC 2.0 MCR challenge with F1 score 0.783. Our code is available at https://github.com/alexa/gravl-bert.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068107537",
                    "name": "Danfeng Guo"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "150290293",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2052990580",
                    "name": "Arijit Biswas"
                },
                {
                    "authorId": "2116675467",
                    "name": "Chien-Wei Lin"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ]
        },
        {
            "paperId": "a66b091b71f862dc8f8865e9e1cc1249a3d13252",
            "title": "Context-Situated Pun Generation",
            "abstract": "Previous work on pun generation commonly begins with a given pun word (a pair of homophones for heterographic pun generation and a polyseme for homographic pun generation) and seeks to generate an appropriate pun. While this may enable efficient pun generation, we believe that a pun is most entertaining if it fits appropriately within a given context, e.g., a given situation or dialogue. In this work, we propose a new task, context-situated pun generation, where a specific context represented by a set of keywords is provided, and the task is to first identify suitable pun words that are appropriate for the context, then generate puns based on the context keywords and the identified pun words. We collect a new dataset, CUP (Context-sitUated Pun), containing 4.5k tuples of context words and pun pairs. Based on the new data and setup, we propose a pipeline system for context-situated pun generation, including a pun word retrieval module that identifies suitable pun words for a given context, and a pun generation module that generates puns from context keywords and pun words. Human evaluation shows that 69% of our top retrieved pun words can be used to generate context-situated puns, and our generation module yields successful puns 31% of the time given a plausible tuple of context words and pun pair, almost tripling the yield of a state-of-the-art pun generation model. With an end-to-end evaluation, our pipeline system with the top-1 retrieved pun pair for a given context can generate successful puns 40% of the time, better than all other modeling variations but 32% lower than the human success rate. This highlights the difficulty of the task, and encourages more research in this direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145478138",
                    "name": "Jiao Sun"
                },
                {
                    "authorId": "1414599717",
                    "name": "Anjali Narayan-Chen"
                },
                {
                    "authorId": "3114640",
                    "name": "Shereen Oraby"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "49025105",
                    "name": "Jing-huan Huang"
                },
                {
                    "authorId": "2152802662",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        }
    ]
}