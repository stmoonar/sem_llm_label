{
    "authorId": "52154863",
    "papers": [
        {
            "paperId": "07c53cf78f0ec5b95e89dd4fc4f5774ab486c4b5",
            "title": "DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages",
            "abstract": "Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for language varieties and one step towards advancing it further. Code/data: https://github.com/ffaisal93/DialectBench",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48556979",
                    "name": "FAHIM FAISAL"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2169175069",
                    "name": "Aarohi Srivastava"
                },
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "2263760783",
                    "name": "David Chiang"
                },
                {
                    "authorId": "2287930119",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2273733474",
                    "name": "Antonios Anastasopoulos"
                }
            ]
        },
        {
            "paperId": "09d75de9475a516e0d24ca09931b4269480a9453",
            "title": "BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English",
            "abstract": "In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam questions for middle and high school levels in Bangladesh. Our dataset consists of approximately 5K questions covering several subjects in science with different types of questions, including factual, application, and reasoning-based questions. We benchmark several Large Language Models (LLMs) with our parallel dataset and observe a notable performance disparity between the models in Bengali and English. We also investigate some prompting methods, and find that Chain-of-Thought prompting is beneficial mostly on reasoning questions, but not so much on factual ones. We also find that appending English translation helps to answer questions in Bengali. Our findings point to promising future research directions for improving the performance of LLMs in Bengali and more generally in low-resource languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292040447",
                    "name": "H. M. Q. H. Sheikh Shafayat"
                },
                {
                    "authorId": "2292090396",
                    "name": "Minhajur Rahman"
                },
                {
                    "authorId": "2292040215",
                    "name": "Chowdhury Mahim"
                },
                {
                    "authorId": "2292034518",
                    "name": "Rifki Afina"
                },
                {
                    "authorId": "2292036957",
                    "name": "James Putri"
                },
                {
                    "authorId": "2292036951",
                    "name": "Alice Thorne"
                },
                {
                    "authorId": "2292033561",
                    "name": "Oh"
                },
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "35831406",
                    "name": "Rishav Hada"
                },
                {
                    "authorId": "104014275",
                    "name": "Millicent Ochieng"
                },
                {
                    "authorId": "2292042313",
                    "name": "Prachi Jain"
                },
                {
                    "authorId": "2135014756",
                    "name": "Harshita Diddee"
                },
                {
                    "authorId": "2212460959",
                    "name": "Samuel Maina"
                },
                {
                    "authorId": "1785978",
                    "name": "T. Ganu"
                },
                {
                    "authorId": "2242814985",
                    "name": "Sameer Segal"
                },
                {
                    "authorId": "2212463954",
                    "name": "Maxamed Axmed"
                },
                {
                    "authorId": "3086996",
                    "name": "Kalika Bali"
                },
                {
                    "authorId": "2279986783",
                    "name": "Jonathan H. Choi"
                },
                {
                    "authorId": "2279921858",
                    "name": "Kristin E. Hickman"
                },
                {
                    "authorId": "2247943406",
                    "name": "Amy Monahan"
                },
                {
                    "authorId": "2265493391",
                    "name": "Peter Clark"
                },
                {
                    "authorId": "3390191",
                    "name": "Isaac Cowhey"
                },
                {
                    "authorId": "2282542651",
                    "name": "O. Etzioni"
                },
                {
                    "authorId": "2236429",
                    "name": "Tushar Khot"
                },
                {
                    "authorId": "48229640",
                    "name": "Ashish Sabharwal"
                },
                {
                    "authorId": "3393851",
                    "name": "Carissa Schoenick"
                },
                {
                    "authorId": "2292036520",
                    "name": "Oyvind Tafjord. 2018"
                },
                {
                    "authorId": "6062736",
                    "name": "K. Cobbe"
                },
                {
                    "authorId": "13622184",
                    "name": "V. Kosaraju"
                },
                {
                    "authorId": "2275251620",
                    "name": "Mo Bavarian"
                },
                {
                    "authorId": "2292054802",
                    "name": "Mark Chen"
                },
                {
                    "authorId": "35450887",
                    "name": "Heewoo Jun"
                },
                {
                    "authorId": "2275230678",
                    "name": "Lukasz Kaiser"
                },
                {
                    "authorId": "3407285",
                    "name": "Matthias Plappert"
                },
                {
                    "authorId": "2065005836",
                    "name": "Jerry Tworek"
                },
                {
                    "authorId": "2286540856",
                    "name": "Jacob Hilton"
                },
                {
                    "authorId": "7406311",
                    "name": "Reiichiro Nakano"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "2256987672",
                    "name": "Nurul Aisyah"
                },
                {
                    "authorId": "2282714065",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2076041174",
                    "name": "Tiffany H. Kung"
                },
                {
                    "authorId": "40315563",
                    "name": "Morgan Cheatham"
                },
                {
                    "authorId": "2036654064",
                    "name": "Arielle Medenilla"
                },
                {
                    "authorId": "2205202770",
                    "name": "Czarina Sillos"
                },
                {
                    "authorId": "2292036703",
                    "name": "Lorie De Leon"
                },
                {
                    "authorId": "2205195617",
                    "name": "Camille Elepa\u00f1o"
                },
                {
                    "authorId": "2205195620",
                    "name": "Maria Madriaga"
                },
                {
                    "authorId": "2205195054",
                    "name": "Rimel Aggabao"
                },
                {
                    "authorId": "2205233740",
                    "name": "Giezel Diaz-Candido"
                },
                {
                    "authorId": "2205202767",
                    "name": "James Maningo"
                },
                {
                    "authorId": "2292035884",
                    "name": "V. Dac"
                },
                {
                    "authorId": "2292022310",
                    "name": "Trung Ngo"
                },
                {
                    "authorId": "2285453048",
                    "name": "Amir Pouran"
                },
                {
                    "authorId": "2292035577",
                    "name": "Ben Vey-seh"
                },
                {
                    "authorId": "2062898274",
                    "name": "Hieu Man"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2265648617",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "2165171109",
                    "name": "T. Huu"
                },
                {
                    "authorId": "2272980805",
                    "name": "ChatGPT"
                }
            ]
        },
        {
            "paperId": "0d72949f3ccbd5d36974028299093632b3cb53f6",
            "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
            "abstract": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model in order to maintain the performance on the downstream task. In this work, we aim to modularly debias a pre-trained language model across multiple dimensions. Previous works extensively explored debiasing PLMs by using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of the approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3094662",
                    "name": "Prachi Jain"
                },
                {
                    "authorId": "2266397701",
                    "name": "Ashutosh Sathe"
                },
                {
                    "authorId": "2140408530",
                    "name": "Varun Gumma"
                },
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "2256989615",
                    "name": "Sunayana Sitaram"
                }
            ]
        },
        {
            "paperId": "9b9e44d43eb6bc0be7a07a92b2100dd6a7f3b158",
            "title": "Learning Syntax Without Planting Trees: Understanding When and Why Transformers Generalize Hierarchically",
            "abstract": "Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge. We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically. We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure. When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order). Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2030973547",
                    "name": "Madhur Panwar"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2298441996",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2286065792",
                    "name": "Navin Goyal"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "62ad7ea9467bbcdbfe325b9ee561cab3908e4583",
            "title": "MEGA: Multilingual Evaluation of Generative AI",
            "abstract": "Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "35831406",
                    "name": "Rishav Hada"
                },
                {
                    "authorId": "104014275",
                    "name": "Millicent Ochieng"
                },
                {
                    "authorId": "3094662",
                    "name": "Prachi Jain"
                },
                {
                    "authorId": "2029651808",
                    "name": "Harshita Diddee"
                },
                {
                    "authorId": "1455192856",
                    "name": "Krithika Ramesh"
                },
                {
                    "authorId": "2212460959",
                    "name": "Samuel Maina"
                },
                {
                    "authorId": "1785978",
                    "name": "T. Ganu"
                },
                {
                    "authorId": "2242814985",
                    "name": "Sameer Segal"
                },
                {
                    "authorId": "2212463954",
                    "name": "Maxamed Axmed"
                },
                {
                    "authorId": "3086996",
                    "name": "Kalika Bali"
                },
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                }
            ]
        },
        {
            "paperId": "9b71c89686334ba4f1247aa18990740a94e25cc3",
            "title": "Breaking Language Barriers with a LEAP: Learning Strategies for Polyglot LLMs",
            "abstract": "Large language models (LLMs) are at the forefront of transforming numerous domains globally. However, their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages. This paper tackles the imperative challenge of enhancing the multilingual performance of LLMs, specifically focusing on Generative models. Through systematic investigation and evaluation of diverse languages using popular question-answering (QA) datasets, we present novel techniques that unlock the true potential of LLMs in a polyglot landscape. Our approach encompasses three key strategies that yield remarkable improvements in multilingual proficiency. First, by meticulously optimizing prompts tailored for polyglot LLMs, we unlock their latent capabilities, resulting in substantial performance boosts across languages. Second, we introduce a new hybrid approach that synergizes GPT generation with multilingual embeddings and achieves significant multilingual performance improvement on critical tasks like QA and retrieval. Finally, to further propel the performance of polyglot LLMs, we introduce a novel learning algorithm that dynamically selects the optimal prompt strategy, LLM model, and embeddings per query. This dynamic adaptation maximizes the efficacy of LLMs across languages, outperforming best static and random strategies. Our results show substantial advancements in multilingual understanding and generation across a diverse range of languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51464520",
                    "name": "A. Nambi"
                },
                {
                    "authorId": "1564592237",
                    "name": "Vaibhav Balloli"
                },
                {
                    "authorId": "1381199788",
                    "name": "M. Ranjit"
                },
                {
                    "authorId": "1785978",
                    "name": "T. Ganu"
                },
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                },
                {
                    "authorId": "3086996",
                    "name": "Kalika Bali"
                }
            ]
        },
        {
            "paperId": "c4cb9f0145cebb169fbdf235fb1f76e1e5b82e72",
            "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
            "abstract": "While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2063799722",
                    "name": "Aniket Vashishtha"
                },
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                }
            ]
        },
        {
            "paperId": "da7627dd7769210fc6ec917d13b00365d0989c97",
            "title": "Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world",
            "abstract": "This tutorial will describe various aspects of scaling up language technologies to many of the world\u2019s languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also talk about some of the real-world challenges in deploying these models in language communities in the field. With the performance of MMLMs improving in the zero-shot setting for many languages, it is now becoming feasible to use them for building language technologies in many languages of the world, and this tutorial will provide the computational linguistics community with unique insights from the latest research in multilingual models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                },
                {
                    "authorId": "143990839",
                    "name": "M. Choudhury"
                },
                {
                    "authorId": "27419446",
                    "name": "Barun Patra"
                },
                {
                    "authorId": "113810201",
                    "name": "Vishrav Chaudhary"
                },
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "3086996",
                    "name": "Kalika Bali"
                }
            ]
        },
        {
            "paperId": "f4d543ff431359947bf41152ac01233b8062221f",
            "title": "In-Context Learning through the Bayesian Prism",
            "abstract": "In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "48248617",
                    "name": "Madhuri Panwar"
                },
                {
                    "authorId": "144260125",
                    "name": "Navin Goyal"
                }
            ]
        },
        {
            "paperId": "04207549bf872158d117600029dbe1f1cf8e5b59",
            "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
            "abstract": "Although recent Massively Multilingual Language Models (MMLMs) like mBERT and XLMR support around 100 languages, most existing multilingual NLP benchmarks provide evaluation data in only a handful of these languages with little linguistic diversity. We argue that this makes the existing practices in multilingual evaluation unreliable and does not provide a full picture of the performance of MMLMs across the linguistic landscape. We propose that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages. We compare performance prediction with translating test data with a case study on four different multilingual datasets, and observe that these methods can provide reliable estimates of the performance that are often on-par with the translation based approaches, without the need for any additional translation as well as evaluation costs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "34725175",
                    "name": "Sandipan Dandapat"
                },
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                },
                {
                    "authorId": "143990839",
                    "name": "M. Choudhury"
                }
            ]
        }
    ]
}