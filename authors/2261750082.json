{
    "authorId": "2261750082",
    "papers": [
        {
            "paperId": "237bfa636f1a575f4784d2ae81a47ac29fa38522",
            "title": "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning",
            "abstract": "Large Language Models (LLMs) demonstrate remarkable proficiency in comprehending and handling text-based tasks. Many efforts are being made to transfer these attributes to video modality, which are termed Video-LLMs. However, existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments. In light of these challenges, we propose Momentor, a Video-LLM capable of accomplishing fine-grained temporal understanding tasks. To support the training of Momentor, we design an automatic data generation engine to construct Moment-10M, a large-scale video instruction dataset with segment-level instruction data. We train Momentor on Moment-10M, enabling it to perform segment-level reasoning and localization. Zero-shot evaluations on several tasks demonstrate that Momentor excels in fine-grained temporally grounded comprehension and localization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159713431",
                    "name": "Long Qian"
                },
                {
                    "authorId": "2249538542",
                    "name": "Juncheng Li"
                },
                {
                    "authorId": "2279975706",
                    "name": "Yu Wu"
                },
                {
                    "authorId": "2284935143",
                    "name": "Yaobo Ye"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2281747744",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                }
            ]
        },
        {
            "paperId": "9e6547e841754684b1c157cbea707d0f3f0fad60",
            "title": "Multi-view Counterfactual Contrastive Learning for Fact-checking Fake News Detection",
            "abstract": "Fact-checking fake news detection involves using verified accurate factual information in news reports as \"evidence\" to validate objective statement \"claim\". Existing works primarily focus on identifying critical elements within the evidence that support or refute specific claims by assessing the congruence or divergence between the claim and the associated evidence. These methods can broadly be divided into text-based and graph-based-the former centers on understanding the nuances of unstructured text to extract semantic word-level information. At the same time, the latter is proficient at analyzing the node-level structure of graphs it creates from the text to reveal topological insights. Each type provides a distinct view on identifying critical elements for fact-checking. To enhance the complementary nature of the two perspectives, this paper proposes an end-to-end framework for fact-checking fake news detection entitled Multi-view Counterfactual Contrastive Learning (MCCL). The framework incorporates a counterfactual technique to refine the fused features from both the \"entity-view\" of textual content and the \"centrality-view\" of the graph structure. Additionally, it employs contrastive learning to sharpen the distinctions among multi-view features, which facilitates the exact identification of critical elements in the evidence related to their respective claims. Experimental results on real datasets demonstrate that the proposed MCCL outperforms state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305456471",
                    "name": "Yongcheng Zhang"
                },
                {
                    "authorId": "2305443422",
                    "name": "Lingou Kong"
                },
                {
                    "authorId": "2305514579",
                    "name": "Sheng Tian"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2305401343",
                    "name": "Changpeng Xiang"
                },
                {
                    "authorId": "2261899704",
                    "name": "Huan Wang"
                },
                {
                    "authorId": "2275845778",
                    "name": "Xiaomei Wei"
                }
            ]
        },
        {
            "paperId": "04ebb7c24f1b2b8991031958289c7d2f2b0d2214",
            "title": "Towards Complex-query Referring Image Segmentation: A Novel Benchmark",
            "abstract": "Referring Image Understanding (RIS) has been extensively studied over the past decade, leading to the development of advanced algorithms. However, there has been a lack of research investigating how existing algorithms should be benchmarked with complex language queries, which include more informative descriptions of surrounding objects and backgrounds (\\eg \\textit{\"the black car.\"} vs. \\textit{\"the black car is parking on the road and beside the bus.\"}). Given the significant improvement in the semantic understanding capability of large pre-trained models, it is crucial to take a step further in RIS by incorporating complex language that resembles real-world applications. To close this gap, building upon the existing RefCOCO and Visual Genome datasets, we propose a new RIS benchmark with complex queries, namely \\textbf{RIS-CQ}. The RIS-CQ dataset is of high quality and large scale, which challenges the existing RIS with enriched, specific and informative queries, and enables a more realistic scenario of RIS research. Besides, we present a nichetargeting method to better task the RIS-CQ, called dual-modality graph alignment model (\\textbf{\\textsc{DuMoGa}}), which outperforms a series of RIS methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144540018",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2156060734",
                    "name": "Li Li"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2249738557",
                    "name": "Xiangyan Liu"
                },
                {
                    "authorId": "2269729307",
                    "name": "Xu Yang"
                },
                {
                    "authorId": "2249538542",
                    "name": "Juncheng Li"
                },
                {
                    "authorId": "2249532794",
                    "name": "Roger Zimmermann"
                }
            ]
        },
        {
            "paperId": "06673e37f8d8a515d6fe26511dd1fbae69a9237d",
            "title": "Partial Annotation-based Video Moment Retrieval via Iterative Learning",
            "abstract": "Given a descriptive language query, Video Moment Retrieval (VMR) aims to seek the corresponding semantic-consistent moment clip in the video, which is represented as a pair of the start and end timestamps. Although current methods have achieved satisfying performance, training these models heavily relies on the fully-annotated VMR datasets. Nonetheless, precise video temporal annotations are extremely labor-intensive and ambiguous due to the diverse preferences of different annotators. Although there are several works trying to explore weakly supervised VMR tasks with scattered annotated frames as labels, there is still much room to improve in terms of accuracy. Therefore, we design a new setting of VMR where users can easily point to small segments of non-controversy video moments and our proposed method can automatically fill in the remaining parts based on the video and query semantics. To support this, we propose a new framework named Video Moment Retrieval via Iterative Learning (VMRIL). It treats the partial temporal region as the seed, then expands the pseudo label by iterative training. In order to restrict the expansion with reasonable boundaries, we utilize a pretrained video action localization model to provide coarse guidance of potential video segments. Compared with other VMR methods, our VMRIL achieves a trade-off between satisfying performance and annotation efficiency. Experimental results show that our proposed method can achieve the SOTA performance in the weakly supervised VMR setting, and are even comparable with some fully-supervised VMR methods but with much less annotation cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2261748535",
                    "name": "Renjie Liang"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "7f807249c0ef0fe07d5e9c810684cd5daba0edc5",
            "title": "De-fine: Decomposing and Refining Visual Programs with Auto-Feedback",
            "abstract": "Visual programming, a modular and generalizable paradigm, integrates different modules and Python operators to solve various vision-language tasks. Unlike end-to-end models that need task-specific data, it advances in performing visual processing and reasoning in an unsupervised manner. Current visual programming methods generate programs in a single pass for each task where the ability to evaluate and optimize based on feedback, unfortunately, is lacking, which consequentially limits their effectiveness for complex, multi-step problems. Drawing inspiration from benders decomposition, we introduce De-fine, a training-free framework that automatically decomposes complex tasks into simpler subtasks and refines programs through auto-feedback. This model-agnostic approach can improve logical reasoning performance by integrating the strengths of multiple models. Our experiments across various visual tasks show that De-fine creates more robust programs. Moreover, viewing each feedback module as an independent agent will yield fresh prospects for the field of agent research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2211467299",
                    "name": "Minghe Gao"
                },
                {
                    "authorId": "2261788275",
                    "name": "Juncheng Li"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2268315960",
                    "name": "Liang Pang"
                },
                {
                    "authorId": "2267729005",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2195413103",
                    "name": "Guoming Wang"
                },
                {
                    "authorId": "2108125912",
                    "name": "Wenqiao Zhang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                }
            ]
        },
        {
            "paperId": "e46dea661e82d2c2468dbdb758aae3e4ba878277",
            "title": "Deep Multimodal Learning for Information Retrieval",
            "abstract": "Information retrieval (IR) is a fundamental technique that aims to acquire information from a collection of documents, web pages, or other sources. While traditional text-based IR has achieved great success, the under-utilization of varied data sources in different modalities (i.e., text, images, audio, and video) would hinder IR techniques from giving its full advancement and thus limits the applications in the real world. Within recent years, the rapid development of deep multimodal learning paves the way for advancing IR with multi-modality. Benefiting from a variety of data types and modalities, some latest prevailing techniques are invented to show great facilitation in multi-modal and IR learning, such as CLIP, ChatGPT, GPT4, etc. In the context of IR, deep multi-modal learning has shown the prominent potential to improve the performance of retrieval systems, by enabling them to better understand and process the diverse types of data that they encounter. Given the great potential shown by multimodal-empowered IR, there can be still unsolved challenges and open questions in the related directions. With this workshop, we aim to provide a platform for discussion about multi-modal IR among scholars, practitioners, and other interested parties.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "1887997",
                    "name": "Yin-wei Wei"
                },
                {
                    "authorId": "2187311001",
                    "name": "Zhedong Zheng"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        }
    ]
}