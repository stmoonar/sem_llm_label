{
    "authorId": "1966497",
    "papers": [
        {
            "paperId": "df913c9400d61da45953e849514be568447b4572",
            "title": "PLAME: Piecewise-Linear Approximate Measure for Additive Kernel SVM",
            "abstract": "Additive Kernel SVM has been extensively used in many applications, including human activity detection and pedestrian detection. Since training an additive kernel SVM model is very time-consuming, which is not scalable to large-scale datasets, many efficient solutions have been developed in the past few years. However, most of the existing methods normally fail to achieve one of these three important conditions which are (1) low classification error, (2) low memory space, and (3) low training time. In order to simultaneously fulfill these three conditions, we develop the new piecewise-linear approximate measure (PLAME) for additive kernels. By incorporating PLAME with the well-known dual coordinate descent method, we theoretically show that this approach can achieve the above three conditions. Experimental results on twelve real datasets show that our approach can achieve the best trade-off between the accuracy, memory space, and training time compared with different types of state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2610571",
                    "name": "Tsz Nam Chan"
                },
                {
                    "authorId": "2211069719",
                    "name": "Zhe Li"
                },
                {
                    "authorId": "1830414684",
                    "name": "Leong Hou U"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "f6ef26036703cb045a6e55d120dfb5ef1b7c3ccd",
            "title": "Machine Learning for Subgraph Extraction: Methods, Applications and Challenges",
            "abstract": "Subgraphs are obtained by extracting a subset of vertices and a subset of edges from the associated original graphs, and many graph properties are known to be inherited by subgraphs. Subgraphs can be applied in many areas such as social networks, recommender systems, biochemistry and fraud discovery. Researchers from various communities have paid a great deal of attention to investigate numerous subgraph problems, by proposing algorithms that mainly extract important structures of a given graph. There are however some limitations that should be addressed, with regard to the efficiency, effectiveness and scalability of these traditional algorithms. As a consequence, machine learning techniques---one of the most latest trends---have recently been employed in the database community to address various subgraph problems considering that they have been shown to be beneficial in dealing with graph-related problems. We discuss learning-based approaches for four well known subgraph problems in this tutorial, namely subgraph isomorphism, maximum common subgraph, community detection and community search problems. We give a general description of each proposed model, and analyse its design and performance. To allow further investigations on relevant subgraph problems, we suggest some potential future directions in this area. We believe that this work can be used as one of the primary resources, for researchers who intend to develop learning models in solving problems that are closely related to subgraphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "102590041",
                    "name": "Kai Siong Yow"
                },
                {
                    "authorId": "1940000983",
                    "name": "Ningyi Liao"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "16402ca4c10344fe997a554f3592bcb1e0beb9da",
            "title": "A Convex-Programming Approach for Efficient Directed Densest Subgraph Discovery",
            "abstract": "Given a directed graph G, the directed densest subgraph (DDS) problem refers to finding a subgraph from G, whose density is the highest among all subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fake follower detection and community mining. Theoretically, the DDS problem closely connects to other essential graph problems, such as network flow and bipartite matching. However, existing DDS solutions suffer from efficiency and scalability issues. In this paper, we develop a convex-programming-based solution by transforming the DDS problem into a set of linear programs. Based on the duality of linear programs, we develop efficient exact and approximation algorithms. Especially, our approximation algorithm can support flexible parameterized approximation guarantees. We have performed an extensive empirical evaluation of our approaches on eight real large datasets. The results show that our proposed algorithms are up to five orders of magnitude faster than the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "151480433",
                    "name": "Xiaolin Han"
                }
            ]
        },
        {
            "paperId": "1e881c61eb38bddd4e527bbe0e851c6d86951bed",
            "title": "Efficient Algorithms for Kernel Aggregation Queries",
            "abstract": "Kernel functions support a broad range of applications that require tasks like density estimation, classification, regression or outlier detection. For these tasks, a common online operation is to compute the weighted aggregation of kernel function values with respect to a set of points. However, scalable aggregation methods are still unknown for typical kernel functions (e.g., Gaussian kernel, polynomial kernel, sigmoid kernel and additive kernels) and weighting schemes. In this paper, we propose a novel and effective bounding technique, by leveraging index structures, to speed up the computation of kernel aggregation. In addition, we extend our technique to additive kernel functions, including <inline-formula><tex-math notation=\"LaTeX\">$\\chi ^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>\u03c7</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"chan-ieq1-3018376.gif\"/></alternatives></inline-formula>, intersection, JS and Hellinger kernels, which are widely used in different communities, e.g., computer vision, medical science, Geoscience etc. To handle the additive kernel functions, we further develop the novel and effective bound functions to efficiently evaluate the kernel aggregation. Experimental studies on many real datasets reveal that our proposed solution KARL achieves at least one order of magnitude speedup over the state-of-the-art for different types of kernel functions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2610571",
                    "name": "Tsz Nam Chan"
                },
                {
                    "authorId": "1830414684",
                    "name": "Leong Hou U"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1722082",
                    "name": "Man Lung Yiu"
                },
                {
                    "authorId": "2017687033",
                    "name": "Shivansh Mittal"
                }
            ]
        },
        {
            "paperId": "6718de1a347458ade4c3a9c4a281af0c25d09c79",
            "title": "Taming System Dynamics on Resource Optimization for Data Processing Workflows: A Probabilistic Approach",
            "abstract": "In many data-intensive applications, workflow is often used as an important model for organizing data processing tasks and resource provisioning is an important and challenging problem for improving the performance of workflows. Recently, system variations in the cloud and large-scale clusters, such as those in I/O and network performances and failure events, have been observed to greatly affect the performance of workflows. Traditional resource provisioning methods, which overlook these variations, can lead to suboptimal resource provisioning results. In this article, we provide a general solution for workflow performance optimizations considering system variations. Specifically, we model system dynamics as time-dependent random variables and take their probability distributions as optimization input. Despite its effectiveness, this solution involves heavy computation overhead. Thus, we propose three pruning techniques to simplify workflow structure and reduce the probability evaluation overhead. We implement our techniques in a runtime library, which allows users to incorporate efficient probabilistic optimization into existing resource provisioning methods. Experiments show that probabilistic solutions can improve the performance by up to 65 percent compared to state-of-the-art static solutions, and our pruning techniques can greatly reduce the overhead of our probabilistic approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2494447",
                    "name": "Amelie Chi Zhou"
                },
                {
                    "authorId": "2142347594",
                    "name": "Weilin Xue"
                },
                {
                    "authorId": "2116642787",
                    "name": "Yao Xiao"
                },
                {
                    "authorId": "143824511",
                    "name": "Bingsheng He"
                },
                {
                    "authorId": "2629067",
                    "name": "Shadi Ibrahim"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "0086577bfe5fff7a3a40c10caa595b6c8f997d8b",
            "title": "On Analyzing Graphs with Motif-Paths",
            "abstract": "\n Path-based solutions have been shown to be useful for various graph analysis tasks, such as link prediction and graph clustering. However, they are no longer adequate for handling complex and gigantic graphs. Recently,\n motif-based analysis\n has attracted a lot of attention. A motif, or a small graph with a few nodes, is often considered as a fundamental unit of a graph. Motif-based analysis captures high-order structure between nodes, and performs better than traditional \"edge-based\" solutions. In this paper, we study\n motif-path\n , which is conceptually a concatenation of one or more motif instances. We examine how motif-paths can be used in three path-based mining tasks, namely link prediction, local graph clustering and node ranking. We further address the situation when two graph nodes are not connected through a motif-path, and develop a novel defragmentation method to enhance it. Experimental results on real graph datasets demonstrate the use of motif-paths and defragmentation techniques improves graph analysis effectiveness.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144442653",
                    "name": "Xiaodong Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "143922493",
                    "name": "K. Chang"
                },
                {
                    "authorId": "145663545",
                    "name": "Caihua Shan"
                },
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "2117832282",
                    "name": "Hongtai Cao"
                }
            ]
        },
        {
            "paperId": "06d138c67735f7f47843c09a3f411e8d44db2cde",
            "title": "GLAD: A Grid and Labeling Framework with Scheduling for Conflict-Aware <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq1-2942585.gif\"/></alternatives></inline-formula>NN Queries",
            "abstract": "The intelligent transportation systems, e.g., DiDi and Uber, have served as essential travel tools for customers, which foster plenty of studies for the location-based queries on road network. In particular, given a set <inline-formula><tex-math notation=\"LaTeX\">$O$</tex-math><alternatives><mml:math><mml:mi>O</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq2-2942585.gif\"/></alternatives></inline-formula> of objects and a query point <inline-formula><tex-math notation=\"LaTeX\">$q$</tex-math><alternatives><mml:math><mml:mi>q</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq3-2942585.gif\"/></alternatives></inline-formula> on a road network, the <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq4-2942585.gif\"/></alternatives></inline-formula> Nearest Neighbor (<inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq5-2942585.gif\"/></alternatives></inline-formula>NN) query returns the <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq6-2942585.gif\"/></alternatives></inline-formula> nearest objects in <inline-formula><tex-math notation=\"LaTeX\">$O$</tex-math><alternatives><mml:math><mml:mi>O</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq7-2942585.gif\"/></alternatives></inline-formula> with the shortest road network distance to <inline-formula><tex-math notation=\"LaTeX\">$q$</tex-math><alternatives><mml:math><mml:mi>q</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq8-2942585.gif\"/></alternatives></inline-formula>. In literature, most existing solutions for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq9-2942585.gif\"/></alternatives></inline-formula>NN queries tend to reduce the query time, indexing storage, or throughput of the <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq10-2942585.gif\"/></alternatives></inline-formula>NN queries while overlooking the correctness of the queries caused by query-query and update-query conflicts. In our work, we propose a grid-based framework on conflict-aware <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq11-2942585.gif\"/></alternatives></inline-formula>NN queries on moving objects which aims to optimize system throughput while guaranteeing query correctness. In particular, we first propose efficient index structures and new query algorithms that significantly improve the throughput. We further present novel scheduling algorithms that aim to avoid conflicts and improve the system throughput. Moreover, we devise approximate solutions that provide a controllable trade-off between the conflict of <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq12-2942585.gif\"/></alternatives></inline-formula>NN queries and system throughput. Finally, we propose a cost-based dispatching strategy to assign the <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"he-ieq13-2942585.gif\"/></alternatives></inline-formula>NN results to the corresponding queries. Extensive experiments on real-world data demonstrate the effectiveness and efficiency of our proposed solutions over alternatives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064927355",
                    "name": "Dan He"
                },
                {
                    "authorId": "39996718",
                    "name": "Sibo Wang"
                },
                {
                    "authorId": "48667278",
                    "name": "Xiaofang Zhou"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "3561c268f27a017d7a69ffde9ca2101ae0e9f9e1",
            "title": "On Directed Densest Subgraph Discovery",
            "abstract": "Given a directed graph G, the directed densest subgraph (DDS) problem refers to the finding of a subgraph from G, whose density is the highest among all the subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fraud detection, community mining, and graph compression. However, existing DDS solutions suffer from efficiency and scalability problems: on a 3,000-edge graph, it takes three days for one of the best exact algorithms to complete. In this article, we develop an efficient and scalable DDS solution. We introduce the notion of [x, y]-core, which is a dense subgraph for G, and show that the densest subgraph can be accurately located through the [x, y]-core with theoretical guarantees. Based on the [x, y]-core, we develop exact and approximation algorithms. We further study the problems of maintaining the DDS over dynamic directed graphs and finding the weighted DDS on weighted directed graphs, and we develop efficient non-trivial algorithms to solve these two problems by extending our DDS algorithms. We have performed an extensive evaluation of our approaches on 15 real large datasets. The results show that our proposed solutions are up to six orders of magnitude faster than the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2402302",
                    "name": "Wenjie Zhang"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "4231b64ec70cb28dd5001d09aa741d118d4df813",
            "title": "Efficient Directed Densest Subgraph Discovery",
            "abstract": "Given a directed graph G, the directed densest subgraph (DDS) problem refers to the finding of a subgraph from G, whose density is the highest among all the subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fraud detection, community mining, and graph compression. However, existing DDS solutions suffer from efficiency and scalability problems: on a threethousand- edge graph, it takes three days for one of the best exact algorithms to complete. In this paper, we develop an efficient and scalable DDS solution. We introduce the notion of [x, y]-core, which is a dense subgraph for G, and show that the densest subgraph can be accurately located through the [x, y]-core with theoretical guarantees. Based on the [x, y]-core, we develop both exact and approximation algorithms. We have performed an extensive evaluation of our approaches on eight real large datasets. The results show that our proposed solutions are up to six orders of magnitude faster than the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2402302",
                    "name": "Wenjie Zhang"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "45b47774935a1fdcf79c9b7734812914c771786a",
            "title": "Achieving low tail-latency and high scalability for serializable transactions in edge computing",
            "abstract": "A distributed database utilizing the wide-spread edge computing servers to provide low-latency data access with the serializability guarantee is highly desirable for emerging edge computing applications. In an edge database, nodes are divided into regions, and a transaction can be categorized as intra-region (IRT) or cross-region (CRT) based on whether it accesses data in different regions. In addition to serializability, we insist that a practical edge database should provide low tail latency for both IRTs and CRTs, and such low latency must be scalable to a large number of regions. Unfortunately, none of existing geo-replicated serializable databases or edge databases can meet such requirements. In this paper, we present Dast (Decentralized Anticipate and STretch), the first edge database that can meet the stringent performance requirements with serializability. Our key idea is to order transactions by anticipating when they are ready to execute: Dast binds an IRT to the latest timestamp and binds a CRT to a future timestamp to avoid the coordination of CRTs blocking IRTs. Dast also carries a new stretchable clock abstraction to tolerate inaccurate anticipations and to handle cross-region data reads. Our evaluation shows that, compared to three relevant serializable databases, Dast's 99-percentile latency was 87.9%~93.2% lower for IRTs and 27.7%~70.4% lower for CRTs; Dast's low latency is scalable to a large number of regions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49794864",
                    "name": "Xusheng Chen"
                },
                {
                    "authorId": "2112980777",
                    "name": "Haoze Song"
                },
                {
                    "authorId": "11324564",
                    "name": "Jianyu Jiang"
                },
                {
                    "authorId": "51510890",
                    "name": "Chaoyi Ruan"
                },
                {
                    "authorId": "2143636797",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "2116230383",
                    "name": "Sen Wang"
                },
                {
                    "authorId": "49288277",
                    "name": "Gong Zhang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2944075",
                    "name": "Heming Cui"
                }
            ]
        },
        {
            "paperId": "5b3f526bd80f31359d81d734f2ece211e153936e",
            "title": "Fast Augmentation Algorithms for Network Kernel Density Visualization",
            "abstract": "Network kernel density visualization, or NKDV, has been extensively used to visualize spatial data points in various domains, including traffic accident hotspot detection, crime hotspot detection, ...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "ee95de1283baf14bbffee9b84a0da356d5ac32ca",
            "title": "KDV-Explorer: A Near Real-Time Kernel Density Visualization System for Spatial Analysis",
            "abstract": "Kernel density visualization (KDV) is a commonly used visualization tool for many spatial analysis tasks, including disease outbreak detection, crime hotspot detection, and traffic accident hotspot detection. Although the most popular geographical information systems, e.g., QGIS, and ArcGIS, can also support this operation, these solutions are not scalable to generate a single KDV for datasets with million-scale data points, let alone to support exploratory operations (e.g., zoom in, zoom out, and panning operations) with KDV in near real-time (< 5 sec). In this demonstration, we develop a near real-time visualization system, called KDV-Explorer, that is built on top of our prior study on the efficient kernel density computation. Participants will be invited to conduct some kernel density analysis on three large-scale datasets (up to 1.3 million data points), including the traffic accident dataset, crime dataset and COVID-19 dataset. We will also compare the performance of our solution and the solutions in QGIS and ArcGIS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2610571",
                    "name": "Tsz Nam Chan"
                },
                {
                    "authorId": "2123130015",
                    "name": "Pak Lon Ip"
                },
                {
                    "authorId": "1830414684",
                    "name": "Leong Hou U"
                },
                {
                    "authorId": "2124879152",
                    "name": "Weng Hou Tong"
                },
                {
                    "authorId": "2017687033",
                    "name": "Shivansh Mittal"
                },
                {
                    "authorId": "2110765899",
                    "name": "Ye Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "18bf86e85f3ca72d843709982eb35084dcb38403",
            "title": "M-Cypher: A GQL Framework Supporting Motifs",
            "abstract": "Graph databases witness the rise of Graph Query Language (GQL) in recent years, which enables non-programmers to express a graph query. However, the current solution does not support motif-related queries on knowledge graphs, which are proven important in many real-world scenarios. In this paper, we propose a GQL framework for mining knowledge graphs, named M-Cypher. It supports motif-related graph queries in an effective, efficient and user-friendly manner. We demonstrate the usage of the system by the emerging Covid-19 knowledge graph analytic tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144442653",
                    "name": "Xiaodong Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1998588439",
                    "name": "Matin Najafi"
                },
                {
                    "authorId": "143922493",
                    "name": "K. Chang"
                },
                {
                    "authorId": "151480433",
                    "name": "Xiaolin Han"
                },
                {
                    "authorId": "2117832282",
                    "name": "Hongtai Cao"
                }
            ]
        },
        {
            "paperId": "24968720d2f03acb93616747026134d8cc81ca8b",
            "title": "Traffic Incident Detection: A Trajectory-based Approach",
            "abstract": "Incident detection (ID), or the automatic discovery of anomalies from road traffic data (e.g., road sensor and GPS data), enables emergency actions (e.g., rescuing injured people) to be carried out in a timely fashion. Existing ID solutions based on data mining or machine learning often rely on dense traffic data; for instance, sensors installed in highways provide frequent updates of road information. In this paper, we ask the question: Can ID be performed on sparse traffic data (e.g., location data obtained from GPS devices equipped on vehicles)\u0192 As these data may not be enough to describe the state of the roads involved, they can undermine the effectiveness of existing ID solutions. To tackle this challenge, we borrow an important insight from the transportation area, which uses trajectories (i.e., moving histories of vehicles) to derive incident patterns. We study how to obtain incident patterns from trajectories and devise a new solution (called Filter-Discovery-Match (FDM)) to detect anomalies in sparse traffic data. Experiments on a taxi dataset in Hong Kong and a simulated dataset show that FDM is more effective than state-of-the-art ID solutions on sparse traffic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151480433",
                    "name": "Xiaolin Han"
                },
                {
                    "authorId": "3431016",
                    "name": "Tobias Grubenmann"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "35449705",
                    "name": "S. Wong"
                },
                {
                    "authorId": "2144442653",
                    "name": "Xiaodong Li"
                },
                {
                    "authorId": "46912256",
                    "name": "Wenya Sun"
                }
            ]
        },
        {
            "paperId": "3696b37ac1b9e869948e726ed7bae1fbf1f2783e",
            "title": "MC-Explorer: Analyzing and Visualizing Motif-Cliques on Large Networks",
            "abstract": "Large networks with labeled nodes are prevalent in various applications, such as biological graphs, social networks, and e-commerce graphs. To extract insight from this rich information source, we propose MC-Explorer, which is an advanced analysis and visualization system. A highlight of MC-Explorer is its ability to discover motif-cliques from a graph with labeled nodes. A motif, such as a 3-node triangle, is a fundamental building block of a graph. A motif-clique is a \"complete\" subgraph in a network with respect to a desired higher-order connection pattern. For example, on a large biological graph, we found out some motif-cliques, which disclose new side effects of a drug, and potential drugs for healing diseases. MC-Explorer includes online and interactive facilities for exploring a large labeled network through the use of motif-cliques. We will demonstrate how MC-Explorer can facilitate the analysis and visualization of a labeled biological network.An online demo video of MC-Explorer can be accessed from https://www.dropbox.com/s/vkalumc28wqp8yl/demo.mov",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720745347",
                    "name": "Boxuan Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1720773781",
                    "name": "Min Ou"
                },
                {
                    "authorId": "1789382",
                    "name": "Ruibang Luo"
                },
                {
                    "authorId": "143922493",
                    "name": "K. Chang"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "38c440e23c1059e2db029c55f3403e50bff614c2",
            "title": "A Toolkit for Managing Multiple Crowdsourced Top-K Queries",
            "abstract": "Crowdsourced ranking and top-k queries have attracted significant attention recently. Their goal is to combine human cognitive abilities and machine intelligence to rank computer hostile but human friendly items. Many task assignment algorithms and inference approaches have been proposed to publish suitable micro-tasks to the crowd, obtain informative answers, and aggregate the rank from noisy human answers. However, they are all focused on single query processing. To the best of our knowledge, no prior work helps users manage multiple crowdsourced top-k queries. We propose a toolkit, which seamlessly works with most existing inference and task assignment methods, for crowdsourced top-k query management. Our toolkit attempts to optimize human resource allocation and continuously monitors query quality at any stage of the crowdsourcing process. A user can terminate a query early, if the estimated quality already fulfills her requirements. Besides, the toolkit provides user-friendly interfaces for users to initialize queries, monitor execution status, and do more operations by hand.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145663545",
                    "name": "Caihua Shan"
                },
                {
                    "authorId": "1830414684",
                    "name": "Leong Hou U"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "8349976b7f72413a76d76268a26ea12c7ad73d6c",
            "title": "QUAD: Quadratic-Bound-based Kernel Density Visualization",
            "abstract": "Kernel density visualization, or KDV, is used to view and understand data points in various domains, including traffic or crime hotspot detection, ecological modeling, chemical geology, and physical modeling. Existing solutions, which are based on computing kernel density (KDE) functions, are computationally expensive. Our goal is to improve the performance of KDV, in order to support large datasets (e.g., one million points) and high screen resolutions (e.g., 1280 x 960 pixels). We examine two widely-used variants of KDV, namely approximate kernel density visualization (EKDV) and thresholded kernel density visualization (TKDV). For these two operations, we develop fast solution, called QUAD, by deriving quadratic bounds of KDE functions for different types of kernel functions, including Gaussian, triangular etc. We further adopt a progressive visualization framework for KDV, in order to stream partial visualization results to users continuously. Extensive experiment results show that our new KDV techniques can provide at least one-order-of-magnitude speedup over existing methods, without degrading visualization quality. We further show that QUAD can produce the reasonable visualization results in real-time (0.5 sec) by combining the progressive visualization framework in single machine setting without using GPU and parallel computation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2610571",
                    "name": "Tsz Nam Chan"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1722082",
                    "name": "Man Lung Yiu"
                }
            ]
        },
        {
            "paperId": "971c244dd3eb9432e7b63692a755271a2f1203c4",
            "title": "TSA: A Truthful Mechanism for Social Advertising",
            "abstract": "Social advertising exploits the interconnectivity of users in social networks to spread advertisement and generate user engagements. A lot of research has focused on how to select the best subset of users in a social network to maximize the number of engagements or the generated revenue of the advertisement. However, there is a lack of studies that consider the advertiser's value-per-engagement, i.e., how much an advertiser is maximally willing to pay for each engagement. Prior work on social advertising is based on the classical framework of influence maximization. In this paper, we propose a model where advertisers compete in an auction mechanism for the influential users within a social network. The auction mechanism can dynamically determine payments for advertisers based on their reported values. The main problem is to find auctions which incentivize advertisers to truthfully reveal their values, and also respect each advertiser's budget constraint. To tackle this problem, we propose a new truthful auction mechanism called TSA. Compared with existing approaches on real and synthetic datasets, TSA performs significantly better in terms of generated revenue.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3431016",
                    "name": "Tobias Grubenmann"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "a4b62f87a8e13a214da65df48953ed10e3f44bf4",
            "title": "Efficient Algorithms for Densest Subgraph Discovery on Large Directed Graphs",
            "abstract": "Given a directed graph G, the directed densest subgraph (DDS) problem refers to the finding of a subgraph from G, whose density is the highest among all the subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fraud detection, community mining, and graph compression. However, existing DDS solutions suffer from efficiency and scalability problems: on a three-thousand-edge graph, it takes three days for one of the best exact algorithms to complete. In this paper, we develop an efficient and scalable DDS solution. We introduce the notion of [x, y]-core, which is a dense subgraph for G, and show that the densest subgraph can be accurately located through the [x, y]-core with theoretical guarantees. Based on the [x, y]-core, we develop exact and approximation algorithms. We have performed an extensive evaluation of our approaches on eight real large datasets. The results show that our proposed solutions are up to six orders of magnitude faster than the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2402302",
                    "name": "Wenjie Zhang"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "b521bab5ed54dedf3c158eb49e38a7460e7602c0",
            "title": "Effective and Efficient Discovery of Top-k Meta Paths in Heterogeneous Information Networks",
            "abstract": "<italic>Heterogeneous information networks (HINs)</italic>, which are typed graphs with labeled nodes and edges, have attracted tremendous interest from academia and industry. Given two HIN nodes <inline-formula><tex-math notation=\"LaTeX\">$s$</tex-math><alternatives><mml:math><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq1-3037218.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq2-3037218.gif\"/></alternatives></inline-formula>, and a natural number <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq3-3037218.gif\"/></alternatives></inline-formula>, we study the discovery of the <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq4-3037218.gif\"/></alternatives></inline-formula> most important meta paths in real time, which can be used to support friend search, product recommendation, anomaly detection, and graph clustering. In this work, we argue that the shortest path between <inline-formula><tex-math notation=\"LaTeX\">$s$</tex-math><alternatives><mml:math><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq5-3037218.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq6-3037218.gif\"/></alternatives></inline-formula> may not necessarily be the most important path. As such, we combine several ranking functions, which are based on <italic>frequency</italic> and <italic>rarity</italic>, to redefine the unified importance function of the meta paths between <inline-formula><tex-math notation=\"LaTeX\">$s$</tex-math><alternatives><mml:math><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq7-3037218.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq8-3037218.gif\"/></alternatives></inline-formula>. Although this importance function can capture more information, it is very time-consuming to find top-<inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq9-3037218.gif\"/></alternatives></inline-formula> meta paths using this importance function. Therefore, we integrate this importance function into a multi-step framework, which can efficiently filter some impossible meta paths between <inline-formula><tex-math notation=\"LaTeX\">$s$</tex-math><alternatives><mml:math><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq10-3037218.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"zhu-ieq11-3037218.gif\"/></alternatives></inline-formula>. In addition, we combine bidirectional searching algorithm with this framework to further boost the efficiency performance. The experiment on different datasets shows that our proposed method outperforms state-of-the-art algorithms in terms of effectiveness with reasonable response time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140564220",
                    "name": "Zichen Zhu"
                },
                {
                    "authorId": "2610571",
                    "name": "Tsz Nam Chan"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "144919491",
                    "name": "Loc Do"
                },
                {
                    "authorId": "50823909",
                    "name": "Zhipeng Huang"
                },
                {
                    "authorId": "2315442655",
                    "name": "Haoci Zhang"
                }
            ]
        },
        {
            "paperId": "c5e855ffd33f23579226d98c050a91d6a5903a1b",
            "title": "A Crowdsourcing Framework for Collecting Tabular Data",
            "abstract": "In crowdsourcing, human workers are employed to tackle problems that are traditionally difficult for computers (e.g., data cleaning, missing value filling, and sentiment analysis). In this paper, we study the effective use of crowdsourcing in filling missing values in a given relation (e.g., a table containing different attributes of celebrity stars, such as nationality and age). A task given to a worker typically consists of questions about the missing attribute values (e.g., What is the age of Jet Li?). Although this problem has been studied before, existing work often treats related attributes independently, leading to suboptimal performance. In this paper, we present T-Crowd, which is a crowdsourcing system that considers attribute relationships. Particularly, T-Crowd integrates each worker's answers on different attributes to effectively learn his/her trustworthiness and the true data values. The attribute relationship information is used to guide task allocation to workers. Our solution seamlessly supports categorical and continuous attributes. Our extensive experiments on real and synthetic datasets show that T-Crowd outperforms state-of-the-art methods, improving the quality of truth inference and reducing the monetary cost of crowdsourcing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145663545",
                    "name": "Caihua Shan"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "50823909",
                    "name": "Zhipeng Huang"
                },
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                }
            ]
        },
        {
            "paperId": "ca30ab8d537aa8dfe35a0f196a022e89cef1b794",
            "title": "SCODED: Statistical Constraint Oriented Data Error Detection",
            "abstract": "Statistical Constraints (SCs) play an important role in statistical modeling and analysis. This paper brings the concept to data cleaning and studies how to leverage SCs for error detection. SCs provide a novel approach that has various application scenarios and works harmoniously with downstream statistical modeling. Entailment relationships between SCs and integrity constraints provide analytical insight into SCs. We develop SCODED, an SC-Oriented Data Error Detection system, comprising two key components: (1) SC Violation Detection : checks whether an SC is violated on a given dataset, and (2) Error Drill Down : identifies the top-k records that contribute most to the violation of an SC. Experiments on synthetic and real-world data show that SCs are effective in detecting data errors that violate them, compared to state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48270014",
                    "name": "J. Yan"
                },
                {
                    "authorId": "2166203",
                    "name": "O. Schulte"
                },
                {
                    "authorId": "2112104616",
                    "name": "Mohan Zhang"
                },
                {
                    "authorId": "1743709",
                    "name": "Jiannan Wang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "e004e8286c3a790be52c28d50d4d2a75a3322bb5",
            "title": "An ML-Powered Human Behavior Management System",
            "abstract": "Our work aims to develop novel technologies for building an efficient data infrastructure as a backbone for a human behavior management system. Our infrastructure aims at facilitating behavior modeling, discovery, and exploitation, leading to two major outcomes: a behavior data management back-end and a high-level behavior specification API that supports mining, indexing and search, and AI-powered algorithms that provide the ability to extract insights on human behavior and to leverage data to advance human capital. We discuss the role of ML in populating and maintaining the back-end, and in exploiting it for human interest.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2037875626",
                    "name": "Mohamed Bouadi"
                },
                {
                    "authorId": "2037873034",
                    "name": "Abdelouahab Chibah"
                },
                {
                    "authorId": "47224252",
                    "name": "M. Esfandiari"
                },
                {
                    "authorId": "2142202662",
                    "name": "Jiangping Zhou"
                },
                {
                    "authorId": "1557281863",
                    "name": "Nan Zhang"
                },
                {
                    "authorId": "2057725909",
                    "name": "Eric H. Y. Lau"
                },
                {
                    "authorId": "2120309280",
                    "name": "Yuguo Li"
                },
                {
                    "authorId": "151480433",
                    "name": "Xiaolin Han"
                },
                {
                    "authorId": "2017687033",
                    "name": "Shivansh Mittal"
                }
            ]
        },
        {
            "paperId": "f5e863551ef2a2a437f8544ff7044368f73dd862",
            "title": "ROAM: A Fundamental Routing Query on Road Networks with Efficiency",
            "abstract": "Novel road-network applications often recommend a moving object (e.g., a vehicle) about interesting services or tasks on its way to a destination. A taxi-sharing system, for instance, suggests a new passenger to a taxi while it is serving another one. The traveling cost is then shared among these passengers. A fundamental query is: given two nodes <inline-formula><tex-math notation=\"LaTeX\">$s$</tex-math><alternatives><mml:math><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq1-2906188.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq2-2906188.gif\"/></alternatives></inline-formula>, and an area <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {A}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">A</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq3-2906188.gif\"/></alternatives></inline-formula> on road network graph <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {G}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">G</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq4-2906188.gif\"/></alternatives></inline-formula>, is there a \u201cgood\u201d route (e.g., short enough path) <inline-formula><tex-math notation=\"LaTeX\">$P$</tex-math><alternatives><mml:math><mml:mi>P</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq5-2906188.gif\"/></alternatives></inline-formula> from <inline-formula><tex-math notation=\"LaTeX\">$s$</tex-math><alternatives><mml:math><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq6-2906188.gif\"/></alternatives></inline-formula> to <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq7-2906188.gif\"/></alternatives></inline-formula> that crosses <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {A}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">A</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq8-2906188.gif\"/></alternatives></inline-formula> in <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {G}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">G</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq9-2906188.gif\"/></alternatives></inline-formula>? In a taxi-sharing system, <inline-formula><tex-math notation=\"LaTeX\">$s$</tex-math><alternatives><mml:math><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq10-2906188.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq11-2906188.gif\"/></alternatives></inline-formula> can be a taxi's current and destined locations, and <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {A}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">A</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq12-2906188.gif\"/></alternatives></inline-formula> contains all the places to which a person waiting for a taxi is willing to walk. Answering this <italic>Route and Area Matching</italic> (ROAM) Query allows the application involved to recommend appropriate services to users efficiently. In this paper, we examine efficient ROAM query algorithms. Particularly, we develop solutions for finding a <italic><inline-formula><tex-math notation=\"LaTeX\">$\\rho$</tex-math><alternatives><mml:math><mml:mi>\u03c1</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq13-2906188.gif\"/></alternatives></inline-formula>-route</italic>, which is an <inline-formula><tex-math notation=\"LaTeX\">$s$</tex-math><alternatives><mml:math><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq14-2906188.gif\"/></alternatives></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq15-2906188.gif\"/></alternatives></inline-formula> path that passes <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {A}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">A</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq16-2906188.gif\"/></alternatives></inline-formula>, with a length of at most <inline-formula><tex-math notation=\"LaTeX\">$(1+\\rho)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>\u03c1</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"luo-ieq17-2906188.gif\"/></alternatives></inline-formula> times the shortest distance between <inline-formula><tex-math notation=\"LaTeX\">$s$</tex-math><alternatives><mml:math><mml:mi>s</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq18-2906188.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq19-2906188.gif\"/></alternatives></inline-formula>. The existence of a <inline-formula><tex-math notation=\"LaTeX\">$\\rho$</tex-math><alternatives><mml:math><mml:mi>\u03c1</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq20-2906188.gif\"/></alternatives></inline-formula>-route implies that a service or task located at <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {A}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">A</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq21-2906188.gif\"/></alternatives></inline-formula> can be found for a given moving object <inline-formula><tex-math notation=\"LaTeX\">$m$</tex-math><alternatives><mml:math><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq22-2906188.gif\"/></alternatives></inline-formula>, and that <inline-formula><tex-math notation=\"LaTeX\">$m$</tex-math><alternatives><mml:math><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href=\"luo-ieq23-2906188.gif\"/></alternatives></inline-formula> only deviates slightly from its current route. We present comprehensive studies on index-free and index-based algorithms for answering ROAM queries. Comprehensive experiments show that our algorithm runs up to 30 times faster than baseline algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "50730331",
                    "name": "Shuigeng Zhou"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                }
            ]
        },
        {
            "paperId": "f650ad4bb5285a9db6cc8cbb8b6e38de8c72d910",
            "title": "Integrating Domain Knowledge in AI-Assisted Criminal Sentencing of Drug Trafficking Cases",
            "abstract": "Judgment prediction is the task of predicting various outcomes of legal cases of which sentencing prediction is one of the most important yet difficult challenges. We study the applicability of machine learning (ML) techniques in predicting prison terms of drug trafficking cases. In particular, we study how legal domain knowledge can be integrated with ML models to construct highly accurate predictors. We illustrate how our criminal sentence predictors can be applied to address four important issues in legal knowledge management, which include (1) discovery of model drifts in legal rules, (2) identification of critical features in legal judgments, (3) fairness in machine predictions, and (4) explainability of machine predictions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "81034267",
                    "name": "Tien-Hsuan Wu"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "8350270",
                    "name": "A. Cheung"
                },
                {
                    "authorId": "2061477168",
                    "name": "Michael M. K. Cheung"
                },
                {
                    "authorId": "2109115016",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "2155261579",
                    "name": "Yongxi Chen"
                },
                {
                    "authorId": "22662827",
                    "name": "Guowen Yuan"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "04c9ab94362e02b8e4c6c6a8d55ba468a2f7018e",
            "title": "Exploring Communities in Large Profiled Graphs (Extended Abstract)",
            "abstract": "Given a graph G and a vertex q \u220a G, the community search (CS) problem aims to efficiently find a subgraph of G whose vertices are closely related to q. Communities are prevalent in social and biological networks, and can be used in product advertisement and social event recommendation. In this paper, we study profiled community search (PCS), where CS is performed on a profiled graph. This is a graph in which each vertex has labels arranged in a hierarchical manner. Compared with existing CS approaches, PCS can sufficiently identify vertices with semantic commonalities and thus find more high-quality diverse communities. As a naive solution for PCS is highly expensive, we have developed a tree index, which facilitates efficient and online solutions for PCS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2681738",
                    "name": "Yankai Chen"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2117014417",
                    "name": "Yun Li"
                },
                {
                    "authorId": "1702033",
                    "name": "Xiaojun Chen"
                },
                {
                    "authorId": "36611093",
                    "name": "Jie Zhang"
                }
            ]
        },
        {
            "paperId": "259e958163b9d5e66e08a1c80f15fbd5240e8753",
            "title": "Discovering Maximal Motif Cliques in Large Heterogeneous Information Networks",
            "abstract": "We study the discovery of cliques (or \"complete\" subgraphs) in heterogeneous information networks (HINs). Existing clique-finding solutions often ignore the rich semantics of HINs. We propose motif clique, or m-clique, which redefines subgraph completeness with respect to a given motif. A motif, essentially a small subgraph pattern, is a fundamental building block of an HIN. The m-clique concept is general and allows us to analyse \"complete\" subgraphs in an HIN with respect to desired high-order connection patterns. We further investigate the maximal m-clique enumeration problem (MMCE), which finds all maximal m-cliques not contained in any other m-cliques. Because MMCE is NP-hard, developing an accurate and efficient solution for MMCE is not straightforward. We thus present the META algorithm, which employs advanced pruning strategies to effectively reduce the search space. We also design fast techniques to avoid generating duplicated maximal m-clique instances. Our extensive experiments on large real and synthetic HINs show that META is highly effective and efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "143922493",
                    "name": "K. Chang"
                },
                {
                    "authorId": "1826045",
                    "name": "Aravind Sankar"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "152835266",
                    "name": "B. Lam"
                }
            ]
        },
        {
            "paperId": "29f52245b3c80683ec0ae2dd81ba1d8c6dd92752",
            "title": "Discovering Motif Cliques in Large Heterogeneous Information Networks",
            "abstract": "\u2014We study the discovery of cliques (or \u201ccomplete\u201d subgraphs) in heterogeneous information networks (HINs). Existing clique-\ufb01nding solutions often ignore the rich semantics of HINs. We propose motif clique , or m-clique , which rede\ufb01nes subgraph completeness with respect to a given motif . A motif, essentially a small subgraph pattern, is a fundamental building block of an HIN. The m-clique concept is general and allows us to analyse \u201ccomplete\u201d subgraphs in an HIN with respect to desired high-order connection patterns. We further investigate the maximal m-clique enumeration problem (MMCE), which \ufb01nds all maximal m-cliques not contained in any other m-cliques. Because MMCE is NP-hard, developing an accurate and ef\ufb01cient solution for MMCE is not straightforward. We thus present the META algorithm, which employs advanced pruning strategies to effectively reduce the search space. We also design fast techniques to avoid generating duplicated maximal m-clique instances. Our extensive experiments on large real and synthetic HINs show that META is highly effective and ef\ufb01cient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "143922493",
                    "name": "K. Chang"
                },
                {
                    "authorId": "1826045",
                    "name": "Aravind Sankar"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "2053014337",
                    "name": "Yee Lam"
                }
            ]
        },
        {
            "paperId": "43cc567fd98c6c5cb192911ffe3ad7f9da4ffca4",
            "title": "LINC: A Motif Counting Algorithm for Uncertain Graphs",
            "abstract": "In graph applications (e.g., biological and social networks), various analytics tasks (e.g., clustering and community search) are carried out to extract insight from large and complex graphs. Central to these tasks is the counting of the number of motifs, which are graphs with a few nodes. Recently, researchers have developed several fast motif counting algorithms. Most of these solutions assume that graphs are deterministic, i.e., the graph edges are certain to exist. However, due to measurement and statistical prediction errors, this assumption may not hold, and hence the analysis quality can be affected. To address this issue, we examine how to count motifs on uncertain graphs, whose edges only exist probabilistically. Particularly, we propose a solution framework that can be used by existing deterministic motif counting algorithms. We further propose an approximation algorithm. Extensive experiments on real datasets show that our algorithms are more effective and efficient than existing solutions. PVLDB Reference Format: Chenhao Ma, Reynold Cheng, Laks V.S. Lakshmanan, Tobias Grubenmann, Yixiang Fang, and Xiaodong Li. LINC: A Motif Counting Algorithm for Uncertain Graphs. PVLDB, 13(2): 155-168, 2019. DOI: https://doi.org/10.14778/3364324.3364330",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "3431016",
                    "name": "Tobias Grubenmann"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "2144442653",
                    "name": "Xiaodong Li"
                }
            ]
        },
        {
            "paperId": "4d88527df43ef2e81f17d54ec12831827d17dbb2",
            "title": "Effective and Efficient Community Search Over Large Directed Graphs",
            "abstract": "Communities are prevalent in social networks, knowledge graphs, and biological networks. Recently, the topic of community search (CS), extracting a dense subgraph containing a query vertex $q$q from a graph, has received great attention. However, existing CS solutions are designed for undirected graphs, and overlook directions of edges which potentially lose useful information carried on directions. In many applications (e.g., Twitter), users\u2019 relationships are often modeled as directed graphs (e.g., if a user $a$a follows another user $b$b, then there is an edge from $a$a to $b$b). In this paper, we study the problem of CS on directed graph. Given a vertex $q$q of a graph $G$G, we aim to find a densely connected subgraph containing $q$q from $G$G, in which vertices have strong interactions and high similarities, by using the minimum in/out-degrees metric. We first develop a baseline algorithm based on the concept of D-core. We further propose three index structures and corresponding query algorithms. Our experimental results on seven real graphs show that our solutions are very effective and efficient. For example, on a graph with over 1 billion of edges, we only need around 40mins to index it and 1$\\sim$\u223c2sec to answer a query.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1999387709",
                    "name": "Zhongran Wang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "46506118",
                    "name": "Hongzhi Wang"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                }
            ]
        },
        {
            "paperId": "591c03cf949761094a083da07b44ca9686dbf8c5",
            "title": "MPR \u2014 A Partitioning-Replication Framework for Multi-Processing kNN Search on Road Networks",
            "abstract": "We study the problem of executing road-network k-nearest-neighbor (\\knn) search on multi-core machines. State-of-the-art kNN algorithms on road networks often involve elaborate index structures and complex computational logic. Moreover, most kNN algorithms are inherently sequential. These make the traditional approach of parallel programming very costly, laborious, and ineffective when they are applied to kNN algorithms. We propose the MPR (Multi-layer Partitioning-Replication) mechanism that orchestrates CPU cores and schedules kNN query and index update processes to run on the cores. The MPR mechanism performs workload analysis to determine the best arrangement of the cores with the objective of optimizing quality-of-service (QoS) measures, such as system throughput and query response time. We demonstrate the effectiveness of MPR by applying it to a number of state-of-the-art kNN indexing methods running on a multi-core machine. Our experiments show that multi-processing using our MPR approach requires minimal programming effort. It also leads to significant improvements in query response time and system throughput compared with other baseline parallelization methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "2154765398",
                    "name": "Xiaowei Wu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "92fa888603cf5482bb4e157fd251a78b9daeeb4c",
            "title": "Incorporating Probabilistic Optimizations for Resource Provisioning of Data Processing Workflows",
            "abstract": "Workflow is an important model for big data processing and resource provisioning is crucial to the performance of workflows. Recently, system variations in the cloud and large-scale clusters, such as those in I/O and network performances, have been observed to greatly affect the performance of workflows. Traditional resource provisioning methods, which overlook these variations, can lead to suboptimal resource provisioning results. In this paper, we provide a general solution for workflow performance optimizations considering system variations. Specifically, we model system variations as time-dependent random variables and take their probability distributions as optimization input. Despite its effectiveness, this solution involves heavy computation overhead. Thus, we propose three pruning techniques to simplify workflow structure and reduce the probability evaluation overhead. We implement our techniques in a runtime library, which allows users to incorporate efficient probabilistic optimization into existing resource provisioning methods. Experiments show that probabilistic solutions can improve the performance by 51% compared to state-of-the-art static solutions while guaranteeing budget constraint, and our pruning techniques can greatly reduce the overhead of probabilistic optimization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2494447",
                    "name": "Amelie Chi Zhou"
                },
                {
                    "authorId": "2116642787",
                    "name": "Yao Xiao"
                },
                {
                    "authorId": "143824511",
                    "name": "Bingsheng He"
                },
                {
                    "authorId": "2629067",
                    "name": "Shadi Ibrahim"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "97932ab77940df76c8b81fca51be061302195779",
            "title": "Efficient Algorithms for Densest Subgraph Discovery",
            "abstract": "\n Densest subgraph discovery\n (DSD) is a fundamental problem in graph mining. It has been studied for decades, and is widely used in various areas, including network science, biological analysis, and graph databases. Given a graph\n G\n , DSD aims to find a subgraph\n D\n of\n G\n with the highest density (e.g., the number of edges over the number of vertices in\n D\n ). Because DSD is difficult to solve, we propose a new solution paradigm in this paper. Our main observation is that the densest subgraph can be accurately found through a\n k\n -core (a kind of dense subgraph of\n G\n ), with theoretical guarantees. Based on this intuition, we develop efficient exact and approximation solutions for DSD. Moreover, our solutions are able to find the densest subgraphs for a wide range of graph density definitions, including clique-based- and general pattern-based density. We have performed extensive experimental evaluation on both real and synthetic datasets. Our results show that our algorithms are up to four orders of magnitude faster than existing approaches.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "2114076598",
                    "name": "Kaiqiang Yu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "ae29d3fa974552f116c1972097b921815edad9e8",
            "title": "Exploring Communities in Large Profiled Graphs",
            "abstract": "Given a graph $G$G and a vertex $q\\in G$q\u2208G, the community search (CS) problem aims to efficiently find a subgraph of $G$G whose vertices are closely related to $q$q. Communities are prevalent in social and biological networks, and can be used in product advertisement and social event recommendation. In this paper, we study profiled community search (PCS), where CS is performed on a profiled graph. This is a graph in which each vertex has labels arranged in a hierarchical manner. Extensive experiments show that PCS can identify communities with themes that are common to their vertices, and is more effective than existing CS approaches. As a naive solution for PCS is highly expensive, we have also developed a tree index, which facilitates efficient and online solutions for PCS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2681738",
                    "name": "Yankai Chen"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2117014417",
                    "name": "Yun Li"
                },
                {
                    "authorId": "1702033",
                    "name": "Xiaojun Chen"
                },
                {
                    "authorId": "36611093",
                    "name": "Jie Zhang"
                }
            ]
        },
        {
            "paperId": "bb209841724bce77b6d5c14f34c72f786b4b36b9",
            "title": "Spatial pattern matching: a new direction for finding spatial objects",
            "abstract": "In this paper, we study the spatial pattern matching (SPM) query. Given a set D of spatial objects (e.g., houses and shops), each with a textual description, we aim at finding all combinations of objects from D that match a user-defined spatial pattern P. A pattern P is a graph whose vertices represent spatial objects, and edges denote distance relationships between them. The SPM query returns the instances that satisfy P. An example of P can be \"a house within 10-minute walk from a school, which is at least 2km away from a hospital\". The SPM query can benefit users such as house buyers, urban planners, and archaeologists. We first formally formulate the SPM problem, and then propose efficient query algorithms. We also develop an online system, called SpaceKey, which is based on the SPM query, to support some real applications such as property searching. Finally, we point out a list of possible research directions for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117014417",
                    "name": "Yun Li"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "19262604",
                    "name": "W. Zhang"
                }
            ]
        },
        {
            "paperId": "c7618599f933230adc88d1078c4f4d4cdeb59cf2",
            "title": "An End-to-End Deep RL Framework for Task Arrangement in Crowdsourcing Platforms",
            "abstract": "In this paper, we propose a Deep Reinforcement Learning (RL) framework for task arrangement, which is a critical problem for the success of crowdsourcing platforms. Previous works conduct the personalized recommendation of tasks to workers via supervised learning methods. However, the majority of them only consider the benefit of either workers or requesters independently. In addition, they do not consider the real dynamic environments (e.g., dynamic tasks, dynamic workers), so they may produce sub-optimal results. To address these issues, we utilize Deep Q-Network (DQN), an RL-based method combined with a neural network to estimate the expected long-term return of recommending a task. DQN inherently considers the immediate and the future rewards and can be updated quickly to deal with evolving data and dynamic changes. Furthermore, we design two DQNs that capture the benefit of both workers and requesters and maximize the profit of the platform. To learn value functions in DQN effectively, we also propose novel state representations, carefully design the computation of Q values, and predict transition probabilities and future states. Experiments on synthetic and real datasets demonstrate the superior performance of our framework.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145663545",
                    "name": "Caihua Shan"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "47875796",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "1882202",
                    "name": "Yuqiu Qian"
                }
            ]
        },
        {
            "paperId": "ce9c4f37d4f0c4950a8a35784306be59615f71d9",
            "title": "Detecting Data Errors with Statistical Constraints",
            "abstract": "A powerful approach to detecting erroneous data is to check which potentially dirty data records are incompatible with a user's domain knowledge. Previous approaches allow the user to specify domain knowledge in the form of logical constraints (e.g., functional dependency and denial constraints). We extend the constraint-based approach by introducing a novel class of statistical constraints (SCs). An SC treats each column as a random variable, and enforces an independence or dependence relationship between two (or a few) random variables. Statistical constraints are expressive, allowing the user to specify a wide range of domain knowledge, beyond traditional integrity constraints. Furthermore, they work harmoniously with downstream statistical modeling. We develop CODED, an SC-Oriented Data Error Detection system that supports three key tasks: (1) Checking whether an SC is violated or not on a given dataset, (2) Identify the top-k records that contribute the most to the violation of an SC, and (3) Checking whether a set of input SCs have conflicts or not. We present effective solutions for each task. Experiments on synthetic and real-world data illustrate how SCs apply to error detection, and provide evidence that CODED performs better than state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48270014",
                    "name": "J. Yan"
                },
                {
                    "authorId": "2166203",
                    "name": "O. Schulte"
                },
                {
                    "authorId": "1743709",
                    "name": "Jiannan Wang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "d6f2119eefd6b5c95620b3f07af2ec55e052a185",
            "title": "An Efficient Framework for Correctness-Aware kNN Queries on Road Networks",
            "abstract": "Given a set O of objects and a query point q on a road network, the k Nearest Neighbor (kNN) query returns the k nearest objects in O with the shortest road network distance to q. These kNN queries find many applications in location-based services, e.g., ride-hailing services, where each taxi is regarded as an object. In such applications, objects are constantly moving such that even for the same query point, the correct answer of a kNN query may vary with time. Ideally, the returned answer should be adequately correct with respect to the moving object set. However, in literature, all existing solutions for kNN queries mainly focus on reducing the query time, indexing storage, or throughput of the kNN queries with little focus on their correctness. Motivated by this, we propose a framework on correctness-aware kNN queries which aim to optimize system throughput while guaranteeing query correctness on moving objects. We formally define the serializable-kNN query that ensures the correctness of the query answer when considering moving objects and dependencies of different queries. We propose several techniques to optimize the throughput of serializable-kNN queries: firstly, we propose efficient index structures and new query algorithms that significantly improve the throughput; we further present novel scheduling algorithms that aim to avoid conflicts and improve the system throughput. Moreover, we devise approximate solutions that provide a controllable trade-off between the correctness of kNN queries and system throughput. Extensive experiments on real-world data demonstrate the effectiveness and efficiency of our proposed solutions over alternatives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064927355",
                    "name": "Dan He"
                },
                {
                    "authorId": "39996718",
                    "name": "Sibo Wang"
                },
                {
                    "authorId": "48667278",
                    "name": "Xiaofang Zhou"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "d7b4ca3e0cd30be3ca53ec7d871101269aa601f1",
            "title": "Effective and Efficient Community Search Over Large Directed Graphs (Extended Abstract)",
            "abstract": "Communities are prevalent in social networks, knowledge graphs, and biological networks. Recently, the topic of community search (CS), extracting a dense subgraph containing a query vertex q from a graph, has received great attention. However, existing CS solutions are designed for undirected graphs, and overlook directions of edges which potentially lose useful information carried on directions. In many applications (e.g., Twitter), users' relationships are often modeled as directed graphs (e.g., if a user a follows another user b, then there is an edge from a to b). In this paper, we study the problem of CS on directed graph. Given a vertex q of a graph G, we aim to find a densely connected subgraph containing q from G, in which vertices have strong interactions and high similarities, by using the minimum in/out-degrees metric. We first develop a baseline algorithm based on the concept of D-core. We further propose three index structures and corresponding query algorithms. Our experimental results on seven real graphs show that our solutions are very effective and efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1999387709",
                    "name": "Zhongran Wang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "46506118",
                    "name": "Hongzhi Wang"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                }
            ]
        },
        {
            "paperId": "fee69aca091ebd023c6a51e4df9fdaa2d52ed15a",
            "title": "On Spatial-Aware Community Search",
            "abstract": "Communities are prevalent in social networks, knowledge graphs, and biological networks. Recently, the topic of community search (CS) has received plenty of attention. The CS problem aims to look for a dense subgraph that contains a query vertex. Existing CS solutions do not consider the spatial extent of a community. They can yield communities whose locations of vertices span large areas. In applications that facilitate setting social events (e.g., finding conference attendees to join a dinner), it is important to find groups of people who are physically close to each other, so it is desirable to have a <italic>spatial-aware community</italic> (or SAC), whose vertices are close structurally and spatially. Given a graph <inline-formula><tex-math notation=\"LaTeX\">$G$</tex-math><alternatives><inline-graphic xlink:href=\"fang-ieq1-2845414.gif\"/></alternatives></inline-formula> and a query vertex <inline-formula><tex-math notation=\"LaTeX\">$q$</tex-math><alternatives><inline-graphic xlink:href=\"fang-ieq2-2845414.gif\"/></alternatives></inline-formula>, we develop an exact solution to find the SAC containing <inline-formula><tex-math notation=\"LaTeX\">$q$</tex-math><alternatives><inline-graphic xlink:href=\"fang-ieq3-2845414.gif\"/></alternatives></inline-formula>, but it cannot scale to large datasets, so we design three approximation algorithms. We further study the problem of continuous SAC search on a \u201cdynamic spatial graph,\u201d whose vertices\u2019 locations change with time, and propose three fast solutions. We evaluate the solutions on both real and synthetic datasets, and the results show that SACs are better than communities returned by existing solutions. Moreover, our approximation solutions perform accurately and efficiently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "48707996",
                    "name": "Z. Wang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2144442653",
                    "name": "Xiaodong Li"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "1583657241",
                    "name": "Xiaojun Chen"
                }
            ]
        },
        {
            "paperId": "06fcc127c5095ea4da7549e6ea0394666ff6d3a4",
            "title": "SpaceKey: Exploring Patterns in Spatial Databases",
            "abstract": "Spatial objects associated with keywords are prevalent in applications such as Google Maps and Twitter. Recently, the topic of spatial keyword queries has received plenty of attention. Spatial Group Keyword (SGK) search is a popular class of queries; their goal is to find a set of objects which are close to each other and are associated to a set of input keywords. In this paper, we propose SpaceKey, a system for retrieving and visualizing spatial objects returned by SGK queries. In addition to existing SGK query types, SpaceKey supports a novel query, called SPM query. An SPM query is defined by a spatial pattern, a graph whose vertices contain keywords and its edges are associated with distance constraints. The results are sets of objects that match the pattern. SpaceKey allows users to perform comparison analysis between different SGK query types. We plan to make SpaceKey an open-source web-based platform, and design API functions for software developers to plug other SGK query algorithms into our system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2110203776",
                    "name": "Jikun Wang"
                },
                {
                    "authorId": "79453319",
                    "name": "Lukito Budiman"
                },
                {
                    "authorId": "144145211",
                    "name": "G. Cong"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                }
            ]
        },
        {
            "paperId": "49b198a36b9284c29a7fb41670d03f905f74e460",
            "title": "TOAIN: A Throughput Optimizing Adaptive Index for Answering Dynamic kNN Queries on Road Networks",
            "abstract": "We study the classical kNN queries on road networks. Existing solutions mostly focus on reducing query processing time. In many applications, however, system throughput is a more important measure. We devise a mathematical model that describes throughput in terms of a number of system characteristics. We show that query time is only one of the many parameters that impact throughput. Others include update time and query/update arrival rates. We show that the traditional approach of improving query time alone is generally inadequate in optimizing throughput. Moreover, existing solutions lack flexibility in adapting to environments of different characteristics. We propose Toain, which is a very flexible algorithm that can be easily trained to adapt to a given environment for maximizing query throughput. We conduct extensive experiments on both real and synthetic data and show that Toain gives significantly higher throughput compared with existing solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                }
            ]
        },
        {
            "paperId": "6fedfed15f359495c0135c3b7e598732b9f920f9",
            "title": "Scalable Evaluation of k-NN Queries on Large Uncertain Graphs",
            "abstract": "Large graphs are prevalent in social networks, traffic networks, and biology. These graphs are often inexact. For example, in a friendship network, an edge between two nodesu andv indicates that users u and v have a close relationship. This edge may only exist with a probability. To model such information, the uncertain graph model has been proposed, in which each edge e is augmented with a probability that indicate the chance that e exists. Given a node q in an uncertain graph G, we study the k-NN query of q, which looks for k nodes in G whose distances from q are the shortest. The k-NN query can be used in friend-search, data mining, and pattern-recognition. Despite the importance of this query, it has not been well studied. In this paper, we develop a tree-based structure called the U-tree. Given a k-NN query, the U-tree produces a compact representation of G, based on which the query can be executed efficiently. Our results on real and synthetic datasets show that our algorithm can scale to large graphs, and is 75% faster than the state-of-the-art solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144442653",
                    "name": "Xiaodong Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "2590886",
                    "name": "Silviu Maniu"
                }
            ]
        },
        {
            "paperId": "91197df721ce9b82de3b553c7bd1b4fea3f3eea2",
            "title": "On Spatial Pattern Matching",
            "abstract": "In this paper, we study the spatial pattern matching (SPM) query. Given a set D of spatial objects (e.g., houses and shops), each with a textual description, we aim at finding all combinations of objects from D that match a user-defined spatial pattern P. A pattern P is a graph where vertices represent spatial objects, and edges denote distance relationships between them. The SPM query returns the instances that satisfy P. An example of P can be \"a house within 10-minute walk from a school, which is at least 2km away from a hospital\". The SPM query can benefit users such as house buyers, urban planners, and archaeologists. We prove that answering such queries is computationally intractable, and propose two efficient algorithms for their evaluation. Extensive experimental evaluation and cases studies on four real datasets show that our proposed solutions are highly effective and efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "144145211",
                    "name": "G. Cong"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "2117014417",
                    "name": "Yun Li"
                }
            ]
        },
        {
            "paperId": "b0e94ba7b279a751e75c7712f1fb3140b17679e6",
            "title": "Entropy-Based Scheduling Policy for Cross Aggregate Ranking Workloads",
            "abstract": "Many data exploration applications require the ability to identify the top-<inline-formula> <tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><inline-graphic xlink:href=\"dai-ieq1-2586062.gif\"/> </alternatives></inline-formula> results according to a scoring function. We study a class of top-<inline-formula> <tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><inline-graphic xlink:href=\"dai-ieq2-2586062.gif\"/> </alternatives></inline-formula> ranking problems where top-<inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math> <alternatives><inline-graphic xlink:href=\"dai-ieq3-2586062.gif\"/></alternatives></inline-formula> candidates in a dataset are scored with the assistance of another set. We call this class of workloads <italic>cross aggregate ranking </italic>. Example computation problems include evaluating the Hausdorff distance between two datasets, finding the medoid or radius within one dataset, and finding the closest or farthest pair between two datasets. In this paper, we propose a parallel and distributed solution to process cross aggregate ranking workloads. Our solution subdivides the aggregate score computation of each candidate into tasks while constantly maintains the tentative top-<inline-formula> <tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><inline-graphic xlink:href=\"dai-ieq4-2586062.gif\"/> </alternatives></inline-formula> results as an uncertain top-<inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math> <alternatives><inline-graphic xlink:href=\"dai-ieq5-2586062.gif\"/></alternatives></inline-formula> result set. The crux of our proposed approach lies in our entropy-based scheduling technique to determine result-yielding tasks based on their abilities to reduce the uncertainty of the tentative result set. Experimental results show that our proposed approach consistently outperforms the best existing one in two different types of cross aggregate rank workloads using real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35052883",
                    "name": "Chengcheng Dai"
                },
                {
                    "authorId": "2304090",
                    "name": "Sarana Nutanong"
                },
                {
                    "authorId": "40043945",
                    "name": "Chi-Yin Chow"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "be2652969f985f84a3323735a5e52317a8d9041a",
            "title": "Entity-Based Query Recommendation for Long-Tail Queries",
            "abstract": "Query recommendation, which suggests related queries to search engine users, has attracted a lot of attention in recent years. Most of the existing solutions, which perform analysis of users\u2019 search history (or query logs), are often insufficient for long-tail queries that rarely appear in query logs. To handle such queries, we study the use of entities found in queries to provide recommendations. Specifically, we extract entities from a query, and use these entities to explore new ones by consulting an information source. The discovered entities are then used to suggest new queries to the user. In this article, we examine two information sources: (1) a knowledge base (or KB), such as YAGO and Freebase; and (2) a click log, which contains the URLs accessed by a query user. We study how to use these sources to find new entities useful for query recommendation. We further study a hybrid framework that integrates different query recommendation methods effectively. As shown in the experiments, our proposed approaches provide better recommendations than existing solutions for long-tail queries. In addition, our query recommendation process takes less than 100ms to complete. Thus, our solution is suitable for providing online query recommendation services for search engines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50823909",
                    "name": "Zhipeng Huang"
                },
                {
                    "authorId": "2772242",
                    "name": "Bogdan Cautis"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "48270014",
                    "name": "J. Yan"
                }
            ]
        },
        {
            "paperId": "ca7381fad6ecbe6628eb72d7001b47cfae0f7698",
            "title": "Evaluating Top-k Meta Path Queries on Large Heterogeneous Information Networks",
            "abstract": "Heterogeneous information networks (HINs), which are typed graphs with labeled nodes and edges, have attracted tremendous interest from academia and industry. Given two HIN nodes s and t, and a natural number k, we study the discovery of the k most important paths in real time. The paths found can be used to support friend search, product recommendation, anomaly detection, and graph clustering. Although related algorithms have been proposed before, they were primarily designed to return the k shortest paths from unlabeled graphs. This leads to two problems: (1) there are often many shortest paths between s and t, and so it is not easy to choose the k best ones; and (2) it is arguable whether a shorter path implies a more crucial one. To address these issues, we study the top-k meta path query for a HIN. A meta path abstracts multiple path instances into a high-level path pattern, thereby giving more insight between two nodes. We further study several ranking functions that evaluate the importance of meta paths based on frequency and rarity, rather than on path length. We propose a solution that seamlessly integrates these functions into an A* search framework. The connectivity experiment on ACM dataset shows that our proposed method outperforms state-of-the-art algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140564220",
                    "name": "Zichen Zhu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "144919491",
                    "name": "Loc Do"
                },
                {
                    "authorId": "50823909",
                    "name": "Zhipeng Huang"
                },
                {
                    "authorId": "2315442655",
                    "name": "Haoci Zhang"
                }
            ]
        },
        {
            "paperId": "07fdd23ac7b30eeddd298aa4c362becdae47a6fc",
            "title": "On Embedding Uncertain Graphs",
            "abstract": "Graph data are prevalent in communication networks, social media, and biological networks. These data, which are often noisy or inexact, can be represented by uncertain graphs, whose edges are associated with probabilities to indicate the chances that they exist. Recently, researchers have studied various algorithms (e.g., clustering, classification, and k-NN) for uncertain graphs. These solutions face two problems: (1) high dimensionality: uncertain graphs are often highly complex, which can affect the mining quality; and (2) low reusability, where an existing mining algorithm has to be redesigned to deal with uncertain graphs. To tackle these problems, we propose a solution called URGE, or UnceRtain Graph Embedding. Given an uncertain graph G, URGE generates G's embedding, or a set of low-dimensional vectors, which carry the proximity information of nodes in G. This embedding enables the dimensionality of G to be reduced, without destroying node proximity information. Due to its simplicity, existing mining solutions can be used on the embedding. We investigate two low- and high-order node proximity measures in the embedding generation process, and develop novel algorithms to enable fast evaluation. To our best knowledge, there is no prior study on the use of embedding for uncertain graphs. We have further performed extensive experiments for clustering, classification, and k-NN on several uncertain graph datasets. Our results show that URGE attains better effectiveness than current uncertain data mining algorithms, as well as state-of-the-art embedding solutions. The embedding and mining performance is also highly efficient in our experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "50823909",
                    "name": "Zhipeng Huang"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                }
            ]
        },
        {
            "paperId": "175b2566c36365707923a40e369564a9edc7b530",
            "title": "Efficient Pattern-Based Aggregation on Sequence Data",
            "abstract": "A <italic>Sequence OLAP</italic> (S-OLAP) system provides a platform on which pattern-based aggregate (PBA) queries on a sequence database are evaluated. In its simplest form, a PBA query consists of a pattern template <inline-formula> <tex-math notation=\"LaTeX\">$T$</tex-math><alternatives><inline-graphic xlink:href=\"he-ieq1-2618856.gif\"/></alternatives> </inline-formula> and an aggregate function <inline-formula><tex-math notation=\"LaTeX\">$F$</tex-math><alternatives> <inline-graphic xlink:href=\"he-ieq2-2618856.gif\"/></alternatives></inline-formula>. A pattern template is a sequence of variables, each is defined over a domain. Each variable is instantiated with all possible values in its corresponding domain to derive all possible patterns of the template. Sequences are grouped based on the patterns they possess. The answer to a PBA query is a <italic>sequence cuboid</italic> (s-cuboid), which is a multidimensional array of cells. Each cell is associated with a pattern instantiated from the query's pattern template. The value of each s-cuboid cell is obtained by applying the aggregate function <inline-formula><tex-math notation=\"LaTeX\">$F$</tex-math> <alternatives><inline-graphic xlink:href=\"he-ieq3-2618856.gif\"/></alternatives></inline-formula> to the set of data sequences that belong to that cell. Since a pattern template can involve many variables and can be arbitrarily long, the induced s-cuboid for a PBA query can be huge. For most analytical tasks, however, only <italic>iceberg cells </italic> with very large aggregate values are of interest. This paper proposes an efficient approach to identifying and evaluating iceberg cells of s-cuboids. Experimental results show that our algorithms are orders of magnitude faster than existing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2966924",
                    "name": "Zhian He"
                },
                {
                    "authorId": "2807649",
                    "name": "Petrie Wong"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "145373095",
                    "name": "Eric Lo"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "7357727",
                    "name": "Ziqiang Feng"
                }
            ]
        },
        {
            "paperId": "20c0fca0975bd18957e7d6df3926297a7760c69f",
            "title": "C-Explorer: Browsing Communities in Large Graphs",
            "abstract": "Community retrieval (CR) algorithms, which enable the extraction of subgraphs from large social networks (e.g., Facebook and Twitter), have attracted tremendous interest. Various CR solutions, such as k-core and codicil, have been proposed to obtain graphs whose vertices are closely related. In this paper, we propose the C-Explorer system to assist users in extracting, visualizing, and analyzing communities. C-Explorer provides online and interactive CR facilities, allowing a user to view her interesting graphs, indicate her required vertex q, and display the communities to which q belongs. A seminal feature of C-Explorer is that it uses an attributed graph, whose vertices are associated with labels and keywords, and looks for an attributed community (or AC), whose vertices are structurally and semantically related. Moreover, C-Explorer implements several state-of-the-art CR algorithms, as well as functions for analyzing their effectiveness. We plan to make C-Explorer an open-source web-based platform, and design API functions for software developers to test their CR algorithms in our system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "2112767999",
                    "name": "Kai Huang"
                }
            ]
        },
        {
            "paperId": "248050ce9041bb628dad05789181eaac15505509",
            "title": "T-Crowd: Effective Crowdsourcing for Tabular Data",
            "abstract": "We study the effective use of crowdsourcing in filling missing values in a given relation (e.g., a table containing different attributes of celebrity stars, such as nationality and age). A task given to a worker typically consists of questions about the missing attribute values (e.g., what is the age of Jet Li?). Existing work often treats related attributes independently, leading to suboptimal performance. We present T-Crowd: a crowdsourcing system that considers attribute relationships. T-Crowd integrates each worker's answers on different attributes to effectively learn his/her trustworthiness and the true data values. Our solution seamlessly supports categorical and continuous attributes. Our experiments on real datasets show that T-Crowd outperforms state-of-the-art methods, improving the quality of truth inference.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145663545",
                    "name": "Caihua Shan"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "50823909",
                    "name": "Zhipeng Huang"
                },
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                }
            ]
        },
        {
            "paperId": "a4c25f7870f28301fd48ff232ed108ecbb8eee10",
            "title": "On Minimal Steiner Maximum-Connected Subgraph Queries",
            "abstract": "Given a graph <inline-formula><tex-math notation=\"LaTeX\">$G$</tex-math><alternatives> <inline-graphic xlink:href=\"hu-ieq1-2730873.gif\"/></alternatives></inline-formula> and a set <inline-formula> <tex-math notation=\"LaTeX\">$Q$</tex-math><alternatives><inline-graphic xlink:href=\"hu-ieq2-2730873.gif\"/></alternatives> </inline-formula> of query nodes, we examine the <italic>Steiner Maximum-Connected Subgraph</italic> (SMCS) problem. The SMCS, or <inline-formula><tex-math notation=\"LaTeX\">$G$</tex-math><alternatives> <inline-graphic xlink:href=\"hu-ieq3-2730873.gif\"/></alternatives></inline-formula>'s induced subgraph that contains <inline-formula><tex-math notation=\"LaTeX\">$Q$</tex-math><alternatives> <inline-graphic xlink:href=\"hu-ieq4-2730873.gif\"/></alternatives></inline-formula> with the largest connectivity, can be useful for customer prediction, product promotion, and team assembling. Despite its importance, the SMCS problem has only been recently studied. Existing solutions evaluate the <italic>maximum SMCS</italic>, whose number of nodes is the largest among all the SMCSs of <inline-formula><tex-math notation=\"LaTeX\">$Q$</tex-math><alternatives> <inline-graphic xlink:href=\"hu-ieq5-2730873.gif\"/></alternatives></inline-formula>. However, the maximum SMCS, which may contain a lot of nodes, can be difficult to interpret. In this paper, we investigate the <italic>minimal SMCS </italic>, which is the minimal subgraph of <inline-formula><tex-math notation=\"LaTeX\">$G$</tex-math><alternatives> <inline-graphic xlink:href=\"hu-ieq6-2730873.gif\"/></alternatives></inline-formula> with the maximum connectivity containing <inline-formula><tex-math notation=\"LaTeX\">$Q$</tex-math><alternatives> <inline-graphic xlink:href=\"hu-ieq7-2730873.gif\"/></alternatives></inline-formula>. The minimal SMCS contains much fewer nodes than its maximum counterpart, and is thus easier to be understood. However, the minimal SMCS can be costly to evaluate. We thus propose efficient Expand-Refine algorithms, as well as their approximate versions with accuracy guarantees. We further develop a cache-based processing model to improve the efficiency for an important case when <inline-formula><tex-math notation=\"LaTeX\">$Q$</tex-math><alternatives> <inline-graphic xlink:href=\"hu-ieq8-2730873.gif\"/></alternatives></inline-formula> consists of a single node. Extensive experiments on large real and synthetic graph datasets validate the effectiveness and efficiency of our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "2154765398",
                    "name": "Xiaowei Wu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                }
            ]
        },
        {
            "paperId": "b1661ee77758b396a3f3ef41e1b8420dd496b0ab",
            "title": "Truth Inference in Crowdsourcing: Is the Problem Solved?",
            "abstract": "Crowdsourcing has emerged as a novel problem-solving paradigm, which facilitates addressing problems that are hard for computers, e.g., entity resolution and sentiment analysis. However, due to the openness of crowdsourcing, workers may yield low-quality answers, and a redundancy-based method is widely employed, which first assigns each task to multiple workers and then infers the correct answer (called truth) for the task based on the answers of the assigned workers. A fundamental problem in this method is Truth Inference, which decides how to effectively infer the truth. Recently, the database community and data mining community independently study this problem and propose various algorithms. However, these algorithms are not compared extensively under the same framework and it is hard for practitioners to select appropriate algorithms. To alleviate this problem, we provide a detailed survey on 17 existing algorithms and perform a comprehensive evaluation using 5 real datasets. We make all codes and datasets public for future research. Through experiments we find that existing algorithms are not stable across different datasets and there is no algorithm that outperforms others consistently. We believe that the truth inference problem is not fully solved, and identify the limitations of existing algorithms and point out promising research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "4769653",
                    "name": "Yuanbing Li"
                },
                {
                    "authorId": "145663545",
                    "name": "Caihua Shan"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "b4c48d6f675df9cb65286cfa8e7b6f59a982ffec",
            "title": "SEQ: Example-based Query for Spatial Objects",
            "abstract": "Spatial object search is prevalent in map services (e.g., Google Maps). To rent an apartment, for example, one will take into account its nearby facilities, such as supermarkets, hospitals, and subway stations. Traditional keyword search solutions, such as the nearby function in Google Maps, are insufficient in expressing the often complex attribute/spatial requirements of users. Those require- ments, however, are essential to reflect the user search intention. In this paper, we propose the Spatial Exemplar Query (SEQ), which allows the user to input a result example over an interface inside the map service. We then propose an effective similarity measure to evaluate the proximity between a candidate answer and the given example. We conduct a user study to validate the effectiveness of SEQ. Our result shows that more than 88% of users would like to have an example assisted search in map services. Moreover, SEQ gets a user satisfactory score of 4.3/5.0, which is more than 2 times higher than that of a baseline solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "48270014",
                    "name": "J. Yan"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                }
            ]
        },
        {
            "paperId": "bb1e12133315f502c1157d6b65ddeaccf8b8a11d",
            "title": "An Indexing Framework for Queries on Probabilistic Graphs",
            "abstract": "Information in many applications, such as mobile wireless systems, social networks, and road networks, is captured by graphs. In many cases, such information is uncertain. We study the problem of querying a probabilistic graph, in which vertices are connected to each other probabilistically. In particular, we examine \u201csource-to-target\u201d queries (ST-queries), such as computing the shortest path between two vertices. The major difference with the deterministic setting is that query answers are enriched with probabilistic annotations. Evaluating ST-queries over probabilistic graphs is #P-hard, as it requires examining an exponential number of \u201cpossible worlds\u201d\u2014database instances generated from the probabilistic graph. Existing solutions to the ST-query problem, which sample possible worlds, have two downsides: (i) a possible world can be very large and (ii) many samples are needed for reasonable accuracy. To tackle these issues, we study the ProbTree, a data structure that stores a succinct, or indexed, version of the possible worlds of the graph. Existing ST-query solutions are executed on top of this structure, with the number of samples and sizes of the possible worlds reduced. We examine lossless and lossy methods for generating the ProbTree, which reflect the tradeoff between the accuracy and efficiency of query evaluation. We analyze the correctness and complexity of these approaches. Our extensive experiments on real datasets show that the ProbTree is fast to generate and small in size. It also enhances the accuracy and efficiency of existing ST-query algorithms significantly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590886",
                    "name": "Silviu Maniu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "d7dc257883aaf61fce867c40af978585b9700b59",
            "title": "Crowdsourced Data Management: Overview and Challenges",
            "abstract": "Many important data management and analytics tasks cannot be completely addressed by automated processes. Crowdsourcing is an effective way to harness human cognitive abilities to process these computer-hard tasks, such as entity resolution, sentiment analysis, and image recognition. Crowdsourced data management has been extensively studied in research and industry recently. In this tutorial, we will survey and synthesize a wide spectrum of existing studies on crowdsourced data management. We first give an overview of crowdsourcing, and then summarize the fundamental techniques, including quality control, cost control, and latency control, which must be considered in crowdsourced data management. Next we review crowdsourced operators, including selection, collection, join, top-k, sort, categorize, aggregation, skyline, planning, schema matching, mining and spatial crowdsourcing. We also discuss crowdsourcing optimization techniques and systems. Finally, we provide the emerging challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                },
                {
                    "authorId": "1704755170",
                    "name": "Ju Fan"
                },
                {
                    "authorId": "1743709",
                    "name": "Jiannan Wang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "fec574940187ec0fd7fafa546aca90d55ede3f46",
            "title": "Effective Community Search over Large Spatial Graphs",
            "abstract": "Communities are prevalent in social networks, knowledge graphs, and biological networks. Recently, the topic of community search (CS) has received plenty of attention. Given a query vertex, CS looks for a dense subgraph that contains it. Existing CS solutions do not consider the spatial extent of a community. They can yield communities whose locations of vertices span large areas. In applications that facilitate the creation of social events (e.g., finding conference attendees to join a dinner), it is important to find groups of people who are physically close to each other. In this situation, it is desirable to have a spatial-aware community (or SAC), whose vertices are close structurally and spatially. Given a graph G and a query vertex q, we develop exact solutions for finding an SAC that contains q. Since these solutions cannot scale to large datasets, we have further designed three approximation algorithms to compute an SAC. We have performed an experimental evaluation for these solutions on both large real and synthetic datasets. Experimental results show that SAC is better than the communities returned by existing solutions. Moreover, our approximation solutions can find SACs accurately and efficiently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2144442653",
                    "name": "Xiaodong Li"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                }
            ]
        },
        {
            "paperId": "070ae1beb3a2f11482565a6f5fa364297e5a326c",
            "title": "Managing uncertainty of large spatial databases",
            "abstract": "Spatial data are prevalent in location-based services (LBS), sensor networks, and RFID monitoring systems. Data readings collected in these applications are often imprecise. The uncertainty in the data can arise from multiple sources, including measurement errors due to the sensing instrument and discrete sampling of the measurements. It is often important to record the imprecision and also to take it into account when processing the spatial data. The challenges of handling the uncertainty in spatial data includes modeling, semantics, query operators and types, efficient execution, and user interfaces. Probabilistic models have been proposed for handling the uncertainty. In this paper, we examine the modeling and querying issues of this kind of databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "2a52bdd306ce9c8ecf1ab008435b560a0fa29d6a",
            "title": "DOCS: a domain-aware crowdsourcing system using knowledge bases",
            "abstract": "Crowdsourcing is a new computing paradigm that harnesses human effort to solve computer-hard problems, such as entity resolution and photo tagging. The crowd (or workers) have diverse qualities and it is important to effectively model a worker's quality. Most of existing worker models assume that workers have the same quality on different tasks. In practice, however, tasks belong to a variety of diverse domains, and workers have different qualities on different domains. For example, a worker who is a basketball fan should have better quality for the task of labeling a photo related to 'Stephen Curry' than the one related to 'Leonardo DiCaprio'. In this paper, we study how to leverage domain knowledge to accurately model a worker's quality. We examine using knowledge base (KB), e.g., Wikipedia and Freebase, to detect the domains of tasks and workers. We develop Domain Vector Estimation, which analyzes the domains of a task with respect to the KB. We also study Truth Inference, which utilizes the domain-sensitive worker model to accurately infer the true answer of a task. We design an Online Task Assignment algorithm, which judiciously and efficiently assigns tasks to appropriate workers. To implement these solutions, we have built DOCS, a system deployed on the Amazon Mechanical Turk. Experiments show that DOCS performs much better than the state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "2a775a2d9630f1b70aef37e80ac57b1bc5b066f6",
            "title": "Querying Minimal Steiner Maximum-Connected Subgraphs in Large Graphs",
            "abstract": "Given a graph G and a set Q of query nodes, we examine the Steiner Maximum-Connected Subgraph (SMCS). The SMCS, or G's induced subgraph that contains Q with the largest connectivity, can be useful for customer prediction, product promotion, and team assembling. Despite its importance, the SMCS problem has only been recently studied. Existing solutions evaluate the maximum SMCS, whose number of nodes is the largest among all the SMCSs of Q. However, the maximum SMCS, which may contain a lot of nodes, can be difficult to interpret. In this paper, we investigate the minimal SMCS, which is the minimal subgraph of G with the maximum connectivity containing Q. The minimal SMCS contains much fewer nodes than its maximum counterpart, and is thus easier to be understood. However, the minimal SMCS can be costly to evaluate. We thus propose efficient Expand-Refine algorithms, as well as their approximate versions with accuracy guarantees. Extensive experiments on six large real graph datasets validate the effectiveness and efficiency of our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                },
                {
                    "authorId": "2154765398",
                    "name": "Xiaowei Wu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                }
            ]
        },
        {
            "paperId": "3630eb92cbe625060c796fdef5e7565ee5d30bae",
            "title": "Meta Structure: Computing Relevance in Large Heterogeneous Information Networks",
            "abstract": "A heterogeneous information network (HIN) is a graph model in which objects and edges are annotated with types. Large and complex databases, such as YAGO and DBLP, can be modeled as HINs. A fundamental problem in HINs is the computation of closeness, or relevance, between two HIN objects. Relevance measures can be used in various applications, including entity resolution, recommendation, and information retrieval. Several studies have investigated the use of HIN information for relevance computation, however, most of them only utilize simple structure, such as path, to measure the similarity between objects. In this paper, we propose to use meta structure, which is a directed acyclic graph of object types with edge types connecting in between, to measure the proximity between objects. The strength of meta structure is that it can describe complex relationship between two HIN objects (e.g., two papers in DBLP share the same authors and topics). We develop three relevance measures based on meta structure. Due to the computational complexity of these measures, we further design an algorithm with data structures proposed to support their evaluation. Our extensive experiments on YAGO and DBLP show that meta structure-based relevance is more effective than state-of-the-art approaches, and can be efficiently computed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50823909",
                    "name": "Zhipeng Huang"
                },
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "47875796",
                    "name": "Xiang Li"
                }
            ]
        },
        {
            "paperId": "66ddc455215b677c70b19e15bf57b78e6166100b",
            "title": "DOCS: Domain-Aware Crowdsourcing System",
            "abstract": "Crowdsourcing is a new computing paradigm that harnesses human effort to solve computer-hard problems, such as entity resolution and photo tagging. The crowd (or workers) have diverse qualities and it is important to effectively model a worker\u2019s quality. Most of existing worker models assume that workers have the same quality on different tasks. In practice, however, tasks belong to a variety of diverse domains, and workers have different qualities on different domains. For example, a worker who is a basketball fan should have better quality for the task of labeling a photo related to \u2018Stephen Curry\u2019 than the one related to \u2018Leonardo DiCaprio\u2019. In this paper, we study how to leverage domain knowledge to accurately model a worker\u2019s quality. We examine using knowledge base (KB), e.g., Wikipedia and Freebase, to detect the domains of tasks and workers. We develop Domain Vector Estimation, which analyzes the domains of a task with respect to the KB. We also study Truth Inference, which utilizes the domain-sensitive worker model to accurately infer the true answer of a task. We design an Online Task Assignment algorithm, which judiciously and efficiently assigns tasks to appropriate workers. To implement these solutions, we have built DOCS, a system deployed on the Amazon Mechanical Turk. Experiments show that DOCS performs much better than the state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "714d7187e8d2198d4b0c29913a8dd70565560359",
            "title": "Scalable Algorithms for Nearest-Neighbor Joins on Big Trajectory Data",
            "abstract": "Trajectory data are prevalent in systems that monitor the locations of moving objects. In a location-based service, for instance, the positions of vehicles are continuously monitored through GPS; the trajectory of each vehicle describes its movement history. We study joins on two sets of trajectories, generated by two sets M and R of moving objects. For each entity in M, a join returns its k nearest neighbors from R. We examine how this query can be evaluated in cloud environments. This problem is not trivial, due to the complexity of the trajectory, and the fact that both the spatial and temporal dimensions of the data have to be handled. To facilitate this operation, we propose a parallel solution framework based on MapReduce. We also develop a novel bounding technique, which enables trajectories to be pruned in parallel. Our approach can be used to parallelize existing single-machine trajectory join algorithms. We also study a variant of the join, which can further improve query efficiency. To evaluate the efficiency and the scalability of our solutions, we have performed extensive experiments on large real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "37081584",
                    "name": "Wenbin Tang"
                },
                {
                    "authorId": "2590886",
                    "name": "Silviu Maniu"
                },
                {
                    "authorId": "1764231",
                    "name": "Xuan S. Yang"
                }
            ]
        },
        {
            "paperId": "751d8c4c8c3615d904c5dbce39898fa723870a7f",
            "title": "Effective Community Search for Large Attributed Graphs",
            "abstract": "Given a graph G and a vertex q \u2208 G, the community search query returns a subgraph of G that contains vertices related to q. Communities, which are prevalent in attributed graphs such as social networks and knowledge bases, can be used in emerging applications such as product advertisement and setting up of social events. In this paper, we investigate the attributed community query (or ACQ), which returns an attributed community (AC) for an attributed graph. The AC is a subgraph of G, which satisfies both structure cohesiveness (i.e., its vertices are tightly connected) and keyword cohesiveness (i.e., its vertices share common keywords). The AC enables a better understanding of how and why a community is formed (e.g., members of an AC have a common interest in music, because they all have the same keyword \"music\"). An AC can be \"personalized\"; for example, an ACQ user may specify that an AC returned should be related to some specific keywords like \"research\" and \"sports\". \n \nTo enable efficient AC search, we develop the CL-tree index structure and three algorithms based on it. We evaluate our solutions on four large graphs, namely Flickr, DBLP, Tencent, and DBpedia. Our results show that ACs are more effective and efficient than existing community retrieval approaches. Moreover, an AC contains more precise and personalized information than that of existing community search and detection methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "2219045206",
                    "name": "Jiafeng Hu"
                }
            ]
        },
        {
            "paperId": "c5ea13351aecf45997f6ce67a6d0e9dde898868b",
            "title": "Guest Editorial: Advances in Social Computing",
            "abstract": "We are pleased to introduce this special issue of the Journal of Information Technology on advances in social computing. The social computing field is becoming a very important part of information technology as the rapid development of mobile intelligent devices, cloud storage, social media and so on. Various social applications such as blogs, instant messaging, social network services (Facebook, MySpace, Twitter, LinkedIn, etc.), wikis, and social bookmarking have been widely popularized where people interact socially beyond time and space limits. Social computing within an organization can empower and motivate employees' cooperation and as a consequence benefit for the business. It is also used to provide better services to their customers for sale promotion. Having such a wide range of applications, social computing has emerged as an exciting multidisciplinary area of research that broadly refers to computing-supported approaches for providing a deep understanding of user behaviors within a social context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47366519",
                    "name": "Yuqing Sun"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "144010725",
                    "name": "B. Hu"
                }
            ]
        },
        {
            "paperId": "e85a79d5bb433d44dede0389162f40fd8fd75b73",
            "title": "KB-Enabled Query Recommendation for Long-Tail Queries",
            "abstract": "In recent years, query recommendation algorithms have been designed to provide related queries for search engine users. Most of these solutions, which perform extensive analysis of users' search history (or query logs), are largely insufficient for long-tail queries that rarely appear in query logs. To handle such queries, we study a new solution, which makes use of a knowledge base (or KB), such as YAGO and Freebase. A KB is a rich information source that describes how real-world entities are connected. We extract entities from a query, and use these entities to explore new ones in the KB. Those discovered entities are then used to suggest new queries to the user. As shown in our experiments, our approach provides better recommendation results for long-tail queries than existing solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50823909",
                    "name": "Zhipeng Huang"
                },
                {
                    "authorId": "2772242",
                    "name": "Bogdan Cautis"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                }
            ]
        },
        {
            "paperId": "f1ec11281f157e488a6385dbdb823a54cb6f79c6",
            "title": "Crowdsourced POI labelling: Location-aware result inference and Task Assignment",
            "abstract": "Identifying the labels of points of interest (POIs), aka POI labelling, provides significant benefits in location-based services. However, the quality of raw labels manually added by users or generated by artificial algorithms cannot be guaranteed. Such low-quality labels decrease the usability and result in bad user experiences. In this paper, by observing that crowdsourcing is a best-fit for computer-hard tasks, we leverage crowdsourcing to improve the quality of POI labelling. To our best knowledge, this is the first work on crowdsourced POI labelling tasks. In particular, there are two sub-problems: (1) how to infer the correct labels for each POI based on workers' answers, and (2) how to effectively assign proper tasks to workers in order to make more accurate inference for next available workers. To address these two problems, we propose a framework consisting of an inference model and an online task assigner. The inference model measures the quality of a worker on a POI by elaborately exploiting (i) worker's inherent quality, (ii) the spatial distance between the worker and the POI, and (iii) the POI influence, which can provide reliable inference results once a worker submits an answer. As workers are dynamically coming, the online task assigner judiciously assigns proper tasks to them so as to benefit the inference. The inference model and task assigner work alternately to continuously improve the overall quality. We conduct extensive experiments on a real crowdsourcing platform, and the results on two real datasets show that our method significantly outperforms state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802748",
                    "name": "Huiqi Hu"
                },
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                },
                {
                    "authorId": "39505779",
                    "name": "Z. Bao"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "145556736",
                    "name": "Jianhua Feng"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "413395a9b6e363e0f9f631dac542dd186dd04529",
            "title": "Discovering Meta-Paths in Large Heterogeneous Information Networks",
            "abstract": "The Heterogeneous Information Network (HIN) is a graph data model in which nodes and edges are annotated with class and relationship labels. Large and complex datasets, such as Yago or DBLP, can be modeled as HINs. Recent work has studied how to make use of these rich information sources. In particular, meta-paths, which represent sequences of node classes and edge types between two nodes in a HIN, have been proposed for such tasks as information retrieval, decision making, and product recommendation. Current methods assume meta-paths are found by domain experts. However, in a large and complex HIN, retrieving meta-paths manually can be tedious and difficult. We thus study how to discover meta-paths automatically. Specifically, users are asked to provide example pairs of nodes that exhibit high proximity. We then investigate how to generate meta-paths that can best explain the relationship between these node pairs. Since this problem is computationally intractable, we propose a greedy algorithm to select the most relevant meta-paths. We also present a data structure to enable efficient execution of this algorithm. We further incorporate hierarchical relationships among node classes in our solutions. Extensive experiments on real-world HIN show that our approach captures important meta-paths in an efficient and scalable manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2574673",
                    "name": "Changping Meng"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2590886",
                    "name": "Silviu Maniu"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                },
                {
                    "authorId": "2516526",
                    "name": "Wangda Zhang"
                }
            ]
        },
        {
            "paperId": "58f9a394654d9cd8439a8f180906c9a8e18f84d4",
            "title": "Online Influence Maximization",
            "abstract": "Social networks are commonly used for marketing purposes. For example, free samples of a product can be given to a few influential social network users (or seed nodes), with the hope that they will convince their friends to buy it. One way to formalize this objective is through the problem of influence maximization (or IM), whose goal is to find the best seed nodes to activate under a fixed budget, so that the number of people who get influenced in the end is maximized. Solutions to IM rely on the influence probability that a user influences another one. However, this probability information may be unavailable or incomplete. In this paper, we study IM in the absence of complete information on influence probability. We call this problem Online Influence Maximization (OIM), since we learn influence probabilities at the same time we run influence campaigns. To solve OIM, we propose a multiple-trial approach, where (1) some seed nodes are selected based on existing influence information; (2) an influence campaign is started with these seed nodes; and (3) user feedback is used to update influence information. We adopt Explore-Exploit strategies, which can select seed nodes using either the current influence probability estimation (exploit), or the confidence bound on the estimation (explore). Any existing IM algorithm can be used in this framework. We also develop an incremental algorithm that can significantly reduce the overhead of handling user feedback information. Our experiments show that our solution is more effective than traditional IM methods on the partial information.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "1726980",
                    "name": "Siyu Lei"
                },
                {
                    "authorId": "2590886",
                    "name": "Silviu Maniu"
                },
                {
                    "authorId": "2076891",
                    "name": "Luyi Mo"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "6ac7262028d905ad97bbabce37c610e5d84e4c6f",
            "title": "On Optimality of Jury Selection in Crowdsourcing",
            "abstract": "Recent advances in crowdsourcing technologies enable computationally challenging tasks (e.g., sentiment analysis and entity resolution) to be performed by Internet workers, driven mainly by monetary incentives. A fundamental question is: how should workers be selected, so that the tasks in hand can be accomplished successfully and economically? In this paper, we study the Jury Selection Problem (JSP): Given a monetary budget, and a set of decision-making tasks (e.g., \u201cIs Bill Gates still the CEO of Microsoft now?\u201d), return the set of workers (called jury), such that their answers yield the highest \u201cJury Quality\u201d (or JQ). Existing JSP solutions make use of the Majority Voting (MV) strategy, which uses the answer chosen by the largest number of workers. We show that MV does not yield the best solution for JSP. We further prove that among all voting strategies (including deterministic and randomizedstrategies), BayesianVoting(BV)canoptimallysolveJSP. We then examine how to solve JSP based on BV. This is technically challenging, since computing the JQ with BV is NP-hard. We solve this problem by proposing an approximate algorithm that is computationally efficient. Our approximate JQ computation algorithm is also highly accurate, and its error is proved to be bounded within 1%. We extend our solution by considering the task owner\u2019s \u201cbelief\u201d (or prior) on the answers of the tasks. Experiments on synthetic and real datasets show that our new approach is consistently better than the best JSP solution known.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2590886",
                    "name": "Silviu Maniu"
                },
                {
                    "authorId": "2076891",
                    "name": "Luyi Mo"
                }
            ]
        },
        {
            "paperId": "b34c083e169a9389887d16b84fa791e1c9df027a",
            "title": "QASCA: A Quality-Aware Task Assignment System for Crowdsourcing Applications",
            "abstract": "A crowdsourcing system, such as the Amazon Mechanical Turk (AMT), provides a platform for a large number of questions to be answered by Internet workers. Such systems have been shown to be useful to solve problems that are difficult for computers, including entity resolution, sentiment analysis, and image recognition. In this paper, we investigate the online task assignment problem: Given a pool of n questions, which of the k questions should be assigned to a worker? A poor assignment may not only waste time and money, but may also hurt the quality of a crowdsourcing application that depends on the workers' answers. We propose to consider quality measures (also known as evaluation metrics) that are relevant to an application during the task assignment process. Particularly, we explore how Accuracy and F-score, two widely-used evaluation metrics for crowdsourcing applications, can facilitate task assignment. Since these two metrics assume that the ground truth of a question is known, we study their variants that make use of the probability distributions derived from workers' answers. We further investigate online assignment strategies, which enables optimal task assignments. Since these algorithms are expensive, we propose solutions that attain high quality in linear time. We develop a system called the Quality-Aware Task Assignment System for Crowdsourcing Applications (QASCA) on top of AMT. We evaluate our approaches on five real crowdsourcing applications. We find that QASCA is efficient, and attains better result quality (of more than 8% improvement) compared with existing methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2985907",
                    "name": "Yudian Zheng"
                },
                {
                    "authorId": "1743709",
                    "name": "Jiannan Wang"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145556736",
                    "name": "Jianhua Feng"
                }
            ]
        },
        {
            "paperId": "ea5c87cb46ea8acfb4896904cabe231e25c6ccea",
            "title": "Walking in the Cloud: Parallel SimRank at Scale",
            "abstract": "\n Despite its popularity, SimRank is computationally costly, in both time and space. In particular, its recursive nature poses a great challenge in using modern distributed computing power, and also prevents querying similarities individually. Existing solutions suffer greatly from these practical issues. In this paper, we break such dependency for maximum efficiency possible. Our method consists of offline and online phases. In offline phase, a length-\n n\n indexing vector is derived by solving a linear system in parallel. At online query time, the similarities are computed instantly from the index vector. Throughout, the Monte Carlo method is used to maximally reduce time and space. Our algorithm, called CloudWalker, is highly parallelizable, with only linear time and space. Remarkably, it responses to both single-pair and single-source queries in constant time. CloudWalker is orders of magnitude more efficient and scalable than existing solutions for large-scale problems. Implemented on Spark with 10 machines and tested on the web-scale clue-web graph with 1 billion nodes and 43 billion edges, it takes 110 hours for offline indexing, 64 seconds for a single-pair query, and 188 seconds for a single-source query. To the best of our knowledge, our work is the first to report results on clue-web, which is 10x larger than the largest graph ever reported for SimRank computation.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7718952",
                    "name": "Zhenguo Li"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "2112263136",
                    "name": "Qin Liu"
                },
                {
                    "authorId": "40374706",
                    "name": "Jiefeng Cheng"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145462046",
                    "name": "John C.S. Lui"
                }
            ]
        },
        {
            "paperId": "0a1627c4c811446651b555ce5b06e267826ede8d",
            "title": "Web technologies and applications : 17th Asia-Pacific Web Conference, APWeb 2015, Guangzhou, China, September 18-20, 2015 : proceedings",
            "abstract": "On The Marriage of SPARQL and Keywords.- An Online Inference Algorithm for Labeled Latent Dirichlet Allocation.- Efficient Buffer Management for PCM-Enhanced Hybrid Memory Architecture.- Efficient Location-dependent Skyline Queries in Wireless Broadcast Environments.- Distance and Friendship: A Distance-based Model for Link Prediction in Social Networks.- Multi-Label Emotion Tagging for Online News by Supervised Topic Model.- Distinguishing Specific and Daily topics.- Matching Reviews to Object Based on 2-Stage CRF.- Discovering Restricted Regular Expressions with Interleaving.- Efficient Algorithms for Distance-based Representative Skyline Computation in 2D Space.- Trustworthy Collaborative Filtering through Downweighting Noise and Redundancy.- A Co-Ranking Framework to Select Optimal Seed Set for Influence Maximization in Heterogeneous Network.- Hashtag Sense Induction Based on Co-Occurrence Graphs.- Hybrid-LSH for Spatio-Textual Similarity Queries.- Sleep Quality Evaluation of Active Microblog Users.- Distributed XML Twig Query Processing using MapReduce.- Sentiment Word Identification with Sentiment Contextual Factors.- Large-Scale Graph Classification based on Evolutionary Computation with Map Reduce.- Multiple Attribute Aware Personalized Ranking.- Knowledge Base Completion Using Matrix Factorization.- MATAR: Keywords Enhanced Multi-Label Learning for Tag Recommendation.- Reverse Direction-based Surrounder Queries.- Learning to Hash for Recommendation with Tensor Data.- Spare Part Demand Prediction based on Context-aware Matrix Factorization.- Early Outbreak Detection in Social Networks.- Location Sensitive Friend Recommendation in Social Network.- A Compression-Based Filtering Mechanism in Content-Based Publish/Subscribe System.- Sentiment Classification for Chinese Product Reviews Based on Semantic Relevance of Phrase.- Overlapping Schema Summarization based on Multi-label Propagation.- Analysis of Subjective City Happiness Index Based on Large Scale Microblog Data.- Tree-Based Metric Learning for Distance Computation in Data Mining.- GSCS - Graph Stream Classification with Side Information.- A Supervised Parameter Estimation Method of LDA.- Learning Similarity Functions for Urban Events Detection by Mining Hotline Phone Records.- Answering Spatial Approximate Keyword Queries on Disks.- Hashing Multi-Instance Data from Bag and Instance Level.- A Multi-news Timeline Summarization Algorithm Based on Aging Theory.- Extended Strategies for Document Clustering with Word Co-occurrences.- A Lightweight Evaluation Framework for Table Layouts in MapReduce Based Query Systems.- An Extended Graph-based Label Propagation Method for Readability Assessment.- Simple is Beautiful: An Online Collaborative Filtering Recommendation Solution with Higher Accuracy.- Random-based Algorithm for Efficient Entity Matching.- A New Similarity Measure Between Semantic Trajectories based on Road Networks.- Effective Citation Recommendation by Unbiased Reference Priority Recognition.- UserGreedy: Exploiting the Activation Set to Solve Influence Maximization Problem.- AILabel: A Fast Interval Labeling Approach for Reachability Query on Very Large Graphs.- Graph-based Hybrid Recommendation Using Random Walk and Topic Modeling.- RDQS: A Relevant and Diverse Query Suggestion Generation Framework.- Minimizing the cost to win competition in social network.- Ad Dissemination Game in Ephemeral Networks.- User Generated Content Oriented Chinese Taxonomy Construction.- Mining Weighted Frequent Itemsets with the Recency Constraint.- Boosting Explicit Semantic Analysis by Clustering Paragraph Vectors of Wikipedia Articles.- Research on Semantic Disambiguation in Treebank.- PDMA: A Probabilistic Framework for Diversifying Recommendation Lists.- User Behavioral Context-Aware Service Recommendation for Personalized Mashups in Pervasive Environments.- On Coherent Indented Tree Visualization of RDF Graphs.- Online Feature Selection based on Passive-Aggressive Algorithm with Retaining Features.- Online Personalized Recommendation Based on Streaming Implicit User Feedback.- A Self-Learning Rule-based Approach for Sci-tech Compound Phrase Entity Recognition.- Batch Mode Active Learning For Geographical Image Classification.- A Multi-view Retweeting Behaviors Prediction in Social Networks.- Probabilistic Frequent Pattern Mining by PUH-Mine.- A Secure and Efficient Framework for Privacy Preserving Social Recommendation.- DistDL: A Distributed Deep Learning Service Schema with GPU Accelerating.- A Semi-Supervised Solution for Cold Start Issue on Recommender Systems.- Hybrid Cloud Deployment of an ERP-based Student Administration System.- A Benchmark Evaluation of Enterprise Cloud Infrastructure.- A Fast Data Ingestion and Indexing Scheme for Real-time Log Analytics.- A Fair Data Market System with Data Quality Evaluation and Repairing Recommendation.- HouseIn: A Housing Rental Platform with Non-Redundant Information Integrated from Multiple Sources.- ONCAPS: An Ontology-based Car Purchase Guiding System.- A Multiple Trust Paths Selection Tool in Contextual Online Social Networks.- EPEMS: An Entity Matching System For E-Commerce Products.- PPS-POI-Rec: A Privacy Preserving Social Point-of-Interest Recommender System.- Incorporating Contextual Information into a Mobile Advertisement Recommender System.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "1bc88f017881715511df71115d3cdb5d1e694e92",
            "title": "Evaluating multi-way joins over discounted hitting time",
            "abstract": "The discounted hitting time (DHT), which is a random-walk similarity measure for graph node pairs, is useful in various applications, including link prediction, collaborative recommendation, and reputation ranking. We examine a novel query, called the multi-way join (or n-way join), on DHT scores. Given a graph and n sets of nodes, the n-way join retrieves a set of n-tuples with the k highest scores, according to some aggregation function of DHT values. This query enables analysis and prediction of complex relationship among n sets of nodes. Since an n-way join is expensive to compute, we develop the Partial Join algorithm (or PJ). This solution decomposes an n-way join into a number of top-m 2-way joins, and combines their results to construct the answer of the n-way join. Since PJ may necessitate the computation of top-(m+ 1) 2-way joins, we study an incremental solution, which allows the top-(m+ 1) 2-way join to be derived quickly from the top-m 2-way join results earlier computed. We further examine fast processing and pruning algorithms for 2-way joins. An extensive evaluation on three real datasets shows that PJ accurately evaluates n-way joins, and is four orders of magnitude faster than basic solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2516526",
                    "name": "Wangda Zhang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                }
            ]
        },
        {
            "paperId": "404a12bf70684b74197ead49066f22d4be00c1de",
            "title": "CLUDE: An Efficient Algorithm for LU Decomposition Over a Sequence of Evolving Graphs",
            "abstract": "In many applications, entities and their relationships are represented by graphs. Examples include the WWW (web pages and hyperlinks) and bibliographic networks (authors and co-authorship). A graph can be conveniently modeled by a matrix from which various quantitative measures are derived. Some example measures include PageRank and SALSA (which measure nodes\u2019 importance), and Personalized PageRank and Random Walk with Restart (which measure proximities between nodes). To compute these measures, linear systems of the form Ax = b, where A is a matrix that captures a graph\u2019s structure, need to be solved. To facilitate solving the linear system, the matrix A is often decomposed into two triangular matrices (L and U). In a dynamic world, the graph that models it changes with time and thus is the matrix A that represents the graph. We consider a sequence of evolving graphs and its associated sequence of evolving matrices. We study how LU-decomposition should be done over the sequence so that (1) the decomposition is efficient and (2) the resulting LU matrices best preserve the sparsity of the matrices A\u2019s (i.e., the number of extra non-zero entries introduced in L and U are minimized.) We propose a cluster-based algorithm CLUDE for solving the problem. Through an experimental study, we show that CLUDE is about an order of magnitude faster than the traditional incremental update algorithm. The number of extra non-zero entries introduced by CLUDE is also about an order of magnitude fewer than that of the traditional algorithm. CLUDE is thus an efficient algorithm for LU decomposition that produces high-quality LU matrices over an evolving matrix sequence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40641823",
                    "name": "Chenghui Ren"
                },
                {
                    "authorId": "2076891",
                    "name": "Luyi Mo"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                }
            ]
        },
        {
            "paperId": "65ab4132df1c1efb28c68e8efe565a4ff9158c1d",
            "title": "Managing uncertainty in spatial and spatio-temporal data",
            "abstract": "Location-related data has a tremendous impact in many applications of high societal relevance and its growing volume from heterogeneous sources is one true example of a Big Data [1]. An inherent property of any spatio-temporal dataset is uncertainty due to various sources of imprecision. This tutorial provides a comprehensive overview of the different challenges involved in managing uncertain spatial and spatio-temporal data and presents state-of-the-art techniques for addressing them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2277396",
                    "name": "Tobias Emrich"
                },
                {
                    "authorId": "1688561",
                    "name": "H. Kriegel"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "1723035",
                    "name": "M. Renz"
                },
                {
                    "authorId": "1776969",
                    "name": "Goce Trajcevski"
                },
                {
                    "authorId": "3221975",
                    "name": "Andreas Z\u00fcfle"
                }
            ]
        },
        {
            "paperId": "6f7de974396b99347a1921273536e9fca04a4572",
            "title": "Uncertain Data Classification",
            "abstract": "16.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "2096228933",
                    "name": "M. Renz"
                }
            ]
        },
        {
            "paperId": "a5d12cce7e53c9832496c8e4c8f03081250c70ba",
            "title": "ProbTree: a query-efficient representation of probabilistic graphs",
            "abstract": "Information in many applications, such as mobile wireless systems, social networks, and road networks, is captured by graphs, in many cases uncertain. We study the problem of querying a probabilistic graph; in particular, we examine \u201csource-to-target\u201d queries, such as computing the shortest path between two vertices. Evaluating ST-queries over probabilistic graphs is #P-hard, as it requires examining an exponential number of \u201cpossible worlds\u201d. Existing solutions to the ST-query problem, which sample possible worlds, have two downsides: (i) many samples are needed for reasonable accuracy, and (ii) a possible world can be very large. To tackle these issues, we study the ProbTree, a data structure that stores a succinct representation of the probabilistic graph. Existing ST-query solutions are executed on top of this structure, with the number of samples and possible world sizes reduced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590886",
                    "name": "Silviu Maniu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "d371018088eaff333dd737460b5858d276ab0601",
            "title": "iTag: Incentive-based tagging",
            "abstract": "In social tagging systems, such as Delicious1 and Flickr2, users are allowed to annotate resources (e.g., Web URLs and images) with textual descriptions called tags. Tags have proven to be invaluable building blocks in algorithms for searching, mining and recommending resources. In practice, however, not all resources receive the same attention from users, and as a result, most tags are added to the few highly-popular resources, while most of the resources receive few tags. Crucially, this incomplete tagging on resources can severely affect the effectiveness of all tagging applications. We present iTag, an incentive-based tagging system, which aims at improving tagging quality of resources, by incentivizing taggers under budget constraints. Our system is built upon traditional crowdsourcing systems such as Amazon Mechanical Turk (MTurk). In our demonstration, we will show how our system allows users to use simple but powerful strategies to significantly improve the tagging quality of resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1726980",
                    "name": "Siyu Lei"
                },
                {
                    "authorId": "1764231",
                    "name": "Xuan S. Yang"
                },
                {
                    "authorId": "2076891",
                    "name": "Luyi Mo"
                },
                {
                    "authorId": "2590886",
                    "name": "Silviu Maniu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "f4237d76799416b0720e4b1749404a135fc954f0",
            "title": "Scalable Evaluation of Trajectory Queries over Imprecise Location Data",
            "abstract": "Trajectory queries, which retrieve nearby objects for every point of a given route, can be used to identify alerts of potential threats along a vessel route, or monitor the adjacent rescuers to a travel path. However, the locations of these objects (e.g., threats, succours) may not be precisely obtained due to hardware limitations of measuring devices, as well as complex natures of the surroundings. For such data, we consider a common model, where the possible locations of an object are bounded by a closed region, called \u201cimprecise region\u201d. Ignoring or coarsely wrapping imprecision can render low query qualities, and cause undesirable consequences such as missing alerts of threats and poor response rescue time. Also, the query is quite time-consuming, since all points on the trajectory are considered. In this paper, we study how to efficiently evaluate trajectory queries over imprecise objects, by proposing a novel concept, u-bisector, which is an extension of bisector specified for imprecise data. Based on the u-bisector, we provide an efficient and versatile solution which supports different shapes of commonly-used imprecise regions (e.g., rectangles, circles, and line segments). Extensive experiments on real datasets show that our proposal achieves better efficiency, quality, and scalability than its competitors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1678927",
                    "name": "Xike Xie"
                },
                {
                    "authorId": "1722082",
                    "name": "Man Lung Yiu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "144106863",
                    "name": "Hua Lu"
                }
            ]
        },
        {
            "paperId": "365e500333c586a38dc1a08901239c4129df6a91",
            "title": "Optimizing plurality for human intelligence tasks",
            "abstract": "In a crowdsourcing system, Human Intelligence Tasks (HITs) (e.g., translating sentences, matching photos, tagging videos with keywords) can be conveniently specified. HITs are made available to a large pool of workers, who are paid upon completing the HITs they have selected. Since workers may have different capabilities, some difficult HITs may not be satisfactorily performed by a single worker. If more workers are employed to perform a HIT, the quality of the HIT's answer could be statistically improved. Given a set of HITs and a fixed \"budget\", we address the important problem of determining the number of workers (or plurality) of each HIT so that the overall answer quality is optimized. We propose a dynamic programming (DP) algorithm for solving the plurality assignment problem (PAP). We identify two interesting properties, namely, monotonicity and diminishing return, which are satisfied by a HIT if the quality of the HIT's answer increases monotonically at a decreasing rate with its plurality. We show for HITs that satisfy the two properties (e.g., multiple-choice-question HITs), the PAP is approximable. We propose an efficient greedy algorithm for such case. We conduct extensive experiments on synthetic and real datasets to evaluate our algorithms. Our experiments show that our greedy algorithm provides close-to-optimal solutions in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2076891",
                    "name": "Luyi Mo"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "1764231",
                    "name": "Xuan S. Yang"
                },
                {
                    "authorId": "40641823",
                    "name": "Chenghui Ren"
                },
                {
                    "authorId": "1726980",
                    "name": "Siyu Lei"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                },
                {
                    "authorId": "145373095",
                    "name": "Eric Lo"
                }
            ]
        },
        {
            "paperId": "4617730ff2bc089733c26b75c6b80b4355c1e8ea",
            "title": "Voronoi-based nearest neighbor search for multi-dimensional uncertain databases",
            "abstract": "In Voronoi-based nearest neighbor search, the Voronoi cell of every point p in a database can be used to check whether p is the closest to some query point q. We extend the notion of Voronoi cells to support uncertain objects, whose attribute values are inexact. Particularly, we propose the Possible Voronoi cell (or PV-cell). A PV-cell of a multi-dimensional uncertain object o is a region R, such that for any point p\u03f5R, o may be the nearest neighbor of p. If the PV-cells of all objects in a database S are known, they can be used to identify objects that have a chance to be the nearest neighbor of q. However, there is no efficient algorithm for computing an exact PV-cell. We hence study how to derive an axis-parallel hyper-rectangle (called the Uncertain Bounding Rectangle, or UBR) that tightly contains a PV-cell. We further develop the PV-index, a structure that stores UBRs, to evaluate probabilistic nearest neighbor queries over uncertain data. An advantage of the PV-index is that upon updates on S, it can be incrementally updated. Extensive experiments on both synthetic and real datasets are carried out to validate the performance of the PV-index.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31008643",
                    "name": "Peiwu Zhang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "1723035",
                    "name": "M. Renz"
                },
                {
                    "authorId": "3221975",
                    "name": "Andreas Z\u00fcfle"
                },
                {
                    "authorId": "2119309654",
                    "name": "Yu Tang"
                },
                {
                    "authorId": "2277396",
                    "name": "Tobias Emrich"
                }
            ]
        },
        {
            "paperId": "7bffe31cd4e39d21b890b7abf38c9e9004941143",
            "title": "Cleaning uncertain data for top-k queries",
            "abstract": "The information managed in emerging applications, such as sensor networks, location-based services, and data integration, is inherently imprecise. To handle data uncertainty, probabilistic databases have been recently developed. In this paper, we study how to quantify the ambiguity of answers returned by a probabilistic top-k query. We develop efficient algorithms to compute the quality of this query under the possible world semantics. We further address the cleaning of a probabilistic database, in order to improve top-k query quality. Cleaning involves the reduction of ambiguity associated with the database entities. For example, the uncertainty of a temperature value acquired from a sensor can be reduced, or cleaned, by requesting its newest value from the sensor. While this \u201ccleaning operation\u201d may produce a better query result, it may involve a cost and fail. We investigate the problem of selecting entities to be cleaned under a limited budget. Particularly, we propose an optimal solution and several heuristics. Experiments show that the greedy algorithm is efficient and close to optimal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2076891",
                    "name": "Luyi Mo"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2144440567",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                },
                {
                    "authorId": "1764231",
                    "name": "Xuan S. Yang"
                }
            ]
        },
        {
            "paperId": "8ed9807f1a2cefc780d6e340a852b1f3530d4259",
            "title": "On incentive-based tagging",
            "abstract": "A social tagging system, such as del.icio.us and Flickr, allows users to annotate resources (e.g., web pages and photos) with text descriptions called tags. Tags have proven to be invaluable information for searching, mining, and recommending resources. In practice, however, not all resources receive the same attention from users. As a result, while some highly-popular resources are over-tagged, most of the resources are under-tagged. Incomplete tagging on resources severely affects the effectiveness of all tag-based techniques and applications. We address an interesting question: if users are paid to tag specific resources, how can we allocate incentives to resources in a crowd-sourcing environment so as to maximize the tagging quality of resources? We address this question by observing that the tagging quality of a resource becomes stable after it has been tagged a sufficient number of times. We formalize the concepts of tagging quality (TQ) and tagging stability (TS) in measuring the quality of a resource's tag description. We propose a theoretically optimal algorithm given a fixed \u201cbudget\u201d (i.e., the amount of money paid for tagging resources). This solution decides the amount of rewards that should be invested on each resource in order to maximize tagging stability. We further propose a few simple, practical, and efficient incentive allocation strategies. On a dataset from del.icio.us, our best strategy provides resources with a close-to-optimal gain in tagging stability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1764231",
                    "name": "Xuan S. Yang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2076891",
                    "name": "Luyi Mo"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                }
            ]
        },
        {
            "paperId": "95489c0e03388ff133fee101867dcb83f7028b46",
            "title": "Fast evaluation of iceberg pattern-based aggregate queries",
            "abstract": "A Sequence OLAP (S-OLAP) system provides a platform on which pattern-based aggregate (PBA) queries on a sequence database are evaluated. In its simplest form, a PBA query consists of a pattern template T and an aggregate function F. A pattern template is a sequence of variables, each is defined over a domain. For example, the template T = (X,Y ,Y ,X) consists of two variables X and Y . Each variable is instantiated with all possible values in its corresponding domain to derive all possible patterns of the template. Sequences are grouped based on the patterns they possess. The answer to a PBA query is a sequence cuboid (s-cuboid), which is a multidimensional array of cells. Each cell is associated with a pattern instantiated from the query's pattern template. The value of each s-cuboid cell is obtained by applying the aggregate function F to the set of data sequences that belong to that cell. Since a pattern template can involve many variables and can be arbitrarily long, the induced s-cuboid for a PBA query can be huge. For most analytical tasks, however, only iceberg cells with very large aggregate values are of interest. This paper proposes an efficient approach to identify and evaluate iceberg cells of s-cuboids. Experimental results show that our algorithms are orders of magnitude faster than existing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2966924",
                    "name": "Zhian He"
                },
                {
                    "authorId": "2807649",
                    "name": "Petrie Wong"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "145373095",
                    "name": "Eric Lo"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "9ba39737b48ae4ac6c0e06c6a68b3def29191030",
            "title": "Earth Mover's Distance based Similarity Search at Scale",
            "abstract": "Earth Mover's Distance (EMD), as a similarity measure, has received a lot of attention in the fields of multimedia and probabilistic databases, computer vision, image retrieval, machine learning, etc. EMD on multidimensional histograms provides better distinguishability between the objects approximated by the histograms (e.g., images), compared to classic measures like Euclidean distance. Despite its usefulness, EMD has a high computational cost; therefore, a number of effective filtering methods have been proposed, to reduce the pairs of histograms for which the exact EMD has to be computed, during similarity search. Still, EMD calculations in the refinement step remain the bottleneck of the whole similarity search process. In this paper, we focus on optimizing the refinement phase of EMD-based similarity search by (i) adapting an efficient min-cost flow algorithm (SIA) for EMD computation, (ii) proposing a dynamic distance bound, which can be used to terminate an EMD refinement early, and (iii) proposing a dynamic refinement order for the candidates which, paired with a concurrent EMD refinement strategy, reduces the amount of needless computations. Our proposed techniques are orthogonal to and can be easily integrated with the state-of-the-art filtering techniques, reducing the cost of EMD-based similarity queries by orders of magnitude.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119309654",
                    "name": "Yu Tang"
                },
                {
                    "authorId": "1830414684",
                    "name": "Leong Hou U"
                },
                {
                    "authorId": "144288507",
                    "name": "Yilun Cai"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "acefba7bbd5a74992f4ff5ad550a954442c37b24",
            "title": "Privacy Preserving Path Recommendation for Moving User on Location Based Service",
            "abstract": "With the increasing adoption of location based services, privacy is becoming a major concern. To hide the identity and location of a request on location based service, most methods consider a set of users in a reasonable region so as to confuse their requests. When there are not enough users, the cloaking region needs expanding to a larger area or the response needs delay. Either way degrades the quality-of-service. In this paper, we tackle the privacy problem in a predication way by recommending a privacy-preserving path for a requester. We consider the popular navigation application, where users may continuously query different location based servers during their movements. Based on a set of metrics on privacy, distance and the quality of services that a LBS requester often desires, a secure path is computed for each request according to user's preference, and can be dynamically adjusted when the situation is changed. A set of experiments are performed to verify our method and the relationship between parameters are discussed in details. We also discuss how to apply our method into practical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47366519",
                    "name": "Yuqing Sun"
                },
                {
                    "authorId": null,
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "221e0930453f929872db626ce80b21df479f81f4",
            "title": "DQR: a probabilistic approach to diversified query recommendation",
            "abstract": "Web search queries issued by casual users are often short and with limited expressiveness. Query recommendation is a popular technique employed by search engines to help users refine their queries. Traditional similarity-based methods, however, often result in redundant and monotonic recommendations. We identify five basic requirements of a query recommendation system. In particular, we focus on the requirements of redundancy-free and diversified recommendations. We propose the DQR framework, which mines a search log to achieve two goals: (1) It clusters search log queries to extract query concepts, based on which recommended queries are selected. (2) It employs a probabilistic model and a greedy heuristic algorithm to achieve recommendation diversification. Through a comprehensive user study we compare DQR against five other recommendation methods. Our experiment shows that DQR outperforms the other methods in terms of relevancy, diversity, and ranking performance of the recommendations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47370334",
                    "name": "Ruirui Li"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "2555622",
                    "name": "Bin Bi"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145373095",
                    "name": "Eric Lo"
                }
            ]
        },
        {
            "paperId": "54f9d7f6a5cf764ebacfba5a652366d2d6d72eb8",
            "title": "Efficient Mining of Frequent Item Sets on Large Uncertain Databases",
            "abstract": "The data handled in emerging applications like location-based services, sensor monitoring systems, and data integration, are often inexact in nature. In this paper, we study the important problem of extracting frequent item sets from a large uncertain database, interpreted under the Possible World Semantics (PWS). This issue is technically challenging, since an uncertain database contains an exponential number of possible worlds. By observing that the mining process can be modeled as a Poisson binomial distribution, we develop an approximate algorithm, which can efficiently and accurately discover frequent item sets in a large uncertain database. We also study the important issue of maintaining the mining result for a database that is evolving (e.g., by inserting a tuple). Specifically, we propose incremental mining algorithms, which enable Probabilistic Frequent Item set (PFI) results to be refreshed. This reduces the need of re-executing the whole mining algorithm on the new database, which is often more expensive and unnecessary. We examine how an existing algorithm that extracts exact item sets, as well as our approximate algorithm, can support incremental mining. All our approaches support both tuple and attribute uncertainty, which are two common uncertain database models. We also perform extensive evaluation on real and synthetic data sets to validate our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693997",
                    "name": "Liang Wang"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "72490665",
                    "name": "Sau-dan. Lee"
                },
                {
                    "authorId": "1764231",
                    "name": "Xuan S. Yang"
                }
            ]
        },
        {
            "paperId": "64e0eb93badc4b65748db6afe014aab9eba257cf",
            "title": "Trajectory Possible Nearest Neighbor Queries over Imprecise Location Data",
            "abstract": "Trajectory queries, which retrieve nearby objects for every point of a given route, can be used to identify alerts of potential threats along a vessel route, or monitor the adjacent rescuers to a travel path. However, the locations of these objects (e.g., threats, succours) may not be precisely obtained due to hardware limitations of measuring devices, as well as complex natures of the surroundings. For such data, we consider a common model, where the possible locations of an object are bounded by a closed region, called \u201cimprecise region\u201d. Ignoring or coarsely wrapping imprecision can render low query qualities, and cause undesirable consequences such as missing alerts of threats and poor response rescue time. Also, the query is quite time-consuming, since all points on the trajectory are considered. In this paper, we study how to efficiently evaluate trajectory queries over imprecise objects, by proposing a novel concept, u-bisector, which is an extension of bisector specified for imprecise data. Based on the u-bisector, we provide an efficient and versatile solution which supports different shapes of commonlyused imprecise regions (e.g., rectangles, circles, and line segments). Extensive experiments on real datasets show that our proposal achieves better efficiency, quality, and scalability than its competitors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "113555568",
                    "name": "Xike Xie Scott"
                },
                {
                    "authorId": "1722082",
                    "name": "Man Lung Yiu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "144106863",
                    "name": "Hua Lu"
                }
            ]
        },
        {
            "paperId": "91d9502c81b4978c17d5c496fd71d98f92da2ad6",
            "title": "Evaluating Probabilistic Queries over Uncertain Matching",
            "abstract": "A matching between two database schemas, generated by machine learning techniques (e.g., COMA++), is often uncertain. Handling the uncertainty of schema matching has recently raised a lot of research interest, because the quality of applications rely on the matching result. We study query evaluation over an inexact schema matching, which is represented as a set of ``possible mappings'', as well as the probabilities that they are correct. Since the number of possible mappings can be large, evaluating queries through these mappings can be expensive. By observing the fact that the possible mappings between two schemas often exhibit a high degree of overlap, we develop two efficient solutions. We also present a fast algorithm to compute answers with the k highest probabilities. An extensive evaluation on real schemas shows that our approaches improve the query performance by almost an order of magnitude.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2112953927",
                    "name": "Jian Gong"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                },
                {
                    "authorId": "40374706",
                    "name": "Jiefeng Cheng"
                }
            ]
        },
        {
            "paperId": "955fe759e2ecf8f6ba533e90c7020a57af571dd6",
            "title": "On querying historical evolving graph sequences",
            "abstract": "In many applications, information is best represented as graphs. In a dynamic world, information changes and so the graphs representing the information evolve with time. We propose that historical graph-structured data be maintained for analytical processing. We call a historical evolving graph sequence an EGS. We observe that in many applications, graphs of an EGS are large and numerous, and they often exhibit much redundancy among them. We study the problem of efficient query processing on an EGS and put forward a solution framework called FVF. Through extensive experiments on both real and synthetic datasets, we show that our FVF framework is highly efficient in EGS query processing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40641823",
                    "name": "Chenghui Ren"
                },
                {
                    "authorId": "145373095",
                    "name": "Eric Lo"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "2116314146",
                    "name": "Xinjie Zhu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "a2e64839e3ba8207c255997f9a0130b902f0778f",
            "title": "On link-based similarity join",
            "abstract": "Graphs can be found in applications like social networks, bibliographic networks, and biological databases. Understanding the relationship, or links, among graph nodes enables applications such as link prediction, recommendation, and spam detection. In this paper, we propose link-based similarity join (LS-join), which extends the similarity join operator to link-based measures. Given two sets of nodes in a graph, the LS-join returns all pairs of nodes that are highly similar to each other, with respect to an e-function. The e-function generalizes common measures like Personalized PageRank (PPR) and SimRank (SR). We study an efficient LS-join algorithm on a large graph. We further improve our solutions for PPR and SR, which involve expensive random-walk operations. We validate our solutions by performing extensive experiments on three real graph datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2088408",
                    "name": "Liwen Sun"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2144440567",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "d97a90fc859088eb34153983df51e17526f58f04",
            "title": "Processing Continuous Range Queries with Spatiotemporal Tolerance",
            "abstract": "Continuous queries are often employed to monitor the locations of mobile objects (MOs), which are determined by sensing devices like GPS receivers. In this paper, we tackle two challenges in processing continuous range queries (CRQs): coping with data uncertainty inherently associated with location data, and reducing the energy consumption of battery-powered MOs. We propose the concept of spatiotemporal tolerance for CRQ to relax a query's accuracy requirements in terms of a maximal acceptable error. Unlike previous works, our definition considers tolerance in both the spatial and temporal dimensions, which offers applications more flexibility in specifying their individual accuracy requirements. As we will show, these tolerance bounds can provide well-defined query semantics in spite of different sources of data uncertainty. In addition, we present efficient algorithms that carefully control when an MO should sense or report a location, while satisfying these tolerances. Thereby, we particularly reduce the number of position sensing operations substantially, which constitute a considerable source of energy consumption. Extensive simulations confirm that the proposed algorithms result in large energy savings compared to nontolerant query processing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145242211",
                    "name": "Tobias Farrell"
                },
                {
                    "authorId": "1700118",
                    "name": "K. Rothermel"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "ebfe6042baa1c50d1342dc2b8904491f384fbc88",
            "title": "CubeLSI: An effective and efficient method for searching resources in social tagging systems",
            "abstract": "In a social tagging system, resources (such as photos, video and web pages) are associated with tags. These tags allow the resources to be effectively searched through tag-based keyword matching using traditional IR techniques. We note that in many such systems, tags of a resource are often assigned by a diverse audience of causal users (taggers). This leads to two issues that gravely affect the effectiveness of resource retrieval: (1) Noise: tags are picked from an uncontrolled vocabulary and are assigned by untrained taggers. The tags are thus noisy features in resource retrieval. (2) A multitude of aspects: different taggers focus on different aspects of a resource. Representing a resource using a flattened bag of tags ignores this important diversity of taggers. To improve the effectiveness of resource retrieval in social tagging systems, we propose CubeLSI \u2014 a technique that extends traditional LSI to include taggers as another dimension of feature space of resources. We compare CubeLSI against a number of other tag-based retrieval models and show that CubeLSI significantly outperforms the other models in terms of retrieval accuracy. We also prove two interesting theorems that allow CubeLSI to be very efficiently computed despite the much enlarged feature space it employs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2555622",
                    "name": "Bin Bi"
                },
                {
                    "authorId": "72490665",
                    "name": "Sau-dan. Lee"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "ec7cca4c428b2ce2baf138d19939ddb305d3e0af",
            "title": "I/O-efficient algorithms for answering pattern-based aggregate queries in a sequence OLAP system",
            "abstract": "Many kinds of real-life data exhibit logical ordering among their data items and are thus sequential in nature. In recent years, the concept of Sequence OLAP (S-OLAP) has been proposed. The biggest distinguishing feature of SOLAP from traditional OLAP is that data sequences managed by an S-OLAP system are characterized by the subsequence/substring patterns they possess. An S-OLAP system thus supports pattern-based grouping and aggregation. Conceptually, an S-OLAP system maintains a sequence data cube which is composed of sequence cuboids. Each sequence cuboid presents the answer of a pattern-based aggregate (PBA) query. This paper focuses on the I/O aspects of evaluating PBA queries. We study the problems of joining plan selection and execution planning, which are the core issues in the design of I/O-efficient cuboid materialization algorithms. Through an empirical study, we show that our algorithms lead to a very I/O-efficient strategy for sequence cuboid materialization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24849138",
                    "name": "C. Chui"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "145373095",
                    "name": "Eric Lo"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "058c3e80ac3607d6dec1cacb846bf81c4cc88d0c",
            "title": "Explore or exploit?",
            "abstract": "Data ambiguity is inherent in applications such as data integration, location-based services, and sensor monitoring. In many situations, it is possible to \"clean\", or remove, ambiguities from these databases. For example, the GPS location of a user is inexact due to measurement errors, but context information (e.g., what a user is doing) can be used to reduce the imprecision of the location value. In order to obtain a database with a higher quality, we study how to disambiguate a database by appropriately selecting candidates to clean. This problem is challenging because cleaning involves a cost, is limited by a budget, may fail, and may not remove all ambiguities. Moreover, the statistical information about how likely database objects can be cleaned may not be precisely known. We tackle these challenges by proposing two types of algorithms. The first type makes use of greedy heuristics to make sensible decisions; however, these algorithms do not make use of cleaning information and require user input for parameters to achieve high cleaning effectiveness. We propose the Explore-Exploit (or EE) algorithm, which gathers valuable information during the cleaning process to determine how the remaining cleaning budget should be invested. We also study how to fine-tune the parameters of EE in order to achieve optimal cleaning effectiveness. Experimental evaluations on real and synthetic datasets validate the effectiveness and efficiency of our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145373095",
                    "name": "Eric Lo"
                },
                {
                    "authorId": "1764231",
                    "name": "Xuan S. Yang"
                },
                {
                    "authorId": "36530097",
                    "name": "Ming-Hay Luk"
                },
                {
                    "authorId": "2144440567",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "1678927",
                    "name": "Xike Xie"
                }
            ]
        },
        {
            "paperId": "2b48c4412e5abd240d971e6f2a5f9b0a4f34feb5",
            "title": "Filtering Data Streams for Entity-Based Continuous Queries",
            "abstract": "The idea of allowing query users to relax their correctness requirements in order to improve performance of a data stream management system (e.g., location-based services and sensor networks) has been recently studied. By exploiting the maximum error (or tolerance) allowed in query answers, algorithms for reducing the use of system resources have been developed. In most of these works, however, query tolerance is expressed as a numerical value, which may be difficult to specify. We observe that in many situations, users may not be concerned with the actual value of an answer, but rather which object satisfies a query (e.g., \"who is my nearest neighbor?\u00bf). In particular, an entity-based query returns only the names of objects that satisfy the query. For these queries, it is possible to specify a tolerance that is \"nonvalue-based.\u00bf In this paper, we study fraction-based tolerance, a type of nonvalue-based tolerance, where a user specifies the maximum fractions of a query answer that can be false positives and false negatives. We develop fraction-based tolerance for two major classes of entity-based queries: 1) nonrank-based query (e.g., range queries) and 2) rank-based query (e.g., k-nearest-neighbor queries). These definitions provide users with an alternative to specify the maximum tolerance allowed in their answers. We further investigate how these definitions can be exploited in a distributed stream environment. We design adaptive filter algorithms that allow updates be dropped conditionally at the data stream sources without affecting the overall query correctness. Extensive experimental results show that our protocols reduce the use of network and energy resources significantly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "2064026029",
                    "name": "Alan Kwan"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                },
                {
                    "authorId": "2812268",
                    "name": "Yi-Cheng Tu"
                }
            ]
        },
        {
            "paperId": "3f5c6364e35a77bc5343c2d9ebc020ecfebd8c6d",
            "title": "Accelerating probabilistic frequent itemset mining: a model-based approach",
            "abstract": "Data uncertainty is inherent in emerging applications such as location-based services, sensor monitoring systems, and data integration. To handle a large amount of imprecise information, uncertain databases have been recently developed. In this paper, we study how to efficiently discover frequent itemsets from large uncertain databases, interpreted under the Possible World Semantics. This is technically challenging, since an uncertain database induces an exponential number of possible worlds. To tackle this problem, we propose a novel method to capture the itemset mining process as a Poisson binomial distribution. This model-based approach extracts frequent itemsets with a high degree of accuracy, and supports large databases. We apply our techniques to improve the performance of the algorithms for: (1) finding itemsets whose frequentness probabilities are larger than some threshold; and (2) mining itemsets with the k highest frequentness probabilities. Our approaches support both tuple and attribute uncertainty models, which are commonly used to represent uncertain databases. Extensive evaluation on real and synthetic datasets shows that our methods are highly accurate. Moreover, they are orders of magnitudes faster than previous approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693997",
                    "name": "Liang Wang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "72490665",
                    "name": "Sau-dan. Lee"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                }
            ]
        },
        {
            "paperId": "4001c8e68875db6ae54b2361076052464bd1c961",
            "title": "Mining uncertain data with probabilistic guarantees",
            "abstract": "Data uncertainty is inherent in applications such as sensor monitoring systems, location-based services, and biological databases. To manage this vast amount of imprecise information, probabilistic databases have been recently developed. In this paper, we study the discovery of frequent patterns and association rules from probabilistic data under the Possible World Semantics. This is technically challenging, since a probabilistic database can have an exponential number of possible worlds. We propose two effcient algorithms, which discover frequent patterns in bottom-up and top-down manners. Both algorithms can be easily extended to discover maximal frequent patterns. We also explain how to use these patterns to generate association rules. Extensive experiments, using real and synthetic datasets, were conducted to validate the performance of our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2088408",
                    "name": "Liwen Sun"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                },
                {
                    "authorId": "40374706",
                    "name": "Jiefeng Cheng"
                }
            ]
        },
        {
            "paperId": "79f4e1add6eff7e03363cf532000fc6b77a690cd",
            "title": "Guest Editors' Introduction: Special Section on Mining Large Uncertain and Probabilistic Databases",
            "abstract": "The four papers in this special section were selected from 23 submissions and represent recent advances in the mining of uncertain databases. The works present new techniques for mining patterns, clustering, and ranking on uncertain data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145884195",
                    "name": "M. Chau"
                },
                {
                    "authorId": "1738210",
                    "name": "Minos N. Garofalakis"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                }
            ]
        },
        {
            "paperId": "81aa239cb8f106a379f7571ac7b1498fb58ec712",
            "title": "Similarity search and mining in uncertain databases",
            "abstract": "Managing, searching and mining uncertain data has achieved much attention in the database community recently due to new sensor technologies and new ways of collecting data. There is a number of challenges in terms of collecting, modelling, representing, querying, indexing and mining uncertain data. In its scope, the diversity of approaches addressing these topics is very high because the underlying assumptions of uncertainty are different across different papers. This tutorial provides a comprehensive and comparative overview of general techniques for the key topics in the fields of querying, indexing and mining uncertain data. In particular, it identifies the most generic types of probabilistic similarity queries and discusses general algorithmic methods to answer such queries efficiently. In addition, the tutorial sketches probabilistic methods for important data mining applications in the context of uncertain data with special emphasis on probabilistic clustering and probabilistic pattern mining. The intended audience of this tutorial ranges from novice researchers to advanced experts as well as practitioners from any application domain dealing with uncertain data retrieval and mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723035",
                    "name": "M. Renz"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1688561",
                    "name": "H. Kriegel"
                },
                {
                    "authorId": "3221975",
                    "name": "Andreas Z\u00fcfle"
                },
                {
                    "authorId": "3049466",
                    "name": "T. Bernecker"
                }
            ]
        },
        {
            "paperId": "bc1aa9a5608efc1d4811a23cb55743bc7c4c4655",
            "title": "UV-diagram: A Voronoi diagram for uncertain data",
            "abstract": "The Voronoi diagram is an important technique for answering nearest-neighbor queries for spatial databases. In this paper, we study how the Voronoi diagram can be used on uncertain data, which are inherent in scientific and business applications. In particular, we propose the Uncertain-Voronoi Diagram (or UV-diagram in short). Conceptually, the data space is divided into distinct \u201cUV-partitions\u201d, where each UV-partition P is associated with a set S of objects; any point q located in P has the set S as its nearest neighbor with non-zero probabilities. The UV-diagram facilitates queries that inquire objects for having non-zero chances of being the nearest neighbor of a given query point. It also allows analysis of nearest neighbor information, e.g., finding out how many objects are the nearest neighbors in a given area. However, a UV-diagram requires exponential construction and storage costs. To tackle these problems, we devise an alternative representation for UV-partitions, and develop an adaptive index for the UV-diagram. This index can be constructed in polynomial time. We examine how it can be extended to support other related queries. We also perform extensive experiments to validate the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1678927",
                    "name": "Xike Xie"
                },
                {
                    "authorId": "1722082",
                    "name": "Man Lung Yiu"
                },
                {
                    "authorId": "2108304551",
                    "name": "Jinchuan Chen"
                },
                {
                    "authorId": "2088408",
                    "name": "Liwen Sun"
                }
            ]
        },
        {
            "paperId": "bd4fa22b80864fc94ca103d37a1f744d207cdd32",
            "title": "Managing uncertainty of XML schema matching",
            "abstract": "Despite of advances in machine learning technologies, a schema matching result between two database schemas (e.g., those derived from COMA++) is likely to be imprecise. In particular, numerous instances of \u201cpossible mappings\u201d between the schemas may be derived from the matching result. In this paper, we study the problem of managing possible mappings between two heterogeneous XML schemas. We observe that for XML schemas, their possible mappings have a high degree of overlap. We hence propose a novel data structure, called the block tree, to capture the commonalities among possible mappings. The block tree is useful for representing the possible mappings in a compact manner, and can be generated efficiently. Moreover, it supports the evaluation of probabilistic twig query (PTQ), which returns the probability of portions of an XML document that match the query pattern. For users who are interested only in answers with k-highest probabilities, we also propose the top-k PTQ, and present an efficient solution for it. The second challenge we have tackled is to efficiently generate possible mappings for a given schema matching. While this problem can be solved by existing algorithms, we show how to improve the performance of the solution by using a divide-and-conquer approach. An extensive evaluation on realistic datasets show that our approaches significantly improve the efficiency of generating, storing, and querying possible mappings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2112953927",
                    "name": "Jian Gong"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                }
            ]
        },
        {
            "paperId": "3260d0442c2bbb2e9bf87667dedb753b61295bf6",
            "title": "Minimizing the communication cost for continuous skyline maintenance",
            "abstract": "Existing work in the skyline literature focuses on optimizing the processing cost. This paper aims at minimization of the communication overhead in client-server architectures, where a server continuously maintains the skyline of dynamic objects. Our first contribution is a Filter method that avoids transmission of updates from objects that cannot influence the skyline. Specifically, each object is assigned a filter so that it needs to issue an update only if it violates its filter. Filter achieves significant savings over the naive approach of transmitting all updates. Going one step further, we introduce the concept of frequent skyline query over a sliding window(FSQW). The motivation is that snapshot skylines are not very useful in streaming environments because they keep changing over time. Instead, FSQW reports the objects that appear in the skylines of at least \u03b8 \u22c5 s of the s most recent timestamps (0 < \u03b8 \u2264 1). Filter can be easily adapted to FSQW processing, however, with potentially high overhead for large and frequently updated datasets. To further reduce the communication cost, we propose a Sampling method, which returns approximate FSQW results without computing each snapshot skyline. Finally, we integrate Filter and Sampling in a Hybrid approach that combines their individual advantages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109512262",
                    "name": "Zhenjie Zhang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1746338",
                    "name": "Dimitris Papadias"
                },
                {
                    "authorId": "1699730",
                    "name": "A. Tung"
                }
            ]
        },
        {
            "paperId": "4e6a0f43958fa0ae7225b66d0e8ff85ce8c4f4a6",
            "title": "Location Privacy in Moving-Object Environments",
            "abstract": "The expanding use of location-based services has profound implications on the privacy of personal information. If no adequate protection is adopted, information about movements of specific individuals could be disclosed to unauthorized subjects or organizations, thus resulting in privacy breaches. In this paper, we propose a framework for preserving location privacy in moving-object environments. Our approach is based on the idea of sending to the service provider suitably modified location information. Such modifications, that include transformations like scaling, are performed by agents interposed between users and service providers. Agents execute data transformation and the service provider directly processes the transformed dataset. Our technique not only prevents the service provider from knowing the exact locations of users, but also protects information about user movements and locations from being disclosed to other users who are not authorized to access this information. A key characteristic of our approach is that it achieves privacy without degrading service quality. We also define a privacy model to analyze our framework, and examine our approach experimentally.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145732061",
                    "name": "D. Lin"
                },
                {
                    "authorId": "1743774",
                    "name": "E. Bertino"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                }
            ]
        },
        {
            "paperId": "700ecfc211e56a993ab368bb8ec5c539d7c22e2b",
            "title": "Naive Bayes Classification of Uncertain Data",
            "abstract": "Traditional machine learning algorithms assume that data are exact or precise. However, this assumption may not hold in some situations because of data uncertainty arising from measurement errors, data staleness, and repeated measurements, etc. With uncertainty, the value of each data item is represented by a probability distribution function (pdf). In this paper, we propose a novel naive Bayes classification algorithm for uncertain data with a pdf. Our key solution is to extend the class conditional probability estimation in the Bayes model to handle pdf\u2019s. Extensive experiments on UCI datasets show that the accuracy of naive Bayes model can be improved by taking into account the uncertainty information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46606181",
                    "name": "Jiangtao Ren"
                },
                {
                    "authorId": "72490665",
                    "name": "Sau-dan. Lee"
                },
                {
                    "authorId": "2259934",
                    "name": "Xianlu Chen"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1723793",
                    "name": "D. Cheung"
                }
            ]
        },
        {
            "paperId": "e2d5c87fe259c3fce3d872159f7f5a7e77775dbd",
            "title": "Evaluating probability threshold k-nearest-neighbor queries over uncertain data",
            "abstract": "In emerging applications such as location-based services, sensor monitoring and biological management systems, the values of the database items are naturally imprecise. For these uncertain databases, an important query is the Probabilistic k-Nearest-Neighbor Query (k-PNN), which computes the probabilities of sets of k objects for being the closest to a given query point. The evaluation of this query can be both computationally- and I/O-expensive, since there is an exponentially large number of k object-sets, and numerical integration is required. Often a user may not be concerned about the exact probability values. For example, he may only need answers that have sufficiently high confidence. We thus propose the Probabilistic Threshold k-Nearest-Neighbor Query (T-k-PNN), which returns sets of k objects that satisfy the query with probabilities higher than some threshold T. Three steps are proposed to handle this query efficiently. In the first stage, objects that cannot constitute an answer are filtered with the aid of a spatial index. The second step, called probabilistic candidate selection, significantly prunes a number of candidate sets to be examined. The remaining sets are sent for verification, which derives the lower and upper bounds of answer probabilities, so that a candidate set can be quickly decided on whether it should be included in the answer. We also examine spatially-efficient data structures that support these methods. Our solution can be applied to uncertain data with arbitrary probability density functions. We have also performed extensive experiments to examine the effectiveness of our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2108304551",
                    "name": "Jinchuan Chen"
                },
                {
                    "authorId": "1678927",
                    "name": "Xike Xie"
                }
            ]
        },
        {
            "paperId": "eb055c353a195852872fc6714cbb56750bf9f4ba",
            "title": "Indexing uncertain data",
            "abstract": "Querying uncertain data has emerged as an important problem in data management due to the imprecise nature of many measurement data. In this paper we study answering range queries over uncertain data. Specifically, we are given a collection P of n points in R, each represented by its one-dimensional probability density function (pdf). The goal is to build an index on P such that given a query interval I and a probability threshold \u03c4, we can quickly report all points of P that lie in I with probability at least \u03c4. We present various indexing schemes with linear or near-linear space and logarithmic query time. Our schemes support pdf's that are either histograms or more complex ones such as Gaussian or piecewise algebraic. They also extend to the external memory model in which the goal is to minimize the number of disk accesses when querying the index.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "047fe177f48294447ee2f0628b88cce8af2d9468",
            "title": "Position transformation: a location privacy protection method for moving objects",
            "abstract": "The expanding use of location-based services has profound implications on the privacy of personal information. In this paper, we propose a framework for preserving location privacy based on the idea of sending to the service provider suitably modified location information. Agents execute data transformation and the service provider directly processes the transformed dataset. Our technique not only prevents the service provider from knowing the exact locations of users, but also protects information about user movements and locations from being disclosed to other users who are not authorized to access this information. We also define a privacy model to analyze our framework, and examine our approach experimentally.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145732061",
                    "name": "D. Lin"
                },
                {
                    "authorId": "1743774",
                    "name": "E. Bertino"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                }
            ]
        },
        {
            "paperId": "10c5fe5269bed9a7ca2701cea1f943d2458f0d46",
            "title": "Cleaning uncertain data with quality guarantees",
            "abstract": "Uncertain or imprecise data are pervasive in applications like location-based services, sensor monitoring, and data collection and integration. For these applications, probabilistic databases can be used to store uncertain data, and querying facilities are provided to yield answers with statistical confidence. Given that a limited amount of resources is available to \"clean\" the database (e.g., by probing some sensor data values to get their latest values), we address the problem of choosing the set of uncertain objects to be cleaned, in order to achieve the best improvement in the quality of query answers. For this purpose, we present the PWS-quality metric, which is a universal measure that quantifies the ambiguity of query answers under the possible world semantics. We study how PWS-quality can be efficiently evaluated for two major query classes: (1) queries that examine the satisfiability of tuples independent of other tuples (e.g., range queries); and (2) queries that require the knowledge of the relative ranking of the tuples (e.g., MAX queries). We then propose a polynomial-time solution to achieve an optimal improvement in PWS-quality. Other fast heuristics are presented as well. Experiments, performed on both real and synthetic datasets, show that the PWS-quality metric can be evaluated quickly, and that our cleaning algorithm provides an optimal solution with high efficiency. To our best knowledge, this is the first work that develops a quality metric for a probabilistic database, and investigates how such a metric can be used for data cleaning purposes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2108304551",
                    "name": "Jinchuan Chen"
                },
                {
                    "authorId": "1678927",
                    "name": "Xike Xie"
                }
            ]
        },
        {
            "paperId": "618b21a1abf742629567ebb36b3b8b283e37d0b6",
            "title": "Acceleration of a procedure to generate fractal curves of a given dimension through the probabilistic analysis of execution time",
            "abstract": "In a previous work [Ortega et al. 03], the authors have described the use of grammatical evolution to automatically generate L Systems (LS) representing fractal curves with a pre-determined fractal dimension. The experiments presented in this paper prove that the efficiency of this procedure is variable, with very different execution times for different executions with the same fractal dimension target. The paper shows that the probabilistic distribution of execution times belongs to a well-known family of random variables: heavytailed distributions. This analysis explains the erratic performance of the algorithm and suggests the use of a technique that corrects this variability and improves the efficiency about one order of magnitude. Acknowledgements: This paper has been sponsored by the Spanish Ministry of Science and Technology, project numbers TIC2001-0685-C02-01 and TIC 200201948. INTRODUCTION Our procedure to generate fractal curves with a required dimension consists of three parts: a) representation of fractals by means of L Systems (LS); b) computation of the fractal dimension from the grammar; c) application of a grammar-evolution based genetic algorithm to get a grammar representing a fractal with the required dimension. LS provide a powerful tool to represent fractals in the class of recursive transformations. The iterator may be represented by means of production rules, while the initiator corresponds to the axiom. The fractal curve is generated by the sequence of words derived from the axiom, by means of a representation scheme: vector graphics or turtle graphics. In a previous work [Alfonseca and Ortega 01] we have described an algorithm that estimates the fractal dimension of a non-trivial set of these fractals from their equivalent LS by means of symbolic manipulation, without the need of graphical procedures. In [Ortega et al. 03] we applied Grammar Evolution (GE) [O\u2019Neill and Ryan 2001] to obtain the LS equivalent to a fractal with the required dimension. The proposed procedure has a clear biological inspiration acting on three different levels: a genotype (a vector of integers) an intermediate level, equivalent to proteins (LS) and a phenotype (the fractal curve). See Figure 1.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112893280",
                    "name": "P. Gray"
                },
                {
                    "authorId": "1796237",
                    "name": "Todd Eavis"
                },
                {
                    "authorId": "1733199",
                    "name": "A. Inselberg"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1799376",
                    "name": "G. Graefe"
                },
                {
                    "authorId": "2061778718",
                    "name": "Hansj\u00f6rg Zeller"
                },
                {
                    "authorId": "1685125",
                    "name": "Esther Pacitti"
                },
                {
                    "authorId": "145624228",
                    "name": "Christoph E. Koch"
                },
                {
                    "authorId": "143758471",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "1733268",
                    "name": "B. Kemme"
                },
                {
                    "authorId": "2115529060",
                    "name": "Yuqing Wu"
                },
                {
                    "authorId": "2117996586",
                    "name": "Hong Cheng"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1393644275",
                    "name": "P. Cudr\u00e9-Mauroux"
                },
                {
                    "authorId": "1740364",
                    "name": "Wojciech Galuba"
                },
                {
                    "authorId": "69947993",
                    "name": "Sarunas Girdzijauskas"
                },
                {
                    "authorId": "1743906",
                    "name": "P. Felber"
                },
                {
                    "authorId": "9169892",
                    "name": "E. Biersack"
                },
                {
                    "authorId": "1687892",
                    "name": "Anastasios Kementsietsidis"
                },
                {
                    "authorId": "1732298",
                    "name": "P. Triantafillou"
                },
                {
                    "authorId": "1757596",
                    "name": "Ioannis Aekaterinidis"
                },
                {
                    "authorId": "145688317",
                    "name": "Anwitaman Datta"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                },
                {
                    "authorId": "1714278",
                    "name": "Alexander Thomasian"
                },
                {
                    "authorId": "48843863",
                    "name": "P. Bonnet"
                },
                {
                    "authorId": "1695878",
                    "name": "D. Shasha"
                },
                {
                    "authorId": "1809784",
                    "name": "N. Lorentzos"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "35119829",
                    "name": "Ruihua Song"
                },
                {
                    "authorId": "145898058",
                    "name": "Wil M.P. van der Aalst"
                },
                {
                    "authorId": "1710939",
                    "name": "C. Dyreson"
                },
                {
                    "authorId": "2958795",
                    "name": "S. Lightstone"
                },
                {
                    "authorId": "2108568943",
                    "name": "Ryan Johnson"
                },
                {
                    "authorId": "1781993",
                    "name": "E. Pitoura"
                },
                {
                    "authorId": "1783767",
                    "name": "K. Wada"
                },
                {
                    "authorId": "1725663",
                    "name": "David Toman"
                },
                {
                    "authorId": "1702232",
                    "name": "George Karabatis"
                },
                {
                    "authorId": "1896793",
                    "name": "Cristina Sirangelo"
                },
                {
                    "authorId": "1788571",
                    "name": "G. Grahne"
                },
                {
                    "authorId": "1393591007",
                    "name": "J. Domingo-Ferrer"
                },
                {
                    "authorId": "3270238",
                    "name": "Ethan Zhang"
                },
                {
                    "authorId": "11859917",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "1750995",
                    "name": "Ben Carterette"
                },
                {
                    "authorId": "1703980",
                    "name": "Nick Craswell"
                },
                {
                    "authorId": "1753628",
                    "name": "J. Kamps"
                },
                {
                    "authorId": "1796253",
                    "name": "Y. Manolopoulos"
                },
                {
                    "authorId": "1714996",
                    "name": "Y. Theodoridis"
                },
                {
                    "authorId": "1761528",
                    "name": "V. Tsotras"
                },
                {
                    "authorId": "152555512",
                    "name": "H. Shen"
                },
                {
                    "authorId": "1707003",
                    "name": "P. Hung"
                },
                {
                    "authorId": "1758225",
                    "name": "Vivying S. Y. Cheng"
                },
                {
                    "authorId": "2935513",
                    "name": "Chris Clifton"
                },
                {
                    "authorId": "153697517",
                    "name": "Y. Zheng"
                },
                {
                    "authorId": "48922886",
                    "name": "S. Chow"
                },
                {
                    "authorId": "1398881851",
                    "name": "S. Fischer-H\u00fcbner"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "1785829",
                    "name": "T. Roelleke"
                },
                {
                    "authorId": "2152808927",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2072942186",
                    "name": "S. Robertson"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2108304551",
                    "name": "Jinchuan Chen"
                },
                {
                    "authorId": "1728462",
                    "name": "V. S. Subrahmanian"
                },
                {
                    "authorId": "48183556",
                    "name": "Ben He"
                },
                {
                    "authorId": "1691929",
                    "name": "D. Hiemstra"
                },
                {
                    "authorId": "1739132",
                    "name": "N. Palmer"
                },
                {
                    "authorId": "1739500",
                    "name": "D. Ardagna"
                },
                {
                    "authorId": "2549968",
                    "name": "Pat Helland"
                },
                {
                    "authorId": "143619288",
                    "name": "G. Ram\u00edrez"
                },
                {
                    "authorId": "145980720",
                    "name": "A. Trotman"
                },
                {
                    "authorId": "1687211",
                    "name": "P. Boncz"
                },
                {
                    "authorId": "1402263442",
                    "name": "K. Pinel-Sauvagnat"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                },
                {
                    "authorId": "1399712184",
                    "name": "Sarah Cohen-Boulakia"
                },
                {
                    "authorId": "145641161",
                    "name": "H. Jacobsen"
                },
                {
                    "authorId": "1891854",
                    "name": "Y. Diao"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "145249669",
                    "name": "D. Maier"
                },
                {
                    "authorId": "33811160",
                    "name": "Peter A. Tucker"
                }
            ]
        },
        {
            "paperId": "73edb362fb7256b4e6b8775f6e4d1f726d5e81a0",
            "title": "Probabilistic Verifiers: Evaluating Constrained Nearest-Neighbor Queries over Uncertain Data",
            "abstract": "In applications like location-based services, sensor monitoring and biological databases, the values of the database items are inherently uncertain in nature. An important query for uncertain objects is the probabilistic nearest-neighbor query (PNN), which computes the probability of each object for being the nearest neighbor of a query point. Evaluating this query is computationally expensive, since it needs to consider the relationship among uncertain objects, and requires the use of numerical integration or Monte-Carlo methods. Sometimes, a query user may not be concerned about the exact probability values. For example, he may only need answers that have sufficiently high confidence. We thus propose the constrained nearest-neighbor query (C-PNN), which returns the IDs of objects whose probabilities are higher than some threshold, with a given error bound in the answers. The C-PNN can be answered efficiently with probabilistic verifiers. These are methods that derive the lower and upper bounds of answer probabilities, so that an object can be quickly decided on whether it should be included in the answer. We have developed three probabilistic verifiers, which can be used on uncertain data with arbitrary probability density functions. Extensive experiments were performed to examine the effectiveness of these approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2108304551",
                    "name": "Jinchuan Chen"
                },
                {
                    "authorId": "1756679",
                    "name": "M. Mokbel"
                },
                {
                    "authorId": "40043945",
                    "name": "Chi-Yin Chow"
                }
            ]
        },
        {
            "paperId": "e035b1726fefa129ea0211f7dfacaab39f31c7cb",
            "title": "Database Support for Probabilistic Attributes and Tuples",
            "abstract": "The inherent uncertainty of data present in numerous applications such as sensor databases, text annotations, and information retrieval motivate the need to handle imprecise data at the database level. Uncertainty can be at the attribute or tuple level and is present in both continuous and discrete data domains. This paper presents a model for handling arbitrary probabilistic uncertain data (both discrete and continuous) natively at the database level. Our approach leads to a natural and efficient representation for probabilistic data. We develop a model that is consistent with possible worlds semantics and closed under basic relational operators. This is the first model that accurately and efficiently handles both continuous and discrete uncertainty. The model is implemented in a real database system (PostgreSQL) and the effectiveness and efficiency of our approach is validated experimentally.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49613919",
                    "name": "Sarvjeet Singh"
                },
                {
                    "authorId": "2063024647",
                    "name": "Chris Mayfield"
                },
                {
                    "authorId": "144347727",
                    "name": "Rahul Shah"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                },
                {
                    "authorId": "1708512",
                    "name": "Susanne E. Hambrusch"
                },
                {
                    "authorId": "144050371",
                    "name": "Jennifer Neville"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "17184fb8afd9733bb21a56ce26dbabc82caf9538",
            "title": "Range search on multidimensional uncertain data",
            "abstract": "In an uncertain database, every object <i>o</i> is associated with a probability density function, which describes the likelihood that <i>o</i> appears at each position in a multidimensional workspace. This article studies two types of range retrieval fundamental to many analytical tasks. Specifically, a nonfuzzy query returns all the objects that appear in a search region <i>r</i><sub><i>q</i></sub> with at least a certain probability <i>t</i><sub><i>q</i></sub>. On the other hand, given an uncertain object <i>q</i>, fuzzy search retrieves the set of objects that are within distance \u03f5<sub><i>q</i></sub> from <i>q</i> with no less than probability <i>t</i><sub><i>q</i></sub>. The core of our methodology is a novel concept of \u201cprobabilistically constrained rectangle\u201d, which permits effective pruning/validation of nonqualifying/qualifying data. We develop a new index structure called the U-tree for minimizing the query overhead. Our algorithmic findings are accompanied with a thorough theoretical analysis, which reveals valuable insight into the problem characteristics, and mathematically confirms the efficiency of our solutions. We verify the effectiveness of the proposed techniques with extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144779196",
                    "name": "Yufei Tao"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "418365de515a14258bb14a761a1a016fc311e9f8",
            "title": "Efficient Evaluation of Imprecise Location-Dependent Queries",
            "abstract": "In location-based services, it is common for a user to issue a query based on his/her current position. One such example is \"find the available cabs within two miles of my current location\". Very often, the query issuers' locations are imprecise due to measurement error, sampling error, or message delay. They may also want to protect their privacy by providing a less precise location. In this paper, we study the efficiency of queries that return probabilistic guarantees for location data with uncertainty. We classify this query into two types, based on whether the data (1) has no uncertainty (e.g., shops and restaurants), or (2) has a controlled degree of uncertainty (e.g., moving vehicles). Based on this classification, we develop three methods to improve the computational and I/O performance. The first method expands the query range based on the query issuer's uncertainty. The second idea exchanges the roles of query and data. The third technique exploits the fact that users may only be interested in answers with probabilities higher than some threshold. Experimental simulation over a realistic dataset reveals that our approaches improve the query performance significantly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108304551",
                    "name": "Jinchuan Chen"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "5dd38c33fb49e94ffc8e3995bc42792123252179",
            "title": "Energy-Efficient Monitoring of Mobile Objects with Uncertainty-Aware Tolerances",
            "abstract": "In location-based services, continuous queries are often employed to monitor the locations of mobile objects that are determined by sensing devices like GPS receivers. Due to limited battery resources, it is important for these objects to acquire and report location data only if necessary. We study how these energy- consuming operations can be reduced with a controlled impact on query accuracy of continuous range queries (CRQs). Specifically, we develop uncertainty- aware tolerances, which are user-defined error bounds that provide correctness guarantees, with consideration of different sources of data uncertainty: sensing uncertainty, sampling uncertainty, and communication delay. Novel algorithms are developed to control carefully when an object should acquire and update a location, while satisfying these tolerances. Extensive simulations validate the effectiveness of our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145242211",
                    "name": "Tobias Farrell"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1700118",
                    "name": "K. Rothermel"
                }
            ]
        },
        {
            "paperId": "922b8c93bd488b90feaa38d3b00c29143f89c251",
            "title": "Reducing UK-Means to K-Means",
            "abstract": "This paper proposes an optimisation to the UK-means algorithm, which generalises the k-means algorithm to han- dle objects whose locations are uncertain. The location of each object is described by a probability density function (pdf). The UK-means algorithm needs to compute expected distances (EDs) between each object and the cluster repre- sentatives. The evaluation of ED from first principles is very costly operation, because the pdf 's are different and arbi- trary. But UK-means needs to evaluate a lot of EDs. This is a major performance burden of the algorithm. In this pa- per, we derive a formula for evaluating EDs efficiently. This tremendously reduces the execution time of UK-means, as demonstrated by our preliminary experiments. We also il- lustrate that this optimised formula effectively reduces the UK-means problem to the traditional clustering algorithm addressed by the k-means algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72490665",
                    "name": "Sau-dan. Lee"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                }
            ]
        },
        {
            "paperId": "335498604c0c92ba54bfb6d6fa2b1e498c6f3208",
            "title": "Efficient join processing over uncertain data",
            "abstract": "In many applications data values are inherently uncertain. This includes moving-objects, sensors and biological databases. There has been recent interest in the development of database management systems that can handle uncertain data. Some proposals for such systems include attribute values that are uncertain. In particular, an attribute value can be modeled as a range of possible values, associated with a probability density function. Previous efforts for this type of data have only addressed simple queries such as range and nearest-neighbor queries. Queries that join multiple relations have not been addressed in earlier work despite the significance of joins in databases. In this paper we address join queries over uncertain data. We propose a semantics for the join operation, define probabilistic operators over uncertain data, and propose join algorithms that provide efficient execution of probabilistic joins. The paper focuses on an important class of joins termed probabilistic threshold joins that avoid some of the semantic complexities of dealing with uncertain data. For this class of joins we develop three sets of optimization techniques: item-level, page-level, and index-level pruning. These techniques facilitate pruning with little space and time overhead, and are easily adapted to most join algorithms. We verify the performance of these techniques experimentally.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "49613919",
                    "name": "Sarvjeet Singh"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                },
                {
                    "authorId": "144347727",
                    "name": "Rahul Shah"
                },
                {
                    "authorId": "1725956",
                    "name": "J. Vitter"
                },
                {
                    "authorId": "2940605",
                    "name": "Yuni Xia"
                }
            ]
        },
        {
            "paperId": "fb3a86bceae6c59da55a73dff64423a622c3c5a9",
            "title": "Efficient Clustering of Uncertain Data",
            "abstract": "We study the problem of clustering data objects whose locations are uncertain. A data object is represented by an uncertainty region over which a probability density function (pdf) is defined. One method to cluster uncertain objects of this sort is to apply the UK-means algorithm, which is based on the traditional K-means algorithm. In UK-means, an object is assigned to the cluster whose representative has the smallest expected distance to the object. For arbitrary pdf, calculating the expected distance between an object and a cluster representative requires expensive integration computation. We study various pruning methods to avoid such expensive expected distance calculation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2263618",
                    "name": "Wang Kay Ngai"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "24849138",
                    "name": "C. Chui"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145884195",
                    "name": "M. Chau"
                },
                {
                    "authorId": "2287757690",
                    "name": "Kevin Y. Yip"
                }
            ]
        },
        {
            "paperId": "2b3dc8956679b5f2d2662b2a014d5f21b255d288",
            "title": "Change tolerant indexing for constantly evolving data",
            "abstract": "Index structures are designed to optimize search performance, while at the same time supporting efficient data updates. Although not explicit, existing index structures are typically based upon the assumption that the rate of updates will be small compared to the rate of querying. This assumption is not valid in streaming data environments such as sensor and moving object databases, where updates are received incessantly. In fact, for many applications, the rate of updates may well exceed the rate of querying. In such environments, index structures suffer from poor performance due to the large overhead of keeping the index updated with the latest data. Recent efforts at indexing moving object data assume objects move in a restrictive manner (e.g. in straight lines with constant velocity). In this paper, we propose an index structure explicitly designed to perform well for both querying and updating. We assume a more relaxed model of object movement. In particular, we observe that objects often stay in a region (e.g., building) for an extended amount of time, and exploit this phenomenon to optimize an index for both updates and queries. The paper is developed with the example of R-trees, but the ideas can be extended to other index structures as well. We present the design of the change tolerant R-tree, and an experimental evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2940605",
                    "name": "Yuni Xia"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                },
                {
                    "authorId": "144347727",
                    "name": "Rahul Shah"
                }
            ]
        },
        {
            "paperId": "3bd4b06b59488e14bf5e015c4191aba34273065c",
            "title": "Sensors, Uncertainty Models, and Probabilistic Queries",
            "abstract": "Sensors are often used to monitor the status of an environment continuously. The sensor readings are reported to the application for making decisions and answering user queries. For example, a fire-alarm system in a building employs temperature sensors to detect any abrupt change in temperature. An aircraft is equipped with sensors to track the wind speed, and radars are used to report the aircraft\u2019s location to a military application. These applications usually include a database or server to which the sensor readings are sent. Limited network bandwidth and battery power imply that it is often not practical for the server to record the exact status of an entity it monitors at every time instant. In particular, if the value of an entity (e.g., temperature, location) being monitored is constantly evolving, the recorded data value may differ from the actual value. Querying the database can then produce incorrect results. Consider a simple example where a user asks the database: \u201cwhich room has a temperature between 10",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                }
            ]
        },
        {
            "paperId": "4de55b008a141dcf7ab4891b97e33e1a921225fa",
            "title": "Indexing continuously changing data with mean-variance tree",
            "abstract": "Constantly evolving data arise in various mobile applications such as location-based services and sensor networks. The problem of indexing the data for efficient query processing is of increasing importance. Due to the constant changing nature of the data, traditional indexes suffer from a high update overhead which leads to poor performance. In this paper, we propose a novel index structure, the MVTree, which is built based on the mean and variance of the data instead of the actual data values that are in constant flux. Since the mean and variance are relatively stable features compared to the actual values, the MVTree significantly reduces the index update cost. The distribution interval and probability distribution function of the data are not required to be known a priori. The mean and variance for each data item can be dynamically adjusted to match the observed fluctuation of the data. Experiments show that compared to traditional index schemes, the MVTree substantially improves index update performance while maintaining satisfactory query performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2940605",
                    "name": "Yuni Xia"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                },
                {
                    "authorId": "50113713",
                    "name": "Shan Lei"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "144347727",
                    "name": "Rahul Shah"
                }
            ]
        },
        {
            "paperId": "5c15f0d29b3e2511718ac02462639a7ef9975fff",
            "title": "Efficient Join Processing over Uncertain Data Technical Report",
            "abstract": "In database systems that collect information about the external environment, such as temperature and location values, it is often infeasible to obtain accurate information due to measurement and sampling errors, and resource limitations. Queries evaluated over these inaccurate data can potentially yield incorrect results. To avoid these problems. the idea of using uncertainty models (such as an interval associated with a probability density function) instead of a single value for modeling a data item has been explored in recent years. These works have focussed on simple queries such as range and nearest-neighbor queries. Queries that join multiple relations have not been addressed in earlier work despite the significance of joins in databases. In this paper we address join queries over uncertain data. As with other queries over uncertain data, these joins return probabilistic answers. A probabilistic Join Query (PJQ) augments the results with probability guarantees to indicate the likelihood of each join tuple being part of the result. Traditional join operators, such as equality and inequality, need to be extended to support uncertain data. In this paper, we present the notion of equality and inequality operators for uncertainty. vVe also introduce the concept of \"approximation\" in these comparison operators. Although PJQs are more informative than traditional joins. they are expensive to evaluate. To overcome this problem, we observe that often it is only necessary to know whether the probability of the results exceeds a given threshold. instead of the precise probability value. By incorporating this constraint into PJQ, it is possible to achieve much better performance. In particular, we develop three sets of optimization techniques, namely item-leveL page-level and index-level pruning. for different join operators. These techniques facilitate pruning with little space and time overhead, and are easily adapted to most join algorithms. Extensive simulation results show that these techniques improve the performance of joins significantly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2940605",
                    "name": "Yuni Xia"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                },
                {
                    "authorId": "144347727",
                    "name": "Rahul Shah"
                },
                {
                    "authorId": "1725956",
                    "name": "J. Vitter"
                }
            ]
        },
        {
            "paperId": "607523bafbf5dc948412792dfee02ee35c79d868",
            "title": "Querying Private Data in Moving-Object Environments",
            "abstract": "Location-based services, such as finding the nearest gas station, require users to supply their location information. However, a user\u2019s location can be tracked without her consent or knowledge. Lowering the spatial and temporal resolution of location data sent to the server has been proposed as a solution. Although this technique is effective in protecting privacy, it may be overkill and the quality of desired services can be severely affected. In this paper, we investigate the relationship between uncertainty, privacy, and quality of services. We propose using imprecise queries to hide the location of the query issuer and evaluate uncertain information. We also suggest a framework where uncertainty can be controlled to provide high quality and privacy-preserving services. We study how the idea can be applied to a moving range query over moving objects. We further investigate how the linkability of the proposed solution can be protected against trajectory-tracing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2153637586",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "1743774",
                    "name": "E. Bertino"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                }
            ]
        },
        {
            "paperId": "92233b72ce9ffe1a195ffbb37f1438163f422998",
            "title": "Indexing Multi-Dimensional Uncertain Data with Arbitrary Probability Density Functions",
            "abstract": "In an \"uncertain database\", an object o is associated with a multi-dimensional probability density function(pdf), which describes the likelihood that o appears at each position in the data space. A fundamental operation is the \"probabilistic range search\" which, given a value pq and a rectangular area rq, retrieves the objects that appear in rq with probabilities at least pq. In this paper, we propose the U-tree, an access method designed to optimize both the I/O and CPU time of range retrieval on multi-dimensional imprecise data. The new structure is fully dynamic (i.e., objects can be incrementally inserted/deleted in any order), and does not place any constraints on the data pdfs. We verify the query and update efficiency of U-trees with extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144779196",
                    "name": "Yufei Tao"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "2263618",
                    "name": "Wang Kay Ngai"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                }
            ]
        },
        {
            "paperId": "9ca475f27b3ebf644541860da6f790738cd0045b",
            "title": "Adaptive Stream Filters for Entity-based Queries with Non-Value Tolerance",
            "abstract": "We study the problem of applying adaptive filters for approximate query processing in a distributed stream environment. We propose filter bound assignment protocols with the objective of reducing communication cost. Most previous works focus on value-based queries (e.g., average) with numerical error tolerance. In this paper, we cover entity-based queries (e.g., a nearest neighbor query returns object names rather than a single value). In particular, we study non-value-based tolerance (e.g., the answer to the nearest-neighbor query should rank third or above). We investigate different non-value-based error tolerance definitions and discuss how they are applied to two classes of entity-based queries: non-rankbased and rank-based queries. Extensive experiments show that our protocols achieve significant savings in both communication overhead and server computation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                },
                {
                    "authorId": "2064026029",
                    "name": "Alan Kwan"
                },
                {
                    "authorId": "2812268",
                    "name": "Yi-Cheng Tu"
                }
            ]
        },
        {
            "paperId": "a668944e129616a644315db0a99625555a4e8a58",
            "title": "U-DBMS: A Database System for Managing Constantly-Evolving Data",
            "abstract": "In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "49613919",
                    "name": "Sarvjeet Singh"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                }
            ]
        },
        {
            "paperId": "f09fd8cf1f8003f94162cea45802229aac292f40",
            "title": "Uncertain Data Mining: A New Research Direction",
            "abstract": "Data uncertainty is often found in real-world applications due to reasons such as imprecise measurement, outdated sources, or sampling errors. Recently, much research has been published in the area of managing data uncertainty in databases. We propose that when data mining is performed on uncertain data, data uncertainty has to be considered in order to obtain high quality data mining results. We call this the \"Uncertain Data Mining\" problem. In this paper, we present a framework for possible research directions in this area. We also present the UK-means clustering algorithm as an example to illustrate how the traditional K-means algorithm can be modified to handle data uncertainty in data mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145884195",
                    "name": "M. Chau"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                }
            ]
        },
        {
            "paperId": "04df2771733d740d32fcbfa0f9ef2079536da1bc",
            "title": "Sensor node selection for execution of continuous probabilistic queries in wireless sensor networks",
            "abstract": "Due to the error-prone properties of sensors, it is important to use multiple low-cost sensors to improve the reliability of query results. However, using multiple sensors to generate the value for a data item can be expensive, especially in wireless environments where continuous queries are executed. Further, we need to distinguish effectively which sensors are not working properly and discard them from being used. In this paper, we propose a <i>probabilistic</i> approach to decide what sensor nodes to be used to answer a query. In particular, we propose to solve the problem with the aid of <i>continuous probabilistic query (CPQ)</i>, which is originally used to manage uncertain data and is associated with a probabilistic guarantee on the query result. Based on the historical data values from the sensor nodes, the query type, and the probabilistic requirement on the query result, we derive a simple method to select an appropriate set of sensors to provide reliable answers. We examine a wide range of common <i>aggregate queries</i>: average, sum, minimum, maximum, and range count query, but we believe our method can be extended to other query types. Our goal is to minimize sensor data aggregation workload in a network environment and at the same time meet the probabilistic requirement of the CPQ.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34454712",
                    "name": "K. Lam"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2554758",
                    "name": "Biyu Liang"
                },
                {
                    "authorId": "81628120",
                    "name": "Jonathan J. Chau"
                }
            ]
        },
        {
            "paperId": "09669b6b0eb122ae96872400832b7d470c3eb375",
            "title": "Evaluating probabilistic queries over imprecise data",
            "abstract": "Many applications employ sensors for monitoring entities such as temperature and wind speed. A centralized database tracks these entities to enable query processing. Due to continuous changes in these values and limited resources (e.g., network bandwidth and battery power), it is often infeasible to store the exact values at all times. A similar situation exists for moving object environments that track the constantly changing locations of objects. In this environment, it is possible for database queries to produce incorrect or invalid results based upon old data. However, if the degree of error (or uncertainty) between the actual value and the database value is controlled, one can place more confidence in the answers to queries. More generally, query answers can be augmented with probabilistic estimates of the validity of the answers. In this paper we study probabilistic query evaluation based upon uncertain data. A classification of queries is made based upon the nature of the result set. For each class, we develop algorithms for computing probabilistic answers. We address the important issue of measuring the quality of the answers to these queries, and provide algorithms for efficiently pulling data from relevant sensors or moving objects in order to improve the quality of the executing queries. Extensive experiments are performed to examine the effectiveness of several data update policies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                }
            ]
        },
        {
            "paperId": "52b60e55532930f5abdec463ddfcb88f8bf869b6",
            "title": "Managing uncertainty in sensor database",
            "abstract": "Sensors are often employed to monitor continuously changing entities like locations of moving objects and temperature. The sensor readings are reported to a centralized database system, and are subsequently used to answer queries. Due to continuous changes in these values and limited resources (e.g., network bandwidth and battery power), the database may not be able to keep track of the actual values of the entities, and use the old values instead. Queries that use these old values may produce incorrect answers. However, if the degree of uncertainty between the actual data value and the database value is limited, one can place more confidence in the answers to the queries. In this paper, we present a frame-work that represents uncertainty of sensor data. Depending on the amount of uncertainty information given to the application, different levels of imprecision are presented in a query answer. We examine the situations when answer imprecision can be represented qualitatively and quantitatively. We propose a new kind of probabilistic queries called Probabilistic Threshold Query, which requires answers to have probabilities larger than a certain threshold value. We also study techniques for evaluating queries under different details of uncertainty, and investigate the tradeoff between data uncertainty, answer accuracy and computation costs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                }
            ]
        },
        {
            "paperId": "d25f720872f17cd06edd1b1d935aab3c40bf34ff",
            "title": "Querying imprecise data in moving object environments",
            "abstract": "In moving object environments, it is infeasible for the database tracking the movement of objects to store the exact locations of objects at all times. Typically, the location of an object is known with certainty only at the time of the update. The uncertainty in its location increases until the next update. In this environment, it is possible for queries to produce incorrect results based upon old data. However, if the degree of uncertainty is controlled, then the error of the answers to queries can be reduced. More generally, query answers can be augmented with probabilistic estimates of the validity of the answer. We study the execution of probabilistic range and nearest-neighbor queries. The imprecision in answers to queries is an inherent property of these applications due to uncertainty in data, unlike the techniques for approximate nearest-neighbor processing that trade accuracy for performance. Algorithms for computing these queries are presented for a generic object movement model and detailed solutions are discussed for two common models of uncertainty in moving object databases. We study the performance of these queries through extensive simulations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                }
            ]
        },
        {
            "paperId": "e0cca9a1f4408f1245c1c48bdec7550ec6ec7e7a",
            "title": "Querying imprecise data in moving object environments",
            "abstract": "In moving object environments it is infeasible for the database tracking the movement of objects to store the exact locations of objects at all times. Typically the location of an object is known with certainty only at the time of the update. The uncertainty in its location increases until the next update. In this environment, it is possible for queries to produce incorrect results based upon old data. However, if the degree of uncertainty is controlled, then the error of the answers to certain queries can be reduced. More generally, query answers can be augmented with probabilistic estimates of the validity of the answer. We study the execution of such probabilistic nearest-neighbor queries. The imprecision in answers to the queries is an inherent property of these applications due to uncertainty in the data, unlike the techniques for approximate nearest-neighbor processing that trade accuracy for performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "48905676",
                    "name": "Sunil Prabhakar"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                }
            ]
        },
        {
            "paperId": "e3c4fecb2f598e75617c6a188ad5f365b9d9b742",
            "title": "Maintaining Temporal Consistency of Discrete Objects in Soft Real-Time Database Systems",
            "abstract": "A real-time database system contains base data items which record and model a physical, real-world environment. For better decision support, base data items are summarized and correlated to derive views. These base data and views are accessed by application transactions to generate the ultimate actions taken by the system. As the environment changes, updates are applied to base data, which subsequently trigger view recomputations. There are thus three types of activities: base data update, view recomputation, and transaction execution. In a real-time database system, two timing constraints need to be enforced. We require that transactions meet their deadlines (transaction timeliness) and read fresh data (data timeliness). In this paper, we define the concept of absolute and relative temporal consistency from the perspective of transactions for discrete data objects. We address the important issue of transaction scheduling among the three types of activities such that the two timing requirements can be met. We also discuss how a real-time database system should be designed to enforce different levels of temporal consistency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "34454712",
                    "name": "K. Lam"
                },
                {
                    "authorId": "2832139",
                    "name": "Brad Adelberg"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2110585638",
                    "name": "Tony S. H. Lee"
                }
            ]
        },
        {
            "paperId": "4b351d7d6c9f586e5ad7647ae9b62df1f8ca2d0b",
            "title": "Updates and view maintenance in soft real-time database systems",
            "abstract": "A database system contains base data items which record and model a physical, real world environment. For better decision support, base data items are summarized and correlated to derive views. These base data and views are accessed by application transactions to generate the ultimate actions taken by the system. As the environment changes, updates are applied to the base data, which subsequently trigger view recomputations. There are thus three types of activities: base data update, view recomputation, and transaction execution. In a real-time system, two timing constrains need to be enforced. We require transactions meet their deadlines (transaction timeliness) and read fresh data (data timeliness). In this paper we define the concept of absolute and relative temporal consistency from the perspective of transactions. We address the important issue of transaction scheduling among the three types of activities such that the two timing requirements can be met. We also discuss how a real-time database system should be designed to enforce different levels of temporal consistency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145868757",
                    "name": "B. Kao"
                },
                {
                    "authorId": "34454712",
                    "name": "K. Lam"
                },
                {
                    "authorId": "2832139",
                    "name": "Brad Adelberg"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "2110585638",
                    "name": "Tony S. H. Lee"
                }
            ]
        }
    ]
}