{
    "authorId": "2066796687",
    "papers": [
        {
            "paperId": "360e4cbd79690f75756900dcb4818cf177071995",
            "title": "Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust",
            "abstract": "A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs. This has significant application-level user trust implications as stochastic patterns without a strong sense of meaning cannot be trusted in high-stakes applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2066796687",
                    "name": "Tarun Garg"
                },
                {
                    "authorId": "2216605916",
                    "name": "Vedant Palit"
                },
                {
                    "authorId": "2187301061",
                    "name": "Yuxin Zi"
                },
                {
                    "authorId": "2068096390",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "105ba5f254179b4c6fc812f269f499ae43d3200e",
            "title": "Can Language Models Capture Graph Semantics? From Graphs to Language Model and Vice-Versa",
            "abstract": "Knowledge Graphs are a great resource to capture semantic knowledge in terms of entities and relationships between the entities. However, current deep learning models takes as input distributed representations or vectors. Thus, the graph is compressed in a vectorized representation. We conduct a study to examine if the deep learning model can compress a graph and then output the same graph with most of the semantics intact. Our experiments show that Transformer models are not able to express the full semantics of the input knowledge graph. We find that this is due to the disparity between the directed, relationship and type based information contained in a Knowledge Graph and the fully connected token-token undirected graphical interpretation of the Transformer Attention matrix.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066796687",
                    "name": "Tarun Garg"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                }
            ]
        }
    ]
}