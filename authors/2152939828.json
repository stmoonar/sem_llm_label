{
    "authorId": "2152939828",
    "papers": [
        {
            "paperId": "0e523bd5345125874191e066f8cc73257063215c",
            "title": "Unified Dense Subgraph Detection: Fast Spectral Theory Based Algorithms",
            "abstract": "How can we effectively detect fake reviews or fraudulent links on a website? How can we spot communities that suddenly appear based on users\u2019 interactions? And how can we efficiently find the minimum cut in a large graph? All of these are related to the finding of dense subgraphs, a significant primitive problem in graph analysis with extensive applications across various domains. In this paper, we focus on formulating the problem of the densest subgraph detection and theoretically compare and contrast several correlated problems. Moreover, we propose a unified framework, <sc>GenDS</sc>, for the densest subgraph detection, provide some theoretical analysis based on the network flow and spectral graph theory, and devise simple and computationally efficient algorithms, <sc>SpecGDS</sc> and <sc>GepGDS</sc>, to solve it by leveraging the spectral properties and greedy search. We conduct thorough experiments on 40 real-world networks with up to 1.47 billion edges from various domains. We demonstrate that our <sc>SpecGDS</sc> yields up to <inline-formula><tex-math notation=\"LaTeX\">$58.6 \\ \\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>58</mml:mn><mml:mo>.</mml:mo><mml:mn>6</mml:mn><mml:mspace width=\"4pt\"/><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"feng-ieq1-3272574.gif\"/></alternatives></inline-formula>speedup and achieves better or approximately equal-quality solutions for the densest subgraph detection compared to the baselines. <sc>GepGDS</sc> also reveals some properties of generalized eigenvalue problems for the <sc>GenDS</sc>. Also, our methods scale linearly with the graph size and are proven effective in applications such as finding collaborations that appear suddenly in an extensive, time-evolving co-authorship network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948356",
                    "name": "Wenjie Feng"
                },
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "9484dbcfc77daa9fde4bc4d86b43588c2f929d1c",
            "title": "Node Embedding Preserving Graph Summarization",
            "abstract": "Graph summarization is a useful tool for analyzing large-scale graphs. Some works tried to preserve original node embeddings encoding rich structural information of nodes on the summary graph. However, their algorithms are designed heuristically and not theoretically guaranteed. In this article, we theoretically study the problem of preserving node embeddings on summary graph. We prove that three matrix-factorization-based node embedding methods of the original graph can be approximated by that of the summary graph, and we propose a novel graph summarization method, named HCSumm, based on this analysis. Extensive experiments are performed on real-world datasets to evaluate the effectiveness of our proposed method. The experimental results show that our method outperforms the state-of-the-art methods in preserving node embeddings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157474971",
                    "name": "Houquan Zhou"
                },
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                },
                {
                    "authorId": "2476503",
                    "name": "Huawei Shen"
                },
                {
                    "authorId": "2110251463",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "9927f514fc9118276230ed0fb7f80c402326453f",
            "title": "Time Series Anomaly Detection With Adversarial Reconstruction Networks",
            "abstract": "Time series data naturally exist in many domains including medical data analysis, infrastructure sensor monitoring, and motion tracking. However, a very small portion of anomalous time series can be observed, comparing to the whole data. Most existing approaches are based on the supervised classification model requiring representative labels for anomaly class(es), which is challenging in real-world problems. So can we learn how to detect anomalous time ticks in an effective yet efficient way, given mostly normal time series data? Therefore, we propose an unsupervised reconstruction model named BeatGAN which learns to detect anomalies based on normal data, or data which majority of samples are normal. BeatGAN provides a framework to adversarially learn to reconstruct, which can cooperate with both 1-d CNN and RNN. Rarely observed anomalies can result in larger reconstruction errors, which are then detected based on extreme value theory. Moreover, data augmentation with dynamic time warping regularizes reconstruction and provides robustness. In the experiments, effectiveness and sensitivity are studied in both synthetic data and various real-world time series. BeatGAN achieves better accuracy and fast inference.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                },
                {
                    "authorId": "51483278",
                    "name": "Bin Zhou"
                },
                {
                    "authorId": "73279933",
                    "name": "Quan-Xin Ding"
                },
                {
                    "authorId": "2019961",
                    "name": "Bryan Hooi"
                },
                {
                    "authorId": "2148906471",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2476503",
                    "name": "Huawei Shen"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "bc4bf8864aff1d41ad76d677714ea21606b290c4",
            "title": "A Provable Framework of Learning Graph Embeddings via Summarization",
            "abstract": "Given a large graph, can we learn its node embeddings from a smaller summary graph? What is the relationship between embeddings learned from original graphs and their summary graphs? Graph representation learning plays an important role in many graph mining applications, but learning em-beddings of large-scale graphs remains a challenge. Recent works try to alleviate it via graph summarization, which typ-ically includes the three steps: reducing the graph size by combining nodes and edges into supernodes and superedges,learning the supernode embedding on the summary graph and then restoring the embeddings of the original nodes. How-ever, the justification behind those steps is still unknown.\nIn this work, we propose GELSUMM, a well-formulated graph embedding learning framework based on graph sum-marization, in which we show the theoretical ground of learn-ing from summary graphs and the restoration with the three well-known graph embedding approaches in a closed form.Through extensive experiments on real-world datasets, we demonstrate that our methods can learn graph embeddings with matching or better performance on downstream tasks.This work provides theoretical analysis for learning node em-beddings via summarization and helps explain and under-stand the mechanism of the existing works.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157474971",
                    "name": "Houquan Zhou"
                },
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "2476503",
                    "name": "Huawei Shen"
                },
                {
                    "authorId": "2110251463",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "be19bb89159b720e4a7b8612a57a3b8e3f2451af",
            "title": "Fast Searching The Densest Subgraph And Decomposition With Local Optimality",
            "abstract": "Densest Subgraph Problem (DSP) is an important primitive problem with a wide range of applications, including fraud detection, community detection and DNA motif discovery. Edge-based density is one of the most common metrics in DSP. Although a maximum flow algorithm can exactly solve it in polynomial time, the increasing amount of data and the high complexity of algorithms motivate scientists to find approximation algorithms. Among these, its duality of linear programming derives several iterative algorithms including Greedy++, Frank-Wolfe and FISTA which redistribute edge weights to find the densest subgraph, however, these iterative algorithms vibrate around the optimal solution, which are not satisfactory for fast convergence. We propose our main algorithm Locally Optimal Weight Distribution (LOWD) to distribute the remaining edge weights in a locally optimal operation to converge to the optimal solution monotonically. Theoretically, we show that it will reach the optimal state of a specific linear programming which is called locally-dense decomposition. Besides, we show that it is not necessary to consider most of the edges in the original graph. Therefore, we develop a pruning algorithm using a modified Counting Sort to prune graphs by removing unnecessary edges and nodes, and then we can search the densest subgraph in a much smaller graph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "114689037",
                    "name": "Yu Zhu"
                },
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                },
                {
                    "authorId": "145948356",
                    "name": "Wenjie Feng"
                },
                {
                    "authorId": "2226196831",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "0e8240c14201d4b87c4cc026acad4020a672cc0f",
            "title": "MonLAD: Money Laundering Agents Detection in Transaction Streams",
            "abstract": "Given a stream of money transactions between accounts in a bank, how can we accurately detect money laundering agent accounts and suspected behaviors in real-time? Money laundering agents try to hide the origin of illegally obtained money by dispersive multiple small transactions and evade detection by smart strategies. Therefore, it is challenging to accurately catch such fraudsters in an unsupervised manner. Existing approaches do not consider the characteristics of those agent accounts and are not suitable to the streaming settings. Therefore, we propose MonLAD and MonLAD-W to detect money laundering agent accounts in a transaction stream by keeping track of their residuals and other features; we devise AnoScore algorithm to find anomalies based on the robust measure of statistical deviation. Experimental results show that MonLAD outperforms the state-of-the-art baselines on real-world data and finds various suspicious behavior patterns of money laundering. Additionally, several detected suspected accounts have been manually-verified as agents in real money laundering scenario.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144851774",
                    "name": "Xiaobing Sun"
                },
                {
                    "authorId": "145948356",
                    "name": "Wenjie Feng"
                },
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                },
                {
                    "authorId": "41092234",
                    "name": "Yuyang Xie"
                },
                {
                    "authorId": "3224168",
                    "name": "Siddharth Bhatia"
                },
                {
                    "authorId": "2019961",
                    "name": "Bryan Hooi"
                },
                {
                    "authorId": "2118788801",
                    "name": "Wenhan Wang"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "175618cbd605169252264b4528a647970779c05c",
            "title": "Hierarchical Dense Pattern Detection in Tensors",
            "abstract": "Dense subtensor detection gains remarkable success in spotting anomalies and fraudulent behaviors for multi-aspect data (i.e., tensors), like in social media and event streams. Existing methods detect the densest subtensors flatly and separately, with the underlying assumption that those subtensors are exclusive. However, many real-world tensors usually present hierarchical properties, e.g., the core-periphery structure and dynamic communities in networks. It is also unexplored how to fuse the prior knowledge into dense pattern detection to capture the local behavior. In this article, we propose CatchCore, a novel framework to efficiently find the hierarchical dense subtensors. We first design a unified metric for dense subtensor detection, which can be optimized with gradient-based methods. With the proposed metric, CatchCore detects hierarchical dense subtensors through the hierarchy-wise alternative optimization and finds local dense patterns concerning some items in a query manner. Finally, we utilize the minimum description length principle to measure the quality of detection results and select the optimal hierarchical dense subtensors. Extensive experiments on synthetic and real-world datasets demonstrate that CatchCore outperforms the top competitors in accuracy for detecting dense subtensors and anomaly patterns, like network attacks. Additionally, CatchCore successfully identifies a hierarchical researcher co-authorship group with intense interactions in the DBLP dataset; it can also capture core collaboration and multi-hop relations around some query objects. Meanwhile, CatchCore also scales linearly with all aspects of tensors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948356",
                    "name": "Wenjie Feng"
                },
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                },
                {
                    "authorId": "2110251463",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "776403354aa1dc780427555fb8efd460b24fe2de",
            "title": "On the Cybernetics of Crowdsourcing Innovation: A Process Model",
            "abstract": "For small and medium-sized enterprises and large enterprises alike, crowdsourcing innovation has become an important element of a product\u2019s whole life cycle. It is the open call process of soliciting consumers to harvest and evaluate ideas or other intellectual assets. The previous proposed taxonomic framework for charactering this process is mainly for general crowdsourcing process and summarized by empirical study. The purpose of this paper is to propose a conceptual model for crowdsourcing innovation from a cybernetic and knowledge management perspective by normative research. The authors performed a normative study and deduced five systemic characteristics from the general laws of control system that guarantee ongoing efficiency for the innovation process. The normative research results provide two key contributions. Firstly, general control laws deduce five indispensable characteristics, and they reveal the intrinsic mechanism of crowdsourcing innovation: the knowledge flow controls, which are also the connotation of open innovation. Secondly, the authors have analyzed a five-characteristics system model and tested the model in several classical cases to show the design tricks of cases. This study provides a new conceptual framework that integrates the theory of open innovation and cybernetics to provide a new view of crowdsourcing innovation process design. In practice, this framework guides managers through the design criteria needed to implement a success crowdsourcing process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47965979",
                    "name": "Lei Lin"
                },
                {
                    "authorId": "2476503",
                    "name": "Huawei Shen"
                },
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                },
                {
                    "authorId": "2112319012",
                    "name": "Li Xu"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "b5cd5db5ecc856e333f0b06e09c83f8214562ac0",
            "title": "Learning node embeddings via summary graphs: a brief theoretical analysis",
            "abstract": "Graph representation learning plays an important role in many graph mining applications, but learning embeddings of large-scale graphs remains a problem. Recent works try to improve scalability via graph summarization -- i.e., they learn embeddings on a smaller summary graph, and then restore the node embeddings of the original graph. However, all existing works depend on heuristic designs and lack theoretical analysis. Different from existing works, we contribute an in-depth theoretical analysis of three specific embedding learning methods based on introduced kernel matrix, and reveal that learning embeddings via graph summarization is actually learning embeddings on a approximate graph constructed by the configuration model. We also give analysis about approximation error. To the best of our knowledge, this is the first work to give theoretical analysis of this approach. Furthermore, our analysis framework gives interpretation of some existing methods and provides great insights for future work on this problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157474971",
                    "name": "Houquan Zhou"
                },
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "2476503",
                    "name": "Huawei Shen"
                },
                {
                    "authorId": "2110251463",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "c456d5b5932ba95d227a86696cb24c0b0a29659b",
            "title": "CGCNImp: a causal graph convolutional network for multivariate time series imputation",
            "abstract": "Background Multivariate time series data generally contains missing values, which can be an obstacle to subsequent analysis and may compromise downstream applications. One challenge in this endeavor is the presence of the missing values brought about by sensor failure and transmission packet loss. Imputation is the usual remedy in such circumstances. However, in some multivariate time series data, the complex correlation and temporal dependencies, coupled with the non-stationarity of the data, make imputation difficult. Mehods To address this problem, we propose a novel model for multivariate time series imputation called CGCNImp that considers both correlation and temporal dependency modeling. The correlation dependency module leverages neural Granger causality and a GCN to capture the correlation dependencies among different attributes of the time series data, while the temporal dependency module relies on an attention-driven long short term memory (LSTM) and a time lag matrix to learn its dependencies. Missing values and noise are addressed with total variation reconstruction. Results We conduct thorough empirical analyses on two real-world datasets. Imputation results show that CGCNImp achieves state-of-the-art performance when compared to previous methods.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156553677",
                    "name": "Caizheng Liu"
                },
                {
                    "authorId": "2155928279",
                    "name": "Guangfan Cui"
                },
                {
                    "authorId": "2152939828",
                    "name": "Shenghua Liu"
                }
            ]
        }
    ]
}