{
    "authorId": "2111900571",
    "papers": [
        {
            "paperId": "357613ea0e90bd41fb942fd65f39498e71e2dbc3",
            "title": "Mixture of Soft Prompts for Controllable Data Generation",
            "abstract": "Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when compared against strong baselines. Our method offers an alternate data-centric approach for applying LLMs to complex prediction tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111900571",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "2109420234",
                    "name": "Celine Lee"
                },
                {
                    "authorId": "46215479",
                    "name": "Yunan Lu"
                },
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "9b527b6dd2b2529b0ec2ec49323211bc0a8ea566",
            "title": "Data Augmentation for Intent Classification",
            "abstract": "Training accurate intent classifiers requires labeled data, which can be costly to obtain. Data augmentation methods may ameliorate this issue, but the quality of the generated data varies significantly across techniques. We study the process of systematically producing pseudo-labeled data given a small seed set using a wide variety of data augmentation techniques, including mixing methods together. We find that while certain methods dramatically improve qualitative and quantitative performance, other methods have minimal or even negative impact. We also analyze key considerations when implementing data augmentation methods in production.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111900571",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "2170078856",
                    "name": "Claire Yin"
                }
            ]
        },
        {
            "paperId": "002bdeeb00007247cbb78ee45a089a12baa70397",
            "title": "GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation",
            "abstract": "Practical dialogue systems require robust methods of detecting out-of-scope (OOS) utterances to avoid conversational breakdowns and related failure modes. Directly training a model with labeled OOS examples yields reasonable performance, but obtaining such data is a resource-intensive process. To tackle this limited-data problem, previous methods focus on better modeling the distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal technique that augments existing data to train better OOS detectors operating in low-data regimes. GOLD generates pseudo-labeled candidates using samples from an auxiliary dataset and keeps only the most beneficial candidates for training through a novel filtering mechanism. In experiments across three target benchmarks, the top GOLD model outperforms all existing methods on all key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median baseline performance. We also analyze the unique properties of OOS data to identify key factors for optimally applying our proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111900571",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "1564034697",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "37132b674d87a549d8566894c72d38910ffbd301",
            "title": "Learning with Noisy Labels by Targeted Relabeling",
            "abstract": "Crowdsourcing platforms are often used to collect datasets for training deep neural networks, despite higher levels of inaccurate labeling compared to expert labeling. There are two common strategies to manage the impact of this noise, the first involves aggregating redundant annotations, but comes at the expense of labeling substantially fewer examples. Secondly, prior works have also considered using the entire annotation budget to label as many examples as possible and subsequently apply denoising algorithms to implicitly clean up the dataset. We propose an approach which instead reserves a fraction of annotations to explicitly relabel highly probable labeling errors. In particular, we allocate a large portion of the labeling budget to form an initial dataset used to train a model. This model is then used to identify specific examples that appear most likely to be incorrect, which we spend the remaining budget to relabel. Experiments across three model variations and four natural language processing tasks show our approach outperforms both label aggregation and advanced denoising methods designed to handle noisy labels when allocated the same annotation budget.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111900571",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "1564034697",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "3644767",
                    "name": "Samuel R. Bowman"
                }
            ]
        },
        {
            "paperId": "e13bee68e7c2de3aed873cb8d411143a16bb2f51",
            "title": "DG2: Data Augmentation Through Document Grounded Dialogue Generation",
            "abstract": "Collecting data for training dialog systems can be extremely expensive due to the involvement of human participants and the need for extensive annotation. Especially in document-grounded dialog systems, human experts need to carefully read the unstructured documents to answer the users\u2019 questions. As a result, existing document-grounded dialog datasets are relatively small-scale and obstruct the effective training of dialogue systems. In this paper, we propose an automatic data augmentation technique grounded on documents through a generative dialogue model. The dialogue model consists of a user bot and agent bot that can synthesize diverse dialogues given an input document, which is then used to train a downstream model. When supplementing the original dataset, our method achieves significant improvement over traditional data augmentation methods. We also achieve great performance in the low-resource setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152862795",
                    "name": "Qingyang Wu"
                },
                {
                    "authorId": "145480864",
                    "name": "Song Feng"
                },
                {
                    "authorId": "2111900571",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "1703799",
                    "name": "Sachindra Joshi"
                },
                {
                    "authorId": "8390140",
                    "name": "L. Lastras"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "f13f0cf34c418fa64473d03f2a7b004cc5bc5c41",
            "title": "Clean or Annotate: How to Spend a Limited Data Collection Budget",
            "abstract": "t",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111900571",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "2116680369",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "3644767",
                    "name": "Samuel R. Bowman"
                }
            ]
        },
        {
            "paperId": "81acc759921440b4d2e789279106e49af9d9a569",
            "title": "Function-as-a-Service Application Service Composition: Implications for a Natural Language Processing Application",
            "abstract": "Serverless computing platforms provide Function-as-a-Service (FaaS) to end users for hosting individual functions known as microservices. In this paper, we describe the deployment of a Natural Language Processing (NLP) application using AWS Lambda. We investigate and study the performance and memory implications of two alternate service compositions. First, we evaluate a switchboard architecture, where a single Lambda deployment package aggregates all of the NLP application functions together into a single package. Second, we consider a service isolation architecture where each NLP function is deployed as a separate FaaS function decomposing the application to run across separate runtime containers. We compared the average runtime and processing throughput of these compositions using different pre-trained network weights to initialize our neural networks to perform inference. Additionally, we varied the workload dataset sizes to evaluate implications of inferencing throughput for our NLP application deployed to a FaaS platform. We found our switchboard composition, that shares FaaS runtime containers for all application tasks, produced a 14.75% runtime performance improvement, and also a 17.3% improvement in NLP processing throughput (samples/second). These results demonstrate the potential for careful application service compositions to provide notable performance improvements and ultimately cost savings for application deployments to serverless FaaS platforms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46954772",
                    "name": "M. Fotouhi"
                },
                {
                    "authorId": "2111900571",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "152208762",
                    "name": "W. Lloyd"
                }
            ]
        },
        {
            "paperId": "cac598f2c99c04c475ce41ff380afbddcd32480d",
            "title": "Representing document-level semantics of biomedical literature using pre-trained embedding models: Novel assessments",
            "abstract": "We present two novel tasks aimed at capturing document-level semantics, i.e., high-level topical or thematic content, of bio-medical scientific publications. We use these tasks to evaluate whether word and sequence embedding models pre-trained on biomedical literature can be used to derive meaningful document-level semantic representations for these publications. We evaluate approaches from two broad categories: (1) lexical pooling , or vectorizing documents purely based on aggregation of lexical items, which includes the NCBI\u2019s BioWordVec model and Tf-idf-based vec-torizations, both with and without word pre-filtering based on biomedical ontologies, (2) sequence embedding , which includes the NCBI\u2019s BioSentVec model and BioBERT. For both of our tasks, lexical pooling outperformed sequence embedding, and the best overall method was mean pooling of BioWordVec word embeddings. We also include baselines trained on non-biomedical English to show that training on biomedical literature is warranted. The methods discussed here have potential applications for clustering, comparing, analyzing and recommending scientific literature in the biomedical domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2741197",
                    "name": "J. Stevens"
                },
                {
                    "authorId": "1434552423",
                    "name": "Brandon Punturo"
                },
                {
                    "authorId": "2111900571",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "2110134212",
                    "name": "Mike Kim"
                },
                {
                    "authorId": "2055295868",
                    "name": "Jacob Zimmer"
                }
            ]
        }
    ]
}