{
    "authorId": "46485412",
    "papers": [
        {
            "paperId": "7aa33100b452effabe77d5028770648d84cd1090",
            "title": "Topology-Aware Matrix Partitioning Method for FPGA Real-Time Simulation of Power Electronics Systems",
            "abstract": "Field-programmable gate array (FPGA)-based real-time simulation serves as a powerful tool for testing and validating power electronics system (PES). However, accurate real-time simulation in high-frequency applications demands extremely small time steps, which poses significant computational and memory challenges. Explicit methods can achieve small time steps due to low-computational burdens, but have stability issues. In this article, the authors propose a topology-aware matrix partitioning (TA-MP) method, which adopts the implicit numerical algorithm to ensure stability, and uses iterative methods to solve the implicit equations. In detail, the PES matrix is partitioned into blocks with clear topological meanings, and its constant diagonal blocks are used to construct a constant iterative matrix. Since the iterative matrix is constant, the TA-MP method bypasses the limitations of using iterative methods in real-time simulations by predetermining the number of iterations. The TA-MP method eliminates the need to store inverse matrices for different topologies, requiring only matrix-vector multiplications, which are efficient on FPGA. The TA-MP method's effectiveness is validated with n-port active bridge converters on an FPGA board with a 25 ns step size, ensuring simulation accuracy at the highest switching frequency of 200 kHz while consuming only 1/15 the memory resources of the conventional method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2175565401",
                    "name": "Jialin Zheng"
                },
                {
                    "authorId": "30709173",
                    "name": "Yangbin Zeng"
                },
                {
                    "authorId": "2154741616",
                    "name": "Weicheng Liu"
                },
                {
                    "authorId": "2186424079",
                    "name": "Fuhai Zhao"
                },
                {
                    "authorId": "2238346908",
                    "name": "Chunhui Qu"
                },
                {
                    "authorId": "2238391935",
                    "name": "Zhengming Zhao"
                }
            ]
        },
        {
            "paperId": "f07898530c2746d70ae6c4b01c4d4c1a1ff13f47",
            "title": "An Event Driven Synchronization Framework for Physical Controller Co-Simulation of Megawatt-Level Power Electronic Systems",
            "abstract": "Model-based design (MBD) technology has substantially enhanced safety, reliability, and efficiency in power electronic system (PES) controller development. However, conventional MBD technologies face real-time constraints when collaborated with physical controllers, posing challenges for applying them to megawatt-level (MW-level) PESs with numerous switches. Therefore, this article presents a physical controller co-simulation (PCCO) approach to alleviate real-time constraints and satisfy controller testing requirements for MW-level PESs. Besides, an event-driven synchronization (EDS) framework is proposed to maintain consistent controller behavior in both the PCCO simulation and real -world systems, while using switch-event information to accelerate the simulation. Moreover, a hybrid CPU-FPGA hardware platform is designed for the PCCO simulation, and a 2 MW power electronic transformer with 576 switches is implemented as the case study with the EDS framework. The results show that the EDS framework provides a high-accuracy numerical controller testing environment for MW-level PESs without altering the controller behavior. Comparative analysis with commercial HIL simulators, and prototypes indicates that the proposed framework supports safe and efficient testing of physical controllers in large-scale MW-level PESs, thereby promoting the use of MW-level converters in modern power grids.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2175565401",
                    "name": "Jialin Zheng"
                },
                {
                    "authorId": "30709173",
                    "name": "Yangbin Zeng"
                },
                {
                    "authorId": "2238391935",
                    "name": "Zhengming Zhao"
                },
                {
                    "authorId": "2154741616",
                    "name": "Weicheng Liu"
                },
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "13423923",
                    "name": "Shusheng Wei"
                },
                {
                    "authorId": "2238797763",
                    "name": "Hong Li"
                }
            ]
        },
        {
            "paperId": "09f43150f01824ddd2bcc56629f87e26df57b0c3",
            "title": "Sharpness-Aware Data Poisoning Attack",
            "abstract": "Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs) against data poisoning attacks. These attacks aim to inject poisoning samples into the models' training dataset such that the trained models have inference failures. While previous studies have executed different types of attacks, one major challenge that greatly limits their effectiveness is the uncertainty of the re-training process after the injection of poisoning samples, including the re-training initialization or algorithms. To address this challenge, we propose a novel attack method called ''Sharpness-Aware Data Poisoning Attack (SAPA)''. In particular, it leverages the concept of DNNs' loss landscape sharpness to optimize the poisoning effect on the worst re-trained model. It helps enhance the preservation of the poisoning effect, regardless of the specific retraining procedure employed. Extensive experiments demonstrate that SAPA offers a general and principled strategy that significantly enhances various types of poisoning attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2049444216",
                    "name": "P. He"
                },
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "143702207",
                    "name": "J. Ren"
                },
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1682418",
                    "name": "C. Aggarwal"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "37b1841b6f577cdf30e45b291019cc9bf22543e3",
            "title": "A Semi-Implicit Parallel Leapfrog Solver With Half-Step Sampling Technique for FPGA-Based Real-Time HIL Simulation of Power Converters",
            "abstract": "Field programmable gate array (FPGA) based hardware-in-the-loop (HIL) simulation is an effective tool to verify the performance of physical controllers and shorten the development cycle of power converters. In HIL simulations, sampling accuracy is of concern and is desired to be improved by reducing the step size. However, due to the cost of computational time, the step size is hard to reduce indefinitely to meet the requirements for high switching frequency applications. To improve the sampling accuracy and simulation performance of HIL simulation, this article proposes a semi-implicit parallel leapfrog (SPL) solver with half-step sampling technique. In this solver, the switches and the rest part of the system are implemented to be computed parallel when the switch leg model operates in continuous current mode. Besides, the solver is formulated in leapfrog format to reduce computational costs and to compute at half-step as a minimum step-size. With this format, the half-step sampling technique can be employed to increase the sampling rate by onefold, even in cases where it is challenging to reduce the simulation step size further. A dual active bridge converter case is implemented on the FPGA board with a 12.5-ns sampling step-size, retaining the simulation accuracy while switched at 400 kHz. To further verify the advantages, the results are compared with other HIL method and experimental results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2175565401",
                    "name": "Jialin Zheng"
                },
                {
                    "authorId": "30709173",
                    "name": "Yangbin Zeng"
                },
                {
                    "authorId": "2238391935",
                    "name": "Zhengming Zhao"
                },
                {
                    "authorId": "2154741616",
                    "name": "Weicheng Liu"
                },
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "3894986",
                    "name": "Shiqi Ji"
                }
            ]
        },
        {
            "paperId": "9a6ea2211c53bedb2e4035e981bd08254222381c",
            "title": "DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models",
            "abstract": "Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, DiffusionShield ensures low distortion of the original image, high watermark detection performance, and the ability to embed lengthy messages. We conduct rigorous and comprehensive experiments to show the effectiveness of DiffusionShield in defending against infringement by GDMs and its superiority over traditional watermarking methods. The code for DiffusionShield is accessible in https://github.com/Yingqiancui/DiffusionShield.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "143702207",
                    "name": "J. Ren"
                },
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2185740224",
                    "name": "Pengfei He"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "49755259",
                    "name": "Lichao Sun"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "d59ec829fd4d4f8db2d3671fd98ccd8e3c8c1fc6",
            "title": "Probabilistic Categorical Adversarial Attack and Adversarial Training",
            "abstract": "The studies on adversarial attacks and defenses have greatly improved the robustness of Deep Neural Networks (DNNs). Most advanced approaches have been overwhelmingly designed for continuous data such as images. However, these achievements are still hard to be generalized to categorical data. To bridge this gap, we propose a novel framework, Probabilistic Categorical Ad-versarial Attack (or PCAA) . It transfers the discrete optimization problem of finding categorical adversarial examples to a continuous problem that can be solved via gradient-based methods. We analyze the optimality (attack success rate) and time complexity of PCAA to demonstrate its significant advantage over current search-based attacks. More importantly, through extensive empirical studies, we demonstrate that the well-established defenses for continuous data, such as adversarial training and TRADES, can be easily accommo-dated to defend DNNs for categorical data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2185740224",
                    "name": "Pengfei He"
                },
                {
                    "authorId": "143702207",
                    "name": "J. Ren"
                },
                {
                    "authorId": "2167583580",
                    "name": "Yuxuan Wan"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "41c870ecf22565f567c18a8171038ee796083776",
            "title": "Defense Against Gradient Leakage Attacks via Learning to Obscure Data",
            "abstract": "Federated learning is considered as an effective privacy-preserving learning mechanism that separates the client's data and model training process. However, federated learning is still under the risk of privacy leakage because of the existence of attackers who deliberately conduct gradient leakage attacks to reconstruct the client data. Recently, popular strategies such as gradient perturbation methods and input encryption methods have been proposed to defend against gradient leakage attacks. Nevertheless, these defenses can either greatly sacrifice the model performance, or be evaded by more advanced attacks. In this paper, we propose a new defense method to protect the privacy of clients' data by learning to obscure data. Our defense method can generate synthetic samples that are totally distinct from the original samples, but they can also maximally preserve their predictive features and guarantee the model performance. Furthermore, our defense strategy makes the gradient leakage attack and its variants extremely difficult to reconstruct the client data. Through extensive experiments, we show that our proposed defense method obtains better privacy protection while preserving high accuracy compared with state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2167583580",
                    "name": "Yuxuan Wan"
                },
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2124928119",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "143702207",
                    "name": "J. Ren"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "5ec0ba455f3385528673ce2b1d23ac32ca241b39",
            "title": "Towards Fair Classification against Poisoning Attacks",
            "abstract": "Fair classification aims to stress the classification models to achieve the equality (treatment or prediction quality) among different sensitive groups. However, fair classification can be under the risk of poisoning attacks that deliberately insert malicious training samples to manipulate the trained classifiers' performance. In this work, we study the poisoning scenario where the attacker can insert a small fraction of samples into training data, with arbitrary sensitive attributes as well as other predictive features. We demonstrate that the fairly trained classifiers can be greatly vulnerable to such poisoning attacks, with much worse accuracy&fairness trade-off, even when we apply some of the most effective defenses (originally proposed to defend traditional classification tasks). As countermeasures to defend fair classification tasks, we propose a general and theoretically guaranteed framework which accommodates traditional defense methods to fair classification against poisoning attacks. Through extensive experiments, the results validate that the proposed defense framework obtains better robustness in terms of accuracy and fairness than representative baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2124928119",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "2167583580",
                    "name": "Yuxuan Wan"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "70970b8633e5b71079023c833447f7b672e7ace2",
            "title": "Probabilistic Categorical Adversarial Attack & Adversarial Training",
            "abstract": "The existence of adversarial examples brings huge concern for people to apply Deep Neural Networks (DNNs) in safety-critical tasks. However, how to generate adversarial examples with categorical data is an important problem but lack of extensive exploration. Previously established methods leverage greedy search method, which can be very time-consuming to conduct successful attack. This also limits the development of adversarial training and potential defenses for categorical data. To tackle this problem, we propose Probabilistic Categorical Adversarial Attack (PCAA), which transfers the discrete optimization problem to a continuous problem that can be solved efficiently by Projected Gradient Descent. In our paper, we theoretically analyze its optimality and time complexity to demonstrate its significant advantage over current greedy based attacks. Moreover, based on our attack, we propose an efficient adversarial training framework. Through a comprehensive empirical study, we justify the effectiveness of our proposed attack and defense algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188067727",
                    "name": "Penghei He"
                },
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "143702207",
                    "name": "J. Ren"
                },
                {
                    "authorId": "2167583580",
                    "name": "Yuxuan Wan"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "b1210de5c29f897e6aa02397d88a2f5eb4e395ff",
            "title": "A Novel Asynchronous Evolution Opinion Dynamics Model",
            "abstract": "Persuasion, which is the act of convincing someone to change their idea, attitude, or opinion, happens anywhere and anytime in social life. In this paper, based on the social judgment theory, we propose a novel asynchronous evolution Deffuant-Weisbuch opinion dynamics model by introducing the latitude of non-commitment. In our model, when agents communicate with each other, the evolution of their opinions depend not only on the opinion of themselves and their neighbors but also on the latitude where they locate, thus an opinion evolution rule that close to reality is designed. The model can well explain the persuasion process and captures the assimilation phenomenon in social networks. Simulation results show that the model can finally reach a stable state. Moreover, network topology, initial distribution of agents\u2019 opinions and the range of latitudes of non-commitment all have an impact on the convergence time of opinions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185883877",
                    "name": "Xiao Xiao"
                },
                {
                    "authorId": "47723354",
                    "name": "Minghua Xu"
                },
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                }
            ]
        }
    ]
}