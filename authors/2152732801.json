{
    "authorId": "2152732801",
    "papers": [
        {
            "paperId": "eaf95f973180e70a970a37be2d8ea68fb2422824",
            "title": "Semi-Supervised Medical Report Generation via Graph-Guided Hybrid Feature Consistency",
            "abstract": "Medical report generation generates the corresponding report according to the given radiology image, which has been attracting increasing research interest. However, existing methods mainly adopt supervised training which rely on large amount of medical reports that are actually unavailable owing to the labor-intensive labeling process and privacy protection protocol. In the meanwhile, the intrinsic relationships between local pathological changes in the image are often ignored, which actually are important hints to high quality report generation. To this end, we propose a Relation-Aware Mean Teacher (RAMT) framework, which follows a standard mean teacher paradigm for semi-supervised report generation. The key to the encoder of the backbone network is the Graph-guided Hybrid Feature Encoding (GHFE) module, which exploits a prior disease knowledge graph to encode the intrinsic relations between pathological changes into the graph embedding and learns a word dictionary to retrieve the semantic embedding for each potential pathological change. GHFE combines the graph embedding, semantic embedding and visual features to form hybrid features, which are sent to a Transformer-based decoder for report generation. Extensive experiments on the MIMIC-CXR and IU X-Ray datasets demonstrate the effectiveness of our proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152645158",
                    "name": "Kecheng Zhang"
                },
                {
                    "authorId": "28955559",
                    "name": "Hanliang Jiang"
                },
                {
                    "authorId": "2151811598",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "1689702",
                    "name": "Qingming Huang"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "2153201566",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2114925498",
                    "name": "Weidong Han"
                }
            ]
        },
        {
            "paperId": "26301b4bfa930c23cdb969f08c0e5c8f38695437",
            "title": "Learnable Query Guided Representation Learning for Treatment Effect Estimation",
            "abstract": "The estimation of Individual Treatment Effect (ITE) is a challenging problem in causal inference, due to the missing counterfactual data and the selection bias. In this paper, we propose a novel representation learning framework via the Learnable Query based transformer for Treatment Effect Estimation (LQTEE). A certain number of queries are learned in a sample-agnostic way and extract global critical features from covariate and treatment data separately. We also propose the hierarchical propensity score regularized adversarial loss to obtain balanced covariate representations, and the mutual orthogonal constraint to force queries to focus on diverse parts of covariates, thus the impact of instrumental variables can be adaptively reduced. Treatment representation learning enables our estimator to support general-purpose treatments, and more importantly, it can reveal the underlying patterns of data-generation process efficiently. Extensive experiments show that our ITE estimator significantly outperforms the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2027157056",
                    "name": "Chenchen Fan"
                },
                {
                    "authorId": "2108735515",
                    "name": "Yixin Wang"
                },
                {
                    "authorId": "2108371796",
                    "name": "Yahong Zhang"
                },
                {
                    "authorId": "2066307747",
                    "name": "Wenli Ouyang"
                },
                {
                    "authorId": "2072684355",
                    "name": "Sheng Shi"
                },
                {
                    "authorId": "2088400146",
                    "name": "Wei Fan"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                }
            ]
        },
        {
            "paperId": "2b5b25252c4a738c53a94ba3bb48335f1eae9555",
            "title": "Interactive Two-Stream Network Across Modalities for Deepfake Detection",
            "abstract": "As face forgery techniques have become more mature, the proliferation of deepfakes may threaten the security of human society. Although existing deepfake detection methods achieve good performance for in-dataset evaluation, it remains to be improved in the generalization ability, where the representation of the imperceptible artifacts plays a significant role. In this paper, we propose an Interactive Two-Stream Network (ITSNet) to explore the discriminant inconsistency representation from the perspective of cross-modality. In particular, the patch-wise Decomposable Discrete Cosine Transform (DDCT) is adopted to extract fine-grained high-frequency clues, and information from different modalities communicates with each other via a designed interaction module. To perceive the temporal inconsistency, we first develop a Short-term Embedding Module (SEM) to refine subtle local inconsistency representation between adjacent frames, and then a Long-term Embedding Module (LEM) is designed to further refine the erratic temporal inconsistency representation from the long-range perspective. Extensive experimental results conducted on three public datasets show that ITSNet outperforms the state-of-the-art methods both in terms of in-dataset and cross-dataset evaluations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Jianghao Wu"
                },
                {
                    "authorId": "2388768",
                    "name": "Baopeng Zhang"
                },
                {
                    "authorId": "2156264943",
                    "name": "Zhaoyang Li"
                },
                {
                    "authorId": "35708966",
                    "name": "Guilin Pang"
                },
                {
                    "authorId": "1943020",
                    "name": "Zhu Teng"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                }
            ]
        },
        {
            "paperId": "2d0eb02a551cc25e59d85a40c0ec79f7cd563ad1",
            "title": "Cross-domain recommendation via user interest alignment",
            "abstract": "Cross-domain recommendation aims to leverage knowledge from multiple domains to alleviate the data sparsity and cold-start problems in traditional recommender systems. One popular paradigm is to employ overlapping user representations to establish domain connections, thereby improving recommendation performance in all scenarios. Nevertheless, the general practice of this approach is to train user embeddings in each domain separately and then aggregate them in a plain manner, often ignoring potential cross-domain similarities between users and items. Furthermore, considering that their training objective is recommendation task-oriented without specific regularizations, the optimized embeddings disregard the interest alignment among user\u2019s views, and even violate the user\u2019s original interest distribution. To address these challenges, we propose a novel cross-domain recommendation framework, namely COAST, to improve recommendation performance on dual domains by perceiving the cross-domain similarity between entities and aligning user interests. Specifically, we first construct a unified cross-domain heterogeneous graph and redefine the message passing mechanism of graph convolutional networks to capture high-order similarity of users and items across domains. Targeted at user interest alignment, we develop deep insights from two more fine-grained perspectives of user-user and user-item interest invariance across domains by virtue of affluent unsupervised and semantic signals. We conduct intensive experiments on multiple tasks, constructed from two large recommendation data sets. Extensive results show COAST consistently and significantly outperforms state-of-the-art cross-domain recommendation algorithms as well as classic single-domain recommendation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1409984160",
                    "name": "Chuang Zhao"
                },
                {
                    "authorId": "2869628",
                    "name": "Hongke Zhao"
                },
                {
                    "authorId": "2116974078",
                    "name": "Ming He"
                },
                {
                    "authorId": "2185160730",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                }
            ]
        },
        {
            "paperId": "3699bf4bdd629b7e46ccc6a685bdb7c3a2e343be",
            "title": "Deep Robust Low Rank Correlation With Unifying Clustering Structure for Cross Domain Adaptation",
            "abstract": "Cross domain adaptation aims to improve the performance of the target domain model by making full use of information rich source domain samples. However, as information becomes richer, the noise also increases. In order to improve the reliability of cross domain adaptation, we propose a novel method based on deep robust low rank correlation. Borrowed from the traditional idea of Canonical Correlation Analysis (CCA), we developed a robust correlation model to maximize the correlation between source and target domains. Also, the low-rank characteristics of cross domain data can effectively reduce the negative influence of noisy data. Furthermore, in order that the cross-domain data can share a unifying clustering structure, we introduced a common Laplacian affinity structure. Then the learned features can be smoothed and aligned to the unifying structure. In this way, we obtain a deep robust low rank correlation model with the help of the unifying clustering structure, which can effectively reduce the influence of noise and improve the performance of cross domain adaptation. Experimental results on three datasets including Office-31, ImageCLEF-DA and Office-Home show that our model significantly outperforms state-of-the-art cross domain adaptation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1435353298",
                    "name": "Xiang-Jun Shen"
                },
                {
                    "authorId": "2200209051",
                    "name": "Yanan Cai"
                },
                {
                    "authorId": "2051973969",
                    "name": "Stanley Ebhohimhen Abhadiomhen"
                },
                {
                    "authorId": "2128696107",
                    "name": "Zhi-Feng Liu"
                },
                {
                    "authorId": "20342486",
                    "name": "Y. Zhan"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                }
            ]
        },
        {
            "paperId": "6341ea1ccf40238660249ebde5ae7c1605a5e0d7",
            "title": "VS-Boost: Boosting Visual-Semantic Association for Generalized Zero-Shot Learning",
            "abstract": "Unlike conventional zero-shot learning (CZSL) which only focuses on the recognition of unseen classes by using the classifier trained on seen classes and semantic embeddings, generalized zero-shot learning (GZSL) aims at recognizing both the seen and unseen classes, so it is more challenging due to the extreme training imbalance. Recently, some feature generation methods introduce metric learning to enhance the discriminability of visual features. Although these methods achieve good results, they focus only on metric learning in the visual feature space to enhance features and ignore the association between the feature space and the semantic space. Since the GZSL method uses semantics as prior knowledge to migrate visual knowledge to unseen classes, the consistency between visual space and semantic space is critical. To this end, we propose relational metric learning which can relate the metrics in the two spaces and make the distribution of the two spaces more consistent. Based on the generation method and relational metric learning, we proposed a novel GZSL method, termed VS-Boost, which can effectively boost the association between vision and semantics. The experimental results demonstrate that our method is effective and achieves significant gains on five benchmark datasets compared with the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118013535",
                    "name": "Xiaofan Li"
                },
                {
                    "authorId": "2108122694",
                    "name": "Yachao Zhang"
                },
                {
                    "authorId": "2228934919",
                    "name": "Shiran Bian"
                },
                {
                    "authorId": "1696146",
                    "name": "Yanyun Qu"
                },
                {
                    "authorId": "2118596087",
                    "name": "Yuan Xie"
                },
                {
                    "authorId": "2558130",
                    "name": "Zhongchao Shi"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                }
            ]
        },
        {
            "paperId": "a1975784784db088ec5125b488e9d5374fdef57a",
            "title": "ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos",
            "abstract": "Building benchmarks to systemically analyze different capabilities of video question answering (VideoQA) models is challenging yet crucial. Existing benchmarks often use non-compositional simple questions and suffer from language biases, making it difficult to diagnose model weaknesses incisively. A recent benchmark AGQA [8] poses a promising paradigm to generate QA pairs automatically from pre-annotated scene graphs, enabling it to measure diverse reasoning abilities with granular control. However, its questions have limitations in reasoning about the fine-grained semantics in videos as such information is absent in its scene graphs. To this end, we present ANetQA, a large-scale benchmark that supports fine-grained compositional reasoning over the challenging untrimmed videos from ActivityNet [4]. Similar to AGQA, the QA pairs in ANetQA are automatically generated from annotated video scene graphs. The fine-grained properties of ANetQA are reflected in the following: (i) untrimmed videos with fine-grained semantics; (ii) spatio-temporal scene graphs with fine-grained taxonomies; and (iii) diverse questions generated from fine-grained templates. ANetQA attains 1.4 billion unbalanced and 13.4 million balanced QA pairs, which is an order of magnitude larger than AGQA with a similar number of videos. Comprehensive experiments are performed for state-of-the-art methods. The best model achieves 44.5% accuracy while human performance tops out at 84.5%, leaving sufficient room for improvement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2216492785",
                    "name": "Lixiang Zheng"
                },
                {
                    "authorId": "47122432",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2110922423",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                }
            ]
        },
        {
            "paperId": "ad2ad450f1ee6a0df46bc6fe6916a797c90b68f1",
            "title": "Multi-Task Paired Masking With Alignment Modeling for Medical Vision-Language Pre-Training",
            "abstract": "In recent years, the growing demand for medical imaging diagnosis has placed a significant burden on radiologists. As a solution, Medical Vision-Language Pre-training (Med-VLP) methods have been proposed to learn universal representations from medical images and reports, benefiting downstream tasks without requiring fine-grained annotations. However, existing methods have overlooked the importance of cross-modal alignment in joint image-text reconstruction, resulting in insufficient cross-modal interaction. To address this limitation, we propose a unified Med-VLP framework based on Multi-task Paired Masking with Alignment (MPMA) to integrate the cross-modal alignment task into the joint image-text reconstruction framework to achieve more comprehensive cross-modal interaction, while a Global and Local Alignment (GLA) module is designed to assist self-supervised paradigm in obtaining semantic representations with rich domain knowledge. Furthermore, we introduce a Memory-Augmented Cross-Modal Fusion (MA-CMF) module to fully integrate visual information to assist report reconstruction and fuse the multi-modal representations adequately. Experimental results demonstrate that the proposed unified approach outperforms previous methods in all downstream tasks, including uni-modal, cross-modal, and multi-modal tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152645158",
                    "name": "Kecheng Zhang"
                },
                {
                    "authorId": "2524165",
                    "name": "Han Jiang"
                },
                {
                    "authorId": "47539632",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2111526678",
                    "name": "Qing-An Huang"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2114925498",
                    "name": "Weidong Han"
                }
            ]
        },
        {
            "paperId": "bb7fccda75875cbc25628dfe2e5d76b0ff4bfd08",
            "title": "MRE-Net: Multi-Rate Excitation Network for Deepfake Video Detection",
            "abstract": "The current social media is flooded with hyper realistic face-synthetic videos due to the explosion of DeepFake technology that has brought a serious impact on human society security, which calls for further exploring on deepfake video detection methods. Existing methods attempt to isolated capture spatial artifacts or extract the homogeneous temporal inconsistency to detect deepfake video, but little attention has been paid to the exploitation of dynamic spatial-temporal inconsistency. To mitigate this issue, in this paper, we propose a novel Multi-Rate Excitation Network (MRE-Net) to effectively excite dynamic spatial-temporal inconsistency from the perspective of multiple rates for deepfake video detection. The proposed MRE-Net is composed of two components: Bipartite Group Sampling (BGS) and multiple rate branches. The BGS draws the entire video into multiple bipartite groups with different rates to cover various face motion dynamic evolution. We further design multiple rate branches to capture both short-term and long-term spatial-temporal inconsistency from corresponding bipartite groups of BGS. Concretely, for the early stages of the multi-rate branches, Momentary Inconsistency Excitation (MIE) module is developed to encode the spatial artifacts and intra-group short-term temporal inconsistency. Meanwhile, for the last stages of the multi-rate branches, Longstanding Inconsistency Excitation (LIE) module is constructed to perceive inter-group long-term temporal dynamics. Extensive experiments and visualizations conducted on four popular datasets demonstrate the effectiveness of the proposed method against state-of-the-art deepfake detection methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35708966",
                    "name": "Guilin Pang"
                },
                {
                    "authorId": "2388768",
                    "name": "Baopeng Zhang"
                },
                {
                    "authorId": "1943020",
                    "name": "Zhu Teng"
                },
                {
                    "authorId": "2202963283",
                    "name": "Zige Qi"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                }
            ]
        },
        {
            "paperId": "c52b0022625518f8db0dedbfe452226efe23d41c",
            "title": "Social Image\u2013Text Sentiment Classification With Cross-Modal Consistency and Knowledge Distillation",
            "abstract": "Social media sentiment analysis, which aims to evaluate the attitudes of online users based on their posts, has attracted significant research attention due to its successful application in the field of social media monitoring. It is a beneficial way to utilize multimodal information uploaded by users in order to improve sentiment classification ability. However, existing multimodal fusion-based approaches continue to face difficulties due to the issues of between-modality semantic inconsistency and missing modality. To address these issues, we propose a cross-modal consistency modeling-based knowledge distillation framework for image\u2013text sentiment classification of social media data. Specifically, we design a hybrid curriculum learning strategy to measure the semantic consistency of multimodal data, then gradually train all image\u2013text pairs from easy to hard, which can effectively handle the massive amounts of noise caused by inconsistencies between image and text data on social media. Moreover, in order to alleviate the problem of missing images in unimodal posts, we propose a privileged feature distillation method, in which the teacher model additionally considers images as privileged features, to transfer the visual knowledge to the student model, thereby enhancing the accuracy for text sentiment classification. Extensive experiments conducted over three real-world social media datasets demonstrate the effectiveness and superiority of the proposed multimodal sentiment analysis model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146397492",
                    "name": "Huan Liu"
                },
                {
                    "authorId": "2190502262",
                    "name": "Ke Li"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "152299623",
                    "name": "Caixia Yan"
                },
                {
                    "authorId": "82620854",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "2175683028",
                    "name": "Qinghua Zheng"
                }
            ]
        }
    ]
}