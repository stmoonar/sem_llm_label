{
    "authorId": "4574975",
    "papers": [
        {
            "paperId": "2339ad488b6a39734d5f877557d44ba9ee3577bf",
            "title": "Triplet Knowledge Distillation",
            "abstract": "In Knowledge Distillation, the teacher is generally much larger than the student, making the solution of the teacher likely to be difficult for the student to learn. To ease the mimicking difficulty, we introduce a triplet knowledge distillation mechanism named TriKD. Besides teacher and student, TriKD employs a third role called anchor model. Before distillation begins, the pre-trained anchor model delimits a subspace within the full solution space of the target problem. Solutions within the subspace are expected to be easy targets that the student could mimic well. Distillation then begins in an online manner, and the teacher is only allowed to express solutions within the aforementioned subspace. Surprisingly, benefiting from accurate but easy-to-mimic hints, the student can finally perform well. After the student is well trained, it can be used as the new anchor for new students, forming a curriculum learning strategy. Our experiments on image classification and face recognition with various models clearly demonstrate the effectiveness of our method. Furthermore, the proposed TriKD is also effective in dealing with the overfitting issue. Moreover, our theoretical analysis supports the rationality of our triplet distillation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108203610",
                    "name": "Xijun Wang"
                },
                {
                    "authorId": "2152507966",
                    "name": "Dongyang Liu"
                },
                {
                    "authorId": "1693589",
                    "name": "Meina Kan"
                },
                {
                    "authorId": "2118643247",
                    "name": "Chunrui Han"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "145455919",
                    "name": "S. Shan"
                }
            ]
        },
        {
            "paperId": "12d2008b5aaea345c97b328ed350aa8d84120e19",
            "title": "A Character-Level Span-Based Model for Mandarin Prosodic Structure Prediction",
            "abstract": "The accuracy of prosodic structure prediction is crucial to the naturalness of synthesized speech in Mandarin text-to-speech system, but now is limited by widely-used sequence-to-sequence framework and error accumulation from previous word segmentation results. In this paper, we propose a span-based Mandarin prosodic structure prediction model to obtain an optimal prosodic structure tree, which can be converted to corresponding prosodic label sequence. Instead of the prerequisite for word segmentation, rich linguistic features are provided by Chinese character-level BERT and sent to encoder with self-attention architecture. On top of this, span representation and label scoring are used to describe all possible prosodic structure trees, of which each tree has its corresponding score. To find the optimal tree with the highest score for a given sentence, a bottom-up CKYstyle algorithm is further used. The proposed method can predict prosodic labels of different levels at the same time and accomplish the process directly from Chinese characters in an end-to-end manner. Experiment results on two real-world datasets demonstrate the excellent performance of our span-based method over all sequence-to-sequence baseline approaches.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2145448676",
                    "name": "Xueyuan Chen"
                },
                {
                    "authorId": "151386148",
                    "name": "Chang Song"
                },
                {
                    "authorId": "2031124916",
                    "name": "Yixuan Zhou"
                },
                {
                    "authorId": "50061551",
                    "name": "Zhiyong Wu"
                },
                {
                    "authorId": "2118429091",
                    "name": "Changbin Chen"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "145199941",
                    "name": "H. Meng"
                }
            ]
        },
        {
            "paperId": "147f6253e8c99c815645d8858e2097444b62246b",
            "title": "BERT-LID: Leveraging BERT to Improve Spoken Language Identification",
            "abstract": "Language identification is the task of automatically determining the identity of a language conveyed by a spoken segment. It has a profound impact on the multilingual interoperability of an intelligent speech system. Despite language identification attaining high accuracy on medium or long utterances (>3 s), the performance on short utterances (<=1s) is still far from satisfactory. We propose a BERT-based language identification system (BERT-LID) to improve language identification performance, especially on short-duration speech segments. We extend the original BERT model by taking the phonetic posteriorgrams (PPG) derived from the front-end phone recognizer as input. Then we deployed the optimal deep classifier followed by it for language identification. Our BERT-LID model can improve the baseline accuracy by about 6.5% on long-segment identification and 19.9% on short-segment identification, demonstrating our BERT-LID\u2019s effectiveness to language identification.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "48389592",
                    "name": "Yuting Nie"
                },
                {
                    "authorId": "2145805654",
                    "name": "Junhong Zhao"
                },
                {
                    "authorId": "50549927",
                    "name": "Weiqiang Zhang"
                },
                {
                    "authorId": "2113830377",
                    "name": "Jinfeng Bai"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                }
            ]
        },
        {
            "paperId": "618a9f1380dd2b1952577a02cc8bbc8f8bedc4f0",
            "title": "Syntax-Aware Network for Handwritten Mathematical Expression Recognition",
            "abstract": "Handwritten mathematical expression recognition (HMER) is a challenging task that has many potential applications. Recent methods for HMER have achieved outstanding performance with an encoder-decoder architecture. However, these methods adhere to the paradigm that the prediction is made \u201cfrom one character to another\u201d, which inevitably yields prediction errors due to the complicated structures of mathematical expressions or crabbed handwritings. In this paper, we propose a simple and efficient method for HMER, which is the first to incorporate syntax information into an encoder-decoder network. Specifically, we present a set of grammar rules for converting the LaTeX markup sequence of each expression into a parsing tree; then, we model the markup sequence prediction as a tree traverse process with a deep neural network. In this way, the proposed method can effectively describe the syntax context of expressions, alleviating the structure prediction errors of HMER. Experiments on three benchmark datasets demonstrate that our method achieves better recognition performance than prior arts. To further validate the effectiveness of our method, we create a large-scale dataset consisting of 100k handwritten mathematical expression images acquired from ten thousand writers. The source code, new dataset\u2020\u2020https://ai.100tal.com/dataset, and pre-trained models of this work will be publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144047989",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "2115591290",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2157044776",
                    "name": "Wondimu Dikubab"
                },
                {
                    "authorId": "2146673299",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "15383651",
                    "name": "Zhilong Ji"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "145905113",
                    "name": "X. Bai"
                }
            ]
        },
        {
            "paperId": "72936878421d5a695323f51c739fff330c06cbf3",
            "title": "FusionFormer: Fusing Operations in Transformer for Efficient Streaming Speech Recognition",
            "abstract": "The recently proposed Conformer architecture which combines convolution with attention to capture both local and global dependencies has become the \\textit{de facto} backbone model for Automatic Speech Recognition~(ASR). Inherited from the Natural Language Processing (NLP) tasks, the architecture takes Layer Normalization~(LN) as a default normalization technique. However, through a series of systematic studies, we find that LN might take 10\\% of the inference time despite that it only contributes to 0.1\\% of the FLOPs. This motivates us to replace LN with other normalization techniques, e.g., Batch Normalization~(BN), to speed up inference with the help of operator fusion methods and the avoidance of calculating the mean and variance statistics during inference. After examining several plain attempts which directly remove all LN layers or replace them with BN in the same place, we find that the divergence issue is mainly caused by the unstable layer output. We therefore propose to append a BN layer to each linear or convolution layer where stabilized training results are observed. We also propose to simplify the activations in Conformer, such as Swish and GLU, by replacing them with ReLU. All these exchanged modules can be fused into the weights of the adjacent linear/convolution layers and hence have zero inference cost. Therefore, we name it FusionFormer. Our experiments indicate that FusionFormer is as effective as the LN-based Conformer and is about 10\\% faster.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "153928324",
                    "name": "Xingcheng Song"
                },
                {
                    "authorId": "2118289574",
                    "name": "Di Wu"
                },
                {
                    "authorId": "2107885069",
                    "name": "Binbin Zhang"
                },
                {
                    "authorId": "50061551",
                    "name": "Zhiyong Wu"
                },
                {
                    "authorId": "2108700974",
                    "name": "Wenpeng Li"
                },
                {
                    "authorId": "2142728488",
                    "name": "Dongfang Li"
                },
                {
                    "authorId": "2151330414",
                    "name": "Peng Zhang"
                },
                {
                    "authorId": "2106725555",
                    "name": "Zhendong Peng"
                },
                {
                    "authorId": "39917736",
                    "name": "Fuping Pan"
                },
                {
                    "authorId": "2118508996",
                    "name": "Changbao Zhu"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                }
            ]
        },
        {
            "paperId": "c85f67a49cfdc29a77bd741a59e739cdc814edde",
            "title": "All-In-One Image Restoration for Unknown Corruption",
            "abstract": "In this paper, we study a challenging problem in image restoration, namely, how to develop an all-in-one method that could recover images from a variety of unknown corruption types and levels. To this end, we propose an All-in-one Image Restoration Network (AirNet) consisting of two neural modules, named Contrastive-Based Degraded Encoder (CBDE) and Degradation-Guided Restoration Network (DGRN). The major advantages of AirNet are two-fold. First, it is an all-in-one solution which could recover various degraded images in one network. Second, AirNet is free from the prior of the corruption types and levels, which just uses the observed corrupted image to perform inference. These two advantages enable AirNet to enjoy better flexibility and higher economy in real world scenarios wherein the priors on the corruptions are hard to know and the degradation will change with space and time. Extensive experimental results show the proposed method outperforms 17 image restoration baselines on four challenging datasets. The code is available at https://github.com/XLearning-SCU/2022-CVPR-AirNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150044582",
                    "name": "Boyun Li"
                },
                {
                    "authorId": "2115591290",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "143941721",
                    "name": "Peng Hu"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2075420316",
                    "name": "Jiancheng Lv"
                },
                {
                    "authorId": "9245535",
                    "name": "Xiaocui Peng"
                }
            ]
        },
        {
            "paperId": "e23ac77481ce5285fe0f41b523a47bdbc7592a59",
            "title": "Time-Domain Audio-Visual Speech Separation on Low Quality Videos",
            "abstract": "Incorporating visual information is a promising approach to improve the performance of speech separation. Many related works have been conducted and provide inspiring results. However, low quality videos appear commonly in real scenarios, which may significantly degrade the performance of normal audio-visual speech separation system. In this paper, we propose a new structure to fuse the audio and visual features, which uses the audio feature to select relevant visual features by utilizing the attention mechanism. A Conv-TasNet based model is combined with the proposed attention-based multi-modal fusion, trained with proper data augmentation and evaluated with 3 categories of low quality videos. The experimental results show that our system outperforms the baseline which simply concatenates the audio and visual features when training with normal or low quality data, and is robust to low quality video inputs at inference time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135750715",
                    "name": "Yifei Wu"
                },
                {
                    "authorId": "1391213316",
                    "name": "Chenda Li"
                },
                {
                    "authorId": "2113830377",
                    "name": "Jinfeng Bai"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2480051",
                    "name": "Y. Qian"
                }
            ]
        },
        {
            "paperId": "09e64a6d4aa96aca0f59815a34fd67b24c22c94a",
            "title": "CrowdRL: An End-to-End Reinforcement Learning Framework for Data Labelling",
            "abstract": "Data labelling is very important in many database and machine learning applications. Traditional methods rely on humans (workers or experts) to acquire labels. However, the human cost is rather expensive for a large dataset. Active learning based methods only label a small set of data with large uncertainty, train a model on these labelled data, and use the trained model to label the remainder unlabelled data. However they have two limitations. First, they cannot judiciously select appropriate data (task selection) and assign the tasks to proper humans (task assignment). Moreover, they independently process task selection and task assignment, which cannot capture the correlation between them. Second, they simply infer the truth of a task based on the answers from humans and the trained model (truth inference) by independently modeling humans and models. In other words, they ignore the correlation between them (the labelled data may have noise caused by humans with biases, and the model trained by the noisy labels may bring additional biases), and thus lead to poor inference results.To address these limitations, in this paper, we propose CrowdRL, an end-to-end reinforcement learning (RL) based framework for data labelling. To the best of our knowledge, CrowdRL is the first RL framework designed for the data labelling workflow by seamlessly integrating task selection, task assignment and truth inference together. CrowdRL fully utilizes the power of heterogeneous annotators (experts and crowdsourcing workers) and machine learning models together to infer the truth, which highly improves the quality of data labelling. CrowdRL uses RL to model task assignment and task selection, and designs an agent to judiciously assign tasks to appropriate workers. CrowdRL jointly models the answers of workers, experts and models, and designs a joint inference model to infer the truths. Experimental results on real datasets show that CrowdRL outperforms state-of-the-art approaches with the same (even fewer) monetary cost while achieving 5%-20% higher accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087802",
                    "name": "Kaiyu Li"
                },
                {
                    "authorId": "47949377",
                    "name": "Guoliang Li"
                },
                {
                    "authorId": "2153954005",
                    "name": "Yong Wang"
                },
                {
                    "authorId": "39937384",
                    "name": "Yan Huang"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                }
            ]
        },
        {
            "paperId": "1eeaeb261ea6b65e8500033feacb660e5da98f6d",
            "title": "Audio-Visual Multi-Talker Speech Recognition in a Cocktail Party",
            "abstract": "Speech from microphones is vulnerable in a complex acoustic environment due to noise and reverberation, while the cameras are not. Thus, utilizing the visual modality in the \u201ccock-tail party\u201d scenario with multi-talkers has become a promising and popular approach. In this paper, we have explored the incorporating of visual modality into the end-to-end multi-talker speech recognition task. We propose two methods based on the modality fusion position, which are encoder-based fusion and decoder-based fusion. And for each method, advanced audio-visual fusion techniques including attention mechanism and dual decoder have been explored to \ufb01nd the best usage of the visual modality. With the proposed methods, our best audio-visual multi-talker automatic speech recognition (ASR) model gets almost \u223c 50.0% word error rate (WER) reduction compared to the audio-only multi-talker ASR system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135750715",
                    "name": "Yifei Wu"
                },
                {
                    "authorId": "1391213316",
                    "name": "Chenda Li"
                },
                {
                    "authorId": "2109000452",
                    "name": "Song Yang"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2480051",
                    "name": "Y. Qian"
                }
            ]
        },
        {
            "paperId": "21cdc5c6a1d1af42bb0e34872496767ecab7aead",
            "title": "Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval",
            "abstract": "The current state-of-the-art image-sentence retrieval methods implicitly align the visual-textual fragments, like regions in images and words in sentences, and adopt attention modules to highlight the relevance of cross-modal semantic correspondences. However, the retrieval performance remains unsatisfactory due to a lack of consistent representation in both semantics and structural spaces. In this work, we propose to address the above issue from two aspects: (i) constructing intrinsic structure (along with relations) among the fragments of respective modalities, e.g., \"dog \u2192 play \u2192 ball\" in semantic structure for an image, and (ii) seeking explicit inter-modal structural and semantic correspondence between the visual and textual modalities. In this paper, we propose a novel Structured Multi-modal Feature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In order to jointly and explicitly learn the visual-textual embedding and the cross-modal alignment, SMFEA creates a novel multi-modal structured module with a shared context-aware referral tree. In particular, the relations of the visual and textual fragments are modeled by constructing Visual Context-aware Structured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree encoder (TCS-Tree) with shared labels, from which visual and textual features can be jointly learned and optimized. We utilize the multi-modal tree structure to explicitly align the heterogeneous image-sentence data by maximizing the semantic and structural similarity between corresponding inter-modal tree nodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks demonstrate the superiority of the proposed model in comparison to the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1380224383",
                    "name": "Xuri Ge"
                },
                {
                    "authorId": "2642638",
                    "name": "Fuhai Chen"
                },
                {
                    "authorId": "50686770",
                    "name": "J. Jose"
                },
                {
                    "authorId": "15383651",
                    "name": "Zhilong Ji"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2115591290",
                    "name": "Xiao Liu"
                }
            ]
        }
    ]
}