{
    "authorId": "49889218",
    "papers": [
        {
            "paperId": "28bb68c387909f94588ac546738a4298888267ab",
            "title": "Forcing Diffuse Distributions out of Language Models",
            "abstract": "Despite being trained specifically to follow user instructions, today's instructiontuned language models perform poorly when instructed to produce random outputs. For example, when prompted to pick a number uniformly between one and ten Llama-2-13B-chat disproportionately favors the number five, and when tasked with picking a first name at random, Mistral-7B-Instruct chooses Avery 40 times more often than we would expect based on the U.S. population. When these language models are used for real-world tasks where diversity of outputs is crucial, such as language model assisted dataset construction, their inability to produce diffuse distributions over valid choices is a major hurdle. In this work, we propose a fine-tuning method that encourages language models to output distributions that are diffuse over valid outcomes. The methods we introduce generalize across a variety of tasks and distributions and make large language models practical for synthetic dataset generation with little human intervention.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "102604362",
                    "name": "Avi Schwarzschild"
                },
                {
                    "authorId": "2292200990",
                    "name": "Nicholas Carlini"
                },
                {
                    "authorId": "117539586",
                    "name": "Zico Kolter"
                },
                {
                    "authorId": "7975935",
                    "name": "Daphne Ippolito"
                }
            ]
        },
        {
            "paperId": "5316598d39df1bf47240eb4a8dc0f4770ec3fa72",
            "title": "OpenHEXAI: An Open-Source Framework for Human-Centered Evaluation of Explainable Machine Learning",
            "abstract": "Recently, there has been a surge of explainable AI (XAI) methods driven by the need for understanding machine learning model behaviors in high-stakes scenarios. However, properly evaluating the effectiveness of the XAI methods inevitably requires the involvement of human subjects, and conducting human-centered benchmarks is challenging in a number of ways: designing and implementing user studies is complex; numerous design choices in the design space of user study lead to problems of reproducibility; and running user studies can be challenging and even daunting for machine learning researchers. To address these challenges, this paper presents OpenHEXAI, an open-source framework for human-centered evaluation of XAI methods. OpenHEXAI features (1) a collection of diverse benchmark datasets, pre-trained models, and post hoc explanation methods; (2) an easy-to-use web application for user study; (3) comprehensive evaluation metrics for the effectiveness of post hoc explanation methods in the context of human-AI decision making tasks; (4) best practice recommendations of experiment documentation; and (5) convenient tools for power analysis and cost estimation. OpenHEAXI is the first large-scale infrastructural effort to facilitate human-centered benchmarks of XAI methods. It simplifies the design and implementation of user studies for XAI methods, thus allowing researchers and practitioners to focus on the scientific questions. Additionally, it enhances reproducibility through standardized designs. Based on OpenHEXAI, we further conduct a systematic benchmark of four state-of-the-art post hoc explanation methods and compare their impacts on human-AI decision making tasks in terms of accuracy, fairness, as well as users' trust and understanding of the machine learning model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261475382",
                    "name": "Jiaqi Ma"
                },
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "2131059444",
                    "name": "Chacha Chen"
                },
                {
                    "authorId": "2290718096",
                    "name": "Paul Hamilton"
                },
                {
                    "authorId": "1725415927",
                    "name": "Davor Ljubenkov"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                },
                {
                    "authorId": "2269165090",
                    "name": "Chenhao Tan"
                }
            ]
        },
        {
            "paperId": "12c4b54324f2cfed061a432f5b23660e1b1e0c7e",
            "title": "FLamE: Few-shot Learning from Natural Language Explanations",
            "abstract": "Natural language explanations have the potential to provide rich information that in principle guides model reasoning.Yet, recent work by Lampinen et al. has shown limited utility of natural language explanations in improving classification.To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then fine-tunes a smaller model (e.g., RoBERTa) with generated explanations.Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI.Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions.Additional analyses point to the important role of label-specific cues (e.g., \u201cnot know\u201d for the neutral label) in generated explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1992936194",
                    "name": "Yangqiaoyu Zhou"
                },
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "2111727675",
                    "name": "Chenhao Tan"
                }
            ]
        },
        {
            "paperId": "85a5ffc509fa50c96b415e09ae87fb6e5f435b37",
            "title": "BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases",
            "abstract": "Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "2218424381",
                    "name": "Sravani Nanduri"
                },
                {
                    "authorId": "2112504145",
                    "name": "Liwei Jiang"
                },
                {
                    "authorId": "2116417519",
                    "name": "Tongshuang Wu"
                },
                {
                    "authorId": "2729164",
                    "name": "Maarten Sap"
                }
            ]
        },
        {
            "paperId": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "title": "Effective Prompt Extraction from Language Models",
            "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold on marketplaces. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction from real systems such as Claude 3 and ChatGPT further suggest that system prompts can be revealed by an adversary despite existing defenses in place.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "7975935",
                    "name": "Daphne Ippolito"
                }
            ]
        },
        {
            "paperId": "e2de545750c859b9f7f62919e2b94f9e92ad9147",
            "title": "Selective Explanations: Leveraging Human Input to Align Explainable AI",
            "abstract": "While a vast collection of explainable AI (XAI) algorithms has been developed in recent years, they have been criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective ---a fundamental property of human explanations---by selectively presenting a subset of model reasoning based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small dataset. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three paradigms based on our proposed framework: in Study 1, we ask the participants to provide critique-based or open-ended input to generate selective explanations (self-input). In Study 2, we show the participants selective explanations based on input from a panel of similar users (annotator input). Our experiments demonstrate the promise of selective explanations in reducing over-reliance on AI and improving collaborative decision making and subjective perceptions of the AI system, but also paint a nuanced picture that attributes some of these positive effects to the opportunity to provide one's own input to augment AI explanations. Overall, our work proposes a novel XAI framework inspired by human communication behaviors and demonstrates its potential to encourage future work to make AI explanations more human-compatible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120801533",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "2131059444",
                    "name": "Chacha Chen"
                },
                {
                    "authorId": "144921048",
                    "name": "Q. Liao"
                },
                {
                    "authorId": "2111727675",
                    "name": "Chenhao Tan"
                }
            ]
        },
        {
            "paperId": "b888c632b75631c70bc5323b17fe975f4fc8500e",
            "title": "Learning to Ignore Adversarial Attacks",
            "abstract": "Despite the strong performance of current NLP models, they can be brittle against adversarial attacks. To enable effective learning against adversarial inputs, we introduce the use of rationale models that can explicitly learn to ignore attack tokens. We find that the rationale models can successfully ignore over 90% of attack tokens. This approach leads to consistent sizable improvements (~10%) over baseline models in robustness on three datasets for both BERT and RoBERTa, and also reliably outperforms data augmentation with adversarial examples alone. In many cases, we find that our method is able to close the gap between model performance on a clean test set and an attacked test set and hence reduce the effect of adversarial attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "2150920617",
                    "name": "Yan Zhou"
                },
                {
                    "authorId": "40502796",
                    "name": "Samuel Carton"
                },
                {
                    "authorId": "2111727675",
                    "name": "Chenhao Tan"
                }
            ]
        },
        {
            "paperId": "b8bd29a6104d26a16687400049a4e7e026ae6258",
            "title": "Active Example Selection for In-Context Learning",
            "abstract": "With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "2113511266",
                    "name": "Shi Feng"
                },
                {
                    "authorId": "2111727675",
                    "name": "Chenhao Tan"
                }
            ]
        },
        {
            "paperId": "50718d6bd163967b8353de4c854ed866b2b56c2f",
            "title": "Conversations Gone Alright: Quantifying and Predicting Prosocial Outcomes in Online Conversations",
            "abstract": "Online conversations can go in many directions: some turn out poorly due to antisocial behavior, while others turn out positively to the benefit of all. Research on improving online spaces has focused primarily on detecting and reducing antisocial behavior. Yet we know little about positive outcomes in online conversations and how to increase them\u2014is a prosocial outcome simply the lack of antisocial behavior or something more? Here, we examine how conversational features lead to prosocial outcomes within online discussions. We introduce a series of new theory-inspired metrics to define prosocial outcomes such as mentoring and esteem enhancement. Using a corpus of 26M Reddit conversations, we show that these outcomes can be forecasted from the initial comment of an online conversation, with the best model providing a relative 24% improvement over human forecasting performance at ranking conversations for predicted outcome. Our results indicate that platforms can use these early cues in their algorithmic ranking of early conversations to prioritize better outcomes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2232600967",
                    "name": "Jiajun Bao"
                },
                {
                    "authorId": "46365383",
                    "name": "J. Wu"
                },
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "143833600",
                    "name": "Eshwar Chandrasekharan"
                },
                {
                    "authorId": "3046220",
                    "name": "David Jurgens"
                }
            ]
        },
        {
            "paperId": "60f5c3057b7c92b042dfb847b510a7ecbfd46f53",
            "title": "Building a Flexible Knowledge Graph to Capture Real-World Events",
            "abstract": "Events and situations unfold quickly in our modern world, generating streams of Internet articles, photos, and videos. The ability to automatically sort through this wealth of information would allow us to identify which pieces of information are most important and credible, and how trends unfold over time. In this paper, we present the first piece of a system to sort through large amounts of political data from the web. Our system takes in raw multimodal input (e.g., text, images, and videos), and generates a knowledge graph connecting entities, events, and relations in meaningful ways. This work is part of the DARPA-funded Active Interpretation of Disparate Alternatives (AIDA) project, which aims to automatically build a knowledge base that can be queried to strategically generate hypotheses about different aspects of an event. We are participating in this project as a TA1 team, building the first step of the overall system. Our approach is outlined in Figure 1 and will be discussed in detail in the following sections. The first step of the pipeline is pre-processing, shown in",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491971694",
                    "name": "Laura Burdick"
                },
                {
                    "authorId": "2108927052",
                    "name": "Mingzhe Wang"
                },
                {
                    "authorId": "48408604",
                    "name": "Steven R. Wilson"
                },
                {
                    "authorId": "49889218",
                    "name": "Yiming Zhang"
                }
            ]
        }
    ]
}