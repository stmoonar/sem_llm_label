{
    "authorId": "1743043",
    "papers": [
        {
            "paperId": "2742460c702206fe19c0b5ceae97fef3499bdf76",
            "title": "Similarity of Neural Network Models: A Survey of Functional and Representational Measures",
            "abstract": "Measuring similarity of neural networks to understand and improve their behavior has become an issue of great importance and research interest. In this survey, we provide a comprehensive overview of two complementary perspectives of measuring neural network similarity: (i) representational similarity, which considers how activations of intermediate layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties of and relationships between these measures, and point to open research problems. We hope our work lays a foundation for more systematic research on the properties and applicability of similarity measures for neural network models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1707027340",
                    "name": "Max Klabunde"
                },
                {
                    "authorId": "39124179",
                    "name": "Tobias Schumacher"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                },
                {
                    "authorId": "2101037",
                    "name": "Florian Lemmerich"
                }
            ]
        },
        {
            "paperId": "82830a0ece0d4cf8ab6e0ad36a17fb5f06bc3db0",
            "title": "CAAW\u201923: The 2nd International Cryptoasset Analytics Workshop",
            "abstract": "This half-day workshop on Cryptoasset Analytics allows researchers from different disciplines to present their newest findings related to cryptoassets. This workshop is relevant for the Web research community for two reasons. First, on a technical level, fundamental concepts of cryptoassets are increasingly integrated with Web technologies. Second, we witness the formation of socio-technical cryptoasset ecosystems, which are tightly connected to the Web. The program will feature a mix of invited talks and a selection of peer-reviewed submissions. Topics range from empirical studies, over analytics methods and tools, to case studies, datasets, and cross-cutting issues like legal or ethical aspects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1679379",
                    "name": "Bernhard Haslhofer"
                },
                {
                    "authorId": "51050025",
                    "name": "Friedhelm Victor"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                },
                {
                    "authorId": "46372373",
                    "name": "Jiahua Xu"
                }
            ]
        },
        {
            "paperId": "92d67bc7d0bc7a9de9ac2a0cce12c410005ead31",
            "title": "Checking Conformance to a Subset of the Python Language",
            "abstract": "Introductory courses usually only teach a small subset of a programming language and its library, in order to focus on the general concepts rather than overwhelm students with the syntactic, semantic and API minutiae of a particular language. This paper presents courseware that checks if a program only uses the subset of the Python language and library defined by the instructor. This allows to automatically check that programming examples, exercises and assessments only use the taught constructs. It also helps detect student code with advanced constructs, possibly copied from Q&A sites or generated by large language models. The tool is easy to install, configure and use. It also checks Python code in Jupyter notebooks, a popular format for interactive textbooks and assessment handouts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1770749",
                    "name": "M. Wermelinger"
                },
                {
                    "authorId": "1718749",
                    "name": "Y. Yu"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                }
            ]
        },
        {
            "paperId": "a11bbf3bf84db7a809d9523c1c1d1166f5abdb60",
            "title": "SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings",
            "abstract": "Adding interpretability to word embeddings represents an area of active research in text representation. Recent work has explored thepotential of embedding words via so-called polar dimensions (e.g. good vs. bad, correct vs. wrong). Examples of such recent approaches include SemAxis, POLAR, FrameAxis, and BiImp. Although these approaches provide interpretable dimensions for words, they have not been designed to deal with polysemy, i.e. they can not easily distinguish between different senses of words. To address this limitation, we present SensePOLAR, an extension of the original POLAR framework that enables word-sense aware interpretability for pre-trained contextual word embeddings. The resulting interpretable word embeddings achieve a level of performance that is comparable to original contextual word embeddings across a variety of natural language processing tasks including the GLUE and SQuAD benchmarks. Our work removes a fundamental limitation of existing approaches by offering users sense aware interpretations for contextual word embeddings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2073883062",
                    "name": "Jan Engler"
                },
                {
                    "authorId": "2632448",
                    "name": "Sandipan Sikdar"
                },
                {
                    "authorId": "2159268668",
                    "name": "Marlene Lutz"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                }
            ]
        },
        {
            "paperId": "32865330ade0a9a976e1c7f24fa6283c98d0d2b1",
            "title": "Characterizing the country-wide adoption and evolution of the Jodel messaging app in Saudi Arabia",
            "abstract": "Social media is subject to constant growth and evolution, yet little is known about their early phases of adoption. To shed light on this aspect, this paper empirically characterizes the initial and country-wide adoption of a new type of social media in Saudi Arabia that happened in 2017. Unlike established social media, the studied network Jodel is anonymous and location-based to form hundreds of independent communities country-wide whose adoption pattern we compare. We take a detailed and full view from the operators perspective on the temporal and geographical dimension on the evolution of these different communities -- from their very first the first months of establishment to saturation. This way, we make the early adoption of a new type of social media visible, a process that is often invisible due to the lack of data covering the first days of a new network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35455751",
                    "name": "J. Reelfs"
                },
                {
                    "authorId": "2700036",
                    "name": "O. Hohlfeld"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                },
                {
                    "authorId": "79815490",
                    "name": "Niklas Henckell"
                }
            ]
        },
        {
            "paperId": "56e39d150d2e36119450e7e91cba4c530610cc08",
            "title": "GetFair: Generalized Fairness Tuning of Classification Models",
            "abstract": "We present GetFair, a novel framework for tuning fairness of classification models. The fair classification problem deals with training models for a given classification task where data points have sensitive attributes. The goal of fair classification models is to not only generate accurate classification results but also to prevent discrimination against subpopulations (i.e., individuals with a specific value for the sensitive attribute). Existing methods for enhancing fairness of classification models, however, are often specifically designed for a particular fairness metric or a classifier model. They may also not be suitable for scenarios with incomplete training data or where optimizing for multiple fairness metrics is important. GetFair represents a general solution to this problem. The GetFair approach works in the following way: First, a given classifier is trained on training data without any fairness objective. This is followed by a reinforcement learning inspired tuning procedure which updates the parameters of the learned model on a given fairness objective. This disentangles classifier training from fairness tuning, making our framework more general and allowing for the adoption of any parameterized classifier model. Because fairness metrics are designed as reward functions during tuning, GetFair generalizes across any fairness metric. We demonstrate the generalizability of GetFair via evaluation over a benchmark suite of datasets, classification models, and fairness metrics. In addition, GetFair can also be deployed in settings where the training data is incomplete or the classifier needs to be tuned on multiple fairness metrics. GetFair not only contributes a flexible method to the repertoire of tools available to improve the fairness of classification models, it also seamlessly adapts to settings where existing fair classification methods may not be suitable or applicable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2632448",
                    "name": "Sandipan Sikdar"
                },
                {
                    "authorId": "2101037",
                    "name": "Florian Lemmerich"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                }
            ]
        },
        {
            "paperId": "587c798846b589ddb69695987f6d27ab60063b1d",
            "title": "Neighborhood Structure Configuration Models",
            "abstract": "We develop a new method to efficiently sample synthetic networks that preserve the d-hop neighborhood structure of a given network for any given d. The proposed algorithm trades off the diversity in network samples against the depth of the neighborhood structure that is preserved. Our key innovation is to employ a colored Configuration Model with colors derived from iterations of the so-called Color Refinement algorithm. We prove that with increasing iterations the preserved structural information increases: the generated synthetic networks and the original network become more and more similar, and are eventually indistinguishable in terms of centrality measures such as PageRank, HITS, Katz centrality and eigenvector centrality. Our work enables to efficiently generate samples with a precisely controlled similarity to the original network, especially for large networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2000792611",
                    "name": "Felix I. Stamm"
                },
                {
                    "authorId": "2105604155",
                    "name": "Michael Scholkemper"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                },
                {
                    "authorId": "8036637",
                    "name": "Michael T. Schaub"
                }
            ]
        },
        {
            "paperId": "9c3ce6668ac4a3e60831de4ff18ee961b9c7dbdd",
            "title": "Minorities in networks and algorithms",
            "abstract": "In this chapter, we provide an overview of recent advances in data-driven and theory-informed complex models of social networks and their potential in understanding societal inequalities and marginalization. We focus on inequalities arising from networks and network-based algorithms and how they affect minorities. In particular, we examine how homophily and mixing biases shape large and small social networks, in\ufb02uence perception of minorities, and affect collaboration patterns. We also discuss dynamical processes on and of networks and the formation of norms and health inequalities. Additionally, we argue that network modeling is paramount for unveiling the effect of ranking and social recommendation algorithms on the visibility of minorities. Finally, we highlight the key challenges and future opportunities in this emerging research topic.",
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51118506",
                    "name": "F. Karimi"
                },
                {
                    "authorId": "2141757242",
                    "name": "Marcos Oliveira"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                }
            ]
        },
        {
            "paperId": "ee21a30bc29f9c210caa36264ccd68a5662bf688",
            "title": "Adversarial Inter-Group Link Injection Degrades the Fairness of Graph Neural Networks",
            "abstract": "We present evidence for the existence and effectiveness of adversarial attacks on graph neural networks (GNNs) that aim to degrade fairness. These attacks can disadvantage a particular subgroup of nodes in GNN-based node classification, where nodes of the underlying network have sensitive attributes, such as race or gender. We conduct qualitative and experimental analyses explaining how adversarial link injection impairs the fairness of GNN predictions. For example, an attacker can compromise the fairness of GNN-based node classification by injecting adversarial links between nodes belonging to opposite subgroups and opposite class labels. Our experiments on empirical datasets demonstrate that adversarial fairness attacks can significantly degrade the fairness of GNN predictions (attacks are effective) with a low perturbation rate (attacks are efficient) and without a significant drop in accuracy (attacks are deceptive). This work demonstrates the vulnerability of GNN models to adversarial fairness attacks. We hope our findings raise awareness about this issue in our community and lay a foundation for the future development of GNN models that are more robust to such attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065159398",
                    "name": "Hussain Hussain"
                },
                {
                    "authorId": "2057070058",
                    "name": "Meng Cao"
                },
                {
                    "authorId": "2632448",
                    "name": "Sandipan Sikdar"
                },
                {
                    "authorId": "1747800",
                    "name": "D. Helic"
                },
                {
                    "authorId": "3124784",
                    "name": "E. Lex"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                },
                {
                    "authorId": "153815524",
                    "name": "Roman Kern"
                }
            ]
        },
        {
            "paperId": "f89c44b0693e46256262385b057398c0a1254ce7",
            "title": "Properties of Group Fairness Metrics for Rankings",
            "abstract": "In recent years, several metrics have been developed for evaluating group fairness of rankings. Given that these metrics were developed with different application contexts and ranking algorithms in mind, it is not straightforward which metric to choose for a given scenario. In this paper, we perform a comprehensive comparative analysis of existing group fairness metrics developed in the context of fair ranking. By virtue of their diverse application contexts, we argue that such a comparative analysis is not straightforward. Hence, we take an axiomatic approach whereby we design a set of thirteen properties for group fairness metrics that consider different ranking settings. A metric can then be selected depending on whether it satisfies all or a subset of these properties. We apply these properties on eleven existing group fairness metrics, and through both empirical and theoretical results we demonstrate that most of these metrics only satisfy a small subset of the proposed properties. These findings highlight limitations of existing metrics, and provide insights into how to evaluate and interpret different fairness metrics in practical deployment. The proposed properties can also assist practitioners in selecting appropriate metrics for evaluating fairness in a specific application.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39124179",
                    "name": "Tobias Schumacher"
                },
                {
                    "authorId": "2159268668",
                    "name": "Marlene Lutz"
                },
                {
                    "authorId": "2632448",
                    "name": "Sandipan Sikdar"
                },
                {
                    "authorId": "1743043",
                    "name": "M. Strohmaier"
                }
            ]
        }
    ]
}