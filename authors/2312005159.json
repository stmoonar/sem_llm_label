{
    "authorId": "2312005159",
    "papers": [
        {
            "paperId": "14005fa355419392fb3d1e1a59b2cc89f41dac28",
            "title": "FLOR: On the Effectiveness of Language Adaptation",
            "abstract": "Large language models have amply proven their great capabilities, both in downstream tasks and real-life settings. However, low- and mid-resource languages do not have access to the necessary means to train such models from scratch, and often have to rely on multilingual models despite being underrepresented in the training data. For the particular case of the Catalan language, we prove that continued pre-training with vocabulary adaptation is a better alternative to take the most out of already pre-trained models, even if these have not seen any Catalan data during their pre-training phase. We curate a 26B tokens corpus and use it to further pre-train BLOOM, giving rise to the FLOR models. We perform an extensive evaluation to assess the effectiveness of our method, obtaining consistent gains across Catalan and Spanish tasks. The models, training data, and evaluation framework are made freely available under permissive licenses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2302564015",
                    "name": "Severino Da Dalt"
                },
                {
                    "authorId": "2165226605",
                    "name": "Joan Llop"
                },
                {
                    "authorId": "2301578899",
                    "name": "Irene Baucells"
                },
                {
                    "authorId": "1850527789",
                    "name": "Marc P\u00e0mies"
                },
                {
                    "authorId": "2312005159",
                    "name": "Yishi Xu"
                },
                {
                    "authorId": "1403836100",
                    "name": "Aitor Gonzalez-Agirre"
                },
                {
                    "authorId": "2301579083",
                    "name": "Marta Villegas"
                }
            ]
        },
        {
            "paperId": "260282639dce1984c1a065aa4feae41cea0fed06",
            "title": "Bilingual Adaptation of Monolingual Foundation Models",
            "abstract": "We present an efficient method for adapting a monolingual Large Language Model (LLM) to another language, addressing challenges of catastrophic forgetting and tokenizer limitations. We focus this study on adapting Llama 2 to Arabic. Our two-stage approach begins with expanding the vocabulary and training only the embeddings matrix, followed by full model continual pre-training on a bilingual corpus. By continually pre-training on a mix of Arabic and English corpora, the model retains its proficiency in English while acquiring capabilities in Arabic. Our approach results in significant improvements in Arabic and slight enhancements in English, demonstrating cost-effective cross-lingual transfer. We perform ablations on embedding initialization techniques, data mix ratios, and learning rates and release a detailed training recipe. To demonstrate generalizability of this approach we also adapted Llama 3 8B to Arabic and Llama 2 13B to Hindi.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238627171",
                    "name": "Gurpreet Gosal"
                },
                {
                    "authorId": "2312005159",
                    "name": "Yishi Xu"
                },
                {
                    "authorId": "2311889065",
                    "name": "Gokul Ramakrishnan"
                },
                {
                    "authorId": "2311894591",
                    "name": "Rituraj Joshi"
                },
                {
                    "authorId": "2311892291",
                    "name": "Avraham Sheinin"
                },
                {
                    "authorId": "2311991181",
                    "name": "Zhiming Chen"
                },
                {
                    "authorId": "2311891083",
                    "name": "Biswajit Mishra"
                },
                {
                    "authorId": "2243228182",
                    "name": "Natalia Vassilieva"
                },
                {
                    "authorId": "3130228",
                    "name": "Joel Hestness"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "2311888768",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "2087720002",
                    "name": "Bokang Jia"
                },
                {
                    "authorId": "2311887646",
                    "name": "Onkar Pandit"
                },
                {
                    "authorId": "2235818050",
                    "name": "Satheesh Katipomu"
                },
                {
                    "authorId": "2203791403",
                    "name": "Samta Kamboj"
                },
                {
                    "authorId": "2313594182",
                    "name": "Samujjwal Ghosh"
                },
                {
                    "authorId": "2235794681",
                    "name": "Rahul Pal"
                },
                {
                    "authorId": "2311889115",
                    "name": "Parvez Mullah"
                },
                {
                    "authorId": "2311887505",
                    "name": "Soundar Doraiswamy"
                },
                {
                    "authorId": "2311887344",
                    "name": "Mohamed El Karim Chami"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        }
    ]
}