{
    "authorId": "2238208116",
    "papers": [
        {
            "paperId": "27cc05d2ad2f48123db8fb6b38690862b34ac75c",
            "title": "Large Generative Graph Models",
            "abstract": "Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno are trained on a huge amount of language corpus, images, videos, and audio that are extremely diverse from numerous domains. This training paradigm over diverse well-curated data lies at the heart of generating creative and sensible content. However, all previous graph generative models (e.g., GraphRNN, MDVAE, MoFlow, GDSS, and DiGress) have been trained only on one dataset each time, which cannot replicate the revolutionary success achieved by LGMs in other fields. To remedy this crucial gap, we propose a new class of graph generative model called Large Graph Generative Model (LGGM) that is trained on a large corpus of graphs (over 5000 graphs) from 13 different domains. We empirically demonstrate that the pre-trained LGGM has superior zero-shot generative capability to existing graph generative models. Furthermore, our pre-trained LGGM can be easily fine-tuned with graphs from target domains and demonstrate even better performance than those directly trained from scratch, behaving as a solid starting point for real-world customization. Inspired by Stable Diffusion, we further equip LGGM with the capability to generate graphs given text prompts (Text-to-Graph), such as the description of the network name and domain (i.e.,\"The power-1138-bus graph represents a network of buses in a power distribution system.\"), and network statistics (i.e.,\"The graph has a low average degree, suitable for modeling social media interactions.\"). This Text-to-Graph capability integrates the extensive world knowledge in the underlying language model, offering users fine-grained control of the generated graphs. We release the code, the model checkpoint, and the datasets at https://lggm-lg.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284900711",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2268675415",
                    "name": "Namyong Park"
                },
                {
                    "authorId": "2305588553",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "30440868",
                    "name": "Puja Trivedi"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "2290558635",
                    "name": "Tyler Derr"
                }
            ]
        },
        {
            "paperId": "5b2f5c995218e85971a8944f427d737aed5c0b5f",
            "title": "Augmenting Textual Generation via Topology Aware Retrieval",
            "abstract": "Despite the impressive advancements of Large Language Models (LLMs) in generating text, they are often limited by the knowledge contained in the input and prone to producing inaccurate or hallucinated content. To tackle these issues, Retrieval-augmented Generation (RAG) is employed as an effective strategy to enhance the available knowledge base and anchor the responses in reality by pulling additional texts from external databases. In real-world applications, texts are often linked through entities within a graph, such as citations in academic papers or comments in social networks. This paper exploits these topological relationships to guide the retrieval process in RAG. Specifically, we explore two kinds of topological connections: proximity-based, focusing on closely connected nodes, and role-based, which looks at nodes sharing similar subgraph structures. Our empirical research confirms their relevance to text relationships, leading us to develop a Topology-aware Retrieval-augmented Generation framework. This framework includes a retrieval module that selects texts based on their topological relationships and an aggregation module that integrates these texts into prompts to stimulate LLMs for text generation. We have curated established text-attributed networks and conducted comprehensive experiments to validate the effectiveness of this framework, demonstrating its potential to enhance RAG with topological awareness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284900711",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2303636546",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2233085914",
                    "name": "Alexa F. Siu"
                },
                {
                    "authorId": "2124210729",
                    "name": "Yuying Zhao"
                },
                {
                    "authorId": "2303464225",
                    "name": "Bo Ni"
                },
                {
                    "authorId": "2304410306",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2067148039",
                    "name": "Tyler Derr"
                }
            ]
        },
        {
            "paperId": "7b98ec261e39b1fe7310243a9d7bfb76c566f2ca",
            "title": "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs",
            "abstract": "Extractive summarization plays a pivotal role in natural language processing due to its wide-range applications in summarizing diverse content efficiently, while also being faithful to the original content. Despite significant advancement achieved in extractive summarization by Large Language Models (LLMs), these summaries frequently exhibit incoherence. An important aspect of the coherent summary is its readability for intended users. Although there have been many datasets and benchmarks proposed for creating coherent extractive summaries, none of them currently incorporate user intent to improve coherence in extractive summarization. Motivated by this, we propose a systematically created human-annotated dataset consisting of coherent summaries for five publicly available datasets and natural language user feedback, offering valuable insights into how to improve coherence in extractive summaries. We utilize this dataset for aligning LLMs through supervised fine-tuning with natural language human feedback to enhance the coherence of their generated summaries. Preliminary experiments with Falcon-40B and Llama-2-13B show significant performance improvements (~10% Rouge-L) in terms of producing coherent summaries. We further utilize human feedback to benchmark results over instruction-tuned models such as FLAN-T5 which resulted in several interesting findings. Data and source code are available at https://github.com/Mihir3009/Extract-AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2310345197",
                    "name": "Mihir Parmar"
                },
                {
                    "authorId": "1787977",
                    "name": "Hanieh Deilamsalehy"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2110654003",
                    "name": "Seunghyun Yoon"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2265648617",
                    "name": "Trung Bui"
                }
            ]
        },
        {
            "paperId": "84e40546ad8f0238fd375cea40f96affa7bb7488",
            "title": "Identifying Speakers in Dialogue Transcripts: A Text-based Approach Using Pretrained Language Models",
            "abstract": "We introduce an approach to identifying speaker names in dialogue transcripts, a crucial task for enhancing content accessibility and searchability in digital media archives. Despite the advancements in speech recognition, the task of text-based speaker identification (SpeakerID) has received limited attention, lacking large-scale, diverse datasets for effective model training. Addressing these gaps, we present a novel, large-scale dataset derived from the MediaSum corpus, encompassing transcripts from a wide range of media sources. We propose novel transformer-based models tailored for SpeakerID, leveraging contextual cues within dialogues to accurately attribute speaker names. Through extensive experiments, our best model achieves a great precision of 80.3\\%, setting a new benchmark for SpeakerID. The data and code are publicly available here: \\url{https://github.com/adobe-research/speaker-identification}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2312035146",
                    "name": "Minh Nguyen"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2110654003",
                    "name": "Seunghyun Yoon"
                },
                {
                    "authorId": "1787977",
                    "name": "Hanieh Deilamsalehy"
                },
                {
                    "authorId": "2312006077",
                    "name": "Hao Tan"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2265648457",
                    "name": "Q. Tran"
                },
                {
                    "authorId": "2265648617",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "2293544012",
                    "name": "Thien Huu Nguyen"
                }
            ]
        },
        {
            "paperId": "b618998f9f3634331c8762342bbf110b74ad3fc0",
            "title": "LongLaMP: A Benchmark for Personalized Long-form Text Generation",
            "abstract": "Long-text generation is seemingly ubiquitous in real-world applications of large language models such as generating an email or writing a review. Despite the fundamental importance and prevalence of long-text generation in many practical applications, existing work on personalized generation has focused on the generation of very short text. To overcome these limitations, we study the problem of personalized long-text generation, that is, generating long-text that is personalized for a specific user while being practically useful for the vast majority of real-world applications that naturally require the generation of longer text. In this work, we demonstrate the importance of user-specific personalization for long-text generation tasks and develop the Long-text Language Model Personalization (LongLaMP) Benchmark. LongLaMP provides a comprehensive and diverse evaluation framework for personalized long-text generation. Extensive experiments on LongLaMP for zero-shot and fine-tuned language tasks demonstrate the effectiveness of the proposed benchmark and its utility for developing and evaluating techniques for personalized long-text generation across a wide variety of long-text generation tasks. The results highlight the importance of personalization across a wide variety of long-text generation tasks. Finally, we release the benchmark for others to use for this important problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2311502672",
                    "name": "Ishita Kumar"
                },
                {
                    "authorId": "2311502900",
                    "name": "Snigdha Viswanathan"
                },
                {
                    "authorId": "2311502181",
                    "name": "Sushrita Yerra"
                },
                {
                    "authorId": "2073044451",
                    "name": "Alireza Salemi"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2257962368",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "1787977",
                    "name": "Hanieh Deilamsalehy"
                },
                {
                    "authorId": "2312278435",
                    "name": "Xiang Chen"
                },
                {
                    "authorId": "2303636546",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2311640933",
                    "name": "Shubham Agarwal"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2295731593",
                    "name": "Hamed Zamani"
                }
            ]
        },
        {
            "paperId": "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b",
            "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",
            "abstract": "Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116085512",
                    "name": "Thuat Nguyen"
                },
                {
                    "authorId": "2186540882",
                    "name": "Chien Van Nguyen"
                },
                {
                    "authorId": "1405279380",
                    "name": "Viet Dac Lai"
                },
                {
                    "authorId": "2027979466",
                    "name": "Hieu Man"
                },
                {
                    "authorId": "1692755523",
                    "name": "Nghia Trung Ngo"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "1811211",
                    "name": "Thien Huu Nguyen"
                }
            ]
        },
        {
            "paperId": "3de4c6545e535562a9b6770eac1c52513aa72694",
            "title": "PDFTriage: Question Answering over Long, Structured Documents",
            "abstract": "Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmented models across several classes of questions where existing retrieval-augmented LLMs fail. To facilitate further research on this fundamental problem, we release our benchmark dataset consisting of 900+ human-generated questions over 80 structured documents from 10 different categories of question types for document QA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2243180037",
                    "name": "Jon Saad-Falcon"
                },
                {
                    "authorId": "40080808",
                    "name": "Joe Barrow"
                },
                {
                    "authorId": "2233085914",
                    "name": "Alexa F. Siu"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                }
            ]
        },
        {
            "paperId": "60d0b4dba75f4f026ac4db01d8fd408ac609217b",
            "title": "How Small Businesses Transform PDF Agreements into Action",
            "abstract": "A legal agreement is a type of procedural document that describes the steps that parties must take to fulfill legal obligations. Following these steps requires human interpretation, which is often inefficient and error prone. For a Small to Medium Sized Business (SMB), this process is laborious. We conduct an interview study to understand how information in PDF agreements is currently understood, processed, and acted upon by SMB employees working in small teams of non-domain experts. Through qualitative analysis and a text highlighting activity, we observe knowledge transfer workflows in SMBs and propose design principles for using AI-extracted information to create actionable documents that address gaps in efficiency, understanding, and agency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257765639",
                    "name": "Jianna So"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2233085914",
                    "name": "Alexa F. Siu"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2257962368",
                    "name": "Franck Dernoncourt"
                }
            ]
        },
        {
            "paperId": "68d7c9693022eefb45f2d73e121c641a00912d68",
            "title": "TaleStream: Supporting Story Ideation with Trope Knowledge",
            "abstract": "Story ideation is a critical part of the story-writing process. It is challenging to support computationally due to its exploratory and subjective nature. Tropes, which are recurring narrative elements across stories, are essential in stories as they shape the structure of narratives and our understanding of them. In this paper, we propose to use tropes as an intermediate representation of stories to approach story ideation. We present TaleStream, a canvas system that uses tropes as building blocks of stories while providing steerable suggestions of story ideas in the form of tropes. Our trope suggestion methods leverage data from the tvtropes.org wiki. We find that 97% of the time, trope suggestions generated by our methods provide better story ideation materials than random tropes. Our system evaluation suggests that TaleStream can support writers\u2019 creative flow and greatly facilitates story development. Tropes, as a rich lexicon of narratives with available examples, play a key role in TaleStream and hold promise for story-creation support systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238210350",
                    "name": "Jean-Peic Chou"
                },
                {
                    "authorId": "2233085914",
                    "name": "Alexa F. Siu"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "1820412",
                    "name": "Maneesh Agrawala"
                }
            ]
        }
    ]
}