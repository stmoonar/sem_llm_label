{
    "authorId": "2144455457",
    "papers": [
        {
            "paperId": "0d5103378a9f4f6e08bfcd364da207f93b31b8b7",
            "title": "Prompt Learning with Optimal Transport for Vision-Language Models",
            "abstract": "With the increasing attention to large vision-language models such as CLIP, there has been a significant amount of effort dedicated to building efficient prompts. Unlike conventional methods of only learning one single prompt, we propose to learn multiple comprehensive prompts to describe diverse characteristics of categories such as intrinsic attributes or extrinsic contexts. However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point. To solve this problem, we propose to apply optimal transport to match the vision and text modalities. Specifically, we first model images and the categories with visual and textual feature sets. Then, we apply a two-stage optimization strategy to learn the prompts. In the inner loop, we optimize the optimal transport distance to align visual features and prompts by the Sinkhorn algorithm, while in the outer loop, we learn the prompts by this distance from the supervised data. Extensive experiments are conducted on the few-shot recognition task and the improvement demonstrates the superiority of our method. The code is available at https://github.com/CHENGY12/PLOT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9230423",
                    "name": "Guangyi Chen"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2144455457",
                    "name": "Xinyue Li"
                },
                {
                    "authorId": "39358728",
                    "name": "Yongming Rao"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        },
        {
            "paperId": "47b82b39c06daf86e264ceb16853326b66e02b9c",
            "title": "xDeepFIG: An eXtreme Deep Model with Feature Interactions and Generation for CTR Prediction",
            "abstract": "In this paper, we propose an eXtreme deep model with feature interactions and generation for CTR prediction, called xDeepFIG. The feature generation module fully leverages some advantages of convolutional neural network (CNN) to generate new local and global features, and concatenates them with raw features. Such new fully fused features are shared by both the deep neural network (DNN) and compressed interaction network (CIN), which can learn both implicit and explicit high-order feature interactions automatically. Numerical results on two benchmark datasets for CTR demonstrates such feature fusion can bring some advantages and the xDeepFIG outperforms recent baseline models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052218689",
                    "name": "Bokai Xu"
                },
                {
                    "authorId": "2151274016",
                    "name": "Shihan Bu"
                },
                {
                    "authorId": "2144455457",
                    "name": "Xinyue Li"
                },
                {
                    "authorId": "2153463080",
                    "name": "Yanzhi Lin"
                },
                {
                    "authorId": "2151278475",
                    "name": "Shengxin Zhu"
                }
            ]
        },
        {
            "paperId": "a4b7070f69d67451225a7a09ce383526de01aa51",
            "title": "Exploration of a Balanced Reference Corpus with a Wide Variety of Text Mining Tools",
            "abstract": "To compare various techniques, the same platform is generally used into which the user will import a text dataset. Another approach uses an evaluation based on a gold standard for a specific task, but a balanced common language corpus is not often used. We choose the Corpus of Contemporary American English Corpus (COCA) as a balanced reference corpus, and split this corpus into categories, such as topics and genres, to apply families of feature extraction and machine learning algorithms. We found that the Stanford CoreNLP method was faster and more accurate than the NLTK method, and was more reliable and easier to understand. The results of clustering show that a higher modularity influences interpretation. For genre and topic classification, all techniques achieved a relatively high score, though these were below the state-of-the-art scores from challenge text datasets. Na\u00efve Bayes outperformed the other alternatives. We hope that balanced corpora from a variety of different vernacular (or low-resource) languages can be used as references to determine the efficiency of the wide diversity of state-of-the-art text mining tools.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2139021",
                    "name": "Nicolas Turenne"
                },
                {
                    "authorId": "2052218689",
                    "name": "Bokai Xu"
                },
                {
                    "authorId": "2144455457",
                    "name": "Xinyue Li"
                },
                {
                    "authorId": "2052220752",
                    "name": "Xindi Xu"
                },
                {
                    "authorId": "2115668971",
                    "name": "Hongyu Liu"
                },
                {
                    "authorId": "49897567",
                    "name": "Xiaolin Zhu"
                }
            ]
        }
    ]
}