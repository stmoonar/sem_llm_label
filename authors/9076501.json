{
    "authorId": "9076501",
    "papers": [
        {
            "paperId": "9b73fc4b7a5b0c0e998203960dfe49c0a10c6d5a",
            "title": "Corporate Communication Companion (CCC): An LLM-empowered Writing Assistant for Workplace Social Media",
            "abstract": "Workplace social media platforms enable employees to cultivate their professional image and connect with colleagues in a semi-formal environment. While semi-formal corporate communication poses a unique set of challenges, large language models (LLMs) have shown great promise in helping users draft and edit their social media posts. However, LLMs may fail to capture individualized tones and voices in such workplace use cases, as they often generate text using a\"one-size-fits-all\"approach that can be perceived as generic and bland. In this paper, we present Corporate Communication Companion (CCC), an LLM-empowered interactive system that helps people compose customized and individualized workplace social media posts. Using need-finding interviews to motivate our system design, CCC decomposes the writing process into two core functions, outline and edit: First, it suggests post outlines based on users' job status and previous posts, and next provides edits with attributions that users can contextually customize. We conducted a within-subjects user study asking participants both to write posts and evaluate posts written by others. The results show that CCC enhances users' writing experience, and audience members rate CCC-enhanced posts as higher quality than posts written using a non-customized writing assistant. We conclude by discussing the implications of LLM-empowered corporate communication.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266767060",
                    "name": "Zhuoran Lu"
                },
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "2243180014",
                    "name": "Tara Safavi"
                },
                {
                    "authorId": "2243181687",
                    "name": "Jennifer Neville"
                },
                {
                    "authorId": "2266801772",
                    "name": "Longqi Yang"
                },
                {
                    "authorId": "2266754711",
                    "name": "Mengting Wan"
                }
            ]
        },
        {
            "paperId": "cac90bdc21cf4777690f83aca4e63113566221e6",
            "title": "Interactive Topic Models with Optimal Transport",
            "abstract": "Topic models are widely used to analyze document collections. While they are valuable for discovering latent topics in a corpus when analysts are unfamiliar with the corpus, analysts also commonly start with an understanding of the content present in a corpus. This may be through categories obtained from an initial pass over the corpus or a desire to analyze the corpus through a predefined set of categories derived from a high level theoretical framework (e.g. political ideology). In these scenarios analysts desire a topic modeling approach which incorporates their understanding of the corpus while supporting various forms of interaction with the model. In this work, we present EdTM, as an approach for label name supervised topic modeling. EdTM models topic modeling as an assignment problem while leveraging LM/LLM based document-topic affinities and using optimal transport for making globally coherent topic-assignments. In experiments, we show the efficacy of our framework compared to few-shot LLM classifiers, and topic models based on clustering and LDA. Further, we show EdTM's ability to incorporate various forms of analyst feedback and while remaining robust to noisy analyst inputs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32709247",
                    "name": "Garima Dhanania"
                },
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "2264486147",
                    "name": "Chau Minh Pham"
                },
                {
                    "authorId": "2136562",
                    "name": "Mohit Iyyer"
                },
                {
                    "authorId": "2309006776",
                    "name": "Hamed Zamani"
                },
                {
                    "authorId": "2261282756",
                    "name": "Andrew McCallum"
                }
            ]
        },
        {
            "paperId": "17170575aa8b4fa4e3eef5d366ada706a94dd836",
            "title": "LaMP: When Large Language Models Meet Personalization",
            "abstract": "This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2073044451",
                    "name": "Alireza Salemi"
                },
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "1815447",
                    "name": "Michael Bendersky"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                }
            ]
        },
        {
            "paperId": "3566e1245bfc90096fe0cdb8b18674da6519c8d6",
            "title": "Large Language Model Augmented Narrative Driven Recommendations",
            "abstract": "Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context \u2013 this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                }
            ]
        },
        {
            "paperId": "35f0cf9f70c408dbaf106e6a675244e2867c164e",
            "title": "PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers",
            "abstract": "Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author's communication style and specialized knowledge. In this paper, we address this challenge by proposing PEARL, a retrieval-augmented LLM writing assistant personalized with a generation-calibrated retriever. Our retriever is trained to select historic user-authored documents for prompt augmentation, such that they are likely to best personalize LLM generations for a user request. We propose two key novelties for training our retriever: 1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and 2) A scale-calibrating KL-divergence objective that ensures that our retriever closely tracks the benefit of a document for personalized generation. We demonstrate the effectiveness of PEARL in generating personalized workplace social media posts and Reddit comments. Finally, we showcase the potential of a generation-calibrated retriever to double as a performance predictor and further improve low-quality generations via LLM chaining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "2266767060",
                    "name": "Zhuoran Lu"
                },
                {
                    "authorId": "2266754711",
                    "name": "Mengting Wan"
                },
                {
                    "authorId": "2266801772",
                    "name": "Longqi Yang"
                },
                {
                    "authorId": "2266756625",
                    "name": "Steve Menezes"
                },
                {
                    "authorId": "2266756161",
                    "name": "Tina Baghaee"
                },
                {
                    "authorId": "2267049385",
                    "name": "Emmanuel Barajas Gonzalez"
                },
                {
                    "authorId": "2243181687",
                    "name": "Jennifer Neville"
                },
                {
                    "authorId": "2243180014",
                    "name": "Tara Safavi"
                }
            ]
        },
        {
            "paperId": "4ea39407170baaa53db370d8f09a61387f4f0161",
            "title": "How Data Scientists Review the Scholarly Literature",
            "abstract": "Keeping up with the research literature plays an important role in the workflow of scientists \u2013 allowing them to understand a field, formulate the problems they focus on, and develop the solutions that they contribute, which in turn shape the nature of the discipline. In this paper, we examine the literature review practices of data scientists. Data science represents a field seeing an exponential rise in papers, and increasingly drawing on and being applied in numerous diverse disciplines. Recent efforts have seen the development of several tools intended to help data scientists cope with a deluge of research and coordinated efforts to develop AI tools intended to uncover the research frontier. Despite these trends indicative of the information overload faced by data scientists, no prior work has examined the specific practices and challenges faced by these scientists in an interdisciplinary field with evolving scholarly norms. In this paper, we close this gap through a set of semi-structured interviews and think-aloud protocols of industry and academic data scientists (N = 20). Our results while corroborating other knowledge workers\u2019 practices uncover several novel findings: individuals (1) are challenged in seeking and sensemaking of papers beyond their disciplinary bubbles, (2) struggle to understand papers in the face of missing details and mathematical content, (3) grapple with the deluge by leveraging the knowledge context in code, blogs, and talks, and (4) lean on their peers online and in-person. Furthermore, we outline future directions likely to help data scientists cope with the burgeoning research literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "145493030",
                    "name": "Mahmood Jasim"
                },
                {
                    "authorId": "2184473339",
                    "name": "Haoru Song"
                },
                {
                    "authorId": "2199838032",
                    "name": "Sarah Akbar"
                },
                {
                    "authorId": "145363201",
                    "name": "Andre Kenneth Chase Randall"
                },
                {
                    "authorId": "1936892",
                    "name": "Narges Mahyar"
                }
            ]
        },
        {
            "paperId": "7843a49a497785eeb2680d3a1079024126142db0",
            "title": "Editable User Profiles for Controllable Text Recommendations",
            "abstract": "Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the controllability of LACE under simulated user interactions. Finally, we implement LACE in an interactive controllable recommender system and conduct a user study to demonstrate that users are able to improve the quality of recommendations they receive through interactions with an editable user profile.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "145493030",
                    "name": "Mahmood Jasim"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                }
            ]
        },
        {
            "paperId": "82e29e97049170c7ebbe11dd30cddd641835b970",
            "title": "Augmenting Scientific Creativity with Retrieval across Knowledge Domains",
            "abstract": "Exposure to ideas in domains outside a scientist's own may benefit her in reformulating existing research problems in novel ways and discovering new application domains for existing solution ideas. While improved performance in scholarly search engines can help scientists efficiently identify relevant advances in domains they may already be familiar with, it may fall short of helping them explore diverse ideas \\textit{outside} such domains. In this paper we explore the design of systems aimed at augmenting the end-user ability in cross-domain exploration with flexible query specification. To this end, we develop an exploratory search system in which end-users can select a portion of text core to their interest from a paper abstract and retrieve papers that have a high similarity to the user-selected core aspect but differ in terms of domains. Furthermore, end-users can `zoom in' to specific domain clusters to retrieve more papers from them and understand nuanced differences within the clusters. Our case studies with scientists uncover opportunities and design implications for systems aimed at facilitating cross-domain exploration and inspiration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26997666",
                    "name": "Hyeonsu B Kang"
                },
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "2112441097",
                    "name": "Kevin Huang"
                },
                {
                    "authorId": "144827671",
                    "name": "Haw-Shiuan Chang"
                },
                {
                    "authorId": "2168098338",
                    "name": "Thorben Prein"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "145234497",
                    "name": "A. Kittur"
                },
                {
                    "authorId": "46651096",
                    "name": "E. Olivetti"
                }
            ]
        },
        {
            "paperId": "6a4deeb40aed8a4d56c8d9401c94b6c7a769e8c3",
            "title": "CSFCube - A Test Collection of Computer Science Research Articles for Faceted Query by Example",
            "abstract": "Query by Example is a well-known information retrieval task in which a document is chosen by the user as the search query and the goal is to retrieve relevant documents from a large collection. However, a document often covers multiple aspects of a topic. To address this scenario we introduce the task of faceted Query by Example in which users can also specify a finer grained aspect in addition to the input query document. We focus on the application of this task in scientific literature search. We envision models which are able to retrieve scientific papers analogous to a query scientific paper along specifically chosen rhetorical structure elements as one solution to this problem. In this work, the rhetorical structure elements, which we refer to as facets, indicate objectives, methods, or results of a scientific paper. We introduce and describe an expert annotated test collection to evaluate models trained to perform this task. Our test collection consists of a diverse set of 50 query documents in English, drawn from computational linguistics and machine learning venues. We carefully follow the annotation guideline used by TREC for depth-k pooling (k = 100 or 250) and the resulting data collection consists of graded relevance scores with high annotation agreement. State of the art models evaluated on our dataset show a significant gap to be closed in further work. Our dataset may be accessed here: https://github.com/iesl/CSFCube",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "1388957618",
                    "name": "Timothy J. O'Gorman"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                }
            ]
        },
        {
            "paperId": "9c403ca58853fbb223f6e9fce446bb638f291692",
            "title": "MS-Mentions: Consistently Annotating Entity Mentions in Materials Science Procedural Text",
            "abstract": "Material science synthesis procedures are a promising domain for scientific NLP, as proper modeling of these recipes could provide insight into new ways of creating materials. However, a fundamental challenge in building information extraction models for material science synthesis procedures is getting accurate labels for the materials, operations, and other entities of those procedures. We present a new corpus of entity mention annotations over 595 Material Science synthesis procedural texts (157,488 tokens), which greatly expands the training data available for the Named Entity Recognition task. We outline a new label inventory designed to provide consistent annotations and a new annotation approach intended to maximize the consistency and annotation speed of domain experts. Inter-annotator agreement studies and baseline models trained upon the data suggest that the corpus provides high-quality annotations of these mention types. This corpus helps lay a foundation for future high-quality modeling of synthesis procedures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388957618",
                    "name": "Timothy J. O'Gorman"
                },
                {
                    "authorId": "67117584",
                    "name": "Z. Jensen"
                },
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "2112441097",
                    "name": "Kevin Huang"
                },
                {
                    "authorId": "97051510",
                    "name": "R. Mahbub"
                },
                {
                    "authorId": "46651096",
                    "name": "E. Olivetti"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                }
            ]
        }
    ]
}