{
    "authorId": "1794486",
    "papers": [
        {
            "paperId": "14ac64f35d5accd44444edc459516911cfbdee02",
            "title": "RTGen: Generating Region-Text Pairs for Open-Vocabulary Object Detection",
            "abstract": "Open-vocabulary object detection (OVD) requires solid modeling of the region-semantic relationship, which could be learned from massive region-text pairs. However, such data is limited in practice due to significant annotation costs. In this work, we propose RTGen to generate scalable open-vocabulary region-text pairs and demonstrate its capability to boost the performance of open-vocabulary object detection. RTGen includes both text-to-region and region-to-text generation processes on scalable image-caption data. The text-to-region generation is powered by image inpainting, directed by our proposed scene-aware inpainting guider for overall layout harmony. For region-to-text generation, we perform multiple region-level image captioning with various prompts and select the best matching text according to CLIP similarity. To facilitate detection training on region-text pairs, we also introduce a localization-aware region-text contrastive loss that learns object proposals tailored with different localization qualities. Extensive experiments demonstrate that our RTGen can serve as a scalable, semantically rich, and effective source for open-vocabulary object detection and continue to improve the model performance when more data is utilized, delivering superior performance compared to the existing state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4062519",
                    "name": "Fangyi Chen"
                },
                {
                    "authorId": "120811666",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "2304169620",
                    "name": "Zhantao Yang"
                },
                {
                    "authorId": "2303844382",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "2304360414",
                    "name": "Kai Hu"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                }
            ]
        },
        {
            "paperId": "543f1506fabb4aa8e07e0cb881d8f5fc89e3ebb9",
            "title": "Efficient Autoregressive Audio Modeling via Next-Scale Prediction",
            "abstract": "Audio generation has achieved remarkable progress with the advance of sophisticated generative models, such as diffusion models (DMs) and autoregressive (AR) models. However, due to the naturally significant sequence length of audio, the efficiency of audio generation remains an essential issue to be addressed, especially for AR models that are incorporated in large language models (LLMs). In this paper, we analyze the token length of audio tokenization and propose a novel \\textbf{S}cale-level \\textbf{A}udio \\textbf{T}okenizer (SAT), with improved residual quantization. Based on SAT, a scale-level \\textbf{A}coustic \\textbf{A}uto\\textbf{R}egressive (AAR) modeling framework is further proposed, which shifts the next-token AR prediction to next-scale AR prediction, significantly reducing the training cost and inference time. To validate the effectiveness of the proposed approach, we comprehensively analyze design choices and demonstrate the proposed AAR framework achieves a remarkable \\textbf{35}$\\times$ faster inference speed and +\\textbf{1.33} Fr\\'echet Audio Distance (FAD) against baselines on the AudioSet benchmark. Code: \\url{https://github.com/qiuk2/AAR}.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2290488011",
                    "name": "Kai Qiu"
                },
                {
                    "authorId": "2304463477",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2307335951",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "2316433304",
                    "name": "Jie Sun"
                },
                {
                    "authorId": "2273572218",
                    "name": "Jinglu Wang"
                },
                {
                    "authorId": "2306884438",
                    "name": "Zhe Lin"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                },
                {
                    "authorId": "2288787089",
                    "name": "Bhiksha Raj"
                }
            ]
        },
        {
            "paperId": "c384a3dbea53a0d294e84026e267e96f8e323e2a",
            "title": "A Reference-Based 3D Semantic-Aware Framework for Accurate Local Facial Attribute Editing",
            "abstract": "Facial attribute editing plays a crucial role in synthesizing realistic faces with specific characteristics while maintaining realistic appearances. Despite advancements, challenges persist in achieving precise, 3D-aware attribute modifications, which are crucial for consistent and accurate representations of faces from different angles. Current methods struggle with semantic entanglement and lack effective guidance for incorporating attributes while maintaining image integrity. To address these issues, we introduce a novel framework that merges the strengths of latent-based and reference-based editing methods. Our approach employs a 3D GAN inversion technique to embed attributes from the reference image into a tri-plane space, ensuring 3D consistency and realistic viewing from multiple perspectives. We utilize blending techniques and predicted semantic masks to locate precise edit regions, merging them with the contextual guidance from the reference image. A coarse-to-fine inpainting strategy is then applied to preserve the integrity of untargeted areas, significantly enhancing realism. Our evaluations demonstrate superior performance across diverse editing tasks, validating our framework's effectiveness in realistic and applicable facial attribute editing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2313593664",
                    "name": "Yu-Kai Huang"
                },
                {
                    "authorId": "3049981",
                    "name": "Yutong Zheng"
                },
                {
                    "authorId": "2313591498",
                    "name": "Yen-Shuo Su"
                },
                {
                    "authorId": "1742475119",
                    "name": "Anudeepsekhar Bolimera"
                },
                {
                    "authorId": "120811666",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "4062519",
                    "name": "Fangyi Chen"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                }
            ]
        },
        {
            "paperId": "0f6debcbabb27effcf5d2a8d74e5d96bf2f0a310",
            "title": "Boosting Transductive Few-Shot Fine-tuning with Margin-based Uncertainty Weighting and Probability Regularization",
            "abstract": "Few-Shot Learning (FSL) has been rapidly developed in recent years, potentially eliminating the requirement for significant data acquisition. Few-shot fine-tuning has been demonstrated to be practically efficient and helpful, especially for out-of-distribution datum [7, 13, 17, 29]. In this work, we first observe that the few-shot fine-tuned methods are learned with the imbalanced class marginal distribution, leading to imbalanced per-class testing accuracy. This observation further motivates us to propose the Transductive Fine-tuning with Margin-based uncertainty weighting and Probability regularization (TF-MP), which learns a more balanced class marginal distribution as shown in Fig. 1. We first conduct sample weighting on unlabeled testing data with margin-based uncertainty scores and fur-ther regularize each test sample's categorical probability. TF-MP achieves state-of-the-art performance on in-/out-of-distribution evaluations of Meta- Dataset [31] and sur-passes previous transductive methods by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26151496",
                    "name": "R. Tao"
                },
                {
                    "authorId": "2242179580",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                }
            ]
        },
        {
            "paperId": "100da279ee981960884a12dfc5a0697c24ed315a",
            "title": "SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning",
            "abstract": "The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051536212",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "26151496",
                    "name": "R. Tao"
                },
                {
                    "authorId": "144356331",
                    "name": "Yue Fan"
                },
                {
                    "authorId": "2108024273",
                    "name": "Yidong Wang"
                },
                {
                    "authorId": "1519290245",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "48920094",
                    "name": "B. Schiele"
                },
                {
                    "authorId": "1576441343",
                    "name": "Xingxu Xie"
                },
                {
                    "authorId": "1681921",
                    "name": "B. Raj"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                }
            ]
        },
        {
            "paperId": "19453e523227b2671cfdc9afbbfbda3a291d39c0",
            "title": "Cov Loss: Covariance-Based Loss for Deep Face Recognition",
            "abstract": "Recently, deep neural networks (DNNs) have emerged as state-of-the-art approaches for various computer vision areas. In this paper, we propose an optimized approach for large-scale face recognition. Our work is motivated through the recent development of deep convolutional neural networks (CNNs) that use different loss functions to learn deep features from face images to perform face recognition. As opposed to previous works, we model Cov loss to optimize deep features along the covariance matrix to enhance discriminative power. We formulate Cov loss to maximize inter-class variance and minimize intra-class variance by optimizing the distance between the deep features and their corresponding class covariances in the Euclidean space. The proposed Cov loss is evaluated on large-scale face recognition problems and present results on LFW, IJB-A Janus, IJB-C Janus, and Celebrity Frontal-Profile (CFP). By optimizing features along both Euclidean and angular spaces, our novel loss function learns more robust feature representations of faces, and improves general performance results comparable to the state-of-the-art results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66514896",
                    "name": "Ibrahim Alkanhal"
                },
                {
                    "authorId": "2074720753",
                    "name": "Abdullah Almansour"
                },
                {
                    "authorId": "2216503631",
                    "name": "Lamia Alsalloom"
                },
                {
                    "authorId": "2134875",
                    "name": "Raied Aljadaany"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                }
            ]
        },
        {
            "paperId": "5f2ffecd734106f061dec51f6600dcec5b9a404f",
            "title": "Fairness in Visual Clustering: A Novel Transformer Clustering Approach",
            "abstract": "Promoting fairness for deep clustering models in unsupervised clustering settings to reduce demographic bias is a challenging goal. This is because of the limitation of large-scale balanced data with well-annotated labels for sensitive or protected attributes. In this paper, we first evaluate demographic bias in deep clustering models from the perspective of cluster purity, which is measured by the ratio of positive samples within a cluster to their correlation degree. This measurement is adopted as an indication of demographic bias. Then, a novel loss function is introduced to encourage a purity consistency for all clusters to maintain the fairness aspect of the learned clustering model. Moreover, we present a novel attention mechanism, Cross-attention, to measure correlations between multiple clusters, strengthening faraway positive samples and improving the purity of clusters during the learning process. Experimental results on a large-scale dataset with numerous attribute settings have demonstrated the effectiveness of the proposed approach on both clustering accuracy and fairness enhancement on several sensitive attributes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "1876581",
                    "name": "C. Duong"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "0e22cc2174c5a9cd27f038b174dc0f7cdba61e17",
            "title": "Enhanced Training of Query-Based Object Detection via Selective Query Recollection",
            "abstract": "This paper investigates a phenomenon where query-based object detectors mispredict at the last decoding stage while predicting correctly at an intermediate stage. We review the training process and attribute the overlooked phenomenon to two limitations: lack of training emphasis and cascading errors from decoding sequence. We design and present Selective Query Recollection (SQR), a simple and effective training strategy for query-based object detectors. It cumulatively collects intermediate queries as decoding stages go deeper and selectively forwards the queries to the downstream stages aside from the sequential structure. Such-wise, SQR places training emphasis on later stages and allows later stages to work with intermediate queries from earlier stages directly. SQR can be easily plugged into various query-based object detectors and significantly enhances their performance while leaving the inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR, and Deformable-DETR across various settings (backbone, number of queries, schedule) and consistently brings 1.4 ~2.8 AP improvement. Code is available at https://github.com/Fangyi-Chen/SQR",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4062519",
                    "name": "Fangyi Chen"
                },
                {
                    "authorId": "120811666",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "1865368410",
                    "name": "Kaiqin Hu"
                },
                {
                    "authorId": "2108736469",
                    "name": "Yu-Kai Huang"
                },
                {
                    "authorId": "47894545",
                    "name": "Chenchen Zhu"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                }
            ]
        },
        {
            "paperId": "151ddaa95cccb7f965be0490c097d9c41bf073a5",
            "title": "Powering Finetuning in Few-Shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling",
            "abstract": "In recent works, utilizing a deep network trained on meta-training set serves as a strong baseline in few-shot learning. In this paper, we move forward to refine novel-class features by finetuning a trained deep network. Finetuning is designed to focus on reducing biases in novel-class feature distributions, which we define as two aspects: class-agnostic and class-specific biases. Class-agnostic bias is defined as the distribution shifting introduced by domain difference, which we propose Distribution Calibration Module(DCM) to reduce. DCM owes good property of eliminating domain difference and fast feature adaptation during optimization. Class-specific bias is defined as the biased estimation using a few samples in novel classes, which we propose Selected Sampling(SS) to reduce. Without inferring the actual class distribution, SS is designed by running sampling using proposal distributions around support-set samples. By powering finetuning with DCM and SS, we achieve state-of-the-art results on Meta-Dataset with consistent performance boosts over ten datasets from different domains. We believe our simple yet effective method demonstrates its possibility to be applied on practical few-shot applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26151496",
                    "name": "R. Tao"
                },
                {
                    "authorId": "120811666",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "3049981",
                    "name": "Yutong Zheng"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                }
            ]
        },
        {
            "paperId": "337af9cc5b0d561d5035f355609898bedaac7917",
            "title": "Unitail: Detecting, Reading, and Matching in Retail Scene",
            "abstract": "To make full use of computer vision technology in stores, it is required to consider the actual needs that fit the characteristics of the retail scene. Pursuing this goal, we introduce the United Retail Datasets (Unitail), a large-scale benchmark of basic visual tasks on products that challenges algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped instances annotated, the Unitail offers a detection dataset to align product appearance better. Furthermore, it provides a gallery-style OCR dataset containing 1454 product categories, 30k text regions, and 21k transcriptions to enable robust reading on products and motivate enhanced product matching. Besides benchmarking the datasets using various state-of-the-arts, we customize a new detector for product detection and provide a simple OCR-based matching solution that verifies its effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4062519",
                    "name": "Fangyi Chen"
                },
                {
                    "authorId": "120811666",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "7719105",
                    "name": "Zaiwang Li"
                },
                {
                    "authorId": "2161241587",
                    "name": "Jiachen Dou"
                },
                {
                    "authorId": "2066123456",
                    "name": "Shentong Mo"
                },
                {
                    "authorId": "2051536212",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "2154716854",
                    "name": "Yongxin Zhang"
                },
                {
                    "authorId": "2066265897",
                    "name": "Uzair Ahmed"
                },
                {
                    "authorId": "47894545",
                    "name": "Chenchen Zhu"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                }
            ]
        }
    ]
}