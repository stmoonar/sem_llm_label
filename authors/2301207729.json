{
    "authorId": "2301207729",
    "papers": [
        {
            "paperId": "5fe3c7923d04d8b34a365225d81fa5318fdfa83f",
            "title": "Hypformer: Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic Space",
            "abstract": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301207729",
                    "name": "Menglin Yang"
                },
                {
                    "authorId": "2309173156",
                    "name": "Harshit Verma"
                },
                {
                    "authorId": "2145943212",
                    "name": "Delvin Ce Zhang"
                },
                {
                    "authorId": "2144131350",
                    "name": "Jiahong Liu"
                },
                {
                    "authorId": "2309174208",
                    "name": "Irwin King"
                },
                {
                    "authorId": "2301161297",
                    "name": "Rex Ying"
                }
            ]
        },
        {
            "paperId": "ae739553e7c7c45c150e48c26cdff2fb2129f1a4",
            "title": "Hyperbolic Fine-tuning for Large Language Models",
            "abstract": "Large language models (LLMs) have demonstrated remarkable performance on various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for embedding tokens in LLMs. In this study, we first investigate the non-Euclidean characteristics of LLMs. Our findings reveal that token frequency follows a power-law distribution, with high-frequency tokens clustering near the origin and low-frequency tokens positioned farther away. Additionally, token embeddings exhibit a high degree of hyperbolicity, indicating a latent tree-like structure in the embedding space. Building on the observation, we propose to efficiently fine-tune LLMs in hyperbolic space to better exploit the underlying complex structures. However, we found that this fine-tuning in hyperbolic space cannot be achieved with naive application of exponential and logarithmic maps, when the embedding and weight matrices both reside in Euclidean space. To address this technique issue, we introduce a new method called hyperbolic low-rank efficient fine-tuning, HypLoRA, that performs low-rank adaptation directly on the hyperbolic manifold, avoiding the cancellation effect caused by the exponential and logarithmic maps, thus preserving the hyperbolic modeling capabilities. Through extensive experiments, we demonstrate that HypLoRA significantly enhances the performance of LLMs on reasoning tasks, particularly for complex reasoning problems. In particular, HypLoRA improves the performance in the complex AQuA dataset by up to 13.0%, showcasing its effectiveness in handling complex reasoning challenges",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301207729",
                    "name": "Menglin Yang"
                },
                {
                    "authorId": "2268782997",
                    "name": "Aosong Feng"
                },
                {
                    "authorId": "2324786242",
                    "name": "Bo Xiong"
                },
                {
                    "authorId": "2324835249",
                    "name": "Jihong Liu"
                },
                {
                    "authorId": "2309174208",
                    "name": "Irwin King"
                },
                {
                    "authorId": "2301161297",
                    "name": "Rex Ying"
                }
            ]
        },
        {
            "paperId": "c45236298d1274d225e0f54e9e33d67755583a08",
            "title": "DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs",
            "abstract": "Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. Despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to DyTAGs, which hinders the potential advancement in many research fields. To address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. To facilitate the use of DTGB, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. These tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with LLM embeddings, along with 6 powerful large language models (LLMs). Our results show the limitations of existing models in handling DyTAGs. Our analysis also demonstrates the utility of DTGB in investigating the incorporation of structural and textual dynamics. The proposed DTGB fosters research on DyTAGs and their broad applications. It offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. The dataset and source code are available at https://github.com/zjs123/DTGB.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1519088854",
                    "name": "Jiasheng Zhang"
                },
                {
                    "authorId": "2263472358",
                    "name": "Jialin Chen"
                },
                {
                    "authorId": "2301207729",
                    "name": "Menglin Yang"
                },
                {
                    "authorId": "2268782997",
                    "name": "Aosong Feng"
                },
                {
                    "authorId": "122563825",
                    "name": "Shuang Liang"
                },
                {
                    "authorId": "2241184642",
                    "name": "Jie Shao"
                },
                {
                    "authorId": "2301161297",
                    "name": "Rex Ying"
                }
            ]
        },
        {
            "paperId": "d97c719939e633fa37cb3b00d33c66c3d7db2b22",
            "title": "Text-Attributed Graph Representation Learning: Methods, Applications, and Challenges",
            "abstract": "Text documents are usually connected in a graph structure, resulting in an important class of data named text-attributed graph, e.g., paper citation graph and Web page hyperlink graph. On the one hand, Graph Neural Networks (GNNs) consider text in each document as general vertex attribute and do not specifically deal with text data. On the other hand, Pre-trained Language Models (PLMs) and Topic Models (TMs) learn effective document embeddings. However, most models focus on text content in each single document only, ignoring link adjacency across documents. The above two challenges motivate the development of text-attributed graph representation learning, combining GNNs with PLMs and TMs into a unified model and learning document embeddings preserving both modalities, which fulfill applications, e.g., text classification, citation recommendation, question answering, etc. In this lecture-style tutorial, we will provide a systematic review of text-attributed graph, including its formal definition, recent methods, diverse applications, and challenges. Specifically, i) we will formally define text-attributed graph and briefly review GNNs, PLMs, and TMs, which are the fundamentals of some existing methods. ii) We will then revisit the technical details of text-attributed graph models, which are generally split into two categories, PLM-based and TM-based. iii) Besides, we will show diverse applications built on text-attributed graph. iv) Finally, we will discuss some challenges of existing models and propose solutions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145943212",
                    "name": "Delvin Ce Zhang"
                },
                {
                    "authorId": "2301207729",
                    "name": "Menglin Yang"
                },
                {
                    "authorId": "2301161297",
                    "name": "Rex Ying"
                },
                {
                    "authorId": "3309003",
                    "name": "Hady W. Lauw"
                }
            ]
        }
    ]
}