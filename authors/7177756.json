{
    "authorId": "7177756",
    "papers": [
        {
            "paperId": "32ffb5e528b54f03a8a359e4573e0f3926a1202b",
            "title": "CgT-GAN: CLIP-guided Text GAN for Image Captioning",
            "abstract": "The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to \"see\" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the semantic guidance reward computed by the CLIP-based reward module. In addition to the cosine similarity as the semantic guidance reward (i.e., CLIP-cos), we further introduce a novel semantic guidance reward called CLIP-agg, which aligns the generated caption with a weighted text embedding by attentively aggregating the entire corpus. Experimental results on three subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms state-of-the-art methods significantly across all metrics. Code is available at https://github.com/Lihr747/CgtGAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7177756",
                    "name": "Jiarui Yu"
                },
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "48387349",
                    "name": "Y. Hao"
                },
                {
                    "authorId": "2106718459",
                    "name": "B. Zhu"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "2da4666811d0095ab0a71e01e238277a1d908c02",
            "title": "Unified QA-aware Knowledge Graph Generation Based on Multi-modal Modeling",
            "abstract": "Understanding the long duration videos' storyline is often considered a major challenge in the field of video understanding. To promote research on understanding longer videos in the community, the deep video understanding (DVU) task is suggested for recognizing interactions at the scene level and relationships at the movie level, as well as answering questions at these two levels. In this work, we propose a unified QA-aware knowledge graph generation approach, which consists of the relation-centric graph and interaction-centric graph and demonstrates the powerful performance of multimodal pre-training models in solving such problems. Extensive validations on the HLVU dataset demonstrate the effectiveness of our proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047997844",
                    "name": "Penggang Qin"
                },
                {
                    "authorId": "7177756",
                    "name": "Jiarui Yu"
                },
                {
                    "authorId": "2145971294",
                    "name": "Yan Gao"
                },
                {
                    "authorId": "2187417059",
                    "name": "Derong Xu"
                },
                {
                    "authorId": "2155905435",
                    "name": "Yunkai Chen"
                },
                {
                    "authorId": "2142349315",
                    "name": "Shiwei Wu"
                },
                {
                    "authorId": "50383766",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2173129111",
                    "name": "Enhong Chen"
                },
                {
                    "authorId": "48387349",
                    "name": "Y. Hao"
                }
            ]
        }
    ]
}