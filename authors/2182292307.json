{
    "authorId": "2182292307",
    "papers": [
        {
            "paperId": "303a86f5bf84d5767a19c888d7d3027b30d85b98",
            "title": "Attentive Modeling and Distillation for Out-of-Distribution Generalization of Federated Learning",
            "abstract": "Out-of-distribution issues lead to different optimization directions between clients, which weakens collaborative modeling in federated learning. Existing methods aims to decouple invariant features in the latent space to mitigate attribute bias. However, their performance is limited by suboptimal decoupling capabilities in complex latent spaces. To address this problem, this paper presents a method, termed FedAKD, that adaptively identifies meaningful visual regions in images to guide the model in learning causal features. It includes two main modules, where the attentive modeling module adaptively locates critical regions to mitigate the negative impact of irrelevant elements, which are considered significant contributors to distribution heterogeneity. The attention-guided representation learning module leverages attentive knowledge to guide the local model to pay more attention to important regions, which acts as a soft attention regularizer to mitigate the trade-off between capturing category-relevant information and irrelevant contextual information in images. Experiments were conducted on four datasets, including performance comparison, ablation study, and case study. The results demonstrate that FedAKD can effectively enhance attention to causal features, which leads to superior performance compared with the state-of-the-art methods. The source codes have been released at https://github.com/qizhuang-qz/FedAKD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2303974934",
                    "name": "Weihao He"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "385dd16d1859525e14020987e451acce1e5511d6",
            "title": "Cross-Modal Content Inference and Feature Enrichment for Cold-Start Recommendation",
            "abstract": "Multimedia recommendation aims to fuse the multi-modal information of items for feature enrichment to improve the recommendation performance. However, existing methods typically introduce multi-modal information based on collaborative information to improve the overall recommendation precision, while failing to explore its cold-start recommendation performance. Meanwhile, these above methods are only applicable when such multi-modal data is available. To address this problem, this paper proposes a recommendation framework, named Cross-modal Content Inference and Feature Enrichment Recommendation (CIERec), which exploits the multi-modal information to improve its cold-start recommendation performance. Specifically, CIERec first introduces image annotation as the privileged information to help guide the mapping of unified features from the visual space to the semantic space in the training phase. And then CIERec enriches the content representation with the fusion of collaborative, visual, and cross-modal inferred representations, so as to improve its cold-start recommendation performance. Experimental results on two real-world datasets show that the content representations learned by CIERec are able to achieve superior cold-start recommendation performance over existing visually-aware recommendation algorithms. More importantly, CIERec can consistently achieve significant improvements with different conventional visually-aware backbones, which verifies its universality and effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2143918656",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "7217fe2b9659fa01d180a135be88a759cfff5880",
            "title": "Multi-channel Attentive Weighting of Visual Frames for Multimodal Video Classification",
            "abstract": "Multimodal video classification aims to incorporate semantic information to regularize the visual representation learning of videos. Conventional methods typically focus on analyzing all information extracted from different modals rather than key information. However, they usually face the problem of handling the redundant video frames of little categorical information. To address this problem, this paper proposes a novel approach that employs multi-channel weighting of visual frames to mitigate the interference of redundant information. Specifically, the proposed algorithm, termed MCA-WF, includes two main modules, where the multi-channel attentive weighting of video frames (McAW) module performs the multi-granularity and multi-channel frame weighting mechanism based on visual self-attention, contrastive attention and cross-modal attention constraints to filter visual noise and redundant information. The visual frame selection (VFS) module explores the combination of multi-channel attention mechanisms to select the key visual information in the video. Experiments were conducted on MSR-VTT and ActivityNet Captions datasets in terms of performance comparison, ablation study, in-depth analysis, and case studies. The results verified that MCA-WF can notice the key information in the classification and effectively improve the ability of information complementation and integration between modals, which leads to better performance than the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2226670159",
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "48211506",
                    "name": "Jinxing Liu"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "88b0df915085ff8c115393fe445b1ce8464194b1",
            "title": "Class-level Structural Relation Modeling and Smoothing for Visual Representation Learning",
            "abstract": "Representation learning for images has been advanced by recent progress in more complex neural models such as the Vision Transformers and new learning theories such as the structural causal models. However, these models mainly rely on the classification loss to implicitly regularize the class-level data distributions, and they may face difficulties when handling classes with diverse visual patterns. We argue that the incorporation of the structural information between data samples may improve this situation. To achieve this goal, this paper presents a framework termed Class-level Structural Relation Modeling and Smoothing for Visual Representation Learning (CSRMS), which includes the Class-level Relation Modelling, Class-aware Graph Sampling, and Relational Graph-Guided Representation Learning modules to model a relational graph of the entire dataset and perform class-aware smoothing and regularization operations to alleviate the issue of intra-class visual diversity and inter-class similarity. Specifically, the Class-level Relation Modelling module uses a clustering algorithm to learn the data distributions in the feature space and identify three types of class-level sample relations for the training set; Class-aware Graph Sampling module extends typical training batch construction process with three strategies to sample dataset-level sub-graphs; and Relational Graph-Guided Representation Learning module employs a graph convolution network with knowledge-guided smoothing operations to ease the projection from different visual patterns to the same class. Experiments demonstrate the effectiveness of structured knowledge modelling for enhanced representation learning and show that CSRMS can be incorporated with any state-of-the-art visual representation learning models for performance gains. The source codes and demos have been released at https://github.com/czt117/CSRMS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262218236",
                    "name": "Zitan Chen"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2262088191",
                    "name": "Xiao Cao"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "a88d1d22c903a1d57b8f141f44678a47ae584f0f",
            "title": "Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data",
            "abstract": "Federated Learning aims to learn a global model on the server side that generalizes to all clients in a privacy-preserving manner, by leveraging the local models from different clients. Existing solutions focus on either regularizing the objective functions among clients or improving the aggregation mechanism for the improved model generalization capability. However, their performance is typically limited by the dataset biases, such as the heterogeneous data distributions and the missing classes. To address this issue, this paper presents a cross-silo prototypical calibration method (FedCSPC), which takes additional prototype information from the clients to learn a unified feature space on the server side. Specifically, FedCSPC first employs the Data Prototypical Modeling (DPM) module to learn data patterns via clustering to aid calibration. Subsequently, the cross-silo prototypical calibration (CSPC) module develops an augmented contrastive learning method to improve the robustness of the calibration, which can effectively project cross-source features into a consistent space while maintaining clear decision boundaries. Moreover, the CSPC module's ease of implementation and plug-and-play characteristics make it even more remarkable. Experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study, and the results verified that FedCSPC is capable of learning the consistent features across different data sources of the same class under the guidance of calibrated model, which leads to better performance than the state-of-the-art methods. The source codes have been released at https://github.com/qizhuang-qz/FedCSPC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "3353201",
                    "name": "Zitan Chen"
                },
                {
                    "authorId": "2228298634",
                    "name": "Han Hu"
                },
                {
                    "authorId": "2305692093",
                    "name": "Hui Lin"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "b571670c93cd0c784bd826a3ccd10ffb55ed14f0",
            "title": "Cross-Training with Prototypical Distillation for improving the generalization of Federated Learning",
            "abstract": "Cross-training has become a promising strategy to handle data heterogeneity problem in federated learning, which re-train a local model across different clients to improve its generalization capability in a privacy-preserving manner. Its main idea is to make the local models to fit the data of all clients. However, the heterogeneity between data sources may lead the local models to quickly forget the knowledge learned in several rounds of cross-training. To address the problem, this paper presents a novel prototype guided cross training mechanism, termed PGCT, to regularize the change of class-level data representations across clients. It includes two main modules, where the prototype guided representation learning module employs client-aware prototypes of data patterns learned by clustering to guide the learning of consistency representation across feature spaces. This maintains the similar decision boundary across different clients. The prototype-based feature augmentation module uses prototypes as soft attention regularizers to further aggregate rich information to enhance the discrimination of historical features. Experiments were conducted on four datasets in terms of performance comparison, ablation study and case study, and the results verified that PGCT can learn discriminative features with different classes under the guidance of prototypes, which leads to better performance than the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2233769098",
                    "name": "Tianhan Liu"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "3353201",
                    "name": "Zitan Chen"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "ce28e4f498d6e74952f5f8a3d3e110ee2548b6ae",
            "title": "Class-aware Convolution and Attentive Aggregation for Image Classification",
            "abstract": "Deep learning has been proven to be effective in image classification tasks. However, existing methods may face difficulties in distinguishing complex images due to the distraction caused by diverse image content. To overcome this challenge, we propose a class-aware convolution and attentive aggregation (CA-Net) framework that improves the effectiveness of representation learning and reduces the influence of irrelevant background. CA-Net includes three main modules: the discrete representation learning (DRL) module that uses a group learning method to learn discriminative representations, the class-aware score of discrete representation (CSDR) module that infers class-aware scores to generate weights for representation learners, and the class-aware representation fusion module(CRF) that aggregates class-aware representations using the class-aware scores as a guide. Our experimental results on three benchmarking datasets show that CA-Net improves the performance of state-of-the-art backbones and enhances feature extraction robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262218236",
                    "name": "Zitan Chen"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2226670159",
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "394c0fe4d4ff4ce3c7e9ca913e7a8abe30b37299",
            "title": "Meta-Causal Feature Learning for Out-of-Distribution Generalization",
            "abstract": "Causal inference has become a powerful tool to handle the out-of-distribution (OOD) generalization problem, which aims to extract the invariant features. However, conventional methods apply causal learners from multiple data splits, which may incur biased representation learning from imbalanced data distributions and difficulty in invariant feature learning from heterogeneous sources. To address these issues, this paper presents a balanced meta-causal learner (BMCL), which includes a balanced task generation module (BTG) and a meta-causal feature learning module (MCFL). Specifically, the BTG module learns to generate balanced subsets by a self-learned partitioning algorithm with constraints on the proportions of sample classes and contexts. The MCFL module trains a meta-learner adapted to different distributions. Experiments conducted on NICO++ dataset verified that BMCL effectively identifies the class-invariant visual regions for classification and may serve as a general framework to improve the performance of the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "1492114275",
                    "name": "Jingyu Li"
                },
                {
                    "authorId": "2173892322",
                    "name": "Xuelong Li"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "467a41c36ee7ae3f9e3d58ac48fc92e37856e826",
            "title": "Open Vocabulary Multi-Label Classification with Dual-Modal Decoder on Aligned Visual-Textual Features",
            "abstract": "In computer vision, multi-label recognition are important tasks with many real-world applications, but classifying previously unseen labels remains a significant challenge. In this paper, we propose a novel algorithm, Aligned Dual moDality ClaSsifier (ADDS), which includes a Dual-Modal decoder (DM-decoder) with alignment between visual and textual features, for open-vocabulary multi-label classification tasks. Then we design a simple and yet effective method called Pyramid-Forwarding to enhance the performance for inputs with high resolutions. Moreover, the Selective Language Supervision is applied to further enhance the model performance. Extensive experiments conducted on several standard benchmarks, NUS-WIDE, ImageNet-1k, ImageNet-21k, and MS-COCO, demonstrate that our approach significantly outperforms previous methods and provides state-of-the-art performance for open-vocabulary multi-label classification, conventional multi-label classification and an extreme case called single-to-multi label classification where models trained on single-label datasets (ImageNet-1k, ImageNet-21k) are tested on multi-label ones (MS-COCO and NUS-WIDE).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3030561",
                    "name": "Shichao Xu"
                },
                {
                    "authorId": "2110468365",
                    "name": "Yikang Li"
                },
                {
                    "authorId": "2064727535",
                    "name": "Jenhao Hsiao"
                },
                {
                    "authorId": "2097800792",
                    "name": "C. Ho"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                }
            ]
        }
    ]
}