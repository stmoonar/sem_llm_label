{
    "authorId": "145203884",
    "papers": [
        {
            "paperId": "16025740c52efdf75a990424acddd078d167d44c",
            "title": "Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning",
            "abstract": "The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the\"over-representation\"of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel\"disentangled\"decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thought that can suggest potential goals, provide suitable goal decomposition and subgoal allocation as well as self-reflection-based replanning. Additionally, SAMA incorporates language-grounded RL to train each agent's subgoal-conditioned policy. SAMA demonstrates considerable advantages in sample efficiency compared to state-of-the-art ASG methods, as evidenced by its performance on two challenging sparse-reward tasks, Overcooked and MiniRTS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108769630",
                    "name": "Wenhao Li"
                },
                {
                    "authorId": "2140207495",
                    "name": "Dan Qiao"
                },
                {
                    "authorId": "2156645203",
                    "name": "Baoxiang Wang"
                },
                {
                    "authorId": "47119103",
                    "name": "Xiangfeng Wang"
                },
                {
                    "authorId": "2057049328",
                    "name": "Bo Jin"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                }
            ]
        },
        {
            "paperId": "227aa3e266c5bb0dfcccf0f1dd18da2a85acb1d5",
            "title": "Discovering Temporal Patterns for Event Sequence Clustering via Policy Mixture Model (Extended Abstract)",
            "abstract": "We focus on the problem of event sequence clustering with different temporal patterns from the view of Reinforcement Learning (RL), whereby the observed sequences are assumed to be generated from a mixture of latent policies. We propose an Expectation-Maximization (EM) based algorithm to cluster the sequences with different temporal patterns into the underlying policies while simultaneously learning each of the policy model, in E-step estimating the cluster labels for each sequence, in M-step learning the respective policy. For each policy learning, we resort to Inverse Reinforcement Learning (IRL) by decomposing the observed sequence into states (hidden embedding of event history) and actions (time interval to next event) in order to learn a reward function. Experiments on synthetic and real-world datasets show the efficacy of our method against the state-of-the-arts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3243116",
                    "name": "Weichang Wu"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                },
                {
                    "authorId": "2159107948",
                    "name": "Xiaokang Yang"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                }
            ]
        },
        {
            "paperId": "320c729de74825d9e5cb8b7db3269e38b192c9d0",
            "title": "Learning to Boost Resilience of Complex Networks via Neural Edge Rewiring",
            "abstract": "The resilience of complex networks refers to their ability to maintain functionality in the face of structural attacks. This ability can be improved by performing minimal modifications to the network structure via degree-preserving edge rewiring-based methods. Existing learning-free edge rewiring methods, although effective, are limited in their ability to generalize to different graphs. Such a limitation cannot be trivially addressed by existing graph neural networks (GNNs)-based learning approaches since there is no rich initial node features for GNNs to learn meaningful representations. In this work, inspired by persistent homology, we specifically design a variant of GNN called FireGNN to learn meaningful node representations solely from graph structures. We then develop an end-to-end inductive method called ResiNet, which aims to discover resi lient net work topologies while balancing network utility. ResiNet reformulates the optimization of network resilience as a Markov decision process equipped with edge rewiring action space. It learns to sequentially select the appropriate edges to rewire for maximizing resilience. Extensive experiments demonstrate that ResiNet outperforms existing approaches and achieves near-optimal resilience gains on various graphs while balancing network",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268669",
                    "name": "Shanchao Yang"
                },
                {
                    "authorId": "47737190",
                    "name": "Kaili Ma"
                },
                {
                    "authorId": "2256865018",
                    "name": "Tianshu Yu"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                }
            ]
        },
        {
            "paperId": "4bbc2bb1d73cfde8a04ffe763374e5601da4b4e3",
            "title": "Learning Optimal \"Pigovian Tax\" in Sequential Social Dilemmas",
            "abstract": "In multi-agent reinforcement learning, each agent acts to maximize its individual accumulated rewards. Nevertheless, individual accumulated rewards could not fully reflect how others perceive them, resulting in selfish behaviors that undermine global performance. The externality theory, defined as ``the activities of one economic actor affect the activities of another in ways that are not reflected in market transactions,'' is applicable to analyze the social dilemmas in MARL. One of its most profound non-market solutions, ``Pigovian Tax'', which internalizes externalities by taxing those who create negative externalities and subsidizing those who create positive externalities, could aid in developing a mechanism to resolve MARL's social dilemmas. The purpose of this paper is to apply externality theory to analyze social dilemmas in MARL. To internalize the externalities in MARL, the \\textbf{L}earning \\textbf{O}ptimal \\textbf{P}igovian \\textbf{T}ax method (LOPT), is proposed, where an additional agent is introduced to learn the tax/allowance allocation policy so as to approximate the optimal ``Pigovian Tax'' which accurately reflects the externalities for all agents. Furthermore, a reward shaping mechanism based on the approximated optimal ``Pigovian Tax'' is applied to reduce the social cost of each agent and tries to alleviate the social dilemmas. Compared with existing state-of-the-art methods, the proposed LOPT leads to higher collective social welfare in both the Escape Room and the Cleanup environments, which shows the superiority of our method in solving social dilemmas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057200190",
                    "name": "Yun Hua"
                },
                {
                    "authorId": "2189316223",
                    "name": "Shang Gao"
                },
                {
                    "authorId": "2108769630",
                    "name": "Wenhao Li"
                },
                {
                    "authorId": "2057049328",
                    "name": "Bo Jin"
                },
                {
                    "authorId": "47119103",
                    "name": "Xiangfeng Wang"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                }
            ]
        },
        {
            "paperId": "53d661d536965daf4ff6dcfd3b7e42ffa9061d78",
            "title": "Diverse Policy Optimization for Structured Action Space",
            "abstract": "Enhancing the diversity of policies is beneficial for robustness, exploration, and transfer in reinforcement learning (RL). In this paper, we aim to seek diverse policies in an under-explored setting, namely RL tasks with structured action spaces with the two properties of composability and local dependencies. The complex action structure, non-uniform reward landscape, and subtle hyperparameter tuning due to the properties of structured actions prevent existing approaches from scaling well. We propose a simple and effective RL method, Diverse Policy Optimization (DPO), to model the policies in structured action space as the energy-based models (EBM) by following the probabilistic RL framework. A recently proposed novel and powerful generative model, GFlowNet, is introduced as the efficient, diverse EBM-based policy sampler. DPO follows a joint optimization framework: the outer layer uses the diverse policies sampled by the GFlowNet to update the EBM-based policies, which supports the GFlowNet training in the inner layer. Experiments on ATSC and Battle benchmarks demonstrate that DPO can efficiently discover surprisingly diverse policies in challenging scenarios and substantially outperform existing state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108769630",
                    "name": "Wenhao Li"
                },
                {
                    "authorId": "2156645203",
                    "name": "Baoxiang Wang"
                },
                {
                    "authorId": "2268669",
                    "name": "Shanchao Yang"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                }
            ]
        },
        {
            "paperId": "568a06279323d89b8e332fc193810343fff3a67c",
            "title": "The Characteristic Analysis of ESDD and NSDD Detection of Composite Insulators Based on Hyperspectral Technology",
            "abstract": "Pollution flashover of insulators has been viewed as a common issue existing in the area of power systems for a long term. However, traditional detection methods such as equivalent salt density and leakage current (LC) can hardly satisfy the requirements of the progressing smart grid. Herein, a novel highly-efficient method is proposed for classifying the pollution degree detection of insulators based on hyperspectral technology. The insulating sheets and actual insulators were set as the research object and their hyperspectral images are obtained. Moreover, the data for describing the characteristic wavelengths and the G color components of the samples were extracted from the images as the characterization of the equivalent salt deposit density (ESDD) and nonsoluble deposit density (NSDD) respectively. The ESDD and NSDD detection model is established based on the multiple linear regression (MLR) algorithm and the random forest (RF) algorithm. Ultimately, the proposed model reaches a detection accuracy of above 84% on the insulating sheets and 75% on the actual insulators, which can be viewed as acceptable. The hyperspectral imaging technique has considerable potential for the non-contact detection of ESDD and NSDD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                },
                {
                    "authorId": "49813784",
                    "name": "Yujun Guo"
                },
                {
                    "authorId": "49420525",
                    "name": "Yicen Liu"
                },
                {
                    "authorId": "2108001232",
                    "name": "Xueqin Zhang"
                },
                {
                    "authorId": "2112869094",
                    "name": "Song Xiao"
                },
                {
                    "authorId": "80934689",
                    "name": "Guoqiang Gao"
                },
                {
                    "authorId": "30870305",
                    "name": "Guangning Wu"
                }
            ]
        },
        {
            "paperId": "8024b875a5137a6215efcb5435c8a34a68c5fcb0",
            "title": "Negotiated Reasoning: On Provably Addressing Relative Over-Generalization",
            "abstract": "Over-generalization is a thorny issue in cognitive science, where people may become overly cautious due to past experiences. Agents in multi-agent reinforcement learning (MARL) also have been found to suffer relative over-generalization (RO) as people do and stuck to sub-optimal cooperation. Recent methods have shown that assigning reasoning ability to agents can mitigate RO algorithmically and empirically, but there has been a lack of theoretical understanding of RO, let alone designing provably RO-free methods. This paper first proves that RO can be avoided when the MARL method satisfies a consistent reasoning requirement under certain conditions. Then we introduce a novel reasoning framework, called negotiated reasoning, that first builds the connection between reasoning and RO with theoretical justifications. After that, we propose an instantiated algorithm, Stein variational negotiated reasoning (SVNR), which uses Stein variational gradient descent to derive a negotiation policy that provably avoids RO in MARL under maximum entropy policy iteration. The method is further parameterized with neural networks for amortized learning, making computation efficient. Numerical experiments on many RO-challenged environments demonstrate the superiority and efficiency of SVNR compared to state-of-the-art methods in addressing RO.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054250743",
                    "name": "Junjie Sheng"
                },
                {
                    "authorId": "2108769630",
                    "name": "Wenhao Li"
                },
                {
                    "authorId": "2057049328",
                    "name": "Bo Jin"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                },
                {
                    "authorId": "2152811657",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "47119103",
                    "name": "Xiangfeng Wang"
                }
            ]
        },
        {
            "paperId": "8cc09ea5032b3259c3c06e06643881b3355ad495",
            "title": "Learning Roles with Emergent Social Value Orientations",
            "abstract": "Social dilemmas can be considered situations where individual rationality leads to collective irrationality. The multi-agent reinforcement learning community has leveraged ideas from social science, such as social value orientations (SVO), to solve social dilemmas in complex cooperative tasks. In this paper, by first introducing the typical\"division of labor or roles\"mechanism in human society, we provide a promising solution for intertemporal social dilemmas (ISD) with SVOs. A novel learning framework, called Learning Roles with Emergent SVOs (RESVO), is proposed to transform the learning of roles into the social value orientation emergence, which is symmetrically solved by endowing agents with altruism to share rewards with other agents. An SVO-based role embedding space is then constructed by individual conditioning policies on roles with a novel rank regularizer and mutual information maximizer. Experiments show that RESVO achieves a stable division of labor and cooperation in ISDs with different complexity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108769630",
                    "name": "Wenhao Li"
                },
                {
                    "authorId": "47119103",
                    "name": "Xiangfeng Wang"
                },
                {
                    "authorId": "2057049328",
                    "name": "Bo Jin"
                },
                {
                    "authorId": "2215814043",
                    "name": "J. Lu"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                }
            ]
        },
        {
            "paperId": "92018bf2418a2276cdadff9b81b740c8b13d89c4",
            "title": "Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples",
            "abstract": "Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2173786903",
                    "name": "Shaokui Wei"
                },
                {
                    "authorId": "2365530",
                    "name": "Mingda Zhang"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                },
                {
                    "authorId": "143905981",
                    "name": "Baoyuan Wu"
                }
            ]
        },
        {
            "paperId": "9277614f4235ffdbca2b6cda4169d4a365cf4ca2",
            "title": "Mean Parity Fair Regression in RKHS",
            "abstract": "We study the fair regression problem under the notion of Mean Parity (MP) fairness, which requires the conditional mean of the learned function output to be constant with respect to the sensitive attributes. We address this problem by leveraging reproducing kernel Hilbert space (RKHS) to construct the functional space whose members are guaranteed to satisfy the fairness constraints. The proposed functional space suggests a closed-form solution for the fair regression problem that is naturally compatible with multiple sensitive attributes. Furthermore, by formulating the fairness-accuracy tradeoff as a relaxed fair regression problem, we derive a corresponding regression function that can be implemented efficiently and provides interpretable tradeoffs. More importantly, under some mild assumptions, the proposed method can be applied to regression problems with a covariance-based notion of fairness. Experimental results on benchmark datasets show the proposed methods achieve competitive and even superior performance compared with several state-of-the-art methods.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2173786903",
                    "name": "Shaokui Wei"
                },
                {
                    "authorId": "2208976905",
                    "name": "Jiayin Liu"
                },
                {
                    "authorId": "2152691760",
                    "name": "Bing Li"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                }
            ]
        }
    ]
}