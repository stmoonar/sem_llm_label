{
    "authorId": "1387484410",
    "papers": [
        {
            "paperId": "06a4491fadcb68a5d2f03110f9b54881dd8611e4",
            "title": "More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness",
            "abstract": "The surge in Large Language Models (LLMs) development has led to improved performance on cognitive tasks as well as an urgent need to align these models with human values in order to safely exploit their power. Despite the effectiveness of preference learning algorithms like Reinforcement Learning From Human Feedback (RLHF) in aligning human preferences, their assumed improvements on model trustworthiness haven't been thoroughly testified. Toward this end, this study investigates how models that have been aligned with general-purpose preference data on helpfulness and harmlessness perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. For model alignment, we focus on three widely used RLHF variants: Supervised Finetuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO). Through extensive empirical investigations, we discover that the improvement in trustworthiness by RLHF is far from guaranteed, and there exists a complex interplay between preference data, alignment algorithms, and specific trustworthiness aspects. Together, our results underscore the need for more nuanced approaches for model alignment. By shedding light on the intricate dynamics of these components within model alignment, we hope this research will guide the community towards developing language models that are both capable and trustworthy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299473032",
                    "name": "Aaron J. Li"
                },
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "157ed5647da39a7f5d33a84a90414b2a9e97e301",
            "title": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence",
            "abstract": "We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2295773183",
                    "name": "Bo Peng"
                },
                {
                    "authorId": "2295733582",
                    "name": "Daniel Goldstein"
                },
                {
                    "authorId": "2260401329",
                    "name": "Quentin Anthony"
                },
                {
                    "authorId": "2044198106",
                    "name": "Alon Albalak"
                },
                {
                    "authorId": "79046907",
                    "name": "Eric Alcaide"
                },
                {
                    "authorId": "103476203",
                    "name": "Stella Biderman"
                },
                {
                    "authorId": "2295732709",
                    "name": "Eugene Cheah"
                },
                {
                    "authorId": "2284067181",
                    "name": "Teddy Ferdinan"
                },
                {
                    "authorId": "2295734267",
                    "name": "Haowen Hou"
                },
                {
                    "authorId": "2274771600",
                    "name": "P. Kazienko"
                },
                {
                    "authorId": "101433524",
                    "name": "G. Kranthikiran"
                },
                {
                    "authorId": "2284064003",
                    "name": "Jan Koco'n"
                },
                {
                    "authorId": "2208962106",
                    "name": "Bartlomiej Koptyra"
                },
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "2295732467",
                    "name": "Ronald McClelland"
                },
                {
                    "authorId": "2037383772",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "2295732463",
                    "name": "Fares Obeid"
                },
                {
                    "authorId": "2186861874",
                    "name": "Atsushi Saito"
                },
                {
                    "authorId": "2295749628",
                    "name": "Guangyu Song"
                },
                {
                    "authorId": "2239200170",
                    "name": "Haoqin Tu"
                },
                {
                    "authorId": "2284064954",
                    "name": "Stanislaw Wo'zniak"
                },
                {
                    "authorId": "2218987422",
                    "name": "Ruichong Zhang"
                },
                {
                    "authorId": "2239378815",
                    "name": "Bingchen Zhao"
                },
                {
                    "authorId": "2295775910",
                    "name": "Qihang Zhao"
                },
                {
                    "authorId": "2296035480",
                    "name": "Peng Zhou"
                },
                {
                    "authorId": "2295965843",
                    "name": "Jian Zhu"
                },
                {
                    "authorId": "2239507803",
                    "name": "Ruijie Zhu"
                }
            ]
        },
        {
            "paperId": "46ff7e02fd4ff5fdfb9f85bc7071725b8089061f",
            "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
            "abstract": "Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "2311406638",
                    "name": "Kalpesh Krishna"
                },
                {
                    "authorId": "51228345",
                    "name": "Anhad Mohananey"
                },
                {
                    "authorId": "2321870362",
                    "name": "Steven Schwarcz"
                },
                {
                    "authorId": "2321871622",
                    "name": "Adam Stambler"
                },
                {
                    "authorId": "2254265068",
                    "name": "Shyam Upadhyay"
                },
                {
                    "authorId": "1779225",
                    "name": "Manaal Faruqui"
                }
            ]
        },
        {
            "paperId": "9fd10119173ea30350db76550c1cbfacf87ce9a2",
            "title": "Understanding the Effects of Iterative Prompting on Truthfulness",
            "abstract": "The development of Large Language Models (LLMs) has notably transformed numerous sectors, offering impressive text generation capabilities. Yet, the reliability and truthfulness of these models remain pressing concerns. To this end, we investigate iterative prompting, a strategy hypothesized to refine LLM responses, assessing its impact on LLM truthfulness, an area which has not been thoroughly explored. Our extensive experiments delve into the intricacies of iterative prompting variants, examining their influence on the accuracy and calibration of model responses. Our findings reveal that naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors. In response to these challenges, we introduce several prompting variants designed to address the identified issues. These variants demonstrate marked improvements over existing baselines, signaling a promising direction for future research. Our work provides a nuanced understanding of iterative prompting and introduces novel approaches to enhance the truthfulness of LLMs, thereby contributing to the development of more accurate and trustworthy AI systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "40228633",
                    "name": "Chirag Agarwal"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "d349f4e924f6cdcbafce75f159437920495cd8a1",
            "title": "Black-Box Access is Insufficient for Rigorous AI Audits",
            "abstract": "External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system\u2019s inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to training and deployment information (e.g., methodology, code, documentation, data, deployment details, findings from internal evaluations) allows auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physical, and legal safeguards for performing these audits with minimal security risks. Given that different forms of access can lead to very different levels of evaluation, we conclude that (1) transparency regarding the access and methods used by auditors is necessary to properly interpret audit results, and (2) white- and outside-the-box access allow for substantially more scrutiny than black-box access alone.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2103487700",
                    "name": "Stephen Casper"
                },
                {
                    "authorId": "2138961040",
                    "name": "Carson Ezell"
                },
                {
                    "authorId": "2281643565",
                    "name": "Charlotte Siegmann"
                },
                {
                    "authorId": "147680150",
                    "name": "Noam Kolt"
                },
                {
                    "authorId": "2281642640",
                    "name": "Taylor Lynn Curtis"
                },
                {
                    "authorId": "2257091099",
                    "name": "Ben Bucknall"
                },
                {
                    "authorId": "2281642834",
                    "name": "Andreas A. Haupt"
                },
                {
                    "authorId": "2280906435",
                    "name": "K. Wei"
                },
                {
                    "authorId": "2267505041",
                    "name": "J'er'emy Scheurer"
                },
                {
                    "authorId": "2267508917",
                    "name": "Marius Hobbhahn"
                },
                {
                    "authorId": "2267502247",
                    "name": "Lee Sharkey"
                },
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "2281641753",
                    "name": "Marvin von Hagen"
                },
                {
                    "authorId": "2281641748",
                    "name": "Silas Alberti"
                },
                {
                    "authorId": "2278422267",
                    "name": "Alan Chan"
                },
                {
                    "authorId": "2281713061",
                    "name": "Qinyi Sun"
                },
                {
                    "authorId": "103352265",
                    "name": "Michael Gerovitch"
                },
                {
                    "authorId": "2281641695",
                    "name": "David Bau"
                },
                {
                    "authorId": "2256989384",
                    "name": "Max Tegmark"
                },
                {
                    "authorId": "2276204785",
                    "name": "David Krueger"
                },
                {
                    "authorId": "1397904824",
                    "name": "Dylan Hadfield-Menell"
                }
            ]
        },
        {
            "paperId": "70017c9096cca317942156e3f91f8141da42eef1",
            "title": "In-Context Explainers: Harnessing LLMs for Explaining Black Box Models",
            "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated exceptional capabilities in complex tasks like machine translation, commonsense reasoning, and language understanding. One of the primary reasons for the adaptability of LLMs in such diverse tasks is their in-context learning (ICL) capability, which allows them to perform well on new tasks by simply using a few task samples in the prompt. Despite their effectiveness in enhancing the performance of LLMs on diverse language and tabular tasks, these methods have not been thoroughly explored for their potential to generate post hoc explanations. In this work, we carry out one of the first explorations to analyze the effectiveness of LLMs in explaining other complex predictive models using ICL. To this end, we propose a novel framework, In-Context Explainers, comprising of three novel approaches that exploit the ICL capabilities of LLMs to explain the predictions made by other predictive models. We conduct extensive analysis with these approaches on real-world tabular and text datasets and demonstrate that LLMs are capable of explaining other predictive models similar to state-of-the-art post hoc explainers, opening up promising avenues for future research into LLM-based post hoc explanations of complex predictive models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256987587",
                    "name": "Nicholas Kroeger"
                },
                {
                    "authorId": "2256981456",
                    "name": "Dan Ley"
                },
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "40228633",
                    "name": "Chirag Agarwal"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "b3abcf5aaa7a754bdb93c1f727d77108f5180507",
            "title": "Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten",
            "abstract": "The Right to Explanation and the Right to be Forgotten are two important principles outlined to regulate algorithmic decision making and data usage in real-world applications. While the right to explanation allows individuals to request an actionable explanation for an algorithmic decision, the right to be forgotten grants them the right to ask for their data to be deleted from all the databases and models of an organization. Intuitively, enforcing the right to be forgotten may trigger model updates which in turn invalidate previously provided explanations, thus violating the right to explanation. In this work, we investigate the technical implications arising due to the interference between the two aforementioned regulatory principles, and propose the first algorithmic framework to resolve the tension between them. To this end, we formulate a novel optimization problem to generate explanations that are robust to model updates due to the removal of training data instances by data deletion requests. We then derive an efficient approximation algorithm to handle the combinatorial complexity of this optimization problem. We theoretically demonstrate that our method generates explanations that are provably robust to worst-case data deletion requests with bounded costs in case of linear models and certain classes of non-linear models. Extensive experimentation with real-world datasets demonstrates the efficacy of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "47793019",
                    "name": "Jiaqi Ma"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "ce279bb0cb7c78423ab37a4430d73e857df4beda",
            "title": "On the Trade-offs between Adversarial Robustness and Actionable Explanations",
            "abstract": "As machine learning models are increasingly being employed in various high-stakes settings, it becomes important to ensure that predictions of these models are not only adversarially robust, but also readily explainable to relevant stakeholders. However, it is unclear if these two notions can be simultaneously achieved or if there exist trade-offs between them. In this work, we make one of the first attempts at studying the impact of adversarially robust models on actionable explanations which provide end users with a means for recourse. We theoretically and empirically analyze the cost (ease of implementation) and validity (probability of obtaining a positive model prediction) of recourses output by state-of-the-art algorithms when the underlying models are adversarially robust vs. non-robust. More specifically, we derive theoretical bounds on the differences between the cost and the validity of the recourses generated by state-of-the-art algorithms for adversarially robust vs. non-robust linear and non-linear models. Our empirical results with multiple real-world datasets validate our theoretical results and show the impact of varying degrees of model robustness on the cost and validity of the resulting recourses. Our analyses demonstrate that adversarially robust models significantly increase the cost and reduce the validity of the resulting recourses, thus shedding light on the inherent trade-offs between adversarial robustness and actionable explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "40228633",
                    "name": "Chirag Agarwal"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "e5754bb65a648f319a02d47c356df0db1e936b7f",
            "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, leads to critical insights for refining in-context learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "47793019",
                    "name": "Jiaqi Ma"
                },
                {
                    "authorId": "153794305",
                    "name": "Dylan Slack"
                },
                {
                    "authorId": "2214185",
                    "name": "Asma Ghandeharioun"
                },
                {
                    "authorId": "144171580",
                    "name": "Sameer Singh"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "442d76268837376f9372d142bd1f99065bad0013",
            "title": "TalkToModel: Understanding Machine Learning Models With Open Ended Dialogues",
            "abstract": "Machine Learning (ML) models are increasingly used to make critical decisions in real-world applications, yet they have become more complex, making them harder to understand. To this end, researchers have proposed several techniques to explain model predictions. However, practitioners struggle to use explanations because they often do not know which one to choose and how to interpret the results. In this work, we address these challenges by introducing TalkToModel: an open-ended dialogue system for understanding machine learning models. Speci\ufb01cally, TalkToModel comprises of three key components: 1) a natural language interface for engaging in dialogues, making understanding ML models highly accessible, 2) a dialogue engine that adapts to any tabular model and dataset, interprets natural language, maps it to appropriate explanations, and generates text responses, and 3) an execution component that constructs the explanations. We carried out extensive quantitative and human subject evaluations of TalkToModel. Overall, we found the system understands user inputs on novel datasets and models with high accuracy, demonstrating the system\u2019s capacity to generalize to new situations. In human evaluations, 73% of healthcare workers (e.g., doctors and nurses) agreed they would use TalkToModel over baseline point-and-click systems, and 85% of ML professionals agreed TalkToModel was easier to use. Our \ufb01ndings demonstrate that TalkToModel is more e\ufb00ective for model understanding than existing systems, presenting a new category of model understanding tools for practitioners. splits in the data where the GPT-J few shot models never exceed 10% parsing accuracy. Overall, these results indicate TalkToModel can understanding user utterances with a high degree of accuracy using our best performing T 5 models. Further, we",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153794305",
                    "name": "Dylan Slack"
                },
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                },
                {
                    "authorId": "34650964",
                    "name": "Sameer Singh"
                }
            ]
        }
    ]
}