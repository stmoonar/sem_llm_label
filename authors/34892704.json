{
    "authorId": "34892704",
    "papers": [
        {
            "paperId": "1bbb96d950e3e6a5f300dff390d3f4bd2782e0e4",
            "title": "Unearthing the Potential of Spiking Neural Networks",
            "abstract": "Spiking neural networks (SNNs) offer a promising alternative to traditional analog neural networks (ANNs), especially for sequential tasks, with enhanced energy efficiency. The internal memory in SNNs obtained through the membrane potential equips them with innate lightweight temporal processing capabilities. However, the unique advantages of this temporal dimension of SNN s have not yet been effectively harnessed. To that end, this article delves deeper into the what, why and where of SNNs. By considering event-based optical flow as an exemplary task in vision-based navigation, we highlight that the true potential of SNNs lies in sequential tasks. The event-driven recurrent dynamics of a spiking neuron merged harmoniously with event camera inputs enables SNNs to outperform corresponding ANNs with a lower number of parameters for optical flow. Furthermore, we demonstrate that SNNs can be synergistically combined with ANNs to form SNN-ANN hybrids to obtain the best of both worlds in terms of accuracy, energy, memory, and training efficiency. Additionally\u2019 the emergence of various near-memory and in-memory computing techniques has propelled efficient implementation of these approaches. Overall, the immediate future of SNNs looks exciting, as we discover the niche of SNN s, comprising sequential tasks with low power requirements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9733229",
                    "name": "Sayeed Shafayet Chowdhury"
                },
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "2219272334",
                    "name": "Deepika Sharma"
                },
                {
                    "authorId": "147403851",
                    "name": "M. Apolinario"
                },
                {
                    "authorId": "2281943686",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "54db271886c55bbdf07bca783e0c2a303a417e46",
            "title": "SHIRE: Enhancing Sample Efficiency using Human Intuition in REinforcement Learning",
            "abstract": "The ability of neural networks to perform robotic perception and control tasks such as depth and optical flow estimation, simultaneous localization and mapping (SLAM), and automatic control has led to their widespread adoption in recent years. Deep Reinforcement Learning has been used extensively in these settings, as it does not have the unsustainable training costs associated with supervised learning. However, DeepRL suffers from poor sample efficiency, i.e., it requires a large number of environmental interactions to converge to an acceptable solution. Modern RL algorithms such as Deep Q Learning and Soft Actor-Critic attempt to remedy this shortcoming but can not provide the explainability required in applications such as autonomous robotics. Humans intuitively understand the long-time-horizon sequential tasks common in robotics. Properly using such intuition can make RL policies more explainable while enhancing their sample efficiency. In this work, we propose SHIRE, a novel framework for encoding human intuition using Probabilistic Graphical Models (PGMs) and using it in the Deep RL training pipeline to enhance sample efficiency. Our framework achieves 25-78% sample efficiency gains across the environments we evaluate at negligible overhead cost. Additionally, by teaching RL agents the encoded elementary behavior, SHIRE enhances policy explainability. A real-world demonstration further highlights the efficacy of policies trained using our framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309435790",
                    "name": "Amogh Joshi"
                },
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "2309180945",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "52251970e022c47f3cdea57d5a1d1aef4bce4366",
            "title": "FEDORA: Flying Event Dataset fOr Reactive behAvior",
            "abstract": "The ability of resource-constrained biological systems such as fruitflies to perform complex and high-speed maneuvers in cluttered environments has been one of the prime sources of inspiration for developing vision-based autonomous systems. To emulate this capability, the perception pipeline of such systems must integrate information cues from tasks including optical flow and depth estimation, object detection and tracking, and segmentation, among others. However, the conventional approach of employing slow, synchronous inputs from standard frame-based cameras constrains these perception capabilities, particularly during high-speed maneuvers. Recently, event-based sensors have emerged as low latency and low energy alternatives to standard frame-based cameras for capturing high-speed motion, effectively speeding up perception and hence navigation. For coherence, all the perception tasks must be trained on the same input data. However, present-day datasets are curated mainly for a single or a handful of tasks and are limited in the rate of the provided ground truths. To address these limitations, we present Flying Event Dataset fOr Reactive behAviour (FEDORA) - a fully synthetic dataset for perception tasks, with raw data from frame-based cameras, event-based cameras, and Inertial Measurement Units (IMU), along with ground truths for depth, pose, and optical flow at a rate much higher than existing datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Amogh Joshi"
                },
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "10715322",
                    "name": "Wachirawit Ponghiran"
                },
                {
                    "authorId": "153839324",
                    "name": "M. Nagaraj"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "7961259eb91273413c5195d9ba39e19c38cdb7e3",
            "title": "AcouSkin: Full Surface Contact localization Using Acoustic Waves",
            "abstract": "Contact sensing and localization capabilities that mimic human skin are highly desirable for robots. In this paper, we introduce AcouSkin, an acoustic wave based full surface contact localization system. Acoustic waves produced by piezoelectric transceivers using a monotone are coupled to surfaces turning them into an active sensor. Our system leverages information from four piezoelectric transceivers mounted on the surface of an acrylic sheet and vacuum cleaner robot bumper to localize contacts to 18 unique segments. We first characterize acoustic wave propagation based on signal and material properties and then propose hardware and software methods to realize full surface contact localization. Our results show that AcouSkin can reliably localize contact on a flat acrylic sheet with 18 uniformly spaced locations across a 54cm length with mean absolute error (MAE) of \u2264 1 locations using maximum likelihood estimator (MLE) and multilayer perceptron (MLP) models. On the vacuum cleaner robot bumper AcouSkin shows a zero MAE. Further, the system is also able to localize contacts made using forces as low as 2N (Newtons) and as high as 20N. Overall, AcouSkin provides full surface contact localization while requiring minimal instrumentation with easy deployment on real-world robots.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "2273975027",
                    "name": "Alexis Burns"
                },
                {
                    "authorId": "2273976843",
                    "name": "Siddharth Rupavarharam"
                },
                {
                    "authorId": "2273991753",
                    "name": "Caleb Escobedo"
                },
                {
                    "authorId": "2274712508",
                    "name": "Daewon Lee"
                },
                {
                    "authorId": "2273989844",
                    "name": "Richard Howard"
                },
                {
                    "authorId": "113588307",
                    "name": "Larry Jackel"
                },
                {
                    "authorId": "1698835",
                    "name": "Volkan Isler"
                }
            ]
        },
        {
            "paperId": "9828b7d68ecd097871001761e1d1b13370d6f7d5",
            "title": "Lightning Talk: A Perspective on Neuromorphic Computing",
            "abstract": "Neuromorphic computing, based on Spiking Neural Networks (SNNs), has recently gained immense popularity in machine learning community. It aims to offer reduced learning complexity, energy and latency through sparse event-driven computations, enabling real-time and sequential edge applications. However, due to their asynchronous spatio-temporal compute, SNNs require specialized sensing as well as algorithms and are not compatible with deployment on standard machine learning hardware such as GPUs. To that effect, there needs to be an end-to-end paradigm shift, from sensors to learning algorithms to the underlying hardware architectures. In this paper, we provide a perspective on the various efforts by the research community towards overcoming these challenges and realizing truly brain-inspired efficient machine intelligence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219272334",
                    "name": "Deepika Sharma"
                },
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "2061563319",
                    "name": "K. Roy"
                }
            ]
        },
        {
            "paperId": "cf1851b98bfafd30b1feaddf4d06e62bd5840266",
            "title": "Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical Flow Estimation",
            "abstract": "In the field of robotics, event-based cameras are emerging as a promising low-power alternative to traditional frame-based cameras for capturing high-speed motion and high dynamic range scenes. This is due to their sparse and asynchronous event outputs. Spiking Neural Networks (SNNs) with their asynchronous event-driven compute, show great potential for extracting the spatio-temporal features from these event streams. In contrast, the standard Analog Neural Networks (ANNs) fail to process event data effectively. However, training SNNs is difficult due to additional trainable parameters (thresholds and leaks), vanishing spikes at deeper layers, and a non-differentiable binary activation function. Furthermore, an additional data structure, membrane potential, responsible for keeping track of temporal information, must be fetched and updated at every timestep in SNNs. To overcome these challenges, we propose a novel SNN-ANN hybrid architecture that combines the strengths of both. Specifically, we leverage the asynchronous compute capabilities of SNN layers to effectively extract the input temporal information. Concurrently, the ANN layers facilitate training and efficient hardware deployment on traditional machine learning hardware such as GPUs. We provide extensive experimental analysis for assigning each layer to be spiking or analog, leading to a network configuration optimized for performance and ease of training. We evaluate our hybrid architecture for optical flow estimation on DSEC-flow and Multi-Vehicle Stereo Event-Camera (MVSEC) datasets. On the DSEC-flow dataset, the hybrid SNN-ANN architecture achieves a 40% reduction in average endpoint error (AEE) with 22% lower energy consumption compared to Full-SNN, and 48% lower AEE compared to Full-ANN, while maintaining comparable energy usage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182565552",
                    "name": "Shubham Negi"
                },
                {
                    "authorId": "2219272334",
                    "name": "Deepika Sharma"
                },
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "2061563319",
                    "name": "K. Roy"
                }
            ]
        },
        {
            "paperId": "0bd1c72ec69f43c21708453723f99852df7fb294",
            "title": "HyperX: A Hybrid RRAM-SRAM partitioned system for error recovery in memristive Xbars",
            "abstract": "Memristive crossbars based on Non-volatile Memory (NVM) technologies such as RRAM, have recently shown great promise for accelerating Deep Neural Networks (DNNs). They achieve this by performing efficient Matrix-Vector-Multiplications (MVMs) while offering dense on-chip storage and minimal off-chip data movement. However, their analog nature of computing introduces functional errors due to non-ideal RRAM devices, significantly degrading the application accuracy. Further, RRAMs suffer from low endurance and high write costs, hindering on-chip trainability. To alleviate these limitations, we propose HyperX, a hybrid RRAM-SRAM system that leverages the complementary benefits of NVM and CMOS technologies. Our proposed system consists of a fixed RRAM block offering area and energy-efficient MVMs and an SRAM block enabling on-chip training to recover the accuracy drop due to the RRAM non-idealities. The improvements are reported in terms of energy and product of latency and area ${\\left(ms\\,\\times \\,mm^{2}\\right)}$, termed as area-normalized latency. Our experiments on CIFAR datasets using ResNet-20 show up to 2.88 \u00d7 and 10.1 \u00d7 improvements in inference energy and area-normalized latency, respectively. In addition, for a transfer learning task from ImageNet to CIFAR datasets using ResNet-18, we observe up to 1.58 \u00d7 and 4.48 \u00d7 improvements in energy and area-normalized latency, respectively. These improvements are with respect to an all-SRAM baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "117455765",
                    "name": "Efstathia Soufleri"
                },
                {
                    "authorId": "1832280",
                    "name": "I. Chakraborty"
                },
                {
                    "authorId": "10614519",
                    "name": "Amogh Agrawal"
                },
                {
                    "authorId": "3468125",
                    "name": "Aayush Ankit"
                },
                {
                    "authorId": "2257216834",
                    "name": "K. Roy"
                }
            ]
        },
        {
            "paperId": "0d89db012ade12105567fb17a51ceeebe0959f5b",
            "title": "Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking Neural Networks with Learnable Neuronal Dynamics",
            "abstract": "Event-based cameras have recently shown great potential for high-speed motion estimation owing to their ability to capture temporally rich information asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired event-driven processing can efficiently handle such asynchronous data, while neuron models such as the leaky-integrate and fire (LIF) can keep track of the quintessential timing information contained in the inputs. SNNs achieve this by maintaining a dynamic state in the neuron memory, retaining important information while forgetting redundant data over time. Thus, we posit that SNNs would allow for better performance on sequential regression tasks compared to similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult to train due to vanishing spikes at later layers. To that effect, we propose an adaptive fully-spiking framework with learnable neuronal dynamics to alleviate the spike vanishing problem. We utilize surrogate gradient-based backpropagation through time (BPTT) to train our deep SNNs from scratch. We validate our approach for the task of optical flow estimation on the Multi-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset. Our experiments on these datasets show an average reduction of \u223c 13% in average endpoint error (AEE) compared to state-of-the-art ANNs. We also explore several down-scaled models and observe that our SNN models consistently outperform similarly sized ANNs offering \u223c10%-16% lower AEE. These results demonstrate the importance of SNNs for smaller models and their suitability at the edge. In terms of efficiency, our SNNs offer substantial savings in network parameters (\u223c 48.3 \u00d7) and computational energy (\u223c 10.2 \u00d7) while attaining \u223c 10% lower EPE compared to the state-of-the-art ANN implementations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "7f26737e49e6b7393985b0036b913cd3a510f44e",
            "title": "Hardware/Software Co-Design With ADC-Less In-Memory Computing Hardware for Spiking Neural Networks",
            "abstract": "Spiking Neural Networks (SNNs) are bio-plausible models that hold great potential for realizing energy-efficient implementations of sequential tasks on resource-constrained edge devices. However, commercial edge platforms based on standard GPUs are not optimized to deploy SNNs, resulting in high energy and latency. While analog In-Memory Computing (IMC) platforms can serve as energy-efficient inference engines, they are accursed by the immense energy, latency, and area requirements of high-precision ADCs (HP-ADC), overshadowing the benefits of in-memory computations. We propose a hardware/software co-design methodology to deploy SNNs into an ADC-Less IMC architecture using sense-amplifiers as 1-bit ADCs replacing conventional HP-ADCs and alleviating the above issues. Our proposed framework incurs minimal accuracy degradation by performing hardware-aware training and is able to scale beyond simple image classification tasks to more complex sequential regression tasks. Experiments on complex tasks of optical flow estimation and gesture recognition show that progressively increasing the hardware awareness during SNN training allows the model to adapt and learn the errors due to the non-idealities associated with ADC-Less IMC. Also, the proposed ADC-Less IMC offers significant energy and latency improvements, <inline-formula><tex-math notation=\"LaTeX\">$2-7\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mn>7</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"apolinario-ieq1-3316121.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$8.9-24.6\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>8</mml:mn><mml:mo>.</mml:mo><mml:mn>9</mml:mn><mml:mo>-</mml:mo><mml:mn>24</mml:mn><mml:mo>.</mml:mo><mml:mn>6</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"apolinario-ieq2-3316121.gif\"/></alternatives></inline-formula>, respectively, depending on the SNN model and the workload, compared to HP-ADC IMC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "147403851",
                    "name": "M. Apolinario"
                },
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "2059786774",
                    "name": "Utkarsh Saxena"
                },
                {
                    "authorId": "2257216834",
                    "name": "K. Roy"
                }
            ]
        },
        {
            "paperId": "af93dded69553e5da704cc7925998b24c970c3a0",
            "title": "HALSIE - Hybrid Approach to Learning Segmentation by Simultaneously Exploiting Image and Event Modalities",
            "abstract": "Standard frame-based algorithms fail to retrieve accurate segmentation maps in challenging real-time applica-tions like autonomous navigation, owing to the limited dynamic range and motion blur prevalent in traditional cameras. Event cameras address these limitations by asynchronously detecting changes in per-pixel intensity to generate event streams with high temporal resolution, high dynamic range, and no motion blur. However, event camera outputs cannot be directly used to generate reliable segmentation maps as they only capture information at the pixels in motion. To augment the missing contextual information, we postulate that fusing spatially dense frames with temporally dense events can generate semantic maps with \ufb01ne-grained predictions. To this end, we propose HALSIE, a hybrid approach to learning segmentation by simultaneously leveraging image and event modalities. To enable ef\ufb01cient learning across modalities, our proposed hybrid framework comprises two input branches, a Spiking Neural Network (SNN) branch and a standard Arti\ufb01cial Neural Network (ANN) branch to process event and frame data respectively, while exploiting their corresponding neural dynamics. Our hybrid network outperforms the state-of-the-art semantic segmentation benchmarks on DDD17 and MVSEC datasets and shows comparable performance on the DSEC-Semantic dataset with upto 33.23 \u00d7 reduction in network parameters. Further, our method shows upto 18.92 \u00d7 improvement in inference cost compared to existing SOTA approaches, making it suitable for resource-constrained edge applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1508703604",
                    "name": "Shristi Das Biswas"
                },
                {
                    "authorId": "34892704",
                    "name": "A. Kosta"
                },
                {
                    "authorId": "7992565",
                    "name": "C. Liyanagedera"
                },
                {
                    "authorId": "147403851",
                    "name": "M. Apolinario"
                },
                {
                    "authorId": "2257216842",
                    "name": "Kaushik Roy"
                }
            ]
        }
    ]
}