{
    "authorId": "2262444503",
    "papers": [
        {
            "paperId": "0a2cdb13a15d95a5ab05aa5ec921518aa3655e93",
            "title": "All Against Some: Efficient Integration of Large Language Models for Message Passing in Graph Neural Networks",
            "abstract": "Graph Neural Networks (GNNs) have attracted immense attention in the past decade due to their numerous real-world applications built around graph-structured data. On the other hand, Large Language Models (LLMs) with extensive pretrained knowledge and powerful semantic comprehension abilities have recently shown a remarkable ability to benefit applications using vision and text data. In this paper, we investigate how LLMs can be leveraged in a computationally efficient fashion to benefit rich graph-structured data, a modality relatively unexplored in LLM literature. Prior works in this area exploit LLMs to augment every node features in an ad-hoc fashion (not scalable for large graphs), use natural language to describe the complex structural information of graphs, or perform computationally expensive finetuning of LLMs in conjunction with GNNs. We propose E-LLaGNN (Efficient LLMs augmented GNNs), a framework with an on-demand LLM service that enriches message passing procedure of graph learning by enhancing a limited fraction of nodes from the graph. More specifically, E-LLaGNN relies on sampling high-quality neighborhoods using LLMs, followed by on-demand neighborhood feature enhancement using diverse prompts from our prompt catalog, and finally information aggregation using message passing from conventional GNN architectures. We explore several heuristics-based active node selection strategies to limit the computational and memory footprint of LLMs when handling millions of nodes. Through extensive experiments&ablation on popular graph benchmarks of varying scales (Cora, PubMed, ArXiv,&Products), we illustrate the effectiveness of our E-LLaGNN framework and reveal many interesting capabilities such as improved gradient flow in deep GNNs, LLM-free inference ability etc.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2253397454",
                    "name": "A. Jaiswal"
                },
                {
                    "authorId": "2262444503",
                    "name": "Nurendra Choudhary"
                },
                {
                    "authorId": "2299329980",
                    "name": "Ravinarayana Adkathimar"
                },
                {
                    "authorId": "1390172802",
                    "name": "M. P. Alagappan"
                },
                {
                    "authorId": "46566733",
                    "name": "G. Hiranandani"
                },
                {
                    "authorId": "2279770479",
                    "name": "Ying Ding"
                },
                {
                    "authorId": "2254949434",
                    "name": "Zhangyang Wang"
                },
                {
                    "authorId": "2057479333",
                    "name": "E-Wen Huang"
                },
                {
                    "authorId": "2691095",
                    "name": "Karthik Subbian"
                }
            ]
        },
        {
            "paperId": "416ce070e8924163afdbcf9966d0431f37641ad0",
            "title": "An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce",
            "abstract": "The problem of search relevance in the E-commerce domain is a challenging one since it involves understanding the intent of a user's short nuanced query and matching it with the appropriate products in the catalog. This problem has traditionally been addressed using language models (LMs) and graph neural networks (GNNs) to capture semantic and inter-product behavior signals, respectively. However, the rapid development of new architectures has created a gap between research and the practical adoption of these techniques. Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare the effectiveness of different models. This lack of interpretability hinders the development and adoption of new techniques in the field. To bridge this gap, we propose Plug and Play Graph LAnguage Model (PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a modular framework with uniform data processing pipelines. It employs additive explanation metrics to independently decide whether to include (i) language model candidates, (ii) GNN model candidates, and (iii) inter-product behavioral signals. For the task of search relevance, we show that PP-GLAM outperforms several state-of-the-art baselines as well as a proprietary model on real-world multilingual, multi-regional e-commerce datasets. To promote better model comprehensibility and adoption, we also provide an analysis of the explainability and computational complexity of our model. We also provide the public codebase and provide a deployment strategy for practical implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262444503",
                    "name": "Nurendra Choudhary"
                },
                {
                    "authorId": "2057479333",
                    "name": "E-Wen Huang"
                },
                {
                    "authorId": "2691095",
                    "name": "Karthik Subbian"
                },
                {
                    "authorId": "2262444977",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "b4f8117309262f04af989f107ab980d64c92fd8f",
            "title": "Context-Aware Clustering using Large Language Models",
            "abstract": "Despite the remarkable success of Large Language Models (LLMs) in text understanding and generation, their potential for text clustering tasks remains underexplored. We observed that powerful closed-source LLMs provide good quality clusterings of entity sets but are not scalable due to the massive compute power required and the associated costs. Thus, we propose CACTUS (Context-Aware ClusTering with aUgmented triplet losS), a systematic approach that leverages open-source LLMs for efficient and effective supervised clustering of entity subsets, particularly focusing on text-based entities. Existing text clustering methods fail to effectively capture the context provided by the entity subset. Moreover, though there are several language modeling based approaches for clustering, very few are designed for the task of supervised clustering. This paper introduces a novel approach towards clustering entity subsets using LLMs by capturing context via a scalable inter-entity attention mechanism. We propose a novel augmented triplet loss function tailored for supervised clustering, which addresses the inherent challenges of directly applying the triplet loss to this problem. Furthermore, we introduce a self-supervised clustering task based on text augmentation techniques to improve the generalization of our model. For evaluation, we collect ground truth clusterings from a closed-source LLM and transfer this knowledge to an open-source LLM under the supervised clustering framework, allowing a faster and cheaper open-source model to perform the same task. Experiments on various e-commerce query and product clustering datasets demonstrate that our proposed approach significantly outperforms existing unsupervised and supervised baselines under various external clustering evaluation metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121914227",
                    "name": "Sindhu Tipirneni"
                },
                {
                    "authorId": "2299329980",
                    "name": "Ravinarayana Adkathimar"
                },
                {
                    "authorId": "2262444503",
                    "name": "Nurendra Choudhary"
                },
                {
                    "authorId": "46566733",
                    "name": "G. Hiranandani"
                },
                {
                    "authorId": "2370909",
                    "name": "Rana Ali Amjad"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2299926084",
                    "name": "Changhe Yuan"
                },
                {
                    "authorId": "2262444977",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "883dab9410349f0660c6d6b05e4409525185f0c3",
            "title": "Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach",
            "abstract": "The progress in hyperbolic neural networks (HNNs) research is hindered by their absence of inductive bias mechanisms, which are essential for generalizing to new tasks and facilitating scalable learning over large datasets. In this paper, we aim to alleviate these issues by learning generalizable inductive biases from the nodes' local subgraph and transfer them for faster learning over new subgraphs with a disjoint set of nodes, edges, and labels in a few-shot setting. We introduce a novel method, Hyperbolic GRAph Meta Learner (H-GRAM), that, for the tasks of node classification and link prediction, learns transferable information from a set of support local subgraphs in the form of hyperbolic meta gradients and label hyperbolic protonets to enable faster learning over a query set of new tasks dealing with disjoint subgraphs. Furthermore, we show that an extension of our meta-learning framework also mitigates the scalability challenges seen in HNNs faced by existing approaches. Our comparative analysis shows that H-GRAM effectively learns and transfers information in multiple challenging few-shot settings compared to other state-of-the-art baselines. Additionally, we demonstrate that, unlike standard HNNs, our approach is able to scale over large graph datasets and improve performance over its Euclidean counterparts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262444503",
                    "name": "Nurendra Choudhary"
                },
                {
                    "authorId": "2262445672",
                    "name": "Nikhil Rao"
                },
                {
                    "authorId": "2262444977",
                    "name": "Chandan K. Reddy"
                }
            ]
        }
    ]
}