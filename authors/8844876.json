{
    "authorId": "8844876",
    "papers": [
        {
            "paperId": "5e0dbdd81493b90d0dadc37863b89374a82549d1",
            "title": "WINOVIZ: Probing Visual Properties of Objects Under Different States",
            "abstract": "Humans interpret visual aspects of objects based on contexts. For example, a banana appears brown when rotten and green when unripe. Previous studies focused on language models\u2019 grasp of typical object properties. We introduce WINOVIZ, a text-only dataset with 1,380 examples of probing language models\u2019 reasoning about diverse visual properties under different contexts. Our task demands pragmatic and visual knowledge reasoning. We also present multi-hop data, a more challenging version requiring multi-step reasoning chains. Experimental findings include: a) GPT-4 excels overall but struggles with multi-hop data. b) Large models perform well in pragmatic reasoning but struggle with visual knowledge reasoning. c) Vision-language models outperform language-only models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "150324514",
                    "name": "Tejas Srinivasan"
                },
                {
                    "authorId": "2284872321",
                    "name": "Jesse Thomason"
                },
                {
                    "authorId": "2284927335",
                    "name": "Xiang Ren"
                }
            ]
        },
        {
            "paperId": "9dd8d84874b691c0c44fa32873628b6f729520d9",
            "title": "GRILL: Grounded Vision-language Pre-training via Aligning Text and Image Regions",
            "abstract": "Generalization to unseen tasks is an important ability for few-shot learners to achieve better zero-/few-shot performance on diverse tasks. However, such generalization to vision-language tasks including grounding and generation tasks has been under-explored; existing few-shot VL models struggle to handle tasks that involve object grounding and multiple images such as visual commonsense reasoning or NLVR2. In this paper, we introduce GRILL, GRounded vIsion Language aLigning, a novel VL model that can be generalized to diverse tasks including visual question answering, captioning, and grounding tasks with no or very few training instances. Specifically, GRILL learns object grounding and localization by exploiting object-text alignments, which enables it to transfer to grounding tasks in a zero-/few-shot fashion. We evaluate our model on various zero-/few-shot VL tasks and show that it consistently surpasses the state-of-the-art few-shot methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "2153510147",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "1752875",
                    "name": "Yelong Shen"
                },
                {
                    "authorId": "2109136147",
                    "name": "Weizhu Chen"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                },
                {
                    "authorId": "144430856",
                    "name": "Damien Jose"
                },
                {
                    "authorId": "1384550891",
                    "name": "Xiang Ren"
                }
            ]
        },
        {
            "paperId": "b5aad38122c14f4287c5feca20b7017fda50d7c0",
            "title": "Hybrid forecasting of geopolitical events\u2020",
            "abstract": "Sound decision-making relies on accurate prediction for tangible outcomes ranging from military conflict to disease outbreaks. To improve crowdsourced forecasting accuracy, we developed SAGE, a hybrid forecasting system that combines human and machine generated forecasts. The system provides a platform where users can interact with machine models and thus anchor their judgments on an objective",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052360522",
                    "name": "Daniel M. Benjamin"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "32494669",
                    "name": "A. Abbas"
                },
                {
                    "authorId": "2919118",
                    "name": "A. Abeliuk"
                },
                {
                    "authorId": "2213032239",
                    "name": "Pavel Atanasov"
                },
                {
                    "authorId": "1667696997",
                    "name": "Stephen T. Bennett"
                },
                {
                    "authorId": "52249013",
                    "name": "Andreas Beger"
                },
                {
                    "authorId": "2088006944",
                    "name": "Saurabh Birari"
                },
                {
                    "authorId": "3275470",
                    "name": "D. Budescu"
                },
                {
                    "authorId": "1754926",
                    "name": "Michele Catasta"
                },
                {
                    "authorId": "48898287",
                    "name": "Emilio Ferrara"
                },
                {
                    "authorId": "2213306048",
                    "name": "Lucas Haravitch"
                },
                {
                    "authorId": "115201201",
                    "name": "Mark Himmelstein"
                },
                {
                    "authorId": "144022002",
                    "name": "K. T. Hossain"
                },
                {
                    "authorId": "35633657",
                    "name": "Yuzhong Huang"
                },
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "51892568",
                    "name": "R. Joseph"
                },
                {
                    "authorId": "1702139",
                    "name": "J. Leskovec"
                },
                {
                    "authorId": "50390828",
                    "name": "Akira Matsui"
                },
                {
                    "authorId": "35416435",
                    "name": "Mehrnoosh Mirtaheri"
                },
                {
                    "authorId": "145201124",
                    "name": "Xiang Ren"
                },
                {
                    "authorId": "1840215",
                    "name": "Gleb Satyukov"
                },
                {
                    "authorId": "31928125",
                    "name": "Rajiv Sethi"
                },
                {
                    "authorId": "2116288277",
                    "name": "Amandeep Singh"
                },
                {
                    "authorId": "48523334",
                    "name": "R. Sosi\u010d"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                },
                {
                    "authorId": "2628881",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "2067815167",
                    "name": "M. D. Ward"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                }
            ]
        },
        {
            "paperId": "b647872ea2f3589e20dae7a5b04b1961861fa215",
            "title": "Analyzing Norm Violations in Live-Stream Chat",
            "abstract": "Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the informational context humans use in live-stream moderation, and train models leveraging context to identify norm violations. Our results show that appropriate contextual information can boost moderation performance by 35\\%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2105675738",
                    "name": "Jihyung Moon"
                },
                {
                    "authorId": "73037511",
                    "name": "Dong-Ho Lee"
                },
                {
                    "authorId": "91009922",
                    "name": "Hyundong Justin Cho"
                },
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "50487261",
                    "name": "Chan Young Park"
                },
                {
                    "authorId": "2220431613",
                    "name": "MinWoo Kim"
                },
                {
                    "authorId": "143823227",
                    "name": "Jonathan May"
                },
                {
                    "authorId": "2634786",
                    "name": "J. Pujara"
                },
                {
                    "authorId": "9149651",
                    "name": "Sungjoon Park"
                }
            ]
        },
        {
            "paperId": "c9f83c0fa1425d61c5b16aadc4492ad53e4fbda2",
            "title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
            "abstract": "Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we apply large language models (LLMs) to these benchmarks using in-context learning (ICL). We investigate whether and to what extent LLMs can be used for TKG forecasting, especially without any fine-tuning or explicit modules for capturing structural and temporal information. For our experiments, we present a framework that converts relevant historical facts into prompts and generates ranked predictions using token probabilities. Surprisingly, we observe that LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully designed and trained for TKG forecasting. Our extensive evaluation presents performances across several models and datasets with different characteristics, compares alternative heuristics for preparing contextual information, and contrasts to prominent TKG methods and simple frequency and recency baselines. We also discover that using numerical indices instead of entity/relation names, i.e., hiding semantic information, does not significantly affect the performance ($\\pm$0.4\\% Hit@1). This shows that prior semantic knowledge is unnecessary; instead, LLMs can leverage the existing patterns in the context to achieve such performance. Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond simple predictions based on common or recent information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115475530",
                    "name": "Dong-Ho Lee"
                },
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "2634786",
                    "name": "J. Pujara"
                }
            ]
        },
        {
            "paperId": "1e1b08b4583c24b1be1e413cfec29a572454b190",
            "title": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-Modal Knowledge Transfer",
            "abstract": "Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias.In this work, we study whether integrating visual knowledge into a language model can fill the gap.We investigate two types of knowledge transfer: (1) text knowledge transfer using image captions that may contain enriched visual knowledge and (2) cross-modal knowledge transfer using both images and captions with vision-language training objectives.On 5 downstream tasks that may need visual knowledge to solve the problem, we perform extensive empirical comparisons over the presented objectives.Our experiments show that visual knowledge transfer can improve performance in both low-resource and fully supervised settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "73037511",
                    "name": "Dong-Ho Lee"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "2634786",
                    "name": "J. Pujara"
                },
                {
                    "authorId": "1384550891",
                    "name": "Xiang Ren"
                }
            ]
        },
        {
            "paperId": "32d59ab951be74be351f9777da2cbc71bb68c3c1",
            "title": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
            "abstract": "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning.However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed.To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners.For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).Furthermore, we analyze the effect of diverse prompts for few-shot tasks.Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x larger model, PICa.In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at https://github.com/woojeongjin/FewVLM",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "153655416",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "1752875",
                    "name": "Yelong Shen"
                },
                {
                    "authorId": "2109136147",
                    "name": "Weizhu Chen"
                },
                {
                    "authorId": "2115257544",
                    "name": "Xiang Ren"
                }
            ]
        },
        {
            "paperId": "8f6770bf003f0c4ada6fd468b5084c5522ef8828",
            "title": "Modality-specific Distillation",
            "abstract": "Large neural networks are impractical to deploy on mobile devices due to their heavy computational cost and slow inference. Knowledge distillation (KD) is a technique to reduce the model size while retaining performance by transferring knowledge from a large \u201cteacher\u201d model to a smaller \u201cstudent\u201d model. However, KD on multimodal datasets such as vision-language datasets is relatively unexplored and digesting such multimodal information is challenging since different modalities present different types of information. In this paper, we propose modality-specific distillation (MSD) to effectively transfer knowledge from a teacher on multimodal datasets. Existing KD approaches can be applied to multimodal setup, but a student doesn\u2019t have access to modality-specific predictions. Our idea aims at mimicking a teacher\u2019s modality-specific predictions by introducing an auxiliary loss term for each modality. Because each modality has different importance for predictions, we also propose weighting approaches for the auxiliary losses; a meta-learning approach to learn the optimal weights on these loss terms. In our experiments, we demonstrate the effectiveness of our MSD and the weighting scheme and show that it achieves better performance than KD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "2095979",
                    "name": "Maziar Sanjabi"
                },
                {
                    "authorId": "35557488",
                    "name": "Shaoliang Nie"
                },
                {
                    "authorId": "48327785",
                    "name": "L Tan"
                },
                {
                    "authorId": "1384550891",
                    "name": "Xiang Ren"
                },
                {
                    "authorId": "22593971",
                    "name": "Hamed Firooz"
                }
            ]
        },
        {
            "paperId": "d1824e84b3837ec4bc115321e3bb35df22d2ccf2",
            "title": "MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding",
            "abstract": "To reduce a model size but retain performance, we often rely on knowledge distillation (KD) which transfers knowledge from a large\"teacher\"model to a smaller\"student\"model. However, KD on multimodal datasets such as vision-language tasks is relatively unexplored, and digesting multimodal information is challenging since different modalities present different types of information. In this paper, we perform a large-scale empirical study to investigate the importance and effects of each modality in knowledge distillation. Furthermore, we introduce a multimodal knowledge distillation framework, modality-specific distillation (MSD), to transfer knowledge from a teacher on multimodal tasks by learning the teacher's behavior within each modality. The idea aims at mimicking a teacher's modality-specific predictions by introducing auxiliary loss terms for each modality. Furthermore, because each modality has different saliency for predictions, we define saliency scores for each modality and investigate saliency-based weighting schemes for the auxiliary losses. We further study a weight learning approach to learn the optimal weights on these loss terms. In our empirical analysis, we examine the saliency of each modality in KD, demonstrate the effectiveness of the weighting scheme in MSD, and show that it achieves better performance than KD on four multimodal datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "2095979",
                    "name": "Maziar Sanjabi"
                },
                {
                    "authorId": "35557488",
                    "name": "Shaoliang Nie"
                },
                {
                    "authorId": "48327785",
                    "name": "L Tan"
                },
                {
                    "authorId": "1384550891",
                    "name": "Xiang Ren"
                },
                {
                    "authorId": "22593971",
                    "name": "Hamed Firooz"
                }
            ]
        },
        {
            "paperId": "30602e3382df3abedb5f225b55b7efce8580f74d",
            "title": "ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data",
            "abstract": "Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our task limits accessible information, and thus a model has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERTbased models and find that our best model achieves 61.0% accuracy on the dataset, which still lags behind human performance by about 19%. We hope ForecastQA will support future research efforts in bridging this gap.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "2109621632",
                    "name": "Suji Kim"
                },
                {
                    "authorId": "1885282",
                    "name": "Rahul Khanna"
                },
                {
                    "authorId": "2115475530",
                    "name": "Dong-Ho Lee"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "1384550891",
                    "name": "Xiang Ren"
                }
            ]
        }
    ]
}