{
    "authorId": "2749478",
    "papers": [
        {
            "paperId": "1abd1efae8c3849e28de926e52d166b7800965a1",
            "title": "DeepAggregation: A New Approach for Aggregating Incomplete Ranked Lists using Multi-Layer Graph Embedding",
            "abstract": "Preference aggregation, and specifically rank aggregation, is a well known problem in the fields of computational social choice and preference handling with broad application including web search and recommendation systems. Inspired by the recent advances in the area of deep neural representation learning, for the first time in the literature, in this paper we leverage unsupervised deep learning techniques - especially graph embeddings - for aggregating a collection of incomplete rank lists and accordingly we develop an algorithm called DeepAggregation. It takes as input a set of incomplete rank lists and constructs a multi-layer graph wherein the nodes are the alternatives that are ranked and the edges capture information contained in the incomplete rank lists. We then compute deep neural representation vectors (i.e. embeddings) for the nodes and then derive the aggregated order using these representation vectors. Our proposed algorithm can handle incomplete rank lists with or without ties. We conduct thorough empirical analysis of the proposed DeepAggregation algorithm using various real life data sets such as TripAdvisor reviews data. We empirically observe that DeepAggregation generates impressive results in comparison with a number of well-known state-of-the-art preference aggregation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2637492",
                    "name": "R. D. Vallam"
                },
                {
                    "authorId": "3196118",
                    "name": "Ramasuri Narayanam"
                },
                {
                    "authorId": "144063173",
                    "name": "Srikanth G. Tamilselvam"
                },
                {
                    "authorId": "143999398",
                    "name": "Nicholas Mattei"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "7ddfb35c329b5f4cb1be5bde0c56b11e3f81988a",
            "title": "Collaborative Reinforcement Learning Model for Sustainability of Cooperation in Sequential Social Dilemmas",
            "abstract": "Learning the emergence of cooperation in conflicting scenarios such as social dilemmas is a centerpiece of research. Many reinforcement learning based theories exist in the literature to address this problem. The well-known fact about RL based model's very slow learning capabilities coupled with large state space exhibit significant negative effects especially in repeated version of social dilemma settings such as repeated Public Goods Game (PGG) and thereby making them ineffective to model sustainability of cooperation. In this paper, we address this research challenge by augmenting the reinforcement learning based models with a notion of collaboration among the agents, motivated by the fact that humans learn not only through their own actions but also by following the actions of other agents who also continuously learn about the environment. In particular, we propose a novel model, which we refer to as Collaborative Reinforcement Learning (CRL), wherein we define collaboration among the agents as the ability of agents to fully follow other agent's actions/decisions. This is also termed as social learning. The proposed CRL model significantly influences the speed of individual learning, which eventually has a large effect on the collective behavior as compared to that of RL only models and thereby effectively explaining the sustainability of cooperation in repeated PGG settings. We also extend the CRL model for PGGs over different generations where agents die out and new agents are born following a birth-death process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38488764",
                    "name": "Ritwik Chaudhuri"
                },
                {
                    "authorId": "2620644",
                    "name": "K. Mukherjee"
                },
                {
                    "authorId": "3196118",
                    "name": "Ramasuri Narayanam"
                },
                {
                    "authorId": "2637492",
                    "name": "R. D. Vallam"
                },
                {
                    "authorId": "2109263513",
                    "name": "Ayush Kumar"
                },
                {
                    "authorId": "123831016",
                    "name": "Antriksh Mathur"
                },
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "d28e0e9bf5a8422b64d42f2eeb42a09b35862f2d",
            "title": "Computational Aspects of Equilibria in Discrete Preference Games",
            "abstract": "We study the complexity of equilibrium computation in discrete preference games. These games were introduced by Chierichetti, Kleinberg, and Oren (EC '13, JCSS '18) to model decision-making by agents in a social network that choose a strategy from a finite, discrete set, balancing between their intrinsic preferences for the strategies and their desire to choose a strategy that is `similar' to their neighbours. There are thus two components: a social network with the agents as vertices, and a metric space of strategies. These games are potential games, and hence pure Nash equilibria exist. Since their introduction, a number of papers have studied various aspects of this model, including the social cost at equilibria, and arrival at a consensus. We show that in general, equilibrium computation in discrete preference games is PLS-complete, even in the simple case where each agent has a constant number of neighbours. If the edges in the social network are weighted, then the problem is PLS-complete even if each agent has a constant number of neighbours, the metric space has constant size, and every pair of strategies is at distance 1 or 2. Further, if the social network is directed, modelling asymmetric influence, an equilibrium may not even exist. On the positive side, we show that if the metric space is a tree metric, or is the product of path metrics, then the equilibrium can be computed in polynomial time.\u00a0",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41017723",
                    "name": "Phani Raj Lolakapuri"
                },
                {
                    "authorId": "3124327",
                    "name": "U. Bhaskar"
                },
                {
                    "authorId": "3196118",
                    "name": "Ramasuri Narayanam"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                },
                {
                    "authorId": "2745442",
                    "name": "Pankaj Dayama"
                }
            ]
        },
        {
            "paperId": "ffa279d565a7970e643e40456bbdd22ad53c3c5b",
            "title": "Dynamic Particle Allocation to Solve Interactive POMDP Models for Social Decision Making",
            "abstract": "In social dilemma settings, such as repeated Public Goods Games (PGGs), humans often come across a dilemma whether to contribute or not based on past contributions from others. In such settings, the decision taken by an agent/human actually depends not only on the belief the agent has about other agents and the environment, but also on their beliefs about others' beliefs. To factor in these aspects, we propose a novel formulation of computational theory of mind (ToM) to model human behavior in a repeated PGG using interactive partially observable Markov decision processes (I-POMDPs). Interactive particle filter (IPF) is a well-known algorithm used to approximately solve I-POMDP models for the agents to find their optimal contributions. Number of particles assigned to an agent in IPF can be translated into time and computational resources. Solving I-POMDPs in a time-memory efficient manner even in the case of small state spaces is a largely intractable problem. Also, maintaining a fixed number of particles assigned to each agent, over time, will be highly inefficient in terms of resource utilization. To address this problem, we propose a dynamic particle allocation algorithm for different agents based on how well they could predict. We validate our proposed algorithm through real experiments involving human agents. Our results suggest that dynamic particle allocation based IPF for I-POMDPs is effective in modelling human behaviours in repeated social dilemma setting while utilizing computational resources in an effective manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2637492",
                    "name": "R. D. Vallam"
                },
                {
                    "authorId": "26936801",
                    "name": "Sarthak Ahuja"
                },
                {
                    "authorId": "1900357845",
                    "name": "Surya Sajja"
                },
                {
                    "authorId": "38488764",
                    "name": "Ritwik Chaudhuri"
                },
                {
                    "authorId": "1999599",
                    "name": "R. Pimplikar"
                },
                {
                    "authorId": "2620644",
                    "name": "K. Mukherjee"
                },
                {
                    "authorId": "3196118",
                    "name": "Ramasuri Narayanam"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "2bff89e6a5d85f34ecb16808e1ecc14272efba74",
            "title": "Cogniculture: Towards a Better Human-Machine Co-evolution",
            "abstract": "Research in Artificial Intelligence is breaking technology barriers every day. New algorithms and high performance computing are making things possible which we could only have imagined earlier. Though the enhancements in AI are making life easier for human beings day by day, there is constant fear that AI based systems will pose a threat to humanity. People in AI community have diverse set of opinions regarding the pros and cons of AI mimicking human behavior. Instead of worrying about AI advancements, we propose a novel idea of cognitive agents, including both human and machines, living together in a complex adaptive ecosystem, collaborating on human computation for producing essential social goods while promoting sustenance, survival and evolution of the agents' life cycle. We highlight several research challenges and technology barriers in achieving this goal. We propose a governance mechanism around this ecosystem to ensure ethical behaviors of all cognitive agents. Along with a novel set of use-cases of Cogniculture, we discuss the road map ahead for this journey.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1999599",
                    "name": "R. Pimplikar"
                },
                {
                    "authorId": "2620644",
                    "name": "K. Mukherjee"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                },
                {
                    "authorId": "27638084",
                    "name": "Harit Vishwakarma"
                },
                {
                    "authorId": "3196118",
                    "name": "Ramasuri Narayanam"
                },
                {
                    "authorId": "26936801",
                    "name": "Sarthak Ahuja"
                },
                {
                    "authorId": "2637492",
                    "name": "R. D. Vallam"
                },
                {
                    "authorId": "38488764",
                    "name": "Ritwik Chaudhuri"
                },
                {
                    "authorId": "9138947",
                    "name": "Joydeep Mondal"
                }
            ]
        },
        {
            "paperId": "27bf66b7915e4664239f0943f61ad5fca1836002",
            "title": "Outplacement time and probability estimation using discrete event simulation",
            "abstract": "In today's rapidly changing technological scenario, tech giants revise their strategic alignment every couple of years. As a result, their workforce has to be adapted to the organization's strategy. Members of the workforce who are neither relevant to the strategic alignment, nor can be made relevant by reskilling, have to be either outplaced (i.e. placed in an another job within organization) or separated from the organization. In geographies like Europe, where the cost of separation is very high, it becomes very important to make the right decision for each employee. In this paper, we describe a simulation based methodology to find the probability and time of outplacement of an employee. These numbers are inputs to a global problem of making the optimal decision for the entire workforce.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "1999599",
                    "name": "R. Pimplikar"
                },
                {
                    "authorId": "38488764",
                    "name": "Ritwik Chaudhuri"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "944412e60a607f8905bb7284f9601cc151e5fe64",
            "title": "Near-Optimal Nonmyopic Contact Center Planning Using Dual Decomposition",
            "abstract": "\n \n We address the problem of minimizing staffing cost in a contact center subject to service level requirements over multiple weeks. We handle both the capacity planning and agent schedule generation aspect of this problem. Our work incorporates two unique business requirements. First, we develop techniques that can provide near-optimal staffing for 247 contact centers over long term, upto eight weeks, rather than planning myopically on a week-on-week basis. Second, our approach is usable in an online interactive setting in which staffing managers using our system expect high quality plans within a short time period. Results on large real world and synthetic instances show that our Lagrangian relaxation based technique can achieve a solution within 94% of optimal on an average, for eight week problems within ten minutes, whereas a generic integer programming solver can only achieve a solution within 80% of optimal. Our approach is also deployed in live business environment and reduces headcount by a decile over techniques used previously by our client business units.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40305195",
                    "name": "Akshat Kumar"
                },
                {
                    "authorId": "2109038579",
                    "name": "S. Singh"
                },
                {
                    "authorId": "2130387885",
                    "name": "Pranav Gupta"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "bbc30bd6af3b52f611285fa7e067c14755f2d766",
            "title": "Learning to Propagate Rare Labels",
            "abstract": "Label propagation is a well-explored family of methods for training a semi-supervised classifier where input data points (both labeled and unlabeled) are connected in the form of a weighted graph. For binary classification, the performance of these methods starts degrading considerably whenever input dataset exhibits following characteristics - (i) one of the class label is rare label or equivalently, class imbalance (CI) is very high, and (ii) degree of supervision (DoS) is very low -- defined as fraction of labeled points. These characteristics are common in many real-world datasets relating to network fraud detection. Moreover, in such applications, the amount of class imbalance is not known a priori. In this paper, we have proposed and justified the use of an alternative formulation for graph label propagation under such extreme behavior of the datasets. In our formulation, objective function is the difference of two convex quadratic functions and the constraints are box constraints. We solve this program using Concave-Convex Procedure (CCCP). Whenever the problem size becomes too large, we suggest to work with a k-NN subgraph of the given graph which can be sampled by using Locality Sensitive Hashing (LSH) technique. We have also discussed various issues that one typically faces while sampling such a k-NN subgraph in practice. Further, we have proposed a novel label flipping method on top of the CCCP solution, which improves the result of CCCP further whenever class imbalance information is made available a priori. Our method can be easily adopted for a MapReduce platform, such as Hadoop. We have conducted experiments on 11 datasets comprising a graph size of up to 20K nodes, CI as high as 99:6%, and DoS as low as 0:5%. Our method has resulted up to 19:5-times improvement in F-measure and up to 17:5-times improvement in AUC-PR measure against baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1999599",
                    "name": "R. Pimplikar"
                },
                {
                    "authorId": "40408530",
                    "name": "Dinesh Garg"
                },
                {
                    "authorId": "2221210",
                    "name": "Deepesh Bharani"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        },
        {
            "paperId": "5c2c4b6a1e530e3120f4dea1277afd5fe92b41f6",
            "title": "Minimum Cost Resource Allocation for Meeting Job Requirements",
            "abstract": "We consider the problem of allocating resources for completing a collection of jobs. Each resource is specified by a start-time, finish-time and the capacity of resource available and has an associated cost, and each job is specified by a start-time, finish-time and the amount of the resource required (demand) during this interval. A feasible solution is a multiset of resources (i.e., multiple units of each resource may be picked) such that at any point of time, the sum of the capacities offered by the resources is at least the total demand of the jobs active at that point of time. The cost of the solution is the sum of the costs of the resources included in the solution (taking into account the units of the resources). The goal is to find a feasible solution of minimum cost. This problem arises naturally in many scenarios. For example, given a set of jobs, we would like to allocate some resource such as machines, memory or bandwidth in order to complete all the jobs. This problem generalizes a covering version of the knapsack problem which is known to be NP-hard. We present a constant factor approximation algorithm for this problem based on a Primal-Dual approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1696818",
                    "name": "Venkatesan T. Chakaravarthy"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                },
                {
                    "authorId": "1706548",
                    "name": "Sambuddha Roy"
                },
                {
                    "authorId": "1787471",
                    "name": "Yogish Sabharwal"
                },
                {
                    "authorId": "2123128590",
                    "name": "Amit Kumar"
                }
            ]
        },
        {
            "paperId": "ca4807a7a26a6d19e4abe6322402f8309b1c29c0",
            "title": "Simultaneously improving CSAT and profit in a retail banking organization",
            "abstract": "Customer satisfaction (CSAT) is the key driver for retention and growth in retail banking and several techniques have been applied by banks to achieve this. For instance, banks in emerging markets with high footfall in branches have gone beyond the traditional approach of segmenting customers and services to optimizing the wait time for customers visiting the bank's branch. While this approach has significantly improved service quality, it has also added a new dimension in the service quality metric : pro-actively identify and address customer needs for (i) efficient banking experience and (ii) enhancing profit by selling additional services to existing customer. In this paper we present a system that addresses the challenge involved in providing better service to retail banking customer while ensuring that a larger share of customer's wallet comes to the branch. We do this by combining predictive analytics, scheduling and process optimization techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145640180",
                    "name": "S. Mehta"
                },
                {
                    "authorId": "1984030",
                    "name": "Ullas Nambiar"
                },
                {
                    "authorId": "1742877",
                    "name": "Vishal S. Batra"
                },
                {
                    "authorId": "34593143",
                    "name": "Sumit Negi"
                },
                {
                    "authorId": "143722566",
                    "name": "Prasad Deshpande"
                },
                {
                    "authorId": "2749478",
                    "name": "G. Parija"
                }
            ]
        }
    ]
}