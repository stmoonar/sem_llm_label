{
    "authorId": "2218932858",
    "papers": [
        {
            "paperId": "b528b5184b6191c6867d28e3d034645ec723274d",
            "title": "A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide",
            "abstract": "Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications. Investigation of deep learning for HOIs, thus, has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, bioinformatics and medical science, time series analysis, and computer vision. Lastly, we conclude with a discussion on limitations and future directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268372451",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "2218932858",
                    "name": "Soo Yong Lee"
                },
                {
                    "authorId": "2294807430",
                    "name": "Yue Gao"
                },
                {
                    "authorId": "31736099",
                    "name": "Alessia Antelmi"
                },
                {
                    "authorId": "2294361631",
                    "name": "Mirko Polato"
                },
                {
                    "authorId": "2242112674",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "b6b8e083c42d6be3c80f59b33b380143b4800648",
            "title": "VilLain: Self-Supervised Learning on Homogeneous Hypergraphs without Features via Virtual Label Propagation",
            "abstract": "Group interactions arise in various scenarios in real-world systems: collaborations of researchers, co-purchases of products, and discussions in online Q&A sites, to name a few. Such higher-order relations are naturally modeled as hypergraphs, which consist of hyperedges (i.e., any-sized subsets of nodes). For hypergraphs, the challenge to learn node representation when features or labels are not available is imminent, given that (a) most real-world hypergraphs are not equipped with external features while (b) most existing approaches for hypergraph learning resort to additional information. Thus, in this work, we propose VilLain, a novel self-supervised hypergraph representation learning method based on the propagation of virtual labels (v-labels). Specifically, we learn for each node a sparse probability distribution over v-labels as its feature vector, and we propagate the vectors to construct the final node embeddings. Inspired by higher-order label homogeneity, which we discover in real-world hypergraphs, we design novel self-supervised loss functions for the v-labels to reproduce the higher-order structure-label pattern. We demonstrate that VilLain is: (a) Requirement-free: learning node embeddings without relying on node labels and features, (b) Versatile: giving embeddings that are not specialized to specific tasks but generalizable to diverse downstream tasks, and (c) Accurate: more accurate than its competitors for node classification, hyperedge prediction, node clustering, and node retrieval tasks. Our code and dataset are available at https://github.com/geon0325/VilLain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261454328",
                    "name": "Geon Lee"
                },
                {
                    "authorId": "2218932858",
                    "name": "Soo Yong Lee"
                },
                {
                    "authorId": "2242112674",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "c5f4db49031cfdd5d71967ab0b66a44342c3e2c4",
            "title": "Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective",
            "abstract": "How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significant improvement in GNN performance following the feature shuffle. Having overlooked the impact of A-X dependence on GNNs, the prior literature does not provide a satisfactory understanding of the phenomenon. Thus, we raise two research questions. First, how should A-X dependence be measured, while controlling for potential confounds? Second, how does A-X dependence affect GNNs? In response, we (i) propose a principled measure for A-X dependence, (ii) design a random graph model that controls A-X dependence, (iii) establish a theory on how A-X dependence relates to graph convolution, and (iv) present empirical analysis on real-world graphs that align with the theory. We conclude that A-X dependence mediates the effect of graph convolution, such that smaller dependence improves GNN-based node classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218932858",
                    "name": "Soo Yong Lee"
                },
                {
                    "authorId": "2268372451",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "2065674134",
                    "name": "Fanchen Bu"
                },
                {
                    "authorId": "2257360769",
                    "name": "Jaemin Yoo"
                },
                {
                    "authorId": "2283301820",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2242112674",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "eed6cac597a7993767f78d0f55419f7e6b23fced",
            "title": "HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs",
            "abstract": "Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple nodes with hyperedges, and better capturing the topology is essential for effective representation learning. Recent advances in generative self-supervised learning (SSL) suggest that hypergraph neural networks learned from generative self supervision have the potential to effectively encode the complex hypergraph topology. Designing a generative SSL strategy for hypergraphs, however, is not straightforward. Questions remain with regard to its generative SSL task, connection to downstream tasks, and empirical properties of learned representations. In light of the promises and challenges, we propose a novel generative SSL strategy for hypergraphs. We first formulate a generative SSL task on hypergraphs, hyperedge filling, and highlight its theoretical connection to node classification. Based on the generative SSL task, we propose a hypergraph SSL method, HypeBoy. HypeBoy learns effective general-purpose hypergraph representations, outperforming 16 baseline methods across 11 benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268372451",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "2160698854",
                    "name": "Shinhwan Kang"
                },
                {
                    "authorId": "2065674134",
                    "name": "Fanchen Bu"
                },
                {
                    "authorId": "2218932858",
                    "name": "Soo Yong Lee"
                },
                {
                    "authorId": "2257360769",
                    "name": "Jaemin Yoo"
                },
                {
                    "authorId": "2242112674",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "f7daa82747a13314c42a1df0c8eb2d4958ac27d6",
            "title": "Tackling Prevalent Conditions in Unsupervised Combinatorial Optimization: Cardinality, Minimum, Covering, and More",
            "abstract": "Combinatorial optimization (CO) is naturally discrete, making machine learning based on differentiable optimization inapplicable. Karalias&Loukas (2020) adapted the probabilistic method to incorporate CO into differentiable optimization. Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization. However, each component confronts unique challenges. First, deriving objectives under various conditions (e.g., cardinality constraints and minimum) is nontrivial. Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding. In this work, we aim to tackle prevalent (i.e., commonly involved) conditions in unsupervised CO. First, we concretize the targets for objective construction and derandomization with theoretical justification. Then, for various conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets. Finally, we apply the derivations to various CO problems. Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t. both optimization quality and speed.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2065674134",
                    "name": "Fanchen Bu"
                },
                {
                    "authorId": "2055335611",
                    "name": "Hyeonsoo Jo"
                },
                {
                    "authorId": "2218932858",
                    "name": "Soo Yong Lee"
                },
                {
                    "authorId": "2301797085",
                    "name": "Sungsoo Ahn"
                },
                {
                    "authorId": "2242112674",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "1114d604da30ac242fa30178cd08bcc4452073e6",
            "title": "Towards Deep Attention in Graph Neural Networks: Problems and Remedies",
            "abstract": "Graph neural networks (GNNs) learn the representation of graph-structured data, and their expressiveness can be further enhanced by inferring node relations for propagation. Attention-based GNNs infer neighbor importance to manipulate the weight of its propagation. Despite their popularity, the discussion on deep graph attention and its unique challenges has been limited. In this work, we investigate some problematic phenomena related to deep graph attention, including vulnerability to over-smoothed features and smooth cumulative attention. Through theoretical and empirical analyses, we show that various attention-based GNNs suffer from these problems. Motivated by our findings, we propose AEROGNN, a novel GNN architecture designed for deep graph attention. AERO-GNN provably mitigates the proposed problems of deep graph attention, which is further empirically demonstrated with (a) its adaptive and less smooth attention functions and (b) higher performance at deep layers (up to 64). On 9 out of 12 node classification benchmarks, AERO-GNN outperforms the baseline GNNs, highlighting the advantages of deep graph attention. Our code is available at https://github.com/syleeheal/AERO-GNN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218932858",
                    "name": "Soo Yong Lee"
                },
                {
                    "authorId": "2065674134",
                    "name": "Fanchen Bu"
                },
                {
                    "authorId": "31888223",
                    "name": "Jaemin Yoo"
                },
                {
                    "authorId": "40553270",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "f6257bc75a3d4a85da54c5158f236c34fc00cb67",
            "title": "You're Not Alone in Battle: Combat Threat Analysis Using Attention Networks and a New Open Benchmark",
            "abstract": "For military commands, combat threat analysis is crucial in predicting future outcomes and informing consequent decisions. Its primary objectives include determining the intention and attack likelihood of the hostiles. The complex, dynamic, and noisy nature of combat, however, presents significant challenges in its analysis. The prior research has been limited in accounting for such characteristics, assuming independence of each entity, no unobserved tactics, and clean combat data. As such, we present spatio-temporal attention for threat analysis (SAFETY) to encode complex interactions that arise within combat. We test the model performance for unobserved tactics and with various perturbations. To do so, we also present the first open-source benchmark for combat threat analysis with two downstream tasks of predicting entity intention and attack probability. Our experiments show that SAFETY achieves a significant improvement in model performance, with enhancements of up to 13% in intention prediction and 7% in attack prediction compared to the strongest competitor, even when confronted with noisy or missing data. This result highlights the importance of encoding dynamic interactions among entities for combat threat analysis. Our codes and dataset are available at https://github.com/syleeheal/SAFETY.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218932858",
                    "name": "Soo Yong Lee"
                },
                {
                    "authorId": "2260819122",
                    "name": "Juwon Kim"
                },
                {
                    "authorId": "2260837293",
                    "name": "Kiwoong Park"
                },
                {
                    "authorId": "2260699260",
                    "name": "Dong Kuk Ryu"
                },
                {
                    "authorId": "2260653969",
                    "name": "Sangheun Shim"
                },
                {
                    "authorId": "40553270",
                    "name": "Kijung Shin"
                }
            ]
        }
    ]
}