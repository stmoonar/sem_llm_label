{
    "authorId": "152888952",
    "papers": [
        {
            "paperId": "08fcc88c211444e27ac634f5e8b0b22c62249c96",
            "title": "Addressing Chest Radiograph Projection Bias in Deep Classification Models",
            "abstract": "Deep learning-based models are widely used for disease classification in chest radiographs. This exam can be performed in one of two projections (posteroanterior or anteroposterior), depending on the direction that the X-ray beam travels through the body. Since projection visibly affects the way anatomical structures appear in the scans, it may introduce bias in classifiers, especially when spurious correlations between a given disease and a projection occur. This paper examines the influence of chest radiograph projection on the performance of deep learning-based classification models and proposes an approach to mitigate projection-induced bias. Results show that a DenseNet-121 model is better at classifying images from the most representative projection in the data set, suggesting that projection is taken into account by the classifier. Moreover, this model can classify chest X-ray projection better than any of the fourteen radiological findings considered, without being explicitly trained for that task, putting it at high risk for projection bias. We propose a label-conditional gradient reversal framework to make the model insensitive to projection, by forcing the extracted features to be simultaneously good for disease classification and bad for projection classification, resulting in a framework with reduced projection-induced bias.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130666498",
                    "name": "Sofia Cardoso Pereira"
                },
                {
                    "authorId": "2058128418",
                    "name": "Joana Rocha"
                },
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                },
                {
                    "authorId": "144731836",
                    "name": "A. Mendon\u00e7a"
                }
            ]
        },
        {
            "paperId": "0b5970fd876b52b676f878079b52e7289dc56025",
            "title": "DeepFixCX: Explainable privacy\u2010preserving image compression for medical image analysis",
            "abstract": "Explanations of a model's biases or predictions are essential to medical image analysis. Yet, explainable machine learning approaches for medical image analysis are challenged by needs to preserve privacy of patient data, and by current trends in deep learning to use unsustainably large models and large datasets. We propose DeepFixCX for explainable and privacy\u2010preserving medical image compression that is nimble and performant. We contribute a review of the field and a conceptual framework for simultaneous privacy and explainability via tools of compression. DeepFixCX compresses images without learning by removing or obscuring spatial and edge information. DeepFixCX is ante\u2010hoc explainable and gives privatized post hoc explanations of spatial and edge bias without accessing the original image. DeepFixCX privatizes images to prevent image reconstruction and mitigate patient re\u2010identification. DeepFixCX is nimble. Compression can occur on a laptop CPU or GPU to compress and privatize 1700 images per second of size 320\u2009\u00d7\u2009320. DeepFixCX enables use of low memory MLP classifiers for vision data; permitting small performance loss gives end\u2010to\u2010end MLP performance over 70\u00d7 faster and batch size over 100\u00d7 larger. DeepFixCX consistently improves predictive classification performance of a Deep Neural Network (DNN) by 0.02 AUC ROC on Glaucoma and Cervix Type detection datasets, and can improve multi\u2010label chest x\u2010ray classification performance in seven of 10 tested settings. In all three datasets, compression to less than 5% of original number of pixels gives matching or improved performance. Our main novelty is to define an explainability versus privacy problem and address it with lossy compression.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "2186979527",
                    "name": "Shreshta Mohan"
                },
                {
                    "authorId": "1998967224",
                    "name": "Elvin Johnson"
                },
                {
                    "authorId": "2210047886",
                    "name": "Yuhao Liu"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        },
        {
            "paperId": "70a540c7ae331cf41ee32ab31e63717fc9d69e1b",
            "title": "ZEBRA: Explaining rare cases through outlying interpretable concepts",
            "abstract": "Anomaly detection methods can detect outliers, but what are the properties of an outlier\u0192 In this paper, we propose ZEBRA, a novel framework for generating explanations of an outlier based on the analysis of feature rarity in an interpretable feature space. The contributions of our work include: (a) a modular model-agnostic framework for explanations of outliers; (b) a statistical explanation method based on a rarity score and weighted aggregation functions; (c) multimodal explanations combining visual, textual, and numeric explanations. ZEBRA simplifies the mapping of low-level features to high-level concepts to generate multimodal and human-readable explanations of outliers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2230825156",
                    "name": "Pedro Madeira"
                },
                {
                    "authorId": "34934072",
                    "name": "A. Carreiro"
                },
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "2065623670",
                    "name": "Lu\u00eds Rosado"
                },
                {
                    "authorId": "2228707705",
                    "name": "Filipe Soares"
                }
            ]
        },
        {
            "paperId": "e00b40a7edac5d5a3b557e26727c62ce98855c26",
            "title": "ExplainFix: Explainable spatially fixed deep networks",
            "abstract": "Is there an initialization for deep networks that requires no learning? ExplainFix adopts two design principles: the \u201cfixed filters\u201d principle that all spatial filter weights of convolutional neural networks can be fixed at initialization and never learned, and the \u201cnimbleness\u201d principle that only few network parameters suffice. We contribute (a) visual model\u2010based explanations, (b) speed and accuracy gains, and (c) novel tools for deep convolutional neural networks. ExplainFix gives key insights that spatially fixed networks should have a steered initialization, that spatial convolution layers tend to prioritize low frequencies, and that most network parameters are not necessary in spatially fixed models. ExplainFix models have up to \u00d7100 fewer spatial filter kernels than fully learned models and matching or improved accuracy. Our extensive empirical analysis confirms that ExplainFix guarantees nimbler models (train up to 17% faster with channel pruning), matching or improved predictive performance (spanning 13 distinct baseline models, four architectures and two medical image datasets), improved robustness to larger learning rate, and robustness to varying model size. We are first to demonstrate that all spatial filters in state\u2010of\u2010the\u2010art convolutional deep networks can be fixed at initialization, not learned.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        },
        {
            "paperId": "24e46dbf23e717824a397fcc6d3d3812f522764e",
            "title": "Privacy-preserving Case-based Explanations: Enabling visual interpretability by protecting privacy",
            "abstract": "Deep Learning achieves state-of-the-art results in many domains, yet its black-box nature limits its application to real-world contexts. An intuitive way to improve the interpretability of Deep Learning models is by explaining their decisions with similar cases. However, case-based explanations cannot be used in contexts where the data exposes personal identity, as they may compromise the privacy of individuals. In this work, we identify the main limitations and challenges in the anonymization of case-based explanations of image data through a survey on case-based interpretability and image anonymization methods. We empirically analyze the anonymization methods in regards to their capacity to remove personally identifiable information while preserving relevant semantic properties of the data. Through this analysis, we conclude that most privacy-preserving methods are not sufficiently good to be applied to case-based explanations. To promote research on this topic, we formalize the privacy protection of visual case-based explanations as a multi-objective problem to preserve privacy, intelligibility, and relevant explanatory evidence regarding a predictive task. We empirically verify the potential of interpretability saliency maps as qualitative evaluation tools for anonymization. Finally, we identify and propose new lines of research to guide future work in the generation of privacy-preserving case-based explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138004124",
                    "name": "Helena Montenegro"
                },
                {
                    "authorId": "153471956",
                    "name": "W. Silva"
                },
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "2623167",
                    "name": "Matt Fredrikson"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "144438693",
                    "name": "Jaime S. Cardoso"
                }
            ]
        },
        {
            "paperId": "27cebebd6ed50686048935013cb0affc27291a3b",
            "title": "HeartSpot: Privatized and Explainable Data Compression for Cardiomegaly Detection",
            "abstract": "Advances in data-driven deep learning for chest X-ray image analysis underscore the need for explainability, privacy, large datasets and significant computational resources. We frame privacy and explainability as a lossy single-image compression problem to reduce both computational and data requirements without training. For Cardiomegaly detection in chest X-ray images, we propose HeartSpot and four spatial bias priors. HeartSpot priors define how to sample pixels based on domain knowledge from medical literature and from machines. HeartSpot privatizes chest X-ray images by discarding up to 97% of pixels, such as those that reveal the shape of the thoracic cage, bones, small lesions and other sensitive features. HeartSpot priors are ante-hoc explainable and give a human-interpretable image of the preserved spatial features that clearly outlines the heart. HeartSpot offers strong compression, with up to 32x fewer pixels and 11 $x$ smaller filesize. Cardiomegaly detectors using HeartSpot are up to 9x faster to train or at least as accurate (up to +.01 AUC ROC) when compared to a baseline DenseNet121. HeartSpot is post-hoc explainable by re-using existing attribution methods without requiring access to the original non-privatized image. In summary, HeartSpot improves speed and accuracy, reduces image size, improves privacy and ensures explainability.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1998967224",
                    "name": "Elvin Johnson"
                },
                {
                    "authorId": "2186979527",
                    "name": "Shreshta Mohan"
                },
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        },
        {
            "paperId": "8520892f859e783533a0da731cb575466425307a",
            "title": "Explainable Deep Learning for Non-Invasive Detection of Pulmonary Artery Hypertension from Heart Sounds",
            "abstract": "Late diagnoses of patients affected by pulmonary artery hypertension (PH) have a poor outcome. This observation has led to a call for earlier, non-invasive PH detection. Cardiac auscultation offers a non-invasive and cost-effective alternative to both right heart catheterization and doppler analysis in analysis of PH. We propose to detect PH via analysis of digital heart sound recordings with over-parameterized deep neural networks. In contrast with previous approaches in the literature, we assess the impact of a pre-processing step aiming to separate S2 sound into the aortic (A2) and pulmonary (P2) components. We obtain an area under the ROC curve of. 95, improving over our adaptation of a state-of-the-art Gaussian mixture model PH detector by +.17. Post-hoc explanations and analysis show that the availability of separated A2 and P2 components contributes significantly to prediction. Analysis of stethoscope heart sound recordings with deep networks is an effective, low-cost and non-invasive solution for the detection of pulmonary hypertension.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "101068995",
                    "name": "Miguel Coimbra"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "29841711",
                    "name": "S. Schmidt"
                },
                {
                    "authorId": "2184497726",
                    "name": "F. Renna"
                }
            ]
        },
        {
            "paperId": "2f418aeb50ed60b4af499c71665c0df848d65024",
            "title": "O\u2010MedAL: Online active deep learning for medical image analysis",
            "abstract": "Active learning (AL) methods create an optimized labeled training set from unlabeled data. We introduce a novel online active deep learning method for medical image analysis. We extend our MedAL AL framework to present new results in this paper. A novel sampling method queries the unlabeled examples that maximize the average distance to all training set examples. Our online method enhances performance of its underlying baseline deep network. These novelties contribute to significant performance improvements, including improving the model's underlying deep network accuracy by 6.30%, using only 25% of the labeled dataset to achieve baseline accuracy, reducing backpropagated images during training by as much as 67%, and demonstrating robustness to class imbalance in binary and multiclass tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "51436490",
                    "name": "Kartik Khandelwal"
                },
                {
                    "authorId": "2521288",
                    "name": "Mostafa Mirshekari"
                },
                {
                    "authorId": "10217959",
                    "name": "Jonathon Fagert"
                },
                {
                    "authorId": "29951387",
                    "name": "Devesh Walawalkar"
                },
                {
                    "authorId": "8182534",
                    "name": "Susu Xu"
                },
                {
                    "authorId": "2161526",
                    "name": "A. Galdran"
                },
                {
                    "authorId": "144617913",
                    "name": "Pei Zhang"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                },
                {
                    "authorId": "1832917",
                    "name": "H. Noh"
                }
            ]
        }
    ]
}