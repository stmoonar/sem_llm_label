{
    "authorId": "1767521",
    "papers": [
        {
            "paperId": "0c72450890a54b68d63baa99376131fda8f06cf9",
            "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
            "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2190751523",
                    "name": "Zhiheng Xi"
                },
                {
                    "authorId": "2240538633",
                    "name": "Wenxiang Chen"
                },
                {
                    "authorId": "2240675422",
                    "name": "Xin Guo"
                },
                {
                    "authorId": "2241408708",
                    "name": "Wei He"
                },
                {
                    "authorId": "2240473116",
                    "name": "Yiwen Ding"
                },
                {
                    "authorId": "2240450431",
                    "name": "Boyang Hong"
                },
                {
                    "authorId": "2240574799",
                    "name": "Ming Zhang"
                },
                {
                    "authorId": "2240465418",
                    "name": "Junzhe Wang"
                },
                {
                    "authorId": "2219131195",
                    "name": "Senjie Jin"
                },
                {
                    "authorId": "2240446306",
                    "name": "Enyu Zhou"
                },
                {
                    "authorId": "2058585152",
                    "name": "Rui Zheng"
                },
                {
                    "authorId": "2241140630",
                    "name": "Xiaoran Fan"
                },
                {
                    "authorId": "2118451107",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2222630539",
                    "name": "Limao Xiong"
                },
                {
                    "authorId": "2109185819",
                    "name": "Qin Liu"
                },
                {
                    "authorId": "2212175381",
                    "name": "Yuhao Zhou"
                },
                {
                    "authorId": "2240703461",
                    "name": "Weiran Wang"
                },
                {
                    "authorId": "2240482661",
                    "name": "Changhao Jiang"
                },
                {
                    "authorId": "51192034",
                    "name": "Yicheng Zou"
                },
                {
                    "authorId": "2144226697",
                    "name": "Xiangyang Liu"
                },
                {
                    "authorId": "2155273086",
                    "name": "Zhangyue Yin"
                },
                {
                    "authorId": "2042683163",
                    "name": "Shihan Dou"
                },
                {
                    "authorId": "24009282",
                    "name": "Rongxiang Weng"
                },
                {
                    "authorId": "2211998291",
                    "name": "Wensen Cheng"
                },
                {
                    "authorId": "2256972399",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2240455955",
                    "name": "Wenjuan Qin"
                },
                {
                    "authorId": "2240450583",
                    "name": "Yongyan Zheng"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2240462620",
                    "name": "Xuanjing Huan"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                }
            ]
        },
        {
            "paperId": "19e1de31b499be0c0427fbccb6ece94e024f6af7",
            "title": "A Composable Generative Framework Based on Prompt Learning for Various Information Extraction Tasks",
            "abstract": "Prompt learning is an effective paradigm that bridges gaps between the pre-training tasks and the corresponding downstream applications. Approaches based on this paradigm have achieved great transcendent results in various applications. However, it still needs to be answered how to design a general-purpose framework based on the prompt learning paradigm for various information extraction tasks. In this article, we propose a novel composable prompt-based generative framework, which could be applied to a wide range of tasks in the field of information extraction. Specifically, we reformulate information extraction tasks into the form of filling slots in pre-designed type-specific prompts, which consist of one or multiple sub-prompts. A strategy of constructing composable prompts is proposed to enhance the generalization ability in data-scarce scenarios. Furthermore, to fit this framework, we transform relation extraction into the task of determining semantic consistency in prompts. The experimental results demonstrate that our approach surpasses compared baselines on real-world datasets in data-abundant and data-scarce scenarios. Further analysis of the proposed framework is presented, as well as numerical experiments conducted to investigate impact factors of performance on various tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150356963",
                    "name": "Zhigang Kan"
                },
                {
                    "authorId": "47010066",
                    "name": "Linhui Feng"
                },
                {
                    "authorId": "2155273086",
                    "name": "Zhangyue Yin"
                },
                {
                    "authorId": "47300931",
                    "name": "Linbo Qiao"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2129495284",
                    "name": "Dongsheng Li"
                }
            ]
        },
        {
            "paperId": "2b448e406d7ca73162b794b018326c8b3d6e350d",
            "title": "Distributed Marker Representation for Ambiguous Discourse Markers and Entangled Relations",
            "abstract": "Discourse analysis is an important task because it models intrinsic semantic structures between sentences in a document. Discourse markers are natural representations of discourse in our daily language. One challenge is that the markers as well as pre-defined and human-labeled discourse relations can be ambiguous when describing the semantics between sentences. We believe that a better approach is to use a contextual-dependent distribution over the markers to express discourse information. In this work, we propose to learn a Distributed Marker Representation (DMR) by utilizing the (potentially) unlimited discourse marker data with a latent discourse sense, thereby bridging markers with sentence pairs. Such representations can be learned automatically from data without supervision, and in turn provide insights into the data itself. Experiments show the SOTA performance of our DMR on the implicit discourse relation recognition task and strong interpretability. Our method also offers a valuable tool to understand complex ambiguity and entanglement among discourse markers and manually defined discourse relations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41017337",
                    "name": "Dongyu Ru"
                },
                {
                    "authorId": "2068178455",
                    "name": "Lin Qiu"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2220665012",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "47294621",
                    "name": "Zheng Zhang"
                }
            ]
        },
        {
            "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
            "title": "Secrets of RLHF in Large Language Models Part I: PPO",
            "abstract": "Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2058585152",
                    "name": "Rui Zheng"
                },
                {
                    "authorId": "2042683163",
                    "name": "Shihan Dou"
                },
                {
                    "authorId": "2181306462",
                    "name": "Songyang Gao"
                },
                {
                    "authorId": "2248291262",
                    "name": "Wei Shen"
                },
                {
                    "authorId": "2117227865",
                    "name": "Wei-Yuan Shen"
                },
                {
                    "authorId": "2188630983",
                    "name": "Bing Wang"
                },
                {
                    "authorId": "2156649786",
                    "name": "Yan Liu"
                },
                {
                    "authorId": "2219131195",
                    "name": "Senjie Jin"
                },
                {
                    "authorId": "2109185819",
                    "name": "Qin Liu"
                },
                {
                    "authorId": "2222630539",
                    "name": "Limao Xiong"
                },
                {
                    "authorId": "2115386043",
                    "name": "Luyao Chen"
                },
                {
                    "authorId": "2190751523",
                    "name": "Zhiheng Xi"
                },
                {
                    "authorId": "2212175381",
                    "name": "Yuhao Zhou"
                },
                {
                    "authorId": "2072805812",
                    "name": "Nuo Xu"
                },
                {
                    "authorId": "2153857410",
                    "name": "Wen-De Lai"
                },
                {
                    "authorId": "40587747",
                    "name": "Minghao Zhu"
                },
                {
                    "authorId": "24009282",
                    "name": "Rongxiang Weng"
                },
                {
                    "authorId": "2227418",
                    "name": "Wen-Chun Cheng"
                },
                {
                    "authorId": "2152341000",
                    "name": "Cheng Chang"
                },
                {
                    "authorId": "2155273086",
                    "name": "Zhangyue Yin"
                },
                {
                    "authorId": "152738167",
                    "name": "Yuan Hua"
                },
                {
                    "authorId": "2039788",
                    "name": "Haoran Huang"
                },
                {
                    "authorId": "153345698",
                    "name": "Tianxiang Sun"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                },
                {
                    "authorId": "47835189",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "612d15383f5bd258f0521b81b295c7d3c811e4d5",
            "title": "From Hypergraph Energy Functions to Hypergraph Neural Networks",
            "abstract": "Hypergraphs are a powerful abstraction for representing higher-order interactions between entities of interest. To exploit these relationships in making downstream predictions, a variety of hypergraph neural network architectures have recently been proposed, in large part building upon precursors from the more traditional graph neural network (GNN) literature. Somewhat differently, in this paper we begin by presenting an expressive family of parameterized, hypergraph-regularized energy functions. We then demonstrate how minimizers of these energies effectively serve as node embeddings that, when paired with a parameterized classifier, can be trained end-to-end via a supervised bilevel optimization process. Later, we draw parallels between the implicit architecture of the predictive models emerging from the proposed bilevel hypergraph optimization, and existing GNN architectures in common use. Empirically, we demonstrate state-of-the-art results on various hypergraph node classification benchmarks. Code is available at https://github.com/yxzwang/PhenomNN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2180352936",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "a865a9d76f6e84b5b5cc87676e328ffd8ccdd3d7",
            "title": "An AMR-based Link Prediction Approach for Document-level Event Argument Extraction",
            "abstract": "Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only implicitly, for instance, as additional features or training signals. Motivated by the fact that all event structures can be inferred from AMR, this work reformulates EAE as a link prediction problem on AMR graphs. Since AMR is a generic structure and does not perfectly suit EAE, we propose a novel graph structure, Tailored AMR Graph (TAG), which compresses less informative subgraphs and edge types, integrates span information, and highlights surrounding events in the same document. With TAG, we further propose a novel method using graph neural networks as a link prediction model to find event arguments. Our extensive experiments on WikiEvents and RAMS show that this simpler approach outperforms the state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so with reduced 56% inference time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145435513",
                    "name": "Yuqing Yang"
                },
                {
                    "authorId": "3187768",
                    "name": "Qipeng Guo"
                },
                {
                    "authorId": "12040998",
                    "name": "Xiangkun Hu"
                },
                {
                    "authorId": "39939186",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "1852415",
                    "name": "Zheng Zhang"
                }
            ]
        },
        {
            "paperId": "eb14f4252012127ac2d35502ab9fe55c08563629",
            "title": "UTC-IE: A Unified Token-pair Classification Architecture for Information Extraction",
            "abstract": "Information Extraction (IE) spans several tasks with different output structures, such as named entity recognition, relation extraction and event extraction. Previously, those tasks were solved with different models because of diverse task output structures. Through re-examining IE tasks, we find that all of them can be interpreted as extracting spans and span relations. They can further be decomposed into token-pair classification tasks by using the start and end token of a span to pinpoint the span, and using the start-to-start and end-to-end token pairs of two spans to determine the relation. Based on the reformulation, we propose a Unified Token-pair Classification architecture for Information Extraction (UTC-IE), where we introduce Plusformer on top of the token-pair feature matrix. Specifically, it models axis-aware interaction with plus-shaped self-attention and local interaction with Convolutional Neural Network over token pairs. Experiments show that our approach outperforms task-specific and unified models on all tasks in 10 datasets, and achieves better or comparable results on 2 joint IE datasets. Moreover, UTC-IE speeds up over state-of-the-art models on IE tasks significantly in most datasets, which verifies the effectiveness of our architecture.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2143831348",
                    "name": "Yu Sun"
                },
                {
                    "authorId": "50080067",
                    "name": "Xiaonan Li"
                },
                {
                    "authorId": "2118117212",
                    "name": "Yunhua Zhou"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                }
            ]
        },
        {
            "paperId": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
            "title": "Do Large Language Models Know What They Don't Know?",
            "abstract": "Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155273086",
                    "name": "Zhangyue Yin"
                },
                {
                    "authorId": "2112455065",
                    "name": "Qiushi Sun"
                },
                {
                    "authorId": "3187768",
                    "name": "Qipeng Guo"
                },
                {
                    "authorId": "2111123834",
                    "name": "Jiawen Wu"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "002c58077a1f1b296468b117230a1199e91f35c2",
            "title": "Black-Box Tuning for Language-Model-as-a-Service",
            "abstract": "Extremely large pre-trained language models (PTMs) such as GPT-3 are usually released as a service. It allows users to design task-specific prompts to query the PTMs through some black-box APIs. In such a scenario, which we call Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually unavailable. Can we optimize the task prompts by only accessing the model inference APIs? This paper proposes the black-box tuning framework to optimize the continuous prompt prepended to the input text via derivative-free optimization. Instead of optimizing in the original high-dimensional prompt space, which is intractable for traditional derivative-free optimization, we perform optimization in a randomly generated subspace due to the low intrinsic dimensionality of large PTMs. The experimental results show that the black-box tuning with RoBERTa on a few labeled samples not only significantly outperforms manual prompt and GPT-3's in-context learning, but also surpasses the gradient-based counterparts, i.e., prompt tuning and full model tuning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153345698",
                    "name": "Tianxiang Sun"
                },
                {
                    "authorId": "95329799",
                    "name": "Yunfan Shao"
                },
                {
                    "authorId": "2112125670",
                    "name": "Hong Qian"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                }
            ]
        },
        {
            "paperId": "08124e30cdc121eebc4ecd4c09cd3a1abfd3a865",
            "title": "Rebuild and Ensemble: Exploring Defense Against Text Adversaries",
            "abstract": "Adversarial attacks can mislead strong neural 001 models; as such, in NLP tasks, substitution-002 based attacks are dif\ufb01cult to defend. Cur-003 rent defense methods usually assume that the 004 substitution candidates are accessible, which 005 cannot be widely applied against substitution-006 agnostic attacks. In this paper, we propose 007 a Rebuild and Ensemble Framework to de-008 fend against adversarial attacks in texts with-009 out knowing the candidates. We propose a re-010 build mechanism to train a robust model and 011 ensemble the rebuilt texts during inference to 012 achieve good adversarial defense results. Ex-013 periments show that our method can improve 014 accuracy under the current strong attack meth-015 ods. 016",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107897400",
                    "name": "Linyang Li"
                },
                {
                    "authorId": "2042948910",
                    "name": "Demin Song"
                },
                {
                    "authorId": "1634814790",
                    "name": "Jiehang Zeng"
                },
                {
                    "authorId": "2146354584",
                    "name": "Ruotian Ma"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                }
            ]
        }
    ]
}