{
    "authorId": "2144507",
    "papers": [
        {
            "paperId": "0c7bb4e93e9ce2c826665c984110b9a4b5ffc3bf",
            "title": "Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case Study using Latent Dirichlet Allocation Method",
            "abstract": "Topic Modelling (TM) is a natural language processing (NLP) method for discovering topics in a collection of documents. Being an unsupervised method, it is a valuable tool when trying to summarise the main topics and topic changes in large quantities of data. In this study, we apply two prevalent topic modelling techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to analyse the change of topics in the Swedish newspaper articles about COVID-19. We describe the corpus we created including 6515 articles, methods applied, and statistics on topic changes over approximately 1 year and two months period of time from 17th January 2020 to 13th March 2021. We hope this work can be an asset for grounding applications of topic modelling and can be inspiring for similar case studies in an era with pandemics, to support socio-economic impact research as well as clinical and healthcare analytics. Our data and source code is openly available at https://github.com/aaronlifenghan/Swed_Covid_TM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2199740743",
                    "name": "Bernadeta Griciut.e"
                },
                {
                    "authorId": "15173692",
                    "name": "Lifeng Han"
                },
                {
                    "authorId": "145542037",
                    "name": "Alexander Koller"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "339d5f5a7387ecfaadcf63339e52e4d3fc7e814a",
            "title": "MedMine: Examining Pre-trained Language Models on Medication Mining",
            "abstract": "Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or improve their overall accuracy by ensemble learning and data augmentation. MedMine is part of the M3 Initiative \\url{https://github.com/HECTA-UoM/M3}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388009478",
                    "name": "Haifa Alrdahi"
                },
                {
                    "authorId": "2196038162",
                    "name": "Lifeng Han"
                },
                {
                    "authorId": "2230108139",
                    "name": "Hendrik vSuvalov"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "586bdad62d35342ec4aae0dd539379fff1ea4547",
            "title": "PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records",
            "abstract": "This paper describes PULSAR, our system submission at the ImageClef 2023 MediQA-Sum task on summarising patient-doctor dialogues into clinical records. The proposed framework relies on domain-specific pre-training, to produce a specialised language model which is trained on task-specific natural data augmented by synthetic data generated by a black-box LLM. We find limited evidence towards the efficacy of domain-specific pre-training and data augmentation, while scaling up the language model yields the best performance gains. Our approach was ranked second and third among 13 submissions on task B of the challenge. Our code is available at https://github.com/yuping-wu/PULSAR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "71034258",
                    "name": "Viktor Schlegel"
                },
                {
                    "authorId": "144966717",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2107934864",
                    "name": "Yuping Wu"
                },
                {
                    "authorId": "2057755271",
                    "name": "Anand Subramanian"
                },
                {
                    "authorId": "2117824172",
                    "name": "Thanh-Tung Nguyen"
                },
                {
                    "authorId": "41124383",
                    "name": "Abhinav Ramesh Kashyap"
                },
                {
                    "authorId": "143984297",
                    "name": "Daniel Beck"
                },
                {
                    "authorId": "2152293660",
                    "name": "Xiaojun Zeng"
                },
                {
                    "authorId": "1400900759",
                    "name": "R. Batista-Navarro"
                },
                {
                    "authorId": "2057271731",
                    "name": "Stefan Winkler"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "5a4b517fd4c6e7623605b091dbe2a3bc9d2e4fa2",
            "title": "Student\u2019s t-Distribution: On Measuring the Inter-Rater Reliability When the Observations are Scarce",
            "abstract": "In natural language processing (NLP) we always rely on human judgement as the golden quality evaluation method. However, there has been an ongoing debate on how to better evaluate inter-rater reliability (IRR) levels for certain evaluation tasks, such as translation quality evaluation (TQE), especially when the data samples (observations) are very scarce. In this work, we first introduce the study on how to estimate the confidence interval for the measurement value when only one data (evaluation) point is available. Then, this leads to our example with two human-generated observational scores, for which, we introduce \u201cStudent\u2019s t-Distribution\u201d method and explain how to use it to measure the IRR score using only these two data points, as well as the confidence intervals (CIs) of the quality evaluation. We give a quantitative analysis of how the evaluation confidence can be greatly improved by introducing more observations, even if only one extra observation. We encourage researchers to report their IRR scores in all possible means, e.g. using Student\u2019s t-Distribution method whenever possible; thus making the NLP evaluation more meaningful, transparent, and trustworthy. This t-Distribution method can be also used outside of NLP fields to measure IRR level for trustworthy evaluation of experimental investigations, whenever the observational data is scarce.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1404106344",
                    "name": "Serge Gladkoff"
                },
                {
                    "authorId": "49953231",
                    "name": "Lifeng Han"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "662b16cf6b212ec73f3508f6666ab89510c84428",
            "title": "Team:PULSAR at ProbSum 2023:PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients\u2019 Problems and Data Augmentation with Black-box Large Language Models",
            "abstract": "Medical progress notes play a crucial role in documenting a patient\u2019s hospital journey, including his or her condition, treatment plan, and any updates for healthcare providers. Automatic summarisation of a patient\u2019s problems in the form of a \u201cproblem list\u201d can aid stakeholders in understanding a patient\u2019s condition, reducing workload and cognitive bias. BioNLP 2023 Shared Task 1A focusses on generating a list of diagnoses and problems from the provider\u2019s progress notes during hospitalisation. In this paper, we introduce our proposed approach to this task, which integrates two complementary components. One component employs large language models (LLMs) for data augmentation; the other is an abstractive summarisation LLM with a novel pre-training objective for generating the patients\u2019 problems summarised as a list. Our approach was ranked second among all submissions to the shared task. The performance of our model on the development and test datasets shows that our approach is more robust on unknown data, with an improvement of up to 3.1 points over the same size of the larger model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144966717",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2107934864",
                    "name": "Yuping Wu"
                },
                {
                    "authorId": "71034258",
                    "name": "Viktor Schlegel"
                },
                {
                    "authorId": "1400900759",
                    "name": "R. Batista-Navarro"
                },
                {
                    "authorId": "2117824172",
                    "name": "Thanh-Tung Nguyen"
                },
                {
                    "authorId": "41124383",
                    "name": "Abhinav Ramesh Kashyap"
                },
                {
                    "authorId": "2152293660",
                    "name": "Xiaojun Zeng"
                },
                {
                    "authorId": "2068522008",
                    "name": "Daniel Beck"
                },
                {
                    "authorId": "2057271731",
                    "name": "Stefan Winkler"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "745e015bbe917a7a3dbd5f50c2198a027d67475d",
            "title": "MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction",
            "abstract": "Extracting meaningful drug-related information chunks, such as adverse drug events (ADE), is crucial for preventing morbidity and saving many lives. Most ADEs are reported via an unstructured conversation with the medical context, so applying a general entity recognition approach is not sufficient enough. In this paper, we propose a new multi-aspect cross-integration framework for drug entity/event detection by capturing and aligning different context/language/knowledge properties from drug-related documents. We first construct multi-aspect encoders to describe semantic, syntactic, and medical document contextual information by conducting those slot tagging tasks, main drug entity/event detection, part-of-speech tagging, and general medical named entity recognition. Then, each encoder conducts cross-integration with other contextual information in three ways: the key-value cross, attention cross, and feedforward cross, so the multi-encoders are integrated in depth. Our model outperforms all SOTA on two widely used tasks, flat entity detection and discontinuous event extraction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34463165",
                    "name": "Jie Yang"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "973ce9aa3251857693abc21438b35432a8d83bec",
            "title": "Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?",
            "abstract": "Translation Quality Estimation (TQE) is an essential step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. This work examines whether the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using \\textbf{eight language pairs} including English to Italian, German, French, Japanese, Dutch, Portuguese, Turkish, and Chinese training corpora, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. \\textit{if the translation needs to be edited}. However, there is definitely much space to improve the model accuracy, e.g. they are 82.42\\% and 83.69\\% for English-Italian and English-German respectively using our experimental settings. English-Italiano bilingual Abstract is available in the paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1404106344",
                    "name": "Serge Gladkoff"
                },
                {
                    "authorId": "2124124385",
                    "name": "G. Erofeev"
                },
                {
                    "authorId": "49953231",
                    "name": "Lifeng Han"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "a788e168b2fe8388fca0b90cbbe89666f178a585",
            "title": "MedTem2.0: Prompt-based Temporal Classification of Treatment Events from Discharge Summaries",
            "abstract": "We use Prompt-based learning on LLMs for Temporal Classification of Treatment Events from Discharge Summaries of clinical data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2221300963",
                    "name": "Yang Cui"
                },
                {
                    "authorId": "2221301351",
                    "name": "Lifeng Han"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "a8395fa055d173df10efb0eb48231de8e9aa068c",
            "title": "Do You Hear The People Sing? Key Point Analysis via Iterative Clustering and Abstractive Summarisation",
            "abstract": "Argument summarisation is a promising but currently under-explored field. Recent work has aimed to provide textual summaries in the form of concise and salient short texts, i.e., key points (KPs), in a task known as Key Point Analysis (KPA). One of the main challenges in KPA is finding high-quality key point candidates from dozens of arguments even in a small corpus. Furthermore, evaluating key points is crucial in ensuring that the automatically generated summaries are useful. Although automatic methods for evaluating summarisation have considerably advanced over the years, they mainly focus on sentence-level comparison, making it difficult to measure the quality of a summary (a set of KPs) as a whole. Aggravating this problem is the fact that human evaluation is costly and unreproducible. To address the above issues, we propose a two-step abstractive summarisation framework based on neural topic modelling with an iterative clustering procedure, to generate key points which are aligned with how humans identify key points. Our experiments show that our framework advances the state of the art in KPA, with performance improvement of up to 14 (absolute) percentage points, in terms of both ROUGE and our own proposed evaluation metrics. Furthermore, we evaluate the generated summaries using a novel set-based evaluation toolkit. Our quantitative analysis demonstrates the effectiveness of our proposed evaluation metrics in assessing the quality of generated KPs. Human evaluation further demonstrates the advantages of our approach and validates that our proposed evaluation metric is more consistent with human judgment than ROUGE scores.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144966717",
                    "name": "Hao Li"
                },
                {
                    "authorId": "71034258",
                    "name": "Viktor Schlegel"
                },
                {
                    "authorId": "1400900759",
                    "name": "R. Batista-Navarro"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "edcef6e8187273418e5b1a1427ab49e230b8f79a",
            "title": "Investigating Large Language Models and Control Mechanisms to Improve Text Readability of Biomedical Abstracts",
            "abstract": "Biomedical literature often uses complex language and inaccessible professional terminologies. That is why sim-plification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (PLABA). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTScore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) mechanisms reported the highest SARI score of 46.54 and T5-base reported the highest BERTScore 72.62. In human evaluation, BART-L-w-CTs achieved a better simplicity score over T5-Base (2.9 vs. 2.2), while T5-Base achieved a better meaning preservation score over BART-L-w-CTs (3.1 vs. 2.6). We also categorised the system outputs with examples, hoping this will shed some light for future research on this task. Our codes, fine-tuned models, and data splits from the system development stage will be available at https://github.comlHECTA-UoMlPLABA-MU",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118273388",
                    "name": "Z. Li"
                },
                {
                    "authorId": "2245373398",
                    "name": "Samuel Belkadi"
                },
                {
                    "authorId": "2188780466",
                    "name": "Nicolo Micheletti"
                },
                {
                    "authorId": "2196038162",
                    "name": "Lifeng Han"
                },
                {
                    "authorId": "2895959",
                    "name": "M. Shardlow"
                },
                {
                    "authorId": "2144507",
                    "name": "G. Nenadic"
                }
            ]
        }
    ]
}