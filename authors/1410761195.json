{
    "authorId": "1410761195",
    "papers": [
        {
            "paperId": "7012710723ad8241bff64d33d38be80abb892d76",
            "title": "The Detection and Classification of Faults by the Use of Machine Learning Technique",
            "abstract": "In order to follow the technological evolution in the 21st century, and to detect the fault with the minimal effort spent, this paper was developed to identify the open and short circuit faults that may occur in the lighting and socket grids used in residential area while using machine learning algorithms. In this research, two cable networks were formed (the first one has a short circuit fault, and the second with an open circuit fault), then a neural network was used in order to detect these faults. Finally, several results will be shown that lead to verify the adequacy of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47162740",
                    "name": "M. Arnaout"
                },
                {
                    "authorId": "1410761195",
                    "name": "Ahmad Ghizzawi"
                },
                {
                    "authorId": "2149280406",
                    "name": "Ali Al-Hajj Hassan"
                },
                {
                    "authorId": "30015657",
                    "name": "Ali Koubayssi"
                },
                {
                    "authorId": "3190780",
                    "name": "M. Kafal"
                },
                {
                    "authorId": "32377380",
                    "name": "Ziad Noun"
                }
            ]
        },
        {
            "paperId": "7bd29fe63f1c22431613755d61c6e2fb60ba0665",
            "title": "Quantifying and Addressing Ranking Disparity in Human-Powered Data Acquisition",
            "abstract": "Algorithmic bias has been identified as a key challenge in many AI applications. One major source of bias is the data used to build these applications. For instance, many AI applications rely on human users to generate training data. The generated data might be biased if the data acquisition process is skewed towards certain groups of people based on say gender, ethnicity or location. This typically happens as a result of a hidden association between the people's qualifications for data acquisition and the people's protected attributes. In this paper, we study how to unveil and address disparity in data acquisition. We focus on the case where the data acquisition process involves ranking of people and we define disparity as the unbalanced targeting of people by the data acquisition process. To quantify disparity, we formulate an optimization problem that partitions people on their protected attributes, computes the qualifications of people in each partition, and finds the partitioning that exhibits the highest disparity in qualifications. Due to the combinatorial nature of our problem, we devise heuristics to navigate the space of partitions. We also discuss how to address disparity between partitions. We conduct a series of experiments on real and simulated datasets that demonstrate that our proposed approach is successful in quantifying and addressing ranking disparity in human-powered data acquisition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1410761195",
                    "name": "Ahmad Ghizzawi"
                },
                {
                    "authorId": "1414128859",
                    "name": "Anas Hosami"
                }
            ]
        },
        {
            "paperId": "5099ede1e3d999c9b469518d34af46f5c31f2897",
            "title": "Fairness of Scoring in Online Job Marketplaces",
            "abstract": "We study fairness of scoring in online job marketplaces. We focus on group fairness and aim to algorithmically explore how a scoring function, through which individuals are ranked for jobs, treats different demographic groups. Previous work on group-level fairness has focused on the case where groups are pre-defined or where they are defined using a single protected attribute (e.g., whites vs. blacks or males vs. females). In this article, we argue for the need to examine fairness for groups of people defined with any combination of protected attributes (the-so called subgroup fairness). Existing work also assumes the availability of worker\u2019s data (i.e., data transparency) and the scoring function (i.e., process transparency). We relax that assumption in this work and run user studies to assess the effect of different data and process transparency settings on the ability to assess fairness. To quantify the fairness of a scoring of a group of individuals, we formulate an optimization problem to find a partitioning of those individuals on their protected attributes that exhibits the highest unfairness with respect to the scoring function. The scoring function yields one histogram of score distributions per partition and we rely on Earth Mover\u2019s Distance, a measure that is commonly used to compare histograms, to quantify unfairness. Since the number of ways to partition individuals is exponential in the number of their protected attributes, we propose a heuristic algorithm to navigate the space of all possible partitionings to identify the one with the highest unfairness. We evaluate our algorithm using a simulation of a crowdsourcing platform and show that it can effectively quantify unfairness of various scoring functions. We additionally run experiments to assess the applicability of our approach in other less-transparent data and process settings. Finally, we demonstrate the effectiveness of our approach in assessing fairness of scoring in a real dataset crawled from the online job marketplace TaskRabbit.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1410761195",
                    "name": "Ahmad Ghizzawi"
                }
            ]
        },
        {
            "paperId": "804f4cfede2f08501e15e5f9af7062489e924660",
            "title": "Fairness in Online Jobs: A Case Study on TaskRabbit and Google",
            "abstract": "Online job marketplaces are becoming very popular. Either jobs or people are ranked by algorithms. For example, Google and Facebook job search return a ranked list of jobs given a search query. TaskRabbit and Fiverr, on the other hand, produce rank-ings of workers for a given query. Qapa, an online marketplace, can be used to rank both workers and jobs. In this paper, we develop a unified framework for fairness to study ranking workers and jobs. We case study two particular sites: Google job search and TaskRabbit. Our framework addresses group fairness where groups are obtained with any combination of protected attributes. We define a measure for unfairness for a given group, query and location. We also define two generic fairness problems that we address in our framework: quantification, such as finding the k groups (resp., queries, locations) for which the site is most or least unfair, and comparison, such as finding the locations at which fairness between two groups differs from all locations, or finding the queries for which fairness at two locations differ from all queries. Since the number of groups, queries and locations can be arbitrarily large, we adapt Fagin top-k algorithms to address our fairness problems. To evaluate our framework, we run extensive experiments on two datasets crawled from TaskRabbit and Google job search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1410761195",
                    "name": "Ahmad Ghizzawi"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "51493818",
                    "name": "Emilie Hoareau"
                },
                {
                    "authorId": "1714010",
                    "name": "P. Mulhem"
                }
            ]
        },
        {
            "paperId": "765ed1e955f823f12d1970b6afd9aa11d42e3979",
            "title": "FaiRank: An Interactive System to Explore Fairness of Ranking in Online Job Marketplaces",
            "abstract": "We demonstrate FaiRank, an interactive system to explore fairness of ranking in online job marketplaces. FaiRank takes as input a set of individuals and their attributes, some of which are protected, and a scoring function, through which those individuals are ranked for jobs. It finds a partitioning of individuals on their protected attributes over which fairness of the scoring function is quantified. FaiRank has several appealing features: (1) It can be used by different users: the auditor whose role is to monitor the fairness of ranking in a job marketplace, the job owner seeking to examine the influence of a scoring function and its variants on the ranking of candidates for a job, and the end-user who wants to assess the fairness of jobs on different marketplaces; (2) It is able to quantify fairness under different data and process transparency settings: when some attributes are anonymized and when only the ranking (and not the scoring function) is available; (3) It is interactive and lets its users explore different scoring functions and examine how fairness evolves; (4) It is generic and provides the ability to quantify different notions of fairness. Our demonstration will provide attendees with several scenarios for fairness of ranking in job marketplaces to experiment with and acquire an understanding of this important research question and its impact in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1410761195",
                    "name": "Ahmad Ghizzawi"
                },
                {
                    "authorId": "46238493",
                    "name": "Julien Marinescu"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2236740",
                    "name": "G. Bisson"
                }
            ]
        },
        {
            "paperId": "df4431c123a62b7537af5a0f79cf599c886a776c",
            "title": "Exploring Fairness of Ranking in Online Job Marketplaces",
            "abstract": "We study fairness of ranking in online job marketplaces. We focus on group fairness and aim to algorithmically explore how a scoring function, through which individuals are ranked for jobs, treats different demographic groups. Previous work on group-level fairness has focused on the case where groups are pre-defined or where they are defined using a single protected attribute (e.g., Caucasian vs Asian). In this paper, we argue for the need to examine fairness for groups of people defined with any combination of protected attributes. To do this, we formulate an optimization problem to find a partitioning of individuals on their protected attributes that exhibits the highest unfairness with respect to the scoring function. The scoring function yields one histogram of score distributions per partition and we rely on Earth Mover's Distance , a measure that is commonly used to compare histograms, to quantify unfairness. Since the number of ways to partition individuals is exponential in the number of their protected attribute values, we propose two heuristic algorithms to navigate the space of all possible partitionings to identify the one with the highest unfairness. We evaluate our algorithms using a simulation of a crowdsourcing platform and show that they can effectively quantify unfairness of various scoring functions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1414231237",
                    "name": "Christine El Atie"
                },
                {
                    "authorId": "1410761195",
                    "name": "Ahmad Ghizzawi"
                },
                {
                    "authorId": "1414308160",
                    "name": "Bilel Oualha"
                }
            ]
        }
    ]
}