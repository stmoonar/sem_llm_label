{
    "authorId": "1400419309",
    "papers": [
        {
            "paperId": "1527d3b661154ff7310fa2759b6dd0ddfd559492",
            "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
            "abstract": "With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generator were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115471757",
                    "name": "Fatemehsadat Mireshghallah"
                },
                {
                    "authorId": "2138547910",
                    "name": "Justus Mattern"
                },
                {
                    "authorId": "39219411",
                    "name": "Sicun Gao"
                },
                {
                    "authorId": "2520493",
                    "name": "R. Shokri"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "15b08595533bfc640f4dd470ca7a2273badec20a",
            "title": "Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction",
            "abstract": "In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2228382825",
                    "name": "Keren Shao"
                },
                {
                    "authorId": "115727183",
                    "name": "K. Chen"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                },
                {
                    "authorId": "2204186",
                    "name": "S. Dubnov"
                }
            ]
        },
        {
            "paperId": "2f4d3ab8ce8a37b035bcf9d007b78cd88bf1453d",
            "title": "Alt-Text with Context: Improving Accessibility for Images on Twitter",
            "abstract": "In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. More than just a special case of image captioning, alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative. We address this task with a multimodal model that conditions on both textual information from the associated social media post as well as visual signal from the image, and demonstrate that the utility of these two information sources stacks. We put forward a new dataset of 371k images paired with alt-text and tweets scraped from Twitter and evaluate on it across a variety of automated metrics as well as human evaluation. We show that our approach of conditioning on both tweet text and visual information significantly outperforms prior work, by more than 2x on BLEU@4.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122931760",
                    "name": "Nikita Srivatsan"
                },
                {
                    "authorId": "151367676",
                    "name": "Sofia Samaniego"
                },
                {
                    "authorId": "1752662",
                    "name": "Omar U. Florez"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "464edfd902f652d3ab6a25dbb6d9fa47cc3246a9",
            "title": "MusicLDM: Enhancing Novelty in text-to-music Generation Using Beat-Synchronous mixup Strategies",
            "abstract": "Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "115727183",
                    "name": "K. Chen"
                },
                {
                    "authorId": "2107957455",
                    "name": "Yusong Wu"
                },
                {
                    "authorId": "151503689",
                    "name": "Haohe Liu"
                },
                {
                    "authorId": "2174178585",
                    "name": "Marianna Nezhurina"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                },
                {
                    "authorId": "2204186",
                    "name": "S. Dubnov"
                }
            ]
        },
        {
            "paperId": "4f1a26b055d9973853c039bf554b19a6587ab286",
            "title": "Jointly modeling products and resource pages for task-oriented recommendation",
            "abstract": "Modeling high-level user intent in recommender systems can improve performance, although it is often difficult to obtain a ground truth measure of this intent. In this paper, we investigate a novel way to obtain such an intent signal by leveraging resource pages associated with a particular task. We jointly model product interactions and resource page interactions to create a system which can recommend both products and resource pages to users. Our experiments consider the domain of home improvement product recommendation, where resource pages are DIY (do-it-yourself) project pages from Lowes.com. Each DIY page provides a list of tools, materials, and step-by-step instructions to complete a DIY project, such as building a deck, installing cabinets, and fixing a leaking pipe. We use this data as an indicator of the intended project, which is a natural high-level intent signal for home improvement shoppers. We then extend a state-of-the-art system to incorporate this new intent data, and show a significant improvement in the ability of the system to recommend products. We further demonstrate that our system can be used to successfully recommend DIY project pages to users. We have taken initial steps towards deploying our method for project recommendation in production on the Lowe\u2019s website and for recommendations through marketing emails.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32370715",
                    "name": "B. Duncan"
                },
                {
                    "authorId": "3378098",
                    "name": "Surya Kallumadi"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "55324ec5662bbea2e226ce3d8e6258a62836e940",
            "title": "Universal Source Separation with Weakly Labelled Data",
            "abstract": "Universal source separation (USS) is a fundamental research task for computational auditory scene analysis, which aims to separate mono recordings into individual source tracks. There are three potential challenges awaiting the solution to the audio source separation task. First, previous audio source separation systems mainly focus on separating one or a limited number of specific sources. There is a lack of research on building a unified system that can separate arbitrary sources via a single model. Second, most previous systems require clean source data to train a separator, while clean source data are scarce. Third, there is a lack of USS system that can automatically detect and separate active sound classes in a hierarchical level. To use large-scale weakly labeled/unlabeled audio data for audio source separation, we propose a universal audio source separation framework containing: 1) an audio tagging model trained on weakly labeled data as a query net; and 2) a conditional source separation model that takes query net outputs as conditions to separate arbitrary sound sources. We investigate various query nets, source separation models, and training strategies and propose a hierarchical USS strategy to automatically detect and separate sound classes from the AudioSet ontology. By solely leveraging the weakly labelled AudioSet, our USS system is successful in separating a wide variety of sound classes, including sound event separation, music source separation, and speech enhancement. The USS system achieves an average signal-to-distortion ratio improvement (SDRi) of 5.57 dB over 527 sound classes of AudioSet; 10.57 dB on the DCASE 2018 Task 2 dataset; 8.12 dB on the MUSDB18 dataset; an SDRi of 7.28 dB on the Slakh2100 dataset; and an SSNR of 9.00 dB on the voicebank-demand dataset. We release the source code at https://github.com/bytedance/uss",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "8391640",
                    "name": "Qiuqiang Kong"
                },
                {
                    "authorId": "115727183",
                    "name": "K. Chen"
                },
                {
                    "authorId": "151503689",
                    "name": "Haohe Liu"
                },
                {
                    "authorId": "46214809",
                    "name": "Xingjian Du"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                },
                {
                    "authorId": "2204186",
                    "name": "S. Dubnov"
                },
                {
                    "authorId": "102690737",
                    "name": "M. Plumbley"
                }
            ]
        },
        {
            "paperId": "7a1ddb1ccf78f43f3c8ca9d0963c4782db8a5b85",
            "title": "Towards Answering Climate Questionnaires from Unstructured Climate Reports",
            "abstract": "The topic of Climate Change (CC) has received limited attention in NLP despite its urgency. Activists and policymakers need NLP tools to effectively process the vast and rapidly growing unstructured textual climate reports into structured form. To tackle this challenge we introduce two new large-scale climate questionnaire datasets and use their existing structure to train self-supervised models. We conduct experiments to show that these models can learn to generalize to climate disclosures of different organizations types than seen during training. We then use these models to help align texts from unstructured climate documents to the semi-structured questionnaires in a human pilot study. Finally, to support further NLP research in the climate domain we introduce a benchmark of existing climate text classification datasets to better evaluate and compare existing models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064240141",
                    "name": "Daniel M. Spokoyny"
                },
                {
                    "authorId": "103242455",
                    "name": "Tanmay Laud"
                },
                {
                    "authorId": "116070555",
                    "name": "Thomas W. Corringham"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "924036c17537adc8574a1476bf13d9d17a6eebae",
            "title": "ClimaBench: A Benchmark Dataset For Climate Change Text Understanding in English",
            "abstract": "The topic of Climate Change (CC) has received limited attention in NLP despite its real world urgency. Activists and policy-makers need NLP tools in order to effectively process the vast and rapidly growing textual data produced on CC. Their utility, however, primarily depends on whether the current state-of-the-art models can generalize across various tasks in the CC domain. In order to address this gap, we introduce Climate Change Benchmark (ClimaBench), a benchmark collection of existing disparate datasets for evaluating model performance across a diverse set of CC NLU tasks systematically. Further, we enhance the benchmark by releasing two large-scale labelled text classi\ufb01cation and question-answering datasets curated from publicly available environmental disclosures. Lastly, we provide an analysis of several generic and CC-oriented models answering whether \ufb01ne-tuning on domain text offers any improvements across these tasks. We hope this work provides a standard assessment tool for research on CC text data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "103242455",
                    "name": "Tanmay Laud"
                },
                {
                    "authorId": "2064240141",
                    "name": "Daniel M. Spokoyny"
                },
                {
                    "authorId": "116070555",
                    "name": "Thomas W. Corringham"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        },
        {
            "paperId": "c70c11d6de5eec2677eaa87fd3112068db6fedfe",
            "title": "CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models",
            "abstract": "Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pre-trained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that the pretrained diffusion prior can reduce the modality transfer gap. While we focus on text-to-audio synthesis, the proposed model can also generate audio from image queries, and it shows competitive performance against a state-of-the-art image-to-audio synthesis model in a subjective listening test. This study offers a new direction of approaching text-to-audio synthesis that leverages the naturally-occurring audio-visual correspondence in videos and the power of pretrained language-vision models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2113412153",
                    "name": "Hao-Wen Dong"
                },
                {
                    "authorId": "2111038051",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "143683184",
                    "name": "Jordi Pons"
                },
                {
                    "authorId": "2058911648",
                    "name": "Gautam Bhattacharya"
                },
                {
                    "authorId": "2074982546",
                    "name": "Santiago Pascual"
                },
                {
                    "authorId": "145783638",
                    "name": "J. Serr\u00e0"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "cb754310302086dfbbcd098263200e2a03f65874",
            "title": "Membership Inference Attacks against Language Models via Neighbourhood Comparison",
            "abstract": "Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138547910",
                    "name": "Justus Mattern"
                },
                {
                    "authorId": "2115471757",
                    "name": "Fatemehsadat Mireshghallah"
                },
                {
                    "authorId": "2111472502",
                    "name": "Zhijing Jin"
                },
                {
                    "authorId": "1707625",
                    "name": "B. Scholkopf"
                },
                {
                    "authorId": "2790926",
                    "name": "Mrinmaya Sachan"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                }
            ]
        }
    ]
}