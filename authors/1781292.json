{
    "authorId": "1781292",
    "papers": [
        {
            "paperId": "098be01c95b4c18e2c7e8b4164d29dbb0903e71f",
            "title": "Can a Multichoice Dataset be Repurposed for Extractive Question Answering?",
            "abstract": "The rapid evolution of Natural Language Processing (NLP) has favored major languages such as English, leaving a significant gap for many others due to limited resources. This is especially evident in the context of data annotation, a task whose importance cannot be underestimated, but which is time-consuming and costly. Thus, any dataset for resource-poor languages is precious, in particular when it is task-specific. Here, we explore the feasibility of repurposing existing datasets for a new NLP task: we repurposed the Belebele dataset (Bandarkar et al., 2023), which was designed for multiple-choice question answering (MCQA), to enable extractive QA (EQA) in the style of machine reading comprehension. We present annotation guidelines and a parallel EQA dataset for English and Modern Standard Arabic (MSA). We also present QA evaluation results for several monolingual and cross-lingual QA pairs including English, MSA, and five Arabic dialects. Our aim is to enable others to adapt our approach for the 120+ other language variants in Belebele, many of which are deemed under-resourced. We also conduct a thorough analysis and share our insights from the process, which we hope will contribute to a deeper understanding of the challenges and the opportunities associated with task reformulation in NLP research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298756162",
                    "name": "Teresa Lynn"
                },
                {
                    "authorId": "51935928",
                    "name": "Malik H. Altakrori"
                },
                {
                    "authorId": "148087360",
                    "name": "S. Magdy"
                },
                {
                    "authorId": "2211732585",
                    "name": "Rocktim Jyoti Das"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2056258384",
                    "name": "Mohamed Nasr"
                },
                {
                    "authorId": "2282523149",
                    "name": "Younes Samih"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2111356",
                    "name": "S. Godbole"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                }
            ]
        },
        {
            "paperId": "18bb9c8430c05d6cdc89b837fab4c976fb82f3a9",
            "title": "Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support",
            "abstract": "Clients wishing to implement generative AI in the domain of IT Support and AIOps face two critical issues: domain coverage and model size constraints due to model choice limitations. Clients might choose to not use larger proprietary models such as GPT-4 due to cost and privacy concerns and so are limited to smaller models with potentially less domain coverage that do not generalize to the client's domain. Retrieval augmented generation is a common solution that addresses both of these issues: a retrieval system first retrieves the necessary domain knowledge which a smaller generative model leverages as context for generation. We present a system developed for a client in the IT Support domain for support case solution recommendation that combines retrieval augmented generation (RAG) for answer generation with an encoder-only model for classification and a generative large language model for query generation. We cover architecture details, data collection and annotation, development journey and preliminary validations, expected final deployment process and evaluation plans, and finally lessons learned.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40521719",
                    "name": "Paulina Toro Isaza"
                },
                {
                    "authorId": "2283157983",
                    "name": "Michael Nidd"
                },
                {
                    "authorId": "2138527995",
                    "name": "Noah Zheutlin"
                },
                {
                    "authorId": "2322816664",
                    "name": "Jae-wook Ahn"
                },
                {
                    "authorId": "2667580",
                    "name": "C. Bhatt"
                },
                {
                    "authorId": "2322615018",
                    "name": "Yu Deng"
                },
                {
                    "authorId": "1785364",
                    "name": "R. Mahindru"
                },
                {
                    "authorId": "2322446926",
                    "name": "Martin Franz"
                },
                {
                    "authorId": "2322447430",
                    "name": "Hans Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                }
            ]
        },
        {
            "paperId": "2355cd6d7415c23e2b97d6835f45e1c00bbd43b0",
            "title": "CHRONOS: A Schema-Based Event Understanding and Prediction System",
            "abstract": "Chronological and Hierarchical Reasoning Over Naturally Occurring Schemas (CHRONOS) is a system that combines language model-based natural language processing with symbolic knowledge representations to analyze and make predictions about newsworthy events. CHRONOS consists of an event-centric information extraction pipeline and a complex event schema instantiation and prediction system. Resulting predictions are detailed with arguments, event types from Wikidata, schema-based justifications, and source document provenance. We evaluate our system by its ability to capture the structure of unseen events described in news articles and make plausible predictions as judged by human annotators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293665108",
                    "name": "Maria Chang"
                },
                {
                    "authorId": "2297836",
                    "name": "Achille Fokoue"
                },
                {
                    "authorId": "2291963641",
                    "name": "Rosario Uceda-Sosa"
                },
                {
                    "authorId": "112884386",
                    "name": "Parul Awasthy"
                },
                {
                    "authorId": "2265758926",
                    "name": "Ken Barker"
                },
                {
                    "authorId": "1666248581",
                    "name": "Sadhana Kumaravel"
                },
                {
                    "authorId": "1728091",
                    "name": "Oktie Hassanzadeh"
                },
                {
                    "authorId": "2293521529",
                    "name": "Elton Soares"
                },
                {
                    "authorId": "2293430358",
                    "name": "Tian Gao"
                },
                {
                    "authorId": "3237019",
                    "name": "D. Bhattacharjya"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                }
            ]
        },
        {
            "paperId": "62390c0002c6de5e9252e12e2eec2e78ebc1c3d4",
            "title": "CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems",
            "abstract": "Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline. ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant room for improvement in grounded RAG. CLAPNQ is publicly available at https://github.com/primeqa/clapnq",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144063596",
                    "name": "Sara Rosenthal"
                },
                {
                    "authorId": "2707234",
                    "name": "Avirup Sil"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                }
            ]
        },
        {
            "paperId": "6995f2c2c3b13518c1e3489617ae40a9e7a11b95",
            "title": "Prompts as Auto-Optimized Training Hyperparameters: Training Best-in-Class IR Models from Scratch with 10 Gold Labels",
            "abstract": "We develop a method for training small-scale (under 100M parameter) neural information retrieval models with as few as 10 gold relevance labels. The method depends on generating synthetic queries for documents using a language model (LM), and the key step is that we automatically optimize the LM prompt that is used to generate these queries based on training quality. In experiments with the BIRCO benchmark, we find that models trained with our method outperform RankZephyr and are competitive with RankLLama, both of which are 7B parameter models trained on over 100K labels. These findings point to the power of automatic prompt optimization for synthetic dataset generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307001376",
                    "name": "Jasper Xian"
                },
                {
                    "authorId": "2306998031",
                    "name": "Saron Samuel"
                },
                {
                    "authorId": "2307001087",
                    "name": "Faraz Khoubsirat"
                },
                {
                    "authorId": "2307001538",
                    "name": "Ronak Pradeep"
                },
                {
                    "authorId": "2937809",
                    "name": "Md Arafat Sultan"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "2707234",
                    "name": "Avirup Sil"
                },
                {
                    "authorId": "2254255092",
                    "name": "Christopher Potts"
                },
                {
                    "authorId": "144112155",
                    "name": "O. Khattab"
                }
            ]
        },
        {
            "paperId": "978aaaeccb4e67041d7c54fc1c3f4520824d8c1a",
            "title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
            "abstract": "It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero-shot baseline as well as a supervised fine-tuned model on human annotated data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2289844084",
                    "name": "Keshav Ramji"
                },
                {
                    "authorId": "2145430350",
                    "name": "Young-Suk Lee"
                },
                {
                    "authorId": "2270425544",
                    "name": "R. Astudillo"
                },
                {
                    "authorId": "2284679524",
                    "name": "M. Sultan"
                },
                {
                    "authorId": "2138053379",
                    "name": "Tahira Naseem"
                },
                {
                    "authorId": "2273469018",
                    "name": "Asim Munawar"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                }
            ]
        },
        {
            "paperId": "cbde6f07977255cd5b571d93b497aa2874a7a544",
            "title": "Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks",
            "abstract": "Large language models (LLMs) have recently shown tremendous promise in serving as the backbone to agentic systems, as demonstrated by their performance in multi-faceted, challenging benchmarks like SWE-Bench and Agent-Bench. However, to realize the true potential of LLMs as autonomous agents, they must learn to identify, call, and interact with external tools and application program interfaces (APIs) to complete complex tasks. These tasks together are termed function calling. Endowing LLMs with function calling abilities leads to a myriad of advantages, such as access to current and domain-specific information in databases and knowledge sources, and the ability to outsource tasks that can be reliably performed by tools, e.g., a Python interpreter or calculator. While there has been significant progress in function calling with LLMs, there is still a dearth of open models that perform on par with proprietary LLMs like GPT, Claude, and Gemini. Therefore, in this work, we introduce the GRANITE-20B-FUNCTIONCALLING model under an Apache 2.0 license. The model is trained using a multi-task training approach on seven fundamental tasks encompassed in function calling, those being Nested Function Calling, Function Chaining, Parallel Functions, Function Name Detection, Parameter-Value Pair Detection, Next-Best Function, and Response Generation. We present a comprehensive evaluation on multiple out-of-domain datasets comparing GRANITE-20B-FUNCTIONCALLING to more than 15 other best proprietary and open models. GRANITE-20B-FUNCTIONCALLING provides the best performance among all open models on the Berkeley Function Calling Leaderboard and fourth overall. As a result of the diverse tasks and datasets used for training our model, we show that GRANITE-20B-FUNCTIONCALLING has better generalizability on multiple tasks in seven different evaluation datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257345798",
                    "name": "Ibrahim Abdelaziz"
                },
                {
                    "authorId": "2257347920",
                    "name": "Kinjal Basu"
                },
                {
                    "authorId": "2266391900",
                    "name": "Mayank Agarwal"
                },
                {
                    "authorId": "1666248581",
                    "name": "Sadhana Kumaravel"
                },
                {
                    "authorId": "2205536978",
                    "name": "Matt Stallone"
                },
                {
                    "authorId": "1819152",
                    "name": "Rameswar Panda"
                },
                {
                    "authorId": "2587541",
                    "name": "Yara Rizk"
                },
                {
                    "authorId": "2127473246",
                    "name": "G. Bhargav"
                },
                {
                    "authorId": "41036307",
                    "name": "M. Crouse"
                },
                {
                    "authorId": "66161659",
                    "name": "Chulaka Gunasekara"
                },
                {
                    "authorId": "3192316",
                    "name": "S. Ikbal"
                },
                {
                    "authorId": "2310220522",
                    "name": "Sachin Joshi"
                },
                {
                    "authorId": "9621738",
                    "name": "Hima P. Karanam"
                },
                {
                    "authorId": "2309215024",
                    "name": "Vineet Kumar"
                },
                {
                    "authorId": "2273469018",
                    "name": "Asim Munawar"
                },
                {
                    "authorId": "2965855",
                    "name": "S. Neelam"
                },
                {
                    "authorId": "1916865",
                    "name": "Dinesh Raghu"
                },
                {
                    "authorId": "2027112872",
                    "name": "Udit Sharma"
                },
                {
                    "authorId": "2284224026",
                    "name": "Adriana Meza Soria"
                },
                {
                    "authorId": "2273710536",
                    "name": "Dheeraj Sreedhar"
                },
                {
                    "authorId": "2077182665",
                    "name": "P. Venkateswaran"
                },
                {
                    "authorId": "2682919",
                    "name": "Merve Unuvar"
                },
                {
                    "authorId": "2309182239",
                    "name": "David Cox"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "2257345942",
                    "name": "Luis A. Lastras"
                },
                {
                    "authorId": "2123450380",
                    "name": "P. Kapanipathi"
                }
            ]
        },
        {
            "paperId": "12aa2b1e9556c20752e37e8b18d0e396c0cea1c5",
            "title": "Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs",
            "abstract": "Using in-context learning (ICL) for data generation, techniques such as Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023) can train strong conversational agents with only a small amount of human supervision. One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public. Here we explore the application of such techniques to language models that are much smaller (around 10B--40B parameters) and have permissive licenses. We find the Self-Instruct approach to be less effective at these sizes and propose new ICL methods that draw on two main ideas: (a) Categorization and simplification of the ICL templates to make prompt learning easier for the LM, and (b) Ensembling over multiple LM outputs to help select high-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct seed tasks and employs separate pipelines for instructions that require an input and instructions that do not. Empirical investigations with different LMs show that: (1) Our proposed method yields higher-quality instruction tuning data than Self-Instruct, (2) It improves performances of both vanilla and instruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned LMs generate more useful outputs than their larger un-tuned counterparts. Our codebase is available at https://github.com/IBM/ensemble-instruct.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145430350",
                    "name": "Young-Suk Lee"
                },
                {
                    "authorId": "2937809",
                    "name": "Md Arafat Sultan"
                },
                {
                    "authorId": "2261287843",
                    "name": "Yousef El-Kurdi"
                },
                {
                    "authorId": "2261287688",
                    "name": "Tahira Naseem Asim Munawar"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "3394760",
                    "name": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                }
            ]
        },
        {
            "paperId": "1e4acf7ec40e6477d8cbb3a956838dbd90c6c150",
            "title": "AMR Parsing with Instruction Fine-tuned Pre-trained Language Models",
            "abstract": "Instruction fine-tuned language models on a collection of instruction annotated datasets (FLAN) have shown highly effective to improve model performance and generalization to unseen tasks. However, a majority of standard parsing tasks including abstract meaning representation (AMR), universal dependency (UD), semantic role labeling (SRL) has been excluded from the FLAN collections for both model training and evaluations. In this paper, we take one of such instruction fine-tuned pre-trained language models, i.e. FLAN-T5, and fine-tune them for AMR parsing. Our extensive experiments on various AMR parsing tasks including AMR2.0, AMR3.0 and BioAMR indicate that FLAN-T5 fine-tuned models out-perform previous state-of-the-art models across all tasks. In addition, full fine-tuning followed by the parameter efficient fine-tuning, LoRA, further improves the model performances, setting new state-of-the-arts in Smatch on AMR2.0 (86.4), AMR3.0 (84.9) and BioAMR (82.3).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145430350",
                    "name": "Young-Suk Lee"
                },
                {
                    "authorId": "3394760",
                    "name": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                },
                {
                    "authorId": "1707117",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "2138053379",
                    "name": "Tahira Naseem"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                }
            ]
        },
        {
            "paperId": "2daf79935cda2a2bbbd44a4f5bf41d2da9823223",
            "title": "Scalable Learning of Latent Language Structure With Logical Offline Cycle Consistency",
            "abstract": "We introduce Logical Offline Cycle Consistency Optimization (LOCCO), a scalable, semi-supervised method for training a neural semantic parser. Conceptually, LOCCO can be viewed as a form of self-learning where the semantic parser being trained is used to generate annotations for unlabeled text that are then used as new supervision. To increase the quality of annotations, our method utilizes a count-based prior over valid formal meaning representations and a cycle-consistency score produced by a neural text generation model as additional signals. Both the prior and semantic parser are updated in an alternate fashion from full passes over the training data, which can be seen as approximating the marginalization of latent structures through stochastic variational inference. The use of a count-based prior, frozen text generation model, and offline annotation process yields an approach with negligible complexity and latency increases as compared to conventional self-learning. As an added bonus, the annotations produced by LOCCO can be trivially repurposed to train a neural text generation model. We demonstrate the utility of LOCCO on the well-known WebNLG benchmark where we obtain an improvement of 2 points against a self-learning parser under equivalent conditions, an improvement of 1.3 points against the previous state-of-the-art parser, and competitive text generation performance in terms of BLEU score.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41036307",
                    "name": "M. Crouse"
                },
                {
                    "authorId": "3394760",
                    "name": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                },
                {
                    "authorId": "2138053379",
                    "name": "Tahira Naseem"
                },
                {
                    "authorId": "34597365",
                    "name": "Subhajit Chaudhury"
                },
                {
                    "authorId": "2123450380",
                    "name": "P. Kapanipathi"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "1703070",
                    "name": "Alexander G. Gray"
                }
            ]
        }
    ]
}