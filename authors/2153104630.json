{
    "authorId": "2153104630",
    "papers": [
        {
            "paperId": "08e0254ede4462bbb506b902c887e7b7f144aff7",
            "title": "Adaptive Surface Normal Constraint for Geometric Estimation From Monocular Images",
            "abstract": "We introduce a novel approach to learn geometries such as depth and surface normal from images while incorporating geometric context. The difficulty of reliably capturing geometric context in existing methods impedes their ability to accurately enforce the consistency between the different geometric properties, thereby leading to a bottleneck of geometric estimation quality. We therefore propose the Adaptive Surface Normal (ASN) constraint, a simple yet efficient method. Our approach extracts geometric context that encodes the geometric variations present in the input image and correlates depth estimation with geometric constraints. By dynamically determining reliable local geometry from randomly sampled candidates, we establish a surface normal constraint, where the validity of these candidates is evaluated using the geometric context. Furthermore, our normal estimation leverages the geometric context to prioritize regions that exhibit significant geometric variations, which makes the predicted normals accurately capture intricate and detailed geometric information. Through the integration of geometric context, our method unifies depth and surface normal estimations within a cohesive framework, which enables the generation of high-quality 3D geometry from images. We validate the superiority of our approach over state-of-the-art methods through extensive evaluations and comparisons on diverse indoor and outdoor datasets, showcasing its efficiency and robustness.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1606258913",
                    "name": "Xiaoxiao Long"
                },
                {
                    "authorId": "2239159684",
                    "name": "Yuhang Zheng"
                },
                {
                    "authorId": "2203905332",
                    "name": "Yupeng Zheng"
                },
                {
                    "authorId": "2143694337",
                    "name": "Beiwen Tian"
                },
                {
                    "authorId": "144428897",
                    "name": "Cheng Lin"
                },
                {
                    "authorId": "2268757583",
                    "name": "Lingjie Liu"
                },
                {
                    "authorId": "2176137981",
                    "name": "Hao Zhao"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2268339722",
                    "name": "Wenping Wang"
                }
            ]
        },
        {
            "paperId": "0c6e38011eaf77c1a2f9a4763eb873dc3ee26bac",
            "title": "Blending Distributed NeRFs with Tri-stage Robust Pose Optimization",
            "abstract": "Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at https://github.com/boilcy/Distributed-NeRF.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299156070",
                    "name": "Baijun Ye"
                },
                {
                    "authorId": "2266007261",
                    "name": "Caiyun Liu"
                },
                {
                    "authorId": "2300815320",
                    "name": "Xiaoyu Ye"
                },
                {
                    "authorId": "2299331993",
                    "name": "Yuantao Chen"
                },
                {
                    "authorId": "2300131467",
                    "name": "Yuhai Wang"
                },
                {
                    "authorId": "2266356767",
                    "name": "Zike Yan"
                },
                {
                    "authorId": "2265947098",
                    "name": "Yongliang Shi"
                },
                {
                    "authorId": "2299023531",
                    "name": "Hao Zhao"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                }
            ]
        },
        {
            "paperId": "38303d98e808bbd0e4784786c6d6e9b5ab88867c",
            "title": "Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics",
            "abstract": "In the past several years, road anomaly segmentation is actively explored in the academia and drawing growing attention in the industry. The rationale behind is straightforward: if the autonomous car can brake before hitting an anomalous object, safety is promoted. However, this rationale naturally calls for a temporally informed setting while existing methods and benchmarks are designed in an unrealistic frame-wise manner. To bridge this gap, we contribute the first video anomaly segmentation dataset for autonomous driving. Since placing various anomalous objects on busy roads and annotating them in every frame are dangerous and expensive, we resort to synthetic data. To improve the relevance of this synthetic dataset to real-world applications, we train a generative adversarial network conditioned on rendering G-buffers for photorealism enhancement. Our dataset consists of 120,000 high-resolution frames at a 60 FPS framerate, as recorded in 7 different towns. As an initial benchmarking, we provide baselines using latest supervised and unsupervised road anomaly segmentation methods. Apart from conventional ones, we focus on two new metrics: temporal consistency and latencyaware streaming accuracy. We believe the latter is valuable as it measures whether an anomaly segmentation algorithm can truly prevent a car from crashing in a temporally informed setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143694337",
                    "name": "Beiwen Tian"
                },
                {
                    "authorId": "2221148510",
                    "name": "Huan-ang Gao"
                },
                {
                    "authorId": "2266112512",
                    "name": "Leiyao Cui"
                },
                {
                    "authorId": "2203905332",
                    "name": "Yupeng Zheng"
                },
                {
                    "authorId": "2278989498",
                    "name": "Lan Luo"
                },
                {
                    "authorId": "2279044450",
                    "name": "Baofeng Wang"
                },
                {
                    "authorId": "2278838858",
                    "name": "Rong Zhi"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2176137981",
                    "name": "Hao Zhao"
                }
            ]
        },
        {
            "paperId": "5735ac778867b3e99816799e982e6d6bcc1aaa86",
            "title": "Camera Relocalization in Shadow-free Neural Radiance Fields",
            "abstract": "Camera relocalization is a crucial problem in computer vision and robotics. Recent advancements in neural radiance fields (NeRFs) have shown promise in synthesizing photo-realistic images. Several works have utilized NeRFs for refining camera poses, but they do not account for lighting changes that can affect scene appearance and shadow regions, causing a degraded pose optimization process. In this paper, we propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization. We implement our scene representation upon a hash-encoded NeRF which significantly boosts up the pose optimization process. To account for the noisy image gradient computing problem in grid-based NeRFs, we further propose a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient averaging technique to smoothen the process. Experimental results on several datasets with varying lighting conditions demonstrate that our method achieves state-of-the-art results in camera relocalization under varying lighting conditions. Code and data will be made publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2302791907",
                    "name": "Shiyao Xu"
                },
                {
                    "authorId": "2266007261",
                    "name": "Caiyun Liu"
                },
                {
                    "authorId": "2299331993",
                    "name": "Yuantao Chen"
                },
                {
                    "authorId": "2292183764",
                    "name": "Zhenxin Zhu"
                },
                {
                    "authorId": "2266356767",
                    "name": "Zike Yan"
                },
                {
                    "authorId": "2265947098",
                    "name": "Yongliang Shi"
                },
                {
                    "authorId": "2299023531",
                    "name": "Hao Zhao"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                }
            ]
        },
        {
            "paperId": "5a97ead7d75758e764b57052582cd47a14f97cb9",
            "title": "PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments",
            "abstract": "Robotic manipulation with two-finger grippers is challenged by objects lacking distinct graspable features. Traditional pre-grasping methods, which typically involve repositioning objects or utilizing external aids like table edges, are limited in their adaptability across different object categories and environments. To overcome these limitations, we introduce PreAfford, a novel pre-grasping planning framework incorporating a point-level affordance representation and a relay training approach. Our method significantly improves adaptability, allowing effective manipulation across a wide range of environments and object types. When evaluated on the ShapeNet-v2 dataset, PreAfford not only enhances grasping success rates by 69% but also demonstrates its practicality through successful real-world experiments. These improvements highlight PreAfford's potential to redefine standards for robotic handling of complex manipulation tasks in diverse settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2294856557",
                    "name": "Kairui Ding"
                },
                {
                    "authorId": "2295095893",
                    "name": "Boyuan Chen"
                },
                {
                    "authorId": "9381012",
                    "name": "Ruihai Wu"
                },
                {
                    "authorId": "2261448933",
                    "name": "Yuyang Li"
                },
                {
                    "authorId": "2294931371",
                    "name": "Zongzheng Zhang"
                },
                {
                    "authorId": "2221148510",
                    "name": "Huan-ang Gao"
                },
                {
                    "authorId": "2295268334",
                    "name": "Siqi Li"
                },
                {
                    "authorId": "2290017516",
                    "name": "Yixin Zhu"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2292234604",
                    "name": "Hao Dong"
                },
                {
                    "authorId": "2293764606",
                    "name": "Hao Zhao"
                }
            ]
        },
        {
            "paperId": "9504af528ff6ca9ae7d39adc640f7974f7b2fe0f",
            "title": "Active Neural Mapping at Scale",
            "abstract": "We introduce a NeRF-based active mapping system that enables efficient and robust exploration of large-scale indoor environments. The key to our approach is the extraction of a generalized Voronoi graph (GVG) from the continually updated neural map, leading to the synergistic integration of scene geometry, appearance, topology, and uncertainty. Anchoring uncertain areas induced by the neural map to the vertices of GVG allows the exploration to undergo adaptive granularity along a safe path that traverses unknown areas efficiently. Harnessing a modern hybrid NeRF representation, the proposed system achieves competitive results in terms of reconstruction accuracy, coverage completeness, and exploration efficiency even when scaling up to large indoor environments. Extensive results at different scales validate the efficacy of the proposed system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281012872",
                    "name": "Zi-Feng Kuang"
                },
                {
                    "authorId": "2266356767",
                    "name": "Zike Yan"
                },
                {
                    "authorId": "2323520685",
                    "name": "Hao Zhao"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2311876288",
                    "name": "Hongbin Zha"
                }
            ]
        },
        {
            "paperId": "9ee524fb385e91b9f91289cb0ea50b3dd6fb4d15",
            "title": "Car-Studio: Learning Car Radiance Fields From Single-View and Unlimited In-the-Wild Images",
            "abstract": "Compositional neural scene graph studies have shown that radiance fields can be an efficient tool in an editable autonomous driving simulator. However, previous studies learned within a sequence of autonomous driving datasets, resulting in unsatisfactory blurring when rotating the car in the simulator. In this letter, we propose a pipeline for learning unconstrained images and building a dataset from processed images. To meet the requirements of the simulator, which demands that the vehicle maintain clarity when the perspective changes and that the contour remains sharp from the background to avoid artifacts when editing, we design a radiation field of the vehicle, a crucial part of the urban scene foreground. Through experiments, we demonstrate that our model achieves competitive performance compared to baselines. Using the datasets built from in-the-wild images, our method gradually presents a controllable appearance editing function.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268440507",
                    "name": "Tianyu Liu"
                },
                {
                    "authorId": "2176137981",
                    "name": "Hao Zhao"
                },
                {
                    "authorId": "2278216058",
                    "name": "Yang Yu"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2221269157",
                    "name": "Mingdao Liu"
                }
            ]
        },
        {
            "paperId": "ca69d41beab396f96462f02ae2421c80fb176887",
            "title": "Block-Map-Based Localization in Large-Scale Environment",
            "abstract": "Accurate localization is an essential technology for the flexible navigation of robots in large-scale environments. Both SLAM-based and map-based localization will increase the computing load due to the increase in map size, which will affect downstream tasks such as robot navigation and services. To this end, we propose a localization system based on Block Maps (BMs) to reduce the computational load caused by maintaining large-scale maps. Firstly, we introduce a method for generating block maps and the corresponding switching strategies, ensuring that the robot can estimate the state in large-scale environments by loading local map information. Secondly, global localization according to Branch-and-Bound Search (BBS) in the 3D map is introduced to provide the initial pose. Finally, a graph-based optimization method is adopted with a dynamic sliding window that determines what factors are being marginalized whether a robot is exposed to a BM or switching to another one, which maintains the accuracy and efficiency of pose tracking. Comparison experiments are performed on publicly available large-scale datasets. Results show that the proposed method can track the robot pose even though the map scale reaches more than 6 kilometers, while efficient and accurate localization is still guaranteed on NCLT [6] and M2DGR [35]. Codes and data will be publicly available on https://github.com/YixFeng/blocklocalization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2315792040",
                    "name": "Yixiao Feng"
                },
                {
                    "authorId": "2291994075",
                    "name": "Zhou Jiang"
                },
                {
                    "authorId": "2265947098",
                    "name": "Yongliang Shi"
                },
                {
                    "authorId": "2315506311",
                    "name": "Yunlong Feng"
                },
                {
                    "authorId": "2298904874",
                    "name": "Xiangyu Chen"
                },
                {
                    "authorId": "2299023531",
                    "name": "Hao Zhao"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                }
            ]
        },
        {
            "paperId": "0ad61806b055af61b59595d3ce2ab185da279a91",
            "title": "Enable Natural Tactile Interaction for Robot Dog based on Large-format Distributed Flexible Pressure Sensors",
            "abstract": "Touch is an important channel for human-robot interaction, while it is challenging for robots to recognize human touch accurately and make appropriate responses. In this paper, we design and implement a set of large-format distributed flexible pressure sensors on a robot dog to enable natural human-robot tactile interaction. Through a heuristic study, we sorted out 81 tactile gestures commonly used when humans interact with real dogs and 44 dog reactions. A gesture classification algorithm based on ResNet is proposed to recognize these 81 human gestures, and the classification accuracy reaches 98.7%. In addition, an action prediction algorithm based on Transformer is proposed to predict dog actions from human gestures, reaching a 1-gram BLEU score of 0.87. Finally, we compare the tactile interaction with the voice interaction during a freedom human-robot-dog interactive playing study. The results show that tactile interaction plays a more significant role in alleviating user anxiety, stimulating user excitement and improving the acceptability of robot dogs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052114898",
                    "name": "Lishuang Zhan"
                },
                {
                    "authorId": "2211587171",
                    "name": "Yancheng Cao"
                },
                {
                    "authorId": "2211591151",
                    "name": "Qitai Chen"
                },
                {
                    "authorId": "2176147031",
                    "name": "Haole Guo"
                },
                {
                    "authorId": "1996101908",
                    "name": "Jiasi Gao"
                },
                {
                    "authorId": "26889828",
                    "name": "Yiyue Luo"
                },
                {
                    "authorId": "2157301149",
                    "name": "Shihui Guo"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                }
            ]
        },
        {
            "paperId": "0de370a501f465a7b61a3cb4e9794e31cf0c528a",
            "title": "H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps",
            "abstract": "Solving real-world complex tasks using reinforcement learning (RL) without high-fidelity simulation environments or large amounts of offline data can be quite challenging. Online RL agents trained in imperfect simulation environments can suffer from severe sim-to-real issues. Offline RL approaches although bypass the need for simulators, often pose demanding requirements on the size and quality of the offline datasets. The recently emerged hybrid offline-and-online RL provides an attractive framework that enables joint use of limited offline data and imperfect simulator for transferable policy learning. In this paper, we develop a new algorithm, called H2O+, which offers great flexibility to bridge various choices of offline and online learning methods, while also accounting for dynamics gaps between the real and simulation environment. Through extensive simulation and real-world robotics experiments, we demonstrate superior performance and flexibility over advanced cross-domain online and offline RL algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122919426",
                    "name": "Haoyi Niu"
                },
                {
                    "authorId": "2072498620",
                    "name": "Tianying Ji"
                },
                {
                    "authorId": "2245044185",
                    "name": "Bingqi Liu"
                },
                {
                    "authorId": "2176137981",
                    "name": "Hao Zhao"
                },
                {
                    "authorId": "2144104090",
                    "name": "Xiangyu Zhu"
                },
                {
                    "authorId": "2244849070",
                    "name": "Jianying Zheng"
                },
                {
                    "authorId": "2244766594",
                    "name": "Pengfei Huang"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2219045220",
                    "name": "Jianming Hu"
                },
                {
                    "authorId": "2242851906",
                    "name": "Xianyuan Zhan"
                }
            ]
        }
    ]
}