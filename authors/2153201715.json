{
    "authorId": "2153201715",
    "papers": [
        {
            "paperId": "a1975784784db088ec5125b488e9d5374fdef57a",
            "title": "ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos",
            "abstract": "Building benchmarks to systemically analyze different capabilities of video question answering (VideoQA) models is challenging yet crucial. Existing benchmarks often use non-compositional simple questions and suffer from language biases, making it difficult to diagnose model weaknesses incisively. A recent benchmark AGQA [8] poses a promising paradigm to generate QA pairs automatically from pre-annotated scene graphs, enabling it to measure diverse reasoning abilities with granular control. However, its questions have limitations in reasoning about the fine-grained semantics in videos as such information is absent in its scene graphs. To this end, we present ANetQA, a large-scale benchmark that supports fine-grained compositional reasoning over the challenging untrimmed videos from ActivityNet [4]. Similar to AGQA, the QA pairs in ANetQA are automatically generated from annotated video scene graphs. The fine-grained properties of ANetQA are reflected in the following: (i) untrimmed videos with fine-grained semantics; (ii) spatio-temporal scene graphs with fine-grained taxonomies; and (iii) diverse questions generated from fine-grained templates. ANetQA attains 1.4 billion unbalanced and 13.4 million balanced QA pairs, which is an order of magnitude larger than AGQA with a similar number of videos. Comprehensive experiments are performed for state-of-the-art methods. The best model achieves 44.5% accuracy while human performance tops out at 84.5%, leaving sufficient room for improvement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2216492785",
                    "name": "Lixiang Zheng"
                },
                {
                    "authorId": "47122432",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2110922423",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                }
            ]
        },
        {
            "paperId": "ad2ad450f1ee6a0df46bc6fe6916a797c90b68f1",
            "title": "Multi-Task Paired Masking With Alignment Modeling for Medical Vision-Language Pre-Training",
            "abstract": "In recent years, the growing demand for medical imaging diagnosis has placed a significant burden on radiologists. As a solution, Medical Vision-Language Pre-training (Med-VLP) methods have been proposed to learn universal representations from medical images and reports, benefiting downstream tasks without requiring fine-grained annotations. However, existing methods have overlooked the importance of cross-modal alignment in joint image-text reconstruction, resulting in insufficient cross-modal interaction. To address this limitation, we propose a unified Med-VLP framework based on Multi-task Paired Masking with Alignment (MPMA) to integrate the cross-modal alignment task into the joint image-text reconstruction framework to achieve more comprehensive cross-modal interaction, while a Global and Local Alignment (GLA) module is designed to assist self-supervised paradigm in obtaining semantic representations with rich domain knowledge. Furthermore, we introduce a Memory-Augmented Cross-Modal Fusion (MA-CMF) module to fully integrate visual information to assist report reconstruction and fuse the multi-modal representations adequately. Experimental results demonstrate that the proposed unified approach outperforms previous methods in all downstream tasks, including uni-modal, cross-modal, and multi-modal tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152645158",
                    "name": "Kecheng Zhang"
                },
                {
                    "authorId": "2524165",
                    "name": "Han Jiang"
                },
                {
                    "authorId": "47539632",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2111526678",
                    "name": "Qing-An Huang"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2114925498",
                    "name": "Weidong Han"
                }
            ]
        },
        {
            "paperId": "e96683b7593cfea1448bee222637beafa54b980d",
            "title": "GLOW: Global Layout Aware Attacks on Object Detection",
            "abstract": "Adversarial attacks aim to perturb images such that a predictor outputs incorrect results. Due to the limited research in structured attacks, imposing consistency checks on natural multi-object scenes is a practical defense against conventional adversarial attacks. More desired attacks should be able to fool defenses with such consistency checks. Therefore, we present the first approach GLOW that copes with various attack requests by generating global layout-aware adversar-ial attacks, in which both categorical and geometric layout constraints are explicitly established. Specifically, we focus on object detection tasks and given a victim image, GLOW first localizes victim objects according to target labels. And then it generates multiple attack plans, together with their context-consistency scores. GLOW, on the one hand, is ca-pable of handling various types of requests, including single or multiple victim objects, with or without specified victim objects. On the other hand, it produces a consistency score for each attack plan, reflecting the overall contextual consistency that both semantic category and global scene layout are considered. We conduct our experiments on MS COCO and Pascal. Extensive experimental results demonstrate that we can achieve about 30% average relative improvement compared to state-of-the-art methods in conventional single object attack request; Moreover, such superiority is also valid across more generic attack requests, under both white-box and zero-query black-box settings. Finally, we conduct comprehensive human analysis, which not only validates our claim further but also provides strong evidence that our evaluation metrics reflect human reviews well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261752562",
                    "name": "Jun Bao"
                },
                {
                    "authorId": "3248862",
                    "name": "Buyu Liu"
                },
                {
                    "authorId": "2265527684",
                    "name": "Kui Ren"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                }
            ]
        },
        {
            "paperId": "03eb9d86eb1e8d17865d3162e6c31593d90d6e8b",
            "title": "Bilaterally Slimmable Transformer for Elastic and Efficient Visual Question Answering",
            "abstract": "Recent Transformer architectures (Vaswani et al., 2017) have brought remarkable improvements to visual question answering (VQA). Nevertheless, Transformer-based VQA models are usually deep and wide to guarantee good performance, so they can only run on powerful GPU servers and cannot run on capacity-restricted platforms such as mobile phones. Therefore, it is desirable to learn an elastic VQA model that supports adaptive pruning at runtime to meet the efficiency constraints of different platforms. To this end, we present the bilaterally slimmable Transformer (BST), a general framework that can be seamlessly integrated into arbitrary Transformer-based VQA models to train a single model once and obtain various slimmed submodels of different widths and depths. To verify the effectiveness and generality of this method, we integrate the proposed BST framework with three typical Transformer-based VQA approaches, namely MCAN (Yu et al., 2019), UNITER (Chen et al., 2020), and CLIP-ViL (Shen et al., 2021), and conduct extensive experiments on two commonly-used benchmark datasets. In particular, one slimmed MCAN$_\\mathsf {BST}$ submodel achieves comparable accuracy on VQA-v2, while being 0.38\u00d7 smaller in model size and having 0.27\u00d7 fewer FLOPs than the reference MCAN model. The smallest MCAN$_\\mathsf {BST}$ submodel only has 9 M parameters and 0.16 G FLOPs during inference, making it possible to deploy it on a mobile device with less than 60 ms latency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2159822630",
                    "name": "Zitian Jin"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2153557200",
                    "name": "Mingliang Xu"
                },
                {
                    "authorId": "2108981037",
                    "name": "Hongbo Wang"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                }
            ]
        },
        {
            "paperId": "465336e6dcc935ace5f8079c431ba40b464d6681",
            "title": "Delegate-based Utility Preserving Synthesis for Pedestrian Image Anonymization",
            "abstract": "The rapidly growing application of pedestrian images has aroused wide concern on visual privacy protection because personal information is under the risk of privacy disclosure. Anonymization is regarded as an effective solution by identity obfuscation. Most recent methods focus on face, but it is not enough when the presence of human body carries lots of identifiable information. This paper presents a new delegate-based utility preserving synthesis (DUPS) approach for pedestrian image anonymization. This is challenging because one may expect that the anonymized image can still be useful in various computer vision tasks. We model DUPS as an adaptive translation process from source to target. To provide a comprehensive identity protection, we first perform anonymous delegate sampling based on image-level differential privacy. To synthesize anonymous images, we then introduce an adaptive translation network and optimize it with a multi-task loss function. Our approach is theoretically sound and can generate diverse results by preserving data utility. The experiments on multiple datasets show that DUPS can not only achieve superior anonymization performance against deep pedestrian recognizers, but also can obtain a better tradeoff between privacy protection and utility preservation compared with state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38203465",
                    "name": "Zhenzhong Kuang"
                },
                {
                    "authorId": "2159268124",
                    "name": "Longbin Teng"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "2285442",
                    "name": "Mingliang Xu"
                }
            ]
        },
        {
            "paperId": "61257bd6109ebf218135cf69c7f53904782afd40",
            "title": "Towards Efficient and Elastic Visual Question Answering with Doubly Slimmable Transformer",
            "abstract": "\u2014Transformer-based approaches have shown great success in visual question answering (VQA). However, they usually require deep and wide models to guarantee good performance, making it dif\ufb01cult to deploy on capacity-restricted platforms. It is a challenging yet valuable task to design an elastic VQA model that supports adaptive pruning at runtime to meet the ef\ufb01ciency constraints of diverse platforms. In this paper, we present the Doubly Slimmable Transformer (DST), a general framework that can be seamlessly integrated into arbitrary Transformer-based VQA models to train one single model once and obtain various slimmed submodels of different widths and depths. Taking two typical Transformer-based VQA approaches, i.e. , MCAN [1] and UNITER [2], as the reference models, the obtained slimmable MCAN DST and UNITER DST models outperform the state-of-the-art methods trained independently on two benchmark datasets. In particular, one slimmed MCAN DST submodel achieves a comparable accuracy on VQA-v2, while being 0.38 \u00d7 smaller in model size and having 0.27 \u00d7 fewer FLOPs than the reference MCAN model. The smallest MCAN DST submodel has 9M parameters and 0.16G FLOPs in the inference stage, making it possible to be deployed on edge devices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2159822630",
                    "name": "Zitian Jin"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2285442",
                    "name": "Mingliang Xu"
                },
                {
                    "authorId": "2152732685",
                    "name": "Jianping Fan"
                }
            ]
        },
        {
            "paperId": "9f97ea2f5b7634fff5157b5def6fa47a88d118bf",
            "title": "PAINT: Photo-realistic Fashion Design Synthesis",
            "abstract": "In this article, we investigate a new problem of generating a variety of multi-view fashion designs conditioned on a human pose and texture examples of arbitrary sizes, which can replace the repetitive and low-level design work for fashion designers. To solve this challenging multi-modal image translation problem, we propose a novel Photo-reAlistic fashIon desigN synThesis (PAINT) framework, which decomposes the framework into three manageable stages. In the first stage, we employ a Layout Generative Network (LGN) to transform an input human pose into a series of person semantic layouts. In the second stage, we propose a Texture Synthesis Network (TSN) to synthesize textures on all transformed semantic layouts. Specifically, we design a novel attentive texture transfer mechanism for precisely expanding texture patches to the irregular clothing regions of the target fashion designs. In the third stage, we leverage an Appearance Flow Network (AFN) to generate the fashion design images of other viewpoints from a single-view observation by learning 2D multi-scale appearance flow fields. Experimental results demonstrate that our method is capable of generating diverse photo-realistic multi-view fashion design images with fine-grained appearance details conditioned on the provided multiple inputs. The source code and trained models are available at https://github.com/gxl-groups/PAINT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7787949",
                    "name": "Xiaoling Gu"
                },
                {
                    "authorId": "2174130716",
                    "name": "Jie Huang"
                },
                {
                    "authorId": "3026404",
                    "name": "Yongkang Wong"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2152732801",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "2072676858",
                    "name": "Pai Peng"
                },
                {
                    "authorId": "145977143",
                    "name": "Mohan S. Kankanhalli"
                }
            ]
        },
        {
            "paperId": "6f762aac8f5f2b7bc5f023a4604b1d4cc8b7928e",
            "title": "Effective De-identification Generative Adversarial Network for Face Anonymization",
            "abstract": "The growing application of face images and modern AI technology has raised another important concern in privacy protection. In many real scenarios like scientific research, social sharing and commercial application, lots of images are released without privacy processing to protect people's identity. In this paper, we develop a novel effective de-identification generative adversarial network (DeIdGAN) for face anonymization by seamlessly replacing a given face image with a different synthesized yet realistic one. Our approach consists of two steps. First, we anonymize the input face to obfuscate its original identity. Then, we use our designed de-identification generator to synthesize an anonymized face. During the training process, we leverage a pair of identity-adversarial discriminators to explicitly constrain identity protection by pushing the synthesized face away from the predefined sensitive faces to resist re-identification and identity invasion. Finally, we validate the effectiveness of our approach on public datasets. Compared with existing methods, our approach can not only achieve better identity protection rates but also preserve superior image quality and data reusability, which suggests the state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38203465",
                    "name": "Zhenzhong Kuang"
                },
                {
                    "authorId": "2005134573",
                    "name": "Huigui Liu"
                },
                {
                    "authorId": "2153201715",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "84107791",
                    "name": "Aikui Tian"
                },
                {
                    "authorId": "2152510653",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2152732685",
                    "name": "Jianping Fan"
                },
                {
                    "authorId": "1727647",
                    "name": "N. Babaguchi"
                }
            ]
        }
    ]
}