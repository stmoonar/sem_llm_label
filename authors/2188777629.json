{
    "authorId": "2188777629",
    "papers": [
        {
            "paperId": "bf6f865337faf618bc2be057d6c8e97972e9b520",
            "title": "Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations",
            "abstract": "Tripartite graph-based recommender systems markedly diverge from traditional models by recommending unique combinations such as user groups and item bundles. Despite their effectiveness, these systems exacerbate the longstanding cold-start problem in traditional recommender systems, because any number of user groups or item bundles can be formed among users or items. To address this issue, we introduce a Consistency and Discrepancy-based graph contrastive learning method for tripartite graph-based Recommendation. This approach leverages two novel meta-path-based metrics consistency and discrepancy to capture nuanced, implicit associations between the recommended objects and the recommendees. These metrics, indicative of high-order similarities, can be efficiently calculated with infinite graph convolutional networks layers under a multi-objective optimization framework, using the limit theory of GCN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188777629",
                    "name": "Linxin Guo"
                },
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2299334688",
                    "name": "Min Gao"
                },
                {
                    "authorId": "2158332892",
                    "name": "Yinghui Tao"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2310713515",
                    "name": "Chen Chen"
                }
            ]
        },
        {
            "paperId": "24eedcec42599fedea2b67a5024d028301bd08f9",
            "title": "What Really Matters for Graph Contrastive Learning-Based Recommendations? A Unified Learning Strategy",
            "abstract": "Graph contrastive learning (GCL) has become increasingly popular in recommendation due to its remarkable ability to reduce reliance on labels. Typically, GCL employs data augmentation methods, e.g., structure perturbation and representation perturbation, and CL loss to enhance performance. Recent studies have shown that structural perturbation plays a minor role in GCL, but there is still a lack of exploration on the representation perturbation. Therefore, we compare the two data perturbations in detail and reveal that both of them have a limited impact on performance. Simply combining recommendation loss and CL loss can produce comparable improvements. Besides, we identify a shared principle between the designs of recommendation loss and CL loss: both aim to optimize representation by increasing similarity between a target node and its positive samples while decreasing similarity with negative samples. Based on these findings, we propose Compact Graph Contrastive Learning (CGCL), a streamlined strategy that eliminates the data augmentation and deeply unifies recommendation loss and CL loss by elegantly incorporating their respective contributions. Leveraging the benefits of the unified strategy, we discover that our model can learn a concentrated mode length distribution of representation, which can enhance the ability to debias and thus improve the performance of the recommendation. This is a novel perspective on representation learning, and we also validate its rationality through rigorous experiments. Our comprehensive study on multiple benchmark datasets demonstrates that CGCL outperforms existing GCL methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157475090",
                    "name": "Hongwei Zhou"
                },
                {
                    "authorId": "2155431892",
                    "name": "Min Gao"
                },
                {
                    "authorId": "2040449292",
                    "name": "Zongwei Wang"
                },
                {
                    "authorId": "2188777629",
                    "name": "Linxin Guo"
                },
                {
                    "authorId": "2158332892",
                    "name": "Yinghui Tao"
                },
                {
                    "authorId": "2275746556",
                    "name": "Wentao Li"
                }
            ]
        },
        {
            "paperId": "696f093d8ff7696e4404f90b17c21d9b947479ca",
            "title": "Efficient Bi-Level Optimization for Recommendation Denoising",
            "abstract": "The acquisition of explicit user feedback (e.g., ratings) in real-world recommender systems is often hindered by the need for active user involvement. To mitigate this issue, implicit feedback (e.g., clicks) generated during user browsing is exploited as a viable substitute. However, implicit feedback possesses a high degree of noise, which significantly undermines recommendation quality. While many methods have been proposed to address this issue by assigning varying weights to implicit feedback, two shortcomings persist: (1) the weight calculation in these methods is iteration-independent, without considering the influence of weights in previous iterations, and (2) the weight calculation often relies on prior knowledge, which may not always be readily available or universally applicable. To overcome these two limitations, we model recommendation denoising as a bi-level optimization problem. The inner optimization aims to derive an effective model for the recommendation, as well as guiding the weight determination, thereby eliminating the need for prior knowledge. The outer optimization leverages gradients of the inner optimization and adjusts the weights in a manner considering the impact of previous weights. To efficiently solve this bi-level optimization problem, we employ a weight generator to avoid the storage of weights and a one-step gradient-matching-based loss to significantly reduce computational time. The experimental results on three benchmark datasets demonstrate that our proposed approach outperforms both state-of-the-art general and denoising recommendation models. The code is available at https://github.com/CoderWZW/BOD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2040449292",
                    "name": "Zongwei Wang"
                },
                {
                    "authorId": "2155431892",
                    "name": "Min Gao"
                },
                {
                    "authorId": "2108743776",
                    "name": "Wentao Li"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2188777629",
                    "name": "Linxin Guo"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                }
            ]
        }
    ]
}