{
    "authorId": "1856878",
    "papers": [
        {
            "paperId": "8ef607b7b96e7834acdf29b827cc504700405d24",
            "title": "Enhancing Machine Translation Experiences with Multilingual Knowledge Graphs",
            "abstract": "Translating entity names, especially when a literal translation is not correct, poses a significant challenge. Although Machine Translation (MT) systems have achieved impressive results, they still struggle to translate cultural nuances and language-specific context. In this work, we show that the integration of multilingual knowledge graphs into MT systems can address this problem and bring two significant benefits: i) improving the translation of utterances that contain entities by leveraging their human-curated aliases from a multilingual knowledge graph, and, ii) increasing the interpretability of the translation process by providing the user with information from the knowledge graph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268401404",
                    "name": "Simone Conia"
                },
                {
                    "authorId": "2268434271",
                    "name": "Daniel Lee"
                },
                {
                    "authorId": "2268432243",
                    "name": "Min Li"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "2268606252",
                    "name": "Yunyao Li"
                }
            ]
        },
        {
            "paperId": "bd44f96232df6f59a0720973b8498d039d66287f",
            "title": "Entity Disambiguation via Fusion Entity Decoding",
            "abstract": "Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked.We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity.Our experiments, conducted on various entity disambiguation benchmarks, demonstrate the strong and robust performance of this model, particularly +1.5% in the ZELDA benchmark compared with GENRE. Furthermore, we integrate this approach into the retrieval/reader framework and observe +1.5% improvements in end-to-end entity linking in the GERBIL benchmark compared with EntQA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280932302",
                    "name": "Junxiong Wang"
                },
                {
                    "authorId": "2243336632",
                    "name": "Ali Mousavi"
                },
                {
                    "authorId": "2294568479",
                    "name": "Omar Attia"
                },
                {
                    "authorId": "2294572235",
                    "name": "Saloni Potdar"
                },
                {
                    "authorId": "2261743768",
                    "name": "Alexander M. Rush"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "2268606252",
                    "name": "Yunyao Li"
                }
            ]
        },
        {
            "paperId": "4f662c42b2bd52422b70d8f44175f33cec4f8969",
            "title": "High-Throughput Vector Similarity Search in Knowledge Graphs",
            "abstract": "There is an increasing adoption of machine learning for encoding data into vectors to serve online recommendation and search use cases. As a result, recent data management systems propose augmenting query processing with online vector similarity search. In this work, we explore vector similarity search in the context of Knowledge Graphs (KGs). Motivated by the tasks of finding related KG queries and entities for past KG query workloads, we focus on hybrid vector similarity search (hybrid queries for short) where part of the query corresponds to vector similarity search and part of the query corresponds to predicates over relational attributes associated with the underlying data vectors. For example, given past KG queries for a song entity, we want to construct new queries for new song entities whose vector representations are close to the vector representation of the entity in the past KG query. But entities in a KG also have non-vector attributes such as a song associated with an artist, a genre, and a release date. Therefore, suggested entities must also satisfy query predicates over non-vector attributes beyond a vector-based similarity predicate. While these tasks are central to KGs, our contributions are generally applicable to hybrid queries. In contrast to prior works that optimize online queries, we focus on enabling efficient batch processing of past hybrid query workloads. We present our system, HQI, for high-throughput batch processing of hybrid queries. We introduce a workload-aware vector data partitioning scheme to tailor the vector index layout to the given workload and describe a multi-query optimization technique to reduce the overhead of vector similarity computations. We evaluate our methods on industrial workloads and demonstrate that HQI yields a 31\u00d7 improvement in throughput for finding related KG queries compared to existing hybrid query processing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047146404",
                    "name": "J. Mohoney"
                },
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "2087061163",
                    "name": "S. R. Chowdhury"
                },
                {
                    "authorId": "143661472",
                    "name": "A. Mousavi"
                },
                {
                    "authorId": "1743316",
                    "name": "Ihab F. Ilyas"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "32546616",
                    "name": "Jeffrey Pound"
                },
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                }
            ]
        },
        {
            "paperId": "8d24e6680a19c2f4c113e45145ec067130069805",
            "title": "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs",
            "abstract": "Recent work in Natural Language Processing and Computer Vision has been using textual information -- e.g., entity names and descriptions -- available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and non-English languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268401404",
                    "name": "Simone Conia"
                },
                {
                    "authorId": "2268432243",
                    "name": "Min Li"
                },
                {
                    "authorId": "2268434271",
                    "name": "Daniel Lee"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "2243335549",
                    "name": "Ihab Ilyas"
                },
                {
                    "authorId": "2268606252",
                    "name": "Yunyao Li"
                }
            ]
        },
        {
            "paperId": "b6f229681cf0a3ba38884d281d89b2d498a853a8",
            "title": "Growing and Serving Large Open-domain Knowledge Graphs",
            "abstract": "Applications of large open-domain knowledge graphs (KGs) to real-world problems pose many unique challenges. In this paper, we present extensions to Saga our platform for continuous construction and serving of knowledge at scale. In particular, we describe a pipeline for training knowledge graph embeddings that powers key capabilities such as fact ranking, fact verification, a related entities service, and support for entity linking. We then describe how our platform, including graph embeddings, can be leveraged to create a Semantic Annotation service that links unstructured Web documents to entities in our KG. Semantic annotation of the Web effectively expands our knowledge graph with edges to open-domain Web content which can be used in various search and ranking problems. Finally, we leverage annotated Web documents to drive Open-domain Knowledge Extraction. This targeted extraction framework identifies important coverage issues in the KG, then finds relevant data sources for target entities on the Web and extracts missing information to enrich the KG. Finally, we describe adaptations to our knowledge platform needed to construct and serve private personal knowledge on-device. This includes private incremental KG construction, cross- device knowledge sync, and global knowledge enrichment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1743316",
                    "name": "Ihab F. Ilyas"
                },
                {
                    "authorId": "2217340760",
                    "name": "JP Lacerda"
                },
                {
                    "authorId": "1718694",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "143661472",
                    "name": "A. Mousavi"
                },
                {
                    "authorId": "32546616",
                    "name": "Jeffrey Pound"
                },
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                },
                {
                    "authorId": "3308088",
                    "name": "C. Sumanth"
                }
            ]
        },
        {
            "paperId": "014d5ab6a5f4af760f854a852294a8d362b6266b",
            "title": "TreeLine: An Update-In-Place Key-Value Store for Modern Storage",
            "abstract": "Many modern key-value stores, such as RocksDB, rely on log-structured merge trees (LSMs). Originally designed for spinning disks, LSMs optimize for write performance by only making sequential writes. But this optimization comes at the cost of reads: LSMs must rely on expensive compaction jobs and Bloom filters---all to maintain reasonable read performance. For NVMe SSDs, we argue that trading off read performance for write performance is no longer always needed. With enough parallelism, NVMe SSDs have comparable random and sequential access performance. This change makes update-in-place designs, which traditionally provide excellent read performance, a viable alternative to LSMs.\n \n In this paper, we close the gap between log-structured and update-in-place designs on modern SSDs with the help of new components that take advantage of data and workload patterns. Specifically, we explore three key ideas: (A)\n record caching\n for efficient point operations, (B)\n page grouping\n for high-performance range scans, and (C)\n insert forecasting\n to reduce the reorganization costs of accommodating new records. We evaluate these ideas by implementing them in a prototype update-in-place key-value store called\n TreeLine.\n On YCSB, we find that TreeLine outperforms RocksDB and LeanStore by 2.20\u00d7 and 2.07\u00d7 respectively on average across the point workloads, and by up to 10.95\u00d7 and 7.52\u00d7 overall.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2191212297",
                    "name": "Geoffrey X. Yu"
                },
                {
                    "authorId": "2094294894",
                    "name": "Markos Markakis"
                },
                {
                    "authorId": "1922104",
                    "name": "Andreas Kipf"
                },
                {
                    "authorId": "144014124",
                    "name": "P. Larson"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "1746961",
                    "name": "Tim Kraska"
                }
            ]
        },
        {
            "paperId": "024ae66edebfec23a070f6512959278140248f9f",
            "title": "ML-In-Databases: Assessment and Prognosis",
            "abstract": "With the rapid adoption of Machine Learning (ML) in Computing, there has been a \ufb02urry of recent research considering using ML to build the internal component of database systems. While initial work in this area has shown interesting results, the jury is still out on whether these methods will replace existing methods. A group of researchers with opinions on both sides of this issue met to assess the state of this area and to formulate a plan for the next steps that would be needed to determine the potential role of these new ML-based methods in building future database systems. This article summarizes the collective perspectives that resulted from these discussions. First, this article describes broad forces that are changing the landscape in which database systems are deployed, connecting several trends that likely require rethinking how future database engines are built. Next, this article describes the different perspectives on this topic of using ML methods to replace existing internal database components. Finally, the key takeaways from this discussion are presented, and these takeaways also point to directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1746961",
                    "name": "Tim Kraska"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "143993045",
                    "name": "Thomas Neumann"
                },
                {
                    "authorId": "1967562",
                    "name": "Olga Papaemmanouil"
                },
                {
                    "authorId": "2070427422",
                    "name": "Jignesh M. Patel"
                },
                {
                    "authorId": "1803218",
                    "name": "Christopher R\u00e9"
                },
                {
                    "authorId": "145345023",
                    "name": "M. Stonebraker"
                }
            ]
        },
        {
            "paperId": "36a548e81222e4bdf74b240b559bddc5458b7321",
            "title": "APEX: A High-Performance Learned Index on Persistent Memory",
            "abstract": "The recently released persistent memory (PM) offers high performance, persistence, and is cheaper than DRAM. This opens up new possibilities for indexes that operate and persist data directly on the memory bus. Recent learned indexes exploit data distribution and have shown great potential for some workloads. However, none support persistence or instant recovery, and existing PM-based indexes typically evolve B+-trees without considering learned indexes.\n This paper proposes APEX, a new PM-optimized learned index that offers high performance, persistence, concurrency, and instant recovery. APEX is based on ALEX, a state-of-the-art updatable learned index, to combine and adapt the best of past PM optimizations and learned indexes, allowing it to reduce PM accesses while still exploiting machine learning. Our evaluation on Intel DCPMM shows that APEX can perform up to ~15\u00d7 better than existing PM indexes and can recover from failures in ~42ms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1672469312",
                    "name": "Baotong Lu"
                },
                {
                    "authorId": "2647009",
                    "name": "Jialin Ding"
                },
                {
                    "authorId": "145373095",
                    "name": "Eric Lo"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "48469973",
                    "name": "Tianzheng Wang"
                }
            ]
        },
        {
            "paperId": "984dfa89bab11be1263646be3be9236524999e79",
            "title": "Bounding the Last Mile: Efficient Learned String Indexing",
            "abstract": "We introduce the RadixStringSpline (RSS) learned index structure for efficiently indexing strings. RSS is a tree of radix splines each indexing a fixed number of bytes. RSS approaches or exceeds the performance of traditional string indexes while using 7-70$\\times$ less memory. RSS achieves this by using the minimal string prefix to sufficiently distinguish the data unlike most learned approaches which index the entire string. Additionally, the bounded-error nature of RSS accelerates the last mile search and also enables a memory-efficient hash-table lookup accelerator. We benchmark RSS on several real-world string datasets against ART and HOT. Our experiments suggest this line of research may be promising for future memory-intensive database applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2683016",
                    "name": "Benjamin Spector"
                },
                {
                    "authorId": "1922104",
                    "name": "Andreas Kipf"
                },
                {
                    "authorId": "7188311",
                    "name": "Kapil Vaidya"
                },
                {
                    "authorId": "2116627318",
                    "name": "Chi Wang"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "1746961",
                    "name": "Tim Kraska"
                }
            ]
        },
        {
            "paperId": "9f362a8d36777addaaafb51f10129e30860f74cc",
            "title": "Instance-Optimized Data Layouts for Cloud Analytics Workloads",
            "abstract": "Today, businesses rely on efficiently running analytics on large amounts of operational and historical data to gain business insights and competitive advantage. Increasingly, such analytics are run using cloud-based data analytics services, such as Google BigQuery, Microsoft Azure Synapse, Amazon Redshift, and Snowflake. These services persist and process data in compressed, columnar formats, stored in large blocks, each of which contains thousands or millions of records. For these services, disk I/O from (remote) cloud storage is often one of the dominant costs for query processing. To reduce the amount of I/O, services often maintain per-block metadata, such as zone maps, which are used to skip blocks that are irrelevant to the query, leading to lower query execution times. However, the effectiveness of block skipping via zone maps is dependent on how the records are assigned to blocks. Recent work on instance-optimized data layouts aims to maximize block skipping by specializing the block assignment strategy to a specific dataset and workload. However, these existing approaches only optimize the layout for a single table. In this paper, we propose MTO, an instance-optimized data layout framework that determines the blocking strategy for all tables in a multi-table database in the presence of joins, such as in a star or snowflake schema common in real-world workloads. MTO takes advantage of sideways information passing through joins to jointly optimize the layout for all tables, which results in better block skipping and hence reduced query execution times. Experiments on a commercial cloud-based analytics service show that MTO achieves up to 93% reduction in blocks accessed and 75% reduction in end-to-end query times compared to alternative blocking strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2647009",
                    "name": "Jialin Ding"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "2241445",
                    "name": "Badrish Chandramouli"
                },
                {
                    "authorId": null,
                    "name": "Chi Wang"
                },
                {
                    "authorId": "2144424734",
                    "name": "Yinan Li"
                },
                {
                    "authorId": "2155509266",
                    "name": "Ying Li"
                },
                {
                    "authorId": "1691108",
                    "name": "Donald Kossmann"
                },
                {
                    "authorId": "143614516",
                    "name": "J. Gehrke"
                },
                {
                    "authorId": "1746961",
                    "name": "Tim Kraska"
                }
            ]
        }
    ]
}