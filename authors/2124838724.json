{
    "authorId": "2124838724",
    "papers": [
        {
            "paperId": "a9086398e27327e5cc664cbabf8a7bbb25c2c50f",
            "title": "ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction",
            "abstract": "The prediction of protein-protein interactions (PPIs) is crucial for understanding biological functions and diseases. Previous machine learning approaches to PPI prediction mainly focus on direct physical interactions, ignoring the broader context of nonphysical connections through intermediate proteins, thus limiting their effectiveness. The emergence of Large Language Models (LLMs) provides a new opportunity for addressing this complex biological challenge. By transforming structured data into natural language prompts, we can map the relationships between proteins into texts. This approach allows LLMs to identify indirect connections between proteins, tracing the path from upstream to downstream. Therefore, we propose a novel framework ProLLM that employs an LLM tailored for PPI for the first time. Specifically, we propose Protein Chain of Thought (ProCoT), which replicates the biological mechanism of signaling pathways as natural language prompts. ProCoT considers a signaling pathway as a protein reasoning process, which starts from upstream proteins and passes through several intermediate proteins to transmit biological signals to downstream proteins. Thus, we can use ProCoT to predict the interaction between upstream proteins and downstream proteins. The training of ProLLM employs the ProCoT format, which enhances the model\u2019s understanding of complex biological problems. In addition to ProCoT, this paper also contributes to the exploration of embedding replacement of protein sites in natural language prompts, and instruction fine-tuning in protein knowledge datasets. We demonstrate the efficacy of ProLLM through rigorous validation against benchmark datasets, showing significant improvement over existing methods in terms of prediction accuracy and generalizability. Our results highlight the potential of LLMs to transform the field of PPI, serving as a robust potential tool for various categories of biological and medical research. The code is available at: https://github.com/MingyuJ666/ProLLM.",
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220539385",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2219696383",
                    "name": "Haochen Xue"
                },
                {
                    "authorId": "2292292249",
                    "name": "Zhenting Wang"
                },
                {
                    "authorId": "2298210065",
                    "name": "Boming Kang"
                },
                {
                    "authorId": "2124838724",
                    "name": "Ruosong Ye"
                },
                {
                    "authorId": "2298221156",
                    "name": "Kaixiong Zhou"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2279766837",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "f89e85059a55b647c93822aefa7e985376e0ef20",
            "title": "AIOS: LLM Agent Operating System",
            "abstract": "The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS) as the brain of the OS, enabling an operating system\"with soul\"-- an important step towards AGI. Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, and maintain access control for agents. We present the architecture of such an operating system, outline the core challenges it aims to resolve, and provide the basic design and implementation of the AIOS. Our experiments on concurrent execution of multiple agents demonstrate the reliability and efficiency of our AIOS modules. Through this, we aim to not only improve the performance and efficiency of LLM agents but also to pioneer for better development and deployment of the AIOS ecosystem in the future. The project is open-source at https://github.com/agiresearch/AIOS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261740874",
                    "name": "Kai Mei"
                },
                {
                    "authorId": "2109968285",
                    "name": "Zelong Li"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2124838724",
                    "name": "Ruosong Ye"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "8d65b5940a4dbed8c18e02ca35e3a9d7a14ea76b",
            "title": "Language is All a Graph Needs",
            "abstract": "The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data like images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, languages, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is available at https://github.com/agiresearch/InstructGLM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2124838724",
                    "name": "Ruosong Ye"
                },
                {
                    "authorId": "2294313959",
                    "name": "Caiqi Zhang"
                },
                {
                    "authorId": "5989884",
                    "name": "Runhui Wang"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2188012070",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "70471865ab4d424499fe4b7d8f295e3316017e8b",
            "title": "Reinforcement Learning for Robot Navigation with Adaptive ExecutionDuration (AED) in a Semi-Markov Model",
            "abstract": "\u2014Deep reinforcement learning (DRL) algorithms have proven effective in robot navigation, especially in unknown environments, through directly mapping perception inputs into robot control commands. Most existing methods adopt uniform execution duration with robots taking commands at \ufb01xed intervals. As such, the length of execution duration becomes a crucial parameter to the navigation algorithm. In particular, if the duration is too short, then the navigation policy would be executed at a high frequency, with increased training dif\ufb01culty and high computational cost. Meanwhile, if the duration is too long, then the policy becomes unable to handle complex situations, like those with crowded obstacles. It is thus tricky to \ufb01nd the \u201csweet\u201d duration range; some duration values may render a DRL model to fail to \ufb01nd a navigation path. In this paper, we propose to employ adaptive execution duration to overcome this problem. Speci\ufb01cally, we formulate the navigation task as a Semi-Markov Decision Process (SMDP) problem to handle adaptive execution duration. We also improve the distributed proximal policy optimization (DPPO) algorithm and provide its theoretical guarantee for the speci\ufb01ed SMDP problem. We evaluate our approach both in the simulator and on an actual robot. The results show that our approach outperforms the other DRL-based method (with \ufb01xed execution duration) by 10 . 3% in terms of the navigation success rate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144036786",
                    "name": "Yu'an Chen"
                },
                {
                    "authorId": "2124838724",
                    "name": "Ruosong Ye"
                },
                {
                    "authorId": "2125197615",
                    "name": "Ziyang Tao"
                },
                {
                    "authorId": "2130173",
                    "name": "Hongjian Liu"
                },
                {
                    "authorId": "145003028",
                    "name": "Guangda Chen"
                },
                {
                    "authorId": "2107481406",
                    "name": "J. Peng"
                },
                {
                    "authorId": "2152611550",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2153635348",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2124819888",
                    "name": "Yanyong Zhang"
                },
                {
                    "authorId": "39574278",
                    "name": "Jianmin Ji"
                }
            ]
        },
        {
            "paperId": "ad91f492c35610bf4c36cf0674471a851774e050",
            "title": "Reinforcement Learning for Robot Navigation with Adaptive Forward Simulation Time (AFST) in a Semi-Markov Model",
            "abstract": "Deep reinforcement learning (DRL) algorithms have proven effective in robot navigation, especially in unknown environments, by directly mapping perception inputs into robot control commands. However, most existing methods ignore the local minimum problem in navigation and thereby cannot handle complex unknown environments. In this paper, we propose the first DRL-based navigation method modeled by a semi-Markov decision process (SMDP) with continuous action space, named Adaptive Forward Simulation Time (AFST), to overcome this problem. Specifically, we reduce the dimensions of the action space and improve the distributed proximal policy optimization (DPPO) algorithm for the specified SMDP problem by modifying its GAE to better estimate the policy gradient in SMDPs. Experiments in various unknown environments demonstrate the effectiveness of AFST.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144036786",
                    "name": "Yu'an Chen"
                },
                {
                    "authorId": "2124838724",
                    "name": "Ruosong Ye"
                },
                {
                    "authorId": "2125197615",
                    "name": "Ziyang Tao"
                },
                {
                    "authorId": "2115571277",
                    "name": "Hongjian Liu"
                },
                {
                    "authorId": "145003028",
                    "name": "Guangda Chen"
                },
                {
                    "authorId": "2122806801",
                    "name": "Jie Peng"
                },
                {
                    "authorId": "2152611550",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2153635348",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "39574278",
                    "name": "Jianmin Ji"
                },
                {
                    "authorId": "2124819888",
                    "name": "Yanyong Zhang"
                }
            ]
        }
    ]
}