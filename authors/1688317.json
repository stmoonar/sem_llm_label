{
    "authorId": "1688317",
    "papers": [
        {
            "paperId": "5aae89ffce7bdfcad0f2b53e0566774b9c2fcf35",
            "title": "Data Structures for Density Estimation",
            "abstract": "We study statistical/computational tradeoffs for the following density estimation problem: given $k$ distributions $v_1, \\ldots, v_k$ over a discrete domain of size $n$, and sampling access to a distribution $p$, identify $v_i$ that is\"close\"to $p$. Our main result is the first data structure that, given a sublinear (in $n$) number of samples from $p$, identifies $v_i$ in time sublinear in $k$. We also give an improved version of the algorithm of Acharya et al. (2018) that reports $v_i$ in time linear in $k$. The experimental evaluation of the latter algorithm shows that it achieves a significant reduction in the number of operations needed to achieve a given accuracy compared to prior work.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "23998886",
                    "name": "Anders Aamand"
                },
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "2108178952",
                    "name": "Justin Y. Chen"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "50812086",
                    "name": "Shyam Narayanan"
                },
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                }
            ]
        },
        {
            "paperId": "5c990832e6a03fa753953f6d4ed06cf41f00d749",
            "title": "Space-Optimal Profile Estimation in Data Streams with Applications to Symmetric Functions",
            "abstract": "We revisit the problem of estimating the profile (also known as the rarity) in the data stream model. Given a sequence of $m$ elements from a universe of size $n$, its profile is a vector $\\phi$ whose $i$-th entry $\\phi_i$ represents the number of distinct elements that appear in the stream exactly $i$ times. A classic paper by Datar and Muthukrishan from 2002 gave an algorithm which estimates any entry $\\phi_i$ up to an additive error of $\\pm \\epsilon D$ using $O(1/\\epsilon^2 (\\log n + \\log m))$ bits of space, where $D$ is the number of distinct elements in the stream. In this paper, we considerably improve on this result by designing an algorithm which simultaneously estimates many coordinates of the profile vector $\\phi$ up to small overall error. We give an algorithm which, with constant probability, produces an estimated profile $\\hat\\phi$ with the following guarantees in terms of space and estimation error: - For any constant $\\tau$, with $O(1 / \\epsilon^2 + \\log n)$ bits of space, $\\sum_{i=1}^\\tau |\\phi_i - \\hat\\phi_i| \\leq \\epsilon D$. - With $O(1/ \\epsilon^2\\log (1/\\epsilon) + \\log n + \\log \\log m)$ bits of space, $\\sum_{i=1}^m |\\phi_i - \\hat\\phi_i| \\leq \\epsilon m$. In addition to bounding the error across multiple coordinates, our space bounds separate the terms that depend on $1/\\epsilon$ and those that depend on $n$ and $m$. We prove matching lower bounds on space in both regimes. Application of our profile estimation algorithm gives estimates within error $\\pm \\epsilon D$ of several symmetric functions of frequencies in $O(1/\\epsilon^2 + \\log n)$ bits. This generalizes space-optimal algorithms for the distinct elements problems to other problems including estimating the Huber and Tukey losses as well as frequency cap statistics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108178952",
                    "name": "Justin Y. Chen"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2268673253",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "a7a8ea981552e13c87f33a1066c27f0eb8f27491",
            "title": "Learned Interpolation for Better Streaming Quantile Approximation with Worst-Case Guarantees",
            "abstract": "An $\\varepsilon$-approximate quantile sketch over a stream of $n$ inputs approximates the rank of any query point $q$ - that is, the number of input points less than $q$ - up to an additive error of $\\varepsilon n$, generally with some probability of at least $1 - 1/\\mathrm{poly}(n)$, while consuming $o(n)$ space. While the celebrated KLL sketch of Karnin, Lang, and Liberty achieves a provably optimal quantile approximation algorithm over worst-case streams, the approximations it achieves in practice are often far from optimal. Indeed, the most commonly used technique in practice is Dunning's t-digest, which often achieves much better approximations than KLL on real-world data but is known to have arbitrarily large errors in the worst case. We apply interpolation techniques to the streaming quantiles problem to attempt to achieve better approximations on real-world data sets than KLL while maintaining similar guarantees in the worst case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2833768",
                    "name": "Nicholas Schiefer"
                },
                {
                    "authorId": "2108178952",
                    "name": "Justin Y. Chen"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "50812086",
                    "name": "Shyam Narayanan"
                },
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "b32d81820a5a915c881c66dc788a93163361cf14",
            "title": "A Near-Linear Time Algorithm for the Chamfer Distance",
            "abstract": "For any two point sets $A,B \\subset \\mathbb{R}^d$ of size up to $n$, the Chamfer distance from $A$ to $B$ is defined as $\\text{CH}(A,B)=\\sum_{a \\in A} \\min_{b \\in B} d_X(a,b)$, where $d_X$ is the underlying distance measure (e.g., the Euclidean or Manhattan distance). The Chamfer distance is a popular measure of dissimilarity between point clouds, used in many machine learning, computer vision, and graphics applications, and admits a straightforward $O(d n^2)$-time brute force algorithm. Further, the Chamfer distance is often used as a proxy for the more computationally demanding Earth-Mover (Optimal Transport) Distance. However, the \\emph{quadratic} dependence on $n$ in the running time makes the naive approach intractable for large datasets. We overcome this bottleneck and present the first $(1+\\epsilon)$-approximate algorithm for estimating the Chamfer distance with a near-linear running time. Specifically, our algorithm runs in time $O(nd \\log (n)/\\varepsilon^2)$ and is implementable. Our experiments demonstrate that it is both accurate and fast on large high-dimensional datasets. We believe that our algorithm will open new avenues for analyzing large high-dimensional point clouds. We also give evidence that if the goal is to \\emph{report} a $(1+\\varepsilon)$-approximate mapping from $A$ to $B$ (as opposed to just its value), then any sub-quadratic time algorithm is unlikely to exist.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35300553",
                    "name": "Ainesh Bakshi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "20900712",
                    "name": "Rajesh Jayaram"
                },
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                },
                {
                    "authorId": "1700373",
                    "name": "Erik Waingarten"
                }
            ]
        },
        {
            "paperId": "00f6c714087a3dd176f303a72c679f6f4c9fa595",
            "title": "Streaming Algorithms for Support-Aware Histograms",
            "abstract": "Histograms, i.e., piece-wise constant approximations, are a popular tool used to represent data distributions. Traditionally, the difference between the histogram and the underlying distribution (i.e., the approximation error) is measured using the L p norm, which sums the differences between the two functions over all items in the domain. Although useful in many applications, the drawback of this error measure is that it treats approximation errors of all items in the same way, irrespec-tive of whether the mass of an item is important for the downstream application that uses the approximation. As a result, even relatively simple distributions cannot be approximated by succinct histograms without incurring large error. In this paper, we address this issue by adapting the de\ufb01nition of approximation so that only the errors of the items that belong to the support of the distribution are considered. Under this de\ufb01nition, we develop ef\ufb01cient 1-pass and 2-pass streaming algorithms that compute near-optimal histograms in sub-linear space. We also present lower bounds on the space complexity of this problem. Surpris-ingly, under this notion of error, there is an exponential gap in the space complexity of 1-pass and 2-pass streaming algorithms. Finally, we demon-strate the utility of our algorithms on a collection of real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108178952",
                    "name": "Justin Y. Chen"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "15d63f80f62c2d1ccc879b26f27ac75ba3ab63ea",
            "title": "Generalization Bounds for Data-Driven Numerical Linear Algebra",
            "abstract": "Data-driven algorithms can adapt their internal structure or parameters to inputs from unknown application-specific distributions, by learning from a training sample of inputs. Several recent works have applied this approach to problems in numerical linear algebra, obtaining significant empirical gains in performance. However, no theoretical explanation for their success was known. In this work we prove generalization bounds for those algorithms, within the PAC-learning framework for data-driven algorithm selection proposed by Gupta and Roughgarden (SICOMP 2017). Our main results are closely matching upper and lower bounds on the fat shattering dimension of the learning-based low rank approximation algorithm of Indyk et al.~(NeurIPS 2019). Our techniques are general, and provide generalization bounds for many other recently proposed data-driven algorithms in numerical linear algebra, covering both sketching-based and multigrid-based methods. This considerably broadens the class of data-driven algorithms for which a PAC-learning analysis is available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1745169",
                    "name": "P. Bartlett"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "2abbbc44ac51362540cdac1e34a99c03091424da",
            "title": "Triangle and Four Cycle Counting with Predictions in Graph Streams",
            "abstract": "We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature. Recently, (Hsu 2018) and (Jiang 2020) applied machine learning techniques in other data stream problems, using a trained oracle that can predict certain properties of the stream elements to improve on prior\"classical\"algorithms that did not use oracles. In this paper, we explore the power of a\"heavy edge\"oracle in multiple graph edge streaming models. In the adjacency list model, we present a one-pass triangle counting algorithm improving upon the previous space upper bounds without such an oracle. In the arbitrary order model, we present algorithms for both triangle and four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal. We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs. Our methodology expands upon prior work on\"classical\"streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle. Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-the-art streaming algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108178952",
                    "name": "Justin Y. Chen"
                },
                {
                    "authorId": "27422922",
                    "name": "T. Eden"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "48444613",
                    "name": "Honghao Lin"
                },
                {
                    "authorId": "50812086",
                    "name": "Shyam Narayanan"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                },
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                },
                {
                    "authorId": "2159325846",
                    "name": "Michael Zhang"
                }
            ]
        },
        {
            "paperId": "45c93611afb2ef4d4d6b3ad9336ea7979f70c244",
            "title": "Faster Linear Algebra for Distance Matrices",
            "abstract": "The distance matrix of a dataset $X$ of $n$ points with respect to a distance function $f$ represents all pairwise distances between points in $X$ induced by $f$. Due to their wide applicability, distance matrices and related families of matrices have been the focus of many recent algorithmic works. We continue this line of research and take a broad view of algorithm design for distance matrices with the goal of designing fast algorithms, which are specifically tailored for distance matrices, for fundamental linear algebraic primitives. Our results include efficient algorithms for computing matrix-vector products for a wide class of distance matrices, such as the $\\ell_1$ metric for which we get a linear runtime, as well as an $\\Omega(n^2)$ lower bound for any algorithm which computes a matrix-vector product for the $\\ell_{\\infty}$ case, showing a separation between the $\\ell_1$ and the $\\ell_{\\infty}$ metrics. Our upper bound results, in conjunction with recent works on the matrix-vector query model, have many further downstream applications, including the fastest algorithm for computing a relative error low-rank approximation for the distance matrix induced by $\\ell_1$ and $\\ell_2^2$ functions and the fastest algorithm for computing an additive error low-rank approximation for the $\\ell_2$ metric, in addition to applications for fast matrix multiplication among others. We also give algorithms for constructing distance matrices and show that one can construct an approximate $\\ell_2$ distance matrix in time faster than the bound implied by the Johnson-Lindenstrauss lemma.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                }
            ]
        },
        {
            "paperId": "c7ce06fad686f60649f0ebe38702ad16ab3353bb",
            "title": "Sub-quadratic Algorithms for Kernel Matrices via Kernel Density Estimation",
            "abstract": "Kernel matrices, as well as weighted graphs represented by them, are ubiquitous objects in machine learning, statistics and other related fields. The main drawback of using kernel methods (learning and inference using kernel matrices) is efficiency -- given $n$ input points, most kernel-based algorithms need to materialize the full $n \\times n$ kernel matrix before performing any subsequent computation, thus incurring $\\Omega(n^2)$ runtime. Breaking this quadratic barrier for various problems has therefore, been a subject of extensive research efforts. We break the quadratic barrier and obtain $\\textit{subquadratic}$ time algorithms for several fundamental linear-algebraic and graph processing primitives, including approximating the top eigenvalue and eigenvector, spectral sparsification, solving linear systems, local clustering, low-rank approximation, arboricity estimation and counting weighted triangles. We build on the recent Kernel Density Estimation framework, which (after preprocessing in time subquadratic in $n$) can return estimates of row/column sums of the kernel matrix. In particular, we develop efficient reductions from $\\textit{weighted vertex}$ and $\\textit{weighted edge sampling}$ on kernel graphs, $\\textit{simulating random walks}$ on kernel graphs, and $\\textit{importance sampling}$ on matrices to Kernel Density Estimation and show that we can generate samples from these distributions in $\\textit{sublinear}$ (in the support of the distribution) time. Our reductions are the central ingredient in each of our applications and we believe they may be of independent interest. We empirically demonstrate the efficacy of our algorithms on low-rank approximation (LRA) and spectral sparsification, where we observe a $\\textbf{9x}$ decrease in the number of kernel evaluations over baselines for LRA and a $\\textbf{41x}$ reduction in the graph size for spectral sparsification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35300553",
                    "name": "Ainesh Bakshi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1471876925",
                    "name": "Praneeth Kacham"
                },
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                },
                {
                    "authorId": "3393628",
                    "name": "Samson Zhou"
                }
            ]
        },
        {
            "paperId": "fcbfac9975f01d69f4f81bb567afc3bed8d23418",
            "title": "Exponentially Improving the Complexity of Simulating the Weisfeiler-Lehman Test with Graph Neural Networks",
            "abstract": "Recent work shows that the expressive power of Graph Neural Networks (GNNs) in distinguishing non-isomorphic graphs is exactly the same as that of the Weisfeiler-Lehman (WL) graph test. In particular, they show that the WL test can be simulated by GNNs. However, those simulations involve neural networks for the 'combine' function of size polynomial or even exponential in the number of graph nodes $n$, as well as feature vectors of length linear in $n$. We present an improved simulation of the WL test on GNNs with \\emph{exponentially} lower complexity. In particular, the neural network implementing the combine function in each node has only a polylogarithmic number of parameters in $n$, and the feature vectors exchanged by the nodes of GNN consists of only $O(\\log n)$ bits. We also give logarithmic lower bounds for the feature vector length and the size of the neural networks, showing the (near)-optimality of our construction.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "23998886",
                    "name": "Anders Aamand"
                },
                {
                    "authorId": "2108178952",
                    "name": "Justin Y. Chen"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "50812086",
                    "name": "Shyam Narayanan"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                },
                {
                    "authorId": "2833768",
                    "name": "Nicholas Schiefer"
                },
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "2121c6910b5f187fdaecf65981ed76a6a668a559",
            "title": "Targeted Supervised Contrastive Learning for Long-Tailed Recognition",
            "abstract": "Real-world data often exhibits long tail distributions with heavy class imbalance, where the majority classes can dominate the training process and alter the decision bound-aries of the minority classes. Recently, researchers have in-vestigated the potential of supervised contrastive learning for long-tailed recognition, and demonstrated that it provides a strong performance gain. In this paper, we show that while supervised contrastive learning can help improve performance, past baselines suffer from poor uniformity brought in by imbalanced data distribution. This poor uni-formity manifests in samples from the minority class having poor separability in the feature space. To address this problem, we propose targeted supervised contrastive learning (TSC), which improves the uniformity of the feature distribution on the hypersphere. TSC first generates a set of targets uniformly distributed on a hypersphere. It then makes the features of different classes converge to these distinct and uniformly distributed targets during training. This forces all classes, including minority classes, to main-tain a uniform distribution in the feature space, improves class boundaries, and provides better generalization even in the presence of long-tail data. Experiments on multi-ple datasets show that TSC achieves state-of-the-art performance on long-tailed recognition tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118909373",
                    "name": "Tianhong Li"
                },
                {
                    "authorId": "152550893",
                    "name": "Peng Cao"
                },
                {
                    "authorId": "46499812",
                    "name": "Yuan Yuan"
                },
                {
                    "authorId": "2548303",
                    "name": "Lijie Fan"
                },
                {
                    "authorId": "2125052692",
                    "name": "Yuzhe Yang"
                },
                {
                    "authorId": "1723233",
                    "name": "R. Feris"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                }
            ]
        },
        {
            "paperId": "4ed0161867a97ea7fb6f00ea0668c68312f3ce31",
            "title": "Optimal (Euclidean) Metric Compression",
            "abstract": "We study the problem of representing all distances between $n$ points in $\\mathbb R^d$, with arbitrarily small distortion, using as few bits as possible. We give asymptotically tight bounds for this problem, for Euclidean metrics, for $\\ell_1$ (a.k.a.~Manhattan) metrics, and for general metrics. Our bounds for Euclidean metrics mark the first improvement over compression schemes based on discretizing the classical dimensionality reduction theorem of Johnson and Lindenstrauss (Contemp.~Math.~1984). Since it is known that no better dimension reduction is possible, our results establish that Euclidean metric compression is possible beyond dimension reduction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "759b754c9060233eb6ab33f687b2245affaaa7d9",
            "title": "Faster Kernel Matrix Algebra via Density Estimation",
            "abstract": "We study fast algorithms for computing fundamental properties of a positive semidefinite kernel matrix $K \\in \\mathbb{R}^{n \\times n}$ corresponding to $n$ points $x_1,\\ldots,x_n \\in \\mathbb{R}^d$. In particular, we consider estimating the sum of kernel matrix entries, along with its top eigenvalue and eigenvector. We show that the sum of matrix entries can be estimated to $1+\\epsilon$ relative error in time $sublinear$ in $n$ and linear in $d$ for many popular kernels, including the Gaussian, exponential, and rational quadratic kernels. For these kernels, we also show that the top eigenvalue (and an approximate eigenvector) can be approximated to $1+\\epsilon$ relative error in time $subquadratic$ in $n$ and linear in $d$. Our algorithms represent significant advances in the best known runtimes for these problems. They leverage the positive definiteness of the kernel matrix, along with a recent line of work on efficient kernel density estimation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2032038",
                    "name": "Cameron Musco"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "8b0531da015c830f9ae31cb5356516285d73f2fe",
            "title": "(Optimal) Online Bipartite Matching with Degree Information",
            "abstract": "We propose a model for online graph problems where algorithms are given access to an oracle that predicts (e.g., based on modeling assumptions or on past data) the degrees of nodes in the graph. Within this model, we study the classic problem of online bipartite matching, and a natural greedy matching algorithm called MinPredictedDegree, which uses predictions of the degrees of offline nodes. For the bipartite version of a stochastic graph model due to Chung, Lu, and Vu where the expected values of the offline degrees are known and used as predictions, we show that MinPredictedDegree stochastically dominates any other online algorithm, i.e., it is optimal for graphs drawn from this model. Since the\"symmetric\"version of the model, where all online nodes are identical, is a special case of the well-studied\"known i.i.d. model\", it follows that the competitive ratio of MinPredictedDegree on such inputs is at least 0.7299. For the special case of graphs with power law degree distributions, we show that MinPredictedDegree frequently produces matchings almost as large as the true maximum matching on such graphs. We complement these results with an extensive empirical evaluation showing that MinPredictedDegree compares favorably to state-of-the-art online algorithms for online matching.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "23998886",
                    "name": "Anders Aamand"
                },
                {
                    "authorId": "2108178952",
                    "name": "Justin Y. Chen"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "91932ef92a2dbea21297e950e8b77d42580b95fd",
            "title": "Frequency Estimation with One-Sided Error",
            "abstract": "Frequency estimation is one of the most fundamental problems in streaming algorithms. Given a stream $S$ of elements from some universe $U=\\{1 \\ldots n\\}$, the goal is to compute, in a single pass, a short sketch of $S$ so that for any element $i \\in U$, one can estimate the number $x_i$ of times $i$ occurs in $S$ based on the sketch alone. Two state of the art solutions to this problems are the Count-Min and Count-Sketch algorithms. The frequency estimator $\\tilde{x}$ produced by Count-Min, using $O(1/\\varepsilon \\cdot \\log n)$ dimensions, guarantees that $\\|\\tilde{x}-x\\|_{\\infty} \\le \\varepsilon \\|x\\|_1$ with high probability, and $\\tilde{x} \\ge x$ holds deterministically. Also, Count-Min works under the assumption that $x \\ge 0$. On the other hand, Count-Sketch, using $O(1/\\varepsilon^2 \\cdot \\log n)$ dimensions, guarantees that $\\|\\tilde{x}-x\\|_{\\infty} \\le \\varepsilon \\|x\\|_2$ with high probability. A natural question is whether it is possible to design the best of both worlds sketching method, with error guarantees depending on the $\\ell_2$ norm and space comparable to Count-Sketch, but (like Count-Min) also has the no-underestimation property. Our main set of results shows that the answer to the above question is negative. We show this in two incomparable computational models: linear sketching and streaming algorithms. We also study the complementary problem, where the sketch is required to not over-estimate, i.e., $\\tilde{x} \\le x$ should hold always.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "50812086",
                    "name": "Shyam Narayanan"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "bd5c32f003c670366e5d62c59fe02e747a470647",
            "title": "Learning-based Support Estimation in Sublinear Time",
            "abstract": "We consider the problem of estimating the number of distinct elements in a large data set (or, equivalently, the support size of the distribution induced by the data set) from a random sample of its elements. The problem occurs in many applications, including biology, genomics, computer systems and linguistics. A line of research spanning the last decade resulted in algorithms that estimate the support up to $ \\pm \\varepsilon n$ from a sample of size $O(\\log^2(1/\\varepsilon) \\cdot n/\\log n)$, where $n$ is the data set size. Unfortunately, this bound is known to be tight, limiting further improvements to the complexity of this problem. In this paper we consider estimation algorithms augmented with a machine-learning-based predictor that, given any element, returns an estimation of its frequency. We show that if the predictor is correct up to a constant approximation factor, then the sample complexity can be reduced significantly, to \\[ \\ \\log (1/\\varepsilon) \\cdot n^{1-\\Theta(1/\\log(1/\\varepsilon))}. \\] We evaluate the proposed algorithms on a collection of data sets, using the neural-network based estimators from {Hsu et al, ICLR'19} as predictors. Our experiments demonstrate substantial (up to 3x) improvements in the estimation accuracy compared to the state of the art algorithm.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "27422922",
                    "name": "T. Eden"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "50812086",
                    "name": "Shyam Narayanan"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                },
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "bdbaf59d78a17db9bca840958f865f680aab0447",
            "title": "Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering",
            "abstract": "Random dimensionality reduction is a versatile tool for speeding up algorithms for highdimensional problems. We study its application to two clustering problems: the facility location problem, and the single-linkage hierarchical clustering problem, which is equivalent to computing the minimum spanning tree. We show that if we project the input pointset X onto a random d = O(dX)-dimensional subspace (where dX is the doubling dimension of X), then the optimum facility location cost in the projected space approximates the original cost up to a constant factor. We show an analogous statement for minimum spanning tree, but with the dimension d having an extra log log n term and the approximation factor being arbitrarily close to 1. Furthermore, we extend these results to approximating solutions instead of just their costs. Lastly, we provide experimental results to validate the quality of solutions and the speedup due to the dimensionality reduction. Unlike several previous papers studying this approach in the context of k-means and k-medians, our dimension bound does not depend on the number of clusters but only on the intrinsic dimensionality of X .",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "50812086",
                    "name": "Shyam Narayanan"
                },
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "3147193",
                    "name": "Or Zamir"
                }
            ]
        },
        {
            "paperId": "d8fc221e97d210c219cb60672c943a9877f5ad7d",
            "title": "Few-Shot Data-Driven Algorithms for Low Rank Approximation",
            "abstract": "Recently, data-driven and learning-based algorithms for low rank matrix approximation were shown to outperform classical data-oblivious algorithms by wide margins in terms of accuracy. Those algorithms are based on the optimization of sparse sketching matrices, which lead to large savings in time and memory during testing. However, they require long training times on a large amount of existing data, and rely on access to specialized hardware and software. In this work, we develop new data-driven low rank approximation algorithms with better computational efficiency in the training phase, alleviating these drawbacks. Furthermore, our methods are interpretable: while previous algorithms choose the sketching matrix either at random or by black-box learning, we show that it can be set (or initialized) to clearly interpretable values extracted from the dataset. Our experiments show that our algorithms, either by themselves or in combination with previous methods, achieve significant empirical advantages over previous work, improving training times by up to an order of magnitude toward achieving the same target accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "dde7bb7bb548a513cac38757fffd66dfa8f8bed0",
            "title": "Online Bipartite Matching with Predicted Degrees",
            "abstract": "We propose a model for online graph problems where algorithms are given access to an oracle that predicts the degrees of nodes in the graph (e.g., based on past data). Within this model, we study the classic problem of online bipartite matching. An extensive empirical evaluation shows that a greedy algorithm called Min-PredictedDegree compares favorably to state-of-the-art online algorithms for this problem. We also initiate the theoretical study of MinPredictedDegree on a natural random graph model with power law degree distribution and show that it produces matchings almost as large as the maximum matching on such graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108178952",
                    "name": "Justin Y. Chen"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "e7288c193542d2d97f186d1e36227694f3d3501e",
            "title": "Embeddings and labeling schemes for A",
            "abstract": "A* is a classic and popular method for graphs search and path finding. It assumes the existence of a heuristic function $h(u,t)$ that estimates the shortest distance from any input node $u$ to the destination $t$. Traditionally, heuristics have been handcrafted by domain experts. However, over the last few years, there has been a growing interest in learning heuristic functions. Such learned heuristics estimate the distance between given nodes based on\"features\"of those nodes. In this paper we formalize and initiate the study of such feature-based heuristics. In particular, we consider heuristics induced by norm embeddings and distance labeling schemes, and provide lower bounds for the tradeoffs between the number of dimensions or bits used to represent each graph node, and the running time of the A* algorithm. We also show that, under natural assumptions, our lower bounds are almost optimal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27422922",
                    "name": "T. Eden"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2118533958",
                    "name": "Haike Xu"
                }
            ]
        },
        {
            "paperId": "2d70288c89a05c59f5306b6edf4342cda2e8cc15",
            "title": "Online Page Migration with ML Advice",
            "abstract": "We consider online algorithms for the {\\em page migration problem} that use predictions, potentially imperfect, to improve their performance. The best known online algorithms for this problem, due to Westbrook'94 and Bienkowski et al'17, have competitive ratios strictly bounded away from 1. In contrast, we show that if the algorithm is given a prediction of the input sequence, then it can achieve a competitive ratio that tends to $1$ as the prediction error rate tends to $0$. Specifically, the competitive ratio is equal to $1+O(q)$, where $q$ is the prediction error rate. We also design a ``fallback option'' that ensures that the competitive ratio of the algorithm for {\\em any} input sequence is at most $O(1/q)$. Our result adds to the recent body of work that uses machine learning to improve the performance of ``classic'' algorithms.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1403790644",
                    "name": "Frederik Mallmann-Trenn"
                },
                {
                    "authorId": "2063077684",
                    "name": "Slobodan Mitrovi'c"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                }
            ]
        },
        {
            "paperId": "4995b33c1e80b95b71624e7802e48bdcd3ffd01b",
            "title": "Addressing Feature Suppression in Unsupervised Visual Representations",
            "abstract": "Contrastive learning is one of the fastest growing research areas in machine learning due to its ability to learn useful representations without labeled data. However, contrastive learning is susceptible to feature suppression \u2013 i.e., it may discard important information relevant to the task of interest, and learn irrelevant features. Past work has addressed this limitation via handcrafted data augmentations that eliminate irrelevant information. This approach however does not work across all datasets and tasks. Further, data augmentations fail in addressing feature suppression in multi-attribute classification when one attribute can suppress features relevant to other attributes. In this paper, we analyze the objective function of contrastive learning and formally prove that it is vulnerable to feature suppression. We then present Predictive Contrastive Learning (PrCL), a framework for learning unsupervised representations that are robust to feature suppression. The key idea is to force the learned representation to predict the input, and hence prevent it from discarding important information. Extensive experiments verify that PrCL is robust to feature suppression and outperforms state-of-the-art contrastive learning methods on a variety of datasets and tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118909373",
                    "name": "Tianhong Li"
                },
                {
                    "authorId": "2548303",
                    "name": "Lijie Fan"
                },
                {
                    "authorId": "46499812",
                    "name": "Yuan Yuan"
                },
                {
                    "authorId": "153168451",
                    "name": "Hao He"
                },
                {
                    "authorId": "2476765",
                    "name": "Yonglong Tian"
                },
                {
                    "authorId": "1723233",
                    "name": "R. Feris"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                }
            ]
        },
        {
            "paperId": "07e92b8cf0d5b60ae0271bdeb82b7c3663108572",
            "title": "Learning Space Partitions for Nearest Neighbor Search",
            "abstract": "Space partitions of $\\mathbb{R}^d$ underlie a vast and important class of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification. We instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145595795",
                    "name": "Yihe Dong"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2111443",
                    "name": "Ilya P. Razenshteyn"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "62dbca4c48dc48eab0af3efa569c060fb8bfe514",
            "title": "Neural Embeddings for Nearest Neighbor Search Under Edit Distance",
            "abstract": "The edit distance between two sequences is an important metric with many applications. The drawback, however, is the high computational cost of many basic problems involving this notion, such as the nearest neighbor search. A natural approach to overcoming this issue is to embed the sequences into a vector space such that the geometric distance in the target space approximates the edit distance in the original space. However, the known edit distance embedding algorithms, such as Chakraborty et al. (2016), construct embeddings that are data-independent, i.e., do not exploit any structure of embedded sets of strings. In this paper we propose an alternative approach, which learns the embedding function according to the data distribution. Our experiments show that the new algorithm has much better empirical performance than prior data-independent methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108217022",
                    "name": "Xiyuan Zhang"
                },
                {
                    "authorId": "145155436",
                    "name": "Yang Yuan"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "6978959aa7011b437292556a923750fd91bca799",
            "title": "Space and Time Efficient Kernel Density Estimation in High Dimensions",
            "abstract": "Recently, Charikar and Siminelakis (2017) presented a framework for kernel density estimation in provably sublinear query time, for kernels that possess a certain hashing-based property. However, their data structure requires a significantly increased super-linear storage space, as well as super-linear preprocessing time. These limitations inhibit the practical applicability of their approach on large datasets. In this work, we present an improvement to their framework that retains the same query time, while requiring only linear space and linear preprocessing time. We instantiate our framework with the Laplacian and Exponential kernels, two popular kernels which possess the aforementioned property. Our experiments on various datasets verify that our approach attains accuracy and query time similar to Charikar and Siminelakis (2017), with significantly improved space and preprocessing time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "85b0473f5147fa9ba3b05e3fa40dc2bf2c26062d",
            "title": "Learning-Based Low-Rank Approximations",
            "abstract": "We introduce a \u201clearning-based\u201d algorithm for the low-rank decomposition problem: given an $n \\times d$ matrix $A$, and a parameter $k$, compute a rank-$k$ matrix $A'$ that minimizes the approximation loss $\\|A-A'\\|_F$. The algorithm uses a training set of input matrices in order to optimize its performance. Specifically, some of the most efficient approximate algorithms for computing low-rank approximations proceed by computing a projection $SA$, where $S$ is a sparse random $m \\times n$ \u201csketching matrix\u201d, and then performing the singular value decomposition of $SA$. We show how to replace the random matrix $S$ with a \u201clearned\u201d matrix of the same sparsity to reduce the error. Our experiments show that, for multiple types of data sets, a learned sketch matrix can substantially reduce the approximation loss compared to a random matrix $S$, sometimes up to one order of magnitude. We also study mixed matrices where only some of the rows are trained and the remaining ones are random, and show that matrices still offer improved performance while retaining worst-case guarantees. Finally, to understand the theoretical aspects of our approach, we study the special case of $m=1$. In particular, we give an approximation algorithm for minimizing the empirical loss, with approximation factor depending on the stable rank of matrices in the training set. We also show generalization bounds for the sketch matrix learning problem.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2115885855",
                    "name": "A. Vakilian"
                },
                {
                    "authorId": "145155436",
                    "name": "Yang Yuan"
                }
            ]
        },
        {
            "paperId": "8e57942fc2fbfdcf643fc21c927dd9ca7fd181e0",
            "title": "Sample-Optimal Low-Rank Approximation of Distance Matrices",
            "abstract": "A distance matrix $A \\in \\mathbb R^{n \\times m}$ represents all pairwise distances, $A_{ij}=\\mathrm{d}(x_i,y_j)$, between two point sets $x_1,...,x_n$ and $y_1,...,y_m$ in an arbitrary metric space $(\\mathcal Z, \\mathrm{d})$. Such matrices arise in various computational contexts such as learning image manifolds, handwriting recognition, and multi-dimensional unfolding. \nIn this work we study algorithms for low-rank approximation of distance matrices. Recent work by Bakshi and Woodruff (NeurIPS 2018) showed it is possible to compute a rank-$k$ approximation of a distance matrix in time $O((n+m)^{1+\\gamma}) \\cdot \\mathrm{poly}(k,1/\\epsilon)$, where $\\epsilon>0$ is an error parameter and $\\gamma>0$ is an arbitrarily small constant. Notably, their bound is sublinear in the matrix size, which is unachievable for general matrices. \nWe present an algorithm that is both simpler and more efficient. It reads only $O((n+m) k/\\epsilon)$ entries of the input matrix, and has a running time of $O(n+m) \\cdot \\mathrm{poly}(k,1/\\epsilon)$. We complement the sample complexity of our algorithm with a matching lower bound on the number of entries that must be read by any algorithm. We provide experimental results to validate the approximation quality and running time of our algorithm.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2347383",
                    "name": "A. Vakilian"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "a0935dcedc9d55702829b7cfa6b4652731d84c13",
            "title": "Estimating Entropy of Distributions in Constant Space",
            "abstract": "We consider the task of estimating the entropy of $k$-ary distributions from samples in the streaming model, where space is limited. Our main contribution is an algorithm that requires $O\\left(\\frac{k \\log (1/\\varepsilon)^2}{\\varepsilon^3}\\right)$ samples and a constant $O(1)$ memory words of space and outputs a $\\pm\\varepsilon$ estimate of $H(p)$. Without space limitations, the sample complexity has been established as $S(k,\\varepsilon)=\\Theta\\left(\\frac k{\\varepsilon\\log k}+\\frac{\\log^2 k}{\\varepsilon^2}\\right)$, which is sub-linear in the domain size $k$, and the current algorithms that achieve optimal sample complexity also require nearly-linear space in $k$. Our algorithm partitions $[0,1]$ into intervals and estimates the entropy contribution of probability values in each interval. The intervals are designed to trade bias and variance. Distribution property estimation and testing with limited memory is a largely unexplored research area. We hope our work will motivate research in this field.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2595479",
                    "name": "Jayadev Acharya"
                },
                {
                    "authorId": "12163241",
                    "name": "Sourbh Bhadane"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "8908922",
                    "name": "Ziteng Sun"
                }
            ]
        },
        {
            "paperId": "a377aa14e7f7f6c432edc0d8bdd695fb338f3fe2",
            "title": "Scalable Fair Clustering",
            "abstract": "We study the fair variant of the classic $k$-median problem introduced by Chierichetti et al. [2017]. In the standard $k$-median problem, given an input pointset $P$, the goal is to find $k$ centers $C$ and assign each input point to one of the centers in $C$ such that the average distance of points to their cluster center is minimized. \nIn the fair variant of $k$-median, the points are colored, and the goal is to minimize the same average distance objective while ensuring that all clusters have an \"approximately equal\" number of points of each color. \nChierichetti et al. proposed a two-phase algorithm for fair $k$-clustering. In the first step, the pointset is partitioned into subsets called fairlets that satisfy the fairness requirement and approximately preserve the $k$-median objective. In the second step, fairlets are merged into $k$ clusters by one of the existing $k$-median algorithms. The running time of this algorithm is dominated by the first step, which takes super-quadratic time. \nIn this paper, we present a practical approximate fairlet decomposition algorithm that runs in nearly linear time. Our algorithm additionally allows for finer control over the balance of resulting clusters than the original work. We complement our theoretical bounds with empirical evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1730859",
                    "name": "Krzysztof Onak"
                },
                {
                    "authorId": "1771729",
                    "name": "B. Schieber"
                },
                {
                    "authorId": "2347383",
                    "name": "A. Vakilian"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "bf2c9e04ad1963c38f9c4bf84efb6df77093bd81",
            "title": "Learning Sublinear-Time Indexing for Nearest Neighbor Search",
            "abstract": "Most of the efficient sublinear-time indexing algorithms for the high-dimensional nearest neighbor search problem (NNS) are based on space partitions of the ambient space $\\mathbb{R}^d$. Inspired by recent theoretical work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn, Waingarten STOC 2018, FOCS 2018], we develop a new framework for constructing such partitions that reduces the problem to balanced graph partitioning followed by supervised classification. We instantiate this general approach with the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS, our experiments show that the partitions found by Neural LSH consistently outperform partitions found by quantization- and tree-based methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145595795",
                    "name": "Yihe Dong"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2111443",
                    "name": "Ilya P. Razenshteyn"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "c29a7919c85a606b6da82f24cb3818ae5f6b4fd6",
            "title": "(Learned) Frequency Estimation Algorithms under Zipfian Distribution",
            "abstract": "The frequencies of the elements in a data stream are an important statistical measure and the task of estimating them arises in many applications within data analysis and machine learning. Two of the most popular algorithms for this problem, Count-Min and Count-Sketch, are widely used in practice. \nIn a recent work [Hsu et al., ICLR'19], it was shown empirically that augmenting Count-Min and Count-Sketch with a machine learning algorithm leads to a significant reduction of the estimation error. The experiments were complemented with an analysis of the expected error incurred by Count-Min (both the standard and the augmented version) when the input frequencies follow a Zipfian distribution. Although the authors established that the learned version of Count-Min has lower estimation error than its standard counterpart, their analysis of the standard Count-Min algorithm was not tight. Moreover, they provided no similar analysis for Count-Sketch. \nIn this paper we resolve these problems. First, we provide a simple tight analysis of the expected error incurred by Count-Min. Second, we provide the first error bounds for both the standard and the augmented version of Count-Sketch. These bounds are nearly tight and again demonstrate an improved performance of the learned version of Count-Sketch. \nIn addition to demonstrating tight gaps between the aforementioned algorithms, we believe that our bounds for the standard versions of Count-Min and Count-Sketch are of independent interest. In particular, it is a typical practice to set the number of hash functions in those algorithms to $\\Theta (\\log n)$. In contrast, our results show that to minimise the \\emph{expected} error, the number of hash functions should be a constant, strictly greater than $1$.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "23998886",
                    "name": "Anders Aamand"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2115885855",
                    "name": "A. Vakilian"
                }
            ]
        },
        {
            "paperId": "cf6f6b96f139995b79c30ef06079fbd0e29b47d3",
            "title": "Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm",
            "abstract": "``Composable core-sets'' are an efficient framework for solving optimization problems in massive data models. In this work, we consider efficient construction of composable core-sets for the determinant maximization problem. This can also be cast as the MAP inference task for determinantal point processes, that have recently gained a lot of interest for modeling diversity and fairness. The problem was recently studied in [IMOR'18], where they designed composable core-sets with the optimal approximation bound of $\\tilde O(k)^k$. On the other hand, the more practical Greedy algorithm has been previously used in similar contexts. In this work, first we provide a theoretical approximation guarantee of $O(C^{k^2})$ for the Greedy algorithm in the context of composable core-sets; Further, we propose to use a Local Search based algorithm that while being still practical, achieves a nearly optimal approximation bound of $O(k)^{2k}$; Finally, we implement all three algorithms and show the effectiveness of our proposed algorithm on standard data sets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1681474",
                    "name": "S. Gharan"
                },
                {
                    "authorId": "2059329143",
                    "name": "A. Rezaei"
                }
            ]
        },
        {
            "paperId": "d2526933a4aa1a46818e8efbda95b5b560b27456",
            "title": "Scalable Nearest Neighbor Search for Optimal Transport",
            "abstract": "The Optimal Transport (a.k.a. Wasserstein) distance is an increasingly popular similarity measure for rich data domains, such as images or text documents. This raises the necessity for fast nearest neighbor search with respect to this distance, a problem that poses a substantial computational bottleneck for various tasks on massive datasets. \nIn this work, we study fast tree-based approximation algorithms for searching nearest neighbors w.r.t. the Wasserstein-1 distance. A standard tree-based technique, known as Quadtree, has been previously shown to obtain good results. We introduce a variant of this algorithm, called Flowtree, and formally prove it achieves asymptotically better accuracy. Our extensive experiments, on real-world text and image datasets, show that Flowtree improves over various baselines and existing methods in either running time or accuracy. In particular, its quality of approximation is in line with previous high-accuracy methods, while its running time is much faster.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145595795",
                    "name": "Yihe Dong"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2111443",
                    "name": "Ilya P. Razenshteyn"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "ea445b416704572a6e8ac622ab2aa346a32fb636",
            "title": "Tight Trade-offs for the Maximum k-Coverage Problem in the General Streaming Model",
            "abstract": "We study the maximum k-coverage problem in the general edge-arrival streaming model: given a collection of m sets F, each subset of a ground set of elements U of size n, the task is to find k sets whose coverage is maximized. The sets are specified as a sequence of (element, set) pairs in an arbitrary order. Our main result is a tight (up to polylogarithmic factors) trade-off between the space complexity and the approximation factor \u03b1\\in(1/(1-1/e), \\tildeOmega (\\sqrtm )]$ of any single-pass streaming algorithm that estimates the maximum coverage size. Specifically, we show that the optimal space bound is $\\tildeTheta (m/\u03b1^2)$. Moreover, we design a single-pass algorithm that reports an \u03b1-approximate solution in $\\tildeO (m/\u03b1^2 + k)$ space. Our algorithm heavily exploits data stream sketching techniques, which could lead to further connections between vector sketching methods and streaming algorithms for combinatorial optimization tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2347383",
                    "name": "A. Vakilian"
                }
            ]
        },
        {
            "paperId": "2151dc91d6373c55aad002b36ed7634b9a535650",
            "title": "Approximate Nearest Neighbors in Limited Space",
            "abstract": "We consider the $(1+\\epsilon)$-approximate nearest neighbor search problem: given a set $X$ of $n$ points in a $d$-dimensional space, build a data structure that, given any query point $y$, finds a point $x \\in X$ whose distance to $y$ is at most $(1+\\epsilon) \\min_{x \\in X} \\|x-y\\|$ for an accuracy parameter $\\epsilon \\in (0,1)$. Our main result is a data structure that occupies only $O(\\epsilon^{-2} n \\log(n) \\log(1/\\epsilon))$ bits of space, assuming all point coordinates are integers in the range $\\{-n^{O(1)} \\ldots n^{O(1)}\\}$, i.e., the coordinates have $O(\\log n)$ bits of precision. This improves over the best previously known space bound of $O(\\epsilon^{-2} n \\log(n)^2)$, obtained via the randomized dimensionality reduction method of Johnson and Lindenstrauss (1984). We also consider the more general problem of estimating all distances from a collection of query points to all data points $X$, and provide almost tight upper and lower bounds for the space complexity of this problem.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "70792aa3bb6fb98494d7fdfbb39387966b145546",
            "title": "Efficient Density Evaluation for Smooth Kernels",
            "abstract": "Given a kernel function k(.,.) and a dataset P\u2282 R^d, the kernel density function of P at a point x\u03b5 R^d is equal to KDF_P(x):= 1/|P| \u03a3_y\u03b5P k(x, y). Kernel density evaluation has numerous applications, in scientific computing, statistics, computer vision, machine learning and other fields. In all of them it is necessary to evaluate KDF_P(x) quickly, often for many inputs x and large point-sets P. In this paper we present a collection of algorithms for efficient KDF evaluation under the assumptions that the kernel k is \"smooth\", i.e. the value changes at most polynomially with the distance. This assumption is satisfied by several well-studied kernels, including the (generalized) t-student kernel and rational quadratic kernel. For smooth kernels, we give a data structure that, after O(dn log (\u03a6 n)/\u03b5^2) preprocessing, estimates KDF_P(x) up to a factor of 1 \u00b1 \u03b5 in O(dlog (\u03a6 n)/\u03b5^2) time, where Phi; is the aspect ratio. The log(\u03a6n) term can be further replaced by log n under an additional decay condition on k, which is satisfied by the aforementioned examples. We further extend the results in two ways. First, we use low-distortion embeddings to extend the results to kernels defined for spaces other than \u2113_2. The key feature of this reduction is that the distortion of the embedding affects only the running time of the algorithm, not the accuracy of the estimation. As a result, we obtain (1+\u03b5)-approximate estimation algorithms for kernels over other \u2113_p norms, Earth-Mover Distance, and other metric spaces. Second, for smooth kernels that are decreasing with distance, we present a general reduction from density estimation to approximate near neighbor in the underlying space. This allows us to construct algorithms for general doubling metrics, as well as alternative algorithms for l_p norms and other spaces.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1745732",
                    "name": "M. Charikar"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2188929",
                    "name": "Paris Siminelakis"
                }
            ]
        },
        {
            "paperId": "72c969a5dc5b236d511fbdaae88c443d14145ae8",
            "title": "Learning-Based Frequency Estimation Algorithms",
            "abstract": "Estimating the frequencies of elements in a data stream is a fundamental task in data analysis and machine learning. The problem is typically addressed using streaming algorithms which can process very large data using limited storage. Today\u2019s streaming algorithms, however, cannot exploit patterns in their input to improve performance. We propose a new class of algorithms that automatically learn relevant patterns in the input data and use them to improve its frequency estimates. The proposed algorithms combine the benefits of machine learning with the formal guarantees available through algorithm theory. We prove that our learning-based algorithms have lower estimation errors than their non-learning counterparts. We also evaluate our algorithms on two real-world datasets and demonstrate empirically their performance gains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2502450",
                    "name": "Chen-Yu Hsu"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "2347383",
                    "name": "A. Vakilian"
                }
            ]
        },
        {
            "paperId": "79359c0962374d35252566c2dcd6f833788121d5",
            "title": "Set Cover in Sub-linear Time",
            "abstract": "We study the classic set cover problem from the perspective of sub-linear algorithms. Given access to a collection of m sets over n elements in the query model, we show that sub-linear algorithms derived from existing techniques have almost tight query complexities. On one hand, first we show an adaptation of the streaming algorithm presented in [17] to the sub-linear query model, that returns an \u03b1-approximate cover using O(m(n/k)1/(\u03b1\u22121) + nk) queries to the input, where k denotes the value of a minimum set cover. We then complement this upper bound by proving that for lower values of k, the required number of queries is [EQUATION], even for estimating the optimal cover size. Moreover, we prove that even checking whether a given collection of sets covers all the elements would require \u03a9(nk) queries. These two lower bounds provide strong evidence that the upper bound is almost tight for certain values of the parameter k. On the other hand, we show that this bound is not optimal for larger values of the parameter k, as there exists a (1 + e)-approximation algorithm with O(mn/ke2) queries. We show that this bound is essentially tight for sufficiently small constant \u03f5, by establishing a lower bound of [EQUATION] query complexity. Our lower-bound results follow by carefully designing two distributions of instances that are hard to distinguish. In particular, our first lower bound involves a probabilistic construction of a certain set system with a minimum set cover of size \u03b1k, with the key property that a small number of \"almost uniformly distributed\" modifications can reduce the minimum set cover size down to k. Thus, these modifications are not detectable unless a large number of queries are asked. We believe that our probabilistic construction technique might find applications to lower bounds for other combinatorial optimization problems.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                },
                {
                    "authorId": "2347383",
                    "name": "A. Vakilian"
                },
                {
                    "authorId": "2288505",
                    "name": "Anak Yodpinyanee"
                }
            ]
        },
        {
            "paperId": "8cce277151eccf5e40efb451d0cc29df1ff72745",
            "title": "Composable Core-sets for Determinant Maximization Problems via Spectral Spanners",
            "abstract": "We study a spectral generalization of classical combinatorial graph spanners to the spectral setting. Given a set of vectors $V\\subseteq \\Re^d$, we say a set $U\\subseteq V$ is an $\\alpha$-spectral spanner if for all $v\\in V$ there is a probability distribution $\\mu_v$ supported on $U$ such that $$vv^\\intercal \\preceq \\alpha\\cdot\\mathbb{E}_{u\\sim\\mu_v} uu^\\intercal.$$ We show that any set $V$ has an $\\tilde{O}(d)$-spectral spanner of size $\\tilde{O}(d)$ and this bound is almost optimal in the worst case. \nWe use spectral spanners to study composable core-sets for spectral problems. We show that for many objective functions one can use a spectral spanner, independent of the underlying functions, as a core-set and obtain almost optimal composable core-sets. For example, for the determinant maximization problem we obtain an $\\tilde{O}(k)^k$-composable core-set and we show that this is almost optimal in the worst case. \nOur algorithm is a spectral analogue of the classical greedy algorithm for finding (combinatorial) spanners in graphs. We expect that our spanners find many other applications in distributed or parallel models of computation. Our proof is spectral. As a side result of our techniques, we show that the rank of diagonally dominant lower-triangular matrices are robust under `small perturbations' which could be of independent interests.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "1681474",
                    "name": "S. Gharan"
                },
                {
                    "authorId": "2059329143",
                    "name": "A. Rezaei"
                }
            ]
        },
        {
            "paperId": "c6effafab3ff2f9dd6f7bb21abda3a4f7faeafc0",
            "title": "Approximate Nearest Neighbor Search in High Dimensions",
            "abstract": "The nearest neighbor problem is defined as follows: Given a set $P$ of $n$ points in some metric space $(X,D)$, build a data structure that, given any point $q$, returns a point in $P$ that is closest to $q$ (its \"nearest neighbor\" in $P$). The data structure stores additional information about the set $P$, which is then used to find the nearest neighbor without computing all distances between $q$ and $P$. The problem has a wide range of applications in machine learning, computer vision, databases and other fields. \nTo reduce the time needed to find nearest neighbors and the amount of memory used by the data structure, one can formulate the {\\em approximate} nearest neighbor problem, where the the goal is to return any point $p' \\in P$ such that the distance from $q$ to $p'$ is at most $c \\cdot \\min_{p \\in P} D(q,p)$, for some $c \\geq 1$. Over the last two decades, many efficient solutions to this problem were developed. In this article we survey these developments, as well as their connections to questions in geometric functional analysis and combinatorial geometry.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2111443",
                    "name": "Ilya P. Razenshteyn"
                }
            ]
        },
        {
            "paperId": "ec514be83e3dd374f4875a822b16108e50181af8",
            "title": "Fast millimeter wave beam alignment",
            "abstract": "There is much interest in integrating millimeter wave radios (mmWave) into wireless LANs and 5G cellular networks to benefit from their multi-GHz of available spectrum. Yet, unlike existing technologies, e.g., WiFi, mmWave radios require highly directional antennas. Since the antennas have pencil-beams, the transmitter and receiver need to align their beams before they can communicate. Existing systems scan the space to find the best alignment. Such a process has been shown to introduce up to seconds of delay, and is unsuitable for wireless networks where an access point has to quickly switch between users and accommodate mobile clients. This paper presents Agile-Link, a new protocol that can find the best mmWave beam alignment without scanning the space. Given all possible directions for setting the antenna beam, Agile-Link provably finds the optimal direction in logarithmic number of measurements. Further, Agile-Link works within the existing 802.11ad standard for mmWave LAN, and can support both clients and access points. We have implemented Agile-Link in a mmWave radio and evaluated it empirically. Our results show that it reduces beam alignment delay by orders of magnitude. In particular, for highly directional mmWave devices operating under 802.11ad, the delay drops from over a second to 2.5 ms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2349340",
                    "name": "Haitham Hassanieh"
                },
                {
                    "authorId": "1404356765",
                    "name": "Omid Salehi-Abari"
                },
                {
                    "authorId": "116347235",
                    "name": "Michael Rodriguez"
                },
                {
                    "authorId": "32490628",
                    "name": "M. Abdelghany"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "2572d99138a9d13144d855d85fcf728fd21cf60c",
            "title": "Beyond P vs. NP: Quadratic-Time Hardness for Big Data Problems",
            "abstract": "The theory of NP-hardness has been very successful in identifying problems that are unlikely to be solvable in polynomial time. However, many other important problems do have polynomial time algorithms, but large exponents in their time bounds can make them run for days, weeks or more. For example, quadratic time algorithms, although practical on moderately sized inputs, can become inefficient on big data problems that involve gigabytes or more of data. Although for many problems no sub-quadratic time algorithms are known, any evidence of quadratic-time hardness has remained elusive. In this talk I will give an overview of recent research that aims to remedy this situation. In particular, I will describe hardness results for problems in string processing (e.g., edit distance computation or regular expression matching) and machine learning (e.g., Support Vector Machines or gradient computation in neural networks). All of them have polynomial time algorithms, but despite extensive amount of research, no near-linear time algorithms have been found for many variants of these problems. I will show that, under a natural complexity-theoretic conjecture, such algorithms do not exist. I will also describe how this framework has led to the development of new algorithms.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "409360b14d9473d3bdbc21c98fabec731887f381",
            "title": "Fractional Set Cover in the Streaming Model",
            "abstract": "We study the Fractional Set Cover problem in the streaming model. That is, we consider the relaxation of the set cover problem over a universe of n elements and a collection of m sets, where each set can be picked fractionally, with a value in [0,1]. We present a randomized (1+a)-approximation algorithm that makes p passes over the data, and uses O(polylog(m,n,1/a) (mn^(O(1/(pa)))+n)) memory space. The algorithm works in both the set arrival and the edge arrival models. To the best of our knowledge, this is the first streaming result for the fractional set cover problem. We obtain our results by employing the multiplicative weights update framework in the streaming settings.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                },
                {
                    "authorId": "5171324",
                    "name": "Jonathan Ullman"
                },
                {
                    "authorId": "2347383",
                    "name": "A. Vakilian"
                },
                {
                    "authorId": "2288505",
                    "name": "Anak Yodpinyanee"
                }
            ]
        },
        {
            "paperId": "45d849c6057660d801968d695a3bf518eacd58d6",
            "title": "On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks",
            "abstract": "Empirical risk minimization (ERM) is ubiquitous in machine learning and underlies most supervised learning methods. While there is a large body of work on algorithms for various ERM problems, the exact computational complexity of ERM is still not understood. We address this issue for multiple popular ERM problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on complexity-theoretic assumptions such as the Strong Exponential Time Hypothesis. Under these assumptions, we show that there are no algorithms that solve the aforementioned ERM problems to high accuracy in sub-quadratic time. We also give similar hardness results for computing the gradient of the empirical loss, which is the main computational burden in many non-convex learning tasks.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "6fb3d2825c01e0fc79adfa8f7560a3c9761b705f",
            "title": "Agile Millimeter Wave Networks with Provable Guarantees",
            "abstract": "There is much interest in integrating millimeter wave radios (mmWave) into wireless LANs and 5G cellular networks to benefit from their multiple GHz of available spectrum. Yet unlike existing technologies, e.g., WiFi, mmWave radios require highly directional antennas. Since the antennas have pencil-beams, the transmitter and receiver need to align their antenna beams before they can communicate. Existing solutions scan the entire space to find the best alignment. Such a process has been shown to introduce up to seconds of delay, and is unsuitable for wireless networks where an access point has to quickly switch between users and accommodate mobile clients. \nThis paper presents Rapid-Link, a new protocol that can find the best mmWave beam alignment without scanning the space. Given all possible directions for setting the antenna beam, Rapid-Link provably finds the optimal direction in logarithmic number of measurements. Further, Rapid-Link works within the existing 802.11ad standard for mmWave LAN, and can support both clients and access points. We have implemented Rapid-Link in a mmWave radio and evaluated it empirically. Our results show that it reduces beam alignment delay by orders of magnitude. In particular, for highly directional mmWave devices operating under 802.11ad, the delay drops from over a second to 2.5 ms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2349340",
                    "name": "Haitham Hassanieh"
                },
                {
                    "authorId": "1404356765",
                    "name": "Omid Salehi-Abari"
                },
                {
                    "authorId": "1411517835",
                    "name": "Michael Rodreguez"
                },
                {
                    "authorId": "32490628",
                    "name": "M. Abdelghany"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "817d465803c8afd008463801ecbc0c2fc0088d02",
            "title": "Better Approximations for Tree Sparsity in Nearly-Linear Time",
            "abstract": "The Tree Sparsity problem is defined as follows: given a node-weighted tree of size n and an integer k, output a rooted subtree of size k with maximum weight. The best known algorithm solves this problem in time O(kn), i.e., quadratic in the size of the input tree for k = \u0398(n). In this work, we design (1+e)-approximation algorithms for the Tree Sparsity problem that run in nearly-linear time. Unlike prior algorithms for this problem, our results offer single criterion approximations, i.e., they do not increase the sparsity of the output solution, and work for arbitrary trees (not only balanced trees). We also provide further algorithms for this problem with different runtime vs approximation trade-offs. Finally, we show that if the exact version of the Tree Sparsity problem can be solved in strongly subquadratic time, then the (min, +) convolution problem can be solved in strongly subquadratic time as well. The latter is a well- studied problem for which no strongly subquadratic time algorithm is known.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "e06144e6e7057baf779e3ded134aa12f9c64a6a3",
            "title": "Practical Data-Dependent Metric Compression with Provable Guarantees",
            "abstract": "We introduce a new distance-preserving compact representation of multi-dimensional point-sets. Given $n$ points in a $d$-dimensional space where each coordinate is represented using $B$ bits (i.e., $dB$ bits per point), it produces a representation of size $O( d \\log(d B/\\epsilon) + \\log n)$ bits per point from which one can approximate the distances up to a factor of $1 \\pm \\epsilon$. Our algorithm almost matches the recent bound of~\\cite{indyk2017near} while being much simpler. We compare our algorithm to Product Quantization (PQ)~\\cite{jegou2011product}, a state of the art heuristic metric compression method. We evaluate both algorithms on several data sets: SIFT (used in \\cite{jegou2011product}), MNIST~\\cite{lecun1998mnist}, New York City taxi time series~\\cite{guha2016robust} and a synthetic one-dimensional data set embedded in a high-dimensional space. With appropriately tuned parameters, our algorithm produces representations that are comparable to or better than those produced by PQ, while having provable guarantees on its performance.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2111443",
                    "name": "Ilya P. Razenshteyn"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "3ccf40561d5a0886c2166bca29066fd42da32491",
            "title": "Near-Optimal (Euclidean) Metric Compression",
            "abstract": "The metric sketching problem is defined as follows. Given a metric on n points, and \u03f5 > 0, we wish to produce a small size data structure (sketch) that, given any pair of point indices, recovers the distance between the points up to a 1 + \u03f5 distortion. In this paper we consider metrics induced by l2 and l1 norms whose spread (the ratio of the diameter to the closest pair distance) is bounded by \u03a6 > 0. A well-known dimensionality reduction theorem due to Johnson and Lindenstrauss yields a sketch of size O(\u03f5\u22122 log(\u03a6n)n log n), i.e., O(\u03f5\u22122 log(\u03a6n)n log n) bits per point. We show that this bound is not optimal, and can be substantially improved to O(\u03f5\u22122 log(1/\u03f5) \u00b7 log n + log log \u03a6) bits per point. Furthermore, we show that our bound is tight up to a factor of log(1/\u03f5). We also consider sketching of general metrics and provide a sketch of size O(n log(1/\u03f5) + log log \u03a6) bits per point, which we show is optimal.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "39549368",
                    "name": "Tal Wagner"
                }
            ]
        },
        {
            "paperId": "54561184aa3970b4c61fcab2a846694b15dcb2d4",
            "title": "Fast recovery from a union of subspaces",
            "abstract": "We address the problem of recovering a high-dimensional but structured vector from linear observations in a general setting where the vector can come from an arbitrary union of subspaces. This setup includes well-studied problems such as compressive sensing and low-rank matrix recovery. We show how to design more efficient algorithms for the union-of subspace recovery problem by using *approximate* projections. Instantiating our general framework for the low-rank matrix recovery problem gives the fastest provable running time for an algorithm with optimal sample complexity. Moreover, we give fast approximate projections for 2D histograms, another well-studied low-dimensional model of data. We complement our theoretical results with experiments demonstrating that our framework also leads to improved time and sample complexity empirically.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144398138",
                    "name": "C. Hegde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "96785aec1fb97f697e7a0ea09e8f09144c5ac02b",
            "title": "Simultaneous Nearest Neighbor Search",
            "abstract": "Motivated by applications in computer vision and databases, we introduce and study the Simultaneous Nearest Neighbor Search (SNN) problem. Given a set of data points, the goal of SNN is to design a data structure that, given a collection of queries, finds a collection of close points that are compatible with each other. Formally, we are given $k$ query points $Q=q_1,\\cdots,q_k$, and a compatibility graph $G$ with vertices in $Q$, and the goal is to return data points $p_1,\\cdots,p_k$ that minimize (i) the weighted sum of the distances from $q_i$ to $p_i$ and (ii) the weighted sum, over all edges $(i,j)$ in the compatibility graph $G$, of the distances between $p_i$ and $p_j$. The problem has several applications, where one wants to return a set of consistent answers to multiple related queries. This generalizes well-studied computational problems, including NN, Aggregate NN and the 0-extension problem. \nIn this paper we propose and analyze the following general two-step method for designing efficient data structures for SNN. In the first step, for each query point $q_i$ we find its (approximate) nearest neighbor point $\\hat{p}_i$; this can be done efficiently using existing approximate nearest neighbor structures. In the second step, we solve an off-line optimization problem over sets $q_1,\\cdots,q_k$ and $\\hat{p}_1,\\cdots,\\hat{p}_k$; this can be done efficiently given that $k$ is much smaller than $n$. Even though $\\hat{p}_1,\\cdots,\\hat{p}_k$ might not constitute the optimal answers to queries $q_1,\\cdots,q_k$, we show that, for the unweighted case, the resulting algorithm is $O(\\log k/\\log \\log k)$-approximation. Also, we show that the approximation factor can be in fact reduced to a constant for compatibility graphs frequently occurring in practice. \nFinally, we show that the \"empirical approximation factor\" provided by the above approach is very close to 1.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2633757",
                    "name": "Robert D. Kleinberg"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "145155436",
                    "name": "Yang Yuan"
                }
            ]
        },
        {
            "paperId": "a44ecd51208add8895d2fe10022a1d17fb5ce593",
            "title": "ICALP 2017 - Call for Papers",
            "abstract": "The 44th International Colloquium on Automata, Languages, and Programming (ICALP) will take place in Warsaw, Poland, on 10-14 July 2017. ICALP is the main conference and annual meeting of the European Association for Theoretical Computer Science (EATCS). As usual, ICALP will be preceded by a series of workshops, which will take place on July 10.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1683918",
                    "name": "A. Muscholl"
                },
                {
                    "authorId": "35141982",
                    "name": "F. Kuhn"
                }
            ]
        },
        {
            "paperId": "ccedc6d206d54ef96e8f02c2d135edd8c39df3ca",
            "title": "Approximate Sparse Linear Regression",
            "abstract": "In the Sparse Linear Regression (SLR) problem, given a $d \\times n$ matrix $M$ and a $d$-dimensional query $q$, the goal is to compute a $k$-sparse $n$-dimensional vector $\\tau$ such that the error $||M \\tau-q||$ is minimized. This problem is equivalent to the following geometric problem: given a set $P$ of $n$ points and a query point $q$ in $d$ dimensions, find the closest $k$-dimensional subspace to $q$, that is spanned by a subset of $k$ points in $P$. In this paper, we present data-structures/algorithms and conditional lower bounds for several variants of this problem (such as finding the closest induced $k$ dimensional flat/simplex instead of a subspace). \nIn particular, we present approximation algorithms for the online variants of the above problems with query time $\\tilde O(n^{k-1})$, which are of interest in the \"low sparsity regime\" where $k$ is small, e.g., $2$ or $3$. For $k=d$, this matches, up to polylogarithmic factors, the lower bound that relies on the affinely degenerate conjecture (i.e., deciding if $n$ points in $\\mathbb{R}^d$ contains $d+1$ points contained in a hyperplane takes $\\Omega(n^d)$ time). Moreover, our algorithms involve formulating and solving several geometric subproblems, which we believe to be of independent interest.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1388677326",
                    "name": "Sariel Har-Peled"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                }
            ]
        },
        {
            "paperId": "361fd62d94196a833d07e10a38de348674ec9563",
            "title": "Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA, USA, January 4-6, 2015",
            "abstract": "This symposium focuses on research topics related to efficient algorithms and data structures for discrete problems. In addition to the design of such methods and structures, the scope also includes their use, performance analysis, and the mathematical problems related to their development or limitations. Performance analyses may be analytical or experimental and may address worst-case or expected-case performance. Studies can be theoretical or based on data sets that have arisen in practice and may address methodological issues involved in performance analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "50645e3dc912d597e89d59bffb96ccc0f8e1aefa",
            "title": "Practical and Optimal LSH for Angular Distance",
            "abstract": "We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets. \n \nWe complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1728281",
                    "name": "Thijs Laarhoven"
                },
                {
                    "authorId": "2111443",
                    "name": "Ilya P. Razenshteyn"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "7c1884e25a6f88cf9f86cc34f4ac25497a54d018",
            "title": "Nearly-optimal bounds for sparse recovery in generic norms, with applications to k-median sketching",
            "abstract": "We initiate the study of trade-offs between sparsity and the number of measurements in sparse recovery schemes for generic norms. Specifically, for a norm $\\|\\cdot\\|$, sparsity parameter $k$, approximation factor $K>0$, and probability of failure $P>0$, we ask: what is the minimal value of $m$ so that there is a distribution over $m \\times n$ matrices $A$ with the property that for any $x$, given $Ax$, we can recover a $k$-sparse approximation to $x$ in the given norm with probability at least $1-P$? We give a partial answer to this problem, by showing that for norms that admit efficient linear sketches, the optimal number of measurements $m$ is closely related to the doubling dimension of the metric induced by the norm $\\|\\cdot\\|$ on the set of all $k$-sparse vectors. By applying our result to specific norms, we cast known measurement bounds in our general framework (for the $\\ell_p$ norms, $p \\in [1,2]$) as well as provide new, measurement-efficient schemes (for the Earth-Mover Distance norm). The latter result directly implies more succinct linear sketches for the well-studied planar $k$-median clustering problem. Finally, our lower bound for the doubling dimension of the EMD norm enables us to address the open question of [Frahling-Sohler, STOC'05] about the space complexity of clustering problems in the dynamic streaming model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2111443",
                    "name": "Ilya P. Razenshteyn"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "afc645b27eb9503844fe86745aff20c230dafe09",
            "title": "Seismic feature extraction using steiner tree methods",
            "abstract": "Identifying \u201cinteresting\u201d features, such as faults, unconformities, and other events in subsurface images is a challenging task in seismic data processing. Existing state-of-the-art methods usually involve manual intervention in the form of a visual inspection by an expert, but this is time-consuming, expensive, and error-prone. In this paper, we propose an efficient, automatic approach for seismic feature extraction. The core idea of our approach involves interpreting a given 2D seismic image as a function defined over the vertices of a specially chosen underlying graph. This enables us to formulate the feature extraction task as an instance of the Prize-Collecting Steiner Tree problem encountered in combinatorial optimization. We develop an efficient algorithm to solve this problem, and demonstrate the utility of our method on a number of synthetic and real examples.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                },
                {
                    "authorId": "144398138",
                    "name": "C. Hegde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1767317",
                    "name": "Ligang Lu"
                },
                {
                    "authorId": "94028236",
                    "name": "Xingang Chi"
                },
                {
                    "authorId": "39443697",
                    "name": "D. Hohl"
                }
            ]
        },
        {
            "paperId": "bdcf456dd03bc7b2b444c8c7be6479146f18df6c",
            "title": "Erratum for: Approximating and Testing k-Histogram Distributions in Sub-linear Time",
            "abstract": "This is an erratum for our PODS 2012 paper \u201cApproximating and Testing k-Histogram Distributions in Sub-linear Time\u201d [ILR12]. We made a mistake in the final accounting of the running time in Theorem 2 in Subsection 3.1. The running time is \u00d5((k/\u03b5) ln n) and not \u00d5((k/\u03b5) lnn) as stated in the theorem. As noted in the proof of the theorem we decrease the number of iterations in Step (7) from ( n 2 )",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "35647859",
                    "name": "Reut Levi"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                }
            ]
        },
        {
            "paperId": "bdd2d814d9f918fa0a96d961cd6b34edc6f0f1ea",
            "title": "Which Regular Expression Patterns Are Hard to Match?",
            "abstract": "Regular expressions constitute a fundamental notion in formal language theory and are frequently used in computer science to define search patterns. In particular, regular expression matching and membership testing are widely used computational primitives, employed in many programming languages and text processing utilities. A classic algorithm for these problems constructs and simulates a non-deterministic finite automaton corresponding to the expression, resulting in an O(m n) running time (where m is the length of the pattern and n is the length of the text). This running time can be improved slightly (by a polylogarithmic factor), but no significantly faster solutions are known. At the same time, much faster algorithms exist for various special cases of regular expressions, including dictionary matching, wildcard matching, subset matching, word break problem etc. In this paper, we show that the complexity of regular expression matching can be characterized based on its depth (when interpreted as a formula). Our results hold for expressions involving concatenation, OR, Kleene star and Kleene plus. For regular expressions of depth two (involving any combination of the above operators), we show the following dichotomy: matching and membership testing can be solved in near-linear time, except for \"concatenations of stars\", which cannot be solved in strongly sub-quadratic time assuming the Strong Exponential Time Hypothesis (SETH). For regular expressions of depth three the picture is more complex. Nevertheless, we show that all problems can either be solved in strongly sub-quadratic time, or cannot be solved in strongly sub-quadratic time assuming SETH. An intriguing special case of membership testing involves regular expressions of the form \"a star of an OR of concatenations\", e.g., [a|ab|bc]*. This corresponds to the so-called word break problem, for which a dynamic programming algorithm with a runtime of (roughly) O(n \u221am) is known. We show that the latter bound is not tight and improve the runtime to O(n m0.44...).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "d5fe9d77eaca5adf4751fcc5c54eaa1f72e84876",
            "title": "Fast Algorithms for Structured Sparsity",
            "abstract": "Sparse representations of signals (i.e., representations that have only few non-zero or large coefficients) have emerged as powerful tools in signal processing theory, algorithms, machine learning and other applications. However, real-world signals often exhibit rich structure beyond mere sparsity. For example, a natural image, once represented in the wavelet domain, often has the property that its large coefficients occupy a subtree of the wavelet hierarchy, as opposed to arbitrary positions. A general approach to capturing this type of additional structure is to model the support of the signal of interest (i.e., the set of indices of large coefficients) as belonging to a particular family of sets. Computing a sparse representation of the signal then corresponds to the problem of finding the support from the family that maximizes the sum of the squares of the selected coefficients. Such a modeling approach has proved to be beneficial in a number of applications including compression, de-noising, compressive sensing and machine learning. However, the resulting optimization problem is often computationally difficult or intractable, which is undesirable in many applications where large signals and datasets are commonplace. In this talk, I will outline some of the past and more recent algorithms for finding structured sparse representations of signals, including piecewise constant approximations, tree-sparse approximations and graph-sparse approximations. The algorithms borrow several techniques from combinatorial optimization (e.g., dynamic programming), graph theory, and approximation algorithms. For many problems the algorithms run in (nearly) linear time, which makes them applicable to very large datasets. Joint work with Chinmay Hegde and Ludwig Schmidt \u2217Machine Learning External Seminar, Gatsby Unit, May 16, 2016.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144398138",
                    "name": "C. Hegde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "e74a1e87a0cbac606a81e9b5f8695d828c07f966",
            "title": "A Nearly-Linear Time Framework for Graph-Structured Sparsity",
            "abstract": "We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, we show that our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments demonstrating that our algorithms also improve on prior work in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144398138",
                    "name": "C. Hegde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "f13c724469899e11b5d08846e32bafc0a44fe23f",
            "title": "Nearly Optimal Deterministic Algorithm for Sparse Walsh-Hadamard Transform",
            "abstract": "For every fixed constant \u03b1 > 0, we design an algorithm for computing the k-sparse Walsh-Hadamard transform (i.e., Discrete Fourier Transform over the Boolean cube) of an N-dimensional vector x \u2208 RN in time k1 + \u03b1(log N)O(1). Specifically, the algorithm is given query access to x and computes a k-sparse x\u02dc \u2208 RN satisfying \u2016 x\u02dc\u2212 x\u02c6\u20161 \u2264 c \u2016 x\u02c6\u2212 Hk(x\u02c6)\u2016\u2016\u2016\u2016\u2016\u2016\u2016\u20161 for an absolute constant c > 0, where x\u02c6 is the transform of x and Hk(x\u02c6) is its best k-sparse approximation. Our algorithm is fully deterministic and only uses nonadaptive queries to x (i.e., all queries are determined and performed in parallel when the algorithm starts). An important technical tool that we use is a construction of nearly optimal and linear lossless condensers, which is a careful instantiation of the GUV condenser (Guruswami et al. [2009]). Moreover, we design a deterministic and nonadaptive \u21131/\u21131 compressed sensing scheme based on general lossless condensers that is equipped with a fast reconstruction algorithm running in time k1 + \u03b1(log N)O(1) (for the GUV-based condenser) and is of independent interest. Our scheme significantly simplifies and improves an earlier expander-based construction due to Berinde, Gilbert, Indyk, Karloff, and Strauss [Berinde et al. 2008]. Our methods use linear lossless condensers in a black box fashion; therefore, any future improvement on explicit constructions of such condensers would immediately translate to improved parameters in our framework (potentially leading to k(log N)O(1) reconstruction time with a reduced exponent in the poly-logarithmic factor, and eliminating the extra parameter \u03b1). By allowing the algorithm to use randomness while still using nonadaptive queries, the runtime of the algorithm can be improved to \u00f5(k log3 N).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1744273",
                    "name": "Mahdi Cheraghchi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "ff1d019415bdcfb465e0e2784593c134ce66c707",
            "title": "Towards Tight Bounds for the Streaming Set Cover Problem",
            "abstract": "We consider the classic Set Cover problem in the data stream model. For n elements and m sets (m \u2265 n) we give a O(1/\u03b4)-pass algorithm with a strongly sub-linear ~O(mn\u03b4) space and logarithmic approximation factor. This yields a significant improvement over the earlier algorithm of Demaine et al. [10] that uses exponentially larger number of passes. We complement this result by showing that the tradeoff between the number of passes and space exhibited by our algorithm is tight, at least when the approximation factor is equal to 1. Specifically, we show that any algorithm that computes set cover exactly using ({1 over 2\u03b4}-1) passes must use ~\u03a9(mn\u03b4) space in the regime of m=O(n). Furthermore, we consider the problem in the geometric setting where the elements are points in R2 and sets are either discs, axis-parallel rectangles, or fat triangles in the plane, and show that our algorithm (with a slight modification) uses the optimal ~O(n) space to find a logarithmic approximation in O(1/\u03b4) passes. Finally, we show that any randomized one-pass algorithm that distinguishes between covers of size 2 and 3 must use a linear (i.e., \u03a9(mn)) amount of space. This is the first result showing that a randomized, approximate algorithm cannot achieve a space bound that is sublinear in the input size. This indicates that using multiple passes might be necessary in order to achieve sub-linear space bounds for this problem while guaranteeing small approximation factors.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "2115885855",
                    "name": "A. Vakilian"
                }
            ]
        },
        {
            "paperId": "039d5c84ff140fd51e86d0fbf060a9d03861fa7c",
            "title": "Better embeddings for planar Earth-Mover Distance over sparse sets",
            "abstract": "We consider the problem of constructing low-distortion embeddings of the Planar Earth-Mover Distance (EMD) into \u2113p spaces. EMD is a popular measure of dis-similarity between sets of points, e.g., bags of geometric features. We present a collection of embeddings with the property that their distortion and/or host-space dimension are parametrized by the size (or the sparsity) of the embedded sets s. Our specific results include: \u2022 An O(log s)-distortion embedding of EMD over s-subsets into \u21131\u2212\u03f5. This is the first embedding of EMD into a \"tractable\" \u2113p space whose distortion is a function of the sparsity, not the size of the ambient space; \u2022 An O(log n)-distortion embedding of EMD into \u21131 with dimension O(s2 log2 n), where the embedded sets are subsets of an n \u00d7 n grid. For low values of s this significantly improves over the best previous dimension bound of O(n2) obtained for general sets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "13f3f1f959586a3ef478a6a5b17182ea141b9bd3",
            "title": "Rapid Sampling for Visualizations with Ordering Guarantees",
            "abstract": "Visualizations are frequently used as a means to understand trends and gather insights from datasets, but often take a long time to generate. In this paper, we focus on the problem of rapidly generating approximate visualizations while preserving crucial visual properties of interest to analysts. Our primary focus will be on sampling algorithms that preserve the visual property of ordering; our techniques will also apply to some other visual properties. For instance, our algorithms can be used to generate an approximate visualization of a bar chart very rapidly, where the comparisons between any two bars are correct. We formally show that our sampling algorithms are generally applicable and provably optimal in theory, in that they do not take more samples than necessary to generate the visualizations with ordering guarantees. They also work well in practice, correctly ordering output groups while taking orders of magnitude fewer samples and much less time than conventional sampling schemes.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080285036",
                    "name": "Albert Kim"
                },
                {
                    "authorId": "3066199",
                    "name": "Eric Blais"
                },
                {
                    "authorId": "145592539",
                    "name": "Aditya G. Parameswaran"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "144478906",
                    "name": "S. Madden"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                }
            ]
        },
        {
            "paperId": "1428d6d7124049c9c92fa6f2394668f6c354aabe",
            "title": "Recent Developments in the Sparse Fourier Transform: A compressed Fourier transform for big data",
            "abstract": "The discrete Fourier transform (DFT) is a fundamental component of numerous computational techniques in signal processing and scientific computing. The most popular means of computing the DFT is the fast Fourier transform (FFT). However, with the emergence of big data problems, in which the size of the processed data sets can easily exceed terabytes, the \"fast\" in FFT is often no longer fast enough. In addition, in many big data applications it is hard to acquire a sufficient amount of data to compute the desired Fourier transform in the first place. The sparse Fourier transform (SFT) addresses the big data setting by computing a compressed Fourier transform using only a subset of the input data, in time smaller than the data set size. The goal of this article is to survey these recent developments, explain the basic techniques with examples and applications in big data, demonstrate tradeoffs in empirical performance of the algorithms, and discuss the connection between the SFT and other techniques for massive data analysis such as streaming algorithms and compressive sensing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1795861",
                    "name": "A. Gilbert"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1743921",
                    "name": "M. Iwen"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "157a9598f54f1a424ae8510953bacbd5a232cc14",
            "title": "(Nearly) Sample-Optimal Sparse Fourier Transform",
            "abstract": "We consider the problem of computing a k-sparse approximation to the discrete Fourier transform of an n-dimensional signal. Our main result is a randomized algorithm that computes such an approximation using O(k log n(log log n)O(1)) signal samples in time O(k log2 n(log log n)O(1)), assuming that the entries of the signal are polynomially bounded. The sampling complexity improves over the recent bound of O(k log n log(n/k)) given in [15], and matches the lower bound of \u03a9(k log(n/k)/log log n) from the same paper up to poly(log log n) factors when k = O(n1-\u03b4) for a constant \u03b4 > 0.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1733218",
                    "name": "M. Kapralov"
                },
                {
                    "authorId": "4989538",
                    "name": "Eric Price"
                }
            ]
        },
        {
            "paperId": "58f0c6f67fb146e8a7d8689b5beee8aa385f9709",
            "title": "Approximation-Tolerant Model-Based Compressive Sensing",
            "abstract": "The goal of sparse recovery is to recover a k-sparse signal x e Rn from (possibly noisy) linear measurements of the form y = Ax, where A e Rmxn describes the measurement process. Standard results in compressive sensing show that it is possible to recover the signal x from m = O(k log(n/k)) measurements, and that this bound is tight. The framework of model-based compressive sensing [BCDH10] overcomes the lower bound and reduces the number of measurements further to O(k) by limiting the supports of x to a subset M of the (nk) possible supports. This has led to many measurement-efficient algorithms for a wide variety of signal models, including block-sparsity and tree-sparsity. \n \nUnfortunately, extending the framework to other, more general models has been stymied by the following obstacle: for the framework to apply, one needs an algorithm that, given a signal x, solves the following optimization problem exactly: \n \n[EQUATION] \n \n(here x[n]\\\u03a9 denotes the projection of x on coordinates not in \u03a9). However, an approximation algorithm for this optimization task is not sufficient. Since many problems of this form are not known to have exact polynomial-time algorithms, this requirement poses an obstacle for extending the framework to a richer class of models. \n \nIn this paper, we remove this obstacle and show how to extend the model-based compressive sensing framework so that it requires only approximate solutions to the aforementioned optimization problems. Interestingly, our extension requires the existence of approximation algorithms for both the maximization and the minimization variants of the optimization problem. \n \nFurther, we apply our framework to the Constrained Earth Mover's Distance (CEMD) model introduced in [SHI13], obtaining a sparse recovery scheme that uses significantly less than O(k log(n/k)) measurements. This is the first non-trivial theoretical bound for this model, since the validation of the approach presented in [SHI13] was purely empirical. The result is obtained by designing a novel approximation algorithm for the maximization version of the problem and proving approximation guarantees for the minimization algorithm described in [SHI13].",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144398138",
                    "name": "C. Hegde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "6657da1a1e435f57ff21440da3b5197cc8767681",
            "title": "Approximation Algorithms for Model-Based Compressive Sensing",
            "abstract": "Compressive sensing (CS) states that a sparse signal can be recovered from a small number of linear measurements, and that this recovery can be performed efficiently in polynomial time. The framework of model-based CS (model-CS) leverages additional structure in the signal and provides new recovery schemes that can reduce the number of measurements even further. This idea has led to measurement-efficient recovery schemes for a variety of signal models. However, for any given model, model-CS requires an algorithm that solves the model-projection problem: given a query signal, report the signal in the model that is closest to the query signal. Often, this optimization can be computationally very expensive. Moreover, an approximation algorithm is not sufficient for this optimization to provably succeed. As a result, the model-projection problem poses a fundamental obstacle for extending model-CS to many interesting classes of models. In this paper, we introduce a new framework that we call approximation-tolerant model-CS. This framework includes a range of algorithms for sparse recovery that require only approximate solutions for the model-projection problem. In essence, our work removes the aforementioned obstacle to model-CS, thereby extending model-CS to a much wider class of signal models. Interestingly, all our algorithms involve both the minimization and the maximization variants of the model-projection problem. We instantiate this new framework for a new signal model that we call the constrained earth mover distance (CEMD) model. This model is particularly useful for signal ensembles, where the positions of the nonzero coefficients do not change significantly as a function of spatial (or temporal) location. We develop novel approximation algorithms for both the maximization and the minimization versions of the model-projection problem via graph optimization techniques. Leveraging these algorithms and our framework results in a nearly sample-optimal sparse recovery scheme for the CEMD model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144398138",
                    "name": "C. Hegde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "c239314d26348d8450cbd4ce95bbea3a10c8375f",
            "title": "Automatic fault localization using the generalized Earth Mover's distance",
            "abstract": "Localizing fault lines and surfaces in seismic subsurface images is a daunting challenge. Existing state-of-the-art approaches usually involve visual interpretation by an expert, but this is time-consuming, expensive and error-prone. In this paper, we propose some initial steps towards a new algorithmic framework for automatic fault localization. The core of our approach is a deterministic model for 2D images that we call the Constrained Generalized Earth Mover's Distance (CGEMD) model. We propose an algorithm that returns the best approximation in the model for any given input 2D image X; the output of this algorithm is then post-processed to reveal the locations of the faults in the image. We demonstrate the validity of this approach on a number of synthetic and real-world examples.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                },
                {
                    "authorId": "144398138",
                    "name": "C. Hegde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "144666206",
                    "name": "J. Kane"
                },
                {
                    "authorId": "1767317",
                    "name": "Ligang Lu"
                },
                {
                    "authorId": "39443697",
                    "name": "D. Hohl"
                }
            ]
        },
        {
            "paperId": "d36ee86a7fb79b18b6324409e1c39ad4fac5058e",
            "title": "Composable core-sets for diversity and coverage maximization",
            "abstract": "In this paper we consider efficient construction of \"composable core-sets\" for basic diversity and coverage maximization problems. A core-set for a point-set in a metric space is a subset of the point-set with the property that an approximate solution to the whole point-set can be obtained given the core-set alone. A composable core-set has the property that for a collection of sets, the approximate solution to the union of the sets in the collection can be obtained given the union of the composable core-sets for the point sets in the collection. Using composable core-sets one can obtain efficient solutions to a wide variety of massive data processing applications, including nearest neighbor search, streaming algorithms and map-reduce computation. Our main results are algorithms for constructing composable core-sets for several notions of \"diversity objective functions\", a topic that attracted a significant amount of research over the last few years. The composable core-sets we construct are small and accurate: their approximation factor almost matches that of the best \"off-line\" algorithms for the relevant optimization problems (up to a constant factor). Moreover, we also show applications of our results to diverse nearest neighbor search, streaming algorithms and map-reduce computation. Finally, we show that for an alternative notion of diversity maximization based on the maximum coverage problem small composable core-sets do not exist.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "145967611",
                    "name": "Mohammad Mahdian"
                },
                {
                    "authorId": "1728881",
                    "name": "V. Mirrokni"
                }
            ]
        },
        {
            "paperId": "eb69f3f98b0dd41a3b8891537f81c9eea07fd922",
            "title": "Edit Distance Cannot Be Computed in Strongly Subquadratic Time (unless SETH is false)",
            "abstract": "The edit distance (a.k.a. the Levenshtein distance) between two strings is defined as the minimum number of insertions, deletions or substitutions of symbols needed to transform one string into another. The problem of computing the edit distance between two strings is a classical computational task, with a well-known algorithm based on dynamic programming. Unfortunately, all known algorithms for this problem run in nearly quadratic time. In this paper we provide evidence that the near-quadratic running time bounds known for the problem of computing edit distance might be {tight}. Specifically, we show that, if the edit distance can be computed in time O(n2-\u03b4) for some constant \u03b4>0, then the satisfiability of conjunctive normal form formulas with N variables and M clauses can be solved in time MO(1) 2(1-\u03b5)N for a constant \u03b5>0. The latter result would violate the Strong Exponential Time Hypothesis, which postulates that such algorithms do not exist.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2064251",
                    "name": "A. Backurs"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "f73832bb8f7251a4f7169bbab5ad9d39dc16ed0e",
            "title": "A fast approximation algorithm for tree-sparse recovery",
            "abstract": "Sparse signals whose nonzeros obey a tree-like structure occur in a range of applications such as image modeling, genetic data analysis, and compressive sensing. An important problem encountered in recovering signals is that of optimal tree-projection, i.e., finding the closest tree-sparse signal for a given query signal. However, this problem can be computationally very demanding: for optimally projecting a length-n signal onto a tree with sparsity k, the best existing algorithms incur a high runtime of O(nk). This can often be impractical. We suggest an alternative approach to tree-sparse recovery. Our approach is based on a specific approximation algorithm for tree-projection and provably has a near-linear runtime of O(n log(kr)) and a memory cost of O(n), where r is the dynamic range of the signal. We leverage this approach in a fast recovery algorithm for tree-sparse compressive sensing that scales extremely well to high-dimensional datasets. Experimental results on several test cases demonstrate the validity of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144398138",
                    "name": "C. Hegde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                }
            ]
        },
        {
            "paperId": "f9d04fb757024e11c15f8f7be5af8e22a90b828b",
            "title": "Sample-Optimal Fourier Sampling in Any Constant Dimension",
            "abstract": "We give an algorithm for \u2113<sub>2</sub>/\u2113<sub>2</sub> sparse recovery from Fourier measurements using O(k log N) samples, matching the lower bound of Do Ba-Indyk-Price-Woodruff'10 for non-adaptive algorithms up to constant factors for any k \u2264 N<sup>1-\u03b4</sup>. The algorithm runs in O\u0303(N) time. Our algorithm extends to higher dimensions, leading to sample complexity of O\u0303<sub>d</sub>(k log N), which is optimal up to constant factors for any d = O(1). These are the first sample optimal algorithms for these problems. A preliminary experimental evaluation indicates that our algorithm has empirical sampling complexity comparable to that of other recovery methods known in the literature, while providing strong provable guarantees on the recovery quality.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1733218",
                    "name": "M. Kapralov"
                }
            ]
        },
        {
            "paperId": "03e1e87ec927707e5c582b6ef86f9df76bd82d4d",
            "title": "Sample-optimal average-case sparse Fourier Transform in two dimensions",
            "abstract": "We present the first sample-optimal sublinear time algorithms for the sparse Discrete Fourier Transform over a two-dimensional \u221an \u00d7 \u221an grid. Our algorithms are analyzed for the average case signals. For signals whose spectrum is exactly sparse, we present algorithms that use O(k) samples and run in O(k log k) time, where k is the expected sparsity of the signal. For signals whose spectrum is approximately sparse, we have an algorithm that uses O(k log n) samples and runs in O(k log2 n) time, for k = \u0398(\u221an). All presented algorithms match the lower bounds on sample complexity for their respective signal models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2529354",
                    "name": "Badih Ghazi"
                },
                {
                    "authorId": "2349340",
                    "name": "Haitham Hassanieh"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "4989538",
                    "name": "Eric Price"
                },
                {
                    "authorId": "37639770",
                    "name": "Lixin Shi"
                }
            ]
        },
        {
            "paperId": "0e77822c713e9311cd888923cba17a89114e153c",
            "title": "Shift Finding in Sub-Linear Time",
            "abstract": "We study the following basic pattern matching problem. Consider a \"code\" sequence c consisting of n bits chosen uniformly at random, and a \"signal\" sequence x obtained by shifting c (modulo n) and adding noise. The goal is to efficiently recover the shift with high probability. The problem models tasks of interest in several applications, including GPS synchronization and motion estimation. \n \nWe present an algorithm that solves the problem in time O(n(f/(1+f)), where O(Nf) is the running time of the best algorithm for finding the closest pair among N \"random\" sequences of length O(log N). A trivial bound of f = 2 leads to a simple algorithm with a running time of O(n2/3). The asymptotic running time can be further improved by plugging in recent more efficient algorithms for the closest pair problem. \n \nOur results also yield a sub-linear time algorithm for approximate pattern matching algorithm for a random signal (text), even for the case when the error between the signal and the code (pattern) is asymptotically as large as the code size. This is the first sublinear time algorithm for such error rates.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "2349340",
                    "name": "Haitham Hassanieh"
                }
            ]
        },
        {
            "paperId": "1ad2e5534204a23b910785fded53d9a0b7ef677c",
            "title": "Diverse near neighbor problem",
            "abstract": "Motivated by the recent research on diversity-aware search, we investigate the k-diverse near neighbor reporting problem. The problem is defined as follows: given a query point q, report the maximum diversity set S of k points in the ball of radius r around q. The diversity of a set S is measured by the minimum distance between any pair of points in $S$ (the higher, the better). We present two approximation algorithms for the case where the points live in a d-dimensional Hamming space. Our algorithms guarantee query times that are sub-linear in n and only polynomial in the diversity parameter k, as well as the dimension d. For low values of k, our algorithms achieve sub-linear query times even if the number of points within distance r from a query $q$ is linear in $n$. To the best of our knowledge, these are the first known algorithms of this type that offer provable guarantees.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3009678",
                    "name": "Sofiane Abbar"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "1757762",
                    "name": "Kasturi R. Varadarajan"
                }
            ]
        },
        {
            "paperId": "51bcfe9229cf4cce00b86f3b857ffebef7adcfa2",
            "title": "Beyond Locality-Sensitive Hashing",
            "abstract": "We present a new data structure for the c-approximate near neighbor problem (ANN) in the Euclidean space. For n points in Rd, our algorithm achieves Oc(n\u03c1 + dlogn) query time and Oc(n1+\u03c1 + dlogn) space, where \u03c1 \u2264 7/(8c2) + O(1/c3) + oc(1). This is the first improvement over the result by Andoni and Indyk (FOCS 2006) and the first data structure that bypasses a locality-sensitive hashing lower bound proved by O'Donnell, Wu and Zhou (ICS 2011). By a standard reduction we obtain a data structure for the Hamming space and e1 norm with \u03c1 \u2264 7/(8c)+ O(1/c3/2)+ oc(1), which is the first improvement over the result of Indyk and Motwani (STOC 1998).",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "143679841",
                    "name": "Huy L. Nguyen"
                },
                {
                    "authorId": "2111443",
                    "name": "Ilya P. Razenshteyn"
                }
            ]
        },
        {
            "paperId": "7d96eac9d3984f3a3187b8c2015fb62528daa0c4",
            "title": "Shot encoding with random projections",
            "abstract": "In order to image complex geological structures, seismic surveys acquire an increasingly large amount of data. While the resulting data sets enable higher-resolution images of the subsurface, they also contain redundant information and require large computational resources for processing. One approach for mitigating this trend is blended imaging, which combines the original shot records into a smaller number of blended shots at the expense of crosstalk in the final image. Since the cost of imaging is roughly proportional to the number of shots, blended imaging directly leads to a faster imaging process. In contrast to the existing shot encoding schemes, we establish a novel connection between blended imaging and dimensionality reduction using the Johnson-Lindenstrauss lemma. We introduce three new shot encoding schemes based on random projections and evaluate their performance. Our experiments on three data sets show that our random shot encoding schemes are competitive with existing shot encoding schemes and outperform decimated shot encoding for small numbers of shots.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "7863407",
                    "name": "Chaohui Chen"
                },
                {
                    "authorId": "1389777849",
                    "name": "A. St.-Cyr"
                },
                {
                    "authorId": "39443697",
                    "name": "D. Hohl"
                }
            ]
        },
        {
            "paperId": "a2ce28e04242421d704b87158ad2270271c91d80",
            "title": "Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing",
            "abstract": "Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing, and many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kd-trees do not perform well) is Locality Sensitive Hashing (LSH), an approximation algorithm for finding similar objects. \n \nIn this paper, we describe a new variant of LSH, called Parallel LSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports high-throughput streaming of new data. Our approach employs several novel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient algorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration algorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to optimize parameter settings. We show that on a workload where we perform similarity search on a dataset of > 1 Billion tweets, with hundreds of millions of new tweets per day, we can achieve query times of 1-2.5 ms. We show that this is an order of magnitude faster than existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH, with table construction times up to 3.7\u00d7 faster and query times that are 8.3\u00d7 faster than a basic implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1789372",
                    "name": "N. Sundaram"
                },
                {
                    "authorId": "2568851",
                    "name": "Aizana Turmukhametova"
                },
                {
                    "authorId": "143758120",
                    "name": "N. Satish"
                },
                {
                    "authorId": "3269633",
                    "name": "Todd Mostak"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "144478906",
                    "name": "S. Madden"
                },
                {
                    "authorId": "145126868",
                    "name": "P. Dubey"
                }
            ]
        },
        {
            "paperId": "b5a7f9305b5af2ac7a2396eebeb21479c614df71",
            "title": "Sketching via hashing: from heavy hitters to compressed sensing to sparse fourier transform",
            "abstract": "Sketching via hashing is a popular and useful method for processing large data sets. Its basic idea is as follows. Suppose that we have a large multi-set of elements S = {a1, . . . as} \u2282 {1 . . . n}, and we would like to identify the elements that occur \u201cfrequently\u201d in S. The algorithm starts by selecting a hash function h that maps the elements into an array c[1 . . .m]. The array entries are initialized to 0. Then, for each element a \u2208 S, the algorithm increments c[h(a)]. At the end of the process, each array entry c[j] contains the count of all data elements a \u2208 S mapped to j. It can be observed that if an element a occurs frequently enough in the data set S, then the value of the counter c[h(a)] must be large. That is, \u201cfrequent\u201d elements are mapped to \u201cheavy\u201d buckets. By identifying the elements mapped to heavy buckets and repeating the process several times, one can efficiently recover the frequent elements, possibly together with a few extra ones (false positives).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "d05b74a72d9799498958143a3255a76edb91470e",
            "title": "Real-time recommendation of diverse related articles",
            "abstract": "News articles typically drive a lot of traffic in the form of comments posted by users on a news site. Such user-generated content tends to carry additional information such as entities and sentiment. In general, when articles are recommended to users, only popularity (e.g., most shared and most commented), recency, and sometimes (manual) editors' picks (based on daily hot topics), are considered. We formalize a novel recommendation problem where the goal is to find the closest most diverse articles to the one the user is currently browsing. Our diversity measure incorporates entities and sentiment extracted from comments. Given the real-time nature of our recommendations, we explore the applicability of nearest neighbor algorithms to solve the problem. Our user study on real opinion articles from aljazeera.net and reuters.com validates the use of entities and sentiment extracted from articles and their comments to achieve news diversity when compared to content-based diversity. Finally, our performance experiments show the real-time feasibility of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009678",
                    "name": "Sofiane Abbar"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                }
            ]
        },
        {
            "paperId": "db4a8bb69e69c8af6ee466c087eedbe3d59d8b84",
            "title": "Euclidean spanners in high dimensions",
            "abstract": "A classical result in metric geometry asserts that any n-point metric admits a linear-size spanner of dilation O(log n) [PS89]. More generally, for any c > 1, any metric space admits a spanner of size O(n1+1/c), and dilation at most c. This bound is tight assuming the well-known girth conjecture of Erdos [Erd63]. \n \nWe show that for a metric induced by a set of n points in high-dimensional Euclidean space, it is possible to obtain improved dilation/size trade-offs. More specifically, we show that any n-point Euclidean metric admits a near-linear size spanner of dilation O(\u221alog n). Using the LSH scheme of Andoni and Indyk [AI06] we further show that for any c > 1, there exist spanners of size roughly O(",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1388677326",
                    "name": "Sariel Har-Peled"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1695128",
                    "name": "Anastasios Sidiropoulos"
                }
            ]
        },
        {
            "paperId": "eafc93125e158c4715d6aefa5b33a0bb52b77d43",
            "title": "Compressive sensing using locality-preserving matrices",
            "abstract": "Compressive sensing is a method for acquiring high dimensional signals (e.g., images) using a small number of linear measurements. Consider an n-pixel image x \u2208 Rn, where each pixel p has value xp. The image is acquired by computing the measurement vector Ax, where A is an m x n measurement matrix, for some m << n. The goal is to design the matrix A and the recovery algorithm which, given Ax, returns an approximation to x. It is known that m=O(k log(n/k)) measurements suffices to recover the k-sparse approximation of x. Unfortunately, this result uses matrices A that are random. Such matrices are difficult to implement in physical devices. In this paper we propose compressive sensing schemes that use matrices A that achieve the near-optimal bound of m=O(k log n), while being highly \"local\". We also show impossibility results for stronger notions of locality.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "39086705",
                    "name": "Elyot Grant"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "f6bd0f2abcb5d74ecfe34e8916d24b42eaf62ad3",
            "title": "Nearly optimal linear embeddings into very low dimensions",
            "abstract": "We propose algorithms for constructing linear embeddings of a finite dataset V \u2282 \u211d<sup>d</sup> into a k-dimensional subspace with provable, nearly optimal distortions. First, we propose an exhaustive-search-based algorithm that yields a k-dimensional linear embedding with distortion at most \u03b5<sub>opt</sub>(k)+\u03b4, for any \u03b4 > 0 where \u03b5<sub>opt</sub>(k) is the smallest achievable distortion over all possible orthonormal embeddings. This algorithm is space-efficient and can be achieved by a single pass over the data V. However, the runtime of this algorithm is exponential in k. Second, we propose a convex-programming-based algorithm that yields an O(k/\u03b4)-dimensional orthonormal embedding with distortion at most (1 + \u03b4)\u03b5<sub>opt</sub>(k). The runtime of this algorithm is polynomial in d and independent of k. Several experiments demonstrate the benefits of our approach over conventional linear embedding techniques, such as principal components analysis (PCA) or random projections.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "39086705",
                    "name": "Elyot Grant"
                },
                {
                    "authorId": "144398138",
                    "name": "C. Hegde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "2b3aabf4173e515a6e9bbc3410cd5dd9c87549ba",
            "title": "Efficient and reliable low-power backscatter networks",
            "abstract": "There is a long-standing vision of embedding backscatter nodes like RFIDs into everyday objects to build ultra-low power ubiquitous networks. A major problem that has challenged this vision is that backscatter communication is neither reliable nor efficient. Backscatter nodes cannot sense each other, and hence tend to suffer from colliding transmissions. Further, they are ineffective at adapting the bit rate to channel conditions, and thus miss opportunities to increase throughput, or transmit above capacity causing errors.\n This paper introduces a new approach to backscatter communication. The key idea is to treat all nodes as if they were a single virtual sender. One can then view collisions as a code across the bits transmitted by the nodes. By ensuring only a few nodes collide at any time, we make collisions act as a sparse code and decode them using a new customized compressive sensing algorithm. Further, we can make these collisions act as a rateless code to automatically adapt the bit rate to channel quality --i.e., nodes can keep colliding until the base station has collected enough collisions to decode. Results from a network of backscatter nodes communicating with a USRP backscatter base station demonstrate that the new design produces a 3.5\u00d7 throughput gain, and due to its rateless code, reduces message loss rate in challenging scenarios from 50% to zero.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109763500",
                    "name": "Jue Wang"
                },
                {
                    "authorId": "2349340",
                    "name": "Haitham Hassanieh"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "4c2693414b9040ab798392f5563877b3d7217f1b",
            "title": "Faster GPS via the sparse fourier transform",
            "abstract": "GPS is one of the most widely used wireless systems. A GPS receiver has to lock on the satellite signals to calculate its position. The process of locking on the satellites is quite costly and requires hundreds of millions of hardware multiplications, leading to high power consumption. The fastest known algorithm for this problem is based on the Fourier transform and has a complexity of O(n log n), where n is the number of signal samples. This paper presents the fastest GPS locking algorithm to date. The algorithm reduces the locking complexity to O(n\u221a(log n)). Further, if the SNR is above a threshold, the algorithm becomes linear, i.e., O(n). Our algorithm builds on recent developments in the growing area of sparse recovery. It exploits the sparse nature of the synchronization problem, where only the correct alignment between the received GPS signal and the satellite code causes their cross-correlation to spike.\n We further show that the theoretical gain translates into empirical gains for GPS receivers. Specifically, we built a prototype of the design using software radios and tested it on two GPS data sets collected in the US and Europe. The results show that the new algorithm reduces the median number of multiplications by 2.2x in comparison to the state of the art design, for real GPS signals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2349340",
                    "name": "Haitham Hassanieh"
                },
                {
                    "authorId": "1761544",
                    "name": "Fadel M. Adib"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "7ae42cc4b475a2192707d0ada7b3c60c347b621e",
            "title": "Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality",
            "abstract": "We present two algorithms for the approximate nearest neighbor problem in high dimensional spaces. For data sets of size n living in IR d , the algorithms require space that is only polynomial in n and d , while achieving query times that are sub-linear in n and polynomial in d . We also show applications to other high-dimensional geometric problems, such as the approximate minimum spanning tree.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388677326",
                    "name": "Sariel Har-Peled"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                }
            ]
        },
        {
            "paperId": "7c69c424784595ea604014971d639ca40b805582",
            "title": "Nearly optimal sparse fourier transform",
            "abstract": "We consider the problem of computing the k-sparse approximation to the discrete Fourier transform of an n-dimensional signal. We show: An O(k log n)-time randomized algorithm for the case where the input signal has at most k non-zero Fourier coefficients, and An O(k log n log(n/k))-time randomized algorithm for general input signals.\n Both algorithms achieve o(n log n) time, and thus improve over the Fast Fourier Transform, for any k=o(n). They are the first known algorithms that satisfy this property. Also, if one assumes that the Fast Fourier Transform is optimal, the algorithm for the exactly k-sparse case is optimal for any k = n\u03a9(1).\n We complement our algorithmic results by showing that any algorithm for computing the sparse Fourier transform of a general signal must use at least \u03a9(k log (n/k) / log log n) signal samples, even if it is allowed to perform adaptive sampling.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2349340",
                    "name": "Haitham Hassanieh"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "4989538",
                    "name": "Eric Price"
                }
            ]
        },
        {
            "paperId": "8419a1520215650001c268958800c8e1efc5378d",
            "title": "Focal plane array folding for efficient information extraction and tracking",
            "abstract": "We develop a novel compressive sensing based approach for detecting point sources in images and tracking of moving point sources across temporal images. One application is the muzzle flash detection and tracking problem. We pursue the concept of lower-dimension signal representation from structured sparse matrices, which is in contrast to the use of random sparse matrices described in common compressive sensing algorithms. The primary motivation is that an approach using structured sparse matrices can lead to efficient hardware implementations and a scheme that we term folding in the focal plane array. This method \u201cbins\u201d pixels modulo a pair of specified numbers across the pixel plane in both the horizontal and vertical directions. Under this paradigm, a significant reduction in the amount of pixel samples is required, which enable high speed target acquisition and tracking while reducing the number of A/D's. Folding is used to acquire a pair of significantly smaller images, in which two different folded images provide the necessary redundancy to uniquely extract location information. We detect the centroid of point sources in each of the two folded images and use the Chinese remainder theorem (CRT) to determine the location of the point sources in the original image. In our work, we successfully demonstrated the correctness of this algorithm through simulation and showed the algorithm is capable of detecting and tracking multiple muzzle flashes in multiple temporal frames. We present both initial results and improvements to the algorithm's robustness, based on robust Chinese remainder theorem (rCRT) in the presence of noise.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "50753640",
                    "name": "L. Hamilton"
                },
                {
                    "authorId": "2083623873",
                    "name": "D. Parker"
                },
                {
                    "authorId": "145248383",
                    "name": "Chris Yu"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "8a23fb198c5651c44c2f51ce9967aca7d5bf6b2c",
            "title": "Simple and practical algorithm for sparse Fourier transform",
            "abstract": "We consider the sparse Fourier transform problem: given a complex vector x of length n, and a parameter k, estimate the k largest (in magnitude) coefficients of the Fourier transform of x. The problem is of key interest in several areas, including signal processing, audio/image/video compression, and learning theory. \n \nWe propose a new algorithm for this problem. The algorithm leverages techniques from digital signal processing, notably Gaussian and Dolph-Chebyshev filters. Unlike the typical approach to this problem, our algorithm is not iterative. That is, instead of estimating \"large\" coefficients, subtracting them and recursing on the reminder, it identifies and estimates the k largest coefficients in \"one shot\", in a manner akin to sketching/streaming algorithms. The resulting algorithm is structurally simpler than its predecessors. As a consequence, we are able to extend considerably the range of sparsity, k, for which the algorithm is faster than FFT, both in theory and practice.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2349340",
                    "name": "Haitham Hassanieh"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "4989538",
                    "name": "Eric Price"
                }
            ]
        },
        {
            "paperId": "c9d1c629a5167a5e9ad9432583745646f22dbba9",
            "title": "Approximating and testing k-histogram distributions in sub-linear time",
            "abstract": "A discrete distribution p, over [n], is a k histogram if its probability distribution function can be represented as a piece-wise constant function with k pieces. Such a function is represented by a list of k intervals and k corresponding values. We consider the following problem: given a collection of samples from a distribution p, find a k-histogram that (approximately) minimizes the l 2 distance to the distribution p. We give time and sample efficient algorithms for this problem. We further provide algorithms that distinguish distributions that have the property of being a k-histogram from distributions that are \u03b5-far from any k-histogram in the l 1 distance and l 2 distance respectively.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "35647859",
                    "name": "Reut Levi"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                }
            ]
        },
        {
            "paperId": "edb61801d9854ca3bc2e9bcab6312d9777a353dd",
            "title": "Efficient and reliable low-power backscatter networks",
            "abstract": "There is a long-standing vision of embedding backscatter nodes like RFIDs into everyday objects to build ultra-low power ubiquitous networks. A major problem that has challenged this vision is that backscatter communication is neither reliable nor efficient. Backscatter nodes cannot sense each other, and hence tend to suffer from colliding transmissions. Further, they are ineffective at adapting the bit rate to channel conditions, and thus miss opportunities to increase throughput, or transmit above capacity causing errors.\n This paper introduces a new approach to backscatter communication. The key idea is to treat all nodes as if they were a single virtual sender. One can then view collisions as a code across the bits transmitted by the nodes. By ensuring only a few nodes collide at any time, we make collisions act as a sparse code and decode them using a new customized compressive sensing algorithm. Further, we can make these collisions act as a rateless code to automatically adapt the bit rate to channel quality --i.e., nodes can keep colliding until the base station has collected enough collisions to decode. Results from a network of backscatter nodes communicating with a USRP backscatter base station demonstrate that the new design produces a 3.5\u00d7 throughput gain, and due to its rateless code, reduces message loss rate in challenging scenarios from 50% to zero.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109763500",
                    "name": "Jue Wang"
                },
                {
                    "authorId": "2349340",
                    "name": "Haitham Hassanieh"
                },
                {
                    "authorId": "1785714",
                    "name": "D. Katabi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "95bdbe558f5bd5affd5c58de07d4e909672152a5",
            "title": "The Complexity of Linear Dependence Problems in Vector Spaces",
            "abstract": "We study extensions of the k-SUM problem to vector spaces over finite fields. Given a subset S \u2286 Fq of size r \u2264 q, an integer k, 2 \u2264 k \u2264 n, and a vector v \u2208 (Fq \\ {0}), we define the TARGETSUM problem to be the problem of finding k elements xi1 , . . . , xik \u2208 S for which \u2211k j=1 vjxij = z, where z may either be an input or a fixed vector. We also study a variant of this, where instead of finding xi1 , . . . , xik \u2208 S for which \u2211k j=1 vjxij = z, we require that z be in span(xi1 , . . . , xik), which we call the (k, r)-LINDEPENDENCEq problem. These problems are natural generalizations of well-studied problems that occur in coding theory and property testing. Indeed, the (k, r)-LINDEPENDENCEq problem is just the MAXIMUM LIKELIHOOD DECODING problem for linear codes. Also, in the TARGETSUM problem, if instead of general z we require z = 0, then this is the WEIGHT DISTRIBUTION problem for linear codes. In property testing, these problems have been implicitly studied in the context of testing linear-invariant properties. We give nearly optimal bounds for TARGETSUM and (k, r)-LINDEPENDENCEq for every r, k, and constant q. Namely, assuming 3-SAT requires exponential time, we show that any algorithm for these problems must run in min(2, r) time. Moreover, we give deterministic upper bounds that match this complexity, up to constant factors in the exponent. Our lower bound strengthens and simplifies an earlier min(2, r 1/4)) lower bound for both the MAXIMUM LIKELIHOOD DECODING and WEIGHT DISTRIBUTION problems. We also give upper and lower bounds for variants of these problems, e.g., for the problem for which we must find xi1 , . . . , xik for which z \u2208 span(xi1 , . . . , xik) but z is not spanned by any proper subset of these vectors, and for the counting version of this problem.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1751082",
                    "name": "Arnab Bhattacharyya"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                },
                {
                    "authorId": "47599221",
                    "name": "Ning Xie"
                }
            ]
        },
        {
            "paperId": "b8ba10b9189a0dc95cf96c7771bdc2941745100e",
            "title": "Compressive sensing with local geometric features",
            "abstract": "We propose a framework for compressive sensing of images with local geometric features. Specifically, let x \u2208 RN be an N-pixel image, where each pixel p has value xp. The image is acquired by computing the measurement vector Ax, where A is an m x N measurement matrix for some m l N. The goal is then to design the matrix A and recovery algorithm which, given Ax, returns an approximation to x.\n In this paper we investigate this problem for the case where x consists of a small number (k) of \"local geometric objects\" (e.g., stars in an image of a sky), plus noise. We construct a matrix A and recovery algorithm with the following features: (i) the number of measurements m is O(k logk N), which undercuts currently known schemes that achieve m=O(k log (N/k)) (ii) the matrix A is ultra-sparse, which is important for hardware considerations (iii) the recovery algorithm is fast and runs in time sub-linear in N. We also present a comprehensive study of an application of our algorithm to a problem in satellite navigation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2109271260",
                    "name": "Rishi Gupta"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "4989538",
                    "name": "Eric Price"
                },
                {
                    "authorId": "2442190",
                    "name": "Y. Rachlin"
                }
            ]
        },
        {
            "paperId": "d6afc69f9947758b0d5742dad38aef486ba1a9d0",
            "title": "K-median clustering, model-based compressive sensing, and sparse recovery for earth mover distance",
            "abstract": "We initiate the study of sparse recovery problems under the Earth-Mover Distance (EMD). Specifically, we design a distribution over m x n matrices A such that for any x, given Ax, we can recover a k-sparse approximation to x under the EMD distance. One construction yields m=O(k log (n/k)) and a 1 + \u03b5 approximation factor, which matches the best achievable bound for other error measures, such as the l1 norm. Our algorithms are obtained by exploiting novel connections to other problems and areas, such as streaming algorithms for k-median clustering and model-based compressive sensing. We also provide novel algorithms and results for the latter problems.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "4989538",
                    "name": "Eric Price"
                }
            ]
        },
        {
            "paperId": "de829c5f9acef9e9be39f2be55b315bd4516e716",
            "title": "On the Power of Adaptivity in Sparse Recovery",
            "abstract": "The goal of (stable) sparse recovery is to recover a $k$-sparse approximation $x^*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is to recover $x^*$ such that$$\\norm{p}{x-x^*} \\le C \\min_{k\\text{-sparse } x'} \\norm{q}{x-x'}$$for some constant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or $p=q=2$, this task can be accomplished using $m=O(k \\log (n/k))$ {\\em non-adaptive}measurements~\\cite{CRT06:Stable-Signal} and that this bound is tight~\\cite{DIPW, FPRU, PW11}. In this paper we show that if one is allowed to perform measurements that are {\\em adaptive}, then the number of measurements can be considerably reduced. Specifically, for $C=1+\\epsilon$ and $p=q=2$ we show\\begin{itemize}\\item A scheme with $m=O(\\frac{1}{\\eps}k \\log \\log (n\\eps/k))$ measurements that uses $O(\\log^* k \\cdot \\log \\log (n\\eps/k))$ rounds. This is a significant improvement over the best possible non-adaptive bound. \\item A scheme with $m=O(\\frac{1}{\\eps}k \\log (k/\\eps) + k \\log (n/k))$ measurements that uses {\\em two} rounds. This improves over the best possible non-adaptive bound. \\end{itemize} To the best of our knowledge, these are the first results of this type.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "4989538",
                    "name": "Eric Price"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "2874730dde419b7989e12e5d97dac9b2a9fad455",
            "title": "Sparse Recovery Using Sparse Matrices",
            "abstract": "In this paper, we survey algorithms for sparse recovery problems that are based on sparse random matrices. Such matrices has several attractive properties: they support algorithms with low computational complexity, and make it easy to perform incremental updates to signals. We discuss applications to several areas, including compressive sensing, data stream computing, and group testing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1795861",
                    "name": "A. Gilbert"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "2a77da9a650bcf5564bcb568f511557366b46eea",
            "title": "Lower bounds for sparse recovery",
            "abstract": "We consider the following <i>k</i>-sparse recovery problem: design an <i>m</i> x <i>n</i> matrix <i>A</i>, such that for any signal <i>x</i>, given <i>Ax</i> we can efficiently recover x satisfying ||<i>x</i> -- x||<sub>i</sub> \u2264 <i>C</i> min<sub><i>k</i></sub>-sparse <i>x</i>' ||<i>x</i> - <i>x</i>'||<sub>1</sub>. It is known that there exist matrices A with this property that have only <i>O</i>(<i>k</i> log(<i>n/k</i>)) rows.\n In this paper we show that this bound is tight. Our bound holds even for the more general <i>randomized</i> version of the problem, where <i>A</i> is a random variable, and the recovery algorithm is required to work for any fixed x with constant probability (over <i>A</i>).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2836805",
                    "name": "Khanh Do Ba"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "4989538",
                    "name": "Eric Price"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "4eb9b7933c9ab2a4726342339a1fcff6e5ddec8d",
            "title": "Efficiently decodable non-adaptive group testing",
            "abstract": "We consider the following \"efficiently decodable\" non-adaptive group testing problem. There is an unknown string <i>x</i> \u2208 {0, 1}<i><sup>n</sup></i> with at most <i>d</i> ones in it. We are allowed to test any subset <i>S</i> \u2286 [<i>n</i>] of the indices. The answer to the test tells whether <i>x</i><sub><i>i</i></sub> = 0 for all <i>i</i> \u2208 <i>S</i> or not. The objective is to design as few tests as possible (say, <i>t</i> tests) such that <i>x</i> can be identified as fast as possible (say, poly(<i>t</i>)-time). Efficiently decodable non-adaptive group testing has applications in many areas, including data stream algorithms and data forensics.\n A non-adaptive group testing strategy can be represented by a <i>t</i> x <i>n</i> matrix, which is the stacking of all the characteristic vectors of the tests. It is well-known that if this matrix is <i>d</i>-disjunct, then any test outcome corresponds uniquely to an unknown input string. Furthermore, we know how to construct <i>d</i>-disjunct matrices with <i>t</i> = <i>O</i>(<i>d</i><sup>2</sup> log <i>n</i>) efficiently. However, these matrices so far only allow for a \"decoding\" time of <i>O</i>(<i>nt</i>), which can be exponentially larger than poly(<i>t</i>) for relatively small values of <i>d</i>.\n This paper presents a randomness efficient construction of <i>d</i>-disjunct matrices with <i>t</i> = <i>O</i>(<i>d</i><sup>2</sup> log <i>n</i>) that can be decoded in time poly(<i>d</i>) \u00b7 <i>t</i> log<sup>2</sup> <i>t</i> + <i>O</i>(<i>t</i><sup>2</sup>). To the best of our knowledge, this is the first result that achieves an efficient decoding time and matches the best known <i>O</i>(<i>d</i><sup>2</sup> log <i>n</i>) bound on the number of tests. We also derandomize the construction, which results in a polynomial time deterministic construction of such matrices when <i>d</i> = <i>O</i>(log <i>n</i> / log log <i>n</i>).\n A crucial building block in our construction is the notion of (<i>d</i>, <i>l</i>)-list disjunct matrices, which represent the more general \"list group testing\" problem whose goal is to output less than <i>d</i> + <i>l</i> positions in <i>x</i>, including all the (at most <i>d</i>) positions that have a one in them. List disjunct matrices turn out to be interesting objects in their own right and were also considered independently by [Cheraghchi, FCT 2009]. We present connections between list disjunct matrices, expanders, dispersers and disjunct matrices. List disjunct matrices have applications in constructing (<i>d</i>, <i>l</i>)-sparsity separator structures [Ganguly, ISAAC 2008] and in constructing tolerant testers for Reed-Solomon codes in the data stream model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "144655602",
                    "name": "H. Ngo"
                },
                {
                    "authorId": "1755572",
                    "name": "A. Rudra"
                }
            ]
        },
        {
            "paperId": "7d946d6c3d6e922f5fe9afed150da4fd5d6fd70d",
            "title": "Sparse Signal Recovery and Acquisition with Graphical Models",
            "abstract": "A great deal of theoretic and algorithmic research has revolved around sparsity view of signals over the last decade to characterize new, sub-Nyquist sampling limits as well as tractable algorithms for signal recovery from dimensionality reduced measurements. Despite the promising advances made, real-life applications require more realistic signal models that can capture the underlying, application-dependent order of sparse coefficients, better sampling matrices with information preserving properties that can be implemented in practical systems, and ever faster algorithms with provable recovery guarantees for real-time operation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145006560",
                    "name": "L. Carin"
                },
                {
                    "authorId": "144908066",
                    "name": "Richard Baraniuk"
                }
            ]
        },
        {
            "paperId": "9b73ca2fc34229874c43e645995d87e2bd619805",
            "title": "Sparse Recovery Using Sparse Matrices Significant results in predicting the operation of equipment such as network routers, or the results of group testing for defective items, can often be obtained from a few samples.",
            "abstract": "In this paper, we survey algorithms for sparse recovery problems that are based on sparse random matrices. Such matrices has several attractive properties: they support algorithms with low computational complexity, and make it easy to perform incremental updates to signals. We discuss applications to several areas, including compressive sensing, data stream computing, and group testing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66816302",
                    "name": "A. Gilbert"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "9ecd8c1882dff08fc67edb931a5371e0833c8acb",
            "title": "Sparse recovery for Earth Mover Distance",
            "abstract": "We initiate the study of sparse recovery problems under the Earth-Mover Distance (EMD). Specifically, we design a distribution over m \u00d7 n matrices A, for m \u00ab n, such that for any x, given Ax, we can recover a k-sparse approximation to x under the EMD distance. We also provide an empirical evaluation of the method that show, in some scenarios, its advantages over the \u201cusual\u201d recovery in the \u2113<inf>p</inf> norms.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2109271260",
                    "name": "Rishi Gupta"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "4989538",
                    "name": "Eric Price"
                }
            ]
        },
        {
            "paperId": "e2dcd4ae28a3db2be2a3c790be7ff2a71ede7200",
            "title": "Motif discovery in physiological datasets: A methodology for inferring predictive elements",
            "abstract": "In this article, we propose a methodology for identifying predictive physiological patterns in the absence of prior knowledge. We use the principle of conservation to identify activity that consistently precedes an outcome in patients, and describe a two-stage process that allows us to efficiently search for such patterns in large datasets. This involves first transforming continuous physiological signals from patients into symbolic sequences, and then searching for patterns in these reduced representations that are strongly associated with an outcome.\n Our strategy of identifying conserved activity that is unlikely to have occurred purely by chance in symbolic data is analogous to the discovery of regulatory motifs in genomic datasets. We build upon existing work in this area, generalizing the notion of a regulatory motif and enhancing current techniques to operate robustly on non-genomic data. We also address two significant considerations associated with motif discovery in general: computational efficiency and robustness in the presence of degeneracy and noise. To deal with these issues, we introduce the concept of active regions and new subset-based techniques such as a two-layer Gibbs sampling algorithm. These extensions allow for a framework for information inference, where precursors are identified as approximately conserved activity of arbitrary complexity preceding multiple occurrences of an event.\n We evaluated our solution on a population of patients who experienced sudden cardiac death and attempted to discover electrocardiographic activity that may be associated with the endpoint of death. To assess the predictive patterns discovered, we compared likelihood scores for motifs in the sudden death population against control populations of normal individuals and those with non-fatal supraventricular arrhythmias. Our results suggest that predictive motif discovery may be able to identify clinically relevant information even in the absence of significant prior knowledge.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "145654745",
                    "name": "Z. Syed"
                },
                {
                    "authorId": "2088297",
                    "name": "Collin M. Stultz"
                },
                {
                    "authorId": "2809250",
                    "name": "Manolis Kellis"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1724429",
                    "name": "J. Guttag"
                }
            ]
        },
        {
            "paperId": "f00fdd61731daefcfd7f0788a2ab29edf0380d18",
            "title": "Space-optimal heavy hitters with strong error bounds",
            "abstract": "The problem of finding heavy hitters and approximating the frequencies of items is at the heart of many problems in data stream analysis. It has been observed that several proposed solutions to this problem can outperform their worst-case guarantees on real data. This leads to the question of whether some stronger bounds can be guaranteed. We answer this in the positive by showing that a class of counter-based algorithms (including the popular and very space-efficient <scp>Frequent</scp> and <scp>SpacesSaving</scp> algorithms) provides much stronger approximation guarantees than previously known. Specifically, we show that errors in the approximation of individual elements do not depend on the frequencies of the most frequent elements, but only on the frequency of the remaining tail. This shows that counter-based methods are the most space-efficient (in fact, space-optimal) algorithms having this strong error bound.\n This tail guarantee allows these algorithms to solve the sparse recovery problem. Here, the goal is to recover a faithful representation of the vector of frequencies, <i>f</i>. We prove that using space <i>O</i>(<i>k</i>), the algorithms construct an approximation <i>f</i>* to the frequency vector <i>f</i> so that the <i>L</i><sub>1</sub> error \u2225\u2225<i>f</i>\u2212\u2225<i>f</i>*\u2225<sub>1</sub> is close to the best possible error min<sub><i>f</i>\u2032</sub> \u2225<i>f</i>\u2032 \u2212 <i>f</i>\u2225<sub>1</sub>, where <i>f\u2032</i> ranges over all vectors with at most <i>k</i> non-zero entries. This improves the previously best known space bound of about <i>O</i>(<i>k</i> log <i>n</i>) for streams without element deletions (where <i>n</i> is the size of the domain from which stream elements are drawn). Other consequences of the tail guarantees are results for skewed (Zipfian) data, and guarantees for accuracy of merging multiple summarized streams.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162899",
                    "name": "Radu Berinde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145542622",
                    "name": "M. Strauss"
                }
            ]
        },
        {
            "paperId": "30c206ba6e7ab7f4f3545f90171e7d0d0522b1d1",
            "title": "Sequential Sparse Matching Pursuit",
            "abstract": "We propose a new algorithm, called Sequential Sparse Matching Pursuit (SSMP), for solving sparse recovery problems. The algorithm provably recovers a k-sparse approximation to an arbitrary n-dimensional signal vector x from only O(k log(n/k)) linear measurements of x. The recovery process takes time that is only near-linear in n. Preliminary experiments indicate that the algorithm works well on synthetic and image data, with the recovery quality often outperforming that of more complex algorithms, such as l1 minimization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162899",
                    "name": "Radu Berinde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "327a2a197145def83d831603a1716a820295aad8",
            "title": "Approximate line nearest neighbor in high dimensions",
            "abstract": "We consider the problem of approximate nearest neighbors in high dimensions, when the queries are lines. In this problem, given n points in Rd, we want to construct a data structure to support efficiently the following queries: given a line L, report the point p closest to L. This problem generalizes the more familiar nearest neighbor problem. From a practical perspective, lines, and low-dimensional flats in general, may model data under linear variation, such as physical objects under different lighting. \n \nFor approximation 1 + e, we achieve a query time of d3n0.5+t, for arbitrary small t > 0, with a space of d2nO(1/e2+1/t2). To the best of our knowledge, this is the first algorithm for this problem with polynomial space and sub-linear query time.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1737712",
                    "name": "Robert Krauthgamer"
                },
                {
                    "authorId": "143679841",
                    "name": "Huy L. Nguyen"
                }
            ]
        },
        {
            "paperId": "469ab1c1fb23d63a57447db90f5695986e31bd58",
            "title": "Learning Approximate Sequential Patterns for Classification",
            "abstract": "In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved motifs that occur with varying statistical properties in positive and negative training examples. We propose a two-step process to discover such patterns. Using locality sensitive hashing (LSH), we first estimate the frequency of all subsequences and their approximate matches within a given Hamming radius in labeled examples. The discriminative ability of each pattern is then assessed from the estimated frequencies by concordance and rank sum testing. The use of LSH to identify approximate matches for each candidate pattern helps reduce the runtime of our method. Space requirements are reduced by decomposing the search problem into an iterative method that uses a single LSH table in memory. We propose two further optimizations to the search for discriminative patterns. Clustering with redundancy based on a 2-approximate solution of the k-center problem decreases the number of overlapping approximate groups while providing exhaustive coverage of the search space. Sequential statistical methods allow the search process to use data from only as many training examples as are needed to assess significance. We evaluated our algorithm on data sets from different applications to discover sequential patterns for classification. On nucleotide sequences from the Drosophila genome compared with random background sequences, our method was able to discover approximate binding sites that were preserved upstream of genes. We observed a similar result in experiments on ChIP-on-chip data. For cardiovascular data from patients admitted with acute coronary syndromes, our pattern discovery approach identified approximately conserved sequences of morphology variations that were predictive of future death in a test population. Our data showed that the use of LSH, clustering, and sequential statistics improved the running time of the search algorithm by an order of magnitude without any noticeable effect on accuracy. These results suggest that our methods may allow for an unsupervised approach to efficiently learn interesting dissimilarities between positive and negative examples that may have a functional role.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145654745",
                    "name": "Z. Syed"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1724429",
                    "name": "J. Guttag"
                }
            ]
        },
        {
            "paperId": "477ed8f498498e981802f2fdc8dcef1f9207e5f2",
            "title": "Efficient Sketches for Earth-Mover Distance, with Applications",
            "abstract": "We provide the first sub-linear sketching algorithm for estimating the planar Earth-Mover Distance with a constant approximation. For sets living in the two-dimensional grid $[\\Delta]^2$, we achieve space$\\Delta^{\\eps}$ for approximation $O(1/\\eps)$, for any desired $0",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "2836805",
                    "name": "Khanh Do Ba"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "7c9fc485babac3c212ded9fba76fb228654bc4e1",
            "title": "Space-optimal heavy hitters with strong error bounds",
            "abstract": "The problem of finding heavy hitters and approximating the frequencies of items is at the heart of many problems in data stream analysis. It has been observed that several proposed solutions to this problem can outperform their worst-case guarantees on real data. This leads to the question of whether some stronger bounds can be guaranteed. We answer this in the positive by showing that a class of \"counter-based algorithms\" (including the popular and very space-efficient FREQUENT and SPACESAVING algorithms) provide much stronger approximation guarantees than previously known. Specifically, we show that errors in the approximation of individual elements do not depend on the frequencies of the most frequent elements, but only on the frequency of the remaining \"tail.\" This shows that counter-based methods are the most space-efficient (in fact, space-optimal) algorithms having this strong error bound. This tail guarantee allows these algorithms to solve the \"sparse recovery\" problem. Here, the goal is to recover a faithful representation of the vector of frequencies, f. We prove that using space O(k), the algorithms construct an approximation f* to the frequency vector f so that the L1 error ||f -- f*||1 is close to the best possible error minf2 ||f2 -- f||1, where f2 ranges over all vectors with at most k non-zero entries. This improves the previously best known space bound of about O(k log n) for streams without element deletions (where n is the size of the domain from which stream elements are drawn). Other consequences of the tail guarantees are results for skewed (Zipfian) data, and guarantees for accuracy of merging multiple summarized streams.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2162899",
                    "name": "Radu Berinde"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "145542622",
                    "name": "M. Strauss"
                }
            ]
        },
        {
            "paperId": "691b297d9cf59189663e77665ba49d2cb8d00a70",
            "title": "Earth mover distance over high-dimensional spaces",
            "abstract": "The Earth Mover Distance (EMD) between two equal-size sets of points in \u211d<i><sup>d</sup></i> is defined to be the minimum cost of a bipartite matching between the two pointsets. It is a natural metric for comparing sets of features, and as such, it has received significant interest in computer vision. Motivated by recent developments in that area, we address computational problems involving EMD over <i>high-dimensional</i> pointsets.\n A natural approach is to embed the EMD metric into <i>l</i><sub>1</sub>, and use the algorithms designed for the latter space. However, Khot and Naor [KN06] show that any embedding of EMD over the <i>d</i>-dimensional Hamming cube into <i>l</i><sub>1</sub> must incur a distortion \u03a9<i>(d)</i>, thus practically losing all distance information. We circumvent this roadblock by focusing on sets with cardinalities upper-bounded by a parameter <i>s</i>, and achieve a distortion of only <i>O</i>(log <i>s</i> \u00b7 log <i>d</i>). Since in applications the feature sets have bounded size, the resulting distortion is much smaller than the \u03a9<i>(d)</i> lower bound. Our approach is quite general and easily extends to EMD over \u211d<i><sup>d</sup>.</i>\n We then provide a strong lower bound on the multi-round communicatic complexity of estimating EMD, which in particular strengthens the known non-embeddability result of [KN06]. Our bound exhibits a smooth tradeoff between approximation and communication, and for example implies that every algorithm that estimates EMD using constant size sketches can only achieve \u03c9(log <i>s</i>) approximation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1737712",
                    "name": "Robert Krauthgamer"
                }
            ]
        },
        {
            "paperId": "a5f6363ba97be9a9d8632ace4c0a01fe18b398ad",
            "title": "Declaring independence via the sketching of sketches",
            "abstract": "We consider the problem of identifying correlations in data streams. Surprisingly, our work seems to be the first to consider this natural problem. In the centralized model, we consider a stream of pairs (i,j) \u2208 [n]2 whose frequencies define a joint distribution (X,Y). In the distributed model, each coordinate of the pair may appear separately in the stream. We present a range of algorithms for approximating to what extent X and Y are independent, i.e., how close the joint distribution is to the product of the marginals. We consider various measures of closeness including \u21131, \u21132, and the mutual information between X and Y. Our algorithms are based on \"sketching sketches\", i.e., composing small-space linear synopses of the distributions. Perhaps ironically, the biggest technical challenges that arise relate to ensuring that different components of our estimates are sufficiently independent.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "144078750",
                    "name": "A. Mcgregor"
                }
            ]
        },
        {
            "paperId": "d6c11833fe4fe1d53a59d4a11138c768c6323033",
            "title": "Practical near-optimal sparse recovery in the L1 norm",
            "abstract": "We consider the approximate sparse recovery problem, where the goal is to (approximately) recover a high-dimensional vector x isin Rn from its lower-dimensional sketch Ax isin Rm. Specifically, we focus on the sparse recovery problem in the l1 norm: for a parameter k, given the sketch Ax, compute an approximation xcirc of x such that the l1 approximation error parx - xcircpar1 is close to minx' parx - x'par1, where x' ranges over all vectors with at most k terms. The sparse recovery problem has been subject to extensive research over the last few years. Many solutions to this problem have been discovered, achieving different trade-offs between various attributes, such as the sketch length, encoding and recovery times.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162899",
                    "name": "Radu Berinde"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1806535",
                    "name": "M. Ruzic"
                }
            ]
        },
        {
            "paperId": "dd0ff1de66068d5aadb3d4cb633ef8734e99fb5b",
            "title": "Nearest-Neighbor Methods in Learning and Vision",
            "abstract": "New techniques characterized by low computational complexity, great learning capability, and efficient decision methods are highly desirable in order to regulate and classify nondeterministic phenomena and events in the emerging areas of signal, image, and video processing. The reason for this great interest is closely related to intrinsic nondeterministic nature of real phenomena, taking the deterministic hypothesis just as an approximation of the actual behavior. The great interest in the nearest-neighbor methods and the absence of reliable solutions for many nondeterministic problems represent the starting points for the formalization and development of specific approaches and suitable techniques. In this excellent book, the editors deal with the state-of-the-art, current best practices, and some innovative applications of nearest-neighbor methods in learning and vision. In fact, this volume brings together contributions of top-level researchers in theory of computation, machine learning, and computer vision with the goal of closing up the gaps between disciplines and current state-of-the-art methods for emerging applications. Therefore, the audience of this volume consists of researchers, scientists, engineers, professionals, and academics, working not only in this field, but also in any field that could benefit from these powerful methods. This book can be particularly useful to researchers working on the basis set expansion networks. Finally, it is worth noting that all the content is well-written, highly relevant, original, and timely. In order to better appreciate the aforementioned remarks let us briefly introduce the book chapters. Chapter 1 written by the editors introduces the book and its organization. Chapter 2 entitled \u201cNearest-neighbor searching and metric space dimension\u201d is authored by K. L. Clarkson. The content deals with the problem definition and its relationship with the concepts of metric space dimension. The state-of-the-art and the current best practices are proposed and properly discussed. In Chapter 3 entitled \u201cLocality-sensitive hashing using stable distributions,\u201d written by A. Andoni, M. Datar, N. Immorlica, P. Indyk, and V. Mirrokni, novel locality-sensitive hashing technique is described, providing an efficient solution to the randomized nearest-neighbor problem. Chapter 4 is entitled \u201cNew algorithms for efficient high-dimensional nonparametric classification,\u201d authored by T. Liu, A. W. Moore, and A. Gray. It deals with new balltree algorithms that on real-world data sets",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2490189",
                    "name": "Gregory Shakhnarovich"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "e578a730b2a2e8b6b7ccb8dd0c05cf396ac385ee",
            "title": "Explicit constructions for compressed sensing of sparse signals",
            "abstract": "Over the recent years, a new approach for obtaining a succinct approximate representation of <i>n</i>-dimensional vectors (or signals) has been discovered. For any signal <i>x</i>, the succinct representation of <i>x</i> is equal to <i>Ax</i>, where <i>A</i> is a carefully chosen <i>R</i> x <i>n</i> real matrix, <i>R</i> \u226a <i>n.</i> Often, <i>A</i> is chosen at random from some distribution over <i>R</i> x <i>n</i> matrices. The vector <i>Ax</i> is often refered to as the <i>measurement vector</i> or a <i>sketch</i> of <i>x.</i> Although the dimension of <i>Ax</i> is much shorter than of <i>x</i>, it contains plenty of useful information about <i>x.</i>",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "e9415d34745bd4a5c5c484b83b007cb711693ecb",
            "title": "Combining geometry and combinatorics: A unified approach to sparse signal recovery",
            "abstract": "There are two main algorithmic approaches to sparse signal recovery: geometric and combinatorial. The geometric approach utilizes geometric properties of the measurement matrix Phi. A notable example is the Restricted Isometry Property, which states that the mapping Phi preserves the Euclidean norm of sparse signals; it is known that random dense matrices satisfy this constraint with high probability. On the other hand, the combinatorial approach utilizes sparse matrices, interpreted as adjacency matrices of sparse (possibly random) graphs, and uses combinatorial techniques to recover an approximation to the signal. In this paper we present a unification of these two approaches. To this end, we extend the notion of Restricted Isometry Property from the Euclidean lscr2 norm to the Manhattan lscr1 norm. Then we show that this new lscr1 -based property is essentially equivalent to the combinatorial notion of expansion of the sparse graph underlying the measurement matrix. At the same time we show that the new property suffices to guarantee correctness of both geometric and combinatorial recovery algorithms. As a result, we obtain new measurement matrix constructions and algorithms for signal recovery which, compared to previous algorithms, are superior in either the number of measurements or computational efficiency of decoders.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2162899",
                    "name": "Radu Berinde"
                },
                {
                    "authorId": "1795861",
                    "name": "A. Gilbert"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1735932",
                    "name": "H. Karloff"
                },
                {
                    "authorId": "145542622",
                    "name": "M. Strauss"
                }
            ]
        },
        {
            "paperId": "4eda6ce887df2a14792a31a930f5af7fddd6e059",
            "title": "Approximation algorithms for embedding general metrics into trees",
            "abstract": "We consider the problem of embedding general metrics into trees. We give the first non-trivial approximation algorithm for minimizing the multiplicative distortion. Our algorithm produces an embedding with distortion (c log n)O(\u221alog \u0394), where c is the optimal distortion, and \u0394 is the spread of the metric (i.e. the ratio of the diameter over the minimum distance). We give an improved O(1)-approximation algorithm for the case where the input is the shortest path metric over an unweighted graph. Moreover, we show that by composing our approximation algorithm for embedding general metrics into trees, with the approximation algorithm of [BCIS05] for embedding trees into the line, we obtain an improved approximation algorithm for embedding general metrics into the line.\n We also provide almost tight bounds for the relation between embedding into trees and embedding into spanning subtrees. We show that for any unweighted graph G, the ratio of the distortion required to embed G into a spanning subtree, over the distortion of an optimal tree embedding of G, is at most O(log n). We complement this bound by exhibiting a family of graphs for which the ratio is \u03a9(log n/log log n).",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2002252",
                    "name": "Mihai Badoiu"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1695128",
                    "name": "Anastasios Sidiropoulos"
                }
            ]
        },
        {
            "paperId": "6beca6c1ffa1cf0902e22ac6aff921de76474f7e",
            "title": "Probabilistic embeddings of bounded genus graphs into planar graphs",
            "abstract": "A probabilistic C-embedding of a (guest) metric M into a collection of(host) metrics M'1, ..., M'k is a randomized mapping F of M intoone of the M'1, ..., M'k such that, for any two points p,q in theguest metric: The distance between F(p) and F(q) in any M'i is not smaller thanthe original distance between p and q. The expected distance between F(p) and F(q) in (random) M'i is notgreater than some constant C times the original distance, for C\u2265 1. The constant C is called the distortion of the embedding. Low-distortion probabilistic embeddings enable reducing algorithmicproblems over \"hard\" guest metrics into \"easy\" host metrics.We show that every metric induced by a graph of bounded genus can beprobabilistically embedded into planar graphs, with constant distortion. The embedding can be computed efficiently, given a drawing of the graphon a genus-g surface.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1695128",
                    "name": "Anastasios Sidiropoulos"
                }
            ]
        },
        {
            "paperId": "6cd9e7bffbd4661d95a8f6e4e8e4df133f80fab3",
            "title": "Nearest-neighbor-preserving embeddings",
            "abstract": "In this article we introduce the notion of nearest-neighbor-preserving embeddings. These are randomized embeddings between two metric spaces which preserve the (approximate) nearest-neighbors. We give two examples of such embeddings for Euclidean metrics with low \u201cintrinsic\u201d dimension. Combining the embeddings with known data structures yields the best-known approximate nearest-neighbor data structures for such metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2949968",
                    "name": "A. Naor"
                }
            ]
        },
        {
            "paperId": "6f1f335389db540d6acf287beb406bffc1b2b914",
            "title": "A near linear time constant factor approximation for Euclidean bichromatic matching (cost)",
            "abstract": "We give an <i>N</i>log<sup><i>O</i>(1)</sup> <i>N</i>-time randomized <i>O</i>(1)-approximation algorithm for computing the cost of minimum bichromatic matching between two planar point-sets of size <i>N</i>.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "00756a336f7254eb04de38056b3770a1fdb3675a",
            "title": "Efficient algorithms for substring near neighbor problem",
            "abstract": "In this paper we consider the problem of finding the approximate nearest neighbor when the data set points are the substrings of a given text <i>T</i>. Specifically, for a string <i>T</i> of length <i>n</i>, we present a data structure which does the following: given a pattern <i>P</i>, if there is a substring of <i>T</i> within the distance <i>R</i> from <i>P</i>, it reports a (possibly different) substring of <i>T</i> within distance <i>cR</i> from <i>P</i>. The length of the pattern <i>P</i>, denoted by <i>m</i>, is <i>not</i> known in advance. For the case where the distances are measured using the Hamming distance, we present a data structure which uses <i>\u00d5</i>(<i>n</i><sup>1+1/<i>c</i></sup>) space<sup>1</sup> and with <i>\u00d5</i>(<i>n</i><sup>1/<i>c</i></sup> + <i>mn</i><sup><i>o</i>(1)</sup>) query time. This essentially matches the earlier bounds of [Ind98], which assumed that the pattern length <i>m</i> is <i>fixed in advance</i>. In addition, our data structure can be constructed in time <i>\u00d5</i>(<i>n</i><sup>1+1/<i>c</i></sup> + <i>n</i><sup>1+<i>o</i>(1)</sup><i>M</i><sup>1/3</sup>), where <i>M</i> is an upper bound for <i>m</i>. This essentially matches the preprocessing bound of [Ind98] as long as the term <i>\u00d5</i>(<i>n</i><sup>1+1/<i>c</i></sup>) dominates the running time, which is the case when, e.g., <i>c</i> < 3.We also extend our results to the case where the distances are measured according to the <i>l</i><inf>1</inf> distance. The query time and the space bound are essentially the same, while the preprocessing time becomes <i>\u00d5</i>(<i>n</i><sup>1+1/<i>c</i></sup> + <i>n</i><sup>1+<i>o</i>(1)</sup><i>M</i><sup>2/3</sup>).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "1d04db2f33762c7e7dca13925b9d1c0f28f6ca6d",
            "title": "On the Optimality of the Dimensionality Reduction Method",
            "abstract": "We investigate the optimality of (1+epsi)-approximation algorithms obtained via the dimensionality reduction method. We show that: any data structure for the (1 + epsi)-approximate nearest neighbor problem in Hamming space, which uses constant number of probes to answer each query, must use nOmega(1/epsi2) space; any algorithm for the (1 + epsi)-approximate closest substring problem must run in time exponential in 1/epsi2 - gamma for any gamma > 0 (unless 3SAT can be solved in sub-exponential time). Both lower bounds are (essentially) tight",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1738800",
                    "name": "M. Patrascu"
                }
            ]
        },
        {
            "paperId": "2323173a0bddac0dd2586b17a2f3ac33f401c45c",
            "title": "Nearest-Neighbor Methods in Learning and Vision: Theory and Practice (Neural Information Processing)",
            "abstract": "Regression and classification methods based on similarity of the input to stored examples have not been widely used in applications involving very large sets of high-dimensional data. Recent advances in computational geometry and machine learning, however, may alleviate the problems in using these methods on large data sets. This volume presents theoretical and practical discussions of nearest-neighbor (NN) methods in machine learning and examines computer vision as an application domain in which the benefit of these advanced methods is often dramatic. It brings together contributions from researchers in theory of computation, machine learning, and computer vision with the goals of bridging the gaps between disciplines and presenting state-of-the-art methods for emerging applications.The contributors focus on the importance of designing algorithms for NN search, and for the related classification, regression, and retrieval tasks, that remain efficient even as the number of points or the dimensionality of the data grows very large. The book begins with two theoretical chapters on computational geometry and then explores ways to make the NN approach practicable in machine learning applications where the dimensionality of the data and the size of the data sets make the naive methods for NN search prohibitively expensive. The final chapters describe successful applications of an NN algorithm, locality-sensitive hashing (LSH), to vision tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2490189",
                    "name": "Gregory Shakhnarovich"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "68b1b86409e6e2e426ebc48ff01d60fb86a83223",
            "title": "Low-Complexity Localized Walsh Decoding For CDMA Systems",
            "abstract": "As software radio becomes increasingly popular for wireless infrastructure, intelligent signal processing algorithms are required to make the most efficient use of the computational cycles available on general-purpose processors. In particular, the design of CDMA receiver algorithms for software radio is challenging because signals must be processed at rates much higher than other wireless standards. While traditional hardware implementations of the CDMA standard IS-95 despread signals received at the base station prior to Walsh decoding, in this paper we take advantage of the flexibility afforded by software implementations and develop three classes of Walsh decoding algorithms that do not require full despreading of incoming signals at the base station. Two proposed classes of algorithms exploit the fact that Walsh codes are locally decodable codes, which have the surprising property that any bit of the message can be recovered (with some probability) by examining only a small number of symbols of the codeword. We also describe a third class of algorithms based on code puncturing. All these algorithms enable trading off computation for performance, and we use them to dynamically minimize the computational requirements of the despreader and subsequent Walsh decoder such that a target bit-error rate is maintained with changing channel conditions. These algorithms are applicable to other CDMA-based systems that use Walsh codes for orthogonal modulation, and the third class is also applicable to CDMA-based systems that use codes for channelization, including lxRTT, EV-DO, and W-CDMA",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145411350",
                    "name": "A. Chan"
                },
                {
                    "authorId": "38838239",
                    "name": "J. Feldman"
                },
                {
                    "authorId": "65909631",
                    "name": "R. Madyastha"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1743286",
                    "name": "David R Karger"
                }
            ]
        },
        {
            "paperId": "75055f879027333048c2182c293b5519212a8c08",
            "title": "Embedding ultrametrics into low-dimensional spaces",
            "abstract": "We study the problem of minimum-distortion embedding of ultrametrics into the plane and higher dimensional spaces. Ultrametrics are a natural class of metrics that frequently occur in applications involving hierarchical clustering. Low-distortion embeddings of ultrametrics into the plane help visualizing complex structures they often represent.Given an ultrametric, a natural question is whether we can efficiently find an optimal-distortion embedding of this ultrametric into the plane, and if not, whether we can design an efficient algorithm that produces embeddings with near-optimal distortion. We show that the problem of finding minimum-distortion embedding of ultrametrics into the plane is NP-hard, and thus approximation algorithms are called for. Given an input ultrametric <i>M</i>, let <i>c</i> denote the minimum distortion achievable by any embedding of <i>M</i> into the plane. Our main result is a linear-time algorithm that produces an <i>O(c</i><sup>3</sup>)-distortion embedding. This result can be generalized to embedding ultrametrics into <i>R<sup>d</sup></i>, for any <i>d</i>\u22652, with distortion <i>c<sup>O(d)</sup></i>, where <i>c</i> is the minimum distortion achievable for embedding the input ultrametric into <i>R<sup>d</sup></i>.Additionally, we show that <i>any</i> ultrametric can be embedded into the plane with distortion <i>O(\u221an)</i>, and in general, into <i>R<sup>d</sup></i> with distortion <i>d<sup>O(1)</sup></i> <i>n</i><sup>1/<i>d</i></sup>. Combining the two results together, we obtain an <i>O(n</i><sup>1/3</sup>)-approximation algorithm for the problem of minimum-distortion embedding of ultrametrics into the plane.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2002252",
                    "name": "Mihai Badoiu"
                },
                {
                    "authorId": "2495534",
                    "name": "Julia Chuzhoy"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1695128",
                    "name": "Anastasios Sidiropoulos"
                }
            ]
        },
        {
            "paperId": "e17529924798975856310a75cb3df3066ac7ccfa",
            "title": "Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions",
            "abstract": "We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "f03f0995d102c6ab5bba2bb1e0d59a520e20f4bd",
            "title": "Locality-Sensitive Hashing Using Stable Distributions",
            "abstract": "This chapter contains sections titled: The Locality-Sensitive Hashing Scheme Based on s-Stable Distributions, Approximate Near Neighbor, Exact Near Neighbor, LSH in Practice: E2LSH, Experimental Results",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2490189",
                    "name": "Gregory Shakhnarovich"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "fd520fe7594d58364aefe695f183fdd5962999f7",
            "title": "New Algorithms for Efficient High-Dimensional Nonparametric Classification",
            "abstract": "This chapter contains sections titled: Introduction, Balltree, Exact Near Neighbor, KNS1: Conventional k-NN Search with Balltree, KNS2: Faster k-NN Classification for Skewed-Class Data, KNS3: Are at Least t of the K-NN Positive?, Experimental Results, Comments and related work",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2490189",
                    "name": "Gregory Shakhnarovich"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "4bda63faf8f8e8f7b3d161c3ef4afc418285c792",
            "title": "LOCAL DECODING OF WALSH CODES TO REDUCE CDMA DESPREADING COMPUTATION",
            "abstract": "The invention is an improved layout for integrated circuits employing local interconnect pads, particularly six-transistor SRAM circuits, comprising a local interconnect pad which electrically bridges two segments of a conducting line and an active device, and a method for employing the layout.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067969142",
                    "name": "T. F. Chan"
                },
                {
                    "authorId": "38838239",
                    "name": "J. Feldman"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1743286",
                    "name": "David R Karger"
                }
            ]
        },
        {
            "paperId": "5c0d4162eb5d34c98417c170e178a59fe49bd646",
            "title": "Low-distortion embeddings of general metrics into the line",
            "abstract": "A low-distortion embedding between two metric spaces is a mapping which preserves the distances between each pair of points, up to a small factor called distortion. Low-distortion embeddings have recently found numerous applications in computer science.Most of the known embedding results are \"absolute\",that is, of the form: any metric <i>Y</i> from a given class of metrics <i>C</i> can be embedded into a metric <i>X</i> with low distortion <i>c</i>. This is beneficial if one can guarantee low distortion for all metrics <i>Y</i> in <i>C</i>. However, in any situations, the worst-case distortion is too large to be meaningful. For example, if <i>X</i> is a line metric, then even very simple metrics (an <i>n</i> - point star or an <i>n</i> -point cycle) are embeddable into <i>X</i> only with distortion linear in <i>n</i>. Nevertheless, embeddings into the line (or into low-dimensional spaces) are important for many applications.A solution to this issue is to consider \"relative\" (or \"approximation\") embedding problems, where the goal is to design an (a-approxiation) algorithm which, given any metric <i>X</i> from <i>C</i> as an input, finds an embedding of <i>X</i> into <i>Y</i> which has distortion <i>a</i> *<i>c</i><inf><i>Y</i></inf> (<i>X</i>), where <i>c</i><inf><i>Y</i></inf> (<i>X</i>)is the best possible distortion of an embedding of <i>X</i> into <i>Y</i>.In this paper we show algorithms and hardness results for relative embedding problems.In particular we give: \u2022an algorith that, given a general metric <i>M</i>, finds an embedding with distortion <i>O</i> (\u0394<sup>3\u20444</sup> poly(<i>c</i> <i><inf>line</inf></i> (<i>M</i>))), where \u0394 is the spread of <i>M</i>\u2022an algorithm that,given a weighted tree etric <i>M</i>, finds an embedding with distortion poly(<i>c</i> <inf><i>line</i></inf> (<i>M</i>)) \u2022a hardness result, showing that computing minimum line distortion is hard to approximate up to a factor polynomial in <i>n</i>,even for weighted tree metrics with spread \u0394=<i>n</i> <i><sup>O</sup></i> (1).",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2002252",
                    "name": "Mihai Badoiu"
                },
                {
                    "authorId": "2495534",
                    "name": "Julia Chuzhoy"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1695128",
                    "name": "Anastasios Sidiropoulos"
                }
            ]
        },
        {
            "paperId": "7315d9a937f0fa093721eb25fbf240efb89e0a25",
            "title": "Linear-time encodable/decodable codes with near-optimal rate",
            "abstract": "We present an explicit construction of linear-time encodable and decodable codes of rate r which can correct a fraction (1-r-/spl epsiv/)/2 of errors over an alphabet of constant size depending only on /spl epsiv/, for every 00. The error-correction performance of these codes is optimal as seen by the Singleton bound (these are \"near-MDS\" codes). Such near-MDS linear-time codes were known for the decoding from erasures; our construction generalizes this to handle errors as well. Concatenating these codes with good, constant-sized binary codes gives a construction of linear-time binary codes which meet the Zyablov bound, and also the more general Blokh-Zyablov bound (by resorting to multilevel concatenation). Our work also yields linear-time encodable/decodable codes which match Forney's error exponent for concatenated codes for communication over the binary symmetric channel. The encoding/decoding complexity was quadratic in Forney's result, and Forney's bound has remained the best constructive error exponent for almost 40 years now. In summary, our results match the performance of the previously known explicit constructions of codes that had polynomial time encoding and decoding, but in addition have linear-time encoding and decoding algorithms.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721077",
                    "name": "V. Guruswami"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "881f6becd2db20d3edcf6e8ac0feb5ecb7ba50f6",
            "title": "Optimal approximations of the frequency moments of data streams",
            "abstract": "We give a 1-pass <i>\u00d5</i>(<i>m</i><sup>1-2\u2044<i>k</i></sup>)-space algorithm for computing the <i>k</i>-th frequency moment of a data stream for any real <i>k</i> > 2. Together with the lower bounds of [1, 2, 4], this resolves the main problem left open by Alon et al in 1996 [1]. Our algorithm also works for streams with deletions and thus gives an <i>\u00d5</i>(<i>m</i> <sup>1-2\u2044p</sup>) space algorithm for the <i>L</i><i><inf>p</inf></i> difference problem for any p > 2. This essentially matches the known \u03a9(<i>m</i><sup>1-2\u2044<i>p</i>-<i>o</i>(1)</sup>) lower bound of [12, 2]. Finally the update time of our algorithms is <i>\u00d5</i>(1).",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "eb1670f5cc44547d89f4de3e4ada50b6828b71f1",
            "title": "Sampling in dynamic data streams and applications",
            "abstract": "A dynamic geometric data stream is a sequence of m Add/Remove operations of points from a discrete geometric space (1,...,\u0394)d [21]. Add(p) inserts a point p from (1,...,\u0394)d into the current point set, Remove(p) deletes p from P. We develop low-storage data structures to (i) maintain \u03b5-approximations of range spaces of P with constant VC-dimension and (ii) maintain an \u03b5-approximation of the weight of the Euclidean minimum spanning tree of P. Our data structures use O(log3\u03b5 \u2022 log3(1/\u03b5) \u2022 log(1/\u03b5)/\u03b52) and O(log (1/\u03b4) \u2022 (log \u0394/\u03b5)O(d)) bits of memory, respectively (we assume that the dimension d is a constant), and they are correct with probability 1-\u03b4. These results are based on a new data structure that maintains a set of elements chosen (almost) uniformly at random from P.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3098847",
                    "name": "Gereon Frahling"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1701715",
                    "name": "C. Sohler"
                }
            ]
        },
        {
            "paperId": "3f1e54ed3bd801766e1897d53a9fc962524dd3c2",
            "title": "Locality-sensitive hashing scheme based on p-stable distributions",
            "abstract": "We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p<1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain \"bounded growth\" condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145112445",
                    "name": "Mayur Datar"
                },
                {
                    "authorId": "1754163",
                    "name": "Nicole Immorlica"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1728881",
                    "name": "V. Mirrokni"
                }
            ]
        },
        {
            "paperId": "5d6dd8dddc1638331111af8cebc5b486ed9970b5",
            "title": "Algorithms for dynamic geometric problems over data streams",
            "abstract": "Computing over data streams is a recent phenomenon that is of growing interest in many areas of computer science, including databases, computer networks and theory of algorithms. In this scenario, it is assumed that the algorithm sees the elements of the input one-by-one in arbitrary order, and needs to compute a certain function of the input. However, it does not have enough memory to store the whole input. Therefore, it must maintain a \u201csketch\u201d of the data. Designing a sketching method for a given problem is a novel and exciting challenge for algorithm design. The initial research in streaming algorithms has focused on computing simple numerical statistics of the input, like median [23], number of distinct elements [11] or frequency moments [1]. More recently, the researchers showed that one can use those algorithms as subroutines to solve more complex problems (e.g., cf. [13]); see the survey [24] for detailed description of the past and recent developments. Still, the scope of algorithmic problems for which stream algorithms exist is not well understood. It is therefore of importance to identify new classes of problems that can be solved in this restricted settings. In this paper we investigate stream algorithms for dynamic geometric problems. Specifically, we present low-storage data structures that maintain approximate solutions to geometric problems, under insertions and deletions of points (this is called a turnstile model in [24]). From the data stream perspective, the stream consists of m operations, each of them is either Add(p) (that adds p to the current",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "7707b48d8118bdb565b76ad8c12073b1fdce5ac9",
            "title": "Fast approximate pattern matching with few indels via embeddings",
            "abstract": "The problem of finding a non-exact occurence of a pattern in a text over some alphabet is a central problem of combinatorial pattern matching and has a variety applications in many areas of computer science. The classic instance of the problem is when the distance between the pattern and a substring of is measured according to the edit distance. That is, the distance between and is equal to the minimum number of insertions, deletions and substitutions of characters that transform into . Despite decades of research, the best algorithm for this problem has running time  if , even if the algorithm is allowed to report approximate answers. This contrasts with the setting when only substitutions are allowed, where nearlinear time approximation algorithms are known. In particular, Karloff [Kar93] showed how to reduce the problem over any to the same problem over , by mapping each symbol from to a sequence of \"!# $&%(' random bits. The binary case can be solved in # \"!) * time using FFT. Several exact subquadratic-time algorithms exist for general as well (references omitted due to lack of space). In this paper we investigate searching under the edit distance in the following setting. Let be a substring of . We call a ,+/., -match if can be transformed into using at most + insertions or deletions and . substitutions. The key feature in this definition is that the number of insertions and deletions (indels) is separated from the number of substitutions. Since substitutions are \u201ceasier\u201d to handle, this allows us to obtain algorithms with better performance than when +102. arbitrary edit operations are allowed. In particular, we show",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2002252",
                    "name": "Mihai Badoiu"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "7caa49576db54f5dbb2fa4be33cb2ddf0e35f4c2",
            "title": "Low-Dimensional Embedding with Extra Information",
            "abstract": "A frequently arising problem in computational geometry is when a physical structure, such as an ad-hoc wireless sensor network or a protein backbone, can measure local information about its geometry (e.g., distances, angles, and/or orientations), and the goal is to reconstruct the global geometry from this partial information. More precisely, we are given a graph, the approximate lengths of the edges, and possibly extra information, and our goal is to assign two-dimensional coordinates to the vertices such that the (multiplicative or additive) error on the resulting distances and other information is within a constant factor of the best possible. We obtain the first pseudo-quasipolynomial-time algorithm for this problem given a complete graph of Euclidean distances with additive error and no extra information. For general graphs, the analogous problem is NP-hard even with exact distances. Thus, for general graphs, we consider natural types of extra information that make the problem more tractable, including approximate angles between edges, the order type of vertices, a model of coordinate noise, or knowledge about the range of distance measurements. Our pseudo-quasipolynomial-time algorithm for no extra information can also be viewed as a polynomial-time algorithm given an \"extremum oracle\" as extra information. We give several approximation algorithms and contrasting hardness results for these scenarios.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2002252",
                    "name": "Mihai Badoiu"
                },
                {
                    "authorId": "1727384",
                    "name": "E. Demaine"
                },
                {
                    "authorId": "1704161",
                    "name": "M. Hajiaghayi"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "a9bb77a7f67fa0cc21ea5ff1a160e6288febcce2",
            "title": "Efficiently decodable codes meeting Gilbert-Varshamov bound for low rates",
            "abstract": "We demonstrate a probabilistic construction of binary linear codes meeting the GV bound (with overwhelming probability) for rates up to about 10-4 together with polynomial time algorithms to perform encoding and decoding up to half the distance. The only previous result of this type (for rates up to about 0.02) suffered from sub-exponential time decoding [3].",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721077",
                    "name": "V. Guruswami"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "ef24871b874ea7cf1b8e9a5238f2db60b74a7eab",
            "title": "Approximate Nearest Neighbor under edit distance via product metrics",
            "abstract": "We present a data structure for the approximate nearest neighbor problem under edit metric (which is defined as the minimum number of insertions, deletions and character substitutions needed to transform one string into another). For any <i>l</i> \u2265 1 and a set of <i>n</i> strings of length <i>d</i>, the data structure reports a 3<sup><i>l</i></sup>-approximate Nearest Neighbor for any given query string <i>q</i> in <i>O</i>(<i>d</i>) time. The space requirement of this data structure is roughly <i>O</i>(<i>n</i><sup><i>d</i><sup>1/(<i>l</i>+1)</sup></sup>), i.e., strongly subexponential. To our knowledge, this is the first data structure for this problem with both <i>o</i>(<i>n</i>) query time and storage subexponential in <i>d</i>.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "47aafd64b02fce9053fa3995309fbca011340b8b",
            "title": "Embeddings and non-approximability of geometric problems",
            "abstract": "In this paper we present two results. Our first theorem shows that \"sparse\" (see definition later) metric spaces, with distances 1 and 2, are isometrically embeddable into lowdimensional loo norm. A similar in \"spirit\" result was earlier proved for lp norms with finite p by Trevisan [Tre97]. Our result is obtained using a different technique (Bourrgain's sampling approach) and could not have been achieved using the techniques of [Tre97]. In the second part of the paper we apply the aforementioned embeddings to show non-approximability results for several geometric problems in Ip norms with logarithmic dimension, namely TSP, k-median and rain-sum k-clustering. In particular, we provide the first known constant factor hardness for the rain-sum k-clustering problem.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721077",
                    "name": "V. Guruswami"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "605f2e601757dfb2d7ccc9239622d330cce75e2e",
            "title": "Probabilistic Analysis for Discrete Attributes of Moving Points",
            "abstract": "We perform a probabilistic study of discrete attributes of moving points. In our probabilistic model, an item is given an initial position and a velocity drawn independently at random from the same distribution. We study the expected number of changes that happen to the closest pair, the Voronoi diagram, and the convex hull of a set of such moving items.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32556251",
                    "name": "Julien Basch"
                },
                {
                    "authorId": "2000457",
                    "name": "H. Devarajan"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "2152830040",
                    "name": "Li Zhang"
                }
            ]
        },
        {
            "paperId": "be6d3c013c5117207e86d6b98ea185e556548a6c",
            "title": "Lower bounds for embedding edit distance into normed spaces",
            "abstract": "1 In t roduc t ion The edit distance (also called Levenshtein metric) between two strings is the minimum number of operations (insertions, deletions and character substitutions) needed to transform one string into another. This distance is of key importance in computational biology, as well as text processing and other areas. Algorithms for problems involving this metric have been extensively investigated. In particular, the quadratic-time dynamic programming algorithm for computing the edit distance between two strings is one of the most investigated and used algorithms in computational biology. Recently, a new approach to problems involving edit distance has been proposed. Its basic component is construction of a mapping ff (called an embedding), which maps any string s into a vector ff(s) E ~d, so that for any pair of strings s, s', the Ip distance IIf(s)-f(s')lip is approximately equal to the edit distance between s and s ~. The approximation factor is called distortion of the embedding f. A low-distortion embedding of edit distance into lp norm would be very useful, for the following reasons:",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1738395",
                    "name": "Alexandr Andoni"
                },
                {
                    "authorId": "1936261",
                    "name": "M. Deza"
                },
                {
                    "authorId": "2110759547",
                    "name": "Anupam Gupta"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1797575",
                    "name": "Sofya Raskhodnikova"
                }
            ]
        },
        {
            "paperId": "d5fdc0deea46853a4ff42523fadc965790cf55c3",
            "title": "Better algorithms for high-dimensional proximity problems via asymmetric embeddings",
            "abstract": "In this paper we give several results based on randomized embeddings of <i>l</i><inf>2</inf> into <i>l</i><inf>\u221e</inf>(or \"<i>l</i><inf>\u221e</inf>-like\") spaces. Our first result is a (1 + \u03b5)-distortion <i>asymmetric</i> embedding of <i>n</i> points in <i>l</i><inf>2</inf> into <i>l</i><inf>\u221e</inf> with polylog(<i>n</i>) dimension, for any 1 + \u03b5. This gives the first known <i>O</i>(1)- approximate nearest neighbor algorithm with fast query time and almost polynomial space for a product of Euclidean norms, a common generalization of both <i>l</i><inf>2</inf> and <i>l</i><inf>\u221e</inf> norms. Our embedding also clarifies the relative complexity of approximate nearest neighbor in <i>l</i><inf>2</inf> and <i>l</i><inf>\u221e</inf> spaces.Our second result in a (1 + \u03b5)-approximate algorithm for the diameter of <i>n</i> points in <i>l<sup>d</sup><inf>2</inf></i>, running in time <i>\u00d5</i>(<i>dn</i><sup>1+l/(1+\u03b5)</sup><sup>2</sup>); the algorithm is fully dynamic. This improves several previous algorithms for this problem (see Table 1 for more information).",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "efd89df38c3c3773e478275b57f909c0ffe48e00",
            "title": "Tight lower bounds for the distinct elements problem",
            "abstract": "We prove strong lower bounds for the space complexity of (/spl epsi/, /spl delta/)-approximating the number of distinct elements F/sub 0/ in a data stream. Let m be the size of the universe from which the stream elements are drawn. We show that any one-pass streaming algorithm for (/spl epsi/, /spl delta/)-approximating F/sub 0/ must use /spl Omega/(1//spl epsi//sup 2/) space when /spl epsi/ = /spl Omega/(m/sup -1/(9 + k)/), for any k > 0, improving upon the known lower bound of /spl Omega/(1//spl epsi/) for this range of /spl epsi/. This lower bound is tight up to a factor of log log m for small /spl epsi/ and log 1//spl epsi/ for large /spl epsi/. Our lower bound is derived from a reduction from the one-way communication complexity of approximating a Boolean function in Euclidean space. The reduction makes use of a low-distortion embedding from an l/sub 2/ to l/sub 1/ norm.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "143982862",
                    "name": "David P. Woodruff"
                }
            ]
        },
        {
            "paperId": "f2fddfcd7c1bcdbe0a55f62bb6732595df0c52c4",
            "title": "Linear time encodable and list decodable codes",
            "abstract": "We present the first construction of error-correcting codes which can be (list) decoded from a noise fraction arbitrarily close to 1 in linear time. Specifically, we present an explicit construction of codes which can be encoded in linear time as well as list decoded in linear time from a fraction (1-\u03b5) of errors for arbitrary \u03b5 > 0. The rate and alphabet size of the construction are constants that depend only on \u03b5. Our construction involves devising a new combinatorial approach to list decoding, in contrast to all previous approaches which relied on the power of decoding algorithms for algebraic codes like Reed-Solomon codes.Our result implies that it is possible to have, and in fact explicitly specifies, a coding scheme for arbitrarily large noise thresholds with only constant redundancy in the encoding and constant amount of work (at both the sending and receiving ends) for each bit of information to be communicated. Such a result was known for certain probabilistic error models, and here we show that this is possible under the stronger adversarial noise model as well.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721077",
                    "name": "V. Guruswami"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "0ff08cadd56bf40913b36654c74f1ca18e5a948f",
            "title": "Maintaining stream statistics over sliding windows: (extended abstract)",
            "abstract": "We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last <i>N</i> data elements seen so far. We refer to this model as the <i>sliding window</i> model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1's in the last <i>N</i> elements seen from the stream. We show that using <i>O</i>(1/<i>e</i> log<sup>2</sup><i>N</i>) bits of memory, we can estimate the number of 1's to within a factor of 1 + \u03b5. We also give a matching lower bound of \u03a9(1/<i>e</i> log<sup>2</sup> <i>N</i>) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last <i>N</i> positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the <i>L<inf>p</inf></i> norms (for <i>p</i> \u03b5 [1, 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of <i>O</i>(1/\u03b5log <i>N</i>) in memory and a 1 + \u03b5 factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145112445",
                    "name": "Mayur Datar"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                }
            ]
        },
        {
            "paperId": "1fbff0d0d59e28bb8f943efc5b161ff0dbd11be0",
            "title": "Approximate clustering via core-sets",
            "abstract": "In this paper, we show that for several clustering problems one can extract a small set of points, so that using those core-sets enable us to perform approximate clustering efficiently. The surprising property of those core-sets is that their size is independent of the dimension.Using those, we present a (1+ \u03b5)-approximation algorithms for the k-center clustering and k-median clustering problems in Euclidean space. The running time of the new algorithms has linear or near linear dependency on the number of points and the dimension, and exponential dependency on 1/\u03b5 and k. As such, our results are a substantial improvement over what was previously known.We also present some other clustering results including (1+ \u03b5)-approximate 1-cylinder clustering, and k-center clustering with outliers.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2002252",
                    "name": "Mihai Badoiu"
                },
                {
                    "authorId": "1388677326",
                    "name": "Sariel Har-Peled"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "21d141bc7ae0046f4015321e7669f6259339fa17",
            "title": "Maintaining Stream Statistics over Sliding Windows",
            "abstract": "We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1's in the last N elements seen from the stream. We show that, using $O(\\frac{1}{\\epsilon} \\log^2 N)$ bits of memory, we can estimate the number of 1's to within a factor of $1 + \\epsilon$. We also give a matching lower bound of $\\Omega(\\frac{1}{\\epsilon}\\log^2 N)$ memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers and provide matching upper and lower bounds for this more general problem as well. We also show how to efficiently compute the Lp norms ($p \\in [1,2]$) of vectors in the sliding window model using our techniques. Using our algorithm, one can adapt many other techniques to work for the sliding window model with a multiplicative overhead of $O(\\frac{1}{\\epsilon}\\log N)$ in memory and a $1 +\\epsilon$ factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145112445",
                    "name": "Mayur Datar"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                }
            ]
        },
        {
            "paperId": "5f18b9bd211d927e84369c61629c78b8b3221ee9",
            "title": "Explicit constructions of selectors and related combinatorial structures, with applications",
            "abstract": "In this paper we present explicit constructions of several combinatorial objects: selectors [CGR00] and selective families [CGGPR00], pseudo-random generators for proof systems [ABRW00] and fixed waking schedules [GPP00]. As a result, we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks [CGR00] and wake-up problem [GPP00]. We also show application of selectors (and its variants) to explicit construction of test sets for coin-weighting problems [DH00]. The parameters of our constructions come close to the best known non-constructive bounds. The constructions are achieved using a common technique, which could be of use for other problems.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "6b34a81c00d65bfc7997be7f211c0e3cf574a133",
            "title": "Near-optimal sparse fourier representations via sampling",
            "abstract": "(MATH) We give an algorithm for finding a Fourier representation <b>R</b> of <i>B</i> terms for a given discrete signal signal <b>A</b> of length <i>N</i>, such that $\\|\\signal-\\repn\\|_2^2$ is within the factor (1 +\u03b5) of best possible $\\|\\signal-\\repn_\\opt\\|_2^2$. Our algorithm can access <b>A</b> by reading its values on a sample set <b><i>T</i></b> \u2286[0,<i>N</i>), chosen randomly from a (non-product) distribution of our choice, independent of <b>A</b>. That is, we sample non-adaptively. The total time cost of the algorithm is polynomial in <i>B</i> log(<i>N</i>)log(<i>M</i>)\u03b5 (where <i>M</i> is the ratio of largest to smallest numerical quantity encountered), which implies a similar bound for the number of samples.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1795861",
                    "name": "A. Gilbert"
                },
                {
                    "authorId": "144392463",
                    "name": "S. Guha"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "145542622",
                    "name": "M. Strauss"
                }
            ]
        },
        {
            "paperId": "8b4731b0f108a4111005196bc69c34f16d00d94e",
            "title": "Fast mining of massive tabular data via approximate distance computations",
            "abstract": "Tabular data abound in many data stores: traditional relational databases store tables, and new applications also generate massive tabular datasets. We present methods for determining similar regions in massive tabular data. Our methods are for computing the \"distance\" between any two subregions of tabular data: they are approximate, but highly accurate as we prove mathematically, and they are fast, running in time nearly linear in the table size. Our methods are general since these distance computations can be applied to any mining or similarity algorithms that use L/sub p/ norms. A novelty of our distance computation procedures is that they work for any L/sub p/ norms, not only the traditional p = 2 or p = 1, but for all p /spl les/ 2; the choice of p, say fractional p, provides an interesting alternative similarity behavior! We use our algorithms in a detailed experimental study of the clustering patterns in real tabular data obtained from one of AT&T's data stores and show that our methods are substantially faster than straightforward methods while remaining highly accurate, and able to detect interesting patterns by varying the value of p.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                }
            ]
        },
        {
            "paperId": "b4f042e8290dc2399faf376dd5600032f96c78ee",
            "title": "Near-optimal linear-time codes for unique decoding and new list-decodable codes over smaller alphabets",
            "abstract": "We present an explicit construction of linear-time encodable and decodable codes of rate r which can correct a fraction (1&mdash:r&egr;\u03b5)/2 of errors over an alphabet of constant size depending only on \u03b5, for every 0 < r < 1 and arbitrarily small \u03b5> 0. The error-correction performance of these codes is optimal as seen by the Singleton bound (these are \"near-MDS\" codes). Such near-MDS linear-time codes were known for the decoding from erasures [2]; our construction generalizes this to handle errors as well. Concatenating these codes with good, constant-sized binary codes gives a construction of linear-time binary codes which meet the so-called \"Zyablov bound\". In a nutshell, our results match the performance of the previously known explicit constructions of codes that had polynomial time encoding and decoding, but in addition have linear time encoding and decoding algorithms.We also obtain some results for list decoding targeted at the situation when the fraction of errors is very large, namely (1\u2014\u03b5) for an arbitrarily small constant \u03b5 > 0. The previously known constructions of such codes of good rate over constant-sized alphabets either used algebraic-geometric codes and thus suffered from complicated constructions and slow decoding, or as in the recent work of the authors [9], had fast encoding/decoding, but suffered from an alphabet size that was exponential in 1/\u03b5. We present two constructions of such codes with rate close to &OHgr;(\u03b52) over an alphabet of size quasi-polynomial in 1/\u03b5. One of the constructions, at the expense of a slight worsening of the rate, can achieve an alphabet size which is polynomial in 1/\u03b5. It also yields constructions of codes for list decoding from erasures which achieve new trade-offs. In particular, we construct codes of rate close to the optimal &OHgr;(\u03b5) rate which can be efficiently list decoded from a fraction (1\u2014\u03b5) of erasures.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721077",
                    "name": "V. Guruswami"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "c16d3646bdce8b6cf8466115b11c3db483076f52",
            "title": "Dynamic multidimensional histograms",
            "abstract": "Histograms are a concise and flexible way to construct summary structures for large data sets. They have attracted a lot of attention in database research due to their utility in many areas, including query optimization, and approximate query answering. They are also a basic tool for data visualization and analysis.In this paper, we present a formal study of dynamic multidimensional histogram structures over continuous data streams. At the heart of our proposal is the use of a dynamic summary data structure (vastly different from a histogram) maintaining a succinct approximation of the data distribution of the underlying continuous stream. On demand, an accurate histogram is derived from this dynamic data structure. We propose algorithms for extracting such an accurate histogram and we analyze their behavior and tradeoffs. The proposed algorithms are able to provide approximate guarantees about the quality of the estimation of the histograms they extract.We complement our analytical results with a thorough experimental evaluation using real data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1909452",
                    "name": "N. Thaper"
                },
                {
                    "authorId": "144392463",
                    "name": "S. Guha"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                }
            ]
        },
        {
            "paperId": "d4cf029d015ccf16fa80fd580a0bb7f9b9555f27",
            "title": "Fast, small-space algorithms for approximate histogram maintenance",
            "abstract": "(MATH) A vector <b>A</b> of length <i>N</i> is defined implicitly, via a stream of updates of the form \"add 5 to <b>A</b><sub>3</sub>.\" We give a <i>sketching</i> algorithm, that constructs a small <i>sketch</i> from the stream of updates, and a <i>reconstruction</i> algorithm, that produces a <i>B</i>-bucket piecewise-constant representation (histogram) <b>H</b> for <b>A</b> from the sketch, such that ||<b>A\u2014H</b>||&xie;(1+\u03b5)||<b>A\u2014H</b><sub>opt</sub>||, where the error |||<b>A\u2014H</b>|| is either $\\ell_1$ (absolute) or $\\ell_2$ (root-mean-square) error. The time to process a single update, time to reconstruct the histogram, and size of the sketch are each bounded by poly(<i>B</i>,log(<i>N</i>),log||<b>A</b>,1/\u03b5. Our result is obtained in two steps. First we obtain what we call a <i>robust</i> histogram approximation for <b>A</b>, a histogram such that adding a small number of buckets does not help improve the representation quality significantly. From the robust histogram, we cull a histogram of desired accruacy and <i>B</i> buckets in the second step. This technique also provides similar results for Haar wavelet representations, under $\\ell_2$ error. Our results have applications in summarizing data distributions fast and succinctly even in distributed settings.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1795861",
                    "name": "A. Gilbert"
                },
                {
                    "authorId": "144392463",
                    "name": "S. Guha"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1751420",
                    "name": "Y. Kotidis"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "145542622",
                    "name": "M. Strauss"
                }
            ]
        },
        {
            "paperId": "e2ab7f5e24d4492b0a10a5bd5ff4a8b5b15a3133",
            "title": "Evaluating strategies for similarity search on the web",
            "abstract": "Finding pages on the Web that are similar to a query page (Related Pages) is an important component of modern search engines. A variety of strategies have been proposed for answering Related Pages queries, but comparative evaluation by user studies is expensive, especially when large strategy spaces must be searched (e.g., when tuning parameters). We present a technique for automatically evaluating strategies using Web hierarchies, such as Open Directory, in place of user feedback. We apply this evaluation methodology to a mix of document representation strategies, including the use of text, anchor-text, and links. We discuss the relative advantages and disadvantages of the various approaches examined. Finally, we describe how to efficiently construct a similarity index out of our chosen strategies, and provide sample results from our index.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2318767",
                    "name": "Taher H. Haveliwala"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "38666915",
                    "name": "D. Klein"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "f8498cca9c22b150dd48b2ddc23f153854f272ff",
            "title": "Derandomized dimensionality reduction with applications",
            "abstract": "The Johnson-Lindenstrauss lemma provides a way to map a number of points in high-dimensional space into a low-dimensional space, with only a small distortion of the distances between the points. The proofs of the lemma are non-constructive: they show that a random mapping induces small distortions with high probability, but they do not construct the actual mapping. In this paper, we provide a procedure that constructs such a mapping deterministically in time almost linear in the number of distances to preserve times the dimension of the original space. We then use that result (together with Nisan's pseudorandom generator) to obtain an efficient derandomization of several approximation algorithms based on semidefinite programming.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2075698823",
                    "name": "Lars Engebretsen"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1410077373",
                    "name": "R. O'Donnell"
                }
            ]
        },
        {
            "paperId": "fa4e1648020b8fc0698e98978bbd1e72c86a67ab",
            "title": "Approximate nearest neighbor algorithms for Frechet distance via product metrics",
            "abstract": "The Nearest Neighbor Search (NNS) problem is: Given a set P of n points in a metric space X, preprocess P so as to e\u00c6ciently answer queries for nding the point in P closest to a query point q. NNS and its approximate versions are among the most extensively studied problems in the elds of Computational Geometry and Algorithms, resulting in discovery of many e\u00c6cient algorithms. In particular, for the case when the metric spaceX is a low-dimensional Euclidean space l 2 , it is known how to construct data structure for exact [5, 15] or approximate [4, 13, 12, 9] NNS with query time (d+log n). Unfortunately, those data structures require space exponential in d. More recently, several data structures using space (quasi)-polynomial in n and d, and query time sublinear in n, have been discovered for approximate NNS under l1 and l2 [14, 12, 11] and l1 [10] norms. While many metrics of interest for nearest neighbor search are norms, quite a few of them are not. A prominent example of the latter is the Frechet metric. Unlike the norms, Frechet metric is de ned for sequences of points (possibly in nite and of di erent length), not vectors. More speci cally, the Frechet distance between two sequences of points is de ned as a minimum, over all alignments between the two sequences, of the maximum distance between two corresponding points (see Preliminaries for formal de nition). Frechet metric is a natural measure of similarity between sequences of points (e.g., handwritten signatures). It also has been studied in the Computational Geometry literature, e.g., in [2] (see also [3]). Unfortunately, the optimum alignment between any two sequences is not xed (i.e., it could be",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "0904fccc90d6d4c402ed5e3a6f1fa6f61fde13e6",
            "title": "Expander-based constructions of efficiently decodable codes",
            "abstract": "We present several novel constructions of codes which share the common thread of using expander (or expander-like) graphs as a component. The expanders enable the design of efficient decoding algorithms that correct a large number of errors through various forms of \"voting\" procedures. We consider both the notions of unique and list decoding, and in all cases obtain asymptotically good codes which are decodable up to a \"maximum\" possible radius and either: (a) achieve a similar rate as the previously best known codes but come with significantly faster algorithms, or (b) achieve a rate better than any prior construction with similar error-correction properties. Among our main results are: i) codes of rate /spl Omega/(/spl epsi//sup 2/) over constant-sized alphabet that can be list decoded in quadratic time from (1-/spl epsi/) errors; ii) codes of rate /spl Omega/(/spl epsi/) over constant-sized alphabet that can be uniquely decoded from (1/2-/spl epsi/) errors in near-linear time (this matches AG-codes with much faster algorithms); iii) linear-time encodable and decodable binary codes of positive rate (in fact, rate /spl Omega/(/spl epsi//sup 2/)) that can correct up to (1/4-/spl epsi/) fraction errors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721077",
                    "name": "V. Guruswami"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "3b14b7c280b74b2d17f56e4c99f1c847802674bf",
            "title": "Similarity Search on the Web: Evaluation and Scalability Considerations",
            "abstract": "Allowing users to find pages on the web similar to a particular query page is a crucial component of modern search engines. A variety of techniques and approaches exist to support \"Related Pages\" queries. In this paper we discuss shortcomings of previous approaches and present a unifying approach that puts special emphasis on the use of text, both within anchors and surrounding anchors. In the central contribution of our paper, we present a novel technique for automating the evaluation process, allowing us to tune our parameters to maximize the quality of the results. Finally, we show how to scale our approach to millions of web pages, using the established Locality-Sensitive-Hashing technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2318767",
                    "name": "Taher H. Haveliwala"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "38666915",
                    "name": "D. Klein"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "6593e5b0da9a368554b5124db85f4e9c31a48ae9",
            "title": "Algorithmic applications of low-distortion geometric embeddings",
            "abstract": "The author surveys algorithmic results obtained using low-distortion embeddings of metric spaces into (mostly) normed spaces. He shows that low-distortion embeddings provide a powerful and versatile toolkit for solving algorithmic problems. Their fundamental nature makes them applicable in a variety of diverse settings, while their relation to rich mathematical fields (e.g., functional analysis) ensures availability of tools for their construction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "8a5db1a9ab649dc05a839ecaa025ca93f2fdf569",
            "title": "On Approximate Nearest Neighbors under linfinity Norm",
            "abstract": "The nearest neighbor search (NNS) problem is the following: Given a set of n points P={p1, ?, pn} in some metric space X, preprocess P so as to efficiently answer queries which require finding a point in P closest to a query point q?X. The approximate nearest neighbor search (c-NNS) is a relaxation of NNS which allows to return any point within c times the distance to the nearest neighbor (called c-nearest neighbor). This problem is of major and growing importance to a variety of applications. In this paper, we give an algorithm for (4?log1+?log4d?+1)-NNS algorithm in ld\u221e with O(dn1+?logO(1)n) storage and O(dlogO(1)n) query time. Moreover, we obtain an algorithm for 3-NNS for l\u221e with nlogd+1 storage. The preprocessing time is close to linear in the size of the data structure. The algorithm can be also used (after simple modifications) to output the exact nearest neighbor in time bounded by O(dlogO(1)n) plus the number of (4?log1+?log4d?+1)-nearest neighbors of the query point. Building on this result, we also obtain an approximation algorithm for a general class of product metrics. Finally, we show that for any c<3 the c-NNS problem in l\u221e is provably as hard as the subset query problem (also called the partial match problem). This indicates that obtaining a sublinear query time and subexponential (in d) space for c<3 might be hard.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "cd6c11f8678543605b6e063bbc528418a89f3aaf",
            "title": "Reductions among high dimensional proximity problems",
            "abstract": "We present improved running times for a wide range of approximate high dimensional proximity problems. We obtain subquadratic running time for each of these problems. These improved running times are obtained by reduction to Nearest Neighbour queries. The problems we consider in this paper are Approximate Diameter, Approximate Furthest Neighbours, Approximate Discrete Center, Approximate Metric Facility Location, Approximate Bottleneck Matching, and Approximate Minimum Weight Matching.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143638107",
                    "name": "Ashish Goel"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1757762",
                    "name": "Kasturi R. Varadarajan"
                }
            ]
        },
        {
            "paperId": "159fde4497771566c190384dc690b6c84d9ba08f",
            "title": "When crossings count \u2014 approximating the minimum spanning tree",
            "abstract": "In the first part of the paper, we present an (1+\\mu)-approximation algorithm to the minimum-spanning tree of points in a planar arrangement of lines, where the metric is the number of crossings between the spanning tree and the lines. The expected running time is O((n/\\mu^5) alpha^3(n) log^5 n), where \\mu > 0 is a prescribed constant. \nIn the second part of our paper, we show how to embed such a crossing metric, into high-dimensions, so that the distances are preserved. As a result, we can deploy a large collection of subquadratic approximations algorithms \\cite im-anntr-98,giv-rahdg-99 for problems involving points with the crossing metric as a distance function. Applications include matching, clustering, nearest-neighbor, and furthest-neighbor.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388677326",
                    "name": "Sariel Har-Peled"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "813dbeefb304f377d572d0be0f473db342fa704d",
            "title": "Scalable Techniques for Clustering the Web",
            "abstract": "Clustering is one of the most crucial techniques for dealing with the massive amount of information present on the web. Clustering can either be performed once offline, independent of search queries, or performed online on the results of search queries. Our offline approach aims to efficiently cluster similar pages on the web, using the technique of Locality-Sensitive Hashing (LSH), in which web pages are hashed in such a way that similar pages have a much higher probability of collision than dissimilar pages. Our preliminary experiments on the Stanford WebBase have shown that the hash-based scheme can be scaled to millions of urls.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2318767",
                    "name": "Taher H. Haveliwala"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "8dc4edd931773be37410fe96fee46d22c913b099",
            "title": "Finding interesting associations without support pruning",
            "abstract": "Association rule mining has heretofore relied on the condition of high support to do its work efficiently. In particular, the well-known a-priori algorithm is only effective when the only rules of interest are relationships that occur very frequently. However, there are a number of applications, such as data mining, identification of similar Web documents, clustering and collaborative filtering, where the rules of interest have comparatively few instances in the data. In these cases, we must look for highly correlated items, or possibly even causal relationships between infrequent items. We develop a family of algorithms for solving this problem, employing a combination of random sampling and hashing techniques. We provide an analysis of the algorithms developed and conduct experiments on real and synthetic data to obtain a comparative performance analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50784021",
                    "name": "E. Cohen"
                },
                {
                    "authorId": "145112445",
                    "name": "Mayur Datar"
                },
                {
                    "authorId": "48394334",
                    "name": "S. Fujiwara"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                },
                {
                    "authorId": "1742391",
                    "name": "J. Ullman"
                },
                {
                    "authorId": "2154173617",
                    "name": "Cheng Yang"
                }
            ]
        },
        {
            "paperId": "93678d29490903b158086315d35f2747e6fe17c9",
            "title": "Dimensionality reduction techniques for proximity problems",
            "abstract": "In this paper we give approximation algorithms for several proximity problems in high dimensional spaces. In particular, we give the first Las Vegas data structure for (1 + e)nearest neighbor with polynomial space and query time polynomial in dimension d and log n, where n is the database size. We also give a deterministic 3-approximation algorithm with similar bounds; this is the first deterministic constant factor approximation algorithm (with polynomial space) for any norm. For the closest pair problem we give a roughly n I+p time Las Vegas algorithm with approximation factor O(1/plog l/p); this is the first Las Vegas algorithm for this problem. Finally, we show a general reduction from the furthest point problem to the nearest neighbor problem. As a corollary, we improve the running time for the (1 + e)approximate diameter problem from n 2-\u00b0(~2) to n 2-\u00b0(~). Our results are unified by the fact that their key component is a dimensionality reduction technique for Hamming spaces. 1 I n t r o d u c t i o n The proximity problems is a class of geometric problems which involve the notion of a distance between points in a d-dimensional space. For example, the closest pair problem, furthest pair (or diameter) problem and nearest neighbor search all belong to this class. If the dimension d is low, these problems have very efficient solutions [13, 3, 12]. However, the running time and/or space requirements of these algorithms grow exponentially with the dimension. This is unfortunate, since the high-dimensional versions of the above problems are of major and growing importance to a variety of applications, usually involving similarity search or clustering; some examples are information retrievM, image and video databases, vector quantization, data mining and pattern recognition. Therefore, a lot of recent ~ p o r t e d by Stanford Graduate Fellowship and NSF Grant IIS-9811904 research focused on approximate algorithms for these problems [4, 10, 7, 11, 6, 2]; this relaxation enables to overcome the \"curse of dimensional i ty ' . In this paper we present several new results for the aforementioned proximity problems. They are unified by the fact that their key component is a dimensionality reduction technique for Hamming spaces (which we describe in more detail at the end of this Section). O u r r esu l t s . The first problem we address is the cnearest neighbor problem. In this problem the goal is to construct (for a given set P of n points from R d) a data structure, which given a query point q, finds any c-approximate nearest neighbor of q in P; the latter is defined as any point p/whose distance to q is bounded by c times the distance from q to its nearest neighbor in P. Several algorithms have been proposed recently for this problem for the cases when the similarity between points is measured by a dot-product [4], ll or 12 norm [10, 7, 11] and lc~ norm [6]. The intriguing common property of almost all of them [4, 10, 7, 11] is that they are randomized Monte Carlo, i.e. have small probability of returning an incorrect answer (the solution of [6] is deterministic, but unlike other solutions does not allow arbitrarily small approximation error). This situation stands in the contrast with the state of the art in lowdimensional Computat ion Geometry, where virtually almost all randomized algorithms are of Las Vegas type (i.e. whose correctness is always guaranteed). Thus one may ask if randomization, in particular of Monte Carlo type, is an inherent property of algorithms for high-dimensional proximity problems. We mention that from the practical perspective the Las Vegas algorithms offer a significant advantage over the Monte Carlo ones (if their running times is comparable) for the following reason: it was observed [1, 5] tha t the average error incurred by the algorithms is much lower than predicted by theoretical analysis. Therefore, one can significantly speed them up by setting artificiMly high upper bound for the error. However, a Monte Carlo algorithm canno~",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "c982cb13d2a870ca09a2a1f0bf3c602cc528724e",
            "title": "Stable distributions, pseudorandom generators, embeddings and data stream computation",
            "abstract": "In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: we show how to maintain (using only O(log n//spl epsiv//sup 2/) words of storage) a sketch C(p) of a point p/spl isin/l/sub 1//sup n/ under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate |p-q|/sub 1/ up to a factor of (1+/spl epsiv/) with large probability. We obtain another sketch function C' which maps l/sub 1//sup n/ into a normed space l/sub 1//sup m/ (as opposed to C), such that m=m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l/sub 1/ norm we give an explicit embedding of l/sub 2//sup n/ into l/sub l//sup nO(log n)/ with distortion (1+1/n/sup /spl theta/(1)/) and a non-constructive embedding of l/sub 2//sup n/ into l/sub 1//sup O(n)/ with distortion (1+/spl epsiv/) such that the embedding can be represented using only O(n log/sup 2/ n) bits (as opposed to at least n/sup 2/ used by earlier methods).",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "e27eba1a3ba1b29b849e26dba5d40f10aa10f321",
            "title": "Identifying Representative Trends in Massive Time Series Data Sets Using Sketches",
            "abstract": "Many data stores, including scientific and financial databases, business warehouses and network repositories, contain time series data. Time series data depict trends for an observed value e.g., value of a stock, number of bytes sent on a router interface, etc., as a function of time. Analysis of the trends over different time windows is of great interest. In this paper, we formalize problems of identifying various 'representative' trends in time series data. Informally, an interval of observations in a time series is defined to be a representative trend if its distance from other intervals satisfy certain properties, for suitably defined distance functions between time series intervals. Natural trends of interest such as periodic or average trends are examples of representative trends. We present efficient algorithms for analyzing massive time series data sets for representative trends over arbitrary windows of interest. Our algorithms are highly processor and 10 efficient; they are approximate but provide probabilistic guarantees for the approximations achieved. Our approach for identifying representative trends relies on a dimensionality reduction technique that replaces each interval by a 'sketch' which is a low dimensional vector. We present efficient algorithms to construct such sketches using a pool of select sketches that we precompute using polynomial convolutions. Using such sketches, we can compute representative trends accurately. Finally, we present results of a detailed experimental study of our technique on very large real data sets. Our results show that, compared to approaches that determine representative trends exactly, our approach shows significant performance gains with only a small loss in accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                }
            ]
        },
        {
            "paperId": "e63863964fa7bd47e38ea3a97e4561e8ed489392",
            "title": "Mining the stock market (extended abstract): which measure is best?",
            "abstract": "In recent years, there has been a lot of interest in the database community in mining time series data. Surprisingly, little work has been done on verifying which measures are most suitable for mining of a given class of data sets. Such work is of crucial importance, since it enables us to identify similarity measures which are useful in a given context and therefore for which efficient algorithms should be further investigated. Moreover, an accurate evaluation of the performance of even existing algorithms is not possible without a good understanding of the data sets occurring in practice. In this work we attempt to fill this gap by studying similarity measures for clustering of similar stocks (which, of course, is an interesting problem on its own). Our approach is to cluster the stocks according to various measures (including several novel ones) and compare the results to the \u201dgroundtruth\u201d clustering based on the Standard and Poor 500 Index. Our experiments reveal several interesting facts about the similarity measures used for stock-market data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2103841411",
                    "name": "Martin Gavrilov"
                },
                {
                    "authorId": "1838674",
                    "name": "Dragomir Anguelov"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                }
            ]
        },
        {
            "paperId": "ecdb81ade4c8bb2bbfae7fba1e4c2b8fbeb55725",
            "title": "Scalable Techniques for Clustering the Web (Extended Abstract)",
            "abstract": "Clustering is one of the most crucial techniques for dealing with the massive amount of information present on the web. Clustering can either be performed once offline, independent of search queries, or performed online on the results of search queries. Our offline approach aims to efficiently cluster similar pages on the web, using the technique of Locality-Sensitive Hashing (LSH), in which web pages are hashed in such a way that similar pages have a much higher probability of collision than dissimilar pages. Our preliminary experiments on the Stanford WebBase have shown that the hash-based scheme can be scaled to millions of urls.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2318767",
                    "name": "Taher H. Haveliwala"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "16547ff69831b55cf54ef5b5236890a3a2050f12",
            "title": "Stochastic load balancing and related problems",
            "abstract": "We study the problems of makespan minimization (load balancing), knapsack, and bin packing when the jobs have stochastic processing requirements or sizes. If the jobs are all Poisson, we present a two approximation for the first problem using Graham's rule, and observe that polynomial time approximation schemes can be obtained for the last two problems. If the jobs are all exponential, we present polynomial time approximation schemes for all three problems. We also obtain quasi-polynomial time approximation schemes for the last two problems if the jobs are Bernoulli variables.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "143638107",
                    "name": "Ashish Goel"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "1986f2e17392163d7318a874eea0f9a5eecafe5d",
            "title": "Approximate nearest neighbor algorithms for Hausdorff metrics via embeddings",
            "abstract": "Hausdorff metrics are used in geometric settings for measuring the distance between sets of points. They have been used extensively in areas such as computer vision, pattern recognition and computational chemistry. While computing the distance between a single pair of sets under the Hausdorff metric has been well studied, no results are known for the nearest-neighbor problem under Hausdorff metrics. Indeed, no results were known for the nearest-neighbor problem for any metric without a norm structure, of which the Hausdorff is one. We present the first nearest-neighbor algorithm for the Hausdorff metric. We achieve our result by embedding Hausdorff metrics into l/sub /spl infin// and by using known nearest-neighbor algorithms for this target metric. We give upper and lower bounds on the number of dimensions needed for such an l/sub /spl infin// embedding. Our bounds require the introduction of new techniques based on superimposed codes and non-uniform sampling.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1398716192",
                    "name": "Mart\u00edn Farach-Colton"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "2e74388f55f2cc704c4de410578887a53a9433b0",
            "title": "Similarity Search in High Dimensions via Hashing",
            "abstract": "The nearestor near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately, all known techniques for solving this problem fall prey to the \\curse of dimensionality.\" That is, the data structures scale poorly with data dimensionality; in fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should su ce for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points Supported by NAVY N00014-96-1-1221 grant and NSF Grant IIS-9811904. Supported by Stanford Graduate Fellowship and NSF NYI Award CCR-9357849. Supported by ARO MURI Grant DAAH04-96-1-0007, NSF Grant IIS-9811904, and NSF Young Investigator Award CCR9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, and Xerox Corporation. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 25th VLDB Conference, Edinburgh, Scotland, 1999. from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our method gives signi cant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition. Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                }
            ]
        },
        {
            "paperId": "37276d974c82c66f177b67baefc2d293b7c471c3",
            "title": "Tree pattern matching and subset matching in deterministic O(n log3 n)-time",
            "abstract": "The main goal of this paper is to give an O(nlog3 n) time deterministic algorithm for the the Subset Matching problem. This immediately yields an algorithm of the same efficiency for the Tree Pattern Matching problem. We also give an O(n log3 n/ log log n) time randomized algorithm for these problems. Finally, we give a O(nlog n(z + logn)) time deterministic algorithm for a useful specialization of the Subset Matching problem in which all sets are intervals of a given length Z.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144021633",
                    "name": "R. Cole"
                },
                {
                    "authorId": "145250487",
                    "name": "R. Hariharan"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "428baff991816f7376b5b37704fc55f0501a06cc",
            "title": "Interpolation of symmetric functions and a new type of combinatorial design",
            "abstract": "Let S be a subset of boolean functions f : {0, 1)\u201d + {0, 1). We say that a set X of binary vectors of length n forms an interpolation set for S if for any function f E S the values off on X uniquely determines f. Small interpolation sets are of interest in leaning theory and combinatorial group testing. In this paper we consider the class S; of functions which are symmetric on their sets of (at most k) relevant variables. Our tist result is a proof of existence of an interpolation set for S; of size O(k\u2019 log k log n This comes close to the known lower bound of S2(k 2 log n/ log k). Our second result is an explicit construction of an interpolation set of size O(k\u2019 log\u2019 n), which also yields an efficient algorithm for reconstructing f from its values on X. No interpolation set for S; of size subexponential in k was previously lmown.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "450dd05897675d11512833537ea749ac75a5305f",
            "title": "Geometric matching under noise: combinatorial bounds and algorithms",
            "abstract": "In geometric pattern matching, we are given two sets of points P and Q in d dimensions, and the problem is to determine the rigid transformation that brings P closest to Q. under some distance measure. More generally, each point can be modelled as a ball of small radius, and we mav wish to find a transformation approximating the closest d&axe between P and Q. This problem has many applications in domains such as computer vision and computational chemistry In this paper we present improved algorithms for this problem, by allowing the running time of our algorithms to depend not only on n, (the number of points in the sets), but also on A, the diameter of the point set. The deoendence on A also allAws us to effectively *process point se& that occur in practice, where diameters tend to be small ([EVW91]). Our algorithms are also simple to implement, in contrast to much of the earlier work. To obtain the above-mentioned results, we introduce a novel discretization technique to reduce geometric pattern matching to combinatorial pattern matching. In addition, we address various generalizations of the classical problem first posed by Erd8s: \u201cGiven a set, of n points in the plane, how many pairs of points can be exactly a unit distance apart?\u201c. The combinatorial bounds we prove enable us to obtain improved results for geometric pattern matching and may have other applications.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                },
                {
                    "authorId": "1747652",
                    "name": "Suresh Venkatasubramanian"
                }
            ]
        },
        {
            "paperId": "4b56d940f156407f8d1b943123430e01c9f58ee7",
            "title": "Efficient regular data structures and algorithms for location and proximity problems",
            "abstract": "Investigates data structures obtained by a recursive partitioning of the input domain into regions of equal size. One of the most well-known examples of such a structure is the quadtree, which is used in this paper as a basis for more complex data structures; we also provide multidimensional versions of the stratified tree of P. van Emde Boas (1997). We show that, under the assumption that the input points have limited precision (i.e. are drawn from an integer grid of size u), these data structures yield efficient solutions to many important problems. In particular, they allow us to achieve O(log log u) time per operation for finding the dynamic approximate nearest neighbor (under insertions and deletions) and the exact online closest pair (under insertions only) in any constant dimension. They allow O(log log u) point location in a given planar shape or in its expansion (dilation by a ball of a given radius). Finally, we provide a linear-time (optimal) algorithm for computing the expansion of a shape represented by a quadtree. This result shows that the spatial order imposed by this regular data structure is sufficient to optimize the dilation by a ball operation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1767364",
                    "name": "A. Amir"
                },
                {
                    "authorId": "1740132",
                    "name": "A. Efrat"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "1719385",
                    "name": "H. Samet"
                }
            ]
        },
        {
            "paperId": "54672a92b5312d95c125c2e84b459a1ed2fc041d",
            "title": "Geometric pattern matching: a performance study",
            "abstract": "In this paper, we undertake a performance study of some recent algorithms for geometric pattern matching. These algorithms cover two general paradigms for pattern matching; alignment and combinatorial pattern matching. We present analytical and empirical evaluations of these schemes. Our results indicate that a proper implementation of an alignmentbased method outperforms other (often asymptotically better) approaches.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2103841411",
                    "name": "Martin Gavrilov"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                },
                {
                    "authorId": "1747652",
                    "name": "Suresh Venkatasubramanian"
                }
            ]
        },
        {
            "paperId": "76b68dd26e0e2cabcbd376deb76e998eeb323d7a",
            "title": "A sublinear time approximation scheme for clustering in metric spaces",
            "abstract": "The metric 2-clustering problem is defined as follows: given a metric (or weighted graph) (X,d), partition X into two sets S(1) and S(2) in order to minimize the value of /spl Sigma//sub i//spl Sigma//sub {u,v}/spl sub/S(i)/d(u,v). In this paper, we show an approximation scheme for this problem.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "a523997047781ced08921427d28a4714e13bc0f9",
            "title": "A small approximately min-wise independent family of hash functions",
            "abstract": "In this paper we give a construction of a small approximately min-wise independent family of hash functions, i.e., a family of hash functions such that for any set of arguments X and x?X, the probability that the value of a random function from that family on x will be the smallest among all values of that function on X is roughly 1/|X|. The number of bits needed to represent each function is O(logn\u00b7log1/?). This construction gives a solution to the main open problem of A. Broder et al. (in \u201cSTOC'98\u201d).",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "e3216147540994248b5f44e82b7dac93e19cbbb6",
            "title": "Sublinear time algorithms for metric space problems",
            "abstract": "In this paper we give approximation algorithms for the following problems on metric spaces: Furthest Pair, kmedian, Minimum Routing Cost Spanning Tree, Multiple Sequence Alignment, Maximum Traveling Salesman Problem, Maximum Spanning Tree and Average Distance. The key property of our algorithms is that their running time is linear in the number of metric space points. As the full specification o\u2018f an n-point metric space is of size Q(n\u2019), the complexity of our algorithms is sublinear with respect to the input size. All previous algorithms (exact or approximate) for the problems we consider have running time n(n\u2019). We believe that OUT techniques can be applied to get similar bounds for other problems.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "3261bb81085f59efae1e1c72453c47daaee777ac",
            "title": "Enhanced hypertext categorization using hyperlinks",
            "abstract": "A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented with pre-classified samples from Yahoo!1 and the US Patent Database2. In previous work, we developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained. This classifier misclassified 36% of the patents, indicating that classifying hypertext can be more difficult than classifying text. Naively using terms in neighboring documents increased error to 38%; our hypertext classifier reduced it to 21%. Results with the Yahoo! sample were more dramatic: the text classifier showed 68% error, whereas our hypertext classifier reduced this to only 21%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40941894",
                    "name": "Soumen Chakrabarti"
                },
                {
                    "authorId": "1786444",
                    "name": "B. Dom"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "4bf603943d9b4ccc563a2c6a2ca4679485012dd0",
            "title": "Faster algorithms for string matching problems: matching the convolution bound",
            "abstract": "In this paper we give a randomized O(nlogn)-time algorithm for the string matching with don't cares problem. This improves the Fischer-Paterson bound from 1974 and answers the open problem posed (among others) by Weiner and Galil. Using the same technique, we give an O(nlogn)-time algorithm for other problems, including subset matching, tree pattern matching, (general) approximate threshold matching and point set matching. As this bound essentially matches the complexity of computing of the fast Fourier transform which is the only known technique for solving problems of this type, it is likely that the algorithms are in fact optimal. Additionally the technique used for the threshold matching problem can be applied to the on-line version of this problem, in which we are allowed to preprocess the text and require to process the pattern in time sublinear in the text length. This result involves an interesting variant of the Karp-Rabin fingerprint method in which hash functions are locality-sensitive, i.e. the probability of collision of two words depends on the distance between them.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "ed623d49198eb1036c47a09a5560c0a82e5cb1ca",
            "title": "On approximate nearest neighbors in non-Euclidean spaces",
            "abstract": "The nearest neighbor search (NNS) problem is the following: Given a set of n points P={p/sub 1/,...,p/sub n/} in some metric space X, preprocess P so as to efficiently answer queries which require finding a point in P closest to a query point q/spl isin/X. The approximate nearest neighbor search (c-NNS) is a relaxation of NNS which allows to return any point within c times the distance to the nearest neighbor (called c-nearest neighbor). This problem is of major and growing importance to a variety of applications. In this paper we give an algorithm for (4log/sub 1+/spl rho//log4d+3)-NNS algorithm in l/sub /spl infin///sup d/ with O(dn/sup 1+/spl rho//logn) storage and O(dlogn) query time. In particular this yields the first algorithm for O(1)-NNS for l/sub /spl infin// with subexponential storage. The preprocessing time is linear in the size of the data structure. The algorithm can be also used (after simple modifications) to output the exact nearest neighbor in time bounded bounded O(dlogn) plus the number of (4log/sub 1+/spl rho//log4d+3)-nearest neighbors of the query point. Building on this result, we also obtain an approximation algorithm for a general class of product metrics. Finally: we show that for any c<3 the c-NNS problem in l/sub /spl infin// is provably hard for a version of the indexing model introduced by Hellerstein et al. (1997).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "7a54a395f859ab332cfe48e63b3ba6d25559a8f5",
            "title": "Probabilistic analysis for combinatorial functions of moving points",
            "abstract": "We initiate a probabilistic study of configuration functions of moving points. In our probabilistic model, a particle is given an initiaf position and a velocity drawn independently at random from the same distribution D. We show that if n particles are drawn independently at random from the uniform distribution on the square, their convex hull undergoes El(logz n) combinatorial changes in expectation, their Voronoi diagram undergoes e(n312 ) combinatorial changes, and their closest pair undergoes El(n) combinatorial changes.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2152830040",
                    "name": "Li Zhang"
                },
                {
                    "authorId": "2000457",
                    "name": "H. Devarajan"
                },
                {
                    "authorId": "32556251",
                    "name": "Julien Basch"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "8b2f2d92642846540f7114ed3d43f54851e252ce",
            "title": "Deterministic superimposed coding with applications to pattern matching",
            "abstract": "A superimposed code is a set of binary vectors having the property that no vector is contained in a boolean sum (i.e. bitwise OR) of a small number of others. Such codes are used in information retrieval for constructing so-called signature files; they also have applications in other areas. In this paper we introduce a new notion of data-dependent superimposed codes and give a deterministic algorithm for constructing short such codes. We then show that these codes can be used to achieve an almost optimal de-randomization of several pattern matching algorithms, including the almost-linear algorithm for tree pattern matching developed recently. Thus, we give the first almost-linear time deterministic algorithms for these problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                }
            ]
        },
        {
            "paperId": "cda964bcf343437cf9f17d4306a5e653258ddde9",
            "title": "Locality-preserving hashing in multidimensional spaces",
            "abstract": "We consider localitg-preserving hashing \u2014 in which adjacent points in the domain are mapped to adjacent or nearlyadjacent points in the range \u2014 when the domain is a ddimensional cube. This problem has applications to highdimensional search and multimedia indexing. We show that simple and natural classes of hash functions are provably good for this problem. We complement this with lower bounds suggesting that our results are essentially the best possible.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                },
                {
                    "authorId": "145503401",
                    "name": "P. Raghavan"
                },
                {
                    "authorId": "1737804",
                    "name": "S. Vempala"
                }
            ]
        },
        {
            "paperId": "4617984ccec73169531879017da0b96030529f91",
            "title": "Fast estimation of diameter and shortest paths (without matrix multiplication)",
            "abstract": "In the recent past, there has been considerable progress in devising algorithms for the all-pairs shortest paths (APSP) problem running in time significantly smaller than the obvious time bound of O(n3). Unfortunately, all the new algorithms are based on fast matrix multiplication algorithms that are notoriously impractical. Our work is motivated by the goal of devising purely combinatorial algorithms that match these improved running times. Our results come close to achieving this goal, in that we present algorithms with a small additive error in the length of the paths obtained. Our algorithms are easy to implement, have the desired property of being combinatorial in nature, and the hidden constants in the running time bound are fairly small. \nOur main result is an algorithm which solves the APSP problem in unweighted, undirected graphs with an additive error of 2 in time $O(n^{2.5}\\sqrt{\\log n})$. This algorithm returns actual paths and not just the distances. In addition, we give more efficient algorithms with running time {\\footnotesize $O(n^{1.5} \\sqrt{k \\log n} + n^2 \\log^2 n)$} for the case where we are only required to determine shortest paths between k specified pairs of vertices rather than all pairs of vertices. The starting point for all our results is an $O(m \\sqrt{n \\log n})$ algorithm for distinguishing between graphs of diameter 2 and 4, and this is later extended to obtaining a ratio 2/3 approximation to the diameter in time $O(m \\sqrt{n \\log n} + n^2 \\log n)$. Unlike in the case of APSP, our results for approximate diameter computation can be extended to the case of directed graphs with arbitrary positive real weights on the edges.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2128207",
                    "name": "Donald Aingworth"
                },
                {
                    "authorId": "1733122",
                    "name": "C. Chekuri"
                },
                {
                    "authorId": "1688317",
                    "name": "P. Indyk"
                },
                {
                    "authorId": "84095744",
                    "name": "R. Motwani"
                }
            ]
        }
    ]
}