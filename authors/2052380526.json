{
    "authorId": "2052380526",
    "papers": [
        {
            "paperId": "3e5ca56f4d289ff8ae39a1b3edfb0b5eb3298427",
            "title": "The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default",
            "abstract": "In recent years fairness in machine learning (ML) has emerged as a highly active area of research and development. Most define fairness in simple terms, where fairness means reducing gaps in performance or outcomes between demographic groups while preserving as much of the accuracy of the original system as possible. This oversimplification of equality through fairness measures is troubling. Many current fairness measures suffer from both fairness and performance degradation, or\"levelling down,\"where fairness is achieved by making every group worse off, or by bringing better performing groups down to the level of the worst off. When fairness can only be achieved by making everyone worse off in material or relational terms through injuries of stigma, loss of solidarity, unequal concern, and missed opportunities for substantive equality, something would appear to have gone wrong in translating the vague concept of 'fairness' into practice. This paper examines the causes and prevalence of levelling down across fairML, and explore possible justifications and criticisms based on philosophical and legal theories of equality and distributive justice, as well as equality law jurisprudence. We find that fairML does not currently engage in the type of measurement, reporting, or analysis necessary to justify levelling down in practice. We propose a first step towards substantive equality in fairML:\"levelling up\"systems by design through enforcement of minimum acceptable harm thresholds, or\"minimum rate constraints,\"as fairness constraints. We likewise propose an alternative harms-based framework to counter the oversimplified egalitarian framing currently dominant in the field and push future discussion more towards substantive equality opportunities and away from strict egalitarianism by default. N.B. Shortened abstract, see paper for full abstract.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3127701",
                    "name": "B. Mittelstadt"
                },
                {
                    "authorId": "12806133",
                    "name": "Sandra Wachter"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "4e62a5c8ab3529425ed3a7140d59e3134ac8377b",
            "title": "Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning",
            "abstract": "Recent years have seen a surge of interest in learning high-level causal representations from low-level image pairs under interventions. Yet, existing efforts are largely limited to simple synthetic settings that are far away from real-world problems. In this paper, we present Causal Triplet, a causal representation learning benchmark featuring not only visually more complex scenes, but also two crucial desiderata commonly overlooked in previous works: (i) an actionable counterfactual setting, where only certain object-level variables allow for counterfactual observations whereas others do not; (ii) an interventional downstream task with an emphasis on out-of-distribution robustness from the independent causal mechanisms principle. Through extensive experiments, we find that models built with the knowledge of disentangled or object-centric representations significantly outperform their distributed counterparts. However, recent causal representation learning methods still struggle to identify such latent structures, indicating substantial challenges and opportunities for future work. Our code and datasets will be available at https://sites.google.com/view/causaltriplet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14772333",
                    "name": "Yuejiang Liu"
                },
                {
                    "authorId": "3304525",
                    "name": "Alexandre Alahi"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                },
                {
                    "authorId": "2186406615",
                    "name": "Max Horn"
                },
                {
                    "authorId": "52306341",
                    "name": "Dominik Zietlow"
                },
                {
                    "authorId": "1707625",
                    "name": "B. Scholkopf"
                },
                {
                    "authorId": "2265580622",
                    "name": "Francesco Locatello"
                }
            ]
        },
        {
            "paperId": "535c0642aa2b5178317f082d04f1eb6a37c86bc9",
            "title": "Evaluating the Fairness of Discriminative Foundation Models in Computer Vision",
            "abstract": "We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI\u2019s CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes over ten diverse datasets. We find that fair PCA, a post-processing method for fair representations, works very well for debiasing in most of the aforementioned tasks while incurring only minor loss of performance. However, different debiasing approaches vary in their effectiveness depending on the task. Hence, one should choose the debiasing approach depending on the specific use case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064705296",
                    "name": "Junaid Ali"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "39798982",
                    "name": "F. Wenzel"
                },
                {
                    "authorId": "3207499",
                    "name": "Kailash Budhathoki"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "ed0bf9fe699a6f426f18ea92fe96e368c3d95888",
            "title": "When do Minimax-fair Learning and Empirical Risk Minimization Coincide?",
            "abstract": "Minimax-fair machine learning minimizes the error for the worst-off group. However, empirical evidence suggests that when sophisticated models are trained with standard empirical risk minimization (ERM), they often have the same performance on the worst-off group as a minimax-trained model. Our work makes this counter-intuitive observation concrete. We prove that if the hypothesis class is sufficiently expressive and the group information is recoverable from the features, ERM and minimax-fairness learning formulations indeed have the same performance on the worst-off group. We provide additional empirical evidence of how this observation holds on a wide range of datasets and hypothesis classes. Since ERM is fundamentally easier than minimax optimization, our findings have implications on the practice of fair machine learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "3144230",
                    "name": "R. Chunara"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "06ed4b4bf0ba037b94c67c8233746cdc33dae388",
            "title": "Measuring Fairness of Rankings under Noisy Sensitive Information",
            "abstract": "Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2043975",
                    "name": "Azin Ghazimatin"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                },
                {
                    "authorId": "2034349",
                    "name": "Ziawasch Abedjan"
                },
                {
                    "authorId": "1411129857",
                    "name": "Jacek Golebiowski"
                }
            ]
        },
        {
            "paperId": "1f067c408c517d4b73afe1edc7f1c028f965ef84",
            "title": "Counterfactual Models for Fair and Adequate Explanations",
            "abstract": "Recent efforts have uncovered various methods for providing explanations that can help interpret the behavior of machine learning programs. Exact explanations with a rigorous logical foundation provide valid and complete explanations, but they have an epistemological problem: they are often too complex for humans to understand and too expensive to compute even with automated reasoning methods. Interpretability requires good explanations that humans can grasp and can compute. We take an important step toward specifying what good explanations are by analyzing the epistemically accessible and pragmatic aspects of explanations. We characterize sufficiently good, or fair and adequate, explanations in terms of counterfactuals and what we call the conundra of the explainee, the agent that requested the explanation. We provide a correspondence between logical and mathematical formulations for counterfactuals to examine the partiality of counterfactual explanations that can hide biases; we define fair and adequate explanations in such a setting. We provide formal results about the algorithmic complexity of fair and adequate explanations. We then detail two sophisticated counterfactual models, one based on causal graphs, and one based on transport theories. We show transport based models have several theoretical advantages over the competition as explanation frameworks for machine learning algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1916126",
                    "name": "Nicholas M. Asher"
                },
                {
                    "authorId": "2113689649",
                    "name": "Lucas de Lara"
                },
                {
                    "authorId": "2331518",
                    "name": "Soumya Paul"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "4b43903dadb79d3d00b793d41baefb23afe0d5bd",
            "title": "The Concentration-after-Personalisation Index (CAPI): Governing effects of personalisation using the example of targeted online advertising",
            "abstract": "Firms are increasingly personalising their offers and services, leading to an ever finer-grained segmentation of consumers online. Targeted online advertising and online price discrimination are salient examples of this development. While personalisation's overall effects on consumer welfare are expectably ambiguous, it can lead to concentration in the distribution of advertising and commercial offers. Constellations are possible in which a market is generally open to competition, but the targeted consumer is only made aware of one possible seller. For the consumer, such a market could effectively resemble a monopoly. We call such extreme cases \u2018targeting pockets\u2019. Competition-law metrics such as the Herfindahl\u2013Hirschman Index and traditional means of public oversight of adverts would not detect this concentration. We, therefore, suggest a novel metric, the Concentration-after-Personalisation Index (CAPI). The CAPI treats every consumer as a separate \u2018market\u2019, computes a measure of concentration for personalised adverts and offers for each individual consumer separately, and then averages the result to measure the exposure experienced by an average consumer. We demonstrate how the CAPI can serve as a monitoring tool for regulators and auditors and thus help to enforce existing consumer law as well as proposed new regulations such as the European Union's Digital Services Act and its Artificial Intelligence Act. We further show how adding noise via randomly distributed non-personalised adverts can dilute the potential harm of overly concentrated personalisation. We demonstrate how the CAPI can identify the optimal degree of added noise, balancing the protection of consumer choice with the economic interests of advertisers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "137878515",
                    "name": "Johann Laux"
                },
                {
                    "authorId": "23985569",
                    "name": "F. Stephany"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                },
                {
                    "authorId": "12806133",
                    "name": "Sandra Wachter"
                },
                {
                    "authorId": "3127701",
                    "name": "B. Mittelstadt"
                }
            ]
        },
        {
            "paperId": "74270eb578c75d5bdd9c89b047c9f211bd7e15be",
            "title": "Are Two Heads the Same as One? Identifying Disparate Treatment in Fair Neural Networks",
            "abstract": "We show that deep networks trained to satisfy demographic parity often do so through a form of race or gender awareness, and that the more we force a network to be fair, the more accurately we can recover race or gender from the internal state of the network. Based on this observation, we investigate an alternative fairness approach: we add a second classification head to the network to explicitly predict the protected attribute (such as race or gender) alongside the original task. After training the two-headed network, we enforce demographic parity by merging the two heads, creating a network with the same architecture as the original network. We establish a close relationship between existing approaches and our approach by showing (1) that the decisions of a fair classifier are well-approximated by our approach, and (2) that an unfair and optimally accurate classifier can be recovered from a fair classifier and our second head predicting the protected attribute. We use our explicit formulation to argue that the existing fairness approaches, just as ours, demonstrate disparate treatment and that they are likely to be unlawful in a wide range of scenarios under US law.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "147521198",
                    "name": "Michael Lohaus"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "9557137",
                    "name": "Francesco Locatello"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "2bc7a72795b692c4ac3175e6b55f2ffe3a4bfe1b",
            "title": "Bias Preservation in Machine Learning: The Legality of Fairness Metrics Under EU Non-Discrimination Law",
            "abstract": "Western societies are marked by diverse and extensive biases and inequality that are unavoidably embedded in the data used to train machine learning. Algorithms trained on biased data will, without intervention, produce biased outcomes and increase the inequality experienced by historically disadvantaged groups. Recognising this problem, much work has emerged in recent years to test for bias in machine learning and AI systems using various fairness and bias metrics. Often these metrics address technical bias but ignore the underlying causes of inequality. In this paper we make three contributions. First, we assess the compatibility of fairness metrics used in machine learning against the aims and purpose of EU non-discrimination law. We show that the fundamental aim of the law is not only to prevent ongoing discrimination, but also to change society, policies, and practices to \u2018level the playing field\u2019 and achieve substantive rather than merely formal equality. Based on this, we then propose a novel classification scheme for fairness metrics in machine learning based on how they handle pre-existing bias and thus align with the aims of non-discrimination law. Specifically, we distinguish between \u2018bias preserving\u2019 and \u2018bias transforming\u2019 fairness metrics. Our classification system is intended to bridge the gap between non-discrimination law and decisions around how to measure fairness in machine learning and AI in practice. Finally, we show that the legal need for justification in cases of indirect discrimination can impose additional obligations on developers, deployers, and users that choose to use bias preserving fairness metrics when making decisions about individuals because they can give rise to prima facie discrimination. To achieve substantive equality in practice, and thus meet the aims of the law, we instead recommend using bias transforming metrics. To conclude, we provide concrete recommendations including a user-friendly checklist for choosing the most appropriate fairness metric for uses of machine learning and AI under EU non-discrimination law.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12806133",
                    "name": "Sandra Wachter"
                },
                {
                    "authorId": "3127701",
                    "name": "B. Mittelstadt"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "9ff848c998275e3d3d13b741c682e07d60751b0b",
            "title": "Pairwise Fairness for Ordinal Regression",
            "abstract": "We initiate the study of fairness for ordinal regression. We adapt two fairness notions previously considered in fair ranking and propose a strategy for training a predictor that is approximately fair according to either notion. Our predictor has the form of a threshold model, composed of a scoring function and a set of thresholds, and our strategy is based on a reduction to fair binary classification for learning the scoring function and local search for choosing the thresholds. We provide generalization guarantees on the error and fairness violation of our predictor, and we illustrate the effectiveness of our approach in extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "2397253",
                    "name": "S. Samadi"
                },
                {
                    "authorId": "2625318",
                    "name": "M. B. Zafar"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        }
    ]
}