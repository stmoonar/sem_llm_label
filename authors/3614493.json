{
    "authorId": "3614493",
    "papers": [
        {
            "paperId": "05ed60d669c60f078ea43c9d2cbed86c0f8e5bac",
            "title": "Explain Like I'm Five: Using LLMs to Improve PDE Surrogate Models with Text",
            "abstract": "Solving Partial Differential Equations (PDEs) is ubiquitous in science and engineering. Computational complexity and difficulty in writing numerical solvers has motivated the development of machine learning techniques to generate solutions quickly. Many existing methods are purely data driven, relying solely on numerical solution fields, rather than known system information such as boundary conditions and governing equations. However, the recent rise in popularity of Large Language Models (LLMs) has enabled easy integration of text in multimodal machine learning models. In this work, we use pretrained LLMs to integrate various amounts known system information into PDE learning. Our multimodal approach significantly outperforms our baseline model, FactFormer, in both next-step prediction and autoregressive rollout performance on the 2D Heat, Burgers, Navier-Stokes, and Shallow Water equations. Further analysis shows that pretrained LLMs provide highly structured latent space that is consistent with the amount of system information provided through text.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "1753617971",
                    "name": "Cooper Lorsung"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        },
        {
            "paperId": "0789ab8a994e6de7a1971da91b7c028fccd8e90e",
            "title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing",
            "abstract": "Industry 4.0 has revolutionized manufacturing by driving digitalization and shifting the paradigm toward additive manufacturing (AM). Fused Deposition Modeling (FDM), a key AM technology, enables the creation of highly customized, cost-effective products with minimal material waste through layer-by-layer extrusion, posing a significant challenge to traditional subtractive methods. However, the susceptibility of material extrusion techniques to errors often requires expert intervention to detect and mitigate defects that can severely compromise product quality. While automated error detection and machine learning models exist, their generalizability across diverse 3D printer setups, firmware, and sensors is limited, and deep learning methods require extensive labeled datasets, hindering scalability and adaptability. To address these challenges, we present a process monitoring and control framework that leverages pre-trained Large Language Models (LLMs) alongside 3D printers to detect and address printing defects. The LLM evaluates print quality by analyzing images captured after each layer or print segment, identifying failure modes and querying the printer for relevant parameters. It then generates and executes a corrective action plan. We validated the effectiveness of the proposed framework in identifying defects by comparing it against a control group of engineers with diverse AM expertise. Our evaluation demonstrated that LLM-based agents not only accurately identify common 3D printing errors, such as inconsistent extrusion, stringing, warping, and layer adhesion, but also effectively determine the parameters causing these failures and autonomously correct them without any need for human intervention.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254271338",
                    "name": "Yayati Jadhav"
                },
                {
                    "authorId": "2298756328",
                    "name": "P. Pak"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        },
        {
            "paperId": "1e8ecf48912bf9fae153fee6be55e729e52c7c66",
            "title": "Multi-Peptide: Multimodality Leveraged Language-Graph Learning of Peptide Properties",
            "abstract": "Peptides are essential in biological processes and therapeutics. In this study, we introduce Multi-Peptide, an innovative approach that combines transformer-based language models with Graph Neural Networks (GNNs) to predict peptide properties. We combine PeptideBERT, a transformer model tailored for peptide property prediction, with a GNN encoder to capture both sequence-based and structural features. By employing Contrastive Language-Image Pre-training (CLIP), Multi-Peptide aligns embeddings from both modalities into a shared latent space, thereby enhancing the model's predictive accuracy. Evaluations on hemolysis and nonfouling datasets demonstrate Multi-Peptide's robustness, achieving state-of-the-art 86.185% accuracy in hemolysis prediction. This study highlights the potential of multimodal learning in bioinformatics, paving the way for accurate and reliable predictions in peptide-based research and applications.",
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2310233274",
                    "name": "Srivathsan Badrinarayanan"
                },
                {
                    "authorId": "2120957111",
                    "name": "Chakradhar Guntuboina"
                },
                {
                    "authorId": "2101395648",
                    "name": "P. Mollaei"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        },
        {
            "paperId": "208a10f7fe9e5aa859a3e285f88547e696bc1181",
            "title": "Pretraining Strategies for Structure Agnostic Material Property Prediction",
            "abstract": "In recent years, machine learning (ML), especially graph neural network (GNN) models, has been successfully used for fast and accurate prediction of material properties. However, most ML models rely on relaxed crystal structures to develop descriptors for accurate predictions. Generating these relaxed crystal structures can be expensive and time-consuming, thus requiring an additional processing step for models that rely on them. To address this challenge, structure-agnostic methods have been developed, which use fixed-length descriptors engineered based on human knowledge about the material. However, the fixed-length descriptors are often hand-engineered and require extensive domain knowledge and generally are not used in the context of learnable models which are known to have a superior performance. Recent advancements have proposed learnable frameworks that can construct representations based on stoichiometry alone, allowing the flexibility of using deep learning frameworks as well as leveraging structure-agnostic learning. In this work, we propose three different pretraining strategies that can be used to pretrain these structure-agnostic, learnable frameworks to further improve the downstream material property prediction performance. We incorporate strategies such as self-supervised learning (SSL), fingerprint learning (FL), and multimodal learning (ML) and demonstrate their efficacy on downstream tasks for the Roost architecture, a popular structure-agnostic framework. Our results show significant improvement in small data sets and data efficiency in the larger data sets, underscoring the potential of our pretrain strategies that effectively leverage unlabeled data for accurate material property prediction.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2236697209",
                    "name": "Hongshuo Huang"
                },
                {
                    "authorId": "1573570190",
                    "name": "Rishikesh Magar"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        },
        {
            "paperId": "2174286ae1e3fa3537b8cfa602173f8d6222c225",
            "title": "Large Language Model Agent as a Mechanical Designer",
            "abstract": "Conventional mechanical design paradigms rely on experts systematically refining concepts through experience-guided modification and FEA to meet specific requirements. However, this approach can be time-consuming and heavily dependent on prior knowledge and experience. While numerous machine learning models have been developed to streamline this intensive and expert-driven iterative process, these methods typically demand extensive training data and considerable computational resources. Furthermore, methods based on deep learning are usually restricted to the specific domains and tasks for which they were trained, limiting their applicability across different tasks. This creates a trade-off between the efficiency of automation and the demand for resources. In this study, we present a novel approach that integrates pre-trained LLMs with a FEM module. The FEM module evaluates each design and provides essential feedback, guiding the LLMs to continuously learn, plan, generate, and optimize designs without the need for domain-specific training. We demonstrate the effectiveness of our proposed framework in managing the iterative optimization of truss structures, showcasing its capability to reason about and refine designs according to structured feedback and criteria. Our results reveal that these LLM-based agents can successfully generate truss designs that comply with natural language specifications with a success rate of up to 90%, which varies according to the applied constraints. By employing prompt-based optimization techniques we show that LLM based agents exhibit optimization behavior when provided with solution-score pairs to iteratively refine designs to meet specifications. This ability of LLM agents to produce viable designs and optimize them based on their inherent reasoning capabilities highlights their potential to develop and implement effective design strategies autonomously.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254271338",
                    "name": "Yayati Jadhav"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        },
        {
            "paperId": "2e53874f0767ab0ca86e8d711790bdcf475ba876",
            "title": "BeadSight: An Inexpensive Tactile Sensor Using Hydro-Gel Beads",
            "abstract": "In robotic manipulation, tactile sensors are indispensable, especially when dealing with soft objects, objects of varying dimensions, or those out of the robot's direct line of sight. Traditional tactile sensors often grapple with challenges related to cost and durability. To address these issues, our study introduces a novel approach to visuo-tactile sensing with an emphasis on economy and replacablity. Our proposed sensor, BeadSight, uses hydro-gel beads encased in a vinyl bag as an economical, easily replaceable sensing medium. When the sensor makes contact with a surface, the deformation of the hydrogel beads is observed using a rear camera. This observation is then passed through a U-net Neural Network to predict the forces acting on the surface of the bead bag, in the form of a pressure map. Our results show that the sensor can accurately predict these pressure maps, detecting the location and magnitude of forces applied to the surface. These abilities make BeadSight an effective, inexpensive, and easily replaceable tactile sensor, ideal for many robotics applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186052477",
                    "name": "Abraham George"
                },
                {
                    "authorId": "2302811437",
                    "name": "Yibo Chen"
                },
                {
                    "authorId": "2224846836",
                    "name": "Atharva Dikshit"
                },
                {
                    "authorId": "2298756328",
                    "name": "P. Pak"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        },
        {
            "paperId": "3108bbb3d6b0345af4fa68568f7c3437c2d18a21",
            "title": "Pretraining Strategy for Neural Potentials",
            "abstract": "We propose a masked pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water and small organic molecule systems. GNNs are pretrained by recovering the spatial information of masked-out atoms from molecules selected with certain ratios and then transferred and fine-tuned on atomic force fields. Through such pretraining, GNNs learn meaningful prior about the structural and underlying physical information of molecule systems that are useful for downstream tasks. With comprehensive experiments and ablation studies, we show that the proposed method improves both the accuracy and convergence speed of GNNs compared to their counterparts trained from scratch or with other pretraining techniques. This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2288259287",
                    "name": "Zehua Zhang"
                },
                {
                    "authorId": "1752871628",
                    "name": "Zijie Li"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        },
        {
            "paperId": "33a70c6454927fea54f9cd41b20e0c78b6979a3f",
            "title": "Zero-Shot Uncertainty Quantification using Diffusion Probabilistic Models",
            "abstract": "The success of diffusion probabilistic models in generative tasks, such as text-to-image generation, has motivated the exploration of their application to regression problems commonly encountered in scientific computing and various other domains. In this context, the use of diffusion regression models for ensemble prediction is becoming a practice with increasing popularity. Under such background, we conducted a study to quantitatively evaluate the effectiveness of ensemble methods on solving different regression problems using diffusion models. We consider the ensemble prediction of a diffusion model as a means for zero-shot uncertainty quantification, since the diffusion models in our study are not trained with a loss function containing any uncertainty estimation. Through extensive experiments on 1D and 2D data, we demonstrate that ensemble methods consistently improve model prediction accuracy across various regression tasks. Notably, we observed a larger accuracy gain in auto-regressive prediction compared with point-wise prediction, and that enhancements take place in both the mean-square error and the physics-informed loss. Additionally, we reveal a statistical correlation between ensemble prediction error and ensemble variance, offering insights into balancing computational complexity with prediction accuracy and monitoring prediction confidence in practical applications where the ground truth is unknown. Our study provides a comprehensive view of the utility of diffusion ensembles, serving as a useful reference for practitioners employing diffusion models in regression problem-solving.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3193452",
                    "name": "Dule Shu"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        },
        {
            "paperId": "33e1bcb2d657bb277fa5971d5d329632afe22808",
            "title": "Inpainting Computational Fluid Dynamics with Deep Learning",
            "abstract": "Fluid data completion is a research problem with high potential benefit for both experimental and computational fluid dynamics. An effective fluid data completion method reduces the required number of sensors in a fluid dynamics experiment, and allows a coarser and more adaptive mesh for a Computational Fluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid data completion problem makes it prohibitively difficult to obtain a theoretical solution and presents high numerical uncertainty and instability for a data-driven approach (e.g., a neural network model). To address these challenges, we leverage recent advancements in computer vision, employing the vector quantization technique to map both complete and incomplete fluid data spaces onto discrete-valued lower-dimensional representations via a two-stage learning procedure. We demonstrated the effectiveness of our approach on Kolmogorov flow data (Reynolds number: 1000) occluded by masks of different size and arrangement. Experimental results show that our proposed model consistently outperforms benchmark models under different occlusion settings in terms of point-wise reconstruction accuracy as well as turbulent energy spectrum and vorticity distribution.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "3193452",
                    "name": "Dule Shu"
                },
                {
                    "authorId": "2287842079",
                    "name": "Wilson Zhen"
                },
                {
                    "authorId": "1752871628",
                    "name": "Zijie Li"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        },
        {
            "paperId": "3ff049bd27cb41c10bdd7278ff423e74f1283615",
            "title": "LLM-Craft: Robotic Crafting of Elasto-Plastic Objects with Large Language Models",
            "abstract": "When humans create sculptures, we are able to reason about how geometrically we need to alter the clay state to reach our target goal. We are not computing point-wise similarity metrics, or reasoning about low-level positioning of our tools, but instead determining the higher-level changes that need to be made. In this work, we propose LLM-Craft, a novel pipeline that leverages large language models (LLMs) to iteratively reason about and generate deformation-based crafting action sequences. We simplify and couple the state and action representations to further encourage shape-based reasoning. To the best of our knowledge, LLM-Craft is the first system successfully leveraging LLMs for complex deformable object interactions. Through our experiments, we demonstrate that with the LLM-Craft framework, LLMs are able to successfully reason about the deformation behavior of elasto-plastic objects. Furthermore, we find that LLM-Craft is able to successfully create a set of simple letter shapes. Finally, we explore extending the framework to reaching more ambiguous semantic goals, such as\"thinner\"or\"bumpy\". For videos please see our website: https://sites.google.com/andrew.cmu.edu/llmcraft.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185508465",
                    "name": "Alison Bartsch"
                },
                {
                    "authorId": "3614493",
                    "name": "A. Farimani"
                }
            ]
        }
    ]
}