{
    "authorId": "2124016792",
    "papers": [
        {
            "paperId": "b4f5d23f139e65a90c5806391e5d22a1cc3df248",
            "title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
            "abstract": "Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2128965713",
                    "name": "Shibo Hao"
                },
                {
                    "authorId": "2112578816",
                    "name": "Yi Gu"
                },
                {
                    "authorId": "2261698849",
                    "name": "Haotian Luo"
                },
                {
                    "authorId": "2115347044",
                    "name": "Tianyang Liu"
                },
                {
                    "authorId": "2218793118",
                    "name": "Xiyan Shao"
                },
                {
                    "authorId": "2261680539",
                    "name": "Xinyuan Wang"
                },
                {
                    "authorId": "2295679772",
                    "name": "Shuhua Xie"
                },
                {
                    "authorId": "2295874218",
                    "name": "Haodi Ma"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "2295678976",
                    "name": "Qiyue Gao"
                },
                {
                    "authorId": "2261683280",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2295863002",
                    "name": "Zhiting Hu"
                }
            ]
        },
        {
            "paperId": "4d295abfdaee6a34dd9004724e0381c3d1dbb4a7",
            "title": "SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank",
            "abstract": "Deep neural classifiers trained with cross-entropy loss (CE loss) often suffer from poor calibration, necessitating the task of out-of-distribution (OOD) detection. Traditional supervised OOD detection methods require expensive manual annotation of in-distribution and OOD samples. To address the annotation bottleneck, we introduce SELFOOD, a self-supervised OOD detection method that requires only in-distribution samples as supervision. We cast OOD detection as an inter-document intra-label (IDIL) ranking problem and train the classifier with our pairwise ranking loss, referred to as IDIL loss. Specifically, given a set of in-distribution documents and their labels, for each label, we train the classifier to rank the softmax scores of documents belonging to that label to be higher than the scores of documents that belong to other labels. Unlike CE loss, our IDIL loss function reaches zero when the desired confidence ranking is achieved and gradients are backpropagated to decrease probabilities associated with incorrect labels rather than continuously increasing the probability of the correct label. Extensive experiments with several classifiers on multiple classification datasets demonstrate the effectiveness of our method in both coarse- and fine-grained settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7565696",
                    "name": "Dheeraj Mekala"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "2113540861",
                    "name": "Chengyu Dong"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "e682e4a39c3597d3be8a921919989077047db2dd",
            "title": "Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis",
            "abstract": "Recent advances in the area of long document matching have primarily focused on using transformer-based models for long document encoding and matching. There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost \u2013 both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time. In this work, we empirically demonstrate the effectiveness of simple neural models (such as feed-forward networks, and CNNs) and simple embeddings (like GloVe, and Paragraph Vector) over transformer-based models on the task of document matching. We show that simple models outperform the more complex BERT-based models while taking significantly less training time, energy, and memory. The simple models are also more robust to variations in document length and text perturbations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36701727",
                    "name": "Akshita Jha"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "3195704",
                    "name": "J. Chandrashekar"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "aa6597dea80ea6439c6f20e1df366c57b2d7cd14",
            "title": "Are Malware Detection Models Adversarial Robust Against Evasion Attack?",
            "abstract": "The ever-increasing number of android malware still poses a critical security challenge to the smartphone ecosystem. Literature suggests that machine and deep learning models can detect android malware with high accuracy and low false positivity. However, the result of arms-race in the adversarial setting of these detection models will shape their integration in real-world applications. Therefore, we first constructed four different malware detection models by applying machine and deep learning algorithms. Then, we stepped into the malware developer\u2019s shoes and created an adversarial setting framework to investigate the robustness of the above detection models. We developed an evasion attack (Gradient Modification Attack) to exploit the vulnerabilities and force massive misclassifications in the above detection models. The attack drastically reduces the average accuracy of the above four detection models from 95.13% to $59. 97$%. Later, we also developed a potential defense mechanism (Correlated Distillation Retraining) to mitigate such adversarial attacks. In the end, we conclude that investigation of malware detection models in adversarial settings is essential for improving their robustness and real-world deployment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36882468",
                    "name": "Hemant Rathore"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "1806129",
                    "name": "S. K. Sahay"
                },
                {
                    "authorId": "2138408476",
                    "name": "Mohit Sewak"
                }
            ]
        },
        {
            "paperId": "86e0c6e038ba928b091329950400b8debe79363f",
            "title": "Supervised Contrastive Learning for Interpretable Long Document Comparison",
            "abstract": "Recent advancements in deep learning techniques have transformed the area of semantic text matching. However, most of the state-of-the-art models are designed to operate with short documents such as tweets, user reviews, comments, etc., and have fundamental limitations when applied to long-form documents such as scientific papers, legal documents, and patents. When handling such long documents, there are three primary challenges: (i) The presence of different contexts for the same word throughout the document, (ii) Small sections of contextually similar text between two documents, but dissimilar text in the remaining parts \u2013 this defies the basic understanding of \"similarity\", and (iii) The coarse nature of a single global similarity measure which fails to capture the heterogeneity of the document content. In this paper, we describe CoLDE : Co ntrastive L ong D ocument E ncoder \u2013 a transformer-based framework that addresses these challenges and allows for interpretable comparisons of long documents. CoLDE uses unique positional embeddings and a multi-headed chunkwise attention layer in conjunction with a contrastive learning framework to capture similarity at three different levels: (i) high-level similarity scores between a pair of documents, (ii) similarity scores between different sections within and across documents, and (iii) similarity scores between different chunks in the same document and also other documents. These fine-grained similarity scores aid in better interpretability. We evaluate CoLDE on three long document datasets namely, ACL Anthology publications, Wikipedia articles, and USPTO patents. Besides outperforming the state-of-the-art methods on the document comparison task, CoLDE also proves interpretable and robust to changes in document length and text perturbations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36701727",
                    "name": "Akshita Jha"
                },
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "3195704",
                    "name": "J. Chandrashekar"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "e9fe90baf1b811411943d73f40daf4d947f9102e",
            "title": "Supervised Contrastive Learning for Interpretable Long-Form Document Matching",
            "abstract": "Recent advancements in deep learning techniques have transformed the area of semantic text matching (STM). However, most state-of-the-art models are designed to operate with short documents such as tweets, user reviews, comments, and so on. These models have fundamental limitations when applied to long-form documents such as scientific papers, legal documents, and patents. When handling such long documents, there are three primary challenges: (i) the presence of different contexts for the same word throughout the document, (ii) small sections of contextually similar text between two documents, but dissimilar text in the remaining parts (this defies the basic understanding of \u201csimilarity\u201d), and (iii) the coarse nature of a single global similarity measure which fails to capture the heterogeneity of the document content. In this article, we describe CoLDE: Contrastive Long Document Encoder\u2014a transformer-based framework that addresses these challenges and allows for interpretable comparisons of long documents. CoLDE uses unique positional embeddings and a multi-headed chunkwise attention layer in conjunction with a supervised contrastive learning framework to capture similarity at three different levels: (i) high-level similarity scores between a pair of documents, (ii) similarity scores between different sections within and across documents, and (iii) similarity scores between different chunks in the same document and across other documents. These fine-grained similarity scores aid in better interpretability. We evaluate CoLDE on three long document datasets namely, ACL Anthology publications, Wikipedia articles, and USPTO patents. Besides outperforming the state-of-the-art methods on the document matching task, CoLDE is also robust to changes in document length and text perturbations and provides interpretable results. The code for the proposed model is publicly available at https://github.com/InterDigitalInc/CoLDE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36701727",
                    "name": "Akshita Jha"
                },
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "3195704",
                    "name": "J. Chandrashekar"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        }
    ]
}