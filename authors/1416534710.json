{
    "authorId": "1416534710",
    "papers": [
        {
            "paperId": "8f1da730deeefa0ae8a344617c69408877a518a4",
            "title": "A Differentially Privacy Assisted Federated Learning Scheme to Preserve Data Privacy for IoMT Applications",
            "abstract": "The rapid development of Artificial Intelligence (AI) has had a significant impact on various industries, including healthcare. The Internet of Medical Things (IoMT) has played a vital role in this evolution. However, while AI has contributed to many benefits in healthcare, concerns about data privacy and security persist. To address these concerns, we propose a framework that combines Federated Learning (FL) and Differential Privacy (DP) to enhance data protection within IoMT. By integrating FL\u2019s decentralized approach with DP\u2019s mechanism to prevent data reconstruction from model outputs, we can improve data confidentiality. This integrated approach is used to develop and analyze high-performing Convolutional Neural Networks (CNNs) for detecting Tuberculosis using chest X-ray datasets. The framework undergo thorough performance evaluation, utilizing various metrics to establish its superiority over baseline models. The results demonstrate the effectiveness of our framework as a robust solution for secure and private AI applications in healthcare.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1777141",
                    "name": "A. Barnawi"
                },
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2239649",
                    "name": "Rajkumar Tekchandani"
                },
                {
                    "authorId": "2128025648",
                    "name": "Neeraj Kumar"
                },
                {
                    "authorId": "2329385",
                    "name": "B. Alzahrani"
                }
            ]
        },
        {
            "paperId": "29b3ce4de9dd9d784ca1d876957950f4b2d3796a",
            "title": "Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs",
            "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                }
            ]
        },
        {
            "paperId": "5c07fe90fb8e1fe4bee7ff40190c3f1e83667310",
            "title": "Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models",
            "abstract": "Multimodal Large Language Models (LLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) \u2013 a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether multimodal LLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose three automatic visual cropping methods as inference time mechanisms to improve the zero-shot performance of multimodal LLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that multimodal LLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. Our code and data are publicly available. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                }
            ]
        },
        {
            "paperId": "5f33a76f45d61b583a2667c267a8a98aea9ecce4",
            "title": "Privacy Aware Question-Answering System for Online Mental Health Risk Assessment",
            "abstract": "Social media platforms have enabled individuals suffering from mental illnesses to share their lived experiences and find the online support necessary to cope. However, many users fail to receive genuine clinical support, thus exacerbating their symptoms. Screening users based on what they post online can aid providers in administering targeted healthcare and minimize false positives. Pre-trained Language Models (LMs) can assess users\u2019 social media data and classify them in terms of their mental health risk. We propose a Question-Answering (QA) approach to assess mental health risk using the Unified-QA model on two large mental health datasets. To protect user data, we extend Unified-QA by anonymizing the model training process using differential privacy. Our results demonstrate the effectiveness of modeling risk assessment as a QA task, specifically for mental health use cases. Furthermore, the model\u2019s performance decreases by less than 1% with the inclusion of differential privacy. The proposed system\u2019s performance is indicative of a promising research direction that will lead to the development of privacy-aware diagnostic systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "52132581",
                    "name": "Ujjwal Pasupulety"
                },
                {
                    "authorId": "2115580462",
                    "name": "J. Marshall"
                },
                {
                    "authorId": "40342733",
                    "name": "Dhiraj Chaurasia"
                },
                {
                    "authorId": "2117770952",
                    "name": "Shwetanjali Kumari"
                }
            ]
        },
        {
            "paperId": "70f2372ea589d0fa5af3fb618569e9677ecbbd62",
            "title": "Knowledge-enhanced Agents for Interactive Text Games",
            "abstract": "Communication via natural language is a key aspect of machine intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. Significant progress has been made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding. Yet, various sequential interactive tasks, as in text-based games, have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a knowledge-injection framework for improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports two representative model classes: reinforcement learning agents and language model agents. Furthermore, we devise multiple injection strategies for the above domain knowledge types and agent architectures, including injection via knowledge graphs and augmentation of the existing input encoding strategies. We experiment with four models on the 10 tasks in the ScienceWorld text-based game environment, to illustrate the impact of knowledge injection on various model configurations and challenging task settings. Our findings provide crucial insights into the interplay between task properties, model architectures, and domain knowledge for interactive contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "26253744",
                    "name": "Jonathan M Francis"
                },
                {
                    "authorId": "22244290",
                    "name": "Kaixin Ma"
                }
            ]
        },
        {
            "paperId": "848c251734c0a96b07096bd83e90695bd6f2f70a",
            "title": "DIGITOUR: Automatic Digital Tours for Real-Estate Properties",
            "abstract": "A virtual or digital tour is a form of virtual reality technology which allows a user to experience a specific location remotely. Currently, these virtual tours are created by following a 2-step strategy. First, a photographer clicks a 360\u00b0 equirectangular image; then, a team of annotators manually links these images for the \u201cwalkthrough\u201d user experience. The major challenge in the mass adoption of virtual tours is the time and cost involved in manual annotation/linking of images. Therefore, this paper presents an end-to-end pipeline to automate the generation of 3D virtual tours using equirectangular images for real-estate properties. We propose a novel HSV-based coloring scheme for paper tags that need to be placed at different locations before clicking the equirectangular images using 360\u00b0 cameras. These tags have two characteristics: i) they are numbered to help the photographer for placement of tags in sequence and; ii) bi-colored, which allows better learning of tag detection (using YOLOv5 architecture) in an image and digit recognition (using custom MobileNet architecture) tasks. Finally, we link/connect all the equirectangular images based on detected tags. We show the efficiency of the proposed pipeline on a real-world equirectangular image dataset collected from the Housing.com database.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2199252870",
                    "name": "Harshul Kuhar"
                },
                {
                    "authorId": "2143369626",
                    "name": "Anil Goyal"
                },
                {
                    "authorId": "2199253416",
                    "name": "Chirag Sharma"
                }
            ]
        },
        {
            "paperId": "9ecf08567b3d944d72633ad6e86a3e0e84f7d4fc",
            "title": "FIRE: Food Image to REcipe generation",
            "abstract": "Food computing has emerged as a prominent multidisciplinary field of research in recent years. An ambitious goal of food computing is to develop end-to-end intelligent systems capable of autonomously producing recipe information for a food image. Current image-to-recipe methods are retrieval-based and their success depends heavily on the dataset size and diversity, as well as the quality of learned embeddings. Meanwhile, the emergence of powerful attention-based vision and language models presents a promising avenue for accurate and generalizable recipe generation, which has yet to be extensively explored. This paper proposes FIRE, a novel multimodal methodology tailored to recipe generation in the food computing domain, which generates the food title, ingredients, and cooking instructions based on input food images. FIRE leverages the BLIP model to generate titles, utilizes a Vision Transformer with a decoder for ingredient extraction, and employs the T5 model to generate recipes incorporating titles and ingredients as inputs. We showcase two practical applications that can benefit from integrating FIRE with large language model prompting: recipe customization to fit recipes to user preferences and recipe-to-code transformation to enable automated cooking processes. Our experimental findings validate the efficacy of our proposed approach, underscoring its potential for future advancements and widespread adoption in food computing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "40342733",
                    "name": "Dhiraj Chaurasia"
                },
                {
                    "authorId": null,
                    "name": "Yifan Jiang"
                },
                {
                    "authorId": "2234360494",
                    "name": "Omkar Masur"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                }
            ]
        },
        {
            "paperId": "eafce53443e9e6800c3850807dff74a5bb8c7c2b",
            "title": "Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models",
            "abstract": "Visual Question Answering is a challenging task, as it requires seamless interaction between perceptual, linguistic, and background knowledge systems. While the recent progress of visual and natural language models like BLIP has led to improved performance on this task, we lack understanding of the ability of such models to perform on different kinds of questions and reasoning types. As our initial analysis of BLIP-family models revealed difficulty with answering fine-detail questions, we investigate the following question: Can visual cropping be employed to improve the performance of state-of-the-art visual question answering models on fine-detail questions? Given the recent success of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP model. We define three controlled subsets of the popular VQA-v2 benchmark to measure whether cropping can help model performance. Besides human cropping, we devise two automatic cropping strategies based on multi-modal embedding by CLIP and BLIP visual QA model gradients. Our experiments demonstrate that the performance of BLIP model variants can be significantly improved through human cropping, and automatic cropping methods can produce comparable benefits. A deeper dive into our findings indicates that the performance enhancement is more pronounced in zero-shot models than in fine-tuned models and more salient with smaller bounding boxes than larger ones. We perform case studies to connect quantitative differences with qualitative observations across question types and datasets. Finally, we see that the cropping enhancement is robust, as we gain an improvement of 4.59% (absolute) in the general VQA-random task by simply inputting a concatenation of the original and gradient-based cropped images. We make our code available to facilitate further innovation on visual cropping methods for question answering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                }
            ]
        },
        {
            "paperId": "0d1c683305f163cf63a15a43363a4e8ea683cfc3",
            "title": "Federated learning-based aerial image segmentation for collision-free movement and landing",
            "abstract": "The utilization of drones has recently revolutionized remote sensing with their high spatial resolution and flexibility in capturing images. In the proposed work, we employ a swarm of drones that communicate in a wireless network. Each drone captures the image frames, and each frame is further used to locate and differentiate different objects in an image frame. The semantic segmentation of the captured images is done using deep learning algorithms. To identify the most suitable, cost-efficient, and accurate segmentation method, various state-of-the-art models, are appraised and compared based on different evaluation metrics. Resnet50 model with U-net segmentation model performs the best out of all used models by providing 91.51% pixel accuracy. Also, to give real-time predictions, we have used federated learning with the drone network. Each drone trains a local model using its accumulated data and then transfers the locally trained model to the central server that aggregates the received models, generates a global federated learning model, and transmits it in the swarm network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2239649",
                    "name": "Rajkumar Tekchandani"
                },
                {
                    "authorId": "2128025648",
                    "name": "Neeraj Kumar"
                },
                {
                    "authorId": "49252696",
                    "name": "S. Tanwar"
                }
            ]
        },
        {
            "paperId": "3f2d6d6f9e4c777e95dce8cfd32ab5658927e881",
            "title": "DCNN-GA: A Deep Neural Net Architecture for Navigation of UAV in Indoor Environment",
            "abstract": "The applications of unmanned aerial vehicles (UAVs) in military, intelligent transportation, agriculture, rescue operations, natural environment mapping, and many other allied domains has increased exponentially during the past few years. Some of the use cases of their applications range from aerial surveillance, data retrieval to their use in real-time communicative networks. Though UAVs were traditionally used only outdoors, many of its indoor applications like for rescue operations, inventory tracking in warehouses, etc., have recently emerged and these use cases are being actively explored. One of the major challenges for indoor drone applications is navigation and obstacle avoidance. Due to indoor operations, the global positioning system fails in accurate localization and navigation. To address this issue, we introduce a scheme that facilitates the autonomous navigation of UAVs (which have an onboard camera) in the indoor corridors of a building using deep-neural-networks-based processing of images. For a deep neural network, the selection of a good combination of hyperparameters for a better prediction is a complicated task. In this article, the hyperparameters tuning of a convolutional neural network is achieved by using genetic algorithms. The proposed architecture (DCNN-GA) is compared with state-of-the-art ImageNet models. The experimental results show the minimum loss and high performance of the proposed algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2239649",
                    "name": "Rajkumar Tekchandani"
                },
                {
                    "authorId": "144996075",
                    "name": "Neeraj Kumar"
                },
                {
                    "authorId": "3185174",
                    "name": "V. Chamola"
                },
                {
                    "authorId": "145837053",
                    "name": "M. Guizani"
                }
            ]
        }
    ]
}