{
    "authorId": "2256986135",
    "papers": [
        {
            "paperId": "3b23659111ac4e2f7c54874b74a1a3df8eea5a2a",
            "title": "MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing",
            "abstract": "Memes have evolved as a prevalent medium for diverse communication, ranging from humour to propaganda. With the rising popularity of image-focused content, there is a growing need to explore its potential harm from different aspects. Previous studies have analyzed memes in closed settings - detecting harm, applying semantic labels, and offering natural language explanations. To extend this research, we introduce MemeMQA, a multimodal question-answering framework aiming to solicit accurate responses to structured questions while providing coherent explanations. We curate MemeMQACorpus, a new dataset featuring 1,880 questions related to 1,122 memes with corresponding answer-explanation pairs. We further propose ARSENAL, a novel two-stage multimodal framework that leverages the reasoning capabilities of LLMs to address MemeMQA. We benchmark MemeMQA using competitive baselines and demonstrate its superiority - ~18% enhanced answer prediction accuracy and distinct text generation lead across various metrics measuring lexical and semantic alignment over the best baseline. We analyze ARSENAL's robustness through diversification of question-set, confounder-based evaluation regarding MemeMQA's generalizability, and modality-specific assessment, enhancing our understanding of meme interpretation in the multimodal communication landscape.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256986135",
                    "name": "Siddhant Agarwal"
                },
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2256999352",
                    "name": "Tanmoy Chakraborty"
                }
            ]
        },
        {
            "paperId": "f18e0e020f1de8108e33968484fd1545e0714b81",
            "title": "Robot Air Hockey: A Manipulation Testbed for Robot Learning with Reinforcement Learning",
            "abstract": "Reinforcement Learning is a promising tool for learning complex policies even in fast-moving and object-interactive domains where human teleoperation or hard-coded policies might fail. To effectively reflect this challenging category of tasks, we introduce a dynamic, interactive RL testbed based on robot air hockey. By augmenting air hockey with a large family of tasks ranging from easy tasks like reaching, to challenging ones like pushing a block by hitting it with a puck, as well as goal-based and human-interactive tasks, our testbed allows a varied assessment of RL capabilities. The robot air hockey testbed also supports sim-to-real transfer with three domains: two simulators of increasing fidelity and a real robot system. Using a dataset of demonstration data gathered through two teleoperation systems: a virtualized control environment, and human shadowing, we assess the testbed with behavior cloning, offline RL, and RL from scratch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38347689",
                    "name": "Caleb Chuck"
                },
                {
                    "authorId": "2300091803",
                    "name": "Carl Qi"
                },
                {
                    "authorId": "115399934",
                    "name": "M. Munje"
                },
                {
                    "authorId": "2300134510",
                    "name": "Shuozhe Li"
                },
                {
                    "authorId": "2293283161",
                    "name": "Max Rudolph"
                },
                {
                    "authorId": "2300139219",
                    "name": "Chang Shi"
                },
                {
                    "authorId": "2256986135",
                    "name": "Siddhant Agarwal"
                },
                {
                    "authorId": "51521430",
                    "name": "Harshit S. Sikchi"
                },
                {
                    "authorId": "2210796518",
                    "name": "Abhinav Peri"
                },
                {
                    "authorId": "2300090516",
                    "name": "Sarthak Dayal"
                },
                {
                    "authorId": "2300090173",
                    "name": "Evan Kuo"
                },
                {
                    "authorId": "2300089970",
                    "name": "Kavan Mehta"
                },
                {
                    "authorId": "2300173422",
                    "name": "Anthony Wang"
                },
                {
                    "authorId": "2292196652",
                    "name": "Peter Stone"
                },
                {
                    "authorId": "2293260683",
                    "name": "Amy Zhang"
                },
                {
                    "authorId": "2791038",
                    "name": "S. Niekum"
                }
            ]
        },
        {
            "paperId": "2095a99fd992258995157e0e1bd67a8957a8b529",
            "title": "f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences",
            "abstract": "Goal-Conditioned Reinforcement Learning (RL) problems often have access to sparse rewards where the agent receives a reward signal only when it has achieved the goal, making policy optimization a difficult problem. Several works augment this sparse reward with a learned dense reward function, but this can lead to sub-optimal policies if the reward is misaligned. Moreover, recent works have demonstrated that effective shaping rewards for a particular problem can depend on the underlying learning algorithm. This paper introduces a novel way to encourage exploration called $f$-Policy Gradients, or $f$-PG. $f$-PG minimizes the f-divergence between the agent's state visitation distribution and the goal, which we show can lead to an optimal policy. We derive gradients for various f-divergences to optimize this objective. Our learning paradigm provides dense learning signals for exploration in sparse reward settings. We further introduce an entropy-regularized policy optimization objective, that we call $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective. We show that several metric-based shaping rewards like L2 can be used with $s$-MaxEnt RL, providing a common ground to study such metric-based shaping rewards with efficient exploration. We find that $f$-PG has better performance compared to standard policy gradient methods on a challenging gridworld as well as the Point Maze and FetchReach environments. More information on our website https://agarwalsiddhant10.github.io/projects/fpg.html.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256986135",
                    "name": "Siddhant Agarwal"
                },
                {
                    "authorId": "9571638",
                    "name": "Ishan Durugkar"
                },
                {
                    "authorId": "2256983154",
                    "name": "Peter Stone"
                },
                {
                    "authorId": "2258311158",
                    "name": "Amy Zhang"
                }
            ]
        }
    ]
}