{
    "authorId": "1686448",
    "papers": [
        {
            "paperId": "0eb69035aef6b190f0fd0c3892b9bbd26388a2a0",
            "title": "P-TA: Using Proximal Policy Optimization to Enhance Tabular Data Augmentation via Large Language Models",
            "abstract": "A multitude of industries depend on accurate and reasonable tabular data augmentation for their business processes. Contemporary methodologies in generating tabular data revolve around utilizing Generative Adversarial Networks (GAN) or fine-tuning Large Language Models (LLM). However, GAN-based approaches are documented to produce samples with common-sense errors attributed to the absence of external knowledge. On the other hand, LLM-based methods exhibit a limited capacity to capture the disparities between synthesized and actual data distribution due to the absence of feedback from a discriminator during training. Furthermore, the decoding of LLM-based generation introduces gradient breakpoints, impeding the backpropagation of loss from a discriminator, thereby complicating the integration of these two approaches. To solve this challenge, we propose using proximal policy optimization (PPO) to apply GANs, guiding LLMs to enhance the probability distribution of tabular features. This approach enables the utilization of LLMs as generators for GANs in synthesizing tabular data. Our experiments demonstrate that PPO leads to an approximately 4\\% improvement in the accuracy of models trained on synthetically generated data over state-of-the-art across three real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283132152",
                    "name": "Shuo Yang"
                },
                {
                    "authorId": "2307865214",
                    "name": "Chenchen Yuan"
                },
                {
                    "authorId": "2307007683",
                    "name": "Yao Rong"
                },
                {
                    "authorId": "2307003068",
                    "name": "Felix Steinbauer"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ]
        },
        {
            "paperId": "28f2d200e056b214d8e038d86fb7c57744df7c02",
            "title": "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI",
            "abstract": "Psychological trauma can manifest following various distressing events and is captured in diverse online contexts. However, studies traditionally focus on a single aspect of trauma, often neglecting the transferability of findings across different scenarios. We address this gap by training language models with progressing complexity on trauma-related datasets, including genocide-related court data, a Reddit dataset on post-traumatic stress disorder (PTSD), counseling conversations, and Incel forum posts. Our results show that the fine-tuned RoBERTa model excels in predicting traumatic events across domains, slightly outperforming large language models like GPT-4. Additionally, SLALOM-feature scores and conceptual explanations effectively differentiate and cluster trauma-related language, highlighting different trauma aspects and identifying sexual abuse and experiences related to death as a common traumatic event across all datasets. This transferability is crucial as it allows for the development of tools to enhance trauma detection and intervention in diverse populations and settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238214421",
                    "name": "Miriam Schirmer"
                },
                {
                    "authorId": "2130899453",
                    "name": "Tobias Leemann"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                },
                {
                    "authorId": "2269536584",
                    "name": "J\u00fcrgen Pfeffer"
                },
                {
                    "authorId": "2315933135",
                    "name": "David Jurgens"
                }
            ]
        },
        {
            "paperId": "2f83292a74c34d97344c5a64cf28603e70ad20f9",
            "title": "Explainability Meets Text Summarization: A Survey",
            "abstract": "Summarizing long pieces of text is a principal task in natural language processing with Machine Learning-based text generation models such as Large Language Models (LLM) being particularly suited to it. Yet these models are often used as black-boxes, making them hard to interpret and debug. This has led to calls by practitioners and regulatory bodies to improve the explainability of such models as they find ever more practical use. In this survey, we present a dual-perspective review of the intersection between explainability and summarization by reviewing the current state of explainable text summarization and also highlighting how summarization techniques are effectively employed to improve explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240530810",
                    "name": "Mahdi Dhaini"
                },
                {
                    "authorId": "2240530396",
                    "name": "Ege Erdogan"
                },
                {
                    "authorId": "2320840351",
                    "name": "Smarth Bakshi"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ]
        },
        {
            "paperId": "31c320e6a4f86188cbba5dc6dcec39892b4d89bb",
            "title": "Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization",
            "abstract": "Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans. This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283132152",
                    "name": "Shuo Yang"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ]
        },
        {
            "paperId": "33b5dd24169ff876420f56c19c0a65b6194cb49f",
            "title": "User Intent Recognition and Satisfaction with Large Language Models: A User Study with ChatGPT",
            "abstract": "The rapid evolution of large language models such as GPT-4 Turbo represents an impactful paradigm shift in digital interaction and content engagement. While these models encode vast amounts of human-generated knowledge and excel in processing diverse data types, recent research shows that they often face the challenge of accurately responding to specific user intents, leading to increased user dissatisfaction. Based on a fine-grained intent taxonomy and intent-based prompt reformulations, we analyze (1) the quality of intent recognition and (2) user satisfaction with answers from intent-based prompt reformulations for two recent ChatGPT models, GPT-3.5 Turbo and GPT-4 Turbo. The results reveal that GPT-4 outperforms GPT-3.5 on the recognition of common intents, but is conversely often outperformed by GPT-3.5 on the recognition of less frequent intents. Moreover, whenever the user intent is correctly recognized, while users are more satisfied with the answers to intent-based reformulations of GPT 4 compared to GPT-3.5, they tend to be more satisfied with the answers of the models to their original prompts compared to the reformulated ones. Finally, the study indicates that users can quickly learn to formulate their prompts more effectively, once they are shown possible reformulation templates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280148018",
                    "name": "Anna Bodonhelyi"
                },
                {
                    "authorId": "123779446",
                    "name": "Efe Bozkir"
                },
                {
                    "authorId": "2283132152",
                    "name": "Shuo Yang"
                },
                {
                    "authorId": "1884159",
                    "name": "Enkelejda Kasneci"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ]
        },
        {
            "paperId": "49a9aafa91c1497e35996222c47013367bb5e6f0",
            "title": "Enhancing Fairness through Reweighting: A Path to Attain the Sufficiency Rule",
            "abstract": "We introduce an innovative approach to enhancing the empirical risk minimization (ERM) process in model training through a refined reweighting scheme of the training data to enhance fairness. This scheme aims to uphold the sufficiency rule in fairness by ensuring that optimal predictors maintain consistency across diverse sub-groups. We employ a bilevel formulation to address this challenge, wherein we explore sample reweighting strategies. Unlike conventional methods that hinge on model size, our formulation bases generalization complexity on the space of sample weights. We discretize the weights to improve training speed. Empirical validation of our method showcases its effectiveness and robustness, revealing a consistent improvement in the balance between prediction performance and fairness metrics across various experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266814883",
                    "name": "Xuan Zhao"
                },
                {
                    "authorId": "2102011",
                    "name": "Klaus Broelemann"
                },
                {
                    "authorId": "2194703719",
                    "name": "Salvatore Ruggieri"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ]
        },
        {
            "paperId": "65a45d8f9bb1d804e097675f8887328099938aa6",
            "title": "Towards Non-Adversarial Algorithmic Recourse",
            "abstract": "The streams of research on adversarial examples and counterfactual explanations have largely been growing independently. This has led to several recent works trying to elucidate their similarities and differences. Most prominently, it has been argued that adversarial examples, as opposed to counterfactual explanations, have a unique characteristic in that they lead to a misclassification compared to the ground truth. However, the computational goals and methodologies employed in existing counterfactual explanation and adversarial example generation methods often lack alignment with this requirement. Using formal definitions of adversarial examples and counterfactual explanations, we introduce non-adversarial algorithmic recourse and outline why in high-stakes situations, it is imperative to obtain counterfactual explanations that do not exhibit adversarial characteristics. We subsequently investigate how different components in the objective functions, e.g., the machine learning model or cost function used to measure distance, determine whether the outcome can be considered an adversarial example or not. Our experiments on common datasets highlight that these design choices are often more critical in deciding whether recourse is non-adversarial than whether recourse or attack algorithms are used. Furthermore, we show that choosing a robust and accurate machine learning model results in less adversarial recourse desired in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130899453",
                    "name": "Tobias Leemann"
                },
                {
                    "authorId": "89583148",
                    "name": "Martin Pawelczyk"
                },
                {
                    "authorId": "32208207",
                    "name": "Bardh Prenkaj"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ]
        },
        {
            "paperId": "ad92fa966bbe6ce7f7498f8d05aeed204d940fd8",
            "title": "GermanPartiesQA: Benchmarking Commercial Large Language Models for Political Bias and Sycophancy",
            "abstract": "LLMs are changing the way humans create and interact with content, potentially affecting citizens' political opinions and voting decisions. As LLMs increasingly shape our digital information ecosystems, auditing to evaluate biases, sycophancy, or steerability has emerged as an active field of research. In this paper, we evaluate and compare the alignment of six LLMs by OpenAI, Anthropic, and Cohere with German party positions and evaluate sycophancy based on a prompt experiment. We contribute to evaluating political bias and sycophancy in multi-party systems across major commercial LLMs. First, we develop the benchmark dataset GermanPartiesQA based on the Voting Advice Application Wahl-o-Mat covering 10 state and 1 national elections between 2021 and 2023. In our study, we find a left-green tendency across all examined LLMs. We then conduct our prompt experiment for which we use the benchmark and sociodemographic data of leading German parliamentarians to evaluate changes in LLMs responses. To differentiate between sycophancy and steerabilty, we use 'I am [politician X], ...' and 'You are [politician X], ...' prompts. Against our expectations, we do not observe notable differences between prompting 'I am' and 'You are'. While our findings underscore that LLM responses can be ideologically steered with political personas, they suggest that observed changes in LLM outputs could be better described as personalization to the given context rather than sycophancy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2312921722",
                    "name": "Jan Batzner"
                },
                {
                    "authorId": "2312924654",
                    "name": "Volker Stocker"
                },
                {
                    "authorId": "2312923912",
                    "name": "Stefan Schmid"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ]
        },
        {
            "paperId": "b33b8f7508f8a802e2e669ec16348bb789ae0470",
            "title": "Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education",
            "abstract": "The integration of Artificial Intelligence (AI), particularly Large Language Model (LLM)-based systems, in education has shown promise in enhancing teaching and learning experiences. However, the advent of Multimodal Large Language Models (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing multimodal data including text, sound, and visual inputs, opens a new era of enriched, personalized, and interactive learning landscapes in education. Grounded in theory of multimedia learning, this paper explores the transformative role of MLLMs in central aspects of science education by presenting exemplary innovative learning scenarios. Possible applications for MLLMs could range from content creation to tailored support for learning, fostering competencies in scientific practices, and providing assessment and feedback. These scenarios are not limited to text-based and uni-modal formats but can be multimodal, increasing thus personalization, accessibility, and potential learning effectiveness. Besides many opportunities, challenges such as data protection and ethical considerations become more salient, calling for robust frameworks to ensure responsible integration. This paper underscores the necessity for a balanced approach in implementing MLLMs, where the technology complements rather than supplants the educator's role, ensuring thus an effective and ethical use of AI in science education. It calls for further research to explore the nuanced implications of MLLMs on the evolving role of educators and to extend the discourse beyond science education to other disciplines. Through the exploration of potentials, challenges, and future implications, we aim to contribute to a preliminary understanding of the transformative trajectory of MLLMs in science education and beyond.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2078316813",
                    "name": "Arne Bewersdorff"
                },
                {
                    "authorId": "2237696517",
                    "name": "Christian Hartmann"
                },
                {
                    "authorId": "2238851593",
                    "name": "Marie Hornberger"
                },
                {
                    "authorId": "2130899842",
                    "name": "Kathrin Se\u00dfler"
                },
                {
                    "authorId": "2613232",
                    "name": "M. Bannert"
                },
                {
                    "authorId": "1884159",
                    "name": "Enkelejda Kasneci"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                },
                {
                    "authorId": "2262445470",
                    "name": "Xiaoming Zhai"
                },
                {
                    "authorId": "2081229210",
                    "name": "Claudia Nerdel"
                }
            ]
        },
        {
            "paperId": "b7cb622c0bace841274a32ce5763680524b06f54",
            "title": "ALPEC: A Comprehensive Evaluation Framework and Dataset for Machine Learning-Based Arousal Detection in Clinical Practice",
            "abstract": "Detecting arousals in sleep is essential for diagnosing sleep disorders. However, using Machine Learning (ML) in clinical practice is impeded by fundamental issues, primarily due to mismatches between clinical protocols and ML methods. Clinicians typically annotate only the onset of arousals, while ML methods rely on annotations for both the beginning and end. Additionally, there is no standardized evaluation methodology tailored to clinical needs for arousal detection models. This work addresses these issues by introducing a novel post-processing and evaluation framework emphasizing approximate localization and precise event count (ALPEC) of arousals. We recommend that ML practitioners focus on detecting arousal onsets, aligning with clinical practice. We examine the impact of this shift on current training and evaluation schemes, addressing simplifications and challenges. We utilize a novel comprehensive polysomnographic dataset (CPS) that reflects the aforementioned clinical annotation constraints and includes modalities not present in existing polysomnographic datasets. We release the dataset alongside this paper, demonstrating the benefits of leveraging multimodal data for arousal onset detection. Our findings significantly contribute to integrating ML-based arousal detection in clinical settings, reducing the gap between technological advancements and clinical needs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166498536",
                    "name": "Stefan Kraft"
                },
                {
                    "authorId": "3444805",
                    "name": "Andreas Theissler"
                },
                {
                    "authorId": "2322096559",
                    "name": "Vera Wienhausen-Wilke"
                },
                {
                    "authorId": "2322097536",
                    "name": "Philipp Walter"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ]
        }
    ]
}