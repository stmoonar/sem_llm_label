{
    "authorId": "3238627",
    "papers": [
        {
            "paperId": "0571b4ba089a01f7142bc9179933c631c2854ae3",
            "title": "Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models",
            "abstract": "In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models. Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers. Our code is available: https://github.com/casszhao/multilingual-faith.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146630592",
                    "name": "Zhixue Zhao"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                }
            ]
        },
        {
            "paperId": "07efed349b0c86153177d6e660334d69f828b109",
            "title": "We Need to Talk About Classification Evaluation Metrics in NLP",
            "abstract": "In Natural Language Processing (NLP) classification tasks such as topic categorisation and sentiment analysis, model generalizability is generally measured with standard metrics such as Accuracy, F-Measure, or AUC-ROC. The diversity of metrics, and the arbitrariness of their application suggest that there is no agreement within NLP on a single best metric to use. This lack suggests there has not been sufficient examination of the underlying heuristics which each metric encodes. To address this we compare several standard classification metrics with more 'exotic' metrics and demonstrate that a random-guess normalised Informedness metric is a parsimonious baseline for task performance. To show how important the choice of metric is, we perform extensive experiments on a wide range of NLP tasks including a synthetic scenario, natural language understanding, question answering and machine translation. Across these tasks we use a superset of metrics to rank models and find that Informedness best captures the ideal model characteristics. Finally, we release a Python implementation of Informedness following the SciKitLearn classifier format.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2278432582",
                    "name": "Peter Vickers"
                },
                {
                    "authorId": "2934336",
                    "name": "Lo\u00efc Barrault"
                },
                {
                    "authorId": "38722908",
                    "name": "Emilio Monti"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                }
            ]
        },
        {
            "paperId": "3089714fd0711983fad8fad2dc5d447f07b07e7a",
            "title": "RISE: Robust Early-exiting Internal Classifiers for Suicide Risk Evaluation",
            "abstract": "Suicide is a serious public health issue, but it is preventable with timely intervention. Emerging studies have suggested there is a noticeable increase in the number of individuals sharing suicidal thoughts online. As a result, utilising advance Natural Language Processing techniques to build automated systems for risk assessment is a viable alternative. However, existing systems are prone to incorrectly predicting risk severity and have no early detection mechanisms. Therefore, we propose RISE, a novel robust mechanism for accurate early detection of suicide risk by ensembling Hyperbolic Internal Classifiers equipped with an abstention mechanism and early-exit inference capabilities. Through quantitative, qualitative and ablative experiments, we demonstrate RISE as an efficient and robust human-in-the-loop approach for risk assessment over the Columbia Suicide Severity Risk Scale (C-SSRS) and CLPsych 2022 datasets. It is able to successfully abstain from 84% incorrect predictions on Reddit data while out-predicting state of the art models upto 3.5x earlier.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161000447",
                    "name": "Ritesh Soun"
                },
                {
                    "authorId": "2157860264",
                    "name": "A. Neerkaje"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "441d9c239ba5fee0be1ac122330052c7b6bf822e",
            "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference",
            "abstract": "The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find that cross-lingual vocabulary adaptation substantially contributes to LLM inference speedups of up to 271.5%. We also show that adapting LLMs that have been pre-trained on more balanced multilingual data results in downstream performance comparable to the original models. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145412147",
                    "name": "Atsuki Yamaguchi"
                },
                {
                    "authorId": "2284592789",
                    "name": "Aline Villavicencio"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                }
            ]
        },
        {
            "paperId": "55f50127a87d07e51316c163c2123b115fff126b",
            "title": "Bayesian Prompt Ensembles: Model Uncertainty Estimation for Black-Box Large Language Models",
            "abstract": "An important requirement for the reliable deployment of pre-trained large language models (LLMs) is the well-calibrated quantification of the uncertainty in their outputs. While the likelihood of predicting the next token is a practical surrogate of the data uncertainty learned during training, model uncertainty is challenging to estimate, i.e., due to lack of knowledge acquired during training. Prior efforts to quantify uncertainty of neural networks require specific architectures or (re-)training strategies, which are impractical to apply to LLMs with several billion parameters, or for black-box models where the architecture and parameters are not available. In this paper, we pro-pose Bayesian Prompts Ensembles (BayesPE), a novel approach to effectively obtain well-calibrated uncertainty for the output of pre-trained LLMs. BayesPE computes output probabilities through a weighted ensemble of different, but semantically equivalent, task instruction prompts. The relative weights of the different prompts in the ensemble are estimated through approximate Bayesian variational inference over a small labeled validation set. We demonstrate that BayesPE approximates a Bayesian input layer for the LLM, providing a lower bound on the expected model error. In our extensive experiments, we show that BayesPE achieves significantly superior uncertainty calibration compared to several baselines over a range of natural language classification tasks, both in zero-and few-shot settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218103116",
                    "name": "Francesco Tonolini"
                },
                {
                    "authorId": "2021104091",
                    "name": "Jordan Massiah"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                },
                {
                    "authorId": "2260650020",
                    "name": "Gabriella Kazai"
                },
                {
                    "authorId": "2310836884",
                    "name": "Entropy Regularisation"
                },
                {
                    "authorId": "2310902810",
                    "name": "Kaitlyn Zhou"
                },
                {
                    "authorId": "2256674786",
                    "name": "Dan Jurafsky"
                },
                {
                    "authorId": "2266400315",
                    "name": "Tatsunori Hashimoto"
                },
                {
                    "authorId": "2311087685",
                    "name": "Xinlei Zhou"
                },
                {
                    "authorId": "2311128772",
                    "name": "Han Liu"
                },
                {
                    "authorId": "1866603",
                    "name": "Farhad Pourpanah"
                },
                {
                    "authorId": "2310836807",
                    "name": "Tieyong"
                },
                {
                    "authorId": "2310895205",
                    "name": "Yongchao Zhou"
                },
                {
                    "authorId": "2189543935",
                    "name": "Andrei Ioan Muresanu"
                },
                {
                    "authorId": "2311315994",
                    "name": "Ziwen Han"
                },
                {
                    "authorId": "73775191",
                    "name": "Keiran Paster"
                },
                {
                    "authorId": "32305445",
                    "name": "Silviu Pitis"
                },
                {
                    "authorId": "2310864634",
                    "name": "Harris Chan"
                }
            ]
        },
        {
            "paperId": "9f1b3fdf2af592c850b7e50c0022cb7cce2ae103",
            "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference",
            "abstract": "The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation (CVA) methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of five CVA methods on four generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find that CVA substantially contributes to LLM inference speedups of up to 271.5\\%. We also show that adapting LLMs that have been pre-trained on more balanced multilingual data results in downstream performance comparable to the original models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145412147",
                    "name": "Atsuki Yamaguchi"
                },
                {
                    "authorId": "2284592789",
                    "name": "Aline Villavicencio"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                }
            ]
        },
        {
            "paperId": "c2643f5653a1fe02c286802fada83dc6c1fd5730",
            "title": "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research",
            "abstract": "Research in natural language processing (NLP) for Computational Social Science (CSS) heavily relies on data from social media platforms. This data plays a crucial role in the development of models for analysing socio-linguistic phenomena within online communities. In this work, we conduct an in-depth examination of 20 datasets extensively used in NLP for CSS to comprehensively examine data quality. Our analysis reveals that social media datasets exhibit varying levels of data duplication. Consequently, this gives rise to challenges like label inconsistencies and data leakage, compromising the reliability of models. Our findings also suggest that data duplication has an impact on the current claims of state-of-the-art performance, potentially leading to an overestimation of model effectiveness in real-world scenarios. Finally, we propose new protocols and best practices for improving dataset development from social media data and its usage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2044574846",
                    "name": "Yida Mu"
                },
                {
                    "authorId": "1999667117",
                    "name": "Mali Jin"
                },
                {
                    "authorId": null,
                    "name": "Xingyi Song"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                }
            ]
        },
        {
            "paperId": "c76a0746402af9a7cf237de24b196ea9597895e8",
            "title": "How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of Target Language Text?",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in many languages beyond English. Yet, LLMs require more inference steps when generating non-English text due to their reliance on English-centric tokenizers and vocabulary, resulting in higher usage costs to non-English speakers. Vocabulary expansion with target language tokens is a widely used cross-lingual vocabulary adaptation approach to remedy this issue. Despite its effectiveness in inference speedup, previous work on vocabulary expansion has focused on high-resource settings assuming access to a substantial amount of target language data to effectively initialize the embeddings of the new tokens and adapt the LLM to the target language. However, vocabulary expansion in low-resource settings has yet to be explored. In this paper, we investigate vocabulary expansion in low-resource settings by considering embedding initialization methods and continual pre-training strategies. Through extensive experiments across typologically diverse languages, tasks and models, we establish a set of strategies to perform vocabulary expansion for faster inference, maintaining competitive downstream performance to baselines with only 30K sentences ($\\sim$0.01GB text data) from the target language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145412147",
                    "name": "Atsuki Yamaguchi"
                },
                {
                    "authorId": "2284592789",
                    "name": "Aline Villavicencio"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                }
            ]
        },
        {
            "paperId": "e07da9cd8cbfca5776388ff06fd69163ebe936e1",
            "title": "Who Is Bragging More Online? A Large Scale Analysis of Bragging in Social Media",
            "abstract": "Bragging is the act of uttering statements that are likely to be positively viewed by others and it is extensively employed in human communication with the aim to build a positive self-image of oneself. Social media is a natural platform for users to employ bragging in order to gain admiration, respect, attention and followers from their audiences. Yet, little is known about the scale of bragging online and its characteristics. This paper employs computational sociolinguistics methods to conduct the first large scale study of bragging behavior on Twitter (U.S.) by focusing on its overall prevalence, temporal dynamics and impact of demographic factors. Our study shows that the prevalence of bragging decreases over time within the same population of users. In addition, younger, more educated and popular users in the U.S. are more likely to brag. Finally, we conduct an extensive linguistics analysis to unveil specific bragging themes associated with different user traits.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1999667117",
                    "name": "Mali Jin"
                },
                {
                    "authorId": "1976147781",
                    "name": "Daniel Preoctiuc-Pietro"
                },
                {
                    "authorId": "1904399",
                    "name": "A. Seza Do\u011fru\u00f6z"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                }
            ]
        },
        {
            "paperId": "14baa9c734020fb59a0a63c24a3249f265f468e5",
            "title": "It\u2019s about Time: Rethinking Evaluation on Rumor Detection Benchmarks using Chronological Splits",
            "abstract": "New events emerge over time influencing the topics of rumors in social media. Current rumor detection benchmarks use random splits as training, development and test sets which typically results in topical overlaps. Consequently, models trained on random splits may not perform well on rumor classification on previously unseen topics due to the temporal concept drift. In this paper, we provide a re-evaluation of classification models on four popular rumor detection benchmarks considering chronological instead of random splits. Our experimental results show that the use of random splits can significantly overestimate predictive performance across all datasets and models. Therefore, we suggest that rumor detection models should always be evaluated using chronological splits for minimizing topical overlaps.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2044574846",
                    "name": "Yida Mu"
                },
                {
                    "authorId": "1785423162",
                    "name": "Kalina Bontcheva"
                },
                {
                    "authorId": "3238627",
                    "name": "Nikolaos Aletras"
                }
            ]
        }
    ]
}