{
    "authorId": "2257349988",
    "papers": [
        {
            "paperId": "42d2e852d9f3d7bcc499584fe0a8e5ad8a2df11e",
            "title": "Structural Knowledge Informed Continual Multivariate Time Series Forecasting",
            "abstract": "Recent studies in multivariate time series (MTS) forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations. However, modeling variable dependencies remains underexplored when MTS is continuously accumulated under different regimes (stages). Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance. To address this issue, we propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting within a continual learning paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects representative MTS samples from each regime for memory replay. Specifically, we develop a forecasting model based on graph structure learning, where a consistency regularization scheme is imposed between the learned variable dependencies and the structural knowledge while optimizing the forecasting objective over the MTS data. As such, MTS representations learned in each regime are associated with distinct structural knowledge, which helps the model memorize a variety of conceivable scenarios and results in accurate forecasts in the continual learning context. Meanwhile, we develop a representation-matching memory replay scheme that maximizes the temporal coverage of MTS data to efficiently preserve the underlying temporal dynamics and dependency structures of each regime. Thorough empirical studies on synthetic and real-world benchmarks validate SKI-CL's efficacy and advantages over the state-of-the-art for continual MTS forecasting tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281612988",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2276324326",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2257349440",
                    "name": "Sahil Garg"
                },
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2246897295",
                    "name": "Yuriy Nevmyvaka"
                }
            ]
        },
        {
            "paperId": "44f6cea2aa05620fef00d8dc9e566918dc6771c9",
            "title": "Empowering Time Series Analysis with Large Language Models: A Survey",
            "abstract": "Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (\\textit{i.e.}, direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2281612988",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "3358065",
                    "name": "Xikun Zhang"
                },
                {
                    "authorId": "2257349440",
                    "name": "Sahil Garg"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2246897295",
                    "name": "Yuriy Nevmyvaka"
                },
                {
                    "authorId": "2276324326",
                    "name": "Dongjin Song"
                }
            ]
        },
        {
            "paperId": "6c14a617dd09dedaea4bdc043e25da0bcef1f59a",
            "title": "Deep Generative Sampling in the Dual Divergence Space: A Data-efficient & Interpretative Approach for Generative AI",
            "abstract": "Building on the remarkable achievements in generative sampling of natural images, we propose an innovative challenge, potentially overly ambitious, which involves generating samples of entire multivariate time series that resemble images. However, the statistical challenge lies in the small sample size, sometimes consisting of a few hundred subjects. This issue is especially problematic for deep generative models that follow the conventional approach of generating samples from a canonical distribution and then decoding or denoising them to match the true data distribution. In contrast, our method is grounded in information theory and aims to implicitly characterize the distribution of images, particularly the (global and local) dependency structure between pixels. We achieve this by empirically estimating its KL-divergence in the dual form with respect to the respective marginal distribution. This enables us to perform generative sampling directly in the optimized 1-D dual divergence space. Specifically, in the dual space, training samples representing the data distribution are embedded in the form of various clusters between two end points. In theory, any sample embedded between those two end points is in-distribution w.r.t. the data distribution. Our key idea for generating novel samples of images is to interpolate between the clusters via a walk as per gradients of the dual function w.r.t. the data dimensions. In addition to the data efficiency gained from direct sampling, we propose an algorithm that offers a significant reduction in sample complexity for estimating the divergence of the data distribution with respect to the marginal distribution. We provide strong theoretical guarantees along with an extensive empirical evaluation using many real-world datasets from diverse domains, establishing the superiority of our approach w.r.t. state-of-the-art deep learning methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2257349440",
                    "name": "Sahil Garg"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2144925136",
                    "name": "Anant Raj"
                },
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2774914",
                    "name": "Yuriy Nevmyvaka"
                },
                {
                    "authorId": "10832213",
                    "name": "S. Gopal"
                },
                {
                    "authorId": "2308468375",
                    "name": "Amit Dhurandhar"
                },
                {
                    "authorId": "2295170930",
                    "name": "Guillermo Cecchi"
                },
                {
                    "authorId": "2109771",
                    "name": "I. Rish"
                }
            ]
        },
        {
            "paperId": "7d97bc47048823585084096a0c9468d8942d3add",
            "title": "Recurrent Interpolants for Probabilistic Time Series Prediction",
            "abstract": "Sequential models like recurrent neural networks and transformers have become standard for probabilistic multivariate time series forecasting across various domains. Despite their strengths, they struggle with capturing high-dimensional distributions and cross-feature dependencies. Recent work explores generative approaches using diffusion or flow-based models, extending to time series imputation and forecasting. However, scalability remains a challenge. This work proposes a novel method combining recurrent neural networks' efficiency with diffusion models' probabilistic modeling, based on stochastic interpolants and conditional generation with control features, offering insights for future developments in this dynamic field.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2321678409",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "2257346119",
                    "name": "Marin Bilovs"
                },
                {
                    "authorId": "2321551287",
                    "name": "Sarthak Mittal"
                },
                {
                    "authorId": "2262002818",
                    "name": "Wei Deng"
                },
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                }
            ]
        },
        {
            "paperId": "a0e7f328781eda3e3dc919c77bd1eedb99b99612",
            "title": "S2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
            "abstract": "Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embeddings space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maximizing the cosine similarity in the joint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as prompts to provide strong indicators (context) for time series that exhibit different temporal dynamics. With thorough empirical studies on multiple benchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve superior forecasting performance over state-of-the-art baselines. Furthermore, our ablation studies and visualizations verify the necessity of prompt learning informed by semantic space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281612988",
                    "name": "Zijie Pan"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2257349440",
                    "name": "Sahil Garg"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2246897295",
                    "name": "Yuriy Nevmyvaka"
                },
                {
                    "authorId": "2276324326",
                    "name": "Dongjin Song"
                }
            ]
        },
        {
            "paperId": "7c9bb230946cf48a7b9de97fd0281f42fbc51d31",
            "title": "Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting",
            "abstract": "Over the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero-shot and few-shot generalization. However, despite the success of foundation models in modalities such as natural language processing and computer vision, the development of foundation models for time series forecasting has lagged behind. We present Lag-Llama, a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates. Lag-Llama is pretrained on a large corpus of diverse time series data from several domains, and demonstrates strong zero-shot generalization capabilities compared to a wide range of forecasting models on downstream datasets across domains. Moreover, when fine-tuned on relatively small fractions of such previously unseen datasets, Lag-Llama achieves state-of-the-art performance, outperforming prior deep learning approaches, emerging as the best general-purpose model on average. Lag-Llama serves as a strong contender to the current state-of-art in time series forecasting and paves the way for future advancements in foundation models tailored to time series data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2253400055",
                    "name": "Arjun Ashok"
                },
                {
                    "authorId": "2258310022",
                    "name": "Andrew Robert Williams"
                },
                {
                    "authorId": "2257347656",
                    "name": "Arian Khorasani"
                },
                {
                    "authorId": "2257347183",
                    "name": "George Adamopoulos"
                },
                {
                    "authorId": "2052115032",
                    "name": "Rishika Bhagwatkar"
                },
                {
                    "authorId": "2257346119",
                    "name": "Marin Bilovs"
                },
                {
                    "authorId": "2257348423",
                    "name": "Hena Ghonia"
                },
                {
                    "authorId": "2124982378",
                    "name": "N. Hassen"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2257349440",
                    "name": "Sahil Garg"
                },
                {
                    "authorId": "2253399166",
                    "name": "Alexandre Drouin"
                },
                {
                    "authorId": "2748188",
                    "name": "Nicolas Chapados"
                },
                {
                    "authorId": "2774914",
                    "name": "Yuriy Nevmyvaka"
                },
                {
                    "authorId": "2109771",
                    "name": "I. Rish"
                }
            ]
        },
        {
            "paperId": "f005889b7d85558f978d40506196e9a853b5ed15",
            "title": "Lag-Llama: Towards Foundation Models for Time Series Forecasting",
            "abstract": "Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama , a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen \u201cout-of-distribution\u201d time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws [7] to fit and predict model scaling behavior. The open source code is made available at https://github",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2253400055",
                    "name": "Arjun Ashok"
                },
                {
                    "authorId": "2258310022",
                    "name": "Andrew Robert Williams"
                },
                {
                    "authorId": "2257347656",
                    "name": "Arian Khorasani"
                },
                {
                    "authorId": "2257347183",
                    "name": "George Adamopoulos"
                },
                {
                    "authorId": "2052115032",
                    "name": "Rishika Bhagwatkar"
                },
                {
                    "authorId": "1388111489",
                    "name": "Marin Bilos"
                },
                {
                    "authorId": "2257348423",
                    "name": "Hena Ghonia"
                },
                {
                    "authorId": "2124982378",
                    "name": "N. Hassen"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2257349440",
                    "name": "Sahil Garg"
                },
                {
                    "authorId": "2253399166",
                    "name": "Alexandre Drouin"
                },
                {
                    "authorId": "2748188",
                    "name": "Nicolas Chapados"
                },
                {
                    "authorId": "2246897295",
                    "name": "Yuriy Nevmyvaka"
                },
                {
                    "authorId": "2239232896",
                    "name": "Irina Rish"
                }
            ]
        },
        {
            "paperId": "14517931dad9f02150a98667d2bdabd75f50a3bc",
            "title": "AutoML Decathlon: Diverse Tasks, Modern Methods, and Efficiency at Scale",
            "abstract": "The vision of Automated Machine Learning (AutoML) is to produce high performing ML pipelines that require very little human involvement or domain expertise to use. Com-petitions and benchmarks have been critical tools for accelerating progress in AutoML. However, much of the prior work on AutoML competitions has focused on well-studied domains in machine learning such as vision and language\u2014these are domains which have benefited from several years of ML pipeline design by domain experts, which brings the usage of AutoML into question in the first place. Recently, AutoML for diverse tasks has emerged as an important research area that aims to bring AutoML to the domains where it can have the most impact: the long tail of ML tasks beyond vision and language . We present a retrospective report of the AutoML Decathlon\u2014an AutoML for diverse tasks competition hosted at NeurIPS 2022. The AutoML Decathlon presented participants with a set of 10 machine learning tasks that are diverse along several axes: domain, input di-mension, output dimension, output type, objective function, and scale. Participants were tasked with developing AutoML methods that performed well on a separate set of 10 hidden diverse test tasks within a certain time budget, so as to discourage overfitting to the initial set of tasks and to encourage efficiency. In this report, we outline the details of the competition, discuss the top-5 submissions, analyze the results, and compare top submissions to additional state-of-the-art baselines designed specifically for diverse tasks. We conclude that the combination of existing efficient AutoML techniques with modern advancements in ML such as large-scale transfer learning, modern architectures, and differentiable Neural Architecture Search (NAS) is a promising direction for AutoML for diverse tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2269050434",
                    "name": "Nicholas Roberts"
                },
                {
                    "authorId": "2269125879",
                    "name": "Samuel Guo"
                },
                {
                    "authorId": "2269128228",
                    "name": "Cong Xu"
                },
                {
                    "authorId": "145532827",
                    "name": "Ameet Talwalkar"
                },
                {
                    "authorId": "2269050062",
                    "name": "David Lander"
                },
                {
                    "authorId": "2269478187",
                    "name": "Lvfang Tao"
                },
                {
                    "authorId": "2269116224",
                    "name": "Linhang Cai"
                },
                {
                    "authorId": "2269050408",
                    "name": "Shuaicheng Niu"
                },
                {
                    "authorId": "2269050255",
                    "name": "Jianyu Heng"
                },
                {
                    "authorId": "2269079539",
                    "name": "Hongyang Qin"
                },
                {
                    "authorId": "2269078527",
                    "name": "Minwen Deng"
                },
                {
                    "authorId": "2269050248",
                    "name": "Johannes Hog"
                },
                {
                    "authorId": "2269050109",
                    "name": "Alexander Pfefferle"
                },
                {
                    "authorId": "2269050048",
                    "name": "Sushil Ammanaghatta Shivakumar"
                },
                {
                    "authorId": "35064488",
                    "name": "Arjun Krishnakumar"
                },
                {
                    "authorId": "2269118023",
                    "name": "Yubo Wang"
                },
                {
                    "authorId": "46219704",
                    "name": "R. Sukthanker"
                },
                {
                    "authorId": "2269049641",
                    "name": "Frank Hutter"
                },
                {
                    "authorId": "2057850058",
                    "name": "Euxhen Hasanaj"
                },
                {
                    "authorId": "2269144585",
                    "name": "Tien-Dung Le"
                },
                {
                    "authorId": "10398264",
                    "name": "M. Khodak"
                },
                {
                    "authorId": "2774914",
                    "name": "Yuriy Nevmyvaka"
                },
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2269049908",
                    "name": "Frederic Sala"
                },
                {
                    "authorId": "2257349988",
                    "name": "Anderson Schneider"
                },
                {
                    "authorId": "2269118801",
                    "name": "Junhong Shen"
                },
                {
                    "authorId": "2269049705",
                    "name": "Evan R. Sparks"
                }
            ]
        }
    ]
}