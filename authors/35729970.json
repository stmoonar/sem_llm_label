{
    "authorId": "35729970",
    "papers": [
        {
            "paperId": "08a23cb1ae7b0748407146520c0630d7f2b51c4c",
            "title": "Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation",
            "abstract": "Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118227093",
                    "name": "Junwei Huang"
                },
                {
                    "authorId": "48064856",
                    "name": "Zhiqing Sun"
                },
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                }
            ]
        },
        {
            "paperId": "269b9d5141c9130738a8c1d122a53307fd34c29a",
            "title": "Robust Cross-Domain Pseudo-Labeling and Contrastive Learning for Unsupervised Domain Adaptation NIR-VIS Face Recognition",
            "abstract": "Near-infrared and visible face recognition (NIR-VIS) is attracting increasing attention because of the need to achieve face recognition in low-light conditions to enable 24-hour secure retrieval. However, annotating identity labels for a large number of heterogeneous face images is time-consuming and expensive, which limits the application of the NIR-VIS face recognition system to larger scale real-world scenarios. In this paper, we attempt to achieve NIR-VIS face recognition in an unsupervised domain adaptation manner. To get rid of the reliance on manual annotations, we propose a novel Robust cross-domain Pseudo-labeling and Contrastive learning (RPC) network which consists of three key components, i.e., NIR cluster-based Pseudo labels Sharing (NPS), Domain-specific cluster Contrastive Learning (DCL) and Inter-domain cluster Contrastive Learning (ICL). Firstly, NPS is presented to generate pseudo labels by exploring robust NIR clusters and sharing reliable label knowledge with VIS domain. Secondly, DCL is designed to learn intra-domain compact yet discriminative representations. Finally, ICL dynamically combines and refines intrinsic identity relationships to guide the instance-level features to learn robust and domain-independent representations. Extensive experiments are conducted to verify an accuracy of over 99% in pseudo label assignment and the advanced performance of RPC network on four mainstream NIR-VIS datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "34633195",
                    "name": "Weipeng Hu"
                },
                {
                    "authorId": "2239399397",
                    "name": "Haiqi Lin"
                },
                {
                    "authorId": "2153821312",
                    "name": "Haifeng Hu"
                }
            ]
        },
        {
            "paperId": "5cf242f223bd00aa6e3613e36bf5a64c0095ed44",
            "title": "Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition",
            "abstract": "To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "34633195",
                    "name": "Weipeng Hu"
                },
                {
                    "authorId": "2153821312",
                    "name": "Haifeng Hu"
                }
            ]
        },
        {
            "paperId": "5f90d43e6ece5c6ee6e8186e4b57d46c85377713",
            "title": "DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization",
            "abstract": "Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48064856",
                    "name": "Zhiqing Sun"
                },
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                }
            ]
        },
        {
            "paperId": "6a42f6362afa3a1a0936f7a6a8927d04a2285cc5",
            "title": "Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs",
            "abstract": "Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into sub goal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on sub goal representation functions and sub goal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent sub goal representations and lack an efficient sub goal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120251897",
                    "name": "Qingyang Zhang"
                },
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "2135060971",
                    "name": "Jingqing Ruan"
                },
                {
                    "authorId": "2025270099",
                    "name": "Xuantang Xiong"
                },
                {
                    "authorId": "144185398",
                    "name": "Dengpeng Xing"
                },
                {
                    "authorId": "2112878667",
                    "name": "Bo Xu"
                }
            ]
        },
        {
            "paperId": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5",
            "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
            "abstract": "Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in\"hallucination\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48064856",
                    "name": "Zhiqing Sun"
                },
                {
                    "authorId": "2191455",
                    "name": "Sheng Shen"
                },
                {
                    "authorId": "31136675",
                    "name": "Shengcao Cao"
                },
                {
                    "authorId": "2143856368",
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "2243126534",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "2714199",
                    "name": "Yikang Shen"
                },
                {
                    "authorId": "144158271",
                    "name": "Chuang Gan"
                },
                {
                    "authorId": "2587808",
                    "name": "Liangyan Gui"
                },
                {
                    "authorId": "2302062",
                    "name": "Yu-Xiong Wang"
                },
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "1732330",
                    "name": "K. Keutzer"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                }
            ]
        },
        {
            "paperId": "acad2fd188ebc0ce6ef2456ca6f000b8ad8c7a1e",
            "title": "Syncretic Space Learning Network for NIR-VIS Face Recognition",
            "abstract": "To overcome the technical bottleneck of face recognition in low-light scenarios, Near-InfraRed and VISible (NIR-VIS) heterogeneous face recognition is proposed for matching well-lit VIS faces with poorly lit NIR faces. Current cross-modal synthesis methods visually convert the NIR modality to the VIS modality and then perform face matching in the VIS modality. However, using a heavyweight GAN network on unpaired NIR-VIS faces may lead to high synthesis difficulty, low inference efficiency, and other problems. To alleviate the above problems, we simultaneously synthesize NIR and VIS images into modality-independent syncretic images and propose a novel syncretic space learning (SSL) model to eliminate the modal gap. First, Syncretic Modality Generator (SMG) synthesizes NIR and VIS images into syncretic images using channel-level convolution with a shallow CNN. In particular, the discriminative structural information is well preserved and the face quality can be further improved with small modal variations in a self-supervised learning manner. Second, Modality-adversarial Syncretic space Learning (MSL) projects NIR and VIS images into the syncretic space by a syncretic-modality adversarial learning strategy with syncretic pattern guided objective, so the modal gap of NIR-VIS faces can be effectively reduced. Finally, the Syncretic Distribution Consistency (SDC) constructed by NIR-syncretic, syncretic-syncretic, and VIS-syncretic consistency can enhance the intra-class compactness and learn discriminative representations. Extensive experiments on three challenging datasets demonstrate the effectiveness of the SSL method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "34633195",
                    "name": "Weipeng Hu"
                },
                {
                    "authorId": "145030935",
                    "name": "Haifeng Hu"
                }
            ]
        },
        {
            "paperId": "130cbe44d51675590a6a5e4247be2a4a9a72b479",
            "title": "CH-Go: Online Go System Based on Chunk Data Storage",
            "abstract": "The training and running of an online Go system require the support of effective data management systems to deal with vast data, such as the initial Go game records, the feature data set obtained by representation learning, the experience data set of self-play, the randomly sampled Monte Carlo tree, and so on. Previous work has rarely mentioned this problem, but the ability and efficiency of data management systems determine the accuracy and speed of the Go system. To tackle this issue, we propose an online Go game system based on the chunk data storage method (CH-Go), which processes the format of 160k Go game data released by Kiseido Go Server (KGS) and designs a Go encoder with 11 planes, a parallel processor and generator for better memory performance. Specifically, we store the data in chunks, take the chunk size of 1024 as a batch, and save the features and labels of each chunk as binary files. Then a small set of data is randomly sampled each time for the neural network training, which is accessed by batch through yield method. The training part of the prototype includes three modules: supervised learning module, reinforcement learning module, and an online module. Firstly, we apply Zobrist-guided hash coding to speed up the Go board construction. Then we train a supervised learning policy network to initialize the self-play for generation of experience data with 160k Go game data released by KGS. Finally, we conduct reinforcement learning based on REINFORCE algorithm. Experiments show that the training accuracy of CH-Go in the sampled 150 games is 99.14%, and the accuracy in the test set is as high as 98.82 %. Under the condition of limited local computing power and time, we have achieved a better level of intelligence. Given the current situation that classical systems such as GOLAXY are not free and open, CH-Go has realized and maintained complete Internet openness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212706205",
                    "name": "Hui Lu"
                },
                {
                    "authorId": "2213161735",
                    "name": "Chuan Li"
                },
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "2213577070",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "2312048736",
                    "name": "Ashraful Islam"
                }
            ]
        },
        {
            "paperId": "62cee6db1d962fd5fa7ea07f049d7ecc576c78f5",
            "title": "Traffic4cast at NeurIPS 2021 - Temporal and Spatial Few-Shot Transfer Learning in Gridded Geo-Spatial Processes",
            "abstract": "The IARAI Traffic4cast competitions at NeurIPS 2019 and 2020 showed that neural networks can successfully predict future traffic conditions 1 hour into the future on simply aggregated GPS probe data in time and space bins. We thus reinterpreted the challenge of forecasting traffic conditions as a movie completion task. U-Nets proved to be the winning architecture, demonstrating an ability to extract relevant features in this complex real-world geo-spatial process. Building on the previous competitions, Traffic4cast 2021 now focuses on the question of model robustness and generalizability across time and space. Moving from one city to an entirely different city, or moving from pre-COVID times to times after COVID hit the world thus introduces a clear domain shift. We thus, for the first time, release data featuring such domain shifts. The competition now covers ten cities over 2 years, providing data compiled from over 10^12 GPS probe data. Winning solutions captured traffic dynamics sufficiently well to even cope with these complex domain shifts. Surprisingly, this seemed to require only the previous 1h traffic dynamic history and static road graph as input.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065328523",
                    "name": "Christian Eichenberger"
                },
                {
                    "authorId": "2217478",
                    "name": "M. Neun"
                },
                {
                    "authorId": "2053800378",
                    "name": "Henry Martin"
                },
                {
                    "authorId": "3433525",
                    "name": "Pedro Herruzo"
                },
                {
                    "authorId": "152137821",
                    "name": "M. Spanring"
                },
                {
                    "authorId": "2141583641",
                    "name": "Yichao Lu"
                },
                {
                    "authorId": "14201699",
                    "name": "Sungbin Choi"
                },
                {
                    "authorId": "94158576",
                    "name": "V. Konyakhin"
                },
                {
                    "authorId": "92598419",
                    "name": "N. Lukashina"
                },
                {
                    "authorId": "35431760",
                    "name": "A. Shpilman"
                },
                {
                    "authorId": "9608938",
                    "name": "Nina Wiedemann"
                },
                {
                    "authorId": "2763102",
                    "name": "M. Raubal"
                },
                {
                    "authorId": "2153212470",
                    "name": "Bo Wang"
                },
                {
                    "authorId": "2128301059",
                    "name": "Hai L. Vu"
                },
                {
                    "authorId": "2231126",
                    "name": "R. Mohajerpoor"
                },
                {
                    "authorId": "1500396195",
                    "name": "Chengguang Cai"
                },
                {
                    "authorId": "46575152",
                    "name": "Inhi Kim"
                },
                {
                    "authorId": "2131845803",
                    "name": "L. Hermes"
                },
                {
                    "authorId": "33016134",
                    "name": "Andrew Melnik"
                },
                {
                    "authorId": "1410423255",
                    "name": "Riza Velioglu"
                },
                {
                    "authorId": "2154421935",
                    "name": "Markus Vieth"
                },
                {
                    "authorId": "1913256",
                    "name": "M. Schilling"
                },
                {
                    "authorId": "30857225",
                    "name": "Alabi Bojesomo"
                },
                {
                    "authorId": "2160887534",
                    "name": "Hasan Al Marzouqi"
                },
                {
                    "authorId": "144015890",
                    "name": "P. Liatsis"
                },
                {
                    "authorId": "1413957680",
                    "name": "Jay Santokhi"
                },
                {
                    "authorId": "2133448644",
                    "name": "Dylan Hillier"
                },
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "2361447",
                    "name": "Joned Sarwar"
                },
                {
                    "authorId": "144314388",
                    "name": "A. Jordan"
                },
                {
                    "authorId": "8042564",
                    "name": "Emil Hewage"
                },
                {
                    "authorId": "2244591",
                    "name": "D. Jonietz"
                },
                {
                    "authorId": "2053847175",
                    "name": "Fei Tang"
                },
                {
                    "authorId": "1790005",
                    "name": "A. Gruca"
                },
                {
                    "authorId": "2058236577",
                    "name": "Michael Kopp"
                },
                {
                    "authorId": "29846943",
                    "name": "David P. Kreil"
                },
                {
                    "authorId": "3308557",
                    "name": "Sepp Hochreiter"
                }
            ]
        },
        {
            "paperId": "9d841bb0960790024e520c87e62e2453e0e803fe",
            "title": "Exploiting Local and Global Features in Transformer-based Extreme Multi-label Text Classification",
            "abstract": "Extreme multi-label text classification (XMTC) is the task of tagging each document with the relevant labels from a very large space of predefined categories. Recently, large pre-trained Transformer models have made significant performance improvements in XMTC, which typically use the embedding of the special CLS token to represent the entire document semantics as a global feature vector, and match it against candidate labels. However, we argue that such a global feature vector may not be sufficient to represent different granularity levels of semantics in the document, and that complementing it with the local word-level features could bring additional gains. Based on this insight, we propose an approach that combines both the local and global features produced by Transformer models to improve the prediction power of the classifier. Our experiments show that the proposed model either outperforms or is comparable to the state-of-the-art methods on benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46752970",
                    "name": "Ruohong Zhang"
                },
                {
                    "authorId": "46394797",
                    "name": "Yau-Shian Wang"
                },
                {
                    "authorId": "35729970",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "2161340676",
                    "name": "Tom Vu"
                },
                {
                    "authorId": "2066699630",
                    "name": "Li Lei"
                }
            ]
        }
    ]
}