{
    "authorId": "5002401",
    "papers": [
        {
            "paperId": "faffa6c51abb671f3117d94cedeb28f3631d2f6a",
            "title": "FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory",
            "abstract": "Multi-turn textual feedback-based fashion image retrieval focuses on a real-world setting, where users can iteratively provide information to refine retrieval results until they find an item that fits all their requirements. In this work, we present a novel memory-based method, called FashionNTM, for such a multi-turn system. Our framework incorporates a new Cascaded Memory Neural Turing Machine (CM-NTM) approach for implicit state management, thereby learning to integrate information across all past turns to retrieve new images, for a given turn. Unlike vanilla Neural Turing Machine (NTM), our CM-NTM operates on multiple inputs, which interact with their respective memories via individual read and write heads, to learn complex relationships. Extensive evaluation results show that our proposed method outperforms the previous state-of-the-art algorithm by 50.5%, on Multi-turn FashionIQ [60] \u2013 the only existing multi-turn fashion dataset currently, in addition to having a relative improvement of 12.6% on Multi-turn Shoes \u2013 an extension of the singleturn Shoes dataset [5] that we created in this work. Further analysis of the model in a real-world interactive setting demonstrates two important capabilities of our model \u2013 memory retention across turns, and agnosticity to turn order for non-contradictory feedback. Finally, user study results show that images retrieved by FashionNTM were favored by 83.1% over other multi-turn models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "5002401",
                    "name": "Sahil Wadhwa"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "2213162331",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "147887099",
                    "name": "Rakesh Chada"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "15a6928d4bb899b10b025f3dc5573767aa236091",
            "title": "Toward Assessing Clinical Trial Publications for Reporting Transparency",
            "abstract": "Objective. To annotate a corpus of randomized controlled trial (RCT) publications with the checklist items of CONSORT reporting guidelines and using the corpus to develop text mining methods for RCT appraisal. Methods. We annotated a corpus of 50 RCT articles at the sentence level using 37 fine-grained CONSORT checklist items. A subset (31 articles) was double-annotated and adjudicated, while 19 were annotated by a single annotator and reconciled by another. We calculated inter-annotator agreement at the article and section level using MASI (Measuring Agreement on Set-Valued Items)and at the CONSORT item level using Krippendorff's alpha. We experimented with two rule-based methods (phrase-based and section header-based) and two supervised learning approaches (support vector machine and BioBERT-based neural network classifiers), for recognizing 17 methodology-related items in the RCT Methods sections. Results. We created CONSORT-TM consisting of 10,709 sentences, 4,845 (45%) of which were annotated with 5,246 labels. A median of 28 CONSORT items (out of possible 37) were annotated per article. Agreement was moderate at the article and section levels (average MASI: 0.60 and 0.64, respectively). Agreement varied considerably among individual checklist items (Krippendorff's alpha= 0.06-0.96). The model based on BioBERT performed best overall for recognizing methodology-related items (micro-precision: 0.82, micro-recall: 0.63, micro-F1: 0.71). Combining models using majority vote and label aggregation further improved precision and recall, respectively. Conclusion. Our annotated corpus, CONSORT-TM, contains more fine-grained information than earlier RCT corpora. Low frequency of some CONSORT items made it difficult to train effective text mining models to recognize them. For the items commonly reported, CONSORT-TM can serve as a testbed for text mining methods that assess RCT transparency, rigor, and reliability, and support methods for peer review and authoring assistance. Minor modifications to the annotation scheme and a larger corpus could facilitate improved text mining models. CONSORT-TM is publicly available at https://github.com/kilicogluh/CONSORT-TM.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "3358452",
                    "name": "H. Kilicoglu"
                },
                {
                    "authorId": "1726167",
                    "name": "Graciela Rosemblat"
                },
                {
                    "authorId": "72971700",
                    "name": "Linh K. Hoang"
                },
                {
                    "authorId": "5002401",
                    "name": "Sahil Wadhwa"
                },
                {
                    "authorId": "2148423298",
                    "name": "Z. Peng"
                },
                {
                    "authorId": "2733587",
                    "name": "M. Mali\u010dki"
                },
                {
                    "authorId": "1692044",
                    "name": "Jodi Schneider"
                },
                {
                    "authorId": "47840662",
                    "name": "G. Riet"
                }
            ]
        },
        {
            "paperId": "9b274d7df60877ad4db65c931d43216dfd5d257f",
            "title": "Contextualized End-to-End Neural Entity Linking",
            "abstract": "We propose an entity linking (EL) model that jointly learns mention detection (MD) and entity disambiguation (ED). Our model applies task-specific heads on top of shared BERT contextualized embeddings. We achieve state-of-the-art results across a standard EL dataset using our model; we also study our model\u2019s performance under the setting when hand-crafted entity candidate sets are not available and find that the model performs well under such a setting too.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047424748",
                    "name": "Haotian Chen"
                },
                {
                    "authorId": "2116227185",
                    "name": "Xi Li"
                },
                {
                    "authorId": "40443405",
                    "name": "Andrej Zukov Gregoric"
                },
                {
                    "authorId": "5002401",
                    "name": "Sahil Wadhwa"
                }
            ]
        },
        {
            "paperId": "5ce0209f480dc2cdde6b797adc44797a35c2bde6",
            "title": "YELM: End-to-End Contextualized Entity Linking",
            "abstract": "We propose yet another entity linking model (YELM) which links words to entities instead of spans. This overcomes any difficulties associated with the selection of good candidate mention spans and makes the joint training of mention detection (MD) and entity disambiguation (ED) easily possible. Our model is based on BERT and produces contextualized word embeddings which are trained against a joint MD and ED objective. We achieve state-of-the-art results on several standard entity linking (EL) datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047424748",
                    "name": "Haotian Chen"
                },
                {
                    "authorId": "5002401",
                    "name": "Sahil Wadhwa"
                },
                {
                    "authorId": "2116227185",
                    "name": "Xi Li"
                },
                {
                    "authorId": "40443405",
                    "name": "Andrej Zukov Gregoric"
                }
            ]
        },
        {
            "paperId": "a13e7151a177847ea54a4b3a949d668e97ec891e",
            "title": "Contextualized End-to-End Neural Entity Linking.",
            "abstract": "We propose yet another entity linking model (YELM) which links words to entities instead of spans. This overcomes any difficulties associated with the selection of good candidate mention spans and makes the joint training of mention detection (MD) and entity disambiguation (ED) easily possible. Our model is based on BERT and produces contextualized word embeddings which are trained against a joint MD and ED objective. We achieve state-of-the-art results on several standard entity linking (EL) datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285157133",
                    "name": "Haotian Chen"
                },
                {
                    "authorId": "1400416026",
                    "name": "Andrej \u017dukov-Gregori\u010d"
                },
                {
                    "authorId": "2285124768",
                    "name": "Xi David Li"
                },
                {
                    "authorId": "5002401",
                    "name": "Sahil Wadhwa"
                }
            ]
        },
        {
            "paperId": "1f010989e6c54fd851f6942e26841dd567fc685e",
            "title": "Evaluating the Readability of Force Directed Graph Layouts: A Deep Learning Approach",
            "abstract": "Existing graph layout algorithms are usually not able to optimize all the aesthetic properties desired in a graph layout. To evaluate how well the desired visual features are reflected in a graph layout, many readability metrics have been proposed in the past decades. However, the calculation of these readability metrics often requires access to the node and edge coordinates and is usually computationally inefficient, especially for dense graphs. Importantly, when the node and edge coordinates are not accessible, it becomes impossible to evaluate the graph layouts quantitatively. In this paper, we present a novel deep learning-based approach to evaluate the readability of graph layouts by directly using graph images. A convolutional neural network architecture is proposed and trained on a benchmark dataset of graph images, which is composed of synthetically generated graphs and graphs created by sampling from real large networks. Multiple representative readability metrics (including edge crossing, node spread, and group overlap) are considered in the proposed approach. We quantitatively compare our approach to traditional methods and qualitatively evaluate our approach by showing usage scenarios and visualizing convolutional layers. This paper is a first step towards using deep learning based methods to quantitatively evaluate images from the visualization field.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9124317",
                    "name": "Hammad Haleem"
                },
                {
                    "authorId": "47905785",
                    "name": "Y. Wang"
                },
                {
                    "authorId": "2542336",
                    "name": "Abishek Puri"
                },
                {
                    "authorId": "5002401",
                    "name": "Sahil Wadhwa"
                },
                {
                    "authorId": "2300486168",
                    "name": "Huamin Qu"
                }
            ]
        },
        {
            "paperId": "c1ffa23af64ec04ed316b26943642eb9e6a10c60",
            "title": "Chaotic hash function based plain-image dependent block ciphering technique",
            "abstract": "Secure hashes have an indispensable purpose to play in modern multimedia image encryptions. Traditional block ciphering techniques are quite complex, command colossal processing time for key generation and sometimes are a source of redundancy. This paper proposes to suggest a method for designing one-way cryptographic hash function and a block ciphering scheme based on proposed hash codes. In the proposed work, we have divided the message into blocks with each block individually processed by chaotic systems. The transitional hashes are created utilizing advanced control and input parameters. The two hash codes are utilized to create a final hash. The experimental outcomes justify striking performance of proposed chaotic hash method. Moreover, the generated hash code is applied for realizing an image block ciphering technique. The encryption process is plain-image dependent thereby exhibits satisfactory encryption effect suitable for practical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5002401",
                    "name": "Sahil Wadhwa"
                },
                {
                    "authorId": "48767189",
                    "name": "Musheer Ahmad"
                },
                {
                    "authorId": "145458518",
                    "name": "Harsh Vijay"
                }
            ]
        }
    ]
}