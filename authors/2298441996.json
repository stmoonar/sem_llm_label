{
    "authorId": "2298441996",
    "papers": [
        {
            "paperId": "92e9acc55013a7408559d5e203eb913378563377",
            "title": "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects",
            "abstract": "Yor\\`ub\\'a an African language with roughly 47 million speakers encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resulting in disparities for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and speech corpus YOR\\`ULECT across three domains and four regional Yor\\`ub\\'a dialects. To develop this corpus, we engaged native speakers, travelling to communities where these dialects are spoken, to collect text and speech data. Using our newly created corpus, we conducted extensive experiments on (text) machine translation, automatic speech recognition, and speech-to-text translation. Our results reveal substantial performance disparities between standard Yor\\`ub\\'a and the other dialects across all tasks. However, we also show that with dialect-adaptive finetuning, we are able to narrow this gap. We believe our dataset and experimental analysis will contribute greatly to developing NLP tools for Yor\\`ub\\'a and its dialects, and potentially for other African languages, by improving our understanding of existing challenges and offering a high-quality dataset for further development. We release YOR\\`ULECT dataset and models publicly under an open license.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2229432740",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2056773747",
                    "name": "Anuoluwapo Aremu"
                },
                {
                    "authorId": "2309005330",
                    "name": "Diana Abagyan"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "2518906",
                    "name": "David Ifeoluwa Adelani"
                },
                {
                    "authorId": "2266838146",
                    "name": "D. Abolade"
                },
                {
                    "authorId": "2298441996",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2287930119",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "9b9e44d43eb6bc0be7a07a92b2100dd6a7f3b158",
            "title": "Learning Syntax Without Planting Trees: Understanding When and Why Transformers Generalize Hierarchically",
            "abstract": "Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge. We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically. We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure. When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order). Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2030973547",
                    "name": "Madhur Panwar"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2298441996",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2286065792",
                    "name": "Navin Goyal"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "9ef2d0327c73393adb509d5793f707447cad2faf",
            "title": "MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization",
            "abstract": "In multilingual settings, non-Latin scripts and low-resource languages are usually disadvantaged in terms of language models' utility, efficiency, and cost. Specifically, previous studies have reported multiple modeling biases that the current tokenization algorithms introduce to non-Latin script languages, the main one being over-segmentation. In this work, we propose MAGNET; multilingual adaptive gradient-based tokenization to reduce over-segmentation via adaptive gradient-based subword tokenization. MAGNET learns to predict segment boundaries between byte tokens in a sequence via sub-modules within the model, which act as internal boundary predictors (tokenizers). Previous gradient-based tokenization methods aimed for uniform compression across sequences by integrating a single boundary predictor during training and optimizing it end-to-end through stochastic reparameterization alongside the next token prediction objective. However, this approach still results in over-segmentation for non-Latin script languages in multilingual settings. In contrast, MAGNET offers a customizable architecture where byte-level sequences are routed through language-script-specific predictors, each optimized for its respective language script. This modularity enforces equitable segmentation granularity across different language scripts compared to previous methods. Through extensive experiments, we demonstrate that in addition to reducing segmentation disparities, MAGNET also enables faster language modelling and improves downstream utility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2229432740",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2282203839",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "2311117905",
                    "name": "Valentin Hoffman"
                },
                {
                    "authorId": "1666636295",
                    "name": "Tomasz Limisiewicz"
                },
                {
                    "authorId": "2287930119",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2298441996",
                    "name": "Noah A. Smith"
                }
            ]
        }
    ]
}