{
    "authorId": "1993956",
    "papers": [
        {
            "paperId": "08d74f55bcd59c4b05e150b0d276a60ac46ec592",
            "title": "Distributed Min\u2013Max Learning Scheme for Neural Networks With Applications to High-Dimensional Classification",
            "abstract": "In this article, a novel learning methodology is introduced for the problem of classification in the context of high-dimensional data. In particular, the challenges introduced by high-dimensional data sets are addressed by formulating a $L_{1}$ regularized zero-sum game where optimal sparsity is estimated through a two-player game between the penalty coefficients/sparsity parameters and the deep neural network weights. In order to solve this game, a distributed learning methodology is proposed where additional variables are utilized to derive layerwise cost functions. Finally, an alternating minimization approach developed to solve the problem where the Nash solution provides optimal sparsity and compensation through the classifier. The proposed learning approach is implemented in a parallel and distributed environment through a novel computational algorithm. The efficiency of the approach is demonstrated both theoretically and empirically with nine data sets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "153726118",
                    "name": "Krishnan Raghavan"
                },
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                },
                {
                    "authorId": "1993956",
                    "name": "V. A. Samaranayake"
                }
            ]
        },
        {
            "paperId": "6d51dd174a9ab38cb98b7f808a90375f68891c03",
            "title": "Direct Error-Driven Learning for Deep Neural Networks With Applications to Big Data",
            "abstract": "In this brief, heterogeneity and noise in big data are shown to increase the generalization error for a traditional learning regime utilized for deep neural networks (deep NNs). To reduce this error, while overcoming the issue of vanishing gradients, a direct error-driven learning (EDL) scheme is proposed. First, to reduce the impact of heterogeneity and data noise, the concept of a neighborhood is introduced. Using this neighborhood, an approximation of generalization error is obtained and an overall error, comprised of learning and the approximate generalization errors, is defined. A novel NN weight-tuning law is obtained through a layer-wise performance measure enabling the direct use of overall error for learning. Additional constraints are introduced into the layer-wise performance measure to guide and improve the learning process in the presence of noisy dimensions. The proposed direct EDL scheme effectively addresses the issue of heterogeneity and noise while mitigating vanishing gradients and noisy dimensions. A comprehensive simulation study is presented where the proposed approach is shown to mitigate the vanishing gradient problem while improving generalization by 6%.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245757298",
                    "name": "Raghavan Krishnan"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                },
                {
                    "authorId": "1993956",
                    "name": "V. A. Samaranayake"
                }
            ]
        },
        {
            "paperId": "a31a34e4b289defd596f7a24ad63bda77cc0e288",
            "title": "Predicting lifespan of Drosophila melanogaster: A novel application of convolutional neural networks and zero\u2010inflated autoregressive conditional Poisson model",
            "abstract": "A model to classify the lifespan of Drosophila, the fruit fly, into short\u2010 and long\u2010lived categories based on a sleep characteristic, extracted from activity data, is developed using a two\u2010stage process. Stage 1 models the per\u2010minute activity counts of each fly using a zero\u2010inflated autoregressive conditional Poisson model. These probabilities are allowed to vary hourly, reflecting the circadian and other cycles present in a fly's sleep architecture. A 5\u2010day moving window is used to model data allowing the model parameters to vary over the course of the fly's life. The resulting probabilities capture information about changes in sleep patterns with age and are hypothesized to contain features that help categorize flies into short\u2010 and long\u2010lived groups. The resulting hourly zero\u2010inflation probabilities over a 24\u2010day period are utilized to create a \u2018heat map\u2019 containing information on the 24\u2010hour daily sleep cycle and its changes across the 24\u2010day observation period. In Stage 2, the heat maps for individual flies are used as inputs to a convolutional neural network to build a classification model. The estimated model provides a reasonably accurate way to group flies into lifespan categories. Grouping flies into such categories would facilitate the discovery of biochemical markers of aging.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153914873",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "1993956",
                    "name": "V. A. Samaranayake"
                },
                {
                    "authorId": "5320876",
                    "name": "G. Olbricht"
                },
                {
                    "authorId": "3207088",
                    "name": "M. Thimgan"
                }
            ]
        },
        {
            "paperId": "6b7def612244bfb77abf7b80ed5ff316fbdb8cec",
            "title": "A Multi-Step Nonlinear Dimension-Reduction Approach with Applications to Big Data",
            "abstract": "In this paper, a novel dimension-reduction approach is presented to overcome challenges such as nonlinear relationships, heterogeneity, and noisy dimensions. Initially, the <inline-formula><tex-math notation=\"LaTeX\">$p$</tex-math><alternatives><mml:math><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href=\"raghavan-ieq1-2876848.gif\"/></alternatives></inline-formula> attributes in the data are first organized into random groups. Next, to systematically remove redundant and noisy dimensions from the data, each group is independently mapped into a low dimensional space via a parametric mapping. The group-wise transformation parameters are estimated using a low-rank approximation of distance covariance. The transformed attributes are reorganized into groups based on the magnitude of their respective eigenvalues. The group-wise organization and reduction process is performed until a user-defined criterion on eigenvalues is satisfied. In addition, novel procedures are introduced to aggregate the transformation parameters when the data is available in batches. Overall performance is demonstrated with extensive simulation analysis on classification by employing 10 data-sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245757298",
                    "name": "Raghavan Krishnan"
                },
                {
                    "authorId": "1993956",
                    "name": "V. A. Samaranayake"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "1e70839c6a8dfe2a3fd57b79054d8d700aaa34f6",
            "title": "A Minimax Approach for Classification with Big-data",
            "abstract": "In this paper, a novel methodology to reduce the generalization errors occurring due to domain shift in big data classification is presented. This reduction is achieved by introducing a suitably selected domain shift to the training data via what is referred to as \"distortion model\". These distortions are introduced through an affine transformation and additional data-samples are obtained. Next, a deep neural network (NN), referred as \"classifier\", is used to classify both the original and the additional data samples. By learning from both the original and additional data-samples, the classifier compensates for the domain shift while maintaining its performance on original data. However, as the exact magnitude of the shift one would encounter in real applications is unknown a priori and difficult to predict. The objective is to compensate for the optimal shift that can be introduced by the distortion model without significantly degrading the performance of the model. A two-player zero-sum game is thus designed where the first player is the distortion model with the aim of increasing the domain shift. The classifier then becomes the second player whose aim is to minimize the impact of domain shift. Finally, a direct error-driven learning scheme is utilized to minimize the impact of the classifier while maximizing the domain shift. A comprehensive simulation study is presented where a 12% improvement in the presence of domain shift is demonstrated. The proposed approach is also shown to improve generalization by 6%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37074394",
                    "name": "R. Krishnan"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                },
                {
                    "authorId": "1993956",
                    "name": "V. A. Samaranayake"
                }
            ]
        },
        {
            "paperId": "c3a377bcee0ff340376820cae7795b0d8bb9ea47",
            "title": "Distributed Learning of Deep Sparse Neural Networks for High-dimensional Classification",
            "abstract": "While analyzing high dimensional data-sets using deep neural network (NN), increased sparsity is desirable but requires careful selection of \"sparsity parameters.\" In this paper, a novel distributed learning methodology is proposed to optimize the NN while addressing this challenge. To address this challenge, the optimal sparsity in the NN is estimated via a two player zero-sum game in the paper. In the proposed game, sparsity parameter is the first player with the aim of increasing sparsity in the NN while NN weights is the second player with the goal of improving its performance in the presence of increased sparsity. To solve the game, additional variables are introduced into the optimization problem such that the output at every layer in the NN depends on this variable instead of the previous layer. Using these additional variables, layer wise cost-functions are derived that are then independently optimized to learn the additional variables, NN weights and the sparsity parameters. To implement the proposed learning procedure in a parallelized and distributed environment, a novel computational algorithm is also proposed. The efficiency of the proposed approach is demonstrated using a total of six data-sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "2130988272",
                    "name": "R. Krishnan"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                },
                {
                    "authorId": "1993956",
                    "name": "V. A. Samaranayake"
                }
            ]
        },
        {
            "paperId": "75eb2c0e6721cd3f938293011733c4bb389cab1b",
            "title": "Deep learning inspired prognostics scheme for applications generating big data",
            "abstract": "In this paper, the relevance of deep neural network (DNN) is studied in big data scenarios, specifically for prognostics applications. It is observed that fault predictions can be performed more efficiently when DNN is used with a pre-processing step. A novel hierarchical dimension reduction (HDR) approach is therefore proposed as a pre-processing step to DNN. This two-step approach is shown to be effective in extracting value from complex and uncertain big data. It is shown that use of HDR prior to DNN improves convergence and allows for the possibility of reduction in model size without any drop in accuracy. A comprehensive methodology is developed to facilitate prognostics using DNN. Simulation results are included to demonstrate the overall methodology using big data-sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130988272",
                    "name": "R. Krishnan"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                },
                {
                    "authorId": "1993956",
                    "name": "V. A. Samaranayake"
                }
            ]
        },
        {
            "paperId": "bfffc64e07a9158887f3422cba085d9771eec55e",
            "title": "Prediction Intervals for Time Series: A Modified Sieve Bootstrap Approach",
            "abstract": "Traditional Box\u2013Jenkins prediction intervals perform poorly when the innovations are not Gaussian. Nonparametric bootstrap procedures overcome this handicap, but most existing methods assume that the AR and MA orders of the process are known. The sieve bootstrap approach requires no such assumption but produces liberal coverage due to the use of residuals that underestimate the actual variance of the innovations and the failure of the methods to capture variations due to sampling error of the mean. A modified approach, that corrects these deficiencies, is implemented. Monte Carlo simulations results show that the modified version achieves nominal or near nominal coverage.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35320607",
                    "name": "P. Mukhopadhyay"
                },
                {
                    "authorId": "1993956",
                    "name": "V. A. Samaranayake"
                }
            ]
        }
    ]
}