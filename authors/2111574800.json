{
    "authorId": "2111574800",
    "papers": [
        {
            "paperId": "200a19739ef76cb91c490be72d409f0fb0468901",
            "title": "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models",
            "abstract": "Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates. The standard process for iterative alignment of LLMs involves collecting new human feedback for each update. However, the data collection process is costly and challenging to scale. To address this issue, we introduce the\"TS-Align\"framework, which fine-tunes a policy model using pairwise feedback data automatically mined from its outputs. This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a small-scale student model. The policy fine-tuning process can be iteratively repeated using on-policy generations within our proposed teacher-student collaborative framework. Through extensive experiments, we demonstrate that our final aligned policy outperforms the base policy model with an average win rate of 69.7% across seven conversational or instruction-following datasets. Furthermore, we show that the ranking capability of the teacher is effectively distilled into the student through our pipeline, resulting in a small-scale yet effective reward model for policy model alignment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2258775321",
                    "name": "Chengguang Tang"
                },
                {
                    "authorId": "2303841789",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "2258553844",
                    "name": "Ke Shi"
                },
                {
                    "authorId": "2065465380",
                    "name": "Guohua Tang"
                },
                {
                    "authorId": "2303951713",
                    "name": "Feng Jiang"
                },
                {
                    "authorId": "2302787029",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "21b4777948797377deedf4a9f1f58ad13f6b8b5d",
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2887412",
                    "name": "Jinchao Li"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2066415714",
                    "name": "Zhengcong Fei"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2257374643",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2116575668",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "1979505",
                    "name": "A. Geramifard"
                },
                {
                    "authorId": "1765212",
                    "name": "Chiori Hori"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "4e2f99a2ad6660825a8b3cac03eb47dec2c06a1b",
            "title": "CrossTune: Black-Box Few-Shot Classification with Label Enhancement",
            "abstract": "Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization of CrossTune, we utilize ChatGPT to generate additional training data through in-context learning. A switch mechanism is implemented to exclude low-quality ChatGPT-generated data. Through extensive experiments on seven benchmark text classification datasets, we demonstrate that our proposed approach outperforms the previous state-of-the-art gradient-free black-box tuning method by 5.7% on average. Even without using ChatGPT-augmented data, CrossTune performs better or comparably than previous black-box tuning methods, suggesting the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292191740",
                    "name": "Danqing Luo"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2256617270",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2240763421",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "8325631bba5ef8492d7171c35dbd2b64fef11897",
            "title": "Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial Framework Driven by Large Language Models",
            "abstract": "The automatic evaluation of natural language generation (NLG) systems presents a long-lasting challenge. Recent studies have highlighted various neural metrics that align well with human evaluations. Yet, the robustness of these evaluators against adversarial perturbations remains largely under-explored due to the unique challenges in obtaining adversarial data for different NLG evaluation tasks. To address the problem, we introduce AdvEval, a novel black-box adversarial framework against NLG evaluators. AdvEval is specially tailored to generate data that yield strong disagreements between human and victim evaluators. Specifically, inspired by the recent success of large language models (LLMs) in text generation and evaluation, we adopt strong LLMs as both the data generator and gold evaluator. Adversarial data are automatically optimized with feedback from the gold and victim evaluator. We conduct experiments on 12 victim evaluators and 11 NLG datasets, spanning tasks including dialogue, summarization, and question evaluation. The results show that AdvEval can lead to significant performance degradation of various victim metrics, thereby validating its efficacy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188651547",
                    "name": "Yiming Chen"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2292191740",
                    "name": "Danqing Luo"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1726720",
                    "name": "R. Tan"
                },
                {
                    "authorId": "2302787029",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "90d04e5f1cb6efb12e14e388730cf58666325edb",
            "title": "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators",
            "abstract": "Automatic evaluation is an integral aspect of dialogue system research. The traditional reference-based NLG metrics are generally found to be unsuitable for dialogue assessment. Consequently, recent studies have suggested various unique, reference-free neural metrics that better align with human evaluations. Notably among them, large language models (LLMs), particularly the instruction-tuned variants like ChatGPT, are shown to be promising substitutes for human judges. Yet, existing works on utilizing LLMs for automatic dialogue evaluation are limited in their scope in terms of the number of meta-evaluation datasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains inconclusive how effective these LLMs are. To this end, we conduct a comprehensive study on the application of LLMs for automatic dialogue evaluation. Specifically, we analyze the multi-dimensional evaluation capability of 30 recently emerged LLMs at both turn and dialogue levels, using a comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the robustness of the LLMs in handling various adversarial perturbations at both turn and dialogue levels. Finally, we explore how model-level and dimension-level ensembles impact the evaluation performance. All resources are available at https://github.com/e0397123/comp-analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "2188651547",
                    "name": "Yiming Chen"
                },
                {
                    "authorId": "2145120631",
                    "name": "Malu Zhang"
                },
                {
                    "authorId": "2258605037",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "9799c17fd287bb9e8d231fe032c6dbf9c0c9d675",
            "title": "Overview of Robust and Multilingual Automatic Evaluation Metrics\n\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4",
            "abstract": "The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220406508",
                    "name": "Mario Rodr'iguez-Cantelar"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1672552269",
                    "name": "Chengguang Tang"
                },
                {
                    "authorId": "2026468506",
                    "name": "Ke Shi"
                },
                {
                    "authorId": "3022427",
                    "name": "Sarik Ghazarian"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "b64b2d28e00e1bdf35393856707cbd133058abab",
            "title": "xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark",
            "abstract": "Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined. This is largely due to the absence of a multilingual dialogue evaluation benchmark. To address the issue, we introduce xDial-Eval, built on top of open-source English dialogue evaluation datasets. xDial-Eval includes 12 turn-level and 6 dialogue-level English datasets, comprising 14930 annotated turns and 8691 annotated dialogues respectively. The English dialogue data are extended to nine other languages with commercial machine translation systems. On xDial-Eval, we conduct comprehensive analyses of previous BERT-based metrics and the recently-emerged large language models. Lastly, we establish strong self-supervised and multilingual baselines. In terms of average Pearson correlations over all datasets and languages, the best baseline outperforms OpenAI's ChatGPT by absolute improvements of 6.5% and 4.6% at the turn and dialogue levels respectively, albeit with much fewer parameters. The data and code are publicly available at https://github.com/e0397123/xDial-Eval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "2258775321",
                    "name": "Chengguang Tang"
                },
                {
                    "authorId": "2258553844",
                    "name": "Ke Shi"
                },
                {
                    "authorId": "2065465380",
                    "name": "Guohua Tang"
                },
                {
                    "authorId": "2258605037",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "b6787179220b6aea5317d072f409bf4ed776cd62",
            "title": "Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation",
            "abstract": "Training or finetuning large-scale language models (LLMs) such as GPT-3 requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One practical area of research is to treat these models as black boxes and interact with them through their inference APIs. In this paper, we investigate how to optimize few-shot text classification without accessing the gradients of the LLMs. To achieve this, we treat the black-box model as a feature extractor and train a classifier with the augmented text data. Data augmentation is performed using prompt-based finetuning on an auxiliary language model with a much smaller parameter size than the black-box model. Through extensive experiments on eight text classification datasets, we show that our approach, dubbed BT-Classifier, significantly outperforms state-of-the-art black-box few-shot learners and performs on par with methods that rely on full-model tuning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2061548489",
                    "name": "Dan Luo"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2303914132",
                    "name": "Jiahui Xu"
                },
                {
                    "authorId": "2222324269",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2188651547",
                    "name": "Yiming Chen"
                },
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2119251083",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "3eda2fc2dbae1d6255c54667df3e63ff39621c2d",
            "title": "Analyzing and Evaluating Faithfulness in Dialogue Summarization",
            "abstract": "Dialogue summarization is abstractive in nature, making it suffer from factual errors. The factual correctness of summaries has the highest priority before practical applications. Many efforts have been made to improve faithfulness in text summarization. However, there is a lack of systematic study on dialogue summarization systems. In this work, we first perform the fine-grained human analysis on the faithfulness of dialogue summaries and observe that over 35% of generated summaries are faithfully inconsistent respective the source dialogues. Furthermore, we present a new model-level faithfulness evaluation method. It examines generation models with multi-choice questions created by rule-based transformations. Experimental results show that our evaluation schema is a strong proxy for the factual correctness of summarization models. The human-annotated faithfulness samples and the evaluation toolkit are released to facilitate future research toward faithful dialogue summarization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2222324269",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2188651547",
                    "name": "Yiming Chen"
                },
                {
                    "authorId": "2119251083",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "45c55fe92abc0ed4c2190fe039c9a23d70a0e33a",
            "title": "Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges",
            "abstract": "This is a report on the NSF Future Directions Workshop on Automatic Evaluation of Dialog. The workshop explored the current state of the art along with its limitations and suggested promising directions for future work in this important and very rapidly changing area of research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32251567",
                    "name": "Shikib Mehri"
                },
                {
                    "authorId": "2155313947",
                    "name": "Jinho Choi"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "1716325",
                    "name": "M. Esk\u00e9nazi"
                },
                {
                    "authorId": "1768624",
                    "name": "Milica Gasic"
                },
                {
                    "authorId": "3194430",
                    "name": "Kallirroi Georgila"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "1681799",
                    "name": "Verena Rieser"
                },
                {
                    "authorId": "39049552",
                    "name": "Samira Shaikh"
                },
                {
                    "authorId": "144518646",
                    "name": "D. Traum"
                },
                {
                    "authorId": "47999368",
                    "name": "Yi-Ting Yeh"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2159589560",
                    "name": "Yizhe Zhang"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                }
            ]
        }
    ]
}