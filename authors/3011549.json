{
    "authorId": "3011549",
    "papers": [
        {
            "paperId": "4d5326408cc5b894fd4e2cc36e3aa5ee4bb2f1c2",
            "title": "Learning Entity Linking Features for Emerging Entities",
            "abstract": "Entity linking (EL) is the process of linking entity mentions appearing in text with their corresponding entities in a knowledge base. EL features of entities (e.g., prior probability, relatedness score, and entity embedding) are usually estimated based on Wikipedia. However, for newly emerging entities (EEs) which have just been discovered in news, they may still not be included in Wikipedia yet. As a consequence, it is unable to obtain required EL features for those EEs from Wikipedia and EL models will always fail to link ambiguous mentions with those EEs correctly as the absence of their EL features. To deal with this problem, in this paper we focus on a new task of learning EL features for emerging entities in a general way. We propose a novel approach called STAMO to learn high-quality EL features for EEs automatically, which needs just a small number of labeled documents for each EE collected from the Web, as it could further leverage the knowledge hidden in the unlabeled data. STAMO is mainly based on self-training, which makes it flexibly integrated with any EL feature or EL model, but also makes it easily suffer from the error reinforcement problem caused by the mislabeled data. Instead of some common self-training strategies that try to throw the mislabeled data away explicitly, we regard self-training as a multiple optimization process with respect to the EL features of EEs, and propose both intra-slot and inter-slot optimizations to alleviate the error reinforcement problem implicitly. We construct two EL datasets involving selected EEs to evaluate the quality of obtained EL features for EEs, and the experimental results show that our approach significantly outperforms other baseline methods of learning EL features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3011549",
                    "name": "Chenwei Ran"
                },
                {
                    "authorId": "144084234",
                    "name": "Wei Shen"
                },
                {
                    "authorId": "1810902946",
                    "name": "Jianbo Gao"
                },
                {
                    "authorId": "2110539208",
                    "name": "Yuhan Li"
                },
                {
                    "authorId": "2447408",
                    "name": "Jianyong Wang"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                }
            ]
        },
        {
            "paperId": "8862f743fb17fc427934b6e442f3b5b440cbf362",
            "title": "An Attention Factor Graph Model for Tweet Entity Linking",
            "abstract": "The rapid expansion of Twitter has attracted worldwide attention. With more than 500 million tweets posted per day, Twitter becomes an invaluable information and knowledge source. Many Twitter related tasks have been studied, such as event extraction, hashtag recommendation, and topic detection. A critical step in understanding and mining information from Twitter is to disambiguate entities in tweets, i.e., tweet entity linking. It is a challenging task because tweets are short, noisy, and fresh. Many tweet-specific signals have been found to solve the tweet entity linking problem, such as user interest, temporal popularity, location information and so on. However, two common weaknesses exist in previous work. First, most proposed models are not flexible and extendable to fit new signals. Second, their scalability is not good enough to handle the large-scale social network like Twitter. In this work, we formalize the tweet entity linking problem into a factor graph model which has shown its effectiveness and efficiency in many other applications. We also propose selective attention over entities to increase the scalability of our model, which brings linear complexity. To adopt the attention mechanism in the factor graph, we propose a new type of nodes called pseudo-variable nodes to solve the asymmetry attention problem caused by the undirected characteristic of the factor graph. We evaluated our model on two different manually annotated tweet datasets. The experimental results show that our model achieves better performance in terms of both effectiveness and efficiency compared with the state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3011549",
                    "name": "Chenwei Ran"
                },
                {
                    "authorId": "144084234",
                    "name": "Wei Shen"
                },
                {
                    "authorId": "2447408",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "a5902e27db14ad7319f047f30cd72f50a26f9ed9",
            "title": "ISCAS_Sogou at TAC-KBP 2017",
            "abstract": "ISCAS_Sogou participated in the entire Chinese Coldstart task in TAC-KBP 2017. This paper describes our cold start knowledge base population system. Our system consists of four components: Entity Discovery and Coreference, Relation Extraction, Event Extraction and BeSt Detection. We first briefly describe the architecture of our system. And then we introduce each module in detail.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3194601",
                    "name": "Xianpei Han"
                },
                {
                    "authorId": "49195810",
                    "name": "Xiliang Song"
                },
                {
                    "authorId": "48444897",
                    "name": "Hongyu Lin"
                },
                {
                    "authorId": "2152209135",
                    "name": "Qichen Zhu"
                },
                {
                    "authorId": "1831434",
                    "name": "Yaojie Lu"
                },
                {
                    "authorId": null,
                    "name": "Le Sun"
                },
                {
                    "authorId": "2774294",
                    "name": "Jingfang Xu"
                },
                {
                    "authorId": "2437444",
                    "name": "Mingrong Liu"
                },
                {
                    "authorId": "2319970",
                    "name": "Ranxu Su"
                },
                {
                    "authorId": "2068282867",
                    "name": "Sheng Shang"
                },
                {
                    "authorId": "3011549",
                    "name": "Chenwei Ran"
                },
                {
                    "authorId": "2084279107",
                    "name": "Feifei Xue"
                }
            ]
        },
        {
            "paperId": "a5017b89f62d77377d4aba564b3a7009556b4c14",
            "title": "Domain-Specific Knowledge Base Enrichment Using Wikipedia Tables",
            "abstract": "The knowledge base is a machine-readable set of knowledge. More and more multi-domain and large-scale knowledge bases have emerged in recent years, and they play an essential role in many information systems and semantic annotation tasks. However we do not have a perfect knowledge base yet and maybe we will never have a perfect one, because all the knowledge bases have limited coverage while new knowledge continues to emerge. Therefore populating and enriching the existing knowledge base become important tasks. Traditional knowledge base population task usually leverages the information embedded in the unstructured free text. Recently researchers found that massive structured tables on the Web are high-quality relational data and easier to be utilized than the unstructured text. Our goal of this paper is to enrich the knowledge base using Wikipedia tables. Here, knowledge means binary relations between entities and we focus on the relations in some specific domains. There are two basic types of information can be used in this task: the existing relation instances and the connection between types and relations. We firstly propose two basic probabilistic models based on two types of information respectively. Then we propose a light-weight aggregated model to combine the advantages of basic models. The experimental results show that our method is an effective approach to enriching the knowledge base with both high precision and recall.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3011549",
                    "name": "Chenwei Ran"
                },
                {
                    "authorId": "144084234",
                    "name": "Wei Shen"
                },
                {
                    "authorId": "2447408",
                    "name": "Jianyong Wang"
                },
                {
                    "authorId": "144809121",
                    "name": "Xuan Zhu"
                }
            ]
        }
    ]
}