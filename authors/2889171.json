{
    "authorId": "2889171",
    "papers": [
        {
            "paperId": "6b0cf0b104fca13ed766852b03d8c65f4bd83a6e",
            "title": "On factorization of rank-one auto-correlation matrix polynomials",
            "abstract": "This article characterizes the rank-one factorization of auto-correlation matrix polynomials. We establish a sufficient and necessary uniqueness condition for uniqueness of the factorization based on the greatest common divisor (GCD) of multiple polynomials. In the unique case, we show that the factorization can be carried out explicitly using GCDs. In the non-unique case, the number of non-trivially different factorizations is given and all solutions are enumerated.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2616556",
                    "name": "K. Usevich"
                },
                {
                    "authorId": "30164482",
                    "name": "Julien Flamant"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "1779178",
                    "name": "D. Brie"
                }
            ]
        },
        {
            "paperId": "779958699c1d5bb40e9887efab8b01f5d152087f",
            "title": "The Barycenter in Free Nilpotent Lie Groups and Its Application to Iterated-Integrals Signatures",
            "abstract": "We establish the well-definedness of the barycenter (in the sense of Buser and Karcher) for every integrable measure on the free nilpotent Lie group of step $L$ (over $\\mathbb{R}^d$). We provide two algorithms for computing it, using methods from Lie theory (namely, the Baker-Campbell-Hausdorff formula) and from the theory of Gr\\\"obner bases of modules. Our main motivation stems from measures induced by iterated-integrals signatures, and we calculate the barycenter for the signature of the Brownian motion.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "35155041",
                    "name": "J. Diehl"
                },
                {
                    "authorId": "2218899279",
                    "name": "Raphael Mignot"
                },
                {
                    "authorId": "122769819",
                    "name": "Leonard Schmitz"
                },
                {
                    "authorId": "2218486221",
                    "name": "Nozomi Sugiura"
                },
                {
                    "authorId": "2281981510",
                    "name": "Konstantin Usevich"
                }
            ]
        },
        {
            "paperId": "c9c3ae012b682d97cba62dca0f579c368fe6aa94",
            "title": "Low-Rank Updates of pre-trained Weights for Multi-Task Learning",
            "abstract": "Multi-Task Learning used with pre-trained models has been quite popular in the field of Natural Language Processing in recent years. This framework remains still challenging due to the complexity of the tasks and the challenges associated with fine-tuning large pre-trained models. In this paper, we propose a new approach for Multi-task learning which is based on stacking the weights of Neural Networks as a tensor. We show that low-rank updates in the canonical polyadic tensor decomposition of this tensor of weights lead to a simple, yet efficient algorithm, which without loss of performance allows to reduce considerably the model parameters. We investigate the interactions between tasks inside the model as well as the inclusion of sparsity to find the best tensor rank and to increase the compression rate. Our strategy is consistent with recent efforts that attempt to use constraints to fine-tune some model components. More precisely, we achieve equivalent performance as the state-of-the-art on the General Language Understanding Evaluation benchmark by training only 0.3% of the parameters per task while not modifying the baseline weights.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223158865",
                    "name": "Alexandre Audibert"
                },
                {
                    "authorId": "144444438",
                    "name": "Massih-Reza Amini"
                },
                {
                    "authorId": "2616556",
                    "name": "K. Usevich"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                }
            ]
        },
        {
            "paperId": "2009a4e68c2ddf1dac5d6b07adaa6885347d7898",
            "title": "Wind power predictions from nowcasts to 4-hour forecasts: a learning approach with variable selection",
            "abstract": "We study short-term prediction of wind speed and wind power (every 10 minutes up to 4 hours ahead). Accurate forecasts for these quantities are crucial to mitigate the negative effects of wind farms' intermittent production on energy systems and markets. We use machine learning to combine outputs from numerical weather prediction models with local observations. The former provide valuable information on higher scales dynamics while the latter gives the model fresher and location-specific data. So as to make the results usable for practitioners, we focus on well-known methods which can handle a high volume of data. We study first variable selection using both a linear technique and a nonlinear one. Then we exploit these results to forecast wind speed and wind power still with an emphasis on linear models versus nonlinear ones. For the wind power prediction, we also compare the indirect approach (wind speed predictions passed through a power curve) and the indirect one (directly predict wind power).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2075126563",
                    "name": "Dimitri Bouche"
                },
                {
                    "authorId": "145258331",
                    "name": "R\u00e9mi Flamary"
                },
                {
                    "authorId": "2065694631",
                    "name": "Florence d'Alch'e-Buc"
                },
                {
                    "authorId": "66567985",
                    "name": "R. Plougonven"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "50366076",
                    "name": "J. Badosa"
                },
                {
                    "authorId": "1939452",
                    "name": "P. Drobinski"
                }
            ]
        },
        {
            "paperId": "4833a3266e774083938785963a5c9815e0f21257",
            "title": "Learning over No-Preferred and Preferred Sequence of Items for Robust Recommendation (Extended Abstract)",
            "abstract": "This paper is an extended version of [Burashnikova et al., 2021, arXiv: 2012.06910], where we proposed a theoretically supported sequential strategy for training a large-scale Recommender System (RS) over implicit feedback, mainly in the form of clicks. The proposed approach consists in minimizing pairwise ranking loss over blocks of consecutive items constituted by a sequence of non-clicked items followed by a clicked one for each user. We present two variants of this strategy where model parameters are updated using either the momentum method or a gradient-based approach. To prevent updating the parameters for an abnormally high number of clicks over some targeted items (mainly due to bots), we introduce an upper and a lower threshold on the number of updates for each user. These thresholds are estimated over the distribution of the number of blocks in the training set. They affect the decision of RS by shifting the distribution of items that are shown to the users. Furthermore, we provide a convergence analysis of both algorithms and demonstrate their practical efficiency over six large-scale collections with respect to various ranking measures.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2037379701",
                    "name": "Aleksandra Burashnikova"
                },
                {
                    "authorId": "3147078",
                    "name": "Yury Maximov"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "1947597",
                    "name": "Charlotte Laclau"
                },
                {
                    "authorId": "3060709",
                    "name": "F. Iutzeler"
                },
                {
                    "authorId": "144444438",
                    "name": "Massih-Reza Amini"
                }
            ]
        },
        {
            "paperId": "6bbd369edd2b43d35a0c5b21cbf2907a41a781cc",
            "title": "IAI @ SocialDisNER : Catch me if you can! Capturing complex disease mentions in tweets",
            "abstract": "Biomedical NER is an active research area today. Despite the availability of state-of-the-art models for standard NER tasks, their performance degrades on biomedical data due to OOV entities and the challenges encountered in specialized domains. We use Flair-NER framework to investigate the effectiveness of various contextual and static embeddings for NER on Spanish tweets, in particular, to capture complex disease mentions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1935766425",
                    "name": "Aman Sinha"
                },
                {
                    "authorId": "2183786393",
                    "name": "Cristina Garc\u00eda Holgado"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "2065729808",
                    "name": "Matthieu Constant"
                }
            ]
        },
        {
            "paperId": "2804a137bbfb7267fc290e05d6a4a1a38f6cc1fd",
            "title": "Bilingual Topic Models for Comparable Corpora",
            "abstract": "Probabilistic topic models like Latent Dirichlet Allocation (LDA) have been previously extended to the bilingual setting. A fundamental modeling assumption in several of these extensions is that the input corpora are in the form of document pairs whose constituent documents share a single topic distribution. However, this assumption is strong for comparable corpora that consist of documents thematically similar to an extent only, which are, in turn, the most commonly available or easy to obtain. In this paper we relax this assumption by proposing for the paired documents to have separate, yet bound topic distributions. % a binding mechanism between the distributions of the paired documents. We suggest that the strength of the bound should depend on each pair's semantic similarity. To estimate the similarity of documents that are written in different languages we use cross-lingual word embeddings that are learned with shallow neural networks. We evaluate the proposed binding mechanism by extending two topic models: a bilingual adaptation of LDA that assumes bag-of-words inputs and a model that incorporates part of the text structure in the form of boundaries of semantically coherent segments. To assess the performance of the novel topic models we conduct intrinsic and extrinsic experiments on five bilingual, comparable corpora of English documents with French, German, Italian, Spanish and Portuguese documents. The results demonstrate the efficiency of our approach in terms of both topic coherence measured by the normalized point-wise mutual information, and generalization performance measured by perplexity and in terms of Mean Reciprocal Rank in a cross-lingual document retrieval task for each of the language pairs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1951080",
                    "name": "Georgios Balikas"
                },
                {
                    "authorId": "144444438",
                    "name": "Massih-Reza Amini"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                }
            ]
        },
        {
            "paperId": "627a636b59d13934b9e2e4e50d5cd017675aa45c",
            "title": "Learning over no-Preferred and Preferred Sequence of items for Robust Recommendation",
            "abstract": "In this paper, we propose a theoretically supported sequential strategy for training a large-scale Recommender System (RS) over implicit feedback, mainly in the form of clicks. The proposed approach consists in minimizing pairwise ranking loss over blocks of consecutive items constituted by a sequence of non-clicked items followed by a clicked one for each user. We present two variants of this strategy where model parameters are updated using either the momentum method or a gradient-based approach. To prevent updating the parameters for an abnormally high number of clicks over some targeted items (mainly due to bots), we introduce an upper and a lower threshold on the number of updates for each user. These thresholds are estimated over the distribution of the number of blocks in the training set. They affect the decision of RS by shifting the distribution of items that are shown to the users. Furthermore, we provide a convergence analysis of both algorithms and demonstrate their practical efficiency over six large-scale collections with respect to various ranking measures and computational time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2037379701",
                    "name": "Aleksandra Burashnikova"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "1947597",
                    "name": "Charlotte Laclau"
                },
                {
                    "authorId": "2037379373",
                    "name": "Frack Iutzeller"
                },
                {
                    "authorId": "3147078",
                    "name": "Yury Maximov"
                },
                {
                    "authorId": "144444438",
                    "name": "Massih-Reza Amini"
                }
            ]
        },
        {
            "paperId": "770bc0ae5dd70c8dd9ef3b86c16d3d78c2796d6b",
            "title": "Morpho-Statistical Description of Networks Through Graph Modelling and Bayesian Inference",
            "abstract": "Collaboration graphs are relevant sources of information to understand behavioural tendencies of groups of individuals. The study of these graphs enables figuring out factors that may affect the efficiency and the sustainability of cooperative work. For example, such a collaboration involves researchers who develop relationships with their external counterparts to address scientific challenges. As relations and projects change over time, the evolution of social structures must be tackled. We propose a statistical approach considering different structural collaboration patterns and captures the dynamic of the relational structures over the years. Our approach combines spatial processes modelling and Exponential Random Graph Models used to analyse social processes. Since the normalising constant involved in classical Markov Chain Monte Carlo (MCMC) approaches is intractable, the inference remains challenging. To overcome this issue, we propose a Bayesian tool that relies on the recent ABC Shadow algorithm. The method is illustrated on real data sets from an open archive of scholarly documents. Through a simple formalism, our approach highlights the interactions between the different types of social relations at stake in the collaboration network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1581628236",
                    "name": "Quentin Laporte-Chabasse"
                },
                {
                    "authorId": "40478427",
                    "name": "R. Stoica"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "2038513",
                    "name": "F. Charoy"
                },
                {
                    "authorId": "31259576",
                    "name": "G. Oster"
                }
            ]
        },
        {
            "paperId": "78db3fcebce6dcbfd30190601b85e58e42df8d4b",
            "title": "Nonlinear Functional Output Regression: a Dictionary Approach",
            "abstract": "In many fields, each data instance consists in a high number of measurements of the same underlying phenomenon. Such high dimensional data generally enjoys strong smoothness across features which can be exploited through functional modelling. In the setting of functional output regression, we introduce projection learning, a novel dictionary-based approach combining a representation of the output in a dictionary with the minimization of a functional loss. This general method is instantiated with square loss and reproducing kernel Hilbert spaces of vector-valued functions, allowing to impose some structure on the model. The resulting algorithm is backed theoretically with an excess risk bound leading to consistency, while experiments on several datasets show that it is competitive compared to other nonlinear approaches at a low computational cost. In addition, the method is shown to be versatile as it can deal with sparsely sampled functions and can be used with various dictionaries.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2075126563",
                    "name": "Dimitri Bouche"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "88884754",
                    "name": "F. Roueff"
                },
                {
                    "authorId": "2065694631",
                    "name": "Florence d'Alch'e-Buc"
                }
            ]
        },
        {
            "paperId": "cfdca322af4f08082f6addd5de7a16323934f3bc",
            "title": "Smooth And Consistent Probabilistic Regression Trees",
            "abstract": "We propose here a generalization of regression trees, referred to as Probabilistic Regression (PR) trees, that adapt to the smoothness of the prediction function relating input and output variables while preserving the interpretability of the prediction and being robust to noise. In PR trees, an observation is associated to all regions of a tree through a probability distribution that reflects how far the observation is to a region. We show that such trees are consistent, meaning that their error tends to 0 when the sample size tends to infinity, a property that has not been established for similar, previous proposals as Soft trees and Smooth Transition Regression trees. We further explain how PR trees can be used in different ensemble methods, namely Random Forests and Gradient Boosted Trees. Lastly, we assess their performance through extensive experiments that illustrate their benefits in terms of performance, interpretability and robustness to noise.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46222073",
                    "name": "Sami Alkhoury"
                },
                {
                    "authorId": "2876902",
                    "name": "Emilie Devijver"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "30784787",
                    "name": "Myriam Tami"
                },
                {
                    "authorId": "48819635",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "1481116184",
                    "name": "G. Oppenheim"
                }
            ]
        },
        {
            "paperId": "926ad1f0c1690acdf94c98bfc5ba73cb67452321",
            "title": "A Convex Approach to Superresolution and Regularization of Lines in Images",
            "abstract": "We present a new convex formulation for the problem of recovering lines in degraded images. Following the recent paradigm of super-resolution, we formulate a dedicated atomic norm penalty and we solve this optimization problem by means of a primal-dual algorithm. This parsimonious model enables the reconstruction of lines from lowpass measurements, even in presence of a large amount of noise or blur. Furthermore, a Prony method performed on rows and columns of the restored image, provides a spectral estimation of the line parameters, with subpixel accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2264476",
                    "name": "K. Polisano"
                },
                {
                    "authorId": "1693631",
                    "name": "Laurent Condat"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "2568298",
                    "name": "V. Perrier"
                }
            ]
        },
        {
            "paperId": "11defb548f8088fd796f277337d6244c9bd33c5f",
            "title": "Decision tree for uncertainty measures",
            "abstract": "The ensemble methods are popular machine learning techniques which are powerful when one wants to deal with both classification or prediction problems. A set of classifiers (regression or classification trees) is constructed, and the classification or the prediction of a new data instance is done by tacking a weighted vote. A tree is a piece-wise constant estimator on partitions obtained from the data. These partitions are induced by recursive dyadic split of the set of input variables. For example, CART (Classification And Regression Trees) [1] is an efficient algorithm for the construction of a tree. The goal is to partition the space of input variable values in the most as possible \"homogeneous\" K disjoint regions. More precisely, each partitioning value has to minimize a risk function. However, in practice, experimental measures can be observed with uncertainty. This work proposes to extend CART algorithm to this kind of data. We present an induced model adapted to uncertainty data and both a prediction and split rule for a tree construction taking into account the uncertainty of each quantitative observation from the data base.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30784787",
                    "name": "Myriam Tami"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "2876902",
                    "name": "Emilie Devijver"
                },
                {
                    "authorId": "48819635",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "119787045",
                    "name": "J. Aubert"
                },
                {
                    "authorId": "104066402",
                    "name": "Chebre"
                }
            ]
        },
        {
            "paperId": "2823fd6cc28ceabda2710b81237888e7507e6da9",
            "title": "Uncertain Trees: Dealing with Uncertain Inputs in Regression Trees.",
            "abstract": "Tree-based ensemble methods, as Random Forests and Gradient Boosted Trees, have been successfully used for regression in many applications and research studies. Furthermore, these methods have been extended in order to deal with uncertainty in the output variable, using for example a quantile loss in Random Forests (Meinshausen, 2006). To the best of our knowledge, no extension has been provided yet for dealing with uncertainties in the input variables, even though such uncertainties are common in practical situations. We propose here such an extension by showing how standard regression trees optimizing a quadratic loss can be adapted and learned while taking into account the uncertainties in the inputs. By doing so, one no longer assumes that an observation lies into a single region of the regression tree, but rather that it belongs to each region with a certain probability. Experiments conducted on several data sets illustrate the good behavior of the proposed extension.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "30784787",
                    "name": "Myriam Tami"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "2876902",
                    "name": "Emilie Devijver"
                },
                {
                    "authorId": "34610498",
                    "name": "A. Dulac"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "2159002",
                    "name": "S. Janaqi"
                },
                {
                    "authorId": "2776796",
                    "name": "M. Ch\u00e8bre"
                }
            ]
        },
        {
            "paperId": "1012cab0c954d5fc41cb033f7388a1bfee4c643b",
            "title": "Topical Coherence in LDA-based Models through Induced Segmentation",
            "abstract": "This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3440447",
                    "name": "Hesam Amoualian"
                },
                {
                    "authorId": "143844110",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "1951080",
                    "name": "Georgios Balikas"
                },
                {
                    "authorId": "144444438",
                    "name": "Massih-Reza Amini"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                }
            ]
        },
        {
            "paperId": "28431102c63b30ce655dadf60a243ed34c106c05",
            "title": "Hyperbolic Wavelet-Fisz Denoising for a Model Arising in Ultrasound Imaging",
            "abstract": "We present an algorithm and its fully data-driven extension for noise reduction in ultrasound imaging. The proposed method computes the hyperbolic wavelet transform of the image, before applying a multiscale variance stabilization technique, via a Fisz transformation. This adapts the wavelet coefficients statistics to the wavelet thresholding paradigm. The use of hyperbolic wavelets makes it possible to recover the image while respecting the anisotropic nature of structural details. The data-driven extension obviates the need for any prior knowledge of the noise model parameters by estimating the noise variance using an isotonic Nadaraya\u2013Watson estimator. Experiments on synthetic and real data demonstrate the potential of the proposed algorithm to recover ultrasound images while preserving tissue details. Furthermore, comparisons with other noise-reduction methods show that our technique is competitive with the state-of-the-art OBNLM filter. Finally, the variance estimation procedure is applied to real images emphasizing the noise model.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2958823",
                    "name": "Y. Farouj"
                },
                {
                    "authorId": "46223798",
                    "name": "Jean-Marc Freyermuth"
                },
                {
                    "authorId": "2054790001",
                    "name": "L. Navarro"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "1746531",
                    "name": "P. Delachartre"
                }
            ]
        },
        {
            "paperId": "7540be391389688d52bf4308a51623bbdee7a647",
            "title": "Eye-tracking data analysis using hidden semi-Markovian models to identify and characterize reading strategies",
            "abstract": "Textual information search is not a homogeneous process in time, neither from a cognitive perspective nor in terms of eye-movement patterns (Simola, 2008). The research objective is to analyze eye-tracking signals acquired through participants achieving a reading task and simultaneously aiming at making a binary decision: whether a text is related or not to some theme given a priori. This activity is expected to involve several phases with contrasted oculometric characteristics, such as normal reading, scanning, careful reading, associated with different cognitive strategies, such as creation and rejection of hypotheses, confirmation and decision. We propose an analytical data-driven method based on hidden semi-Markov models (Yu, 2010) which are composed of two stochastic processes. The former is observed, and corresponds to eye-movement features over time, while the latter is a latent semi-Markov chain, which preconditions the first process, and is used to uncover the information acquisition strategies. Four interpretable strategies were highlighted: normal reading, fast reading, careful reading, and decision making. This interpretation was derived using the model properties such as dwell times, inter-phase transition probabilities, and emission probabilities, which characterize the observed process. More importantly, model selection was performed using both, information theory criterion and some covariates, which were also used to reinforce the interpretation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2630088",
                    "name": "Brice Olivier"
                },
                {
                    "authorId": "40492844",
                    "name": "Jean-Baptiste Durand"
                },
                {
                    "authorId": "1398547891",
                    "name": "A. Gu\u00e9rin-Dugu\u00e9"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                }
            ]
        },
        {
            "paperId": "36968a2ccccd2518dd01276f2e0e5e9d4476e790",
            "title": "Convex super-resolution detection of lines in images",
            "abstract": "In this paper, we present a new convex formulation for the problem of recovering lines in degraded images. Following the recent paradigm of super-resolution, we formulate a dedicated atomic norm penalty and we solve this optimization problem by means of a primal-dual algorithm. This parsimonious model enables the reconstruction of lines from lowpass measurements, even in presence of a large amount of noise or blur. Furthermore, a Prony method performed on rows and columns of the restored image, provides a spectral estimation of the line parameters, with subpixel accuracy.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2264476",
                    "name": "K. Polisano"
                },
                {
                    "authorId": "1693631",
                    "name": "Laurent Condat"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "2568298",
                    "name": "V. Perrier"
                }
            ]
        },
        {
            "paperId": "400828710f967a264e9b196574f0f848b352e591",
            "title": "Streaming-LDA: A Copula-based Approach to Modeling Topic Dependencies in Document Streams",
            "abstract": "We propose in this paper two new models for modeling topic and word-topic dependencies between consecutive documents in document streams. The first model is a direct extension of Latent Dirichlet Allocation model (LDA) and makes use of a Dirichlet distribution to balance the influence of the LDA prior parameters wrt to topic and word-topic distribution of the previous document. The second extension makes use of copulas, which constitute a generic tools to model dependencies between random variables. We rely here on Archimedean copulas, and more precisely on Franck copulas, as they are symmetric and associative and are thus appropriate for exchangeable random variables. Our experiments, conducted on three standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones (as dynamic topic models and temporal \\LDA), both in terms of perplexity and for tracking similar topics in a document stream.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3440447",
                    "name": "Hesam Amoualian"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "144444438",
                    "name": "Massih-Reza Amini"
                }
            ]
        },
        {
            "paperId": "5262d8abc192446e9fe302e28425c02a447244b2",
            "title": "On a Topic Model for Sentences",
            "abstract": "Probabilistic topic models are generative models that describe the content of documents by discovering the latent topics underlying them. However, the structure of the textual input, and for instance the grouping of words in coherent text spans such as sentences, contains much information which is generally lost with these models. In this paper, we propose sentenceLDA, an extension of LDA whose goal is to overcome this limitation by incorporating the structure of the text in the generative and inference processes. We illustrate the advantages of sentenceLDA by comparing it with LDA using both intrinsic (perplexity) and extrinsic (text classification) evaluation tasks on different text collections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1951080",
                    "name": "Georgios Balikas"
                },
                {
                    "authorId": "144444438",
                    "name": "Massih-Reza Amini"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                }
            ]
        },
        {
            "paperId": "726b5705c2c826b07fbd246adf3c90ccbcafcff6",
            "title": "Modeling topic dependencies in semantically coherent text spans with copulas",
            "abstract": "The exchangeability assumption in topic models like Latent Dirichlet Allocation (LDA) often results in inferring inconsistent topics for the words of text spans like noun-phrases, which are usually expected to be topically coherent. We propose copulaLDA, that extends LDA by integrating part of the text structure to the model and relaxes the conditional independence assumption between the word-specific latent topics given the per-document topic distributions. To this end, we assume that the words of text spans like noun-phrases are topically bound and we model this dependence with copulas. We demonstrate empirically the effectiveness of copulaLDA on both intrinsic and extrinsic evaluation tasks on several publicly available corpora.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1951080",
                    "name": "Georgios Balikas"
                },
                {
                    "authorId": "3440447",
                    "name": "Hesam Amoualian"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "144444438",
                    "name": "Massih-Reza Amini"
                }
            ]
        },
        {
            "paperId": "c5c92d217b6f0bcdc537e891c8cd9f7d7ef0db5b",
            "title": "Health Monitoring on Social Media over Time",
            "abstract": "Social media has become a major source for analyzing all aspects of daily life. Thanks to dedicated latent topic analysis methods such as the Ailment Topic Aspect Model (ATAM), public health can now be observed on Twitter. In this work, we are interested in using social media to monitor people\u2019s health over time. The use of tweets has several benefits including instantaneous data availability at virtually no cost. Early monitoring of health data is complementary to post-factum studies and enables a range of applications such as measuring behavioral risk factors and triggering health campaigns. We formulate two problems: health transition detection and  health transition prediction. We first propose the Temporal Ailment Topic Aspect Model (TM\u2013ATAM), a new latent model dedicated to solving the first problem by capturing transitions that involve health-related topics. TM\u2013ATAM is a non-obvious extension to ATAM that was designed to extract health-related topics. It learns health-related topic transitions by minimizing the prediction error on topic distributions between consecutive posts at different time and geographic granularities. To solve the second problem, we develop T\u2013ATAM, a Temporal Ailment Topic Aspect Model where time is treated as a random variable natively inside ATAM. Our experiments on an 8-month corpus of tweets show that TM\u2013ATAM outperforms TM\u2013LDA in estimating health-related transitions from tweets for different geographic populations. We examine the ability of TM\u2013ATAM to detect transitions due to climate conditions in different geographic regions. We then show how T\u2013ATAM can be used to predict the most important transition and additionally compare T\u2013ATAM with CDC (Center for Disease Control) data and Google Flu Trends.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3430051",
                    "name": "Sumit Sidana"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "1412139901",
                    "name": "Majdeddine Rebai"
                },
                {
                    "authorId": "2925880",
                    "name": "S. T. Mai"
                },
                {
                    "authorId": "144444438",
                    "name": "Massih-Reza Amini"
                }
            ]
        },
        {
            "paperId": "9d4c5470bc7cad6d0c54cbb7985b9d2c865ce4bb",
            "title": "Texture modeling by Gaussian fields with prescribed local orientation",
            "abstract": "This paper presents a new framework for oriented texture modeling. We introduce a new class of Gaussian fields, called Locally Anisotropic Fractional Brownian Fields, with prescribed local orientation at any point. These fields are a local version of a specific class of anisotropic self-similar Gaussian fields with stationary increments. The simulation of such textures is obtained using a new algorithm mixing the tangent field formulation and a turning band method, this latter method having proved its efficiency for generating stationary anisotropic textures. Numerical experiments show the ability of the method for synthesis of textures with prescribed local orientation.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2264476",
                    "name": "K. Polisano"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "2568298",
                    "name": "V. Perrier"
                },
                {
                    "authorId": "1693631",
                    "name": "Laurent Condat"
                }
            ]
        },
        {
            "paperId": "ef122c26386536810aae71acc045a690b07f4e90",
            "title": "Cardiac motion analysis using wavelet projections from tagged MR sequences",
            "abstract": "We present an optical flow technique in a differential projected framework adapted to local myocardial motion estimation from MR Tagged images. The algorithm is based on the Dual Tree design of Hilbert transform pairs of wavelet bases. Such a design allows one to construct several orientation-sensitive wavelet filters for better analysis of the complex motion of the heart. The complex wavelet transform (CWT) integrates both energy and phase information in the wavelets coefficients for an effective motion estimation. The CWT also provides a high frequency analysis and enjoys a multiresolution aspect that allows multiscale flow estimate. Performances of the algorithm are evaluated on synthetic tagged MRI sequences for both displacement and strain estimation.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2958823",
                    "name": "Y. Farouj"
                },
                {
                    "authorId": "2144692058",
                    "name": "Liang Wang"
                },
                {
                    "authorId": "2068360",
                    "name": "P. Clarysse"
                },
                {
                    "authorId": "2054790001",
                    "name": "L. Navarro"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "1746531",
                    "name": "P. Delachartre"
                }
            ]
        },
        {
            "paperId": "7b87477b9e7e7dd135b5f64a9fa2f0d1d046efe8",
            "title": "Self-Similar Anisotropic Texture Analysis: The Hyperbolic Wavelet Transform Contribution",
            "abstract": "Textures in images can often be well modeled using self-similar processes while they may simultaneously display anisotropy. The present contribution thus aims at studying jointly selfsimilarity and anisotropy by focusing on a specific classical class of Gaussian anisotropic selfsimilar processes. It will be first shown that accurate joint estimates of the anisotropy and selfsimilarity parameters are performed by replacing the standard 2D-discrete wavelet transform with the hyperbolic wavelet transform, which permits the use of different dilation factors along the horizontal and vertical axes. Defining anisotropy requires a reference direction that needs not a priori match the horizontal and vertical axes according to which the images are digitized; this discrepancy defines a rotation angle. Second, we show that this rotation angle can be jointly estimated. Third, a nonparametric bootstrap based procedure is described, which provides confidence intervals in addition to the estimates themselves and enables us to construct an isotropy test procedure, which can be applied to a single texture image. Fourth, the robustness and versatility of the proposed analysis are illustrated by being applied to a large variety of different isotropic and anisotropic self-similar fields. As an illustration, we show that a true anisotropy built-in self-similarity can be disentangled from an isotropic self-similarity to which an anisotropic trend has been superimposed.",
            "fieldsOfStudy": [
                "Medicine",
                "Mathematics",
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "32045453",
                    "name": "S. Roux"
                },
                {
                    "authorId": "2889171",
                    "name": "M. Clausel"
                },
                {
                    "authorId": "39490894",
                    "name": "B. Vedel"
                },
                {
                    "authorId": "2161649",
                    "name": "S. Jaffard"
                },
                {
                    "authorId": "1731012",
                    "name": "P. Abry"
                }
            ]
        }
    ]
}