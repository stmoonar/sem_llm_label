{
    "authorId": "2151245501",
    "papers": [
        {
            "paperId": "505013ca939bf7e6138b38c58a24d80726c669ee",
            "title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models",
            "abstract": "Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large language models (LLMs), even though they have demonstrated promising performance in other reasoning tasks. Within this context, some recent studies use programming languages (e.g., Python) to express the necessary logic for solving a given instance/question (e.g., Program-of-Thought) as inspired by their strict and precise syntaxes. However, it is non-trivial to write an executable code that expresses the correct logic on the fly within a single inference call. Also, the code generated specifically for an instance cannot be reused for others, even if they are from the same task and might require identical logic to solve. This paper presents Think-and-Execute, a novel framework that decomposes the reasoning process of language models into two steps. (1) In Think, we discover a task-level logic that is shared across all instances for solving a given task and then express the logic with pseudocode; (2) In Execute, we further tailor the generated pseudocode to each instance and simulate the execution of the code. With extensive experiments on seven algorithmic reasoning tasks, we demonstrate the effectiveness of Think-and-Execute. Our approach better improves LMs' reasoning compared to several strong baselines performing instance-specific reasoning (e.g., CoT and PoT), suggesting the helpfulness of discovering task-level logic. Also, we show that compared to natural language, pseudocode can better guide the reasoning of LMs, even though they are trained to follow natural language instructions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2184029886",
                    "name": "Hyungjoo Chae"
                },
                {
                    "authorId": "2294805313",
                    "name": "Yeonghyeon Kim"
                },
                {
                    "authorId": "2184037220",
                    "name": "Seungone Kim"
                },
                {
                    "authorId": "2210267033",
                    "name": "Kai Tzu-iunn Ong"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2294776125",
                    "name": "Moohyeon Kim"
                },
                {
                    "authorId": "2295169937",
                    "name": "Seonghwan Kim"
                },
                {
                    "authorId": "2258722263",
                    "name": "Taeyoon Kwon"
                },
                {
                    "authorId": "2004821977",
                    "name": "Jiwan Chung"
                },
                {
                    "authorId": "2258802490",
                    "name": "Youngjae Yu"
                },
                {
                    "authorId": "2258712913",
                    "name": "Jinyoung Yeo"
                }
            ]
        },
        {
            "paperId": "6b9699058152912efedd9a6d724ec08e0c4c9319",
            "title": "Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset",
            "abstract": "Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relevant to the dialogue context than those in prior datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117956135",
                    "name": "Minjin Kim"
                },
                {
                    "authorId": "2110104000",
                    "name": "Minju Kim"
                },
                {
                    "authorId": "2281098420",
                    "name": "Hana Kim"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2290185769",
                    "name": "Soyeon Chun"
                },
                {
                    "authorId": "2290213007",
                    "name": "Hyunseo Kim"
                },
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "2258802492",
                    "name": "Youngjae Yu"
                },
                {
                    "authorId": "2258712913",
                    "name": "Jinyoung Yeo"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                }
            ]
        },
        {
            "paperId": "6d73697837801b82744667f1659d01f027961b31",
            "title": "Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code",
            "abstract": "This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans' code edit traces for coding questions and machine-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs' code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2184029886",
                    "name": "Hyungjoo Chae"
                },
                {
                    "authorId": "2258722263",
                    "name": "Taeyoon Kwon"
                },
                {
                    "authorId": "2266717741",
                    "name": "Seungjun Moon"
                },
                {
                    "authorId": "2188768271",
                    "name": "Yongho Song"
                },
                {
                    "authorId": "2266420525",
                    "name": "Dongjin Kang"
                },
                {
                    "authorId": "2210267033",
                    "name": "Kai Tzu-iunn Ong"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2307564257",
                    "name": "Seonghyeon Bae"
                },
                {
                    "authorId": "2266441171",
                    "name": "Seung-won Hwang"
                },
                {
                    "authorId": "1898428",
                    "name": "Jinyoung Yeo"
                }
            ]
        },
        {
            "paperId": "c86eba7860f37c47b97fda9d607b5386b7a1cb15",
            "title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics",
            "abstract": "The idea of personality in descriptive psychology, traditionally defined through observable behavior, has now been extended to Large Language Models (LLMs) to better understand their behavior. This raises a question: do LLMs exhibit distinct and consistent personality traits, similar to humans? Existing self-assessment personality tests, while applicable, lack the necessary validity and reliability for precise personality measurements. To address this, we introduce TRAIT, a new tool consisting of 8K multi-choice questions designed to assess the personality of LLMs with validity and reliability. TRAIT is built on the psychometrically validated human questionnaire, Big Five Inventory (BFI) and Short Dark Triad (SD-3), enhanced with the ATOMIC10X knowledge graph for testing personality in a variety of real scenarios. TRAIT overcomes the reliability and validity issues when measuring personality of LLM with self-assessment, showing the highest scores across three metrics: refusal rate, prompt sensitivity, and option order sensitivity. It reveals notable insights into personality of LLM: 1) LLMs exhibit distinct and consistent personality, which is highly influenced by their training data (i.e., data used for alignment tuning), and 2) current prompting techniques have limited effectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in this direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307987815",
                    "name": "Seungbeen Lee"
                },
                {
                    "authorId": "2220330035",
                    "name": "Seungwon Lim"
                },
                {
                    "authorId": "2423429",
                    "name": "Seungju Han"
                },
                {
                    "authorId": "2307917646",
                    "name": "Giyeong Oh"
                },
                {
                    "authorId": "2184029886",
                    "name": "Hyungjoo Chae"
                },
                {
                    "authorId": "2004821977",
                    "name": "Jiwan Chung"
                },
                {
                    "authorId": "2110104000",
                    "name": "Minju Kim"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2307982267",
                    "name": "Yeonsoo Lee"
                },
                {
                    "authorId": "2258907627",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "2258712913",
                    "name": "Jinyoung Yeo"
                },
                {
                    "authorId": "2258802490",
                    "name": "Youngjae Yu"
                }
            ]
        },
        {
            "paperId": "1afeaef110134ebbba11604a206a54be547fdee0",
            "title": "Dual Task Framework for Improving Persona-grounded Dialogue Dataset",
            "abstract": "This paper introduces a simple yet effective data-centric approach for the task of improving persona-conditioned dialogue agents. Prior model-centric approaches unquestioningly depend on the raw crowdsourced benchmark datasets such as Persona-Chat. In contrast, we aim to fix annotation artifacts in benchmarking, which is orthogonally applicable to any dialogue model. Specifically, we augment relevant personas to improve dialogue dataset/agent, by leveraging the primal-dual structure of the two tasks, predicting dialogue responses and personas based on each other. Experiments on Persona-Chat show that our approach outperforms pre-trained LMs by an 11.7 point gain in terms of accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110104000",
                    "name": "Minju Kim"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2116682476",
                    "name": "Youngwook Kim"
                },
                {
                    "authorId": "2155252638",
                    "name": "Hong-in Lee"
                },
                {
                    "authorId": "2153642272",
                    "name": "Seung-won Hwang"
                },
                {
                    "authorId": "1898428",
                    "name": "Jinyoung Yeo"
                }
            ]
        },
        {
            "paperId": "6f9aa703c1dea0bd316ff7c758381199b321a3ba",
            "title": "Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning",
            "abstract": "Commonsense reasoning systems should be able to generalize to diverse reasoning cases. However, most state-of-the-art approaches depend on expensive data annotations and overfit to a specific benchmark without learning how to perform general semantic reasoning. To overcome these drawbacks, zero-shot QA systems have shown promise as a robust learning scheme by transforming a commonsense knowledge graph (KG) into synthetic QA-form samples for model training. Considering the increasing type of different commonsense KGs, this paper aims to extend the zero-shot transfer learning scenario into multiple-source settings, where different KGs can be utilized synergetically. Towards this goal, we propose to mitigate the loss of knowledge from the interference among the different knowledge sources, by developing a modular variant of the knowledge aggregation as a new zero-shot commonsense reasoning framework. Results on five commonsense reasoning benchmarks demonstrate the efficacy of our framework, improving the performance with multiple KGs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2129538296",
                    "name": "Yu Jin Kim"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2116682476",
                    "name": "Youngwook Kim"
                },
                {
                    "authorId": "23181472",
                    "name": "Reinald Kim Amplayo"
                },
                {
                    "authorId": "2153642272",
                    "name": "Seung-won Hwang"
                },
                {
                    "authorId": "1898428",
                    "name": "Jinyoung Yeo"
                }
            ]
        },
        {
            "paperId": "cfa1d074fc74d673b6511289fbac2fdebcc09e17",
            "title": "TrustAL: Trustworthy Active Learning using Knowledge Distillation",
            "abstract": "Active learning can be defined as iterations of data labeling, model training, and data acquisition, until sufficient labels are acquired. A traditional view of data acquisition is that, through iterations, knowledge from human labels and models is implicitly distilled to monotonically increase the accuracy and label consistency. Under this assumption, the most recently trained model is a good surrogate for the current labeled data, from which data acquisition is requested based on uncertainty/diversity. Our contribution is debunking this myth and proposing a new objective for distillation. First, we found example forgetting, which indicates the loss of knowledge learned across iterations. Second, for this reason, the last model is no longer the best teacher-- For mitigating such forgotten knowledge, we select one of its predecessor models as a teacher, by our proposed notion of \"consistency\". We show that this novel distillation is distinctive in the following three aspects; First, consistency ensures to avoid forgetting labels. Second, consistency improves both uncertainty/diversity of labeled data. Lastly, consistency redeems defective labels produced by human annotators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2116682476",
                    "name": "Youngwook Kim"
                },
                {
                    "authorId": "2129538296",
                    "name": "Yu Jin Kim"
                },
                {
                    "authorId": "1716415",
                    "name": "Seung-won Hwang"
                },
                {
                    "authorId": "1898428",
                    "name": "Jinyoung Yeo"
                }
            ]
        }
    ]
}