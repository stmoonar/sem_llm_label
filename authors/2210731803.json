{
    "authorId": "2210731803",
    "papers": [
        {
            "paperId": "0152e59caf854cd75cef30f99307d41634916653",
            "title": "Imp: Highly Capable Large Multimodal Models for Mobile Devices",
            "abstract": "By harnessing the capabilities of large language models (LLMs), recent large multimodal models (LMMs) have shown remarkable versatility in open-world multimodal understanding. Nevertheless, they are usually parameter-heavy and computation-intensive, thus hindering their applicability in resource-constrained scenarios. To this end, several lightweight LMMs have been proposed successively to maximize the capabilities under constrained scale (e.g., 3B). Despite the encouraging results achieved by these methods, most of them only focus on one or two aspects of the design space, and the key design choices that influence model capability have not yet been thoroughly investigated. In this paper, we conduct a systematic study for lightweight LMMs from the aspects of model architecture, training strategy, and training data. Based on our findings, we obtain Imp -- a family of highly capable LMMs at the 2B-4B scales. Notably, our Imp-3B model steadily outperforms all the existing lightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs at the 13B scale. With low-bit quantization and resolution reduction techniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile chip with a high inference speed of about 13 tokens/s.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2210731803",
                    "name": "Zhenwei Shao"
                },
                {
                    "authorId": "2302366930",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2302364245",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2302320439",
                    "name": "Xuecheng Ouyang"
                },
                {
                    "authorId": "2302448325",
                    "name": "Lihao Zheng"
                },
                {
                    "authorId": "2214507738",
                    "name": "Zhenbiao Gai"
                },
                {
                    "authorId": "2302326458",
                    "name": "Mingyang Wang"
                },
                {
                    "authorId": "2293573970",
                    "name": "Jiajun Ding"
                }
            ]
        },
        {
            "paperId": "9e9f675830caceddca3619c2c4d4441e519d894f",
            "title": "Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering",
            "abstract": "Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the blind LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. Finally, the two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. To demonstrate the generality of our approach, we instantiate Prophet with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2210731803",
                    "name": "Zhenwei Shao"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "50469060",
                    "name": "Mei Wang"
                },
                {
                    "authorId": "2161356649",
                    "name": "Jun Yu"
                }
            ]
        }
    ]
}