{
    "authorId": "143728483",
    "papers": [
        {
            "paperId": "0bce2795374e0070138e89c84917bcba9ce9dee3",
            "title": "Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models",
            "abstract": "Data is a crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose Data Advisor, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, Data Advisor monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Data Advisor can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in enhancing model safety against various fine-grained safety issues without sacrificing model utility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2324857481",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": null,
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": null,
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                }
            ]
        },
        {
            "paperId": "2de305c8940a5ac2fbd925d5a16aa95676cbba8f",
            "title": "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs",
            "abstract": "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. We evaluate on two popular benchmark datasets. Our results show that Tree-of-Traversals significantly improves performance on question answering and KG question answering tasks. Code is available at \\url{https://github.com/amazon-science/tree-of-traversals}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2083006949",
                    "name": "Elan Markowitz"
                },
                {
                    "authorId": "2266838160",
                    "name": "Anil Ramakrishna"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "102648923",
                    "name": "Charith Peris"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": "2256646555",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                }
            ]
        },
        {
            "paperId": "30873bb90a189c0f35365449a32ec5e101509696",
            "title": "Policy Learning for Localized Interventions from Observational Data",
            "abstract": "A largely unaddressed problem in causal inference is that of learning reliable policies in continuous, high-dimensional treatment variables from observational data. Especially in the presence of strong confounding, it can be infeasible to learn the entire heterogeneous response surface from treatment to outcome. It is also not particularly useful, when there are practical constraints on the size of the interventions altering the observational treatments. Since it tends to be easier to learn the outcome for treatments near existing observations, we propose a new framework for evaluating and optimizing the effect of small, tailored, and localized interventions that nudge the observed treatment assignments. Our doubly robust effect estimator plugs into a policy learner that stays within the interventional scope by optimal transport. Consequently, the error of the total policy effect is restricted to prediction errors nearby the observational distribution, rather than the whole response surface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10195617",
                    "name": "Myrl G. Marmarelis"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "1719898",
                    "name": "G. V. Steeg"
                }
            ]
        },
        {
            "paperId": "41c24ecf8b61ab80293126ba40ea41dbdfb9ffb5",
            "title": "Agenda-Driven Question Generation: A Case Study in the Courtroom Domain",
            "abstract": "This paper introduces a novel problem of automated question generation for courtroom examinations, CourtQG. While question generation has been studied in domains such as educational testing and product description, CourtQG poses several unique challenges owing to its non-cooperative and agenda-driven nature. Specifically, not only the generated questions need to be relevant to the case and underlying context, they also have to achieve certain objectives such as challenging the opponent\u2019s arguments and/or revealing potential inconsistencies in their answers. We propose to leverage large language models (LLM) for CourtQG by fine-tuning them on two auxiliary tasks, agenda explanation (i.e., uncovering the underlying intents) and question type prediction. We additionally propose cold-start generation of questions from background documents without relying on examination history. We construct a dataset to evaluate our proposed method and show that it generates better questions according to standard metrics when compared to several baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51135899",
                    "name": "Y. Fung"
                },
                {
                    "authorId": "2169129833",
                    "name": "Anoop Kumar"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2301791974",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2301581154",
                    "name": "Prem Natarajan"
                }
            ]
        },
        {
            "paperId": "79cdad81f1aad7b8a036490ae6502bfc6233bdcc",
            "title": "Leveraging LLMs for Dialogue Quality Measurement",
            "abstract": "In task-oriented conversational AI evaluation, unsupervised methods poorly correlate with human judgments, and supervised approaches lack generalization. Recent advances in large language models (LLMs) show robust zeroshot and few-shot capabilities across NLP tasks. This paper explores using LLMs for automated dialogue quality evaluation, experimenting with various configurations on public and proprietary datasets. Manipulating factors such as model size, in-context examples, and selection techniques, we examine\"chain-of-thought\"(CoT) reasoning and label extraction procedures. Our results show that (1) larger models yield more accurate dialogue labels; (2) algorithmic selection of in-context examples outperforms random selection; (3) CoT reasoning where an LLM is asked to provide justifications before outputting final labels improves performance; and (4) fine-tuned LLMs outperform out-of-the-box ones. Our results indicate that LLMs that are suitably fine-tuned and have sufficient reasoning capabilities can be leveraged for automated dialogue evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308420406",
                    "name": "Jinghan Jia"
                },
                {
                    "authorId": "97340738",
                    "name": "A. Komma"
                },
                {
                    "authorId": "5026528",
                    "name": "Timothy Leffel"
                },
                {
                    "authorId": "2309117796",
                    "name": "Xujun Peng"
                },
                {
                    "authorId": "2308098438",
                    "name": "Ajay Nagesh"
                },
                {
                    "authorId": "2291360900",
                    "name": "Tamer Soliman"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2169129833",
                    "name": "Anoop Kumar"
                }
            ]
        },
        {
            "paperId": "8f22fb1ded314b43922d5cec71deab5fc8b63e06",
            "title": "MICo: Preventative Detoxification of Large Language Models through Inhibition Control",
            "abstract": "Large Language Models (LLMs) are powerful tools which have been both dominant and commonplace in the field of Artificial Intelligence. Yet, LLMs have a tendency to devolve into toxic degeneration, wherein otherwise safe and unproblematic models begin generating toxic content. For the sake of social responsibility and inspired by the biological mechanisms of inhibition control, we introduce the paradigm of Education for Societal Norms (ESN). By collecting and labeling examples as acceptable and unacceptable (in this case toxic and non-toxic), and including a corresponding acceptable rewrite with every unacceptable example, we introduce a new mechanism for LLM detoxification. We annotate a dataset of 2,850 entries and use it to fine-tune a model, which we call a Model with Inhibition Control (MICo). Evaluating this model on toxicity detection capability, rewrite detoxification, meaning preservation, and overall toxicity reduction, we discover significant improvements over the baseline model. In our experiments we show that overall toxicity of this model is more than 60% reduced, with over 75% reduction in severe toxicity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2313541602",
                    "name": "Roy Siegelmann"
                },
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "38774604",
                    "name": "Prasoon Goyal"
                },
                {
                    "authorId": "2308474837",
                    "name": "Lisa Bauer"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": "2313538923",
                    "name": "Reza Ghanadan"
                }
            ]
        },
        {
            "paperId": "ba15e41eac064729c634464851ae0a268de777d4",
            "title": "Prompt Perturbation Consistency Learning for Robust Language Models",
            "abstract": "Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbation Consistency Learning (PPCL), which works by regularizing the divergence between losses from clean and perturbed samples. Our experiments show that PPCL can recover on an average 59% and 69% of the performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats data augmentation approach while using ten times fewer augmented data samples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2286869751",
                    "name": "Yao Qiang"
                },
                {
                    "authorId": "23553183",
                    "name": "Subhrangshu Nandi"
                },
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "1719898",
                    "name": "G. V. Steeg"
                },
                {
                    "authorId": "2169129833",
                    "name": "Anoop Kumar"
                },
                {
                    "authorId": "1681193",
                    "name": "Anna Rumshisky"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                }
            ]
        },
        {
            "paperId": "e710115f7251bc552e23311c69086253ddab2429",
            "title": "Contrastive Instruction Tuning",
            "abstract": "Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructions with variations across character, word, sentence, and semantic levels by an average of +2.5% in accuracy. Code is available at https://github.com/luka-group/CoIN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284682152",
                    "name": "Tianyi Yan"
                },
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2110302673",
                    "name": "James Y. Huang"
                },
                {
                    "authorId": "2203076",
                    "name": "Wenxuan Zhou"
                },
                {
                    "authorId": "2284687134",
                    "name": "Fan Yin"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2284681554",
                    "name": "Wenpeng Yin"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "f9a370e21e588509e7c3a80ccd1bcab9e14f184b",
            "title": "Correcting Language Model Outputs by Editing Salient Layers",
            "abstract": "Large language models can accumulate incorrect or outdated knowledge as the real world evolves. Compared to typical solutions such as retraining, retrieval augmented generation, model editing offers an effective yet low cost solution to address this issue. However, existing model editing algorithms employ manual selection of edit layers, which requires prior domain knowledge or expensive architecture-specific empirical layer selection methods, such as causal tracing. In this work, we propose SaLEM (Salient Layers Editing Model), an efficient solution for data driven layer selection for the model editing task. Our solution utilizes layer-wise saliency maps for layer selection, and matches the accuracy of prior approaches but with only 1/3 of their edits, enabling efficient updates to the parametric knowledge in large language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2291361786",
                    "name": "Kshitij Mishra"
                },
                {
                    "authorId": "2291360900",
                    "name": "Tamer Soliman"
                },
                {
                    "authorId": "2266838160",
                    "name": "Anil Ramakrishna"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2169129833",
                    "name": "Anoop Kumar"
                }
            ]
        },
        {
            "paperId": "113c0d97b454f3ec0aa31675bd3ed1582fcd4b64",
            "title": "The First Workshop on Personalized Generative AI @ CIKM 2023: Personalization Meets Large Language Models",
            "abstract": "The First Workshop on Personalized Generative AI1 aims to be a cornerstone event fostering innovation and collaboration in the dynamic field of personalized AI. Leveraging the potent capabilities of Large Language Models (LLMs) to enhance user experiences with tailored responses and recommendations, the workshop is designed to address a range of pressing challenges including knowledge gap bridging, hallucination mitigation, and efficiency optimization in handling extensive user profiles. As a nexus for academics and industry professionals, the event promises rich discussions on a plethora of topics such as the development and fine-tuning of foundational models, strategies for multi-modal personalization, and the imperative ethical and privacy considerations in LLM deployment. Through a curated series of keynote speeches, insightful panel discussions, and hands-on sessions, the workshop aspires to be a catalyst in the development of more precise, contextually relevant, and user-centric AI systems. It aims to foster a landscape where generative AI systems are not only responsive but also anticipatory of individual user needs, marking a significant stride in personalized experiences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2294925859",
                    "name": "Zheng Chen"
                },
                {
                    "authorId": "2112347577",
                    "name": "Ziyan Jiang"
                },
                {
                    "authorId": "2260962165",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "51002202",
                    "name": "Zhankui He"
                },
                {
                    "authorId": "151472453",
                    "name": "Yupeng Hou"
                },
                {
                    "authorId": "2260652958",
                    "name": "Eunah Cho"
                },
                {
                    "authorId": "2258552056",
                    "name": "Julian McAuley"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2256224948",
                    "name": "Xiaohua Hu"
                },
                {
                    "authorId": "2260830306",
                    "name": "Jie Yang"
                }
            ]
        }
    ]
}