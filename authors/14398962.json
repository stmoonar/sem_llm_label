{
    "authorId": "14398962",
    "papers": [
        {
            "paperId": "1bb1606d09db36b2129355b44ca3b5fc0febd105",
            "title": "Diversity, Equity and Inclusion Activities in Database Conferences: A 2023 Report",
            "abstract": "The Diversity, Equity and Inclusion (DEI) initiative started as the Diversity/Inclusion initiative in 2020 [4]. The current report summarizes our activities in 2023.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "2302858948",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "1404555727",
                    "name": "Renata Borovica-Gajic"
                },
                {
                    "authorId": "2314293279",
                    "name": "Jes\u00fas Camacho-Rodr\u00edguez"
                },
                {
                    "authorId": "2314330906",
                    "name": "Jinli Cao"
                },
                {
                    "authorId": "2314297028",
                    "name": "Barbara Catania"
                },
                {
                    "authorId": "2249901748",
                    "name": "P. Chrysanthis"
                },
                {
                    "authorId": "2278429940",
                    "name": "Carlo Curino"
                },
                {
                    "authorId": "117266605",
                    "name": "A. El Abbadi"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2178387374",
                    "name": "Juliana Freire"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "51205357",
                    "name": "Sujaya Maiyya"
                },
                {
                    "authorId": "2266690266",
                    "name": "Alexandra Meliou"
                },
                {
                    "authorId": "37168010",
                    "name": "Madhulika Mohanty"
                },
                {
                    "authorId": "2257398736",
                    "name": "Fatma \u00d6zcan"
                },
                {
                    "authorId": "3139922",
                    "name": "L. Peterfreund"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "2314297947",
                    "name": "Sana Sellami"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2314670128",
                    "name": "Wang-Chiew Tan"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "2303255329",
                    "name": "Yuanyuan Tian"
                },
                {
                    "authorId": "1393643717",
                    "name": "Genoveva Vargas-Solar"
                },
                {
                    "authorId": "2314731432",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "2302886251",
                    "name": "Wenjie Zhang"
                }
            ]
        },
        {
            "paperId": "1f4378f95f89ca545f6f7bdc1fdbb869e9d1d904",
            "title": "Gen-T: Table Reclamation in Data Lakes",
            "abstract": "We introduce the problem of Table Reclamation. Given a Source Table and a large table repository, reclamation finds a set of tables that, when integrated, reproduce the source table as closely as possible. Unlike query discovery problems like Query-by-Example or by-Target, Table Reclamation focuses on reclaiming the data in the Source Table as fully as possible using real tables that may be incomplete or inconsistent. To do this, we define a new measure of table similarity, called error-aware instance similarity, to measure how close a reclaimed table is to a Source Table, a measure grounded in instance similarity used in data exchange. Our search covers not only Select-project-join queries, but integration queries with unions, outerjoins, and the unary operators subsumption and complementation that have been shown to be important in data integration and fusion. Using reclamation, a data scientist can understand if any tables in a repository can be used to exactly reclaim a tuple in the Source. If not, one can understand if this is due to differences in values or to incompleteness in the data. Our solution, Gen-T, performs table discovery to retrieve a set of candidate tables from the table repository, filters these down to a set of originating tables, then integrates these tables to reclaim the Source as closely as possible. We show that our solution, while approximate, is accurate, efficient and scalable in the size of the table repository with experiments on real data lakes containing up to 15K tables, where the average number of tuples varies from small (web tables) to extremely large (open data tables) up to 1M tuples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292405785",
                    "name": "Grace Fan"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2292468813",
                    "name": "Ren'ee J. Miller"
                }
            ]
        },
        {
            "paperId": "47b91f2632cb182001afd84af0f50b68bf7738fd",
            "title": "Eighth Workshop on Human-In-the-Loop Data Analytics (HILDA)",
            "abstract": "HILDA brings together researchers and practitioners to exchange ideas and results on human-data interaction. It explores how data management and analysis can be made more effective when taking into account the people who design and build these processes as well as those who are impacted by their results. Following the past two years, we plan to continue to focus on this year's workshop on early-stage research that is promising and exciting, which includes pairing each accepted paper with a mentor. The theme for this edition of the workshop is HILDA and Large Language Models. However, the workshop is not limited to this theme and other topics are also of interest. In this summary, we describe the workshop, its main focus areas, our review and mentorship plan as well as the keynote and panel sessions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2249909304",
                    "name": "Jean-Daniel Fekete"
                },
                {
                    "authorId": "2302858721",
                    "name": "Kexin Rong"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                }
            ]
        },
        {
            "paperId": "3f4a438bb0eecf03ba46b9ff5aeb5e136e9cd515",
            "title": "LOUC: Leave-One-Out-Calibration Measure for Analyzing Human Matcher Performance",
            "abstract": "Schema matching is a core data integration task, focusing on identifying correspondences among attributes of multiple schemata. Numerous algorithmic approaches were suggested for schema matching over the years, aiming at solving the task with as little human involvement as possible. Yet, humans are still required in the loop -- to validate algorithms and to produce ground truth data for algorithms to be trained against. In recent years, a new research direction investigates the capabilities and behavior of humans while performing matching tasks. Previous works utilized this knowledge to predict, and even improve, the performance of human matchers. In this work, we continue this line of research by suggesting a novel measure to evaluate the performance of human matchers, based on calibration, a common meta-cognition measure. The proposed measure enables detailed analysis of various factors of the behavior of human matchers and their relation to human performance. Such analysis can be further utilized to develop heuristics and methods to better asses and improve the annotation quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2226774750",
                    "name": "Matan Solomon"
                },
                {
                    "authorId": "2185349449",
                    "name": "Bar Genossar"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "70189402",
                    "name": "A. Gal"
                }
            ]
        },
        {
            "paperId": "82960f8f911cf144eebdbc2f8795b8f7ab564364",
            "title": "Explaining Dataset Changes for Semantic Data Versioning with Explain-Da-V (Technical Report)",
            "abstract": "\n In multi-user environments in which data science and analysis is collaborative, multiple versions of the same datasets are generated. While managing and storing data versions has received some attention in the research literature, the semantic nature of such changes has remained under-explored. In this work, we introduce Explain-Da-V, a framework aiming to explain changes between two given dataset versions. Explain-Da-V generates\n explanations\n that use\n data transformations\n to explain changes. We further introduce a set of measures that evaluate the validity, generalizability, and explainability of these explanations. We empirically show, using an adapted existing benchmark and a newly created benchmark, that Explain-Da-V generates better explanations than existing data transformation synthesis methods.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2110417078",
                    "name": "Ren\u00e9e J. Miller"
                }
            ]
        },
        {
            "paperId": "8aa39cec5f3cba107fa52d33c7c98cf768501cb1",
            "title": "ExtractGPT: Exploring the Potential of Large Language Models for Product Attribute Value Extraction",
            "abstract": "E-commerce platforms require structured product data in the form of attribute-value pairs to offer features such as faceted product search or attribute-based product comparison. However, vendors often provide unstructured product descriptions, necessitating the extraction of attribute-value pairs from these texts. BERT-based extraction methods require large amounts of task-specific training data and struggle with unseen attribute values. This paper explores using large language models (LLMs) as a more training-data efficient and robust alternative. We propose prompt templates for zero-shot and few-shot scenarios, comparing textual and JSON-based target schema representations. Our experiments show that GPT-4 achieves the highest average F1-score of 85% using detailed attribute descriptions and demonstrations. Llama-3-70B performs nearly as well, offering a competitive open-source alternative. GPT-4 surpasses the best PLM baseline by 5% in F1-score. Fine-tuning GPT-3.5 increases the performance to the level of GPT-4 but reduces the model's ability to generalize to unseen attribute values.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055814315",
                    "name": "Alexander Brinkmann"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2260336258",
                    "name": "Christian Bizer"
                }
            ]
        },
        {
            "paperId": "9971a0dcdc6e3b077726099b377870c3df5b13b5",
            "title": "The Battleship Approach to the Low Resource Entity Matching Problem",
            "abstract": "Entity matching, a core data integration problem, is the task of deciding whether two data tuples refer to the same real-world entity. Recent advances in deep learning methods, using pre-trained language models, were proposed for resolving entity matching. Although demonstrating unprecedented results, these solutions suffer from a major drawback as they require large amounts of labeled data for training, and, as such, are inadequate to be applied to low resource entity matching problems. To overcome the challenge of obtaining sufficient labeled data we offer a new active learning approach, focusing on a selection mechanism that exploits unique properties of entity matching. We argue that a distributed representation of a tuple pair indicates its informativeness when considered among other pairs. This is used consequently in our approach that iteratively utilizes space-aware considerations. Bringing it all together, we treat the low resource entity matching problem as a Battleship game, hunting indicative samples, focusing on positive ones, through awareness of the latent space along with careful planning of next sampling iterations. An extensive experimental analysis shows that the proposed algorithm outperforms state-of-the-art active learning solutions to low resource entity matching, and although using less samples, can be as successful as state-of-the-art fully trained known algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185349449",
                    "name": "Bar Genossar"
                },
                {
                    "authorId": "2256895579",
                    "name": "Avigdor Gal"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                }
            ]
        },
        {
            "paperId": "ae70b935b93b94c6b03a337cd5f6f9c36972f066",
            "title": "DIALITE: Discover, Align and Integrate Open Data Tables",
            "abstract": "We demonstrate a novel table discovery pipeline called DIALITE that allows users to discover, integrate and analyze open data tables. DIALITE has three main stages. First, it allows users to discover tables from open data platforms using state-of-the-art table discovery techniques. Second, DIALITE integrates the discovered tables to produce an integrated table. Finally, it allows users to analyze the integration result by applying different downstreaming tasks over it. Our pipeline is flexible such that the user can easily add and compare additional discovery and integration algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738848458",
                    "name": "Aamod Khatiwada"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2110417078",
                    "name": "Ren\u00e9e J. Miller"
                }
            ]
        },
        {
            "paperId": "e3899eaf965565f947f9fef8db696bcda2732ee2",
            "title": "Generative Benchmark Creation for Table Union Search",
            "abstract": "Data management has traditionally relied on synthetic data generators to generate structured benchmarks, like the TPC suite, where we can control important parameters like data size and its distribution precisely. These benchmarks were central to the success and adoption of database management systems. But more and more, data management problems are of a semantic nature. An important example is finding tables that can be unioned. While any two tables with the same cardinality can be unioned, table union search is the problem of finding tables whose union is semantically coherent. Semantic problems cannot be benchmarked using synthetic data. Our current methods for creating benchmarks involve the manual curation and labeling of real data. These methods are not robust or scalable and perhaps more importantly, it is not clear how robust the created benchmarks are. We propose to use generative AI models to create structured data benchmarks for table union search. We present a novel method for using generative models to create tables with specified properties. Using this method, we create a new benchmark containing pairs of tables that are both unionable and non-unionable but related. We thoroughly evaluate recent existing table union search methods over existing benchmarks and our new benchmark. We also present and evaluate a new table search methods based on recent large language models over all benchmarks. We show that the new benchmark is more challenging for all methods than hand-curated benchmarks, specifically, the top-performing method achieves a Mean Average Precision of around 60%, over 30% less than its performance on existing manually created benchmarks. We examine why this is the case and show that the new benchmark permits more detailed analysis of methods, including a study of both false positives and false negatives that were not possible with existing benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053245673",
                    "name": "Koyena Pal"
                },
                {
                    "authorId": "1738848458",
                    "name": "Aamod Khatiwada"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2110417078",
                    "name": "Ren\u00e9e J. Miller"
                }
            ]
        },
        {
            "paperId": "e73373a89b54e3ee929bde777092bfb50536dc82",
            "title": "SC-Block: Supervised Contrastive Blocking within Entity Resolution Pipelines",
            "abstract": "The goal of entity resolution is to identify records in multiple datasets that represent the same real-world entity. However, comparing all records across datasets can be computationally intensive, leading to long runtimes. To reduce these runtimes, entity resolution pipelines are constructed of two parts: a blocker that applies a computationally cheap method to select candidate record pairs, and a matcher that afterwards identifies matching pairs from this set using more expensive methods. This paper presents SC-Block, a blocking method that utilizes supervised contrastive learning for positioning records in the embedding space, and nearest neighbour search for candidate set building. We benchmark SC-Block against eight state-of-the-art blocking methods. In order to relate the training time of SC-Block to the reduction of the overall runtime of the entity resolution pipeline, we combine SC-Block with four matching methods into complete pipelines. For measuring the overall runtime, we determine candidate sets with 99.5% pair completeness and pass them to the matcher. The results show that SC-Block is able to create smaller candidate sets and pipelines with SC-Block execute 1.5 to 2 times faster compared to pipelines with other blockers, without sacrificing F1 score. Blockers are often evaluated using relatively small datasets which might lead to runtime effects resulting from a large vocabulary size being overlooked. In order to measure runtimes in a more challenging setting, we introduce a new benchmark dataset that requires large numbers of product offers to be blocked. On this large-scale benchmark dataset, pipelines utilizing SC-Block and the best-performing matcher execute 8 times faster than pipelines utilizing another blocker with the same matcher reducing the runtime from 2.5 hours to 18 minutes, clearly compensating for the 5 minutes required for training SC-Block.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055814315",
                    "name": "Alexander Brinkmann"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "1729154",
                    "name": "Christian Bizer"
                }
            ]
        }
    ]
}