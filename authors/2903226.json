{
    "authorId": "2903226",
    "papers": [
        {
            "paperId": "022d117ecfc97e6a6b06efa2782b8580de39a058",
            "title": "ERM++: An Improved Baseline for Domain Generalization",
            "abstract": "Domain Generalization (DG) measures a classifier's ability to generalize to new distributions of data it was not trained on. Recent work has shown that a hyperparameter-tuned Empirical Risk Minimization (ERM) training procedure, that is simply minimizing the empirical risk on the source domains, can outperform most existing DG methods. ERM has achieved such strong results while only tuning hyper-parameters such as learning rate, weight decay, batch size, and dropout. However there are additional hyperparameters which further limit overfitting and catastrophic forgetting. We therefore focus on tuning previously untuned hyper-parameters, including training amount, initialization, and additional regularizers. We call the resulting stronger baseline ERM++. ERM++ improves the performance of DG by over 5% compared to prior ERM baselines on a standard benchmark of 5 datasets with a ResNet-50 and over 15% with a ViT-B/16, and outperforms all SOTA methods on DomainBed with both architectures. We also explore the relationship between DG performance and similarity to pre-training data, and find that similarity to pre-training data distributions is an important driver of performance, but that ERM++ with stronger initializations can deliver strong performance even on dissimilar datasets.Code is released at https://github.com/piotr-teterwak/erm_plusplus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388407541",
                    "name": "Piotr Teterwak"
                },
                {
                    "authorId": "2116130970",
                    "name": "Kuniaki Saito"
                },
                {
                    "authorId": "1902176",
                    "name": "Theodoros Tsiligkaridis"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "2856622",
                    "name": "Bryan A. Plummer"
                }
            ]
        },
        {
            "paperId": "173f0ba1105373ebf0981f9b5ee30893c43e015d",
            "title": "A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding",
            "abstract": "Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept in existing datasets: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage suite (WikiWeb2M) containing 2M pages with all of the associated image, text, and structure data. We verify its utility on three generative tasks: page description generation, section summarization, and contextual image captioning. We design a novel attention mechanism Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. By using page structure to separate such tokens, it performs better than full attention with lower computational complexity. Extensive experiments show that the new data in WikiWeb2M improves task performance compared to prior work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054917634",
                    "name": "Andrea Burns"
                },
                {
                    "authorId": "1391084712",
                    "name": "Krishna Srinivasan"
                },
                {
                    "authorId": "1643737606",
                    "name": "J. Ainslie"
                },
                {
                    "authorId": "2216499429",
                    "name": "Geoff Brown"
                },
                {
                    "authorId": "2856622",
                    "name": "Bryan A. Plummer"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "2148023",
                    "name": "Jianmo Ni"
                },
                {
                    "authorId": "51150315",
                    "name": "Mandy Guo"
                }
            ]
        },
        {
            "paperId": "2365e7dfce3cff6826cd7875cc986e8e42353418",
            "title": "Label Budget Allocation in Multi-Task Learning",
            "abstract": "The cost of labeling data often limits the performance of machine learning systems. In multi-task learning, related tasks provide information to each other and improve overall performance, but the label cost can vary among tasks. How should the label budget (i.e. the amount of money spent on labeling) be allocated among different tasks to achieve optimal multi-task performance? We are the first to propose and formally define the label budget allocation problem in multi-task learning and to empirically show that different budget allocation strategies make a big difference to its performance. We propose a Task-Adaptive Budget Allocation algorithm to robustly generate the optimal budget allocation adaptive to different multi-task learning settings. Specifically, we estimate and then maximize the extent of new information obtained from the allocated budget as a proxy for multi-task learning performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy of our approach over other widely used heuristic labeling strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2125994125",
                    "name": "Ximeng Sun"
                },
                {
                    "authorId": "1729571",
                    "name": "Kihyuk Sohn"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "3094025",
                    "name": "Clayton Mellina"
                },
                {
                    "authorId": "2233296543",
                    "name": "Xiao Bian"
                }
            ]
        },
        {
            "paperId": "3d743910cded9a81147a73b57c5654233053825b",
            "title": "MaskSketch: Unpaired Structure-guided Masked Image Generation",
            "abstract": "Recent conditional image generation methods produce images of remarkable diversity, fidelity and realism. However, the majority of these methods allow conditioning only on labels or text prompts, which limits their level of control over the generation result. In this paper, we introduce MaskSketch, an image generation method that allows spatial conditioning of the generation result using a guiding sketch as an extra conditioning signal during sampling. MaskSketch utilizes a pretrained masked generative transformer, requiring no model training or paired supervision, and works with input sketches of different levels of abstraction. We show that intermediate self-attention maps of a masked generative transformer encode important structural information of the input image, such as scene layout and object shape, and we propose a novel sampling method based on this observation to enable structure-guided generation. Our results show that MaskSketch achieves high image realism and fidelity to the guiding structure. Evaluated on standard benchmark datasets, MaskSketch outperforms state-of-the-art methods for sketch-to-image translation, as well as unpaired image-to-image translation approaches. The code can be found on our project website: https://masksketch.github.io/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50996059",
                    "name": "D. Bashkirova"
                },
                {
                    "authorId": "143923528",
                    "name": "Jos\u00e9 Lezama"
                },
                {
                    "authorId": "1729571",
                    "name": "Kihyuk Sohn"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "145955800",
                    "name": "Irfan Essa"
                }
            ]
        },
        {
            "paperId": "6afec341440fe74aa198b59ae386c3956179224d",
            "title": "Multiscale Video Pretraining for Long-Term Activity Forecasting",
            "abstract": "Long-term activity forecasting is an especially challenging research problem because it requires understanding the temporal relationships between observed actions, as well as the variability and complexity of human activities. Despite relying on strong supervision via expensive human annotations, state-of-the-art forecasting approaches often generalize poorly to unseen data. To alleviate this issue, we propose Multiscale Video Pretraining (MVP), a novel self-supervised pretraining approach that learns robust representations for forecasting by learning to predict contextualized representations of future video clips over multiple timescales. MVP is based on our observation that actions in videos have a multiscale nature, where atomic actions typically occur at a short timescale and more complex actions may span longer timescales. We compare MVP to state-of-the-art self-supervised video learning approaches on downstream long-term forecasting tasks including long-term action anticipation and video summary prediction. Our comprehensive experiments across the Ego4D and Epic-Kitchens-55/100 datasets demonstrate that MVP out-performs state-of-the-art methods by significant margins. Notably, MVP obtains a relative performance gain of over 20% accuracy in video summary forecasting over existing methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "73441526",
                    "name": "Reuben Tan"
                },
                {
                    "authorId": "144124794",
                    "name": "Matthias De Lange"
                },
                {
                    "authorId": "67294118",
                    "name": "Michael L. Iuzzolino"
                },
                {
                    "authorId": "2856622",
                    "name": "Bryan A. Plummer"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "1788247",
                    "name": "Karl Ridgeway"
                },
                {
                    "authorId": "1732879",
                    "name": "L. Torresani"
                }
            ]
        },
        {
            "paperId": "6c60965ca23a8ae123e2ab4ac6ef08d996ab78d5",
            "title": "Language-Guided Audio-Visual Source Separation via Trimodal Consistency",
            "abstract": "We propose a self-supervised approach for learning to perform audio source separation in videos based on natu-ral language queries, using only unlabeled video and au-dio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised approaches despite not using object detectors or text labels during training. Our project page including publicly available code can be found at https://cs-people.bu.edu/rxtan/projectsNAST.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "73441526",
                    "name": "Reuben Tan"
                },
                {
                    "authorId": "20686092",
                    "name": "Arijit Ray"
                },
                {
                    "authorId": "2054917634",
                    "name": "Andrea Burns"
                },
                {
                    "authorId": "2856622",
                    "name": "Bryan A. Plummer"
                },
                {
                    "authorId": "1786276",
                    "name": "J. Salamon"
                },
                {
                    "authorId": "1899151",
                    "name": "Oriol Nieto"
                },
                {
                    "authorId": "145160921",
                    "name": "Bryan C. Russell"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                }
            ]
        },
        {
            "paperId": "78428efa5d9cffd4f6dea896d441b73cd8dad84f",
            "title": "From Fake to Real: Pretraining on Balanced Synthetic Images to Prevent Spurious Correlations in Image Recognition",
            "abstract": "Visual recognition models are prone to learning spurious correlations induced by a biased training set where certain conditions $B$ (\\eg, Indoors) are over-represented in certain classes $Y$ (\\eg, Big Dogs). Synthetic data from off-the-shelf large-scale generative models offers a promising direction to mitigate this issue by augmenting underrepresented subgroups in the real dataset. However, by using a mixed distribution of real and synthetic data, we introduce another source of bias due to distributional differences between synthetic and real data (\\eg synthetic artifacts). As we will show, prior work's approach for using synthetic data to resolve the model's bias toward $B$ do not correct the model's bias toward the pair $(B, G)$, where $G$ denotes whether the sample is real or synthetic. Thus, the model could simply learn signals based on the pair $(B, G)$ (\\eg, Synthetic Indoors) to make predictions about $Y$ (\\eg, Big Dogs). To address this issue, we propose a simple, easy-to-implement, two-step training pipeline that we call From Fake to Real (FFR). The first step of FFR pre-trains a model on balanced synthetic data to learn robust representations across subgroups. In the second step, FFR fine-tunes the model on real data using ERM or common loss-based bias mitigation methods. By training on real and synthetic data separately, FFR does not expose the model to the statistical differences between real and synthetic data and thus avoids the issue of bias toward the pair $(B, G)$. Our experiments show that FFR improves worst group accuracy over the state-of-the-art by up to 20\\% over three datasets. Code available: \\url{https://github.com/mqraitem/From-Fake-to-Real}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "92749945",
                    "name": "Maan Qraitem"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "2856622",
                    "name": "Bryan A. Plummer"
                }
            ]
        },
        {
            "paperId": "94917c34a35ef3fd9719e54071d9d489c7da4823",
            "title": "DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition With Limited Annotations",
            "abstract": "Multi-label image recognition in the low-label regime is a task of great challenge and practical significance. Previous works have focused on learning the alignment between textual and visual spaces to compensate for limited image labels, yet may suffer from reduced accuracy due to the scarcity of high-quality multi-label annotations. In this research, we leverage the powerful alignment between textual and visual features pretrained with millions of auxiliary image-text pairs. We introduce an efficient and effective framework called Evidence-guided Dual Context Optimization (DualCoOp++), which serves as a unified approach for addressing partial-label and zero-shot multi-label recognition. In DualCoOp++ we separately encode evidential, positive, and negative contexts for target classes as parametric components of the linguistic input (i.e., prompts). The evidential context aims to discover all the related visual content for the target class, and serves as guidance to aggregate positive and negative contexts from the spatial domain of the image, enabling better distinguishment between similar categories. Additionally, we introduce a Winner-Take-All module that promotes inter-class interaction during training, while avoiding the need for extra parameters and costs. As DualCoOp++ imposes minimal additional learnable overhead on the pretrained vision-language framework, it enables rapid adaptation to multi-label recognition tasks with limited annotations and even unseen classes. Experiments on standard multi-label recognition benchmarks across two challenging low-label settings demonstrate the superior performance of our approach compared to state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067770617",
                    "name": "Ping Hu"
                },
                {
                    "authorId": "2125994125",
                    "name": "Ximeng Sun"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                }
            ]
        },
        {
            "paperId": "9b897a8739f04fa4eff7431fd8a3c547b49f4878",
            "title": "Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval",
            "abstract": "In Composed Image Retrieval (CIR), a user combines a query image with text to describe their intended target. Existing methods rely on supervised learning of CIR models using labeled triplets consisting of the query image, text specification, and the target image. Labeling such triplets is expensive and hinders broad applicability of CIR. In this work, we propose to study an important task, Zero-Shot Composed Image Retrieval (ZS-CIR), whose goal is to build a CIR model without requiring labeled triplets for training. To this end, we propose a novel method, called Pic2Word, that requires only weakly labeled image-caption pairs and unlabeled image datasets to train. Unlike existing supervised CIR models, our model trained on weakly labeled or unlabeled datasets shows strong generalization across diverse ZS-CIR tasks, e.g., attribute editing, object composition, and domain conversion. Our approach outperforms several supervised CIR methods on the common CIR benchmark, CIRR and Fashion-IQ. Code will be made publicly available at https://github.com/google-research/composed_image_retrieval",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116130970",
                    "name": "Kuniaki Saito"
                },
                {
                    "authorId": "1729571",
                    "name": "Kihyuk Sohn"
                },
                {
                    "authorId": "48505793",
                    "name": "Xiang Zhang"
                },
                {
                    "authorId": "2136342754",
                    "name": "Chun-Liang Li"
                },
                {
                    "authorId": "50521003",
                    "name": "Chen-Yu Lee"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "1945962",
                    "name": "Tomas Pfister"
                }
            ]
        },
        {
            "paperId": "b083a81fcf36e72bc3918bf8efa3de0e6541fc1e",
            "title": "Socratis: Are large multimodal models emotionally aware?",
            "abstract": "Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2236685883",
                    "name": "Katherine Deng"
                },
                {
                    "authorId": "20686092",
                    "name": "Arijit Ray"
                },
                {
                    "authorId": "73441526",
                    "name": "Reuben Tan"
                },
                {
                    "authorId": "119902504",
                    "name": "Saadia Gabriel"
                },
                {
                    "authorId": "2856622",
                    "name": "Bryan A. Plummer"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                }
            ]
        }
    ]
}