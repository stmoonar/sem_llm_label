{
    "authorId": "2118898475",
    "papers": [
        {
            "paperId": "0e0925bc653bd9fafe41e3dd0dbe5595b150a3b2",
            "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
            "abstract": "Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment, which provides the basis of recommendations for fasting and fed bioequivalence studies to guide the pharmaceutical industry for developing generic drug products. However, manual summarization of food effect from extensive drug application review documents is time-consuming. Therefore, there is a need to develop automated methods to generate food effect summary. Recent advances in natural language processing (NLP), particularly large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability with regard to the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach,iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the quality of the generated summary. We conduct a series of extensive evaluations, ranging from automated metrics to FDA professionals and even evaluation by GPT-4, on 100 NDA review documents selected over the past five years. We observe that the summary quality is progressively improved throughout the iterative prompting process. Moreover, we find that GPT-4 performs better than ChatGPT, as evaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%). Importantly, all the FDA professionals unanimously rated that 85% of the summaries generated by GPT-4 are factually consistent with the golden reference summary, a finding further supported by GPT-4 rating of 72% consistency. Taken together, these results strongly suggest a great potential for GPT-4 to draft food effect summaries that could be reviewed by FDA professionals, thereby improving the efficiency of the PSG assessment cycle and promoting generic drug product development.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2118898475",
                    "name": "Yiwen Shi"
                },
                {
                    "authorId": "2061530524",
                    "name": "Ping Ren"
                },
                {
                    "authorId": "2152438838",
                    "name": "Jing Wang"
                },
                {
                    "authorId": "2220800269",
                    "name": "Biao Han"
                },
                {
                    "authorId": "65986913",
                    "name": "Taha ValizadehAslani"
                },
                {
                    "authorId": "1997966680",
                    "name": "Felix Agbavor"
                },
                {
                    "authorId": "2153915340",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "144642598",
                    "name": "Meng Hu"
                },
                {
                    "authorId": "144010790",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "2220924999",
                    "name": "Hualou Liang"
                }
            ]
        },
        {
            "paperId": "5d1c65510cb422c916c130d999362e51a17bbe56",
            "title": "PharmBERT: a domain-specific BERT model for drug labels",
            "abstract": "Human prescription drug labeling contains a summary of the essential scientific information needed for the safe and effective use of the drug and includes the Prescribing Information, FDA-approved patient labeling (Medication Guides, Patient Package Inserts and/or Instructions for Use), and/or carton and container labeling. Drug labeling contains critical information about drug products, such as pharmacokinetics and adverse events. Automatic information extraction from drug labels may facilitate finding the adverse reaction of the drugs or finding the interaction of one drug with another drug. Natural language processing (NLP) techniques, especially recently developed Bidirectional Encoder Representations from Transformers (BERT), have exhibited exceptional merits in text-based information extraction. A common paradigm in training BERT is to pretrain the model on large unlabeled generic language corpora, so that the model learns the distribution of the words in the language, and then fine-tune on a downstream task. In this paper, first, we show the uniqueness of language used in drug labels, which therefore cannot be optimally handled by other BERT models. Then, we present the developed PharmBERT, which is a BERT model specifically pretrained on the drug labels (publicly available at Hugging Face). We demonstrate that our model outperforms the vanilla BERT, ClinicalBERT and BioBERT in multiple NLP tasks in the drug label domain. Moreover, how the domain-specific pretraining has contributed to the superior performance of PharmBERT is demonstrated by analyzing different layers of PharmBERT, and more insight into how it understands different linguistic aspects of the data is gained.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "65986913",
                    "name": "Taha ValizadehAslani"
                },
                {
                    "authorId": "2118898475",
                    "name": "Yiwen Shi"
                },
                {
                    "authorId": "2061530524",
                    "name": "Ping Ren"
                },
                {
                    "authorId": "2152444658",
                    "name": "Jing Wang"
                },
                {
                    "authorId": "2153915340",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2109753083",
                    "name": "Meng Hu"
                },
                {
                    "authorId": "48096619",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "145177063",
                    "name": "Hualou Liang"
                }
            ]
        },
        {
            "paperId": "3c49a23f72f6337ee68d57911e211fcbbdca6091",
            "title": "Fine-Tuning BERT for Automatic ADME Semantic Labeling in FDA Drug Labeling to Enhance Product-Specific Guidance Assessment",
            "abstract": "Product-specific guidances (PSGs) recommended by the United States Food and Drug Administration (FDA) are instrumental to promote and guide generic drug product development. To assess a PSG, the FDA assessor needs to take extensive time and effort to manually retrieve supportive drug information of absorption, distribution, metabolism, and excretion (ADME) from the reference listed drug labeling. In this work, we leveraged the state-of-the-art pre-trained language models to automatically label the ADME paragraphs in the pharmacokinetics section from the FDA-approved drug labeling to facilitate PSG assessment. We applied a transfer learning approach by fine-tuning the pre-trained Bidirectional Encoder Representations from Transformers (BERT) model to develop a novel application of ADME semantic labeling, which can automatically retrieve ADME paragraphs from drug labeling instead of manual work. We demonstrate that fine-tuning the pre-trained BERT model can outperform conventional machine learning techniques, achieving up to 12.5% absolute F1 improvement. To our knowledge, we were the first to successfully apply BERT to solve the ADME semantic labeling task. We further assessed the relative contribution of pre-training and fine-tuning to the overall performance of the BERT model in the ADME semantic labeling task using a series of analysis methods, such as attention similarity and layer-based ablations. Our analysis revealed that the information learned via fine-tuning is focused on task-specific knowledge in the top layers of the BERT, whereas the benefit from the pre-trained BERT model is from the bottom layers.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2118898475",
                    "name": "Yiwen Shi"
                },
                {
                    "authorId": "2152438838",
                    "name": "Jing Wang"
                },
                {
                    "authorId": "2061530524",
                    "name": "Ping Ren"
                },
                {
                    "authorId": "65986913",
                    "name": "Taha ValizadehAslani"
                },
                {
                    "authorId": "2153910478",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "144642598",
                    "name": "Meng Hu"
                },
                {
                    "authorId": "145177063",
                    "name": "Hualou Liang"
                }
            ]
        },
        {
            "paperId": "45e4563830b021fdd72d45f632feb48a1274ba3f",
            "title": "Two-Stage Fine-Tuning: A Novel Strategy for Learning Class-Imbalanced Data",
            "abstract": ": Classification on long-tailed distributed data is a challenging problem, which suffers from serious class-imbalance and hence poor performance on tail classes with only a few samples. Owing to this paucity of samples, learning on the tail classes is especially challenging for the fine-tuning when transferring a pretrained model to a downstream task. In this work, we present a simple modification of standard fine-tuning to cope with these challenges. Specifically, we propose a two-stage fine-tuning: we first fine-tune the final layer of the pretrained model with class-balanced reweighting loss, and then we perform the standard fine-tuning. Our modification has several benefits: (1) it leverages pretrained representations by only fine-tuning a small portion of the model parameters while keeping the rest untouched; (2) it allows the model to learn an initial representation of the specific task; and importantly (3) it protects the learning of tail classes from being at a disadvantage during the model updating. We conduct extensive experiments on synthetic datasets of both two-class and multi-class tasks of text classification as well as a real-world application to ADME (i.e., absorption, distribution, metabolism, and excretion) semantic labeling. The experimental results show that the proposed two-stage fine-tuning outperforms both fine-tuning with conventional loss and fine-tuning with a reweighting loss on the above datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "65986913",
                    "name": "Taha ValizadehAslani"
                },
                {
                    "authorId": "2118898475",
                    "name": "Yiwen Shi"
                },
                {
                    "authorId": "2152438838",
                    "name": "Jing Wang"
                },
                {
                    "authorId": "2061530524",
                    "name": "Ping Ren"
                },
                {
                    "authorId": "2153910406",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "144642598",
                    "name": "Meng Hu"
                },
                {
                    "authorId": "7900814",
                    "name": "Lianggong Zhao"
                },
                {
                    "authorId": "145177063",
                    "name": "Hualou Liang"
                }
            ]
        },
        {
            "paperId": "4b1886b8fd3c1495fa49016d65ab665b344e5ca8",
            "title": "Improving Imbalanced Learning by Pre-finetuning with Data Augmentation",
            "abstract": "Imbalanced data is ubiquitous in the real world, where there is an uneven distribution of classes in the datasets. Such class imbalance poses a major challenge for modern deep learning, even with the typical class-balanced approaches such as re-sampling and re-weighting. In this work, we introduced a simple training strategy, namely pre-finetuning, as a new intermediate training stage in between the pretrained model and finetuning. We leveraged the idea of data augmentation to learn an initial representation that better fits the imbalanced distribution of the domain task during the pre-finetuning stage. We tested our method on manually contrived imbalanced datasets (both two-class and multi-class) and the FDA drug labeling dataset for ADME (i",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118898475",
                    "name": "Yiwen Shi"
                },
                {
                    "authorId": "65986913",
                    "name": "Taha ValizadehAslani"
                },
                {
                    "authorId": "2152438838",
                    "name": "Jing Wang"
                },
                {
                    "authorId": "2061530524",
                    "name": "Ping Ren"
                },
                {
                    "authorId": "2153915340",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "144642598",
                    "name": "Meng Hu"
                },
                {
                    "authorId": "144010790",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "145177063",
                    "name": "Hualou Liang"
                }
            ]
        },
        {
            "paperId": "b3b4436fc94c60f53d3a7f1f3f4e2da77121b1e6",
            "title": "Linguistic Approach to Segmenting Source Code",
            "abstract": "Source code segmentation is the process of dividing the source code of a program into meaningful pieces, such as in preparation for source code analysis (SCA) tasks. Our goal is to segment code based on the semantics of its content. Specifically such that the segments reflect logical locations that are good candidates for the insertion of manually composed comments or automatically generated comments. Instead of focusing on syntactic boundaries for segmentation, such as function and class declarations, we exploit the semantic content of the code. We use code snippets mined from Github as known semantic segments to train a LSTM Neural Network model. It is able to infer locations in the code where a programmer would likely insert comments. The model can operate on any text and performs well across multiple programming languages for detecting candidate segment boundaries within a program. This semantic code segmentation is especially useful for incomplete code repositories under development, which may be also written in more than one programming language. Additionally, our technique supports a detection threshold parameter so users can adjust the number of suggestions provided by our tool.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "133669537",
                    "name": "Aviel J. Stein"
                },
                {
                    "authorId": "2054535378",
                    "name": "Daniel Ethan Schwartz"
                },
                {
                    "authorId": "2118898475",
                    "name": "Yiwen Shi"
                },
                {
                    "authorId": "2693288",
                    "name": "Spiros Mancoridis"
                }
            ]
        },
        {
            "paperId": "d25a4dcb26f0c6cb3ff0a98bcceb23e04d765dc3",
            "title": "Information Extraction From FDA Drug Labeling to Enhance Product-Specific Guidance Assessment Using Natural Language Processing",
            "abstract": "Towards the objectives of the UnitedStates Food and Drug Administration (FDA) generic drug science and research program, it is of vital importance in developing product-specific guidances (PSGs) with recommendations that can facilitate and guide generic product development. To generate a PSG, the assessor needs to retrieve supportive information about the drug product of interest, including from the drug labeling, which contain comprehensive information about drug products and instructions to physicians on how to use the products for treatment. Currently, although there are many drug labeling data resources, none of them including those developed by the FDA (e.g., Drugs@FDA) can cover all the FDA-approved drug products. Furthermore, these resources, housed in various locations, are often in forms that are not compatible or interoperable with each other. Therefore, there is a great demand for retrieving useful information from a large number of textual documents from different data resources to support an effective PSG development. To meet the needs, we developed a Natural Language Processing (NLP) pipeline by integrating multiple disparate publicly available data resources to extract drug product information with minimal human intervention. We provided a case study for identifying food effect information to illustrate how a machine learning model is employed to achieve accurate paragraph labeling. We showed that the pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is able to outperform the traditional machine learning techniques, setting a new state-of-the-art for labelling food effect paragraphs from drug labeling and approved drug products datasets.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118898475",
                    "name": "Yiwen Shi"
                },
                {
                    "authorId": "2061530524",
                    "name": "Ping Ren"
                },
                {
                    "authorId": "2153915340",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2727036",
                    "name": "Xiajing Gong"
                },
                {
                    "authorId": "2109753083",
                    "name": "Meng Hu"
                },
                {
                    "authorId": "145177063",
                    "name": "Hualou Liang"
                }
            ]
        }
    ]
}