{
    "authorId": "34777258",
    "papers": [
        {
            "paperId": "2e3c9d463e8babf25b406db2f544d7d5fa303b86",
            "title": "PhISANet: Phonetically Informed Speech Animation Network",
            "abstract": "Realistic animation is crucial for immersive and seamless human-avatar interactions as digital avatars become more prevalent. This work presents PhISANet, an encoder-decoder model that realistically animates the face and tongue solely from speech. PhISANet leverages neural audio representations trained on vast amounts of speech to map the speech signal into animation parameters that control the lower face and tongue of realistic 3D models. By integrating a novel multi-task learning strategy during the training phase, PhISANet reincorporates the phonetic information from the input speech, improving articulation in the generated animations. A thorough quantitative and qualitative study validates this improvement, and it determines that WavLM and Whisper features are ideal for training a generalizable speech-animation model regardless of gender, age, and language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "2292220208",
                    "name": "Sarah L. Taylor"
                },
                {
                    "authorId": "2292189926",
                    "name": "Carsten Stoll"
                },
                {
                    "authorId": "2292191569",
                    "name": "Gareth Edwards"
                },
                {
                    "authorId": "2292192991",
                    "name": "Alex Hauptmann"
                },
                {
                    "authorId": "2284215871",
                    "name": "Shinji Watanabe"
                },
                {
                    "authorId": "2292190699",
                    "name": "Iain A. Matthews"
                }
            ]
        },
        {
            "paperId": "7d509c03d3030ec48843a5cf9272f5bed6fd2622",
            "title": "Speech Driven Tongue Animation",
            "abstract": "Advances in speech driven animation techniques allow the creation of convincing animations for virtual characters solely from audio data. Many existing approaches focus on facial and lip motion and they often do not provide realistic animation of the inner mouth. This paper addresses the problem of speech-driven inner mouth animation. Obtaining performance capture data of the tongue and jaw from video alone is difficult because the inner mouth is only partially observable during speech. In this work, we introduce a large-scale speech and mocap dataset that focuses on capturing tongue, jaw, and lip motion. This dataset enables research using data-driven techniques to generate realistic inner mouth animation from speech. We then propose a deep-learning based method for accurate and generalizable speech to tongue and jaw animation, and evaluate several encoder-decoder network architectures and audio feature encoders. We find that recent self-supervised deep learning based audio feature encoders are robust, generalize well to unseen speakers and content, and work best for our task. To demonstrate the practical application of our approach, we show animations on high-quality parametric 3D face models driven by the landmarks generated from our speech-to-tongue animation method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "3358340",
                    "name": "Denis Tom\u00e8"
                },
                {
                    "authorId": "2217934",
                    "name": "Carsten Stoll"
                },
                {
                    "authorId": "1739506",
                    "name": "M. Tiede"
                },
                {
                    "authorId": "1739304",
                    "name": "K. Munhall"
                },
                {
                    "authorId": "145788702",
                    "name": "A. Hauptmann"
                },
                {
                    "authorId": "83165362",
                    "name": "Iain Matthews"
                }
            ]
        },
        {
            "paperId": "6e0a4138a39ce8259c153b917c2c41dbd62f1c69",
            "title": "Importance of Parasagittal Sensor Information in Tongue Motion Capture Through a Diphonic Analysis",
            "abstract": "Our study examines the information obtained by adding two parasagittal sensors to the standard midsagittal configuration of an Electromagnetic Articulography (EMA) observation of lingual articulation. In this work, we present a large and phonetically balanced corpus obtained from an EMA recording session of a single English native speaker reading 1899 sentences from the Harvard and TIMIT corpora. According to a statistical analysis of the diphones produced during the recording session, the motion captured by the parasagittal sensors has a low correlation to the midsagittal sensors in the mediolateral direction. We perform a geometric analysis of the lateral tongue by the measure of its width and using a proxy of the tongue\u2019s curvature that is computed using the Menger curvature. To provide a better understanding of the tongue sensor motion we present dynamic visualizations of all diphones. Finally, we present a summary of the velocity information computed from the tongue sensor information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "145324126",
                    "name": "Sarah L. Taylor"
                },
                {
                    "authorId": "1739506",
                    "name": "M. Tiede"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "2057077632",
                    "name": "Iain A. Matthews"
                }
            ]
        },
        {
            "paperId": "48c8774036f304e7f12e1993afdab2daf402a5a3",
            "title": "Event-Related Bias Removal for Real-time Disaster Events",
            "abstract": "Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volumes of data in real-time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real-time systems requires training on out-of-domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre-trained on similar event types. However, those models capture unnecessary event-specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event-specific biases and improve the performance on tweet importance classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3373316",
                    "name": "Evangelia Spiliopoulou"
                },
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                }
            ]
        },
        {
            "paperId": "0805cb1b26577f08f84190445992f7f0584e4742",
            "title": "OPERA: Operations-oriented Probabilistic Extraction, Reasoning, and Analysis",
            "abstract": "The OPERA system of CMU and USC/ISI performs end-to-end information extraction from multiple media and languages (English, Russian, Ukrainian), integrates the results, builds Knowledge Bases about the domain, and does hypothesis creation and reasoning to answer questions. ",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                },
                {
                    "authorId": "2284176",
                    "name": "Hans Chalupsky"
                },
                {
                    "authorId": "145001267",
                    "name": "A. Gershman"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "1706595",
                    "name": "T. Mitamura"
                },
                {
                    "authorId": "38599655",
                    "name": "Zaid A. W. Sheikh"
                },
                {
                    "authorId": "1741515",
                    "name": "Ankit Dangi"
                },
                {
                    "authorId": "51250894",
                    "name": "Aditi Chaudhary"
                },
                {
                    "authorId": "2135112672",
                    "name": "Xianyang Chen"
                },
                {
                    "authorId": "97791350",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "1410241246",
                    "name": "Bernie Huang"
                },
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "2109279237",
                    "name": "H. Liu"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "1410648718",
                    "name": "Maria Ryskina"
                },
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "2126048085",
                    "name": "Varun Gangal"
                }
            ]
        },
        {
            "paperId": "03d48647ebb267dd4f4ed07777bade1a0ba94862",
            "title": "Where is this? Video geolocation based on neural network features",
            "abstract": "In this work we propose a method that geolocates videos within a delimited widespread area based solely on the frames visual content. Our proposed method tackles video-geolocation through traditional image retrieval techniques considering Google Street View as the reference point. To achieve this goal we use the deep learning features obtained from NetVLAD to represent images, since through this feature vectors the similarity is their L2 norm. In this paper, we propose a family of voting-based methods to aggregate frame-wise geolocation results which boost the video geolocation result. The best aggregation found through our experiments considers both NetVLAD and SIFT similarity, as well as the geolocation density of the most similar results. To test our proposed method, we gathered a new video dataset from Pittsburgh Downtown area to benefit and stimulate more work in this area. Our system achieved a precision of 90% while geolocating videos within a range of 150 meters or two blocks away from the original position.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "2475437",
                    "name": "Zhuyun Dai"
                },
                {
                    "authorId": "2553517",
                    "name": "Yingkai Gao"
                }
            ]
        },
        {
            "paperId": "9e34161648fd9312cef6b9e2a07102b7bb6f9493",
            "title": "A social crowd-controlled orchestra",
            "abstract": "We present a novel social interactive system that brings music creation to the crowds. It allows anyone with a personal electronic device and regardless of their musical expertise to participate in the music an orchestra generates, while also encouraging social interactions among participants. Users can be either musicians or conductors. The latter drive the group's music production, while the others follow simple game-like instructions on their device for playing the conductor's selected songs. The primary mode of interaction for playing songs, consists of rotating one's electronic device in different ways. Our system also provides different social interaction cues that aim for novice and expert users to start conversations with each other and improve the group's performance as well as construct a community among participants. We report on a qualitative analysis, which suggests that users appreciate the system's versatility in providing service to a variety of devices and enjoy participating in the orchestra with their own personal device. Users judged the social interaction cues the system provided to be effective. We believe our findings aid the design of future social intelligent environments that seek to build communities with participants through music.",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1983163",
                    "name": "Saiph Savage"
                },
                {
                    "authorId": "1403852033",
                    "name": "Norma Elva Ch\u00e1vez-Rodr\u00edguez"
                },
                {
                    "authorId": "2602469",
                    "name": "Carlos Toxtli"
                },
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "1405770991",
                    "name": "D. \u00c1lvarez-L\u00f3pez"
                },
                {
                    "authorId": "15425485",
                    "name": "Tobias H\u00f6llerer"
                }
            ]
        }
    ]
}