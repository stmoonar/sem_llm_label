{
    "authorId": "143753639",
    "papers": [
        {
            "paperId": "3566e1245bfc90096fe0cdb8b18674da6519c8d6",
            "title": "Large Language Model Augmented Narrative Driven Recommendations",
            "abstract": "Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context \u2013 this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                }
            ]
        },
        {
            "paperId": "5aa7ed285d8671f3aca85ee9dc282e6a5a22691b",
            "title": "Causal Matching with Text Embeddings: A Case Study in Estimating the Causal Effects of Peer Review Policies",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110045005",
                    "name": "Raymond Zhang"
                },
                {
                    "authorId": "2133303205",
                    "name": "Neha Nayak Kennard"
                },
                {
                    "authorId": "2108917760",
                    "name": "Daniel Smith"
                },
                {
                    "authorId": "100525940",
                    "name": "Daniel A. McFarland"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "145137850",
                    "name": "Katherine A. Keith"
                }
            ]
        },
        {
            "paperId": "64758f4709b5558619d3fa6e7c56fa08be623dc6",
            "title": "Answering Compositional Queries with Set-Theoretic Embeddings",
            "abstract": "The need to compactly and robustly represent item-attribute relations arises in many important tasks, such as faceted browsing and recommendation systems. A popular machine learning approach for this task denotes that an item has an attribute by a high dot-product between vectors for the item and attribute -- a representation that is not only dense, but also tends to correct noisy and incomplete data. While this method works well for queries retrieving items by a single attribute (such as \\emph{movies that are comedies}), we find that vector embeddings do not so accurately support compositional queries (such as movies that are comedies and British but not romances). To address these set-theoretic compositions, this paper proposes to replace vectors with box embeddings, a region-based representation that can be thought of as learnable Venn diagrams. We introduce a new benchmark dataset for compositional queries, and present experiments and analysis providing insights into the behavior of both. We find that, while vector and box embeddings are equally suited to single attribute queries, for compositional queries box embeddings provide substantial advantages over vectors, particularly at the moderate and larger retrieval set sizes that are most useful for users' search and browsing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51133806",
                    "name": "S. Dasgupta"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "2843982",
                    "name": "Steffen Rendle"
                },
                {
                    "authorId": "2152831594",
                    "name": "Li Zhang"
                }
            ]
        },
        {
            "paperId": "672491163a327f80e08ce3ef4751e94c78631822",
            "title": "Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond",
            "abstract": "Is the output softmax layer, which is adopted by most language models (LMs), always the best way to compute the next word probability? Given so many attention layers in a modern transformer-based LM, are the pointer networks redundant nowadays? In this study, we discover that the answers to both questions are no. This is because the softmax bottleneck sometimes prevents the LMs from predicting the desired distribution and the pointer networks can be used to break the bottleneck efficiently. Based on the finding, we propose several softmax alternatives by simplifying the pointer networks and accelerating the word-by-word rerankers. In GPT-2, our proposals are significantly better and more efficient than mixture of softmax, a state-of-the-art softmax alternative. In summarization experiments, without significantly decreasing its training/testing speed, our best method based on T5-Small improves factCC score by 2 points in CNN/DM and XSUM dataset, and improves MAUVE scores by 30% in BookSum paragraph-level dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144827671",
                    "name": "Haw-Shiuan Chang"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "1740689630",
                    "name": "Alolika Gon"
                },
                {
                    "authorId": "2119121475",
                    "name": "Hong Yu"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                }
            ]
        },
        {
            "paperId": "7843a49a497785eeb2680d3a1079024126142db0",
            "title": "Editable User Profiles for Controllable Text Recommendations",
            "abstract": "Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the controllability of LACE under simulated user interactions. Finally, we implement LACE in an interactive controllable recommender system and conduct a user study to demonstrate that users are able to improve the quality of recommendations they receive through interactions with an editable user profile.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9076501",
                    "name": "Sheshera Mysore"
                },
                {
                    "authorId": "145493030",
                    "name": "Mahmood Jasim"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                }
            ]
        },
        {
            "paperId": "7a204cf2879f5052095a799828ad57f340c5ba0b",
            "title": "Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders",
            "abstract": "Cross-encoder models, which jointly encode and score a query-item pair, are typically prohibitively expensive for k -nearest neighbor search. Consequently, k -NN search is performed not with a cross-encoder, but with a heuristic retrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent work proposes ANN CUR (Yadav et al., 2022) which uses CUR matrix factorization to produce an embedding space for ef\ufb01cient vector-based search that directly approximates the cross-encoder without the need for dual-encoders. ANN CUR de\ufb01nes this shared query-item embedding space by scoring the test query against anchor items which are sampled uniformly at random. While this minimizes average approximation error over all items, unsuitably high approximation error on top-k items remains and leads to poor recall of top-k (and especially top-1) items. Increasing the number of anchor items is a straightforward way of improving the approximation error and hence k - NN recall of ANN CUR but at the cost of increased inference latency. In this paper, we propose a new method for adaptively choosing anchor items that minimizes the approximation error for the practically important top-k neighbors for a query with minimal computational overhead. Our proposed method incrementally selects a suitable set of anchor items for a given test query over several rounds, us-ing anchors chosen in previous rounds to inform selection of more anchor items. Empiri-cally, our method consistently improves k -NN recall as compared to both ANN CUR and the widely-used dual-encoder-based retrieve-and-rerank approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144132372",
                    "name": "Nishant Yadav"
                },
                {
                    "authorId": "2121348263",
                    "name": "Nicholas Monath"
                },
                {
                    "authorId": "1771307",
                    "name": "M. Zaheer"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                }
            ]
        },
        {
            "paperId": "a1a228c4e9a3bffb43e4d9eed8cc9623b525aa99",
            "title": "Machine Reading Comprehension using Case-based Reasoning",
            "abstract": "We present an accurate and interpretable method for answer extraction in machine reading comprehension that is reminiscent of case-based reasoning (CBR) from classical AI. Our method (CBR-MRC) builds upon the hypothesis that contextualized answers to similar questions share semantic similarities with each other. Given a test question, CBR-MRC first retrieves a set of similar cases from a non-parametric memory and then predicts an answer by selecting the span in the test context that is most similar to the contextualized representations of answers in the retrieved cases. The semi-parametric nature of our approach allows it to attribute a prediction to the specific set of evidence cases, making it a desirable choice for building reliable and debuggable QA systems. We show that CBR-MRC provides high accuracy comparable with large reader models and outperforms baselines by 11.5 and 8.4 EM on NaturalQuestions and NewsQA, respectively. Further, we demonstrate the ability of CBR-MRC in identifying not just the correct answer tokens but also the span with the most relevant supporting evidence. Lastly, we observe that contexts for certain question types show higher lexical diversity than others and find that CBR-MRC is robust to these variations while performance using fully-parametric methods drops.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064894364",
                    "name": "Dung Ngoc Thai"
                },
                {
                    "authorId": "2057234493",
                    "name": "Dhruv Agarwal"
                },
                {
                    "authorId": "2046903043",
                    "name": "Mudit Chaudhary"
                },
                {
                    "authorId": "143863023",
                    "name": "Rajarshi Das"
                },
                {
                    "authorId": "1771307",
                    "name": "M. Zaheer"
                },
                {
                    "authorId": "15073531",
                    "name": "J. Lee"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                }
            ]
        },
        {
            "paperId": "a90a0e83734e1c51261ce025fe57e4f950406ee3",
            "title": "Improving Dual-Encoder Training through Dynamic Indexes for Negative Mining",
            "abstract": "Dual encoder models are ubiquitous in modern classification and retrieval. Crucial for training such dual encoders is an accurate estimation of gradients from the partition function of the softmax over the large output space; this requires finding negative targets that contribute most significantly (\"hard negatives\"). Since dual encoder model parameters change during training, the use of traditional static nearest neighbor indexes can be sub-optimal. These static indexes (1) periodically require expensive re-building of the index, which in turn requires (2) expensive re-encoding of all targets using updated model parameters. This paper addresses both of these challenges. First, we introduce an algorithm that uses a tree structure to approximate the softmax with provable bounds and that dynamically maintains the tree. Second, we approximate the effect of a gradient update on target encodings with an efficient Nystrom low-rank approximation. In our empirical study on datasets with over twenty million targets, our approach cuts error by half in relation to oracle brute-force negative mining. Furthermore, our method surpasses prior state-of-the-art while using 150x less accelerator memory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121348263",
                    "name": "Nicholas Monath"
                },
                {
                    "authorId": "1771307",
                    "name": "M. Zaheer"
                },
                {
                    "authorId": "145254624",
                    "name": "Kelsey R. Allen"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                }
            ]
        },
        {
            "paperId": "bcd42145cf7bf312484e94b86bd0c6895cb99975",
            "title": "KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51221589",
                    "name": "Sandeep Silwal"
                },
                {
                    "authorId": "37863838",
                    "name": "Sara Ahmadian"
                },
                {
                    "authorId": "2064161903",
                    "name": "A. Nystrom"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "143812128",
                    "name": "Deepak Ramachandran"
                },
                {
                    "authorId": "2470890",
                    "name": "Seyed Mehran Kazemi"
                }
            ]
        },
        {
            "paperId": "cd31009d1163ef892b45b21973d1daf9b254eb7a",
            "title": "Low-Resource Compositional Semantic Parsing with Concept Pretraining",
            "abstract": "Semantic parsing plays a key role in digital voice assistants such as Alexa, Siri, and Google Assistant by mapping natural language to structured meaning representations. When we want to improve the capabilities of a voice assistant by adding a new domain, the underlying semantic parsing model needs to be retrained using thousands of annotated examples from the new domain, which is time-consuming and expensive. In this work, we present an architecture to perform such domain adaptation automatically, with only a small amount of metadata about the new domain and without any new training data (zero-shot) or with very few examples (few-shot). We use a base seq2seq (sequence-to-sequence) architecture and augment it with a concept encoder that encodes intent and slot tags from the new domain. We also introduce a novel decoder-focused approach to pretrain seq2seq models to be concept aware using Wikidata and use it to help our model learn important concepts and perform well in low-resource settings. We report few-shot and zero-shot results for compositional semantic parsing on the TOPv2 dataset and show that our model outperforms prior approaches in few-shot settings for the TOPv2 and SNIPS datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3329096",
                    "name": "Subendhu Rongali"
                },
                {
                    "authorId": "1734869335",
                    "name": "Mukund Sridhar"
                },
                {
                    "authorId": "144165565",
                    "name": "Haidar Khan"
                },
                {
                    "authorId": "1757320",
                    "name": "Konstantine Arkoudas"
                },
                {
                    "authorId": "1836135",
                    "name": "Wael Hamza"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                }
            ]
        }
    ]
}