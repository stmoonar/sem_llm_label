{
    "authorId": "2048340039",
    "papers": [
        {
            "paperId": "1858f931cf47a60672807a36bbe285d8693fd035",
            "title": "Light Field Depth Estimation for Non-Lambertian Objects via Adaptive Cross Operator",
            "abstract": "Light field (LF) depth estimation is a crucial basis for LF-related applications. Most existing methods are based on the Lambertian assumption and cannot deal with non-Lambertian surfaces represented by transparent objects and mirrors. In this paper, we propose a novel Adaptive-Cross-Operator-based(ACO) depth estimation algorithm for non-Lambertian LF. By analyzing the imaging characteristics of non-Lambertian regions, it is found that the difficulty of depth estimation lies in the photo inconsistency of the center view. Combining with the two-branch structure, we propose ACO with an inter-branch cooperation strategy to adaptively separate depth information with different reflectance coefficients. We discover that the bimodal distribution feature of the operator filtering results can assist in the separation of multi-layer scene information. The first detection branch filters the EPI and implicitly records the severity of multi-layer scene aliasing. According to the identification of bimodal distribution features, the non-Lambertian regions are marked out and the depth of the foreground is estimated. The second branch receives guidance from the first to dynamically adjust the inner weight and infer the background\u2019s depth after weakening the interference from the foreground. Finally, the depth information separation of multi-layer scenes is achieved by extracting the unique X-shaped linear structure. Without the reflection coefficients of the non-Lambertian object, the proposed method can produce high-quality depth estimation under the transparency of 90% to 20%. Experimental results show that the proposed ACO outperforms state-of-the-art LF depth estimation methods in terms of accuracy and robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "98379409",
                    "name": "Zhenglong Cui"
                },
                {
                    "authorId": "2057234868",
                    "name": "Hao Sheng"
                },
                {
                    "authorId": "143942184",
                    "name": "D. Yang"
                },
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "2122822005",
                    "name": "Rongshan Chen"
                },
                {
                    "authorId": "4505317",
                    "name": "Wei Ke"
                }
            ]
        },
        {
            "paperId": "cc4908cdcc4726a48402b9adeb8ac43a14000f23",
            "title": "A2CI: A Cloud-based, Service-oriented Geospatial Cyberinfrastructure to Support Atmospheric Research",
            "abstract": "Big earth science data offers the scientific community great opportunities. Many more studies at large-scales, over long-terms and at high resolution can now be conducted using the rich information collected by remote sensing satellites, ground-based sensor networks, and even social media input. However, the hundreds of terabytes of information collected and compiled on an hourly basis by NASA and other government agencies present a significant challenge for atmospheric scientists seeking to improve the understanding of the Earth atmospheric system. These challenges include effective discovery, organization, analysis and visualization of large amounts of data. This paper reports the outcomes of an NSF-funded project that developed a geospatial cyberinfrastructure -- the A2CI (Atmospheric Analysis Cyberinfrastructure) -- to support atmospheric research. We first introduce the service-oriented system framework then describe in detail the implementation of the data discovery module, data management module, data integration module, data analysis and visualization modules following the cloud computing principles-Data-as-a-Service, Software-as-a-Service, Platform-as-a-Service and Infrastructure-as-a-Service. We demonstrate the graphic user interface by performing an analysis between Sea Surface Temperature and the intensity of tropical storms in the North Atlantic and Pacific oceans. We expect this work to contribute to the technical advancement of cyberinfrastructure research as well as to the development of an online, collaborative scientific analysis system for atmospheric science.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47113683",
                    "name": "Wenwen Li"
                },
                {
                    "authorId": "145265121",
                    "name": "H. Shao"
                },
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "2293231586",
                    "name": "Xiran Zhou"
                },
                {
                    "authorId": "2116563965",
                    "name": "Sheng Wu"
                }
            ]
        },
        {
            "paperId": "4b422c479ed1c21c462d4105ccc52d854a350b89",
            "title": "LFNAT 2023 Challenge on Light Field Depth Estimation: Methods and Results",
            "abstract": "This paper reviews the 1st LFNAT challenge on light field depth estimation, which aims at predicting disparity information of central view image in a light field (i.e., pixel offset between central view image and adjacent view image). Compared to multi-view stereo matching, light field depth estimation emphasizes efficient utilization of the 2D angular information from multiple regularly varying views. This challenge specifies UrbanLF [20] light field dataset as the sole data source. There are two phases in total: submission phase and final evaluation phase, in which 75 registered participants successfully submit their predicted results in the first phase and 7 eligible teams compete in the second phase. The performance of all submissions is carefully reviewed and shown in this paper as a new standard for the current state-of-the-art in light field depth estimation. Moreover, the implementation details of these methods are also provided to stimulate related advanced research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057234868",
                    "name": "Hao Sheng"
                },
                {
                    "authorId": "1680777",
                    "name": "Yebin Liu"
                },
                {
                    "authorId": "2152356",
                    "name": "Jingyi Yu"
                },
                {
                    "authorId": "2861519",
                    "name": "Gaochang Wu"
                },
                {
                    "authorId": "2055448549",
                    "name": "Wei Xiong"
                },
                {
                    "authorId": "2174482701",
                    "name": "Ruixuan Cong"
                },
                {
                    "authorId": "2122822005",
                    "name": "Rongshan Chen"
                },
                {
                    "authorId": "2230137509",
                    "name": "Longzhao Guo"
                },
                {
                    "authorId": "2199228530",
                    "name": "Yanlin Xie"
                },
                {
                    "authorId": "2117777418",
                    "name": "Shuo Zhang"
                },
                {
                    "authorId": "2121701750",
                    "name": "Song Chang"
                },
                {
                    "authorId": "2624174",
                    "name": "Youfang Lin"
                },
                {
                    "authorId": "2054523102",
                    "name": "Wentao Chao"
                },
                {
                    "authorId": "2182459255",
                    "name": "Xuechun Wang"
                },
                {
                    "authorId": "122337128",
                    "name": "Guanghui Wang"
                },
                {
                    "authorId": "2303090",
                    "name": "Fuqing Duan"
                },
                {
                    "authorId": "2230166460",
                    "name": "Tun Wang"
                },
                {
                    "authorId": "143942184",
                    "name": "D. Yang"
                },
                {
                    "authorId": "98379409",
                    "name": "Zhenglong Cui"
                },
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "2218650864",
                    "name": "Mingyuan Zhao"
                },
                {
                    "authorId": "2230011956",
                    "name": "Qiong Wang"
                },
                {
                    "authorId": "2157954216",
                    "name": "Qi-An Chen"
                },
                {
                    "authorId": "2572896",
                    "name": "Zhengyu Liang"
                },
                {
                    "authorId": "46396159",
                    "name": "Yingqian Wang"
                },
                {
                    "authorId": "2130743369",
                    "name": "Jung-Mo Yang"
                },
                {
                    "authorId": "2145181345",
                    "name": "Xueting Yang"
                },
                {
                    "authorId": "2153368670",
                    "name": "Junli Deng"
                }
            ]
        },
        {
            "paperId": "929fe680da4675f0591012fc67655ec8a6733536",
            "title": "Assessment of a new GeoAI foundation model for flood inundation mapping",
            "abstract": "Vision foundation models are a new frontier in Geospatial Artificial Intelligence (GeoAI), an interdisciplinary research area that applies and extends AI for geospatial problem solving and geographic knowledge discovery, because of their potential to enable powerful image analysis by learning and extracting important image features from vast amounts of geospatial data. This paper evaluates the performance of the first-of-its-kind geospatial foundation model, IBM-NASA's Prithvi, to support a crucial geospatial analysis task: flood inundation mapping. This model is compared with convolutional neural network and vision transformer-based architectures in terms of mapping accuracy for flooded areas. A benchmark dataset, Sen1Floods11, is used in the experiments, and the models' predictability, generalizability, and transferability are evaluated based on both a test dataset and a dataset that is completely unseen by the model. Results show the good transferability of the Prithvi model, highlighting its performance advantages in segmenting flooded areas in previously unseen regions. The findings also indicate areas for improvement for the Prithvi model in terms of adopting multi-scale representation learning, developing more end-to-end pipelines for high-level image analysis tasks, and offering more flexibility in terms of input data bands.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108785575",
                    "name": "Wenwen Li"
                },
                {
                    "authorId": "2203041854",
                    "name": "Hyunho Lee"
                },
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "2110552703",
                    "name": "Chia-Yu Hsu"
                },
                {
                    "authorId": "6597351",
                    "name": "S. Arundel"
                }
            ]
        },
        {
            "paperId": "a4bf31715db94f304ab081bc596cf5db0a420e24",
            "title": "Multi-view Semantic Information Guidance for Light Field Image Segmentation",
            "abstract": "One of the great important fields of computer vision is semantic segmentation. As for single image semantic segmentation, due to limited available information, it appears poor performance when the occlusion and similar color interference occur, and has difficulty exploiting the rich scene information. In comparison, the special micro-len array structure of light field camera can record multi-view information of the scene, which provides us with a new solution to solve this issue. In this paper, we propose a multi-view semantic information guidance network (MSIGNet) for light field semantic segmentation. It can effectively utilize semantic information from multi-view images to guide pixel feature of center view image. First, we extract feature of each view image and further obtain semantic probability. Then all probabilities are aggregated through a self-adaptive multi-view probability fusion module. Last, the resulting coarse fusion representation interacts with center view feature to obtain the refined segmentation result. The proposed method shows excellent performance on both real-world and synthetic light field datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144437343",
                    "name": "Yiming Li"
                },
                {
                    "authorId": "2174482701",
                    "name": "Ruixuan Cong"
                },
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "2218650864",
                    "name": "Mingyuan Zhao"
                },
                {
                    "authorId": "2145955253",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "153196268",
                    "name": "F. Li"
                },
                {
                    "authorId": "2057234868",
                    "name": "Hao Sheng"
                }
            ]
        },
        {
            "paperId": "ad7700607f13c3944488fd76c6f117179e27e50d",
            "title": "GeoGraphVis: A Knowledge Graph and Geovisualization Empowered Cyberinfrastructure to Support Disaster Response and Humanitarian Aid",
            "abstract": "The past decade has witnessed an increasing frequency and intensity of disasters, from extreme weather, drought, and wildfires to hurricanes, floods, and wars. Providing timely disaster response and humanitarian aid to these events is a critical topic for decision makers and relief experts in order to mitigate impacts and save lives. When a disaster occurs, it is important to acquire first-hand, real-time information about the potentially affected area, its infrastructure, and its people in order to develop situational awareness and plan a response to address the health needs of the affected population. This requires rapid assembly of multi-source geospatial data that need to be organized and visualized in a way to support disaster-relief efforts. In this paper, we introduce a new cyberinfrastructure solution\u2014GeoGraphVis\u2014that is empowered by knowledge graph technology and advanced visualization to enable intelligent decision making and problem solving. There are three innovative features of this solution. First, a location-aware knowledge graph is created to link and integrate cross-domain data to make the graph analytics-ready. Second, expert-driven disaster response workflows are analyzed and modeled as machine-understandable decision paths to guide knowledge exploration via the graph. Third, a scene-based visualization strategy is developed to enable interactive and heuristic visual analytics to better comprehend disaster impact situations and develop action plans for humanitarian aid.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185250078",
                    "name": "Wenwen Li"
                },
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "2211350417",
                    "name": "Xiao Chen"
                },
                {
                    "authorId": "2152948625",
                    "name": "Yuanyuan Tian"
                },
                {
                    "authorId": "40896909",
                    "name": "Zhining Gu"
                },
                {
                    "authorId": "1406553068",
                    "name": "Anna Lopez-Carr"
                },
                {
                    "authorId": "48623101",
                    "name": "A. Schroeder"
                },
                {
                    "authorId": "88100171",
                    "name": "Kitty Currier"
                },
                {
                    "authorId": "1962202",
                    "name": "M. Schildhauer"
                },
                {
                    "authorId": "144649570",
                    "name": "Rui Zhu"
                }
            ]
        },
        {
            "paperId": "c70c5bae4042f8b1b7a15a6c62f2ba3f6bfda982",
            "title": "GeoGraphViz: Geographically constrained 3D force\u2010directed graph for knowledge graph visualization",
            "abstract": "Knowledge graphs are a key technique for linking and integrating cross\u2010domain data, concepts, tools, and knowledge to enable data\u2010driven analytics. As much of the world's data have become massive in size, visualizing graph entities and their interrelationships intuitively and interactively has become a crucial task for ingesting and better utilizing graph content to support semantic reasoning, discovering hidden knowledge discovering, and better scientific understanding of geophysical and social phenomena. Despite the fact that many such phenomena (e.g., disasters) have clear spatial footprints and geographic properties, their location information is considered only as a textual label in existing graph visualization tools, limiting their capability to reveal the geospatial distribution patterns of the graph nodes. In addition, most graph visualization techniques rely on 2D graph visualization, which constrains the dimensions of information that can be presented and lacks support for graph structure examination from multiple angles. To tackle the above challenges, we developed a novel 3D map\u2010based graph visualization algorithm to enable interactive exploration of graph content and patterns in a spatially explicit manner. The algorithm extends a 3D force directed graph by integrating a web map, an additional geolocational force, and a force balancing variable that allows for the dynamic adjustment of the 3D graph structure and layout. This mechanism helps create a balanced graph view between the semantic forces among the graph nodes and the attractive force from a geolocation to a graph node. Our solution offers a new perspective in visualizing and understanding spatial entities and events in a knowledge graph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "2185250078",
                    "name": "Wenwen Li"
                },
                {
                    "authorId": "40896909",
                    "name": "Zhining Gu"
                }
            ]
        },
        {
            "paperId": "d5137f5202aa8d77caab9cb1b3b22a278caa14df",
            "title": "Cross-View Recurrence-Based Self-Supervised Super-Resolution of Light Field",
            "abstract": "Compared with external-supervised learning-based (ESLB) methods, self-supervised learning-based (SSLB) methods can overcome the domain gap problem caused by different light field (LF) acquisition conditions, which results in the performance degradation of light field super-resolution on unseen test datasets. Current SSLB methods exploit the cross-scale recurrence feature in the single view image for super-resolution, ignoring the correlation information among views. Different from previous works, we propose a cross-view recurrence-based self-supervised mapping framework to correlate complementary information among views in the down-scaled input LF. Specifically, the cross-view recurrence information consists of geometry structure features and similar structure features. The former is to provide sub-pixel information according to disparity correlations among adjacent views, and the latter is to acquire similar color and contour information among arbitrary views, which can compensate for error disparity guidance of geometry structure features in sharp variance areas. Moreover, instead of the widely used \u201cAll-to-All\u201d strategy, we propose a \u201cPart-to-Part\u201d mapping strategy, which is better competent for SSLB approaches with limited training examples solely extracted from input LF. Finally, considering that self-supervised methods need to retrain from the beginning toward each test image, based on the proposed \u201cpart-to-part\u201d strategy, an efficient end-to-end network is designed to extract these cross-view features for superior SASR performance with less training time. Experiment results demonstrate that our method outperforms other state-of-the-art ESLB methods on both large and small domain gap cases. Compared with the only SSLB method (LFZSSR), our approach achieves better performance with 524 times less training time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057234868",
                    "name": "Hao Sheng"
                },
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "143942184",
                    "name": "D. Yang"
                },
                {
                    "authorId": "2174482701",
                    "name": "Ruixuan Cong"
                },
                {
                    "authorId": "98379409",
                    "name": "Zhenglong Cui"
                },
                {
                    "authorId": "2122822005",
                    "name": "Rongshan Chen"
                }
            ]
        },
        {
            "paperId": "e4f230dc5fce17293a3f4cf39150a52e45808125",
            "title": "Semantic similarity measure of natural language text through machine learning and a keyword\u2010aware cross\u2010encoder\u2010ranking summarizer\u2014A case study using UCGIS GIS&T body of knowledge",
            "abstract": "Initiated by the University Consortium of Geographic Information Science (UCGIS), the GIS&T Body of Knowledge (BoK) is a community\u2010driven endeavor to define, develop, and document geospatial topics related to geographic information science and technologies (GIS&T). In recent years, GIS&T BoK has undergone rigorous development in terms of its topic re\u2010organization and content updating, resulting in a new digital version of the project. While the BoK topics provide useful materials for researchers and students to learn about GIS, the semantic relationships among the topics, such as semantic similarity, should also be identified so that a better and automated topic navigation can be achieved. Currently, the related topics are either defined manually by editors or authors, which may result in an incomplete assessment of topic relationships. To address this challenge, our research evaluates the effectiveness of multiple natural language processing (NLP) techniques in extracting semantics from text, including both deep neural networks and traditional machine learning approaches. Besides, a novel text summarization\u2014KACERS (Keyword\u2010Aware Cross\u2010Encoder\u2010Ranking Summarizer)\u2014is proposed to generate a semantic summary of scientific publications. By identifying the semantic linkages among key topics, this work guides the future development and content organization of the GIS&T BoK project. It also offers a new perspective on the use of machine learning techniques for analyzing scientific publications and demonstrates the potential of the KACERS summarizer in semantic understanding of long text documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152948625",
                    "name": "Yuanyuan Tian"
                },
                {
                    "authorId": "2185250078",
                    "name": "Wenwen Li"
                },
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "40896909",
                    "name": "Zhining Gu"
                }
            ]
        },
        {
            "paperId": "e663d03c7df2bdfd60323ccb181c4d5270ccadfc",
            "title": "Combining Implicit-Explicit View Correlation for Light Field Semantic Segmentation",
            "abstract": "Since light field simultaneously records spatial information and angular information of light rays, it is considered to be beneficial for many potential applications, and semantic segmentation is one of them. The regular variation of image information across views facilitates a comprehensive scene understanding. However, in the case of limited memory, the high-dimensional property of light field makes the problem more intractable than generic semantic segmentation, manifested in the difficulty of fully exploiting the relationships among views while maintaining contextual information in single view. In this paper, we propose a novel network called LF-IENet for light field semantic segmentation. It contains two different manners to mine complementary information from surrounding views to segment central view. One is implicit feature integration that leverages attention mechanism to compute inter-view and intra-view similarity to modulate features of central view. The other is explicit feature propagation that directly warps features of other views to central view under the guidance of disparity. They complement each other and jointly realize complementary information fusion across views in light field. The proposed method achieves outperforming performance on both real-world and synthetic light field datasets, demonstrating the effectiveness of this new architecture.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2174482701",
                    "name": "Ruixuan Cong"
                },
                {
                    "authorId": "143942184",
                    "name": "D. Yang"
                },
                {
                    "authorId": "2122822005",
                    "name": "Rongshan Chen"
                },
                {
                    "authorId": "2048340039",
                    "name": "Sizhe Wang"
                },
                {
                    "authorId": "98379409",
                    "name": "Zhenglong Cui"
                },
                {
                    "authorId": "2057234868",
                    "name": "Hao Sheng"
                }
            ]
        }
    ]
}