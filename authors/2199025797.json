{
    "authorId": "2199025797",
    "papers": [
        {
            "paperId": "1e8fcf495dbc386591fcbab75df75ac41a503859",
            "title": "Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation",
            "abstract": "Spatially resolved transcriptomics brings exciting breakthroughs to single-cell analysis by providing physical locations along with gene expression. However, as a cost of the extremely high spatial resolution, the cellular level spatial transcriptomic data suffer significantly from missing values. While a standard solution is to perform imputation on the missing values, most existing methods either overlook spatial information or only incorporate localized spatial context without the ability to capture long-range spatial information. Using multi-head self-attention mechanisms and positional encoding, transformer models can readily grasp the relationship between tokens and encode location information. In this paper, by treating single cells as spatial tokens, we study how to leverage transformers to facilitate spatial tanscriptomics imputation. In particular, investigate the following two key questions: (1) $\\textit{how to encode spatial information of cells in transformers}$, and (2) $\\textit{ how to train a transformer for transcriptomic imputation}$. By answering these two questions, we present a transformer-based imputation framework, SpaFormer, for cellular-level spatial transcriptomic data. Extensive experiments demonstrate that SpaFormer outperforms existing state-of-the-art imputation algorithms on three large-scale datasets while maintaining superior computational efficiency.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "30580446",
                    "name": "Haifang Wen"
                },
                {
                    "authorId": "2188792890",
                    "name": "Wenzhuo Tang"
                },
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "46496977",
                    "name": "Jiayuan Ding"
                },
                {
                    "authorId": "50268352",
                    "name": "Renming Liu"
                },
                {
                    "authorId": "2199025797",
                    "name": "Feng Shi"
                },
                {
                    "authorId": "2154871510",
                    "name": "Yuying Xie"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "2bcec7b943bfada05bf26332a2652de2d826da32",
            "title": "Efficient Machine Learning on Large-Scale Graphs",
            "abstract": "Machine learning on graph data has become a common area of interest across academia and industry. However, due to the size of real-world industry graphs (hundreds of millions of vertices and billions of edges) and the special architecture of graph neural net- works, it is still a challenge for practitioners and researchers to perform machine learning tasks on large-scale graph data. It typi- cally takes a powerful and expensive GPU machine to train a graph neural network on a million-vertex scale graph, let alone doing deep learning on real enterprise graphs. In this tutorial, we will cover how to develop and run performant graph algorithms and graph neural network models with TigerGraph [3], a massively parallel platform for graph analytics, and its Machine Learning Workbench with PyTorch Geometric [4] and DGL [8] support. Using an NFT transaction dataset [6], we will first investigate transactions using graph algorithms by themselves as methods of graph traversing, clustering, classification, and determining similarities between data. Secondly, we will show how to use those graph-derived features such as PageRank and embeddings to empower traditional machine learning models. Finally, we will demonstrate how to train common graph neural networks with TigerGraph and how to implement novel graph neural network models. Participants will use the Tiger- Graph ML Workbench Cloud to perform graph feature engineering and train their machine learning algorithms during the session.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181311098",
                    "name": "Parker Erickson"
                },
                {
                    "authorId": "2058056841",
                    "name": "Victor E. Lee"
                },
                {
                    "authorId": "2199025797",
                    "name": "Feng Shi"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        }
    ]
}