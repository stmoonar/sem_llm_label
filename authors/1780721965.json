{
    "authorId": "1780721965",
    "papers": [
        {
            "paperId": "1cffb1289d048caecb939ebbbb143ed863d6712f",
            "title": "RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation",
            "abstract": "LLM agents enhanced by tree search algorithms have yielded notable performances in code generation. However, current search algorithms in this domain suffer from low search quality due to several reasons: 1) Ineffective design of the search space for the high-reasoning demands of code generation tasks, 2) Inadequate integration of code feedback with the search algorithm, and 3) Poor handling of negative feedback during the search, leading to reduced search efficiency and quality. To address these challenges, we propose to search for the reasoning process of the code and use the detailed feedback of code execution to refine erroneous thoughts during the search. In this paper, we introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS) algorithm to conduct thought-level searches before generating code, thereby exploring a wider range of strategies. More importantly, we construct verbal feedback from fine-grained code execution feedback to refine erroneous thoughts during the search. This ensures that the search progresses along the correct reasoning paths, thus improving the overall search quality of the tree by leveraging execution feedback. Through extensive experiments, we demonstrate that RethinkMCTS outperforms previous search-based and feedback-based code generation baselines. On the HumanEval dataset, it improves the pass@1 of GPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It effectively conducts more thorough exploration through thought-level searches and enhances the search quality of the entire tree by incorporating rethink operation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260837684",
                    "name": "Qingyao Li"
                },
                {
                    "authorId": "2154454480",
                    "name": "Wei Xia"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2105646417",
                    "name": "Xinyi Dai"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2317033414",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                }
            ]
        },
        {
            "paperId": "49aa7f0db3edd6bbeadd0e72c583d317eb78604c",
            "title": "SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model",
            "abstract": "Knowledge Tracing (KT) aims to determine whether students will respond correctly to the next question, which is a crucial task in intelligent tutoring systems (ITS). In educational KT scenarios, transductive ID-based methods often face severe data sparsity and cold start problems, where interactions between individual students and questions are sparse, and new questions and concepts consistently arrive in the database. In addition, existing KT models only implicitly consider the correlation between concepts and questions, lacking direct modeling of the more complex relationships in the heterogeneous graph of concepts and questions. In this paper, we propose a Structure-aware Inductive Knowledge Tracing model with large language model (dubbed SINKT), which, for the first time, introduces large language models (LLMs) and realizes inductive knowledge tracing. Firstly, SINKT utilizes LLMs to introduce structural relationships between concepts and constructs a heterogeneous graph for concepts and questions. Secondly, by encoding concepts and questions with LLMs, SINKT incorporates semantic information to aid prediction. Finally, SINKT predicts the student's response to the target question by interacting with the student's knowledge state and the question representation. Experiments on four real-world datasets demonstrate that SINKT achieves state-of-the-art performance among 12 existing transductive KT models. Additionally, we explore the performance of SINKT on the inductive KT task and provide insights into various modules.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171109846",
                    "name": "Lingyue Fu"
                },
                {
                    "authorId": "2309177994",
                    "name": "Hao Guan"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2154454480",
                    "name": "Wei Xia"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                }
            ]
        },
        {
            "paperId": "5d12028de85e84cb0081a3f7eda9a4c6fd031eb3",
            "title": "Extracting Essential and Disentangled Knowledge for Recommendation Enhancement",
            "abstract": "Recommender models play a vital role in various industrial scenarios, while often faced with the catastrophic forgetting problem caused by the fast shifting data distribution, e.g., the evolving user interests, click signals fluctuation during sales promotions, etc. To alleviate this problem, a common approach is to reuse knowledge from the historical data. However, preserving the vast and fast-accumulating data is hard, which causes dramatic storage overhead. Memorizing old data through a parametric knowledge base is then proposed, which compresses the vast amount of raw data into model parameters. Despite the flexibility, how to improve the memorization and generalization capabilities of the parametric knowledge base is challenging. In this paper, we propose two constraints to extract Essential and Disentangled Knowledge from past data for rational and generalized recommendation enhancement, which improves the capabilities of the parametric knowledge base without increasing the size of it. The essential principle helps to compress the input into representative vectors that capture the task-relevant information and filter out the noisy information. The disentanglement principle reduces the redundancy of stored information and pushes the knowledge base to focus on capturing the disentangled invariant patterns. These two rules together promote rational compression of information for robust and generalized knowledge representations. Extensive experiments on two datasets justify the effectiveness of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2304517128",
                    "name": "Jizheng Chen"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2238203237",
                    "name": "Menghui Zhu"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2305537473",
                    "name": "Shuai Li"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "63a1f6f6fafef55ac70af3adff5c353bce9a21ff",
            "title": "CodeGRAG: Bridging the Gap between Natural Language and Programming Language via Graphical Retrieval Augmented Generation",
            "abstract": "Utilizing large language models to generate codes has shown promising meaning in software development revolution. Despite the intelligence shown by the general large language models, their specificity in code generation can still be improved due to the syntactic gap and mismatched vocabulary existing among natural language and different programming languages. In this paper, we propose CodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance the performance of LLMs. CodeGRAG builds the graphical view of code blocks based on the control flow and data flow of them to fill the gap between programming languages and natural language, which can facilitate natural language based LLMs for better understanding of code syntax and serve as a bridge among different programming languages. To take the extracted structural knowledge into the foundation models, we propose 1) a hard meta-graph prompt template to transform the challenging graphical representation into informative knowledge for tuning-free models and 2) a soft prompting technique that injects the domain knowledge of programming languages into the model parameters via finetuning the models with the help of a pretrained GNN expert model. Various experiments and ablations are done on four datasets including both the C++ and python languages to validate the hard meta-graph prompt, the soft prompting technique, and the effectiveness of the objectives for pretrained GNN expert. CodeGRAG improves the code generation ability of LLMs and can even offer performance gain for cross-lingual code generation. The implementation is available at https://anonymous.4open.science/r/Code-5970/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2304517128",
                    "name": "Jizheng Chen"
                },
                {
                    "authorId": "2153850099",
                    "name": "Renting Rui"
                },
                {
                    "authorId": "2220553287",
                    "name": "Huacan Chai"
                },
                {
                    "authorId": "2171109846",
                    "name": "Lingyue Fu"
                },
                {
                    "authorId": "2154454480",
                    "name": "Wei Xia"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                }
            ]
        },
        {
            "paperId": "76d87ee792ac0c47cae7bd55d68694e9dfed091d",
            "title": "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation",
            "abstract": "Large language models have been flourishing in the natural language processing (NLP) domain, and their potential for recommendation has been paid much attention to. Despite the intelligence shown by the recommendation-oriented finetuned models, LLMs struggle to fully understand the user behavior patterns due to their innate weakness in interpreting numerical features and the overhead for long context, where the temporal relations among user behaviors, subtle quantitative signals among different ratings, and various side features of items are not well explored. Existing works only fine-tune a sole LLM on given text data without introducing that important information to it, leaving these problems unsolved. In this paper, we propose ELCoRec to Enhance Language understanding with CoPropagation of numerical and categorical features for Recommendation. Concretely, we propose to inject the preference understanding capability into LLM via a GAT expert model where the user preference is better encoded by parallelly propagating the temporal relations, and rating signals as well as various side information of historical items. The parallel propagation mechanism could stabilize heterogeneous features and offer an informative user preference encoding, which is then injected into the language models via soft prompting at the cost of a single token embedding. To further obtain the user's recent interests, we proposed a novel Recent interaction Augmented Prompt (RAP) template. Experiment results over three datasets against strong baselines validate the effectiveness of ELCoRec. The code is available at https://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2304517128",
                    "name": "Jizheng Chen"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                }
            ]
        },
        {
            "paperId": "a350aa4ee586cac36b92cb8d1a8adc070a90ccb6",
            "title": "DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation",
            "abstract": "Recommender systems play important roles in various applications such as e-commerce, social media, etc. Conventional recommendation methods usually model the collaborative signals within the tabular representation space. Despite the personalization modeling and the efficiency, the latent semantic dependencies are omitted. Methods that introduce semantics into recommendation then emerge, injecting knowledge from the semantic representation space where the general language understanding are compressed. However, existing semantic-enhanced recommendation methods focus on aligning the two spaces, during which the representations of the two spaces tend to get close while the unique patterns are discarded and not well explored. In this paper, we propose DisCo to Disentangle the unique patterns from the two representation spaces and Collaborate the two spaces for recommendation enhancement, where both the specificity and the consistency of the two spaces are captured. Concretely, we propose 1) a dual-side attentive network to capture the intra-domain patterns and the inter-domain patterns, 2) a sufficiency constraint to preserve the task-relevant information of each representation space and filter out the noise, and 3) a disentanglement constraint to avoid the model from discarding the unique information. These modules strike a balance between disentanglement and collaboration of the two representation spaces to produce informative pattern vectors, which could serve as extra features and be appended to arbitrary recommendation backbones for enhancement. Experiment results validate the superiority of our method against different models and the compatibility of DisCo over different backbones. Various ablation studies and efficiency analysis are also conducted to justify each model component.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2304517128",
                    "name": "Jizheng Chen"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2056826850",
                    "name": "Yunjia Xi"
                },
                {
                    "authorId": "2283141638",
                    "name": "Hangyu Wang"
                },
                {
                    "authorId": "2105646417",
                    "name": "Xinyi Dai"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                }
            ]
        },
        {
            "paperId": "b60aed687ba9f807da430dd3eabc453ac65d7a2e",
            "title": "Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation",
            "abstract": "Concept recommendation aims to suggest the next concept for learners to study based on their knowledge states and the human knowledge system. While knowledge states can be predicted using knowledge tracing models, previous approaches have not effectively integrated the human knowledge system into the process of designing these educational models. In the era of rapidly evolving Large Language Models (LLMs), many fields have begun using LLMs to generate and encode text, introducing external knowledge. However, integrating LLMs into concept recommendation presents two urgent challenges: 1) How to construct text for concepts that effectively incorporate the human knowledge system? 2) How to adapt non-smooth, anisotropic text encodings effectively for concept recommendation? In this paper, we propose a novel Structure and Knowledge Aware Representation learning framework for concept Recommendation (SKarREC). We leverage factual knowledge from LLMs as well as the precedence and succession relationships between concepts obtained from the knowledge graph to construct textual representations of concepts. Furthermore, we propose a graph-based adapter to adapt anisotropic text embeddings to the concept recommendation task. This adapter is pre-trained through contrastive learning on the knowledge graph to get a smooth and structure-aware concept representation. Then, it's fine-tuned through the recommendation task, forming a text-to-knowledge-to-recommendation adaptation pipeline, which effectively constructs a structure and knowledge-aware concept representation. Our method does a better job than previous adapters in transforming text encodings for application in concept recommendation. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260837684",
                    "name": "Qingyao Li"
                },
                {
                    "authorId": "2154454480",
                    "name": "Wei Xia"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2302426337",
                    "name": "Qiji Zhang"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                }
            ]
        },
        {
            "paperId": "429e6c09eeadf54e2b245b8f2cddfbf157f9da4c",
            "title": "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
            "abstract": "With large language models (LLMs) achieving remarkable breakthroughs in NLP domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on three real-world public datasets to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension. To be highlighted, with only less than 10% training samples, few-shot ReLLa can outperform traditional CTR models that are trained on the entire training set (e.g., DCNv2, DIN, SIM).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "122215415",
                    "name": "Rongjie Shan"
                },
                {
                    "authorId": "2115802321",
                    "name": "Chenxu Zhu"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "92633145",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2232956640",
                    "name": "Shigang Quan"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2156098229",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                }
            ]
        },
        {
            "paperId": "9618ac49822a8d0d9924b6e7d9a0bb0847649fdb",
            "title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
            "abstract": "With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. Evaluating the programming capabilities of LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has numerous downstream applications. In this paper, we propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension, code generation, and code correction abilities of LLMs. Programming comprehension task tests LLMs on multiple-choice exam questions covering conceptual understanding, commonsense reasoning, and multi-hop reasoning. The code generation task evaluates LLMs through completing C++ functions based on provided descriptions and prototypes. The code correction task asks LLMs to fix real-world erroneous code segments with different error messages. We evaluate 12 widely used LLMs, including both general-purpose and specialized models. GPT-4 exhibits the best programming capabilities, achieving approximate accuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to human performance, there is still significant room for improvement in LLM programming. We hope that CodeApex can serve as a reference for evaluating the coding capabilities of LLMs, further promoting their development and growth.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171109846",
                    "name": "Lingyue Fu"
                },
                {
                    "authorId": "2220553287",
                    "name": "Huacan Chai"
                },
                {
                    "authorId": "2237835901",
                    "name": "Shuang Luo"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2237819504",
                    "name": "Weiming Zhang"
                },
                {
                    "authorId": "2238534808",
                    "name": "Longteng Fan"
                },
                {
                    "authorId": "2237803993",
                    "name": "Jiayi Lei"
                },
                {
                    "authorId": "2153850099",
                    "name": "Renting Rui"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2256784173",
                    "name": "Yuchen Fang"
                },
                {
                    "authorId": "2237947577",
                    "name": "Yifan Liu"
                },
                {
                    "authorId": "2237943909",
                    "name": "Jingkuan Wang"
                },
                {
                    "authorId": "2237801991",
                    "name": "Siyuan Qi"
                },
                {
                    "authorId": "1698918106",
                    "name": "Kangning Zhang"
                },
                {
                    "authorId": "2129458791",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                }
            ]
        },
        {
            "paperId": "271f40f4fad4d112e436565e668b79ede690d755",
            "title": "Inductive Relation Prediction Using Analogy Subgraph Embeddings",
            "abstract": "Prevailing methods for relation prediction in heterogeneous graphs including knowledge graphs aim at learning the latent representations (i.e., embeddings) of observed nodes and relations, and are thus limited to the transductive setting where the relation types must be known during training. In this paper, we propose ANalogy SubGraph Embedding Learning (GraphANGEL), a novel relation prediction framework that predicts relations between each node pair by checking whether the subgraphs containing the pair are similar to other subgraphs containing the considered relation. Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relation types and leads to more explainable predictive models. Our model consistently outperforms existing models in terms of heterogeneous graph based recommendation as well as knowledge graph completion. We also empirically demonstrate the capability of our model in generalizing to new relation types while producing explainable heat maps of attention scores across the discovered logics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                },
                {
                    "authorId": "2156098229",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                }
            ]
        }
    ]
}