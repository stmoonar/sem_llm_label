{
    "authorId": "2064751613",
    "papers": [
        {
            "paperId": "4d628954118f8b1d0e8953cf4cda2ad5118ea72f",
            "title": "Practical Knowledge Distillation: Using DNNs to Beat DNNs",
            "abstract": "For tabular data sets, we explore data and model distillation, as well as data denoising. These techniques improve both gradient-boosting models and a specialized DNN architecture. While gradient boosting is known to outperform DNNs on tabular data, we close the gap for datasets with 100K+ rows and give DNNs an advantage on small data sets. We extend these results with input-data distillation and optimized ensembling to help DNN performance match or exceed that of gradient boosting. As a theoretical justification of our practical method, we prove its equivalence to classical cross-entropy knowledge distillation. We also qualitatively explain the superiority of DNN ensembles over XGBoost on small data sets. For an industry end-to-end real-time ML platform with 4M production inferences per second, we develop a model-training workflow based on data sampling that distills ensembles of models into a single gradient-boosting model favored for high-performance real-time inference, without performance loss. Empirical evaluation shows that the proposed combination of methods consistently improves model accuracy over prior best models across several production applications deployed worldwide.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186425819",
                    "name": "Chungman Lee"
                },
                {
                    "authorId": "2209878373",
                    "name": "Pavlos Anastasios Apostolopulos"
                },
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                }
            ]
        },
        {
            "paperId": "bd42a0a55a3a548a86791aab11474081aac934d9",
            "title": "Enhancing quantum computer performance via symmetrization",
            "abstract": "Large quantum computers promise to solve some critical problems not solvable otherwise. However, modern quantum technologies su\ufb00er various imperfections such as control errors and qubit decoherence, inhibiting their potential utility. The overheads of quantum error correction are too great for near-term quantum computers, whereas error-mitigation strategies that address speci\ufb01c device imperfections may lose relevance as devices improve. To enhance the performance of quantum computers with high-quality qubits, we introduce a strategy based on symmetrization and nonlinear aggregation. On a commercial trapped-ion quantum computer, it improves performance of multiple practical algorithms by 100x with no qubit or gate overhead.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "13747734",
                    "name": "A. Maksymov"
                },
                {
                    "authorId": "48493395",
                    "name": "Jason H. V. Nguyen"
                },
                {
                    "authorId": "2249019",
                    "name": "Y. Nam"
                },
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                }
            ]
        },
        {
            "paperId": "c980d97d78ba913312d1177e78b43aa0946bf992",
            "title": "Efficient Multi-stage Inference on Tabular Data",
            "abstract": "Many ML applications and products train on medium amounts of input data but get bottlenecked in real-time inference. When implementing ML systems, conventional wisdom favors segregating ML code into services queried by product code via Remote Procedure Call (RPC) APIs. This approach clarifies the overall software architecture and simplifies product code by abstracting away ML internals. However, the separation adds network latency and entails additional CPU overhead. Hence, we simplify inference algorithms and embed them into the product code to reduce network communication. For public datasets and a high-performance real-time platform that deals with tabular data, we show that over half of the inputs are often amenable to such optimization, while the remainder can be handled by the original model. By applying our optimization with AutoML to both training and inference, we reduce inference latency by 1.3x, CPU resources by 30%, and network communication between application front-end and ML back-end by about 50% for a commercial end-to-end ML platform that serves millions of real-time decisions per second.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212010051",
                    "name": "Daniel S Johnson"
                },
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                }
            ]
        },
        {
            "paperId": "fa2d1d63f25e4885c318d82dcb86125d08b675a3",
            "title": "RusTitW: Russian Language Text Dataset for Visual Text in-the-Wild Recognition",
            "abstract": "Information surrounds people in modern life. Text is a very efficient type of information that people use for communication for centuries. However, automated text-in-the-wild recognition remains a challenging problem. The major limitation for a DL system is the lack of training data. For the competitive performance, training set must contain many samples that replicate the real-world cases. While there are many high-quality datasets for English text recognition; there are no available datasets for Russian language. In this paper, we present a large-scale human-labeled dataset for Russian text recognition in-the-wild. We also publish a synthetic dataset and code to reproduce the generation process",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                },
                {
                    "authorId": "1854149647",
                    "name": "S. Nesteruk"
                },
                {
                    "authorId": "2155649380",
                    "name": "Andrey Kuznetsov"
                },
                {
                    "authorId": "40455137",
                    "name": "Denis Dimitrov"
                }
            ]
        },
        {
            "paperId": "282f6c2c482ef29966da15a821af1d56c4b44793",
            "title": "Federated Calibration and Evaluation of Binary Classifiers",
            "abstract": "\n We address two major obstacles to practical deployment of AI-based models on distributed private data. Whether a model was trained by a federation of cooperating clients or trained centrally, (1) the output scores must be calibrated, and (2) performance metrics must be evaluated --- all without assembling labels in one place. In particular, we show how to perform calibration and compute the standard metrics of precision, recall, accuracy and ROC-AUC in the federated setting under three privacy models (\n i\n ) secure aggregation, (\n ii\n ) distributed differential privacy, (\n iii\n ) local differential privacy. Our theorems and experiments clarify tradeoffs between privacy, accuracy, and data efficiency. They also help decide if a given application has sufficient data to support federated calibration and evaluation.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                }
            ]
        },
        {
            "paperId": "8b738ac15227358a025b276d5d30be1ff36159d9",
            "title": "Data-Driven Mitigation of Adversarial Text Perturbation",
            "abstract": "Social networks have become an indispensable part of our lives, with billions of people producing ever-increasing amounts of text. At such scales, content policies and their enforcement become paramount. To automate moderation, questionable content is detected by Natural Language Processing (NLP) classifiers. However, high-performance classifiers are hampered by misspellings and adversarial text perturbations. In this paper, we classify intentional and unintentional adversarial text perturbation into ten types and propose a deobfuscation pipeline to make NLP models robust to such perturbations. We propose Continuous Word2Vec (CW2V), our data-driven method to learn word embeddings that ensures that perturbations of words have embeddings similar to those of the original words. We show that CW2V embeddings are generally more robust to text perturbations than embeddings based on character ngrams. Our robust classification pipeline combines deobfuscation and classification, using proposed defense methods and word embeddings to classify whether Facebook posts are requesting engagement such as likes. Our pipeline results in engagement bait classification that goes from 0.70 to 0.67 AUC with adversarial text perturbation, while character ngram-based word embedding methods result in downstream classification that goes from 0.76 to 0.64.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49550275",
                    "name": "Rasika Bhalerao"
                },
                {
                    "authorId": "1414046108",
                    "name": "Mohammad Al-Rubaie"
                },
                {
                    "authorId": "2064709671",
                    "name": "Anand Bhaskar"
                },
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                }
            ]
        },
        {
            "paperId": "10f532fe0b57fa0d999ed14e16f8c23dc3fc332b",
            "title": "Regular Expressions for Fast-response COVID-19 Text Classification",
            "abstract": "Text classifiers are at the core of many NLP applications and use a variety of algorithmic approaches and software. This paper introduces infrastructure and methodologies for text classifiers based on large-scale regular expressions. In particular, we describe how Facebook determines if a given piece of text - anything from a hashtag to a post - belongs to a narrow topic such as COVID-19. To fully define a topic and evaluate classifier performance we employ human-guided iterations of keyword discovery, but do not require labeled data. For COVID-19, we build two sets of regular expressions: (1) for 66 languages, with 99% precision and recall >50%, (2) for the 11 most common languages, with precision >90% and recall >90%. Regular expressions enable low-latency queries from multiple platforms. Response to challenges like COVID-19 is fast and so are revisions. Comparisons to a DNN classifier show explainable results, higher precision and recall, and less overfitting. Our learnings can be applied to other narrow-topic classifiers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                },
                {
                    "authorId": "2108387774",
                    "name": "Jacqueline Liu"
                },
                {
                    "authorId": "2064009253",
                    "name": "Adam Vagner"
                }
            ]
        },
        {
            "paperId": "5c35b96afeab32a8f40a0bbc073bb5af88226c5e",
            "title": "Picasso: Model-free Feature Visualization",
            "abstract": "Today, Machine Learning (ML) applications can have access to tens of thousands of features. With such feature sets, efficiently browsing and curating subsets of most relevant features is a challenge. In this paper, we present a novel approach to visualize up to several thousands of features in a single image. The image not only shows information on individual features, but also expresses feature interactions via the relative positioning of features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144023413",
                    "name": "Binh Vu"
                },
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                }
            ]
        },
        {
            "paperId": "860d6200b957e36e4dbe1878da44474682ac52f9",
            "title": "Prioritizing Original News on Facebook",
            "abstract": "This work outlines how we prioritize original news, a critical indicator of news quality. By examining the landscape and lifecycle of news posts on our social media platform, we identify challenges of building and deploying an originality score. We pursue an approach based on normalized PageRank values and three-step clustering, and refresh the score on an hourly basis to capture the dynamics of online news. We describe a near real-time system architecture, evaluate our methodology, and deploy it to production. Our empirical results validate individual components and show that prioritizing original news increases user engagement with news and improves proprietary cumulative metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20567778",
                    "name": "Xiuyan Ni"
                },
                {
                    "authorId": "2631167",
                    "name": "Shujian Bu"
                },
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                }
            ]
        },
        {
            "paperId": "c89a572ae5a1ecbf903bb57e300ffc6a63dde53f",
            "title": "SkoltechNLP at SemEval-2021 Task 5: Leveraging Sentence-level Pre-training for Toxic Span Detection",
            "abstract": "This work describes the participation of the Skoltech NLP group team (Sk) in the Toxic Spans Detection task at SemEval-2021. The goal of the task is to identify the most toxic fragments of a given sentence, which is a binary sequence tagging problem. We show that fine-tuning a RoBERTa model for this problem is a strong baseline. This baseline can be further improved by pre-training the RoBERTa model on a large dataset labeled for toxicity at the sentence level. While our solution scored among the top 20% participating models, it is only 2 points below the best result. This suggests the viability of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2097711561",
                    "name": "David Dale"
                },
                {
                    "authorId": "2064751613",
                    "name": "Igor L. Markov"
                },
                {
                    "authorId": "145089549",
                    "name": "V. Logacheva"
                },
                {
                    "authorId": "2062842110",
                    "name": "Olga Kozlova"
                },
                {
                    "authorId": "2094581992",
                    "name": "Nikita Semenov"
                },
                {
                    "authorId": "2027664756",
                    "name": "A. Panchenko"
                }
            ]
        }
    ]
}