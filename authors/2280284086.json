{
    "authorId": "2280284086",
    "papers": [
        {
            "paperId": "0ae3d08bff81406b8806f6958e2bf9140593a6b3",
            "title": "Physics-guided Active Sample Reweighting for Urban Flow Prediction",
            "abstract": "Urban flow prediction is a spatio-temporal modeling task that estimates the throughput of transportation services like buses, taxis, and ride-sharing, where data-driven models have become the most popular solution in the past decade. Meanwhile, the implicitly learned mapping between historical observations to the prediction targets tend to over-simplify the dynamics of real-world urban flows, leading to suboptimal predictions. Some recent spatio-temporal prediction solutions bring remedies with the notion of physics-guided machine learning (PGML), which describes spatio-temporal data with nuanced and principled physics laws, thus enhancing both the prediction accuracy and interpretability. However, these spatio-temporal PGML methods are built upon a strong assumption that the observed data fully conforms to the differential equations that define the physical system, which can quickly become ill-posed in urban flow prediction tasks. The observed urban flow data, especially when sliced into time-dependent snapshots to facilitate predictions, is typically incomplete and sparse, and prone to inherent noise incurred in the collection process. As a result, such physical inconsistency between the data and PGML model significantly limits the predictive power and robustness of the solution. Moreover, due to the interval-based predictions and intermittent nature of data filing in many transportation services, the instantaneous dynamics of urban flows can hardly be captured, rendering differential equation-based continuous modeling a loose fit for this setting. To overcome the challenges, we develop a discretized physics-guided network (PN), and propose a data-aware framework Physics-guided Active Sample Reweighting (P-GASR) to enhance PN. Experimental results in four real-world datasets demonstrate that our method achieves state-of-the-art performance with a demonstrable improvement in robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281470021",
                    "name": "Wei Jiang"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "1849611",
                    "name": "Guanhua Ye"
                },
                {
                    "authorId": "2280283715",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2281001972",
                    "name": "Lizhen Cui"
                },
                {
                    "authorId": "2129485845",
                    "name": "Zi-Liang Huang"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "334ebaae0a97fd7af3cd8ec0a9b02fb4fbba2f42",
            "title": "Rethinking and Accelerating Graph Condensation: A Training-Free Approach with Class Partition",
            "abstract": "The increasing prevalence of large-scale graphs poses a significant challenge for graph neural network training, attributed to their substantial computational requirements. In response, graph condensation (GC) emerges as a promising data-centric solution aiming to substitute the large graph with a small yet informative condensed graph to facilitate data-efficient GNN training. However, existing GC methods suffer from intricate optimization processes, necessitating excessive computing resources. In this paper, we revisit existing GC optimization strategies and identify two pervasive issues: 1. various GC optimization strategies converge to class-level node feature matching between the original and condensed graphs, making the optimization target coarse-grained despite the complex computations; 2. to bridge the original and condensed graphs, existing GC methods rely on a Siamese graph network architecture that requires time-consuming bi-level optimization with iterative gradient computations. To overcome these issues, we propose a training-free GC framework termed Class-partitioned Graph Condensation (CGC), which refines the node feature matching from the class-to-class paradigm into a novel class-to-node paradigm. Remarkably, this refinement also simplifies the GC optimization as a class partition problem, which can be efficiently solved by any clustering methods. Moreover, CGC incorporates a pre-defined graph structure to enable a closed-form solution for condensed node features, eliminating the back-and-forth gradient descent in existing GC approaches without sacrificing accuracy. Extensive experiments demonstrate that CGC achieves state-of-the-art performance with a more efficient condensation process. For instance, compared with the seminal GC method (i.e., GCond), CGC condenses the largest Reddit graph within 10 seconds, achieving a 2,680X speedup and a 1.4% accuracy increase.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2280283715",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "1849611",
                    "name": "Guanhua Ye"
                },
                {
                    "authorId": "144133815",
                    "name": "Q. Nguyen"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "575d626c47d1b6b74246d7ce168a64f6bcafbe4c",
            "title": "Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations",
            "abstract": "The rapid expansion of Location-Based Social Networks (LBSNs) has highlighted the importance of effective next Point-of-Interest (POI) recommendations, which leverage historical check-in data to predict users' next POIs to visit. Traditional centralized deep neural networks (DNNs) offer impressive POI recommendation performance but face challenges due to privacy concerns and limited timeliness. In response, on-device POI recommendations have been introduced, utilizing federated learning (FL) and decentralized approaches to ensure privacy and recommendation timeliness. However, these methods often suffer from computational strain on devices and struggle to adapt to new users and regions. This paper introduces a novel collaborative learning framework, Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations (DCPR), leveraging the diffusion model known for its success across various domains. DCPR operates with a cloud-edge-device architecture to offer region-specific and highly personalized POI recommendations while reducing on-device computational burdens. DCPR minimizes on-device computational demands through a unique blend of global and local learning processes. Our evaluation with two real-world datasets demonstrates DCPR's superior performance in recommendation accuracy, efficiency, and adaptability to new users and regions, marking a significant step forward in on-device POI recommendation technology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161605203",
                    "name": "Jing Long"
                },
                {
                    "authorId": "1849611",
                    "name": "Guanhua Ye"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "92ea98ca67767c05e9a32ed4a9e0937257c9b9cf",
            "title": "Graph Condensation: A Survey",
            "abstract": "The rapid growth of graph data poses significant challenges in storage, transmission, and particularly the training of graph neural networks (GNNs). To address these challenges, graph condensation (GC) has emerged as an innovative solution. GC focuses on synthesizing a compact yet highly representative graph, enabling GNNs trained on it to achieve performance comparable to those trained on the original large graph. The notable efficacy of GC and its broad prospects have garnered significant attention and spurred extensive research. This survey paper provides an up-to-date and systematic overview of GC, organizing existing research into five categories aligned with critical GC evaluation criteria: effectiveness, generalization, efficiency, fairness, and robustness. To facilitate an in-depth and comprehensive understanding of GC, this paper examines various methods under each category and thoroughly discusses two essential components within GC: optimization strategies and condensed graph generation. We also empirically compare and analyze representative GC methods with diverse optimization strategies based on the five proposed GC evaluation criteria. Finally, we explore the applications of GC in various fields, outline the related open-source libraries, and highlight the present challenges and novel insights, with the aim of promoting advancements in future research. The related resources can be found at https://github.com/XYGaoG/Graph-Condensation-Papers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2281470021",
                    "name": "Wei Jiang"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2280283715",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "bd603882603101b9e7cf48a09caf93322dbcb1cf",
            "title": "Challenging Low Homophily in Social Recommendation",
            "abstract": "Social relations are leveraged to tackle the sparsity issue of user-item interaction data in recommendation under the assumption of social homophily. However, social recommendation paradigms predominantly focus on homophily based on user preferences. While social information can enhance recommendations, its alignment with user preferences is not guaranteed, thereby posing the risk of introducing informational redundancy. We empirically discover that social graphs in real recommendation data exhibit low preference-aware homophily, which limits the effect of social recommendation models. To comprehensively extract preference-aware homophily information latent in the social graph, we propose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric framework for enhancing existing graph-based social recommendation models. We adopt Graph Rewiring technique to capture and add highly homophilic social relations, and cut low homophilic (or heterophilic) relations. To better refine the user representations from reliable social relations, we integrate a contrastive learning method into the training of SHaRe, aiming to calibrate the user representations for enhancing the result of Graph Rewiring. Experiments on real-world datasets show that the proposed framework not only exhibits enhanced performances across varying homophily ratios but also improves the performance of existing state-of-the-art (SOTA) social recommendation models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281470021",
                    "name": "Wei Jiang"
                },
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2281788009",
                    "name": "Guandong Xu"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "e1d8244289468fa3350fa9b64ad466599c746ca2",
            "title": "Graph Condensation for Open-World Graph Learning",
            "abstract": "The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications. To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance. Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution. This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes. In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated. Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation. This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it. Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2280283715",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2303406950",
                    "name": "Yayong Li"
                },
                {
                    "authorId": "2302792188",
                    "name": "Xiangguo Sun"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "f20d934631943945f95f82d00a5868cb22f0aabb",
            "title": "RobGC: Towards Robust Graph Condensation",
            "abstract": "Graph neural networks (GNNs) have attracted widespread attention for their impressive capability of graph representation learning. However, the increasing prevalence of large-scale graphs presents a significant challenge for GNN training due to their computational demands, limiting the applicability of GNNs in various scenarios. In response to this challenge, graph condensation (GC) is proposed as a promising acceleration solution, focusing on generating an informative compact graph that enables efficient training of GNNs while retaining performance. Despite the potential to accelerate GNN training, existing GC methods overlook the quality of large training graphs during both the training and inference stages. They indiscriminately emulate the training graph distributions, making the condensed graphs susceptible to noises within the training graph and significantly impeding the application of GC in intricate real-world scenarios. To address this issue, we propose robust graph condensation (RobGC), a plug-and-play approach for GC to extend the robustness and applicability of condensed graphs in noisy graph structure environments. Specifically, RobGC leverages the condensed graph as a feedback signal to guide the denoising process on the original training graph. A label propagation-based alternating optimization strategy is in place for the condensation and denoising processes, contributing to the mutual purification of the condensed graph and training graph. Additionally, as a GC method designed for inductive graph inference, RobGC facilitates test-time graph denoising by leveraging the noise-free condensed graph to calibrate the structure of the test graph. Extensive experiments show that RobGC is compatible with various GC methods, significantly boosting their robustness under different types and levels of graph structural noises.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "1849611",
                    "name": "Guanhua Ye"
                },
                {
                    "authorId": "2280283715",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2257282067",
                    "name": "Bin Cui"
                }
            ]
        },
        {
            "paperId": "f9bc159a52dd84842d7048cdb66159d5a99ac1e6",
            "title": "A Decentralized Collaborative Learning Framework Across Heterogeneous Devices for Personalized Predictive Analytics",
            "abstract": "Personalized predictive analytics are changing the way people live, such as recommendation, ehealth, and chatbots. With the prominent development of edge devices, predictive models based on deep neural networks (DNNs) can be conveniently hosted on-device to deliver accurate results, which also provides better privacy and timeliness. Due to the inevitable data sparsity on a single device, collaborative learning (e.g., federated learning) is regarded as an integral approach for learning performant personalized deep models. However, traditional decentralized learning protocols communicate the knowledge by sharing the model parameters (i.e., weights or gradients). Consequently, such methods strictly require homogeneity of all participant models, thus being unable to handle applications where heterogeneous deep models with different architectures are built for the same task but adapt to a variety of device capacities. Furthermore, to support model personalization, a common practice is to pick a curated subset of similar users (i.e., neighbors) for knowledge aggregation, but the neighbor selection process potentially threatens user privacy with the need for exchanging personal information or model parameters that are highly sensitive. In this paper, we propose a Similarity-basedDecentralized KnowledgeDistillation (SD-Dist) framework for collaboratively learning heterogeneous deep models on decentralized devices. By introducing a preloaded reference dataset, SD-Dist enables all participant devices to identify similar users and distil knowledge from themwithout any assumptions on a fixed model architecture. In addition, none of these operations will reveal any sensitive information like personal data and model parameters. Extensive experimental results on three real-life datasets show that SD-Dist can achieve competitive performance with less compute resources, while ensuring model heterogeneity and privacy. As revealed in our experiments, our framework also enhances the resultant models\u2019 robustness when users\u2019 data is sparse and diverse.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1849611",
                    "name": "Guanhua Ye"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                }
            ]
        }
    ]
}