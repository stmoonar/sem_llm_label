{
    "authorId": "145096384",
    "papers": [
        {
            "paperId": "0c827c52d952ca82eb659066e25ca8aa7a3a60b3",
            "title": "How does AI chat change search behaviors?",
            "abstract": "Generative AI tools such as chatGPT are poised to change the way people engage with online information. Recently, Microsoft announced their\"new Bing\"search system which incorporates chat and generative AI technology from OpenAI. Google has announced plans to deploy search interfaces that incorporate similar types of technology. These new technologies will transform how people can search for information. The research presented here is an early investigation into how people make use of a generative AI chat system (referred to simply as chat from here on) as part of a search process, and how the incorporation of chat systems with existing search tools may effect users search behaviors and strategies. We report on an exploratory user study with 10 participants who used a combined Chat+Search system that utilized the OpenAI GPT-3.5 API and the Bing Web Search v5 API. Participants completed three search tasks. In this pre-print paper of preliminary results, we report on ways that users integrated AI chat into their search process, things they liked and disliked about the chat system, their trust in the chat responses, and their mental models of how the chat system generated responses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47504191",
                    "name": "Robert G. Capra"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                }
            ]
        },
        {
            "paperId": "162dc30093e1b74944810754830d9b024dc6d398",
            "title": "Understanding Procedural Search Tasks \u201cin the Wild\u201d",
            "abstract": "People often search online for procedural (i.e., \u201chow-to\u201d) knowledge. A procedural search task might involve a do-it-yourself project, cooking a dish, fixing a problem, or learning a new skill. Prior research has studied procedural search tasks from different perspectives: estimating the frequency of procedural searches online, understanding how people acquire procedural knowledge in specific contexts, and developing tools to support procedural search. Less research has aimed at deeply understanding procedural search tasks \u201cin the wild\u201d. To bridge this gap, we conducted a survey (N = 128) on Amazon Mechanical Turk. Participants were asked to recall a recent procedural task for which they searched online. Participants were asked open-ended questions about the task itself and their unique situation (e.g., constraints and needs). Additionally, participants provided webpages they found useful in their searches and described the characteristics of the page that made it useful. Finally, they provided useful pieces of information from each selected page and explained what they gained from the information. Using an inductive coding approach, we analyzed participants\u2019 responses to gain insights about: (1) procedural task characteristics, (2) goals, (3) constraints, (4) contextual factors, (5) relevance criteria, and (6) gains obtained from useful information. Based on our results, we discuss important implications for future research and system design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30768993",
                    "name": "Bogeum Choi"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                },
                {
                    "authorId": "47504191",
                    "name": "Robert G. Capra"
                }
            ]
        },
        {
            "paperId": "3c81cbb3c51f280e30e5708ffc5f37211707205d",
            "title": "Understanding the Cognitive Influences of Interpretability Features on How Users Scrutinize Machine-Predicted Categories",
            "abstract": "The goal of interpretable machine learning (ML) is to design tools and visualizations to help users scrutinize a system\u2019s predictions. Prior studies have mostly employed quantitative methods to investigate the effects of specific tools/visualizations on outcomes related to objective performance\u2014a human\u2019s ability to correctly agree or disagree with the system\u2014and subjective perceptions of the system. Few studies have employed qualitative methods to investigate how and why specific tools/visualizations influence performance, perceptions, and behaviors. We report on a lab study (N = 30) that investigated the influences of two interpretability features: confidence values and sentence highlighting. Participants judged whether medical articles belong to a predicted medical topic and were exposed to two interface conditions\u2014one with and one without interpretability features. We investigate the effects of our interpretability features on participants\u2019 performance and perceptions. Additionally, we report on a qualitative analysis of participants\u2019 responses during an exit interview. Specifically, we report on how our interpretability features impacted different cognitive activities that participants engaged with during the task\u2014reading, learning, and decision making. We also describe ways in which the interpretability features introduced challenges and sometimes led participants to make mistakes. Insights gained from our results point to future directions for interpretable ML research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "102260290",
                    "name": "Jiaming Qu"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                },
                {
                    "authorId": "3189266",
                    "name": "Yue Wang"
                }
            ]
        },
        {
            "paperId": "f54fbe63b12219f46e39fef9733a5ed507cf9a78",
            "title": "Why and When: Understanding System Initiative during Conversational Collaborative Search",
            "abstract": "In the last decade, conversational search has attracted considerable attention. However, most research has focused on systems that can support a \\emph{single} searcher. In this paper, we explore how systems can support \\emph{multiple} searchers collaborating over an instant messaging platform (i.e., Slack). We present a ``Wizard of Oz'' study in which 27 participant pairs collaborated on three information-seeking tasks over Slack. Participants were unable to search on their own and had to gather information by interacting with a \\emph{searchbot} directly from the Slack channel. The role of the searchbot was played by a reference librarian. Conversational search systems must be capable of engaging in \\emph{mixed-initiative} interaction by taking and relinquishing control of the conversation to fulfill different objectives. Discourse analysis research suggests that conversational agents can take \\emph{two} levels of initiative: dialog- and task-level initiative. Agents take dialog-level initiative to establish mutual belief between agents and task-level initiative to influence the goals of the other agents. During the study, participants were exposed to three experimental conditions in which the searchbot could take different levels of initiative: (1) no initiative, (2) only dialog-level initiative, and (3) both dialog- and task-level initiative. In this paper, we focus on understanding the Wizard's actions. Specifically, we focus on understanding the Wizard's motivations for taking initiative and their rationale for the timing of each intervention. To gain insights about the Wizard's actions, we conducted a stimulated recall interview with the Wizard. We present findings from a qualitative analysis of this interview data and discuss implications for designing conversational search systems to support collaborative search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3176237",
                    "name": "Sandeep Avula"
                },
                {
                    "authorId": "30768993",
                    "name": "Bogeum Choi"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                }
            ]
        },
        {
            "paperId": "4f329aa7610e0e0f71870a09f0ecf9fbcea93f45",
            "title": "The Effects of System Initiative during Conversational Collaborative Search",
            "abstract": "Our research in this paper lies at the intersection of collaborative and conversational search. We report on a Wizard of Oz lab study in which 27 pairs of participants collaborated on search tasks over the Slack messaging platform. To complete tasks, pairs of collaborators interacted with a so-called searchbot with conversational capabilities. The role of the searchbot was played by a reference librarian. It is widely accepted that conversational search systems should be able to engage in mixed-initiative interaction ---take and relinquish control of a multi-agent conversation as appropriate. Research in discourse analysis differentiates between dialog- and task-level initiative. Taking dialog-level initiative involves leading a conversation for the sole purpose of establishing mutual belief between agents. Conversely, taking task-level initiative involves leading a conversation with the intent to influence the goals of the other agent(s). Participants in our study experienced three searchbot conditions, which varied based on the level of initiative the human searchbot was able to take: (1) no initiative, (2) only dialog-level initiative, and (3) both dialog- and task-level initiative. We investigate the effects of the searchbot condition on six different types of outcomes: (RQ1) perceptions of the searchbot's utility, (RQ2) perceptions of workload, (RQ3) perceptions of the collaboration, (RQ4) patterns of communication and collaboration, and perceived (RQ5) benefits and (RQ6) challenges from engaging with the searchbot.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3176237",
                    "name": "Sandeep Avula"
                },
                {
                    "authorId": "30768993",
                    "name": "Bogeum Choi"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                }
            ]
        },
        {
            "paperId": "5afccb8ee0eb96b7660a2d5a63cb0589bbf2d972",
            "title": "Learner, Assignment, and Domain: Contextualizing Search for Comprehension",
            "abstract": "Modern search systems are largely designed and optimized for simple navigational or fact-finding tasks, with little support for complex tasks involving comprehension and learning. In response, the search-as-learning research community has undertaken a wide range of research questions focused on understanding how various types of learning outcomes are affected by searcher characteristics, the search task, and the search system. Typically, these views embed learning within a search system. In this paper we take a different view, embedding search within a framework for an end-to-end learning system designed to support learning in a formal educational context. Our central goal is to motivate research questions aligned to advance progress on techniques for active support of comprehension and formal learning. Thus we intentionally set aside goals for informal and surface learning. We argue that to be effective, such a search-centric learning system must model four key components: individual students (searcher factors), the educational domain (topic factors), academic assignments (task factors), and progress toward learning goals (the objective function of the end-to-end system). In modeling these components, our hypothetical system makes inferences about students\u2019 learning histories, knowledge states, comprehension, and the utilities of different types of information resources. We present examples of possible techniques and data sources for each model. We also introduce the novel concept of leveraging school assignments as rich task context. Our intention is not to propose a functional system, but to frame search-as-learning in the context of comprehension and to inspire research questions arising from an end-to-end view of this important research domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110281905",
                    "name": "Catherine L. Smith"
                },
                {
                    "authorId": "28058736",
                    "name": "Kelsey Urgo"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                }
            ]
        },
        {
            "paperId": "919ff1c5002b6aad4c465752fe48bf2b86e53a3c",
            "title": "Capturing Self-Regulated Learning During Search",
            "abstract": "Researchers in the learning sciences have demonstrated the benefits of effective self-regulated learning (SRL) in improving learning outcomes. The search-as-learning community aims to improve learning outcomes during search, but offers limited research exploring the impact of SRL on learning during search. Current limited research in search-as-learning explores only perceptions of SRL processes after the search process [1]. Results from such analyses are limited in that SRL is a dynamic, active process and participant perceptions of SRL can be unreliable [2, 3]. In this paper, we propose the implementation of an SRL coding framework to capture SRL processes as they unfold throughout a search session. Additionally, we offer several implications for future work using the proposed methodology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28058736",
                    "name": "Kelsey Urgo"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                }
            ]
        },
        {
            "paperId": "abecd1b93a90c939217f18ed89cdda5eea0858e7",
            "title": "Procedural Knowledge Search by Intelligence Analysts",
            "abstract": "Prior studies have explored the information-seeking practices of specific professional communities, including lawyers, physicians, engineers, recruiters, and government workers. In this research, we investigate the information-seeking practices of intelligence analysts (IAs) employed by a U.S. government agency. Specifically, we focus on the needs, practices, and challenges related to IAs searching for procedural knowledge using an internal system called the Tradecraft Hub (TC Hub). The TC Hub is a searchable repository of procedural knowledge documents written by agency employees. Procedural knowledge (as opposed to factual and conceptual knowledge) includes knowledge about step-by-step procedures, techniques, methods, tools, technologies, and skills, and is inherently task-oriented. We report on a survey study involving 22 IAs who routinely use the TC Hub. Our survey was designed to address four research questions. In RQ1, we investigate the types of work-related objectives that motivate IAs to search the TC Hub. In RQ2, we investigate the types of information IAs seek when they search the TC Hub. In RQ3, we investigate important relevance criteria used by IAs when judging the usefulness of information. Finally, in RQ4, we investigate the challenges faced by IAs when searching the TC Hub. Based on our findings, we discuss implications for improving and extending searchable knowledge base systems such as the TC Hub that exist in many organizations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30768993",
                    "name": "Bogeum Choi"
                },
                {
                    "authorId": "104958887",
                    "name": "Sarah Casteel"
                },
                {
                    "authorId": "47504191",
                    "name": "Robert G. Capra"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                }
            ]
        },
        {
            "paperId": "dd7d56f5f51baec1196adc6e3f18b9cd02c41c53",
            "title": "Understanding the \u201cPathway\u201d Towards a Searcher\u2019s Learning Objective",
            "abstract": "Search systems are often used to support learning-oriented goals. This trend has given rise to the \u201csearch-as-learning\u201d movement, which proposes that search systems should be designed to support learning. To this end, an important research question is: How does a searcher\u2019s type of learning objective (LO) influence their trajectory (or pathway) toward that objective? We report on a lab study (N = 36) in which participants gathered information to meet a specific type of LO. To characterize LOs and pathways, we leveraged Anderson and Krathwohl\u2019s (A&K\u2019s) taxonomy [3]. A&K\u2019s taxonomy situates LOs at the intersection of two orthogonal dimensions: (1) cognitive process (CP) (remember, understand, apply, analyze, evaluate, and create) and (2) knowledge type (factual, conceptual, procedural, and metacognitive knowledge). Participants completed learning-oriented search tasks that varied along three CPs (apply, evaluate, and create) and three knowledge types (factual, conceptual, and procedural knowledge). A pathway is defined as a sequence of learning instances (e.g., subgoals) that were also each classified into cells from A&K\u2019s taxonomy. Our study used a think-aloud protocol, and pathways were generated through a qualitative analysis of participants\u2019 think-aloud comments and recorded screen activities. We investigate three research questions. First, in RQ1, we study the impact of the LO on pathway characteristics (e.g., pathway length). Second, in RQ2, we study the impact of the LO on the types of A&K cells traversed along the pathway. Third, in RQ3, we study common and uncommon transitions between A&K cells along pathways conditioned on the knowledge type of the objective. We discuss implications of our results for designing search systems to support learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28058736",
                    "name": "Kelsey Urgo"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                }
            ]
        },
        {
            "paperId": "fe32e3c9bb80e546d744982cec32cde673ec9182",
            "title": "The Influences of a Knowledge Representation Tool on Searchers with Varying Cognitive Abilities",
            "abstract": "While current systems are effective in helping searchers resolve simple information needs (e.g., fact-finding), they provide less support for searchers working on complex information-seeking tasks. Complex search tasks involve a wide range of (meta)cognitive activities, including goal-setting, organizing information, drawing inferences, monitoring progress, and revising mental models and search strategies. We report on a lab study (N = 32) that investigated the influences of a knowledge representation tool called the OrgBox, developed to support searchers with complex tasks. The OrgBox tool was integrated into a custom-built search system and allowed study participants to drag-and-drop textual passages into the tool, organize passages into logical groupings called \u201cboxes\u201d, and make notes on passages and boxes. The OrgBox was compared to a baseline tool (called the Bookmark) that allowed participants to save textual passages, but not organize them nor make notes. Knowledge representation tools such as the OrgBox may provide special benefits for users with different cognitive profiles. In this article, we explore two cognitive abilities: (1) working memory (WM) capacity and (2) switching (SW) ability. Participants in the study were asked to gather information on a complex subject and produce an outline for a hypothetical research article. We investigate the influences of the tool (OrgBox vs. Bookmark) and the participant\u2019s working memory capacity and switching ability on three types of outcomes: (RQ1) search behaviors, (RQ2) post-task perceptions, and (RQ3) the quality of outlines produces by participants.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30768993",
                    "name": "Bogeum Choi"
                },
                {
                    "authorId": "145096384",
                    "name": "Jaime Arguello"
                },
                {
                    "authorId": "47504191",
                    "name": "Robert G. Capra"
                },
                {
                    "authorId": "1562164202",
                    "name": "Austin R. Ward"
                }
            ]
        }
    ]
}