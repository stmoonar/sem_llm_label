{
    "authorId": "22214396",
    "papers": [
        {
            "paperId": "0cfa9e7d53c890488b8501e543e27a6ee7d5014c",
            "title": "A Deep Learning Approach for Selective Relevance Feedback",
            "abstract": "Pseudo-relevance feedback (PRF) can enhance average retrieval effectiveness over a sufficiently large number of queries. However, PRF often introduces a drift into the original information need, thus hurting the retrieval effectiveness of several queries. While a selective application of PRF can potentially alleviate this issue, previous approaches have largely relied on unsupervised or feature-based learning to determine whether a query should be expanded. In contrast, we revisit the problem of selective PRF from a deep learning perspective, presenting a model that is entirely data-driven and trained in an end-to-end manner. The proposed model leverages a transformer-based bi-encoder architecture. Additionally, to further improve retrieval effectiveness with this selective PRF approach, we make use of the model's confidence estimates to combine the information from the original and expanded queries. In our experiments, we apply this selective feedback on a number of different combinations of ranking and feedback models, and show that our proposed approach consistently improves retrieval effectiveness for both sparse and dense ranking models, with the feedback models being either sparse, dense or generative.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52511273",
                    "name": "S. Datta"
                },
                {
                    "authorId": "2280136919",
                    "name": "Debasis Ganguly"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "50418148",
                    "name": "Derek Greene"
                }
            ]
        },
        {
            "paperId": "6dbd1291394fab3f882715e38871810339b62170",
            "title": "Genetic Approach to Mitigate Hallucination in Generative IR",
            "abstract": "Generative language models hallucinate. That is, at times, they generate factually flawed responses. These inaccuracies are particularly insidious because the responses are fluent and well-articulated. We focus on the task of Grounded Answer Generation (part of Generative IR), which aims to produce direct answers to a user's question based on results retrieved from a search engine. We address hallucination by adapting an existing genetic generation approach with a new 'balanced fitness function' consisting of a cross-encoder model for relevance and an n-gram overlap metric to promote grounding. Our balanced fitness function approach quadruples the grounded answer generation accuracy while maintaining high relevance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50359017",
                    "name": "Hrishikesh Kulkarni"
                },
                {
                    "authorId": "1685063",
                    "name": "Nazli Goharian"
                },
                {
                    "authorId": "1741208",
                    "name": "O. Frieder"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                }
            ]
        },
        {
            "paperId": "7d25245a8f43a877ee6fb52002f89f42738b00b7",
            "title": "LexBoost: Improving Lexical Document Retrieval with Nearest Neighbors",
            "abstract": "Sparse retrieval methods like BM25 are based on lexical overlap, focusing on the surface form of the terms that appear in the query and the document. The use of inverted indices in these methods leads to high retrieval efficiency. On the other hand, dense retrieval methods are based on learned dense vectors and, consequently, are effective but comparatively slow. Since sparse and dense methods approach problems differently and use complementary relevance signals, approximation methods were proposed to balance effectiveness and efficiency. For efficiency, approximation methods like HNSW are frequently used to approximate exhaustive dense retrieval. However, approximation techniques still exhibit considerably higher latency than sparse approaches. We propose LexBoost that first builds a network of dense neighbors (a corpus graph) using a dense retrieval approach while indexing. Then, during retrieval, we consider both a document's lexical relevance scores and its neighbors' scores to rank the documents. In LexBoost this remarkably simple application of the Cluster Hypothesis contributes to stronger ranking effectiveness while contributing little computational overhead (since the corpus graph is constructed offline). The method is robust across the number of neighbors considered, various fusion parameters for determining the scores, and different dataset construction methods. We also show that re-ranking on top of LexBoost outperforms traditional dense re-ranking and leads to results comparable with higher-latency exhaustive dense retrieval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50359017",
                    "name": "Hrishikesh Kulkarni"
                },
                {
                    "authorId": "1685063",
                    "name": "Nazli Goharian"
                },
                {
                    "authorId": "1741208",
                    "name": "O. Frieder"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                }
            ]
        },
        {
            "paperId": "8be6a0b8baf3eff8def1a0d976c072317842735a",
            "title": "Neural Passage Quality Estimation for Static Pruning",
            "abstract": "Neural networks -- especially those that use large, pre-trained language models -- have improved search engines in various ways. Most prominently, they can estimate the relevance of a passage or document to a user's query. In this work, we depart from this direction by exploring whether neural networks can effectively predict which of a document's passages are unlikely to be relevant to any query submitted to the search engine. We refer to this query-agnostic estimation of passage relevance as a passage's quality. We find that our novel methods for estimating passage quality allow passage corpora to be pruned considerably while maintaining statistically equivalent effectiveness; our best methods can consistently prune>25% of passages in a corpora, across various retrieval pipelines. Such substantial pruning reduces the operating costs of neural search engines in terms of computing resources, power usage, and carbon footprint -- both when processing queries (thanks to a smaller index size) and when indexing (lightweight models can prune low-quality passages prior to the costly dense or learned sparse encoding step). This work sets the stage for developing more advanced neural\"learning-what-to-index\"methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2311076708",
                    "name": "Xuejun Chang"
                },
                {
                    "authorId": "2310823865",
                    "name": "Debabrata Mishra"
                },
                {
                    "authorId": "2273328040",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                }
            ]
        },
        {
            "paperId": "18eec1048569008a392e2f175cbf4029a220bb85",
            "title": "On the Effects of Regional Spelling Conventions in Retrieval Models",
            "abstract": "One advantage of neural ranking models is that they are meant to generalise well in situations of synonymity i.e. where two words have similar or identical meanings. In this paper, we investigate and quantify how well various ranking models perform in a clear-cut case of synonymity: when words are simply expressed in different surface forms due to regional differences in spelling conventions (e.g., color vs colour). We first explore the prevalence of American and British English spelling conventions in datasets used for the pre-training, training and evaluation of neural retrieval methods, and find that American spelling conventions are far more prevalent. Despite these biases in the training data, we find that retrieval models often generalise well in this case of synonymity. We explore the effect of document spelling normalisation in retrieval and observe that all models are affected by normalising the document's spelling. While they all experience a drop in performance when normalised to a different spelling convention than that of the query, we observe varied behaviour when the document is normalised to share the query spelling convention: lexical models show improvements, dense retrievers remain unaffected, and re-rankers exhibit contradictory behaviour.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "101731305",
                    "name": "A. Chari"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "21de7594eeefe160b9cf5ee32df8b2671e063e82",
            "title": "Genetic Generative Information Retrieval",
            "abstract": "Documents come in all shapes and sizes and are created by many different means, including now-a-days, generative language models. We demonstrate that a simple genetic algorithm can improve generative information retrieval by using a document's text as a genetic representation, a relevance model as a fitness function, and a large language model as a genetic operator that introduces diversity through random changes to the text to produce new documents. By \"mutating\" highly-relevant documents and \"crossing over\" content between documents, we produce new documents of greater relevance to a user's information need --- validated in terms of estimated relevance scores from various models and via a preliminary human evaluation. We also identify challenges that demand further study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50359017",
                    "name": "Hrishikesh Kulkarni"
                },
                {
                    "authorId": "2091339321",
                    "name": "Zachary Young"
                },
                {
                    "authorId": "1685063",
                    "name": "Nazli Goharian"
                },
                {
                    "authorId": "1741208",
                    "name": "O. Frieder"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                }
            ]
        },
        {
            "paperId": "24fcd29e28a27efb06647ae42a64e50c11d49840",
            "title": "Effective Contrastive Weighting for Dense Query Expansion",
            "abstract": "Verbatim queries submitted to search engines often do not sufficiently describe the user\u2019s search intent. Pseudo-relevance feedback (PRF) techniques, which modify a query\u2019srepresentation using the top-ranked documents, have been shown to overcome such inadequacies and improve retrieval effectiveness for both lexical methods (e.g., BM25) and dense methods (e.g., ANCE, ColBERT). For instance, the recent ColBERT-PRF approach heuristically chooses new embeddings to add to the query representation using the inverse document frequency (IDF) of the underlying tokens. However, this heuristic potentially ignores the valuable context encoded by the embeddings. In this work, we present a contrastive solution that learns to select the most useful embeddings for expansion. More specifically, a deep language model-based contrastive weighting model, called CWPRF, is trained to learn to discriminate between relevant and non-relevant documents for semantic search. Our experimental results show that our contrastive weighting model can aid to select useful expansion embeddings and outperform various baselines. In particular, CWPRF can improve nDCG@10 by upto to 4.1% compared to an existing PRF approach for ColBERT while maintaining its efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144550968",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "352bcafbcc95a84d96019688955cab5c43eb23f0",
            "title": "One-Shot Labeling for Automatic Relevance Estimation",
            "abstract": "Dealing with unjudged documents (\"holes\") in relevance assessments is a perennial problem when evaluating search systems with offline experiments. Holes can reduce the apparent effectiveness of retrieval systems during evaluation and introduce biases in models trained with incomplete data. In this work, we explore whether large language models can help us fill such holes to improve offline evaluations. We examine an extreme, albeit common, evaluation setting wherein only a single known relevant document per query is available for evaluation. We then explore various approaches for predicting the relevance of unjudged documents with respect to a query and the known relevant document, including nearest neighbor, supervised, and prompting techniques. We find that although the predictions of these One-Shot Labelers (1SL) frequently disagree with human assessments, the labels they produce yield a far more reliable ranking of systems than the single labels do alone. Specifically, the strongest approaches can consistently reach system ranking correlations of over 0.86 with the full rankings over a variety of measures. Meanwhile, the approach substantially increases the reliability of t-tests due to filling holes in relevance assessments, giving researchers more confidence in results they find to be significant. Alongside this work, we release an easy-to-use software package to enable the use of 1SL for evaluation of other ad-hoc collections or systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "3328733",
                    "name": "Luca Soldaini"
                }
            ]
        },
        {
            "paperId": "43fac75122651e2f840e059cc7174b92d23deadf",
            "title": "Online Distillation for Pseudo-Relevance Feedback",
            "abstract": "Model distillation has emerged as a prominent technique to improve neural search models. To date, distillation taken an offline approach, wherein a new neural model is trained to predict relevance scores between arbitrary queries and documents. In this paper, we explore a departure from this offline distillation strategy by investigating whether a model for a specific query can be effectively distilled from neural re-ranking results (i.e., distilling in an online setting). Indeed, we find that a lexical model distilled online can reasonably replicate the re-ranking of a neural model. More importantly, these models can be used as queries that execute efficiently on indexes. This second retrieval stage can enrich the pool of documents for re-ranking by identifying documents that were missed in the first retrieval stage. Empirically, we show that this approach performs favourably when compared with established pseudo relevance feedback techniques, dense retrieval methods, and sparse-dense ensemble\"hybrid\"approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "2108249583",
                    "name": "Xi Wang"
                }
            ]
        },
        {
            "paperId": "4c0c159c48fb8516cae8a2a42788e3de58ccff3c",
            "title": "Adaptive Latent Entity Expansion for Document Retrieval",
            "abstract": "Despite considerable progress in neural relevance ranking techniques, search engines still struggle to process complex queries effectively - both in terms of precision and recall. Sparse and dense Pseudo-Relevance Feedback (PRF) approaches have the potential to overcome limitations in recall, but are only effective with high precision in the top ranks. In this work, we tackle the problem of search over complex queries using three complementary techniques. First, we demonstrate that applying a strong neural re-ranker before sparse or dense PRF can improve the retrieval effectiveness by 5-8%. This improvement in PRF effectiveness can be attributed directly to improving the precision of the feedback set. Second, we propose an enhanced expansion model, Latent Entity Expansion (LEE), which applies fine-grained word and entity-based relevance modelling incorporating localized features. Specifically, we find that by including both words and entities for expansion achieve a further 2-8% improvement in NDCG. Our analysis also demonstrated that LEE is largely robust to its parameters across datasets and performs well on entity-centric queries. And third, we include an 'adaptive' component in the retrieval process, which iteratively refines the re-ranking pool during scoring using the expansion model and avoids re-ranking additional documents. We find that this combination of techniques achieves the best NDCG, MAP and R@1000 results on the TREC Robust 2004 and CODEC document datasets, demonstrating a significant advancement in expansion effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2113355478",
                    "name": "Shubham Chatterjee"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "49694325",
                    "name": "Jeffrey Stephen Dalton"
                }
            ]
        }
    ]
}