{
    "authorId": "2110659921",
    "papers": [
        {
            "paperId": "a3ffe5939a3cbd1f8424175e6725c2e577b0f518",
            "title": "Effective Unsupervised Constrained Text Generation based on Perturbed Masking",
            "abstract": "Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data. Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps. In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step. Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit. Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty. Since PMCTG does not require supervised data, it could be applied to different generation tasks. We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords- to-sentence generation and paraphrasing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "0faebb113812d14d14a0ebd0479e026a6534bdbe",
            "title": "Cross-Lingual Named Entity Recognition for Heterogenous Languages",
            "abstract": "Previous works on cross-lingual Named Entity Recognition (NER) have achieved great success. However, few of them consider the effect of language families between the source and target languages. In this study, we find that the cross-lingual NER performance of a target language would decrease when its source language is changed from the same (homogenous) into a different (heterogenous) language family with that target language. To improve the NER performance in this situation, we propose a novel cross-lingual NER framework based on self-distillation mechanism and Bilateral-Branch Network (SD-BBN). SD-BBN learns source-language NER knowledge from supervised datasets and obtains target-language knowledge from weakly supervised datasets. These two kinds of knowledge are then fused based on self-distillation mechanism for better identifying entities in the target language. We evaluate SD-BBN on 9 language datasets from 4 different language families. Results show that SD-BBN tends to outperform baseline methods. Remarkably, when the target and source languages are heterogenous, SD-BBN can achieve a greater boost. Our results might suggest that obtaining language-specific knowledge from the target language is essential for improving cross-lingual NER when the source and target languages are heterogenous. This finding could provide a novel insight into further research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "67285699",
                    "name": "Nankai Lin"
                },
                {
                    "authorId": "2152687652",
                    "name": "Boyu Chen"
                },
                {
                    "authorId": null,
                    "name": "Ziyu Yang"
                },
                {
                    "authorId": "2130537542",
                    "name": "Shengyi Jiang"
                }
            ]
        },
        {
            "paperId": "87cdbca61cc35ee3ec736be027aae9badd8c06a1",
            "title": "A Chinese Grammatical Error Correction Model Based On Grammatical Generalization And Parameter Sharing",
            "abstract": "\n Chinese grammatical error correction (CGEC) is a significant challenge in Chinese natural language processing. Deep-learning-based models tend to have tens of millions or even hundreds of millions of parameters since they model the target task as a sequence-to-sequence problem. This may require a vast quantity of annotated corpora for training and parameter tuning. However, there are currently few open-source annotated corpora for the CGEC task; the existing researches mainly concentrate on using data augmentation technology to alleviate the data-hungry problem. In this paper, rather than expanding training data, we propose a competitive CGEC model from a new insight for reducing model parameters. The model contains three main components: a sequence learning module, a grammatical generalization module and a parameter sharing module. Experimental results on two Chinese benchmarks demonstrate that the proposed model could achieve competitive performance over several baselines. Even if the parameter number of our model is reduced by 1/3, it could reach a comparable $F_{0.5}$ value of 30.75%. Furthermore, we utilize English datasets to evaluate the generalization and scalability of the proposed model. This could provide a new feasible research direction for CGEC research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67285699",
                    "name": "Nankai Lin"
                },
                {
                    "authorId": "48030085",
                    "name": "Xiaotian Lin"
                },
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "2130537542",
                    "name": "Shengyi Jiang"
                },
                {
                    "authorId": "2144695891",
                    "name": "Lianxi Wang"
                }
            ]
        },
        {
            "paperId": "bbe5ea6dc1470b33f3396a417bd546638948f535",
            "title": "Self-Training With Double Selectors for Low-Resource Named Entity Recognition",
            "abstract": "Named Entity Recognition (NER) is fundamental to multiple downstream natural language processing (NLP) tasks, but most advanced NER methods heavily rely on massive labeled data with high cost. In this paper, we explore the effectiveness of self-training for low-resource NER. It is one of the semi-supervised approaches to reduce the reliance on manual annotation. However, random pseudo sample selection in standard self-training framework may cause serious error propagation, especially for token-level tasks. To that end, this paper focuses on pseudo sample selection and proposes a new self-training framework with double selectors, namely auxiliary judge task and entropy-based confidence measurement. Specifically, the auxiliary judge task is proposed to filter out the pseudo samples with wrong predictions. The entropy-based confidence measurement is introduced to select pseudo samples with high quality. In addition, to make full use of all pseudo samples, we propose a cumulative function based on the idea of curriculum learning to prompt the model to learn from easy samples to hard ones. Samples with low quality are filtered out through the double selectors, which is more conducive to the training of student models. Experimental results on five NER benchmark datasets from different languages indicate the effectiveness of the proposed framework over several advanced baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "67285699",
                    "name": "Nankai Lin"
                },
                {
                    "authorId": "2143475374",
                    "name": "Xiao-sheng Yu"
                },
                {
                    "authorId": "2130537542",
                    "name": "Shengyi Jiang"
                }
            ]
        },
        {
            "paperId": "c21f5b10bdd49b316a74559790e89aae2dc0a1fc",
            "title": "How to choose \"Good\" Samples for Text Data Augmentation",
            "abstract": "Deep learning-based text classification models need abundant labeled data to obtain competitive performance. Unfortunately, annotating large-size corpus is time-consuming and laborious. To tackle this, multiple researches try to use data augmentation to expand the corpus size. However, data augmentation may potentially produce some noisy augmented samples. There are currently no works exploring sample selection for augmented samples in nature language processing field. In this paper, we propose a novel self-training selection framework with two selectors to select the high-quality samples from data augmentation. Specifically, we firstly use an entropy-based strategy and the model prediction to select augmented samples. Considering some samples with high quality at the above step may be wrongly filtered, we propose to recall them from two perspectives of word overlap and semantic similarity. Experimental results show the effectiveness and simplicity of our framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48030085",
                    "name": "Xiaotian Lin"
                },
                {
                    "authorId": "67285699",
                    "name": "Nankai Lin"
                },
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "2111906039",
                    "name": "Ziyu Yang"
                },
                {
                    "authorId": "2130537542",
                    "name": "Shengyi Jiang"
                }
            ]
        },
        {
            "paperId": "086ed321ae911a25e1f79ffafe7881844d50c8f3",
            "title": "A New Evaluation Method: Evaluation Data and Metrics for Chinese Grammar Error Correction",
            "abstract": "jiangshengyi@163.com ABSTRACT As a fundamental task in natural language processing, Chinese Grammatical Error Correction (CGEC) has gradually received widespread attention and become a research hotspot. However, one obvious deficiency for the existing CGEC evaluation system is that the evaluation values are significantly influenced by the Chinese word segmentation results or different language models. The evaluation values of the same error correction model can vary considerably under different word segmentation systems or different language models. However, it is expected that these metrics should be independent of the word segmentation results and language models, as they may lead to a lack of uniqueness and comparability in the evaluation of different methods. To this end, we propose three novel evaluation metrics for CGEC in two dimensions: reference-based and reference-less. In terms of the reference-based metric, we introduce sentence-level accuracy and char-level BLEU to evaluate the corrected sentences. Besides, in terms of the reference-less metric, we adopt char-level meaning preservation to measure the semantic preservation degree of the corrected sentences. We deeply evaluate and analyze the reasonableness and validity of the three proposed metrics, and we expect them to become a new standard for",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67285699",
                    "name": "Nankai Lin"
                },
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "48030085",
                    "name": "Xiaotian Lin"
                },
                {
                    "authorId": null,
                    "name": "Ziyu Yang"
                },
                {
                    "authorId": "2130537542",
                    "name": "Shengyi Jiang"
                }
            ]
        },
        {
            "paperId": "250a3a5525e89d0314fc36be66b65f8b97a67f5d",
            "title": "BERT 4EVER@LT-EDI-ACL2022-Detecting signs of Depression from Social Media:Detecting Depression in Social Media using Prompt-Learning and Word-Emotion Cluster",
            "abstract": "In this paper, we report the solution of the team BERT 4EVER for the LT-EDI-2022 shared task2: Homophobia/Transphobia Detection in social media comments in ACL 2022, which aims to classify Youtube comments into one of the following categories: no,moderate, or severe depression. We model the problem as a text classification task and a text generation task and respectively propose two different models for the tasks.To combine the knowledge learned from these two different models, we softly fuse the predicted probabilities of the models above and then select the label with the highest probability as the final output.In addition, multiple augmentation strategies are leveraged to improve the model generalization capability, such as back translation and adversarial training.Experimental results demonstrate the effectiveness of the proposed models and two augmented strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48030085",
                    "name": "Xiaotian Lin"
                },
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": null,
                    "name": "Ziyu Yang"
                },
                {
                    "authorId": "67285699",
                    "name": "Nankai Lin"
                },
                {
                    "authorId": "2130537542",
                    "name": "Shengyi Jiang"
                }
            ]
        },
        {
            "paperId": "40a7b334413765c7e5ef1d31799aede6fa99c2ea",
            "title": "CL-XABSA: Contrastive Learning for Cross-Lingual Aspect-Based Sentiment Analysis",
            "abstract": "Aspect-based sentiment analysis (ABSA), an extensively researched area in the field of natural language processing (NLP), predicts the sentiment expressed in a text relative to the corresponding aspect. Unfortunately, most languages lack sufficient annotation resources; thus, an increasing number of recent researchers have focused on cross-lingual aspect-based sentiment analysis (XABSA). However, most recent studies focus only on cross-lingual data alignment instead of model alignment. Therefore, we propose a novel framework, CL-XABSA: contrastive learning for cross-lingual aspect-based sentiment analysis. Based on contrastive learning, we close the distance between samples with the same label in different semantic spaces, achieving convergence of semantic spaces of different languages. Specifically, we design two contrastive objectives, token-level contrastive learning of token embeddings (TL-CTE) and sentiment-level contrastive learning of token embeddings (SL-CTE), to unify the semantic space of source and target languages. Since CL-XABSA can receive datasets in multiple languages during training, it can be further extended to multilingual aspect-based sentiment analysis (MABSA). To further improve the model performance, we perform knowledge distillation with target-language unlabeled data. In the distillation XABSA task, we further explore the effectiveness of different data (source dataset, translated dataset, and code-switched dataset). The results demonstrate that the proposed method has a certain improvement in the three XABSA tasks, distillation XABSA and MABSA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67285699",
                    "name": "Nankai Lin"
                },
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "48030085",
                    "name": "Xiaotian Lin"
                },
                {
                    "authorId": "2116324169",
                    "name": "D. Zhou"
                },
                {
                    "authorId": "2057539878",
                    "name": "Aimin Yang"
                },
                {
                    "authorId": "2130537542",
                    "name": "Shengyi Jiang"
                }
            ]
        },
        {
            "paperId": "642e04ece55c5b532ff7e5408d8723c7d9c835db",
            "title": "MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL",
            "abstract": "Conversational text-to-SQL is designed to translate multi-turn natural language questions into their corresponding SQL queries. Most advanced conversational text-to-SQL methods are incompatible with generative pre-trained language models (PLMs), such as T5. In this paper, we present a two-stage unified MultI-task Generation frAmework (MIGA) that leverages PLMs\u2019 ability to tackle conversational text-to-SQL. In the pre-training stage, MIGA first decomposes the main task into several related sub-tasks and then unifies them into the same sequence-to-sequence (Seq2Seq) paradigm with task-specific natural language prompts to boost the main task from multi-task training. Later in the fine-tuning stage, we propose four SQL perturbations to alleviate the error propagation problem. MIGA tends to achieve state-of-the-art performance on two benchmarks (SparC and CoSQL). We also provide extensive analyses and discussions to shed light on some new perspectives for conversational text-to-SQL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "792f6b8e7da1b36c62ff8d77788d99dd8bd44770",
            "title": "A Dual-Contrastive Framework for Low-Resource Cross-Lingual Named Entity Recognition",
            "abstract": "Cross-lingual Named Entity Recognition (NER) has recently become a research hotspot because it can alleviate the data-hungry problem for low-resource languages. However, few researches have focused on the scenario where the source-language labeled data is also limited in some specific domains. A common approach for this scenario is to generate more training data through translation or generation-based data augmentation method. Unfortunately, we find that simply combining source-language data and the corresponding translation cannot fully exploit the translated data and the improvements obtained are somewhat limited. In this paper, we describe our novel dual-contrastive framework ConCNER for cross-lingual NER under the scenario of limited source-language labeled data. Specifically, based on the source-language samples and their translations, we design two contrastive objectives for cross-language NER at different grammatical levels, namely Translation Contrastive Learning (TCL) to close sentence representations between translated sentence pairs and Label Contrastive Learning (LCL) to close token representations within the same labels. Furthermore, we utilize knowledge distillation method where the NER model trained above is used as the teacher to train a student model on unlabeled target-language data to better fit the target language. We conduct extensive experiments on a wide variety of target languages, and the results demonstrate that ConCNER tends to outperform multiple baseline methods. For reproducibility, our code for this paper is available at https://github.com/GKLMIP/ConCNER.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "67285699",
                    "name": "Nankai Lin"
                },
                {
                    "authorId": null,
                    "name": "Ziyu Yang"
                },
                {
                    "authorId": "2130537542",
                    "name": "Shengyi Jiang"
                }
            ]
        }
    ]
}