{
    "authorId": "40939264",
    "papers": [
        {
            "paperId": "38cad438b847936abc64044ddbd77afefd0b2d7a",
            "title": "MGRL: Mutual-Guidance Representation Learning for Text-to-Image Person Retrieval",
            "abstract": "Text-to-image person retrieval aims to recognize target pedestrians based on specified text. Existing methods mainly obtain image and text features separately through distinct feature extractors, subsequently embedding them into a unified feature space and calculating their similarity. Despite great success, current methods still suffer from the lack of information interaction between images and text. To address this issue, we propose Mutual-guidance Representation Learning (MGRL) for text-to-image person retrieval, which captures the key features for matching via text-image information interaction. Accordingly, our MGRL consists of two customized modules: iterative text-guided feature extraction (ITFE) and vision-assisted specific mask complement (VSMC). Specifically, ITFE is first designed to extract the matching information between the text and the image concerning the local feature attention of the target pedestrians by iterative text guidance. Then, to further ensure the image features extracted by ITFE contain the text description, VSMC is designed to utilize the extracted image features to help complete masked text where the mask is difficult to complete with only unmasked text information. Experiments are conducted on CUHK-PEDES and ICFG-PEDES datasets, and experimental results demonstrate the superiority of the proposed MGRL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292170559",
                    "name": "Tianle Lv"
                },
                {
                    "authorId": "2262272944",
                    "name": "Shuang Li"
                },
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "2240325932",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "5cb9b26e8b86adce350020526c22b3935a110c54",
            "title": "Modality-Free Violence Detection via Cross-Modal Causal Attention and Feature Distillation",
            "abstract": "In this paper, we propose a novel framework, Modality-Free Violence Detection (MFVD), which captures the causal relationships among multimodal cues and ensures stable performance even in the absence of audio information. Specifically, we design a novel Cross-Modal Causal Attention mechanism (CCA) to deal with modality asynchrony by utilizing relative temporal distance and semantic correlation to obtain causal attention between audio and visual information instead of merely calculating correlation scores between audio and visual features. Moreover, to ensure our framework can work well when the audio modality is missing, we design a Cross-Modal Feature Distillation module (CFD), leveraging the common parts of the fused features obtained from CCA to guide the enhancement of visual features. Experimental results on the XD-Violence dataset demonstrate the superior performance of the proposed method in both vision-only and audio-visual modalities, surpassing state-of-the-art methods for both tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "2323528992",
                    "name": "Zhanjie Wu"
                },
                {
                    "authorId": "2184917168",
                    "name": "Mengjingcheng Mo"
                },
                {
                    "authorId": "2187309458",
                    "name": "Mingpi Tan"
                },
                {
                    "authorId": "2262272944",
                    "name": "Shuang Li"
                },
                {
                    "authorId": "2240325932",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "6b1953cf4b171a40ab486f50a48fb82fd8b5ab0e",
            "title": "Beyond Euclidean: Dual-Space Representation Learning for Weakly Supervised Video Violence Detection",
            "abstract": "While numerous Video Violence Detection (VVD) methods have focused on representation learning in Euclidean space, they struggle to learn sufficiently discriminative features, leading to weaknesses in recognizing normal events that are visually similar to violent events (\\emph{i.e.}, ambiguous violence). In contrast, hyperbolic representation learning, renowned for its ability to model hierarchical and complex relationships between events, has the potential to amplify the discrimination between visually similar events. Inspired by these, we develop a novel Dual-Space Representation Learning (DSRL) method for weakly supervised VVD to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features of events while also exploring the intrinsic relations between events, thereby enhancing the discriminative capacity of the features. DSRL employs a novel information aggregation strategy to progressively learn event context in hyperbolic spaces, which selects aggregation nodes through layer-sensitive hyperbolic association degrees constrained by hyperbolic Dirichlet energy. Furthermore, DSRL attempts to break the cyber-balkanization of different spaces, utilizing cross-space attention to facilitate information interactions between Euclidean and hyperbolic space to capture better discriminative features for final violence detection. Comprehensive experiments demonstrate the effectiveness of our proposed DSRL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "2323528992",
                    "name": "Zhanjie Wu"
                },
                {
                    "authorId": "2187309458",
                    "name": "Mingpi Tan"
                },
                {
                    "authorId": "2242965543",
                    "name": "Yiran Liu"
                },
                {
                    "authorId": "2261391709",
                    "name": "Ji Gan"
                },
                {
                    "authorId": "2307335918",
                    "name": "Haosheng Chen"
                },
                {
                    "authorId": "2240325932",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "8bf55e7c4c17342d078fd81c1dee29dd57dc5ccc",
            "title": "Structure-Aware in-Air Handwritten Text Recognition with Graph-Guided Cross-Modality Translator",
            "abstract": "In-air handwriting as a new human-computer interaction way plays an important role in many virtual/mixed-reality applications. Existing methods for in-air handwritten text recognition (IAHTR) typically directly process handwriting trajectories with deep neural networks. However, those methods all simply learn discriminative patterns by modelling low-level relationships between adjacent points of trajectories, while completely ignoring the inherent geometric structures of characters. Instead, we propose a novel Graph-guided Cross-modality Translator for IAHTR, which further explicitly exploits the geometric structures of characters for guiding the decoding of trajectories via graph-guided cross-modality attention mechanism without introducing extra annotation costs. Experiments on benchmarks IAHEW-UCAS2016 & IAM-OnDB show that our method has achieved state-of-the-art performance for handwritten text recognition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2200371961",
                    "name": "Yuyan Chen"
                },
                {
                    "authorId": "2292537195",
                    "name": "Xing Zhao"
                },
                {
                    "authorId": "2261391709",
                    "name": "Ji Gan"
                },
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "2275853709",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2256945745",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "986f4ccc2e75d65e8f74687ded96a7d08c8288af",
            "title": "CLIP-Driven Cloth-Agnostic Feature Learning for Cloth-Changing Person Re-Identification",
            "abstract": "Contrastive Language-Image Pre-Training (CLIP) has shown impressive performance in short-term Person Re-Identification (ReID) due to its ability to extract high-level semantic features of pedestrians, yet its direct application to Cloth-Changing Person Re-Identification (CC-ReID) faces challenges due to CLIP's image encoder overly focusing on clothes clues. To address this, we propose a novel framework called CLIP-Driven Cloth-Agnostic Feature Learning (CCAF) for CC-ReID. Accordingly, two modules were custom-designed: the Invariant Feature Prompting (IFP) and the Clothes Feature Minimization (CFM). These modules guide the model to extract cloth-agnostic features positively and attenuate clothes-related features negatively. Specifically, IFP is designed to extract fine-grained semantic features unrelated to clothes from the raw image, guided by the cloth-agnostic text prompts. This module first covers the clothes in the raw image at the pixel level to obtain the shielding image and then utilizes CLIP's knowledge to generate cloth-agnostic text prompts. Subsequently, it aligns the raw image-text and the raw image-shielding image in the feature space, emphasizing discriminative clues related to identity but unrelated to clothes. Furthermore, CFM is designed to examine and weaken the image encoder's ability to extract clothes features. It first generates text prompts corresponding to clothes pixels. Then, guided by these clothes text prompts, it iteratively examines and disentangles clothes features from pedestrian features, ultimately retaining inherent discriminative features. Extensive experiments have demonstrated the effectiveness of the proposed CCAF, achieving new state-of-the-art performance on several popular CC-ReID benchmarks without any additional inference time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262272944",
                    "name": "Shuang Li"
                },
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "2139738360",
                    "name": "Guozhang Li"
                },
                {
                    "authorId": "2261391709",
                    "name": "Ji Gan"
                },
                {
                    "authorId": "2307335918",
                    "name": "Haosheng Chen"
                },
                {
                    "authorId": "2240325932",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "e0cb5deff4fd3f5e7326afd552d2699031975f38",
            "title": "CRNet: Context-guided Reasoning Network for Detecting Hard Objects",
            "abstract": "Recent studies have shown impressive performance in object detection. However, most current detectors only explore the appearance feature to locate and classify objects but disregard or underestimate the valuable contextual information in the image, which limits the detection performance for those hard objects, such as small objects, occluded objects, blurred objects, etc. In this article, we instead seek to build a novel context modeling framework and conduct more effective context reasoning for object detection. Specifically, we design a Context-guided Reasoning Network (CRNet) to explore the relationships between objects and use easy detected objects to help understand hard ones. In our CRNet, an image is modeled as a graph and local features of objects are viewed as nodes of the graph to learn the relationships between objects. By passing contextual information in the built graph, the features of hard objects can be updated to discriminative features. To this end, we first develop a cascaded center prediction module built upon CenterNet to produce a set of high-quality proposals viewed as nodes of the graph. In addition, to maximize the value of global context information, we present a multi-granularity feature fusion network to encode the whole scene information which is also viewed as nodes of the graph. Then, the spatial and semantic relationships between objects are learned to initialize edges of the graph. Finally, context reasoning is conducted to update the node states iteratively. Extensive experiments are conducted on MS COCO and Pascal VOC to demonstrate the effectiveness of the proposed CRNet. Experimental results show that the proposed CRNet greatly improves the detection performance over existing context-based detectors, and it is comparable with state-of-the-art detectors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "2242965543",
                    "name": "Yiran Liu"
                },
                {
                    "authorId": "2240325932",
                    "name": "Xinbo Gao"
                },
                {
                    "authorId": "2242540101",
                    "name": "Zhihui Wang"
                }
            ]
        },
        {
            "paperId": "e10bb279760e94430953f625fba0a4ee42294642",
            "title": "KDDGAN: Knowledge-Guided Explicit Feature Disentanglement for Facial Attribute Editing",
            "abstract": "Facial attribute editing is a popular direction in face generation, which aims to modify facial attributes in the face image and remain unedited attributes unchanged. However, generative models are prone to affect the unedited attributes when editing multiple facial attributes. Currently, the concatenation of the prior knowledge with hidden features is still data-driven work. Due to the feature coupling in data-driven models, high-entanglement implicit semantics are generated, which is incomprehensible for human beings. Besides, multi-attribute boundaries of the implicit semantics are ambiguous, which is complicated to effectively control the editing process. In this paper, we propose a knowledge-guided explicit feature disentanglement network that is compatible with human cognition, leveraging a classification method with the prior knowledge to encode features. Specifically, we select 13 facial attribute labels for a comprehensive and explicit presentation of this task and design a knowledge-guided feature disentanglement module to transform the implicit feature representations into explicit feature semantics. We also construct a semantic space that can independently manipulate facial attributes. In addition, our proposed model can be combined with existing facial attribute editing models to obtain multiple variant models. Our proposed model is fully validated by various experiments and the variant model has achieved better performance than the benchmark model in facial attribute editing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2247943093",
                    "name": "Ye Wang"
                },
                {
                    "authorId": "2276989494",
                    "name": "Qianmengke Zhao"
                },
                {
                    "authorId": "2115900319",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "2261675105",
                    "name": "Guoyin Wang"
                },
                {
                    "authorId": "2149137916",
                    "name": "Hong Yu"
                },
                {
                    "authorId": "2150979611",
                    "name": "Li Liu"
                },
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                }
            ]
        },
        {
            "paperId": "e3f47b3b6dac4a87e0f9bfe5570918ae7f7ae648",
            "title": "Dual Space Embedding Learning For Weakly Supervised Audio-Visual Violence Detection",
            "abstract": "In this paper, we propose Dual Space Embedding Learning (DSEL) for weakly supervised audio-visual violence detection, which excavates violence information deeply in both Euclidean and Hyperbolic spaces to distinguish violence from non-violence semantically and alleviate the asynchronous issue of violent cues in audio-visual patterns. Specifically, we first design a dual space visual feature interaction module (DSVFI) to deeply investigate the violence information in visual modality, which contains richer information compared to audio counterpart. Then, considering the modality asynchrony between the two modalities, we employ a late modality fusion method and design an asynchrony-aware audio-visual fusion module (AAF), in which visual features receive the violent prompt from the audio features after interacting among snippets and learning the violence information from each other. Experimental results show that our method achieves state-of-the-art performance on XD-Violence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242965543",
                    "name": "Yiran Liu"
                },
                {
                    "authorId": "2323528992",
                    "name": "Zhanjie Wu"
                },
                {
                    "authorId": "2184917168",
                    "name": "Mengjingcheng Mo"
                },
                {
                    "authorId": "2261391709",
                    "name": "Ji Gan"
                },
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "2240325932",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "17f4b7415b9a471c41fe071318040c51ae1b75b2",
            "title": "Multiple Pedestrian Tracking With Graph Attention Map on Urban Road Scene",
            "abstract": "Pedestrians are often vulnerable users of urban roads and ensuring their safety is a pressing challenge in the filed of intelligent transportation. Multiple pedestrian tracking is one of the key technologies for traffic statistics and abnormal behavior analysis, etc. Detection-based tracking methods have achieved remarkable results and have become mainstream processing schemes. However, target association is still immature and less effective in complex scenarios. In the proposed tracking system, several candidates surrounding each detected pedestrian are selected sparsely, and the associating relationship between the target and these candidates is determined based on a graph attention map. These graph attention maps contain positional correlations of the matching pairs and are applicable with pedestrians\u2019 posture variations. The weighted correlating value is estimated with the positional weighted matrix and merged attention map. The correlating relationship is confirmed with the weighted correlating value and distance matching loss. To enhance the computation efficiency of the graph attention maps for these tracked targets and candidates, feature extraction is processed separately. Convolutional features extracted from one specific middle layer of the backbone network are used to represent each target or candidate. The experiment results show that the proposed tracker achieves better performance than the other five state-of-the-art trackers on three publicly available databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108349114",
                    "name": "Zhihui Wang"
                },
                {
                    "authorId": "2162795880",
                    "name": "Zhiyuan Li"
                },
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "39456152",
                    "name": "Ming Li"
                },
                {
                    "authorId": "2075398670",
                    "name": "Lu Bai"
                }
            ]
        },
        {
            "paperId": "26509bc3fbd4c9f5cd89f3cd6b65f84d09f0250d",
            "title": "PRO-Face S: Privacy-preserving Reversible Obfuscation of Face Images via Secure Flow",
            "abstract": "This paper proposes a novel paradigm for facial privacy protection that unifies multiple characteristics including anonymity, diversity, reversibility and security within a single lightweight framework. We name it PRO-Face S, short for Privacy-preserving Reversible Obfuscation of Face images via Secure flow-based model. In the framework, an Invertible Neural Network (INN) is utilized to process the input image along with its pre-obfuscated form, and generate the privacy protected image that visually approximates to the pre-obfuscated one, thus ensuring privacy. The pre-obfuscation applied can be in diversified form with different strengths and styles specified by users. Along protection, a secret key is injected into the network such that the original image can only be recovered from the protection image via the same model given the correct key provided. Two modes of image recovery are devised to deal with malicious recovery attempts in different scenarios. Finally, extensive experiments conducted on three public image datasets demonstrate the superiority of the proposed framework over multiple state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152192126",
                    "name": "Lin Yuan"
                },
                {
                    "authorId": "2068639486",
                    "name": "Kai Liang"
                },
                {
                    "authorId": "2187308496",
                    "name": "Xiao Pu"
                },
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "2112377430",
                    "name": "Tao Wu"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        }
    ]
}