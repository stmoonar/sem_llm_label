{
    "authorId": "34573158",
    "papers": [
        {
            "paperId": "cd4b4cb3d2cf5b2213de1dce7abd7d946867bdba",
            "title": "NETEVOLVE: Social Network Forecasting using Multi-Agent Reinforcement Learning with Interpretable Features",
            "abstract": "Predicting how social networks change in the future is important in many applications. Results in social network research have shown that the change in the network can be explained by a small number of concepts, such as \"homophily\" and \"transitivity\". However, existing prediction methods require many latent features that are not connected to such concepts, making the methods' black boxes and their prediction results difficult to interpret, making them harder to derive scientific knowledge about social networks. In this study, we propose NetEvolve a novel multi-agent reinforcement learning-based method that predicts changes in a given social network. Given a sequence of changes as training data, NetEvolve learns the characteristics of the nodes with interpretable features, such as how the node feels rewards for connecting with similar people and the cost of the connection itself. Based on the learned feature, NetEvolve makes a forecast based on multi-agent simulation. The method achieves comparable or better accuracy than existing methods in predicting network changes in real-world social networks while keeping the prediction results interpretable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2300654064",
                    "name": "Kentaro Miyake"
                },
                {
                    "authorId": "2110352338",
                    "name": "Hiroyoshi Ito"
                },
                {
                    "authorId": "2256724188",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "2300535211",
                    "name": "Hirotomo Matsumoto"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "5b07d719e5b9691959cc466352978272f8c35d69",
            "title": "Hybrid Crowd-AI Learning for Human-Interpretable Symbolic Rules in Image Classification",
            "abstract": "Explainable AI is an indispensable goal for an AI-based society with trust, and deriving human-interpretable symbolic rules is one of the promising ways to verify whether the decision is appropriate. This paper explores a hybrid crowd-AI approach to develop white-box ML models associated with human-interpretable symbolic rules. The key idea of the proposed method is to discover human-interpretable latent features from trained neural networks by leveraging human abductive reasoning. The proposed method automatically generates crowdsourcing tasks that display subsets of images corresponding to each latent feature and ask crowd workers to provide the semantics of the features in natural language. The obtained semantics allow us to use the latent features as human-interpretable predicates that form symbolic rules to define target classes. We provide experimental results showing that the proposed approach can obtain interpretable symbolic rules and explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2148491471",
                    "name": "Ayame Shimizu"
                },
                {
                    "authorId": "2276798443",
                    "name": "Kei Wakabayashi"
                },
                {
                    "authorId": "2066598689",
                    "name": "Masaki Matsubara"
                },
                {
                    "authorId": "2110352338",
                    "name": "Hiroyoshi Ito"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "2ae63d86c5c6f53028daee61fed5ae9278556cfd",
            "title": "HAEM: Obtaining Higher-Quality Classification Task Results with AI Workers",
            "abstract": "Obtaining high-quality results for a fixed set of classification tasks with a limited budget is a critical issue in crowdsourcing. The introduction of AI models to complement the process should be explored. However, there are few existing approaches to directly address the problem, which have been proposed in the context of how to train AI models using noisy crowdsourced data. This paper presents a more direct approach for solving the problem of introducing AI to improve the task results of human workers for a fixed number of tasks with a limited budget; we deal with an AI model as a worker and aggregates the results of both human and AI workers in a symmetric manner. The proposed \u201cHuman-AI EM\u201d (HAEM) algorithm, which extends the Dawid Skene model, treats the AI models as workers and explicitly computes their confusion matrices to derive higher-quality aggregation results. We conducted an extensive set of experiments and compared HAEM with two other methods (MBEM and Dawid Skene model). We found that AI-powered HAEM showed better performance than the two methods in most cases and that the HAEM often performed better than the Dawid Skene model with additional human workers. We also found that AI workers work well when they are good at identifying particular classes even if they do not have very good overall accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143285042",
                    "name": "Yuki Yamashita"
                },
                {
                    "authorId": "2110352338",
                    "name": "Hiroyoshi Ito"
                },
                {
                    "authorId": "2177193",
                    "name": "Kei Wakabayashi"
                },
                {
                    "authorId": "2111044037",
                    "name": "Masaki Kobayashi"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "2f18fe62685cfa3872758edb0d6f8d93b2dd74f1",
            "title": "Multi-Armed Bandit Approach to Qualification Task Assignment across Multi Crowdsourcing Platforms",
            "abstract": "Many existing optimization approaches deal with task assignments on one single crowdsourcing platform. This paper addresses the difficulties of the optimal platform selection for qualification tasks on one single platform. We proposed a novel approach about assigning qualification tasks to workers iteratively on multiple platforms to maximize the total number of collected qualified workers on a limited budget. We applied Multi-Armed Bandit (MAB) algorithms to create strategies for the platform selections to achieve this goal. The conducted experiments revealed that (1) the optimal platform is not always trivial, and (2) the strategies created by MAB algorithms can achieve high-quality assignments under different settings which also satisfied different requesters\u2019 needs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2203119255",
                    "name": "Yunyi Xiao"
                },
                {
                    "authorId": "2143285042",
                    "name": "Yuki Yamashita"
                },
                {
                    "authorId": "2110352338",
                    "name": "Hiroyoshi Ito"
                },
                {
                    "authorId": "2066598689",
                    "name": "Masaki Matsubara"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "32c2b209e52b33705485a48c8057f069f8084862",
            "title": "Efficient Evaluation of AI Workers for the Human+AI Crowd Task Assignment",
            "abstract": "Nowadays, it is a common practice for crowd workers to develop ML models that classify data items. We envision the Human+AI crowd where crowd programmers develop \"AI workers,\" which are black-box software agents that work among other human workers. The problem here is evaluating such AI workers is different from evaluating human workers in that they may not be spam workers, although they have low accuracy at the beginning of their learning process or for a particular label. Therefore, existing work evaluates the output from AI workers every time they output the task results. Obviously, such a naive evaluation does not scale because there are a tremendous number of task results to be evaluated. This paper addresses the problem of how to efficiently evaluate AI worker outputs by skipping the AI evaluation when the AI is unlikely to satisfy the expected accuracy. We conducted an experiment to compare two strategies and found that both reduce the number of evaluations by orders of magnitude while keeping the number of task assignments to AI workers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66303213",
                    "name": "Tomoya Kanda"
                },
                {
                    "authorId": "2110352338",
                    "name": "Hiroyoshi Ito"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "5498366e2924f5435d684bbacfd04b100b496737",
            "title": "Crowdsourced Hypothesis Generation and their Verification: A Case Study on Sleep Quality Improvement",
            "abstract": "A clinical study is often necessary for exploring important research questions; however, this approach is sometimes time and money consuming. Another extreme approach, which is to collect and aggregate opinions from crowds, provides a result drawn from the crowds\u2019 past experiences and knowledge. To explore a solution that takes advantage of both the rigid clinical approach and the crowds\u2019 opinion-based approach, we design a framework that exploits crowdsourcing as a part of the research process, whereby crowd workers serve as if they were a scientist conducting a \u201cpseudo\u201d prospective study. This study evaluates the feasibility of the proposed framework to generate hypotheses on a specified topic and verify them in the real world by employing many crowd workers. The framework comprises two phases of crowd-based workflow. In Phase 1\u2014the hypothesis generation and ranking phase\u2014our system asks workers two types of questions to collect a number of hypotheses and rank them. In Phase 2\u2014the hypothesis verification phase\u2014the system asks workers to verify the top-ranked hypotheses from Phase 1 by implementing one of them in real life. Through experiments, we explore the potential and limitations of the framework to generate and evaluate hypotheses about the factors that result in a good night\u2019s sleep. Our results on significant sleep quality improvement show the basic feasibility of our framework, suggesting that crowd-based research is compatible with experts\u2019 knowledge in a certain domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1824142",
                    "name": "Shoko Wakamiya"
                },
                {
                    "authorId": "2165381230",
                    "name": "Toshiki Mera"
                },
                {
                    "authorId": "3182818",
                    "name": "E. Aramaki"
                },
                {
                    "authorId": "2066598689",
                    "name": "Masaki Matsubara"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "a318f3f1250f313b64df913a4f88b80c4d010629",
            "title": "Image Geolocation by Non-Expert Crowd Workers with an Expert Strategy",
            "abstract": "Identifying the location where a photo was taken is an important operation in many applications such as the disaster response if it is not associated with the location information. This process usually is done by experts who are familiar with the geolocation. However it is not guaranteed that we can find such expert workers. In this paper, we explore an approach to improve the quality of image geolocation with an workflow implement experts\u2019 strategy for the image geolocations. The result of preliminary experiment suggested that our approach is effective in improving the accuracy of non-expert geolocation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109561403",
                    "name": "Seungun Kim"
                },
                {
                    "authorId": "2066598689",
                    "name": "Masaki Matsubara"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "09e5a9edca681e75899d5b21fa09686e5dbcb639",
            "title": "Human+AI Crowd Task Assignment Considering Result Quality Requirements",
            "abstract": "This paper addresses the problem of dynamically assigning tasks to a crowd consisting of AI and human workers.\nCurrently, crowdsourcing the creation of AI programs is a common practice.\nTo apply such kinds of AI programs to the set of tasks, we often take the ``all-or-nothing'' approach that waits for the AI to be good enough.\nHowever, this approach may prevent us from exploiting the answers provided by the AI until the process is completed, and also prevents the exploration of different AI candidates.\nTherefore, integrating the created AI, both with other AIs and human computation, to obtain a more efficient human-AI team is not trivial.\nIn this paper, we propose a method that addresses these issues by adopting a ``divide-and-conquer'' strategy for AI worker evaluation.\nHere, the assignment is optimal when the number of task assignments to humans is minimal, as long as the final results satisfy a given quality requirement. \nThis paper presents some theoretical analyses of the proposed method and an extensive set of experiments conducted with open benchmarks and real-world datasets.\nThe results show that the algorithm can assign many more tasks than the baselines to AI when it is difficult for AIs to satisfy the quality requirement for the whole set of tasks. They also show that it can flexibly change the number of tasks assigned to multiple AI workers in accordance with the performance of the available AI workers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111044037",
                    "name": "Masaki Kobayashi"
                },
                {
                    "authorId": "2177193",
                    "name": "Kei Wakabayashi"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "1775d48025f3efaa407b6fd57b8db6b1b345a13d",
            "title": "Empirical Study on Effects of Self-Correction in Crowdsourced Microtasks",
            "abstract": "Self-correction for crowdsourced tasks is a two-stage setting that allows a crowd worker to review the task results of other workers; the worker is then given a chance to update their results according to the review.Self-correction was proposed as a complementary approach to statistical algorithms, in which workers independently perform the same task.It can provide higher-quality results with low additional costs. However, thus far, the effects have only been demonstrated in simulations, and empirical evaluations are required.In addition, as self-correction provides feedback to workers, an interesting question arises: whether perceptual learning is observed in self-correction tasks.This paper reports our experimental results on self-corrections with a real-world crowdsourcing service.We found that:(1) Self-correction is effective for making workers reconsider their judgments.(2) Self-correction is effective more if workers are shown the task results of higher-quality workers during the second stage.(3) A perceptual learning effect is observed in some cases. Self-correction can provide feedback that shows workers how to provide high-quality answers in future tasks.(4) A Perceptual learning effect is observed, particularly with workers who moderately change answers in the second stage. This suggests that we can measure the learning potential of workers.These findings imply that requesters/crowdsourcing services can construct a positive loop for improved task results by the self-correction approach.However, (5) no long-term effects of the self-correction task were transferred to other similar tasks in two different settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111044037",
                    "name": "Masaki Kobayashi"
                },
                {
                    "authorId": "49517358",
                    "name": "H. Morita"
                },
                {
                    "authorId": "2066598689",
                    "name": "Masaki Matsubara"
                },
                {
                    "authorId": "3037066",
                    "name": "N. Shimizu"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        },
        {
            "paperId": "7ddcdcfa41a16e1f23f28f15deade220826883cb",
            "title": "BUBBLE : A Quality-Aware Human-in-the-loop Entity Matching Framework",
            "abstract": "Entity matching is an issue of interest in information integration and data cleaning. Since the representations of the same entity vary, it is often impossible to fully automate the entity matching and require human inputs. However, to guarantee high-quality entity matching, how to integrate human resources into the entity matching while minimizing the cost of human resources? In this paper, we propose BUBBLE, a novel human-in-the-loop entity matching framework hybridizing Bayesian inference and crowdsourcing. To guarantee entity matching quality, Bayesian inference is conducted to determine whether the matching requires crowdsourcing. We show that we can define Bayesian error rate for this problem. For optimization, we use metric learning to select the candidate matching pairs by nearest-neighbor search in the learned embedding space, and we construct a k-nearest neighbor graph to avoid the redundant matching. We applied BUBBLE to a bibliographic data matching problem on the National Diet Library. The experimental results show that BUBBLE can assign tasks to humans with higher quality results compared to those of the same number of task assignments to humans. The result also shows that our optimization scheme is effective without sacrificing the quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149933069",
                    "name": "Naofumi Osawa"
                },
                {
                    "authorId": "2110352338",
                    "name": "Hiroyoshi Ito"
                },
                {
                    "authorId": "2149933327",
                    "name": "Yukihiro Fukushima"
                },
                {
                    "authorId": "2150194134",
                    "name": "Takashi Harada"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                }
            ]
        }
    ]
}