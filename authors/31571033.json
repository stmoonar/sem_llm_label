{
    "authorId": "31571033",
    "papers": [
        {
            "paperId": "02c009f41b66d2f977fb663f3cb69329f0f03d3f",
            "title": "Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images",
            "abstract": "In this paper, we introduce Recipe1M+, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M+ affords the ability to train high-capacity models on aligned, multimodal data. Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M+ dataset and food and cooking in general. Code, data and models are publicly available.<xref rid=\"fn1\" ref-type=\"fn\"><sup>1</sup></xref><fn id=\"fn1\"><label>1.</label><p><uri>http://im2recipe.csail.mit.edu</uri>.</p> </fn>",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2061199565",
                    "name": "Javier Mar\u00edn"
                },
                {
                    "authorId": "2130796",
                    "name": "A. Biswas"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "49230103",
                    "name": "Nick Hynes"
                },
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                }
            ]
        },
        {
            "paperId": "0dc21823b956251f746415eef82aa6a77d42045e",
            "title": "Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning",
            "abstract": "Cross-modal recipe retrieval has recently gained substantial attention due to the importance of food in people\u2019s lives, as well as the availability of vast amounts of digital cooking recipes and food images to train machine learning models. In this work, we revisit existing approaches for cross-modal recipe retrieval and propose a simplified end-to-end model based on well established and high performing encoders for text and images. We introduce a hierarchical recipe Transformer which attentively encodes individual recipe components (titles, ingredients and instructions). Further, we propose a self-supervised loss function computed on top of pairs of individual recipe components, which is able to leverage semantic relationships within recipes, and enables training using both image-recipe and recipe-only samples. We conduct a thorough analysis and ablation studies to validate our design choices. As a result, our proposed method achieves state-of-the-art performance in the cross-modal recipe retrieval task on the Recipe1M dataset. We make code and models publicly available 1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "2131286",
                    "name": "Erhan Gundogdu"
                },
                {
                    "authorId": "1809420",
                    "name": "Loris Bazzani"
                },
                {
                    "authorId": "1793476",
                    "name": "M. Donoser"
                }
            ]
        },
        {
            "paperId": "10cb854adb764c1e8c6bd73bb706b1ed59d929b7",
            "title": "RVOS: End-To-End Recurrent Network for Video Object Segmentation",
            "abstract": "Multiple object video object segmentation is a challenging task, specially for the zero-shot case, when no object mask is given at the initial frame and the model has to find the objects to be segmented along the sequence. In our work, we propose a Recurrent network for multiple object Video Object Segmentation (RVOS) that is fully end-to-end trainable. Our model incorporates recurrence on two different domains: (i) the spatial, which allows to discover the different object instances within a frame, and (ii) the temporal, which allows to keep the coherence of the segmented objects along time. We train RVOS for zero-shot video object segmentation and are the first ones to report quantitative results for DAVIS-2017 and YouTube-VOS benchmarks. Further, we adapt RVOS for one-shot video object segmentation by using the masks obtained in previous time steps as inputs to be processed by the recurrent module. Our model reaches comparable results to state-of-the-art techniques in YouTube-VOS benchmark and outperforms all previous video object segmentation methods not using online learning in the DAVIS-2017 benchmark. Moreover, our model achieves faster inference runtimes than previous methods, reaching 44ms/frame on a P100 GPU.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38478804",
                    "name": "Carles Ventura"
                },
                {
                    "authorId": "37923017",
                    "name": "Miriam Bellver"
                },
                {
                    "authorId": "40894420",
                    "name": "Andreu Girbau"
                },
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "34854725",
                    "name": "F. Marqu\u00e9s"
                },
                {
                    "authorId": "1398090762",
                    "name": "Xavier Gir\u00f3-i-Nieto"
                }
            ]
        },
        {
            "paperId": "16bc01018618b1ed01942561e6c28a26547aa956",
            "title": "Budget-aware Semi-Supervised Semantic and Instance Segmentation",
            "abstract": "Methods that move towards less supervised scenarios are key for image segmentation, as dense labels demand significant human intervention. Generally, the annotation burden is mitigated by labeling datasets with weaker forms of supervision, e.g. image-level labels or bounding boxes. Another option are semi-supervised settings, that commonly leverage a few strong annotations and a huge number of unlabeled/weakly-labeled data. In this paper, we revisit semi-supervised segmentation schemes and narrow down significantly the annotation budget (in terms of total labeling time of the training set) compared to previous approaches. With a very simple pipeline, we demonstrate that at low annotation budgets, semi-supervised methods outperform by a wide margin weakly-supervised ones for both semantic and instance segmentation. Our approach also outperforms previous semi-supervised works at a much reduced labeling cost. We present results for the Pascal VOC benchmark and unify weakly and semi-supervised approaches by considering the total annotation budget, thus allowing a fairer comparison between methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37923017",
                    "name": "Miriam Bellver"
                },
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "147166602",
                    "name": "Jordi Torres"
                },
                {
                    "authorId": "1398090762",
                    "name": "Xavier Gir\u00f3-i-Nieto"
                }
            ]
        },
        {
            "paperId": "8e93efe94dcd8d5de9c5efc7961c2448a6759b8c",
            "title": "Wav2Pix: Speech-conditioned Face Generation Using Generative Adversarial Networks",
            "abstract": "Speech is a rich biometric signal that contains information about the identity, gender and emotional state of the speaker. In this work, we explore its potential to generate face images of a speaker by conditioning a Generative Adversarial Network (GAN) with raw speech input. We propose a deep neural network that is trained from scratch in an end-to-end fashion, generating a face directly from the raw speech waveform without any additional identity information (e.g reference image or one-hot encoding). Our model is trained in a self-supervised approach by exploiting the audio and visual signals naturally aligned in videos. With the purpose of training from video data, we present a novel dataset collected for this work, with high-quality videos of youtubers with notable expressiveness in both the speech and visual signals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1381525702",
                    "name": "A. Duarte"
                },
                {
                    "authorId": "153738374",
                    "name": "Francisco Roldan"
                },
                {
                    "authorId": "118129922",
                    "name": "Miquel Tubau"
                },
                {
                    "authorId": "115672881",
                    "name": "Janna Escur"
                },
                {
                    "authorId": "2074982546",
                    "name": "Santiago Pascual"
                },
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "2890278",
                    "name": "Eva Mohedano"
                },
                {
                    "authorId": "145470864",
                    "name": "Kevin McGuinness"
                },
                {
                    "authorId": "147166602",
                    "name": "Jordi Torres"
                },
                {
                    "authorId": "1398090762",
                    "name": "Xavier Gir\u00f3-i-Nieto"
                }
            ]
        },
        {
            "paperId": "a572d976738579dd2d05c44555f726fc7fb1330e",
            "title": "Elucidating image-to-set prediction: An analysis of models, losses and datasets",
            "abstract": "In recent years, we have experienced a flurry of contributions in the multi-label classification literature. This problem has been framed under different perspectives, from predicting independent labels, to modeling label co-occurrences via architectural and/or loss function design. Despite great progress, it is still unclear which modeling choices are best suited to address this task, partially due to the lack of well defined benchmarks. Therefore, in this paper, we provide an in-depth analysis on five different computer vision datasets of increasing task complexity that are suitable for multi-label clasification (VOC, COCO, NUS-WIDE, ADE20k and Recipe1M). Our results show that (1) modeling label co-occurrences and predicting the number of labels that appear in the image is important, especially in high-dimensional output spaces; (2) carefully tuning hyper-parameters for very simple baselines leads to significant improvements, comparable to previously reported results; and (3) as a consequence of our analysis, we achieve state-of-the-art results on 3 datasets for which a fair comparison to previously published methods is feasible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2479987",
                    "name": "L. Pineda"
                },
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "3325894",
                    "name": "M. Drozdzal"
                },
                {
                    "authorId": "144290131",
                    "name": "Adriana Romero"
                }
            ]
        },
        {
            "paperId": "d3bebb7c790fa4c033a30fab6b846a845c2d0487",
            "title": "WiCV 2019: The Sixth Women In Computer Vision Workshop",
            "abstract": "In this paper we present the Women in Computer Vision Workshop - WiCV 2019, organized in conjunction with CVPR 2019. This event is meant for increasing the visibility and inclusion of women researchers in computer vision field. Computer vision and machine learning have made incredible progress over the past years, but the number of female researchers is still low both in the academia and in the industry. WiCV is organized especially for this reason: to raise visibility of female researchers, to increase collaborations between them, and to provide mentorship to female junior researchers in the field. In this paper, we present a report of trends over the past years, along with a summary of statistics regarding presenters, attendees, and sponsorship for the current workshop.",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702698",
                    "name": "Irene Amerini"
                },
                {
                    "authorId": "2064247403",
                    "name": "Elena Balashova"
                },
                {
                    "authorId": "27556211",
                    "name": "Sayna Ebrahimi"
                },
                {
                    "authorId": "2053372966",
                    "name": "Kathryn Leonard"
                },
                {
                    "authorId": "19263506",
                    "name": "Arsha Nagrani"
                },
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                }
            ]
        },
        {
            "paperId": "048d133e2ec513ce385c8e736df715d8ff496e17",
            "title": "Recipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images",
            "abstract": "In this paper, we introduce Recipe1M+, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M+ affords the ability to train high-capacity modelson aligned, multimodal data. Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M+ dataset and food and cooking in general. Code, data and models are publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "49230103",
                    "name": "Nick Hynes"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "2061199565",
                    "name": "Javier Mar\u00edn"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                }
            ]
        },
        {
            "paperId": "3dc820383a513da900809ce1cd18d6719496f17e",
            "title": "Inverse Cooking: Recipe Generation From Food Images",
            "abstract": "People enjoy food photography because they appreciate food. Behind each meal there is a story described in a complex recipe and, unfortunately, by simply looking at a food image we do not have access to its preparation process. Therefore, in this paper we introduce an inverse cooking system that recreates cooking recipes given food images. Our system predicts ingredients as sets by means of a novel architecture, modeling their dependencies without imposing any order, and then generates cooking instructions by attending to both image and its inferred ingredients simultaneously. We extensively evaluate the whole system on the large-scale Recipe1M dataset and show that (1) we improve performance w.r.t. previous baselines for ingredient prediction; (2) we are able to obtain high quality recipes by leveraging both image and ingredients; (3) our system is able to produce more compelling recipes than retrieval-based approaches according to human judgment. We make code and models publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "3325894",
                    "name": "M. Drozdzal"
                },
                {
                    "authorId": "1398090762",
                    "name": "Xavier Gir\u00f3-i-Nieto"
                },
                {
                    "authorId": "144290131",
                    "name": "Adriana Romero"
                }
            ]
        },
        {
            "paperId": "42f512d36722b09d1c83d328051badd374769fed",
            "title": "From Pixels to Object Sequences: Recurrent Semantic Instance Segmentation",
            "abstract": "We present a recurrent model for semantic instance segmentation that sequentially generates binary masks and their associated class probabilities for every object in an image. Our proposed system is trainable end-to-end from an input image to a sequence of labeled masks and, compared to methods relying on object proposals, does not require post-processing steps on its output. We study the suitability of our recurrent model on three different instance segmentation benchmarks, namely Pascal VOC 2012, CVPPP Plant Leaf Segmentation and Cityscapes. Further, we analyze the object sorting patterns generated by our model and observe that it learns to follow a consistent pattern, which correlates with the activations learned in the encoder part of our network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "37923017",
                    "name": "Miriam Bellver"
                },
                {
                    "authorId": "144059341",
                    "name": "V\u00edctor Campos"
                },
                {
                    "authorId": "47402054",
                    "name": "Manel Baradad"
                },
                {
                    "authorId": "34854725",
                    "name": "F. Marqu\u00e9s"
                },
                {
                    "authorId": "147166602",
                    "name": "Jordi Torres"
                },
                {
                    "authorId": "1398090762",
                    "name": "Xavier Gir\u00f3-i-Nieto"
                }
            ]
        }
    ]
}