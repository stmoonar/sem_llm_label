{
    "authorId": "72558235",
    "papers": [
        {
            "paperId": "1bb1606d09db36b2129355b44ca3b5fc0febd105",
            "title": "Diversity, Equity and Inclusion Activities in Database Conferences: A 2023 Report",
            "abstract": "The Diversity, Equity and Inclusion (DEI) initiative started as the Diversity/Inclusion initiative in 2020 [4]. The current report summarizes our activities in 2023.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "2302858948",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "1404555727",
                    "name": "Renata Borovica-Gajic"
                },
                {
                    "authorId": "2314293279",
                    "name": "Jes\u00fas Camacho-Rodr\u00edguez"
                },
                {
                    "authorId": "2314330906",
                    "name": "Jinli Cao"
                },
                {
                    "authorId": "2314297028",
                    "name": "Barbara Catania"
                },
                {
                    "authorId": "2249901748",
                    "name": "P. Chrysanthis"
                },
                {
                    "authorId": "2278429940",
                    "name": "Carlo Curino"
                },
                {
                    "authorId": "117266605",
                    "name": "A. El Abbadi"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2178387374",
                    "name": "Juliana Freire"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "51205357",
                    "name": "Sujaya Maiyya"
                },
                {
                    "authorId": "2266690266",
                    "name": "Alexandra Meliou"
                },
                {
                    "authorId": "37168010",
                    "name": "Madhulika Mohanty"
                },
                {
                    "authorId": "2257398736",
                    "name": "Fatma \u00d6zcan"
                },
                {
                    "authorId": "3139922",
                    "name": "L. Peterfreund"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "2314297947",
                    "name": "Sana Sellami"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2314670128",
                    "name": "Wang-Chiew Tan"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "2303255329",
                    "name": "Yuanyuan Tian"
                },
                {
                    "authorId": "1393643717",
                    "name": "Genoveva Vargas-Solar"
                },
                {
                    "authorId": "2314731432",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "2302886251",
                    "name": "Wenjie Zhang"
                }
            ]
        },
        {
            "paperId": "5457cf0fc58ee5a0b2cc8a847bf7141fc2c57c7d",
            "title": "Trustworthy Artificial Intelligence for Securing Transportation Systems",
            "abstract": "Artificial Intelligence (AI) techniques are being applied to numerous applications from Healthcare to Cyber Security to Finance. For example, Machine Learning (ML) algorithms are being applied to solve security problems such as malware analysis and insider threat detection. However, there are many challenges in applying ML algorithms for various applications. For example, (i) the ML algorithms may violate the privacy of individuals. This is because we can gather massive amounts of data and apply ML algorithms to the data to extract highly sensitive information. (ii) ML algorithms may show bias and be unfair to various segments of the population. (iii) ML algorithms themselves may be attacked possibly resulting in catastrophic errors including in cyber-physical systems such as transportation systems. Finally, (iv) the ML algorithms must be safe and not harm society. Therefore, when ML algorithms are applied to transportation systems for handling congestion, preventing accidents, and giving advice to drivers, we must ensure that they are secure, ensure privacy and fairness, as well as provide for the safe operation of the transportation systems. Other AY techniques such as Generative AI (GenAI) are also being applied not only to secure systems design but also to determine the attacks and potential solutions. This presentation is divided into two parts. First, we describe our research over the past decade on Trustworthy ML systems. These are systems that are secure as well as ensure privacy, fairness, and safety. We discuss our ensemble-based ML models for detecting attacks as well as our research on developing Adversarial Machine Learning techniques. We also discuss securing the Internet of Transportation systems that are based on traditional methods such as Extended Kalman Filters to detect cyberattacks. Second, Second, we discuss our work on Finally, we discuss the research we recently started as part of the USDOT National University Technology Center TraCR (Transportation Cybersecurity and Resiliency) led by Clemson University. In particular, we describe (i) the application of federated machine learning techniques for detecting attacks in transportation systems; (ii) publishing synthetic transportation data sets that preserve privacy, (iii) fairness algorithms for transportation systems, and (iv) examining how GenAI systems are being integrated with transportation systems to provide security. Our focus includes the following: \u00b7 Data Privacy: We are designing a Privacy-aware Policy-based Data Management Framework for Transportation Systems. Our work involves collecting the requisite data and developing analysis tools to identify and quantify privacy risks. Existing privacy-preserving, differentially private synthetic data generation techniques, which tailor data utility for generic ML accuracy, are not well suited for specific applications. We are developing synthetic data generation tools for transportation systems applications. We will develop new ML algorithms that can leverage these datasets. \u00b7 Fairness: We have developed a novel adaptive fairness-aware online meta-learning algorithm, FairSAOML, which adapts to changing environments in both bias control and model precision. Our current work is focusing on adapting our framework to fairness in transportation systems. and control bias over time, especially ensuring group fairness across different protected sub-populations; identifying interesting attributes using explainable AI techniques that might help to mitigate bias and develop equitable algorithms. We have also developed a second system, FairDolce, that recognizes objects involving fairness constraints in a changing environment. We are adapting it to transportation applications. For example, pedestrian detection (whether or not the object being seen is a pedestrian) must be fair with respect to the race or gender of the individuals being detected under changing environments (e.g., rainy, cloudy sunny). Adversarial ML: Our prior work on adversarial ML models worked on traditional datasets such as network traffic data. Our current focus is on adapting our approach to AV-based sensor data. Our ML models are being applied to sensor data for object recognition and traffic management. These ML models may be attacked by the adversary. We will study various attack models and investigate ways of how interactions may occur between the model and the adversary and subsequently develop appropriate adversarial ML models that operate on the AV sensor data. \u00b7 Attack Detection - Smart vehicles are often exposed to various attacks making it difficult for manufacturers to collaboratively train anomaly/attack detection models. Yet it would be ideal if all the data available across manufacturers could be used in building robust attack detection systems. To achieve this, we developed FAST-SV, which incorporates federated learning in conjunction with augmentation techniques to build a highly performant attack detection system for smart cars. Safety: Safety has been studied for cyber-physical systems and formal methods have been applied to specify safety properties and subsequently verify that the system satisfies the specifications. However, our goal is to ensure that the ML algorithms utilized by the transportation systems are safe. This would involve developing an AI Governance framework that would require transparency and explainability (among others) of the ML algorithms utilized by the transportation system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                }
            ]
        },
        {
            "paperId": "7543d88f028f149e66d651ee07def4dd675ce0eb",
            "title": "Heterogeneous Domain Adaptation for Multistream Classification on Cyber Threat Data",
            "abstract": "Under a newly introduced setting of multistream classification, two data streams are involved, which are referred to as source and target streams. The source stream continuously generates data instances from a certain domain with labels, while the target stream does the same task without labels from another domain. Existing approaches assume that domains for both data streams are identical, which is not quite true, since data streams from different sources may contain distinct features. Indeed, they may even have different numbers of features. Furthermore, obtaining labels for every instance in a data stream is often expensive and time-consuming. Therefore, it has become an important topic to explore if classes of labeled instances from other related streams are helpful to predict the classes of unlabeled instances in a different stream. Note that domains of source and target streams may have distinct feature spaces and data distributions. Our objective is to predict class labels of data instances in the target stream by using the classifiers trained by the source stream. We propose a framework of multistream classification by using projected data from a common latent feature space, which is embedded from both source and target domains. This framework is also crucial for enterprise system defenders to detect cross-platform attacks, such as Advanced Persistent Threats (APTs). Empirical valuation and analysis on both real-world and synthetic datasets are performed to validate the effectiveness of our proposed algorithm, comparing to state-of-the-art techniques. Experimental results show that our approach significantly outperforms other existing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144893963",
                    "name": "Yifan Li"
                },
                {
                    "authorId": "2145973487",
                    "name": "Yang Gao"
                },
                {
                    "authorId": "39712836",
                    "name": "G. Ayoade"
                },
                {
                    "authorId": "1685603",
                    "name": "L. Khan"
                },
                {
                    "authorId": "1889355",
                    "name": "A. Singhal"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                }
            ]
        },
        {
            "paperId": "7ce9d5c3cbae1e2ac6f51ae8b70f3bd50bbe3b98",
            "title": "An Education Program for Big Data Security and Privacy",
            "abstract": "This paper describes the course that we developed at the University of Texas at Dallas on Big Data Security and Privacy with funding from the National Science Foundation. The project started in 2017 and was completed in 2023. We have taught the course starting Fall 2020, Fall 2021, Fall 2022 and we are teaching it for the fourth time in Fall 2023. This is an extremely popular course and we limit the enrollment to 80 students. Students have some background in cyber security and big data analytics. This paper provides an overview of the organization of the course, the course content, and learning outcomes and the evaluation carried out. The members of the project include those who have expertise in (i) Cyber Security, (ii) Big Data Analytics, and (iii) Education Research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "2308297955",
                    "name": "Kim Nimon"
                },
                {
                    "authorId": "2270997533",
                    "name": "Latifur Khan"
                }
            ]
        },
        {
            "paperId": "a1c887384d1b1ff1000e39460a14f698eed48498",
            "title": "Utilizing Threat Partitioning for More Practical Network Anomaly Detection",
            "abstract": "Anomaly-based network intrusion detection would appear on the surface to be ideal for detection of zero-day network threats. Yet in practice, their often unacceptably high false positive rates keep them on the sideline in favor of signature-based methods, which typically detect known threats. We argue that an anomaly-based network intrusion detection system should not only be specialized to a specific class of related threats, but characteristics of the threat class itself should be utilized when designing both the detection system and structuring the network data to use with the system. To this end, we take two common network threat classes, DDoS-as-a-Smokescreen (DaaSS) and SYN flood, and analyze their characteristics for structure that we can use to specialize anomaly detection. We partition these threat classes into known behavior and unknown behavior, leaving the latter open-ended. Through experimentation on multiple datasets, we show that our proposed detection system based on this threat partitioning approach is capable of detecting DaaSS attacks and zero-day SYN flood variants with very low false positive rates, even in the face of concept drift, and can do so without having to collect large amounts of benign network traffic for training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308103313",
                    "name": "Brian Ricks"
                },
                {
                    "authorId": "2308103394",
                    "name": "Patrick Tague"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "143927103",
                    "name": "S. Natarajan"
                }
            ]
        },
        {
            "paperId": "1c1372875a9f9ee67021ef5f665757352dc3cdf0",
            "title": "Blockchain-Enabled Service Optimizations in Supply Chain Digital Twin",
            "abstract": "Digital twin is considered an alternative for optimizing real-world performance within virtual context, which also applies to the optimization of Supply Chain Management (SCM). Blockchain, which facilitates data secure storage and trusted tracking, is deemed to be a proper assistant technology for achieving digital twin implementation. In this work, we propose a blockchain-based digital twin solution to reengineer SCM system, which promotes the digitization and intelligence of SCM to fit in massive service volumes in complex-intercrossed industry system. A strong-weak consensus mode is developed to achieve energy and time savings. We also design intelligent switch-based algorithms to generate time-saving consensus plans under energy constraints. Finally, we set up multiple experiments to compare our algorithm with three baseline algorithms, including Effective Iterative Greedy (EIG), Two Dimensional Genetic (TDG), and High-level Task Scheduling Dynamic Programming (HTSDP). Findings from evaluation demonstrate the potential of our proposed model. Specifically, our algorithm reduces time and energy consumption of EIG algorithm in consensus by 46.84% and 16.25%, respectively. Compared with TDG algorithm, consensus time and energy consumption of our algorithm are reduced by 50.05% and 48.46%. Our algorithm cuts down time spent of HTSDP algorithm in generating consensus plan by a factor of 9.88.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2529746",
                    "name": "Keke Gai"
                },
                {
                    "authorId": "2145912955",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "144326259",
                    "name": "Meikang Qiu"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                }
            ]
        },
        {
            "paperId": "2b3281dd0c951f2aac751183a3a8e31892ed14a6",
            "title": "Con2Mix: A semi-supervised method for imbalanced tabular security data",
            "abstract": "Con2Mix (Contrastive Double Mixup) is a new semi-supervised learning methodology that innovates a triplet mixup data augmentation approach for finding code vulnerabilities in imbalanced, tabular security data sets. Tabular data sets in cybersecurity domains are widely known to pose challenges for machine learning because of their heavily imbalanced data (e.g., a small number of labeled attack samples buried in a sea of mostly benign, unlabeled data). Semi-supervised learning leverages a small subset of labeled data and a large subset of unlabeled data to train a learning model. While semi-supervised methods have been well studied in image and language domains, in security domains they remain underutilized, especially on tabular security data sets which pose especially difficult contextual information loss and balance challenges for machine learning. Experiments applying Con2Mix to collected security data sets show promise for addressing these challenges, achieving state-of-the-art performance on two evaluated data sets compared with other methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118899428",
                    "name": "Xiaodi Li"
                },
                {
                    "authorId": "145155297",
                    "name": "L. Khan"
                },
                {
                    "authorId": "2057773238",
                    "name": "Mahmoud Zamani"
                },
                {
                    "authorId": "2176238014",
                    "name": "Shamila Wickramasuriya"
                },
                {
                    "authorId": "3071249",
                    "name": "Kevin W. Hamlen"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                }
            ]
        },
        {
            "paperId": "33720237a0a6dbcb592f652d666834cc4e54f208",
            "title": "An Automated Vulnerability Detection Framework for Smart Contracts",
            "abstract": "With the increase of the adoption of blockchain technology in providing decentralized solutions to various problems, smart contracts have become more popular to the point that billions of US Dollars are currently exchanged every day through such technology. Meanwhile, various vulnerabilities in smart contracts have been exploited by attackers to steal cryptocurrencies worth millions of dollars. The automatic detection of smart contract vulnerabilities therefore is an essential research problem. Existing solutions to this problem particularly rely on human experts to define features or different rules to detect vulnerabilities. However, this often causes many vulnerabilities to be ignored, and they are inefficient in detecting new vulnerabilities. In this study, to overcome such challenges, we propose a framework to automatically detect vulnerabilities in smart contracts on the blockchain. More specifically, first, we utilize novel feature vector generation techniques from bytecode of smart contract since the source code of smart contracts are rarely available in public. Next, the collected vectors are fed into our novel metric learning-based deep neural network(DNN) to get the detection result. We conduct comprehensive experiments on large-scale benchmarks, and the quantitative results demonstrate the effectiveness and efficiency of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2058930366",
                    "name": "Feng Mi"
                },
                {
                    "authorId": "36681034",
                    "name": "Chenxu Zhao"
                },
                {
                    "authorId": "2108367541",
                    "name": "Zhuoyi Wang"
                },
                {
                    "authorId": "2173990557",
                    "name": "Sadaf Md. Halim"
                },
                {
                    "authorId": "2118899428",
                    "name": "Xiaodi Li"
                },
                {
                    "authorId": "101145602",
                    "name": "Zhouxiang Wu"
                },
                {
                    "authorId": "1685603",
                    "name": "L. Khan"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                }
            ]
        },
        {
            "paperId": "375fe784605688922a0c73f3d390fde32613b173",
            "title": "SAFE-PASS: Stewardship, Advocacy, Fairness and Empowerment in Privacy, Accountability, Security, and Safety for Vulnerable Groups",
            "abstract": "Our vision is to achieve societally responsible secure and trustworthy cyberspace that puts algorithmic and technological checks and balances on the indiscriminate sharing and analysis of data. We achieve this vision in a holistic manner by framing research directions with four major considerations: (i) Expanding knowledge and understanding of security and privacy perceptions and expectations in vulnerable groups, which significantly contribute to their unwillingness to share data, and use that knowledge to drive research in (a) mitigating missing/imbalanced data problems, (b) understanding and modeling security and privacy risks of data sharing, and (c) modeling utility of data sharing. (ii) Developing a risk-adaptive, policy model capable of capturing and articulating security and privacy expectations of users that are relevant in a particular context and develops associated technology to ensure provenance and accountability. (iii) Developing robust AI/ML algorithms that are transparent and explainable with respect to fairness and bias to reduce/eliminate discrimination, misuse, privacy violations, or other cyber-crimes. (iv) Developing models and techniques for a nuanced, contextually adaptive, and graded privacy paradigm that allows trade-offs between privacy and utility. Towards this, in this paper we present the SAFE-PASS framework to provide Stewardship, Advocacy, Fairness and Empowerment in Privacy, Accountability, Security, and Safety for Vulnerable Groups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257932408",
                    "name": "Indrajit Ray"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "145033630",
                    "name": "Jaideep Vaidya"
                },
                {
                    "authorId": "144156242",
                    "name": "S. Mehrotra"
                },
                {
                    "authorId": "145138841",
                    "name": "V. Atluri"
                },
                {
                    "authorId": "2240174870",
                    "name": "Indrakshi Ray"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "2207842509",
                    "name": "R. Raskar"
                },
                {
                    "authorId": "2124624117",
                    "name": "Babak Salimi"
                },
                {
                    "authorId": "2226780760",
                    "name": "Steve Simske"
                },
                {
                    "authorId": "1732742",
                    "name": "N. Venkatasubramanian"
                },
                {
                    "authorId": "2151448490",
                    "name": "Vivek K. Singh"
                }
            ]
        },
        {
            "paperId": "6256b0c7f2381ccca8eb52b11bfae3c3e66b5227",
            "title": "Attack Some while Protecting Others: Selective Attack Strategies for Attacking and Protecting Multiple Concepts",
            "abstract": "Machine learning models are vulnerable to adversarial attacks. Existing research focuses on attack-only scenarios. In practice, one dataset may be used for learning different concepts, and the attacker may be incentivized to attack some concepts but protect the others. For example, the attacker might tamper a profile image for the \"age'' model to predict \"young'', while the \"attractiveness'' model still predicts \"pretty''. In this work, we empirically demonstrate that attacking the classifier for one learning task may negatively impact classifiers learning other tasks on the same data. This raises an interesting research question: is it possible to attack one set of classifiers while protecting the others trained on the same data? Answers to the above question have interesting implications for the complexity of test-time attacks against learning models, such as avoiding the violation of logical constraints. For example, attacks on images of high school students should not cause these images to be classified as a group of 30-year-old. Such misclassification of age may raise alarms and may easily expose the attacks. In this paper, we address the research question by developing novel attack techniques that can simultaneously attack one set of learning models while protecting the other. In the case of linear classifiers, we provide a theoretical framework for finding an optimal solution to generating such adversarial examples. Using this theoretical framework, we develop a \"multi-concept'' attack strategy in the context of deep learning tasks. Our results demonstrate that our techniques can successfully attack the target classes while protecting the \"protected'' classes in many different settings, which is not possible with the existing test-time attack-only strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2045079863",
                    "name": "Vibha Belavadi"
                },
                {
                    "authorId": "2150920617",
                    "name": "Yan Zhou"
                },
                {
                    "authorId": "2243292428",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                }
            ]
        }
    ]
}