{
    "authorId": "1781697",
    "papers": [
        {
            "paperId": "16e4c481fc6827f4661ac6633278cda0bd303df8",
            "title": "ScreenTK: Seamless Detection of Time-Killing Moments Using Continuous Mobile Screen Text and on-device LLM",
            "abstract": "Smartphones have become essential to people's digital lives, providing a continuous stream of information and connectivity. However, this constant flow can lead to moments where users are simply passing time rather than engaging meaningfully. This underscores the importance of developing methods to identify these\"time-killing\"moments, enabling the delivery of important notifications in a way that minimizes interruptions and enhances user engagement. Recent work has utilized screenshots taken every 5 seconds to detect time-killing activities on smartphones. However, this method often misses to capture phone usage between intervals. We demonstrate that up to 50% of time-killing instances go undetected using screenshots, leading to substantial gaps in understanding user behavior. To address this limitation, we propose a method called ScreenTK that detects time-killing moments by leveraging continuous screen text monitoring and on-device large language models (LLMs). Screen text contains more comprehensive information than screenshots and allows LLMs to summarize detailed phone usage. To verify our framework, we conducted experiments with six participants, capturing 1,034 records of different time-killing moments. Initial results show that our framework outperforms state-of-the-art solutions by 38% in our case study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309734890",
                    "name": "Le Fang"
                },
                {
                    "authorId": "2309704231",
                    "name": "Shiquan Zhang"
                },
                {
                    "authorId": "2309754026",
                    "name": "Hong Jia"
                },
                {
                    "authorId": "2309476061",
                    "name": "Jorge Gon\u00e7alves"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                }
            ]
        },
        {
            "paperId": "1f0ce4623345b00c18b7725b220de73170658f6f",
            "title": "Mobile Near-infrared Sensing\u2014A Systematic Review on Devices, Data, Modeling, and Applications",
            "abstract": "Mobile near-infrared sensing is becoming an increasingly important method in many research and industrial areas. To help consolidate progress in this area, we use the PRISMA guidelines to conduct a systematic review of mobile near-infrared sensing, including (1) existing prototypes and commercial products, (2) data collection techniques, (3) machine learning methods, and (4) relevant application areas. Our work measures historical and current trends and identifies current challenges and future directions for this emerging topic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152123071",
                    "name": "Weiwei Jiang"
                },
                {
                    "authorId": "144769817",
                    "name": "Jorge Gon\u00e7alves"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                }
            ]
        },
        {
            "paperId": "3589418425b500c5910397b21e823a8ab26dbba3",
            "title": "AutoJournaling: A Context-Aware Journaling System Leveraging MLLMs on Smartphone Screenshots",
            "abstract": "Journaling offers significant benefits, including fostering self-reflection, enhancing writing skills, and aiding in mood monitoring. However, many people abandon the practice because traditional journaling is time-consuming, and detailed life events may be overlooked if not recorded promptly. Given that smartphones are the most widely used devices for entertainment, work, and socialization, they present an ideal platform for innovative approaches to journaling. Despite their ubiquity, the potential of using digital phenotyping, a method of unobtrusively collecting data from digital devices to gain insights into psychological and behavioral patterns, for automated journal generation has been largely underexplored. In this study, we propose AutoJournaling, the first-of-its-kind system that automatically generates journals by collecting and analyzing screenshots from smartphones. This system captures life events and corresponding emotions, offering a novel approach to digital phenotyping. We evaluated AutoJournaling by collecting screenshots every 3 seconds from three students over five days, demonstrating its feasibility and accuracy. AutoJournaling is the first framework to utilize seamlessly collected screenshots for journal generation, providing new insights into psychological states through digital phenotyping.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2311284338",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "2309704231",
                    "name": "Shiquan Zhang"
                },
                {
                    "authorId": "2309734890",
                    "name": "Le Fang"
                },
                {
                    "authorId": "2309754026",
                    "name": "Hong Jia"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                },
                {
                    "authorId": "2190065869",
                    "name": "Simon D'Alfonso"
                }
            ]
        },
        {
            "paperId": "838e9c6f8406ef2b8a742d55a7dcf57044b992fe",
            "title": "A Tool for Capturing Smartphone Screen Text",
            "abstract": "Context sensing on smartphones is often used to understand user behaviour. Amongst the many available sensors, the collection of text is crucial due to its richness. However, previous work has been limited to collecting text only from keyboard input, or intermittently collecting screen text indirectly by taking screenshots and applying optical character recognition. Here, we present a novel software sensor that unobtrusively and continuously captures all screen text on smartphones. We conducted a validation study with 21 participants over a two-week period, where they used our software on their personal smartphones. Our findings demonstrate how data from our sensor can be used to understand user behaviour and categorise mobile apps. We also show how smartphone sensing can be enhanced by using our sensor in conjunction with other sensors. We discuss the strengths and limitations of our sensor, highlighting potential areas for improvement and providing recommendations for its use.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301055633",
                    "name": "Songyan Teng"
                },
                {
                    "authorId": "2190065869",
                    "name": "Simon D'Alfonso"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                }
            ]
        },
        {
            "paperId": "91469479b856ae0f04a4cd8da57d8a4fbae5ec66",
            "title": "Predicting Affective States from Screen Text Sentiment",
            "abstract": "The proliferation of mobile sensing technologies has enabled the study of various physiological and behavioural phenomena through unobtrusive data collection from smartphone sensors. This approach offers real-time insights into individuals' physical and mental states, creating opportunities for personalised treatment and interventions. However, the potential of analysing the textual content viewed on smartphones to predict affective states remains underexplored. To better understand how the screen text that users are exposed to and interact with can influence their affects, we investigated a subset of data obtained from a digital phenotyping study of Australian university students conducted in 2023. We employed linear regression, zero-shot, and multi-shot prompting using a large language model (LLM) to analyse relationships between screen text and affective states. Our findings indicate that multi-shot prompting substantially outperforms both linear regression and zero-shot prompting, highlighting the importance of context in affect prediction. We discuss the value of incorporating textual and sentiment data for improving affect prediction, providing a basis for future advancements in understanding smartphone use and wellbeing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301055633",
                    "name": "Songyan Teng"
                },
                {
                    "authorId": "2311284338",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "2190065869",
                    "name": "Simon D'Alfonso"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                }
            ]
        },
        {
            "paperId": "a5e16f0b30ba3295db02a162e68b95d14443fc83",
            "title": "Efficient and Personalized Mobile Health Event Prediction via Small Language Models",
            "abstract": "Healthcare monitoring is crucial for early detection, timely intervention, and the ongoing management of health conditions, ultimately improving individuals' quality of life. Recent research shows that Large Language Models (LLMs) have demonstrated impressive performance in supporting healthcare tasks. However, existing LLM-based healthcare solutions typically rely on cloud-based systems, which raise privacy concerns and increase the risk of personal information leakage. As a result, there is growing interest in running these models locally on devices like mobile phones and wearables to protect users' privacy. Small Language Models (SLMs) are potential candidates to solve privacy and computational issues, as they are more efficient and better suited for local deployment. However, the performance of SLMs in healthcare domains has not yet been investigated. This paper examines the capability of SLMs to accurately analyze health data, such as steps, calories, sleep minutes, and other vital statistics, to assess an individual's health status. Our results show that, TinyLlama, which has 1.1 billion parameters, utilizes 4.31 GB memory, and has 0.48s latency, showing the best performance compared other four state-of-the-art (SOTA) SLMs on various healthcare applications. Our results indicate that SLMs could potentially be deployed on wearable or mobile devices for real-time health monitoring, providing a practical solution for efficient and privacy-preserving healthcare.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2323542492",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2316051686",
                    "name": "Ting Dang"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                },
                {
                    "authorId": "2316274006",
                    "name": "Hong Jia"
                }
            ]
        },
        {
            "paperId": "bdbaa3c801d8ee0576986a27e5decc73f0ff9b4d",
            "title": "Enabling On-Device LLMs Personalization with Smartphone Sensing",
            "abstract": "This demo presents a novel end-to-end framework that combines on-device large language models (LLMs) with smartphone sensing technologies to achieve context-aware and personalized services. The framework addresses critical limitations of current personalization solutions via cloud LLMs, such as privacy concerns, latency and cost, and limited personal information. To achieve this, we innovatively proposed deploying LLMs on smartphones with multimodal sensor data through context-aware sensing and customized prompt engineering, ensuring privacy and enhancing personalization performance. A case study involving a university student demonstrated the capability of the framework to provide tailored recommendations. In addition, we show that the framework achieves the best trade-off in privacy, performance, latency, cost, battery and energy consumption between on-device and cloud LLMs. To the best of our knowledge, this is the first framework to provide on-device LLMs personalization with smartphone sensing. Future work will incorporate more diverse sensor data and involve extensive user studies to enhance personalization. Our proposed framework has the potential to substantially improve user experiences across domains including healthcare, productivity, and entertainment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309704231",
                    "name": "Shiquan Zhang"
                },
                {
                    "authorId": "2310671910",
                    "name": "Ying Ma"
                },
                {
                    "authorId": "2309734890",
                    "name": "Le Fang"
                },
                {
                    "authorId": "2309754026",
                    "name": "Hong Jia"
                },
                {
                    "authorId": "2190065869",
                    "name": "Simon D'Alfonso"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                }
            ]
        },
        {
            "paperId": "c54e153e1d0f2e2423f46094ed1e3b83a19a0aee",
            "title": "OOBKey: Key Exchange with Implantable Medical Devices Using Out-Of-Band Channels",
            "abstract": "Implantable Medical Devices (IMDs) are widely deployed today and often use wireless communication. Establishing a secure communication channel to these devices is vital, however, also challenging in practice. To address this issue, numerous researchers have proposed IMD key exchange protocols, in particular ones that leverage an Out-Of-Band (OOB) channel such as audio, vibration and physiological signals. These solutions have advantages over traditional key exchange, e.g., their plug-and-play nature. However, such protocols are often constructed in an ad-hoc manner and lack stringent evaluation of their security, usability and deployability properties. In this paper, we systematize this area and derive a solid theoretical footing to compare different OOB-based approaches. We review related work in that light and show the shortcomings of previous approaches. We then make the core observation that security imperfections in OOB channels can be mitigated by incorporating password-authenticated key agreement. Accordingly, we propose a new construction for OOB key exchange and formalize the security level. We then derive three protocols from it that only require an inertial sensor in the IMD, which is already available in advanced devices. We analyze those protocols with the proposed formalism to highlight shortcomings and advantages depending on specific practical scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132112600",
                    "name": "Mo Zhang"
                },
                {
                    "authorId": "38955274",
                    "name": "Eduard Marin"
                },
                {
                    "authorId": "144828344",
                    "name": "David F. Oswald"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                },
                {
                    "authorId": "2165161334",
                    "name": "Mark Ryan"
                },
                {
                    "authorId": "2575168",
                    "name": "Benjamin Tag"
                },
                {
                    "authorId": "2226725",
                    "name": "Kleomenis Katevas"
                }
            ]
        },
        {
            "paperId": "e235e869ba3559d5a47e2ed45a524cc9c160f1e7",
            "title": "AI-Driven Mediation Strategies for Audience Depolarisation in Online Debates",
            "abstract": "Online polarisation can tear the fabric of civility through reinforcing social media\u2019s perceptions of division and discord. Social media platforms often rely on content-moderation to combat polarisation, contingent on the reactive removal or flagging of content. However, this approach often remains agnostic of the underlying debate\u2019s ideas and stifles open discourse. In this study, we use prompt-tuned language models to mediate social media debates, applying the strategies of the Thomas-Kilmann Conflict Mode Instrument (TKI). We evaluate multiple mediation strategies in providing targeted responses to the debates, as shown to a debate audience. Our findings show that high-cooperativeness TKI strategies offered more persuasive arguments, while an accommodating argument strategy was the most successful at depolarising the audience\u2019s opinion. Furthermore, high-cooperativeness strategies also increased the perception that the debaters will reach a consensus. Our work paves the way for scalable and personalised tools that mediate social media debates to encourage depolarisation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2203245199",
                    "name": "Jarod Govers"
                },
                {
                    "authorId": "2298908587",
                    "name": "Eduardo Velloso"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                },
                {
                    "authorId": "2297296238",
                    "name": "Jorge Goncalves"
                }
            ]
        },
        {
            "paperId": "34b1d9a291a761d02963275fb266ee7f2bb56e3d",
            "title": "Knock the Reality: Virtual Interface Registration in Mixed Reality",
            "abstract": "We present Knock the Reality, an interaction technique for virtual interface registration in mixed reality (MR). When a user knocks on a physical object, our technique identifies the object based on a knocking sound and registers a customizable virtual interface onto the object. Unlike computer vision-based methods, our approach does not require continuously processing image information. Instead, we utilize audio features which are less computationally expensive. This work presents our implementation and demonstrates an interaction scenario where a user works in MR. Overall, our method offers a simple and intuitive way to register MR interfaces.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152123071",
                    "name": "Weiwei Jiang"
                },
                {
                    "authorId": "51254486",
                    "name": "Difeng Yu"
                },
                {
                    "authorId": "1967200",
                    "name": "Andrew Irlitti"
                },
                {
                    "authorId": "2057296501",
                    "name": "Jorge Gon\u00e7alves"
                },
                {
                    "authorId": "1781697",
                    "name": "V. Kostakos"
                },
                {
                    "authorId": "2116554261",
                    "name": "Xin He"
                }
            ]
        }
    ]
}