{
    "authorId": "3083253",
    "papers": [
        {
            "paperId": "c17b71f31fa708eb01310ff65ab660f2676a12a1",
            "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
            "abstract": "By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge. To this end, we propose Knowledge Card, a modular framework to plug in new factual and relevant knowledge into general-purpose LLMs. We first introduce knowledge cards -- specialized language models trained on corpora from specific domains and sources. Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM. We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs. Finally, we propose two complementary integration approaches to augment the base LLM with the (relevant, factual) knowledge curated from the specialized LMs. Through extensive experiments, we demonstrate that Knowledge Card achieves state-of-the-art performance on six benchmark datasets. Ultimately, Knowledge Card framework enables dynamic synthesis and updates of knowledge from diverse domains. Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "3040379",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2170130468",
                    "name": "Yuyang Bai"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "df2beaae63e4d68ef8e762bcd4704c9f11f856d9",
            "title": "Can Language Models Solve Graph Problems in Natural Language?",
            "abstract": "Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question. The NLGraph benchmark and evaluation code are available at https://github.com/Arthur-Heng/NLGraph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256778370",
                    "name": "Heng Wang"
                },
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "40500540",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "29e7fde2f1383b20c01a80c027a7cb208472d3ed",
            "title": "Controlling the Focus of Pretrained Language Generation Models",
            "abstract": "The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. However, there does not exist a mechanism to directly control the model\u2019s focus. This work aims to develop a control mechanism by which a user can select spans of context as \u201chighlights\u201d for the model to focus on, and generate relevant output. To achieve this goal, we augment a pretrained model with trainable \u201cfocus vectors\u201d that are directly applied to the model\u2019s embeddings, while the model itself is kept fixed. These vectors, trained on automatic annotations derived from attribution methods, act as indicators for context importance. We test our approach on two core generation tasks: dialogue response generation and abstractive summarization. We also collect evaluation data where the highlight-generation pairs are annotated by humans. Our experiments show that the trained focus vectors are effective in steering the model to generate outputs that are relevant to user-selected highlights.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111812169",
                    "name": "Jiabao Ji"
                },
                {
                    "authorId": "38367242",
                    "name": "Yoon Kim"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                },
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                }
            ]
        },
        {
            "paperId": "6494c6149e5036c09ee92da9fd67cbecc998a52f",
            "title": "PCFG-Based Natural Language Interface Improves Generalization for Controlled Text Generation",
            "abstract": "Existing work on controlled text generation (CTG) assumes a control interface of categorical attributes. In this work, we propose a natural language (NL) interface, where we craft a PCFG to embed the control attributes into natural language commands, and propose variants of existing CTG models that take commands as input. In our experiments, we design tailored setups to test the model\u2019s generalization abilities. We find our PCFG-based command generation approach is effective for handling unseen commands compared to fix-set templates. Further, our proposed NL models can effectively generalize to unseen attributes (a new ability enabled by the NL interface), as well as unseen attribute combinations. Interestingly, in model comparisons, the simple conditional generation approach, enhanced with our proposed NL interface, is shown to be a strong baseline in those challenging settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                },
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                }
            ]
        },
        {
            "paperId": "6e0b6ba5cae954a0643baeb00167965e88458fc3",
            "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
            "abstract": "In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github.com/cloudygoose/blindspot_nlg.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2155412834",
                    "name": "Tianle Wang"
                },
                {
                    "authorId": "51467955",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "6fc4a39bb4697a21286bb1cf503ecf17407aeae2",
            "title": "An Empirical Study on Few-shot Knowledge Probing for Pretrained Language Models",
            "abstract": "Prompt-based knowledge probing for 1-hop relations has been used to measure how much world knowledge is stored in pretrained language models. Existing work uses considerable amounts of data to tune the prompts for better performance. In this work, we compare a variety of approaches under a few-shot knowledge probing setting, where only a small number (e.g., 10 or 20) of example triples are available. In addition, we create a new dataset named TREx-2p, which contains 2-hop relations. We report that few-shot examples can strongly boost the probing performance for both 1-hop and 2-hop relations. In particular, we find that a simple-yet-effective approach of finetuning the bias vectors in the model outperforms existing prompt-engineering methods. Our dataset and code are available at \\url{https://github.com/cloudygoose/fewshot_lama}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2111049203",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                }
            ]
        },
        {
            "paperId": "91fed2f5718b9c1dc25b768c8aaf33455f0404c8",
            "title": "Joint Energy-based Model Training for Better Calibrated Natural Language Understanding Models",
            "abstract": "In this work, we explore joint energy-based model (EBM) training during the finetuning of pretrained text encoders (e.g., Roberta) for natural language understanding (NLU) tasks. Our experiments show that EBM training can help the model reach a better calibration that is competitive to strong baselines, with little or no loss in accuracy. We discuss three variants of energy functions (namely scalar, hidden, and sharp-hidden) that can be defined on top of a text encoder, and compare them in experiments. Due to the discreteness of text data, we adopt noise contrastive estimation (NCE) to train the energy-based model. To make NCE training more effective, we train an auto-regressive noise model with the masked language model (MLM) objective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "143775536",
                    "name": "Bryan McCann"
                },
                {
                    "authorId": "2228109",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "1401953786",
                    "name": "Ehsan Hosseini-Asl"
                }
            ]
        },
        {
            "paperId": "fa79334396c6f16eb928f3fda37be5e887a8ac4d",
            "title": "Revisiting Latent-Space Interpolation via a Quantitative Evaluation Framework",
            "abstract": "Latent-space interpolation is commonly used to demonstrate the generalization ability of deep latent variable models. Various algorithms have been proposed to calculate the best trajectory between two encodings in the latent space. In this work, we show how data labeled with semantically continuous attributes can be utilized to conduct a quantitative evaluation of latent-space interpolation algorithms, for variational autoencoders. Our framework can be used to complement the standard qualitative comparison, and also enables evaluation for domains (such as graph) in which the visualization is difficult. Interestingly, our experiments reveal that the superiority of interpolation algorithms could be domain-dependent. While normalised interpolation works best for the image domain, spherical linear interpolation achieves the best performance in the graph domain. Next, we propose a simple-yet-effective method to restrict the latent space via a bottleneck structure in the encoder. We find that all interpolation algorithms evaluated in this work can benefit from this restriction. Finally, we conduct interpolation-aware training with the labeled attributes, and show that this explicit supervision can improve the interpolation performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057207935",
                    "name": "Lu Mi"
                },
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2140350688",
                    "name": "Core Francisco Park"
                },
                {
                    "authorId": "2144222919",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2118462083",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2613669",
                    "name": "N. Shavit"
                }
            ]
        },
        {
            "paperId": "28168223b7635c205d615441cdaf609ddf370558",
            "title": "Constructing a Knowledge Graph from Unstructured Documents without External Alignment",
            "abstract": "Knowledge graphs (KGs) are relevant to many NLP tasks, but building a reliable domain-specific KG is time-consuming and expensive. A number of methods for constructing KGs with minimized human intervention have been proposed, but still require a process to align into the human-annotated knowledge base. To overcome this issue, we propose a novel method to automatically construct a KG from unstructured documents that does not require external alignment and explore its use to extract desired information. To summarize our approach, we first extract knowledge tuples in their surface form from unstructured documents, encode them using a pre-trained language model, and link the surface-entities via the encoding to form the graph structure. We perform experiments with benchmark datasets such as WikiMovies and MetaQA. The experimental results show that our method can successfully create and search a KG with 18K documents and achieve 69.7% hits@10 (close to an oracle model) on a query retrieval task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1885974",
                    "name": "Seunghak Yu"
                },
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                }
            ]
        },
        {
            "paperId": "9b713f5ab3429586f773557afa58a931fba997c1",
            "title": "A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation",
            "abstract": "This work studies the widely adopted ancestral sampling algorithms for auto-regressive language models. We use the quality-diversity (Q-D) trade-off to investigate three popular sampling methods (top-k, nucleus and tempered sampling). We focus on the task of open-ended language generation, and first show that the existing sampling algorithms have similar performance. By carefully inspecting the transformations defined by different sampling algorithms, we identify three key properties that are shared among them: entropy reduction, order preservation, and slope preservation. To validate the importance of the identified properties, we design two sets of new sampling methods: one set in which each algorithm satisfies all three properties, and one set in which each algorithm violates at least one of the properties. We compare their performance with existing algorithms, and find that violating the identified properties could lead to drastic performance degradation, as measured by the Q-D trade-off. On the other hand, we find that the set of sampling algorithms that satisfy these properties performs on par with the existing sampling algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50411022",
                    "name": "Moin Nadeem"
                },
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "145898106",
                    "name": "James R. Glass"
                }
            ]
        }
    ]
}