{
    "authorId": "2276779989",
    "papers": [
        {
            "paperId": "02d25f79f8410bc5a5c08be237d3feec572e6d80",
            "title": "Towards Quantitative Evaluation of Explainable AI Methods for Deepfake Detection",
            "abstract": "In this paper we propose a new framework for evaluating the performance of explanation methods on the decisions of a deepfake detector. This framework assesses the ability of an explanation method to spot the regions of a fake image with the biggest influence on the decision of the deepfake detector, by examining the extent to which these regions can be modified through a set of adversarial attacks, in order to flip the detector\u2019s prediction or reduce its initial prediction; we anticipate a larger drop in deepfake detection accuracy and prediction, for methods that spot these regions more accurately. Based on this framework, we conduct a comparative study using a state-of-the-art model for deepfake detection that has been trained on the FaceForensics++ dataset, and five explanation methods from the literature. The findings of our quantitative and qualitative evaluations document the advanced performance of the LIME explanation method against the other compared ones, and indicate this method as the most appropriate for explaining the decisions of the utilized deepfake detector.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1418234904",
                    "name": "K. Tsigos"
                },
                {
                    "authorId": "9436583",
                    "name": "Evlampios Apostolidis"
                },
                {
                    "authorId": "2163584643",
                    "name": "Spyridon Baxevanakis"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "2238753836",
                    "name": "Vasileios Mezaris"
                }
            ]
        },
        {
            "paperId": "054e1cca44dbba67cf7f35df4da53ddc666fef4b",
            "title": "FaceX: Understanding Face Attribute Classifiers through Summary Model Explanations",
            "abstract": "EXplainable Artificial Intelligence (XAI) approaches are widely applied for identifying fairness issues in Artificial Intelligence (AI) systems. However, in the context of facial analysis, existing XAI approaches, such as pixel attribution methods, offer explanations for individual images, posing challenges in assessing the overall behavior of a model, which would require labor-intensive manual inspection of a very large number of instances and leaving to the human the task of drawing a general impression of the model behavior from the individual outputs. Addressing this limitation, we introduce FaceX, the first method that provides a comprehensive understanding of face attribute classifiers through summary model explanations. Specifically, FaceX leverages the presence of distinct regions across all facial images to compute a region-level aggregation of model activations, allowing for the visualization of the model's region attribution across 19 predefined regions of interest in facial images, such as hair, ears, or skin. Beyond spatial explanations, FaceX enhances interpretability by visualizing specific image patches with the highest impact on the model's decisions for each facial region within a test benchmark. Through extensive evaluation in various experimental setups, including scenarios with or without intentional biases and mitigation efforts on four benchmarks, namely CelebA, FairFace, CelebAMask-HQ, and Racial Faces in the Wild, FaceX demonstrates high effectiveness in identifying the models' biases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388052763",
                    "name": "Ioannis Sarridis"
                },
                {
                    "authorId": "2036760",
                    "name": "C. Koutlis"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "2258707985",
                    "name": "Christos Diou"
                }
            ]
        },
        {
            "paperId": "10c17afcd6f77c29250b8f0a7ebddd403eb6814e",
            "title": "Evaluating AI Group Fairness: a Fuzzy Logic Perspective",
            "abstract": "Artificial intelligence systems often address fairness concerns by evaluating and mitigating measures of group discrimination, for example that indicate biases against certain genders or races. However, what constitutes group fairness depends on who is asked and the social context, whereas definitions are often relaxed to accept small deviations from the statistical constraints they set out to impose. Here we decouple definitions of group fairness both from the context and from relaxation-related uncertainty by expressing them in the axiomatic system of Basic fuzzy Logic (BL) with loosely understood predicates, like encountering group members. We then evaluate the definitions in subclasses of BL, such as Product or Lukasiewicz logics. Evaluation produces continuous instead of binary truth values by choosing the logic subclass and truth values for predicates that reflect uncertain context-specific beliefs, such as stakeholder opinions gathered through questionnaires. Internally, it follows logic-specific rules to compute the truth values of definitions. We show that commonly held propositions standardize the resulting mathematical formulas and we transcribe logic and truth value choices to layperson terms, so that anyone can answer them. We also use our framework to study several literature definitions of algorithmic fairness, for which we rationalize previous expedient practices that are non-probabilistic and show how to re-interpret their formulas and parameters in new contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28075447",
                    "name": "Emmanouil Krasanakis"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                }
            ]
        },
        {
            "paperId": "4b35b92c54a1663b0b6623b1c12dc4707d9e3d85",
            "title": "Towards Optimal Trade-offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge",
            "abstract": "This paper discusses four facets of the Knowledge Distillation (KD) process for Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) architectures, particularly when executed on edge devices with constrained processing capabilities. First, we conduct a comparative analysis of the KD process between CNNs and ViT architectures, aiming to elucidate the feasibility and efficacy of employing different architectural configurations for the teacher and student, while assessing their performance and efficiency. Second, we explore the impact of varying the size of the student model on accuracy and inference speed, while maintaining a constant KD duration. Third, we examine the effects of employing higher resolution images on the accuracy, memory footprint and computational workload. Last, we examine the performance improvements obtained by fine-tuning the student model after KD to specific downstream tasks. Through empirical evaluations and analyses, this research provides AI practitioners with insights into optimal strategies for maximizing the effectiveness of the KD process on edge devices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2573061",
                    "name": "John Violos"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "1715604",
                    "name": "Y. Kompatsiaris"
                }
            ]
        },
        {
            "paperId": "74804cc714cae570673e4b15b6e5b8f2b32245ae",
            "title": "Integrity 2024: Integrity in Social Networks and Media",
            "abstract": "Integrity 2024 is the fifth edition of the Workshop on Integrity in Social Networks and Media, held in conjunction with the ACM Conference on Web Search and Data Mining (WSDM) since the 2020 edition [1-4]. The goal of the workshop is to bring together academic and industry researchers working on integrity, fairness, trust and safety in social networks to discuss the most pressing risks and cutting-edge technologies to reliably measure and mitigate them. The event consists of invited talks from academic experts and industry leaders as well as peer-reviewed papers and posters through an open call-for-papers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2209212295",
                    "name": "Llu\u00eds Garcia-Pueyo"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "2209214648",
                    "name": "Prathyusha Senthil Kumar"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1701195",
                    "name": "Panayiotis Tsaparas"
                },
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                },
                {
                    "authorId": "2313479443",
                    "name": "Giuseppe Manco"
                },
                {
                    "authorId": "2290565901",
                    "name": "Anton Andryeyev"
                },
                {
                    "authorId": "40598011",
                    "name": "S. Cresci"
                },
                {
                    "authorId": "2239640744",
                    "name": "T. Sellis"
                },
                {
                    "authorId": "144000523",
                    "name": "Anthony McCosker"
                }
            ]
        },
        {
            "paperId": "83bbda9a934c8e8b5f48ea1174275867128f954d",
            "title": "MAD '24 Workshop: Multimedia AI against Disinformation",
            "abstract": "Synthetic media generation and manipulation have seen rapid ad- vancements in recent years, making it increasingly easy to create multimedia content that is indistinguishable to the human observer. Moreover, generated content can be used maliciously by individ- uals and organizations in order to spread disinformation, posing a significant threat to society and democracy. Hence, there is an urgent need for AI tools geared towards facilitating a timely and ef- fective media verification process. The MAD'24 workshop seeks to bring together people with diverse backgrounds who are dedicated to combating disinformation in multimedia through the means of AI, by fostering an environment for exploring innovative ideas and sharing experiences. The research areas of interest encompass the identification of manipulated or generated content, along with the investigation of the dissemination of disinformation and its societal repercussions. Recognizing the significance of multimedia, the workshop emphasizes the joint analysis of various modalities within content, as verification can be improved by aggregating multiple forms of content.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305403994",
                    "name": "Cristian Stanciu"
                },
                {
                    "authorId": "2305406315",
                    "name": "Bogdan Ionescu"
                },
                {
                    "authorId": "3281530",
                    "name": "Luca Cuccovillo"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "1403953272",
                    "name": "Giorgos Kordopatis-Zilos"
                },
                {
                    "authorId": "2305405457",
                    "name": "Adrian Popescu"
                },
                {
                    "authorId": "2305404600",
                    "name": "Roberto Caldelli"
                }
            ]
        },
        {
            "paperId": "98ca4bdd55daeb1630acf6c13e7cf74a4abd62cc",
            "title": "Credible, Unreliable or Leaked?: Evidence verification for enhanced automated fact-checking",
            "abstract": "Automated fact-checking (AFC) is garnering increasing attention by researchers aiming to help fact-checkers combat the increasing spread of misinformation online. While many existing AFC methods incorporate external information from the Web to help examine the veracity of claims, they often overlook the importance of verifying the source and quality of collected \u201cevidence\u201d. One overlooked challenge involves the reliance on \u201cleaked evidence\u201d, information gathered directly from fact-checking websites and used to train AFC systems, resulting in an unrealistic setting for early misinformation detection. Similarly, the inclusion of information from unreliable sources can undermine the effectiveness of AFC systems. To address these challenges, we present a comprehensive approach to evidence verification and filtering. We create the \u201cCREDible, Unreliable or LEaked\u201d (CREDULE) dataset, which consists of 91,632 articles classified as Credible, Unreliable and Fact-checked (Leaked). Additionally, we introduce the EVidence VERification Network (EVVER-Net), trained on CREDULE to detect leaked and unreliable evidence in both short and long texts. EVVER-Net can be used to filter evidence collected from the Web, thus enhancing the robustness of end-to-end AFC systems. We experiment with various language models and show that EVVER-Net can demonstrate impressive performance of up to 91.5% and 94.4% accuracy, while leveraging domain credibility scores along with short or long texts, respectively. Finally, we assess the evidence provided by widely-used fact-checking datasets including LIAR-PLUS, MOCHEG, FACTIFY, NewsCLIPpings+ and VERITE, some of which exhibit concerning rates of leaked and unreliable evidence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298967144",
                    "name": "Zacharias Chrysidis"
                },
                {
                    "authorId": "1725431945",
                    "name": "Stefanos-Iordanis Papadopoulos"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "2733886",
                    "name": "P. Petrantonakis"
                }
            ]
        },
        {
            "paperId": "ad6c86f56d30933a577c5a515fc114c06cd42dcf",
            "title": "Towards Standardizing AI Bias Exploration",
            "abstract": "Creating fair AI systems is a complex problem that involves the assessment of context-dependent bias concerns. Existing research and programming libraries express specific concerns as measures of bias that they aim to constrain or mitigate. In practice, one should explore a wide variety of (sometimes incompatible) measures before deciding which ones warrant corrective action, but their narrow scope means that most new situations can only be examined after devising new measures. In this work, we present a mathematical framework that distils literature measures of bias into building blocks, hereby facilitating new combinations to cover a wide range of fairness concerns, such as classification or recommendation differences across multiple multi-value sensitive attributes (e.g., many genders and races, and their intersections). We show how this framework generalizes existing concepts and present frequently used blocks. We provide an open-source implementation of our framework as a Python library, called FairBench, that facilitates systematic and extensible exploration of potential bias concerns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28075447",
                    "name": "Emmanouil Krasanakis"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                }
            ]
        },
        {
            "paperId": "cfe208a4362d5a0033016f629b2e929e8a7151f2",
            "title": "The Visual Saliency Transformer Goes Temporal: TempVST for Video Saliency Prediction",
            "abstract": "The Transformer revolutionized Natural Language Processing and Computer Vision by effectively capturing contextual relationships in sequential data through its attention mechanism. While Transformers have been explored sufficiently in traditional computer vision tasks such as image classification, their application to more intricate tasks, such as Video Saliency Prediction (VSP), remains limited. Video saliency prediction is the task of identifying the most visually salient regions in a video, which are likely to capture a viewer\u2019s attention. In this study, we propose a pure transformer architecture named Temporal Visual Saliency Transformer (TempVST) for the VSP task. Our model leverages the Visual Saliency Transformer (VST) as a backbone, with the addition of a Transformer-based temporal module that can seamlessly transition diverse architectural frameworks from image to video domain, through the incorporation of temporal recurrences. Moreover, we demonstrate that transfer learning is viable in the context of VSP through Transformer architectures and helps reduce the duration of the training phase, leading to a reduction in the duration of the training phase by 41% and 45% in two different datasets. Our experiments were conducted on two benchmark datasets, DHF1K and LEDOV, and our results show that our network can compete with all other state-of-the-art models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2314450713",
                    "name": "Nikos Lazaridis"
                },
                {
                    "authorId": "144567694",
                    "name": "Kostas Georgiadis"
                },
                {
                    "authorId": "3451934",
                    "name": "Fotis P. Kalaganis"
                },
                {
                    "authorId": "1403953272",
                    "name": "Giorgos Kordopatis-Zilos"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "1698652",
                    "name": "S. Nikolopoulos"
                },
                {
                    "authorId": "2285815300",
                    "name": "Ioannis Kompatsiaris"
                }
            ]
        },
        {
            "paperId": "e6d603a8e2cc265451425c80d0fd66bfcc242209",
            "title": "PolyMeme: Fine-Grained Internet Meme Sensing",
            "abstract": "Internet memes are a special type of digital content that is shared through social media. They have recently emerged as a popular new format of media communication. They are often multimodal, combining text with images and aim to express humor, irony, sarcasm, or sometimes convey hatred and misinformation. Automatically detecting memes is important since it enables tracking of social and cultural trends and issues related to the spread of harmful content. While memes can take various forms and belong to different categories, such as image macros, memes with labeled objects, screenshots, memes with text out of the image, and funny images, existing datasets do not account for the diversity of meme formats, styles and content. To bridge this gap, we present the PolyMeme dataset, which comprises approximately 27 K memes from four categories. This was collected from Reddit and a part of it was manually labelled into these categories. Using the manual labels, deep learning networks were trained to classify the unlabelled images with an estimated error rate of 7.35%. The introduced meme dataset in combination with existing datasets of regular images were used to train deep learning networks (ResNet, ViT) on meme detection, exhibiting very high accuracy levels (98% on the test set). In addition, no significant gains were identified from the use of regular images containing text.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2317424521",
                    "name": "Vasileios Arailopoulos"
                },
                {
                    "authorId": "2036760",
                    "name": "C. Koutlis"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "2733886",
                    "name": "P. Petrantonakis"
                }
            ]
        }
    ]
}