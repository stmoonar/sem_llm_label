{
    "authorId": "145645971",
    "papers": [
        {
            "paperId": "3db2dd8fd74ad6de997ed2776c5fa2718de060e6",
            "title": "Attention And Positional Encoding Are (Almost) All You Need For Shape Matching",
            "abstract": "The fast development of novel approaches derived from the Transformers architecture has led to outstanding performance in different scenarios, from Natural Language Processing to Computer Vision. Recently, they achieved impressive results even in the challenging task of non\u2010rigid shape matching. However, little is known about the capability of the Transformer\u2010encoder architecture for the shape matching task, and its performances still remained largely unexplored. In this paper, we step back and investigate the contribution made by the Transformer\u2010encoder architecture compared to its more recent alternatives, focusing on why and how it works on this specific task. Thanks to the versatility of our implementation, we can harness the bi\u2010directional structure of the correspondence problem, making it more interpretable. Furthermore, we prove that positional encodings are essential for processing unordered point clouds. Through a comprehensive set of experiments, we find that attention and positional encoding are (almost) all you need for shape matching. The simple Transformer\u2010encoder architecture, coupled with relative position encoding in the attention mechanism, is able to obtain strong improvements, reaching the current state\u2010of\u2010the\u2010art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3106437",
                    "name": "Alessandro Raganato"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                },
                {
                    "authorId": "1972186",
                    "name": "S. Melzi"
                }
            ]
        },
        {
            "paperId": "3e41cff4bcfd85b049f75dc4ac4e98576d14d697",
            "title": "Personalization in BERT with Adapter Modules and Topic Modelling",
            "abstract": "As a result of the widespread use of intelligent assistants, personalization in dialogue systems has become a hot topic in both research and industry. Typically, training such systems is computationally expensive, especially when using recent large language models. To address this challenge, we develop an approach to personalize dialogue systems using adapter layers and topic modelling. Our implementation enables the model to incorporate user-specific information, achieving promising results by training only a small fraction of parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2227483106",
                    "name": "Marco Braga"
                },
                {
                    "authorId": "3106437",
                    "name": "Alessandro Raganato"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                }
            ]
        },
        {
            "paperId": "7daa0c447fcc5499120ce024d58bd391dd38f764",
            "title": "Retrieval for Extremely Long Queries and Documents with RPRS: A Highly Efficient and Effective Transformer-based Re-Ranker",
            "abstract": "Retrieval with extremely long queries and documents is a well-known and challenging task in information retrieval and is commonly known as Query-by-Document (QBD) retrieval. Specifically designed Transformer models that can handle long input sequences have not shown high effectiveness in QBD tasks in previous work. We propose a Re-Ranker based on the novel Proportional Relevance Score (RPRS) to compute the relevance score between a query and the top-k candidate documents. Our extensive evaluation shows RPRS obtains significantly better results than the state-of-the-art models on five different datasets. Furthermore, RPRS is highly efficient, since all documents can be pre-processed, embedded, and indexed before query time that gives our re-ranker the advantage of having a complexity of O(N), where N is the total number of sentences in the query and candidate documents. Furthermore, our method solves the problem of the low-resource training in QBD retrieval tasks as it does not need large amounts of training data and has only three parameters with a limited range that can be optimized with a grid search even if a small amount of labeled data is available. Our detailed analysis shows that RPRS benefits from covering the full length of candidate documents and queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "77879474",
                    "name": "Arian Askari"
                },
                {
                    "authorId": "1702730",
                    "name": "S. Verberne"
                },
                {
                    "authorId": "2065022402",
                    "name": "Amin Abolghasemi"
                },
                {
                    "authorId": "134866426",
                    "name": "Wessel Kraaij"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                }
            ]
        },
        {
            "paperId": "8db5eecad52ab0735ad0ebd0cda77dfba5c74689",
            "title": "SE-PQA: Personalized Community Question Answering",
            "abstract": "Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to filling this gap by introducing SE-PQA(StackExchange - Personalized Question Answering), a new curated resource to design and evaluate personalized models related to the task of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness of SE-PQA to train effective cQA models; they also show that personalization remarkably improves the effectiveness of all the methods tested. Furthermore, we show the benefits in terms of robustness and generalization of combining data from multiple communities for personalization purposes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187856026",
                    "name": "Pranav Kasela"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                },
                {
                    "authorId": "46188036",
                    "name": "R. Perego"
                }
            ]
        },
        {
            "paperId": "bd12effec7228606c64e675b9ad15d199887982f",
            "title": "Denoising Attention for Query-aware User Modeling in Personalized Search",
            "abstract": "The personalization of search results has gained increasing attention in the past few years, thanks to the development of Neural Networks-based approaches for Information Retrieval and the importance of personalization in many search scenarios. Recent works have proposed to build user models at query time by leveraging the Attention mechanism, which allows weighing the contribution of the user-related information w.r.t. the current query. This approach allows taking into account the diversity of the user's interests by giving more importance to those related to the current search performed by the user. In this paper, we first discuss some shortcomings of the standard Attention formulation when employed for personalization. In particular, we focus on issues related to its normalization mechanism and its inability to entirely filter out noisy user-related information. Then, we introduce the Denoising Attention mechanism: an Attention variant that directly tackles the above shortcomings by adopting a robust normalization scheme and introducing a filtering mechanism. The reported experimental evaluation shows the benefits of the proposed approach over other Attention-based variants.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52211445",
                    "name": "Elias Bassani"
                },
                {
                    "authorId": "2187856026",
                    "name": "Pranav Kasela"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                }
            ]
        },
        {
            "paperId": "2b768ad30497c7f1545f8542ec5eb64d1cd73eb8",
            "title": "Report on the 13th Conference and Labs of the Evaluation Forum (CLEF 2022)",
            "abstract": "This is a report on the thirteenth edition of the Conference and Labs of the Evaluation Forum (CLEF 2022), held on September 5--8, 2022, in Bologna, Italy. CLEF was a four-day hybrid event combining a conference and an evaluation forum. The conference featured keynotes by Benno Stein and Rita Cucchiara, and presentation of peer-reviewed research papers covering a wide range of topics, in addition to many posters. The evaluation forum consisted of fourteen labs: ARQMath, BioASQ, CheckThat!, ChEMU, eRisk, HIPE, iDPP, ImageCLEF, JokeR, LeQua, LifeCLEF, PAN, SimpleText, and Touch\u00e9, addressing a wide range of tasks, media, languages, and ways to go beyond standard test collections. Date: 5--8 September, 2022. Website: https://clef2022.clef-initiative.eu/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "134000266",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "33831241",
                    "name": "M. Esposti"
                },
                {
                    "authorId": "80808662",
                    "name": "G. Faggioli"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                },
                {
                    "authorId": "3046200",
                    "name": "Martin Potthast"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "42b2fd8a0ab69281022de528c591c665f5f2ad3b",
            "title": "A Multi-Domain Benchmark for Personalized Search Evaluation",
            "abstract": "Personalization in Information Retrieval has been a hot topic in both academia and industry for the past two decades. However, there is still a lack of high-quality standard benchmark datasets for conducting offline comparative evaluations in this context. To mitigate this problem, in the past few years, approaches to derive synthetic datasets suited for evaluating Personalized Search models have been proposed. In this paper, we put forward a novel evaluation benchmark for Personalized Search with more than 18 million documents and 1.9 million queries across four domains. We present a detailed description of the benchmark construction procedure, highlighting its characteristics and challenges. We provide baseline performance including pre-trained neural models, opening room for the evaluation of personalized approaches, as well as domain adaptation and transfer learning scenarios. We make both datasets and models available for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52211445",
                    "name": "Elias Bassani"
                },
                {
                    "authorId": "2187856026",
                    "name": "Pranav Kasela"
                },
                {
                    "authorId": "3106437",
                    "name": "Alessandro Raganato"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                }
            ]
        },
        {
            "paperId": "5e0bf5d58b224c6261cc714139a54681a7917149",
            "title": "LeiBi@COLIEE 2022: Aggregating Tuned Lexical Models with a Cluster-driven BERT-based Model for Case Law Retrieval",
            "abstract": "This paper summarizes our approaches submitted to the case law retrieval task in the Competition on Legal Information Extraction/Entailment (COLIEE) 2022. Our methodology consists of four steps; in detail, given a legal case as a query, we reformulate it by extracting various meaningful sentences or n-grams. Then, we utilize the pre-processed query case to retrieve an initial set of possible relevant legal cases, which we further re-rank. Lastly, we aggregate the relevance scores obtained by the first stage and the re-ranking models to improve retrieval effectiveness. In each step of our methodology, we explore various well-known and novel methods. In particular, to reformulate the query cases aiming to make them shorter, we extract unigrams using three different statistical methods: KLI, PLM, IDF-r, as well as models that leverage embeddings (e.g., KeyBERT). Moreover, we investigate if automatic summarization using Longformer-Encoder-Decoder (LED) can produce an effective query representation for this retrieval task. Furthermore, we propose a novel re-ranking cluster-driven approach, which leverages Sentence-BERT models that are pre-tuned on large amounts of data for embedding sentences from query and candidate documents. Finally, we employ a linear aggregation method to combine the relevance scores obtained by traditional IR models and neural-based models, aiming to incorporate the semantic understanding of neural models and the statistically measured topical relevance. We show that aggregating these relevance scores can improve the overall retrieval effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "77879474",
                    "name": "Arian Askari"
                },
                {
                    "authorId": "46227908",
                    "name": "Georgios Peikos"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                },
                {
                    "authorId": "1702730",
                    "name": "S. Verberne"
                }
            ]
        },
        {
            "paperId": "609e720f4969240207059cce9586a5f02908a597",
            "title": "UNIMIB at TREC 2021 Clinical Trials Track",
            "abstract": "This contribution summarizes the participation of the UNIMIB team to the TREC 2021 Clinical Trials Track. We have investigated the effect of different query representations combined with several retrieval models on the retrieval performance. First, we have implemented a neural re-ranking approach to study the effectiveness of dense text representations. Additionally, we have investigated the effectiveness of a novel decision-theoretic model for relevance estimation. Finally, both of the above relevance models have been compared with standard retrieval approaches. In particular, we combined a keyword extraction method with a standard retrieval process based on the BM25 model and a decision-theoretic relevance model that exploits the characteristics of this particular search task. The obtained results show that the proposed keyword extraction method improves 84% of the queries over the TREC's median NDCG@10 measure when combined with either traditional or decision-theoretic relevance models. Moreover, regarding RPEC@10, the employed decision-theoretic model improves 85% of the queries over the reported TREC's median value.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46227908",
                    "name": "Georgios Peikos"
                },
                {
                    "authorId": "31322004",
                    "name": "Oscar Espitia"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                }
            ]
        },
        {
            "paperId": "dd9e10ce452aaae707835a97b5dcd1decae1825f",
            "title": "Benchmark for Research Theme Classification of Scholarly Documents",
            "abstract": "We present a new gold-standard dataset and a benchmark for the Research Theme Identification task, a sub-task of the Scholarly Knowledge Graph Generation shared task, at the 3rd Workshop on Scholarly Document Processing. The objective of the shared task was to label given research papers with research themes from a total of 36 themes. The benchmark was compiled using data drawn from the largest overall assessment of university research output ever undertaken globally (the Research Excellence Framework - 2014). We provide a performance comparison of a transformer-based ensemble, which obtains multiple predictions for a research paper, given its multiple textual fields (e.g. title, abstract, reference), with traditional machine learning models. The ensemble involves enriching the initial data with additional information from open-access digital libraries and Argumentative Zoning techniques (CITATION). It uses a weighted sum aggregation for the multiple predictions to obtain a final single prediction for the given research paper. Both data and the ensemble are publicly available on https://www.kaggle.com/competitions/sdp2022-scholarly-knowledge-graph-generation/data?select=task1_test_no_label.csv and https://github.com/ProjectDoSSIER/sdp2022, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187453401",
                    "name": "\u00d3scar E. Mendoza"
                },
                {
                    "authorId": "50320098",
                    "name": "Wojciech Kusa"
                },
                {
                    "authorId": "1412404246",
                    "name": "Alaa El-Ebshihy"
                },
                {
                    "authorId": "2153457425",
                    "name": "R. Wu"
                },
                {
                    "authorId": "2064119650",
                    "name": "David Pride"
                },
                {
                    "authorId": "1848764",
                    "name": "Petr Knoth"
                },
                {
                    "authorId": "1839605",
                    "name": "Drahomira Herrmannova"
                },
                {
                    "authorId": "3309646",
                    "name": "Florina Piroi"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                }
            ]
        }
    ]
}