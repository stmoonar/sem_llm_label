{
    "authorId": "50625437",
    "papers": [
        {
            "paperId": "0d7ba2bfdef0e780a76212c9d3cc30746d6bd3a1",
            "title": "GUICourse: From General Vision Language Models to Versatile GUI Agents",
            "abstract": "Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse, a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at https://github.com/yiye3/GUICourse.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265461972",
                    "name": "Wentong Chen"
                },
                {
                    "authorId": "2292213035",
                    "name": "Junbo Cui"
                },
                {
                    "authorId": "92837695",
                    "name": "Jinyi Hu"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2257383989",
                    "name": "Junjie Fang"
                },
                {
                    "authorId": "2249846477",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2249899670",
                    "name": "Chongyi Wang"
                },
                {
                    "authorId": "2307431855",
                    "name": "Jun Liu"
                },
                {
                    "authorId": "2247499423",
                    "name": "Gui-Fang Chen"
                },
                {
                    "authorId": "2306957427",
                    "name": "Yupeng Huo"
                },
                {
                    "authorId": "1390925224",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "2427350",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2305652650",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "66eae99f971b8dc3b3fd83e335848d9c95594bb5",
            "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models",
            "abstract": "Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268575667",
                    "name": "Zhicheng Guo"
                },
                {
                    "authorId": "2110844331",
                    "name": "Sijie Cheng"
                },
                {
                    "authorId": "2267445794",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2242676146",
                    "name": "Shihao Liang"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "144326610",
                    "name": "Peng Li"
                },
                {
                    "authorId": "2288774230",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2257187607",
                    "name": "Maosong Sun"
                },
                {
                    "authorId": "2284726267",
                    "name": "Yang Liu"
                }
            ]
        },
        {
            "paperId": "a70f0f9b9b9dc7d5caadcb23a551ea4213727548",
            "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
            "abstract": "In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163940136",
                    "name": "Xueyang Feng"
                },
                {
                    "authorId": "2241452075",
                    "name": "Zhiyuan Chen"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2257310922",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2265519430",
                    "name": "Xu Chen"
                },
                {
                    "authorId": "2284825514",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2266834033",
                    "name": "Ji-Rong Wen"
                }
            ]
        },
        {
            "paperId": "bb39d2b273d4c75f8e489f1f66dec42a7e14c5fd",
            "title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214603370",
                    "name": "Runchu Tian"
                },
                {
                    "authorId": "2114059497",
                    "name": "Yining Ye"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "2427350",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "c86b98d0b5fe07e46fd862f47f717271c45b96fe",
            "title": "UniMem: Towards a Unified View of Long-Context Large Language Models",
            "abstract": "Long-context processing is a critical ability that constrains the applicability of large language models (LLMs). Although there exist various methods devoted to enhancing the long-context processing ability of LLMs, they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a Unified framework that reformulates existing long-context methods from the view of Memory augmentation of LLMs. Distinguished by its four core dimensions-Memory Management, Memory Writing, Memory Reading, and Memory Injection, UniMem empowers researchers to conduct systematic exploration of long-context methods. We re-formulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an innovative approach that integrates the strengths of these algorithms. Experimental results show that UniMix achieves superior performance in handling long contexts with significantly lower perplexity than baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257383989",
                    "name": "Junjie Fang"
                },
                {
                    "authorId": "2153720923",
                    "name": "Likai Tang"
                },
                {
                    "authorId": "2282538520",
                    "name": "Hongzhe Bi"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2109509345",
                    "name": "Si Sun"
                },
                {
                    "authorId": "2282619799",
                    "name": "Zhenyu Li"
                },
                {
                    "authorId": "2282714061",
                    "name": "Haolun Li"
                },
                {
                    "authorId": "2282543766",
                    "name": "Yongjian Li"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "2277242040",
                    "name": "Yukun Yan"
                },
                {
                    "authorId": "2257017314",
                    "name": "Xiaodong Shi"
                },
                {
                    "authorId": "2282508059",
                    "name": "Sen Song"
                },
                {
                    "authorId": "2427350",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "ce37a3c0a233019da9b97a6f607aff5c1997ec91",
            "title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution",
            "abstract": "This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy for enhancing the adaptability and flexibility of AI agents through inter-task self-evolution. Unlike existing methods focused on intra-task learning, ICE promotes the transfer of knowledge between tasks for genuine self-evolution, similar to human experience learning. The strategy dynamically investigates planning and execution trajectories, consolidates them into simplified workflows and pipelines, and exploits them for improved task execution. Our experiments on the XAgent framework demonstrate ICE's effectiveness, reducing API calls by as much as 80% and significantly decreasing the demand for the model's capability. Specifically, when combined with GPT-3.5, ICE's performance matches that of raw GPT-4 across various agent tasks. We argue that this self-evolution approach represents a paradigm shift in agent design, contributing to a more robust AI community and ecosystem, and moving a step closer to full autonomy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2082473972",
                    "name": "Cheng Qian"
                },
                {
                    "authorId": "2242676146",
                    "name": "Shihao Liang"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2114059497",
                    "name": "Yining Ye"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "2427350",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2281076178",
                    "name": "Yesai Wu"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "deb44bb1bc3b61a6b6a11d671947974da3ee4f73",
            "title": "Exploring Universal Intrinsic Task Subspace for Few-Shot Learning via Prompt Tuning",
            "abstract": "Why can pre-trained language models (PLMs) learn universal representations and effectively adapt to broad NLP tasks differing a lot superficially? In this work, we empirically find evidence indicating that the adaptations of PLMs to various few-shot tasks can be reparameterized as optimizing only a few free parameters in a unified low-dimensional intrinsic task subspace, which may help us understand why PLMs could easily adapt to various NLP tasks with small-scale data. To find such a subspace and examine its universality, we propose an analysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort to the recent success of prompt tuning and decompose the soft prompts of multiple NLP tasks into the same low-dimensional nonlinear subspace, then we learn to adapt the PLM to unseen data or tasks by only tuning parameters in this subspace. In the experiments, we study diverse few-shot NLP tasks and surprisingly find that in a 250-dimensional subspace found with 100 tasks, by only tuning 250 free parameters, we can recover 97% and 83% of the full prompt tuning performance for 100 seen tasks (using different training data) and 20 unseen tasks, respectively, showing great generalization ability of the found intrinsic task subspace. Besides being an analysis tool, IPTcould further help us improve the prompt tuning stability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "48631777",
                    "name": "Xiaozhi Wang"
                },
                {
                    "authorId": "48576745",
                    "name": "Yusheng Su"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2284766400",
                    "name": "Ning Ding"
                },
                {
                    "authorId": "2106388389",
                    "name": "Jing Yi"
                },
                {
                    "authorId": "2109136284",
                    "name": "Weize Chen"
                },
                {
                    "authorId": "2308485553",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2294164848",
                    "name": "Juanzi Li"
                },
                {
                    "authorId": "2301159894",
                    "name": "Lei Hou"
                },
                {
                    "authorId": "2209965245",
                    "name": "Peng Li"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                },
                {
                    "authorId": "2257088385",
                    "name": "Jie Zhou"
                }
            ]
        },
        {
            "paperId": "f1b6c8bc0f5675c6398d56fdb86d574d48b3f91d",
            "title": "RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation",
            "abstract": "Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging. However, their utilization in the domain of code documentation generation remains underexplored. To this end, we introduce RepoAgent, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation. Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation. The code and results are publicly accessible at https://github.com/OpenBMB/RepoAgent.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287929551",
                    "name": "Qinyu Luo"
                },
                {
                    "authorId": "2114059497",
                    "name": "Yining Ye"
                },
                {
                    "authorId": "2242676146",
                    "name": "Shihao Liang"
                },
                {
                    "authorId": "2277307110",
                    "name": "Zhong Zhang"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2191753738",
                    "name": "Ya-Ting Lu"
                },
                {
                    "authorId": "2281076178",
                    "name": "Yesai Wu"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "2427350",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2287832490",
                    "name": "Yingli Zhang"
                },
                {
                    "authorId": "2287837514",
                    "name": "Xiaoyin Che"
                },
                {
                    "authorId": "2284825514",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "f4e4f2090b639372daf32d4b466752b0fb32b261",
            "title": "Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents",
            "abstract": "Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understanding and execution, revealing that our approach notably excels at identifying vague user tasks, recovering and summarizing critical missing information, setting precise and necessary agent execution goals, and minimizing redundant tool usage, thus boosting overall efficiency. All the data and codes are released.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2082473972",
                    "name": "Cheng Qian"
                },
                {
                    "authorId": "2171127494",
                    "name": "Bingxiang He"
                },
                {
                    "authorId": "2284557819",
                    "name": "Zhuang Zhong"
                },
                {
                    "authorId": "2284299688",
                    "name": "Jia Deng"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "2277307110",
                    "name": "Zhong Zhang"
                },
                {
                    "authorId": "2257088385",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2427350",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
            "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
            "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2163374235",
                    "name": "Shi Liang"
                },
                {
                    "authorId": "2114059497",
                    "name": "Yining Ye"
                },
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2214613855",
                    "name": "Lan Yan"
                },
                {
                    "authorId": "2191753738",
                    "name": "Ya-Ting Lu"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "47274259",
                    "name": "Xiangru Tang"
                },
                {
                    "authorId": "2226120351",
                    "name": "Bill Qian"
                },
                {
                    "authorId": "2226184989",
                    "name": "Sihan Zhao"
                },
                {
                    "authorId": "2214603370",
                    "name": "Runchu Tian"
                },
                {
                    "authorId": "3360722",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "49640256",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "152573911",
                    "name": "M. Gerstein"
                },
                {
                    "authorId": "2144118403",
                    "name": "Dahai Li"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                }
            ]
        }
    ]
}