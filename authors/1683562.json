{
    "authorId": "1683562",
    "papers": [
        {
            "paperId": "0212ed7f530033432bf25a252cb9732be0e54c5e",
            "title": "Team TheSyllogist at SemEval-2023 Task 3: Language-Agnostic Framing Detection in Multi-Lingual Online News: A Zero-Shot Transfer Approach",
            "abstract": "We describe our system for SemEval-2022 Task 3 subtask 2 which on detecting the frames used in a news article in a multi-lingual setup. We propose a multi-lingual approach based on machine translation of the input, followed by an English prediction model. Our system demonstrated good zero-shot transfer capability, achieving micro-F1 scores of 53% for Greek (4th on the leaderboard) and 56.1% for Georgian (3rd on the leaderboard), without any prior training on translated data for these languages. Moreover, our system achieved comparable performance on seven other languages, including German, English, French, Russian, Italian, Polish, and Spanish. Our results demonstrate the feasibility of creating a language-agnostic model for automatic framing detection in online news.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2221319300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "208b091d2458d1ddef1e18d39f839fff7f992748",
            "title": "BERTastic at SemEval-2023 Task 3: Fine-Tuning Pretrained Multilingual Transformers Does Order Matter?",
            "abstract": "The naive approach for fine-tuning pretrained deep learning models on downstream tasks involves feeding them mini-batches of randomly sampled data. In this paper, we propose a more elaborate method for fine-tuning Pretrained Multilingual Transformers (PMTs) on multilingual data. Inspired by the success of curriculum learning approaches, we investigate the significance of fine-tuning PMTs on multilingual data in a sequential fashion language by language. Unlike the curriculum learning paradigm where the model is presented with increasingly complex examples, we do not adopt a notion of \u201ceasy\u201d and \u201chard\u201d samples. Instead, our experiments draw insight from psychological findings on how the human brain processes new information and the persistence of newly learned concepts. We perform our experiments on a challenging news-framing dataset that contains texts in six languages. Our proposed method outperforms the na\u00efve approach by achieving improvements of 2.57\\% in terms of F1 score. Even when we supplement the na\u00efve approach with recency fine-tuning, we still achieve an improvement of 1.34\\% with a 3.63\\%$ convergence speed-up. Moreover, we are the first to observe an interesting pattern in which deep learning models exhibit a human-like primacy-recency effect.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "2286562f2b185ce476a23f218f2de83b0561fbe9",
            "title": "bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark",
            "abstract": "We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at https://bgglue.github.io, and we hope that it will enable further advancements in developing NLU models for Bulgarian.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3255454",
                    "name": "Momchil Hardalov"
                },
                {
                    "authorId": "145676297",
                    "name": "Pepa Atanasova"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "3203341",
                    "name": "G. Angelova"
                },
                {
                    "authorId": "143964258",
                    "name": "K. Simov"
                },
                {
                    "authorId": "1710801",
                    "name": "P. Osenova"
                },
                {
                    "authorId": "1389924486",
                    "name": "Ves Stoyanov"
                },
                {
                    "authorId": "52553663",
                    "name": "Ivan Koychev"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                }
            ]
        },
        {
            "paperId": "3bb1a0193cb0b5dd9405a729b16320c6ec31b1dd",
            "title": "Enhancing Arabic Content Generation with Prompt Augmentation Using Integrated GPT and Text-to-Image Models",
            "abstract": "With the current and continuous advancements in the field of text-to-image modeling, it has become critical to design prompts that make the best of these model capabilities and guides them to generate the most desirable images, and thus the field of prompt engineering has emerged. Here, we study a method to use prompt engineering to enhance text-to-image model representation of the Arabic culture. This work proposes a simple, novel approach for prompt engineering that uses the domain knowledge of a state-of-the-art language model, GPT, to perform the task of prompt augmentation, where a simple, initial prompt is used to generate multiple, more detailed prompts related to the Arabic culture from multiple categories through a GPT model through a process known as in-context learning. The augmented prompts are then used to generate images enhanced for the Arabic culture. We perform multiple experiments with a number of participants to evaluate the performance of the proposed method, which shows promising results, specially for generating prompts that are more inclusive of the different Arabic countries and with a wider variety in terms of image subjects, where we find that our proposed method generates image with more variety 85 % of the time and are more inclusive of the Arabic countries more than 72.66 % of the time, compared to the direct approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2235098848",
                    "name": "Wala Elsharif"
                },
                {
                    "authorId": "2042066180",
                    "name": "James She"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2111124581",
                    "name": "Simon Wong"
                }
            ]
        },
        {
            "paperId": "47a1263ba21a72790334544f2a11b7c0ee4b5e76",
            "title": "Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications",
            "abstract": "Multimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal tasks. This study summarizes the (i) recent task-specific deep learning methodologies, (ii) the pretraining types and multimodal pretraining objectives, (iii) from state-of-the-art pretrained multimodal approaches to unifying architectures, and (iv) multimodal task categories and possible future improvements that can be devised for better multimodal learning. Moreover, we prepare a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning. Finally, major challenges, gaps, and potential research topics are explored. A constantly-updated paperlist related to our survey is maintained at https://github.com/marslanm/multimodality-representation-learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282539097",
                    "name": "Muhammad Arslan Manzoor"
                },
                {
                    "authorId": "2129589403",
                    "name": "S. Albarri"
                },
                {
                    "authorId": "2203864747",
                    "name": "Ziting Xian"
                },
                {
                    "authorId": "3451645",
                    "name": "Zaiqiao Meng"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "3279808",
                    "name": "Shangsong Liang"
                }
            ]
        },
        {
            "paperId": "5c577988ccebfea96de86678d04fd94fad367d2e",
            "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
            "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "3422905",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "2087720002",
                    "name": "Bokang Jia"
                },
                {
                    "authorId": "2235818050",
                    "name": "Satheesh Katipomu"
                },
                {
                    "authorId": "49404498",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2203791403",
                    "name": "Samta Kamboj"
                },
                {
                    "authorId": "22171629",
                    "name": "O. Pandit"
                },
                {
                    "authorId": "2235794681",
                    "name": "Rahul Pal"
                },
                {
                    "authorId": "2076256459",
                    "name": "Lalit Pradhan"
                },
                {
                    "authorId": "123838298",
                    "name": "Zainul Mujahid"
                },
                {
                    "authorId": "1380273855",
                    "name": "Massa Baali"
                },
                {
                    "authorId": "2110982198",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "100468503",
                    "name": "Zhengzhong Liu"
                },
                {
                    "authorId": "2235826325",
                    "name": "Andy Hock"
                },
                {
                    "authorId": "77917645",
                    "name": "Andrew Feldman"
                },
                {
                    "authorId": "2235945609",
                    "name": "Jonathan Lee"
                },
                {
                    "authorId": "2064974174",
                    "name": "A. Jackson"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "145465286",
                    "name": "Timothy Baldwin"
                },
                {
                    "authorId": "2064963077",
                    "name": "Eric P. Xing"
                }
            ]
        },
        {
            "paperId": "60730c7baeeabf4ff2fd824effc40bca465b1334",
            "title": "M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries. However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia. In this study, we strive to create automated systems that can detect machine-generated texts and pinpoint potential misuse. We first introduce a large-scale benchmark M4, which is a multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and that there is a lot of room for improvement. We believe that our dataset will enable future research towards more robust approaches to this pressing societal problem. The dataset is available at https://github.com/mbzuai-nlp/M4",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115829571",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2058455381",
                    "name": "Petar Ivanov"
                },
                {
                    "authorId": "2116966710",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "6729422d35054c0bccf752e6df306638e9bf1401",
            "title": "Gpachov at CheckThat!-2023: A Diverse Multi-approach Ensemble for Subjectivity Detection in News Articles",
            "abstract": "The wide-spread use of social networks has given rise to subjective, misleading, and even false information on the Internet. Thus, subjectivity detection can play an important role in ensuring the objectiveness and the quality of a piece of information. This paper presents the solution built by the Gpachov team for the CLEF-2023 CheckThat! lab Task~2 on subjectivity detection. Three different research directions are explored. The first one is based on fine-tuning a sentence embeddings encoder model and dimensionality reduction. The second one explores a sample-efficient few-shot learning model. The third one evaluates fine-tuning a multilingual transformer on an altered dataset, using data from multiple languages. Finally, the three approaches are combined in a simple majority voting ensemble, resulting in 0.77 macro F1 on the test set and achieving 2nd place on the English subtask.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2239200277",
                    "name": "Georgi Pachov"
                },
                {
                    "authorId": "2105813793",
                    "name": "Dimitar I. Dimitrov"
                },
                {
                    "authorId": "52553663",
                    "name": "Ivan Koychev"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "7219a157ede104e3c33df5afdb04437ba53b7cc9",
            "title": "Can You Answer This? - Exploring Zero-Shot QA Generalization Capabilities in Large Language Models (Student Abstract)",
            "abstract": "The buzz around Transformer-based language models (TLM) such as BERT, RoBERTa, etc. is well-founded owing to their impressive results on an array of tasks. However, when applied to areas needing specialized knowledge (closed-domain), such as medical, finance, etc. their performance takes drastic hits, sometimes more than their older recurrent/convolutional counterparts. In this paper, we explore zero-shot capabilities of large LMs for extractive QA. Our objective is to examine performance change in the face of domain drift i.e. when the target domain data is vastly different in semantic and statistical properties from the source domain and attempt to explain the subsequent behavior. To this end, we present two studies in this paper while planning further experiments later down the road. Our findings indicate flaws in the current generation of TLM limiting their performance on closed-domain tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057927933",
                    "name": "Saptarshi Sengupta"
                },
                {
                    "authorId": "2180318921",
                    "name": "Shreya Ghosh"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "143930195",
                    "name": "P. Mitra"
                }
            ]
        },
        {
            "paperId": "7dcb732c92e9c5b53aff482e543db4909dfa62dc",
            "title": "Learning Through Interpolative Augmentation of Dynamic Curvature Spaces",
            "abstract": "Mixup is an efficient data augmentation technique, which improves generalization by interpolating random examples. While numerous approaches have been developed for Mixup in the Euclidean and in the hyperbolic space, they do not fully use the intrinsic properties of the examples, i.e., they manually set the geometry (Euclidean or hyperbolic) based on the overall dataset, which may be sub-optimal since each example may require a different geometry. We propose DynaMix, a framework that automatically selects an example-specific geometry and performs Mixup between the different geometries to improve training dynamics and generalization. Through extensive experiments in image and text modalities we show that DynaMix outperforms state-of-the-art methods over six downstream applications. We find that DynaMix is more useful in low-resource and semi-supervised settings likely because it displays a probabilistic view of the geometry.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186676914",
                    "name": "Parth Chhabra"
                },
                {
                    "authorId": "2157860264",
                    "name": "A. Neerkaje"
                },
                {
                    "authorId": "1923351",
                    "name": "Shivam Agarwal"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2275117",
                    "name": "S. Chava"
                }
            ]
        }
    ]
}