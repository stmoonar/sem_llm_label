{
    "authorId": "2152797401",
    "papers": [
        {
            "paperId": "21b4777948797377deedf4a9f1f58ad13f6b8b5d",
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2887412",
                    "name": "Jinchao Li"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2066415714",
                    "name": "Zhengcong Fei"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2257374643",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2116575668",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "1979505",
                    "name": "A. Geramifard"
                },
                {
                    "authorId": "1765212",
                    "name": "Chiori Hori"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "05405e73f1afcf9564a5f81de0db7570cfa3d792",
            "title": "LMGQS: A Large-scale Dataset for Query-focused Summarization",
            "abstract": "Query-focused summarization (QFS) aims to extract or generate a summary of an input document that directly answers or is relevant to a given query. The lack of large-scale datasets in the form of documents, queries, and summaries has hindered model development in this area. In contrast, multiple large-scale high-quality datasets for generic summarization exist. We hypothesize that there is a hidden query for each summary sentence in a generic summarization annotation, and we utilize a large-scale pretrained language model to recover it. In this way, we convert four generic summarization benchmarks into a new QFS benchmark dataset, LMGQS, which consists of over 1 million document-query-summary samples. We thoroughly investigate the properties of our proposed dataset and establish baselines with state-of-the-art summarization models. By fine-tuning a language model on LMGQS, we achieve state-of-the-art zero-shot and supervised performance on multiple existing QFS benchmarks, demonstrating the high quality and diversity of LMGQS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "2117074851",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "48262024",
                    "name": "Michael Zeng"
                }
            ]
        },
        {
            "paperId": "1d75f8de31bf47ec46fa5586056420ec8bc97e86",
            "title": "Using In-Context Learning to Improve Dialogue Safety",
            "abstract": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150247363",
                    "name": "Nicholas Meade"
                },
                {
                    "authorId": "2921001",
                    "name": "Spandana Gella"
                },
                {
                    "authorId": "8223433",
                    "name": "Devamanyu Hazarika"
                },
                {
                    "authorId": "1491232062",
                    "name": "Prakhar Gupta"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "145732771",
                    "name": "Siva Reddy"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "2ad96c799edcbba5b097c8842461f0d9da4aa1cc",
            "title": "Identifying Entrainment in Task-Oriented Conversations",
            "abstract": "Human interlocutors adapt their behavior to each other in a conversation through entrainment. While entrainment has been found in long chit-chat conversations, much less research has been conducted on task-oriented dialogs. In this paper, we investigate short task-oriented Wizard-of-Oz conversations for acoustic-prosodic and lexical entrainment. We conduct significance tests that reveal changes in speech pitch and frequent words as important indicators of entrainment. Our findings will guide user-entraining dialog systems to improve the quality of conversations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188747624",
                    "name": "Run Chen"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "32057282",
                    "name": "J. Hirschberg"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "abstract": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                }
            ]
        },
        {
            "paperId": "6f9d3c3a628e8d842de83f4ba2e41dd1b4b404ac",
            "title": "PLACES: Prompting Language Models for Social Conversation Synthesis",
            "abstract": "Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66615738",
                    "name": "Maximillian Chen"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "46387857",
                    "name": "Chenyang Tao"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "146177177",
                    "name": "Andrew Rosenbaum"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "9591e76f296afe9589a42e899266e1c6a355d9ba",
            "title": "InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT",
            "abstract": "While large models such as GPT-3 demonstrate exceptional performance in zeroshot and fewshot summarization tasks, their extensive serving and fine-tuning costs hinder their utilization in various applications. Conversely, previous studies have found that although automatic metrics tend to favor smaller fine-tuned models, the quality of the summaries they generate is inferior to that of larger models like GPT-3 when assessed by human evaluators. To address this issue, we propose InheritSumm, a versatile and compact summarization model derived from GPT-3.5 through distillation. InheritSumm not only exhibits comparable zeroshot and fewshot summarization capabilities to GPT-3.5 but is also sufficiently compact for fine-tuning purposes. Experimental results demonstrate that InheritSumm achieves similar or superior performance to GPT-3.5 in zeroshot and fewshot settings. Furthermore, it outperforms the previously established best small models in both prefix-tuning and full-data fine-tuning scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "48262024",
                    "name": "Michael Zeng"
                }
            ]
        },
        {
            "paperId": "992cff59f13f5944a74eb2e62a8cb11849204780",
            "title": "KILM: Knowledge Injection into Encoder-Decoder Language Models",
            "abstract": "Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters. To enhance this implicit knowledge, we propose Knowledge Injection into Language Models (KILM), a novel approach that injects entity-related knowledge into encoder-decoder PLMs, via a generative knowledge infilling objective through continued pre-training. This is done without architectural modifications to the PLMs or adding additional parameters. Experimental results over a suite of knowledge-intensive tasks spanning numerous datasets show that KILM enables models to retain more knowledge and hallucinate less while preserving their original performance on general NLU and NLG tasks. KILM also demonstrates improved zero-shot performances on tasks such as entity disambiguation, outperforming state-of-the-art models having 30x more parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "98271906",
                    "name": "Yan Xu"
                },
                {
                    "authorId": "2171886",
                    "name": "Mahdi Namazifar"
                },
                {
                    "authorId": "8223433",
                    "name": "Devamanyu Hazarika"
                },
                {
                    "authorId": "2110665",
                    "name": "Aishwarya Padmakumar"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "a30ee4285c17d027d89659b539bd6d909258b137",
            "title": "\u201cWhat do others think?\u201d: Task-Oriented Conversational Modeling with Subjective Knowledge",
            "abstract": "Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that assist users in accomplishing specific goals, such as booking a hotel or a restaurant. Traditional TODs rely on domain-specific APIs/DBs or external factual knowledge to generate responses, which cannot accommodate subjective user requests (e.g.,\u201dIs the WIFI reliable?\u201d or \u201cDoes the restaurant have a good atmosphere?\u201d). To address this issue, we propose a novel task of subjective-knowledge-based TOD (SK-TOD). We also propose the first corresponding dataset, which contains subjective knowledge-seeking dialogue contexts and manually annotated responses grounded in subjective knowledge sources. When evaluated with existing TOD approaches, we find that this task poses new challenges such as aggregating diverse opinions from multiple knowledge snippets. We hope this task and dataset can promote further research on TOD and subjective content understanding. The code and the dataset are available at https://github.com/alexa/dstc11-track5.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151480423",
                    "name": "Chao Zhao"
                },
                {
                    "authorId": "2921001",
                    "name": "Spandana Gella"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "8223433",
                    "name": "Devamanyu Hazarika"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "2171886",
                    "name": "Mahdi Namazifar"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "bfef29e0b88a45b3b6d8018b37d6272d0f32efe2",
            "title": "In-Context Demonstration Selection with Cross Entropy Difference",
            "abstract": "Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration. We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to rank and select in-context demonstrations independently for each test input. We evaluate our method on a mix-domain dataset that combines 8 benchmarks, representing 4 text generation tasks, showing that CED for in-context demonstration selection can improve performance for a variety of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "4099006",
                    "name": "Reid Pryzant"
                },
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                }
            ]
        }
    ]
}