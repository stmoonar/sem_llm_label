{
    "authorId": "2192083394",
    "papers": [
        {
            "paperId": "334ebaae0a97fd7af3cd8ec0a9b02fb4fbba2f42",
            "title": "Rethinking and Accelerating Graph Condensation: A Training-Free Approach with Class Partition",
            "abstract": "The increasing prevalence of large-scale graphs poses a significant challenge for graph neural network training, attributed to their substantial computational requirements. In response, graph condensation (GC) emerges as a promising data-centric solution aiming to substitute the large graph with a small yet informative condensed graph to facilitate data-efficient GNN training. However, existing GC methods suffer from intricate optimization processes, necessitating excessive computing resources. In this paper, we revisit existing GC optimization strategies and identify two pervasive issues: 1. various GC optimization strategies converge to class-level node feature matching between the original and condensed graphs, making the optimization target coarse-grained despite the complex computations; 2. to bridge the original and condensed graphs, existing GC methods rely on a Siamese graph network architecture that requires time-consuming bi-level optimization with iterative gradient computations. To overcome these issues, we propose a training-free GC framework termed Class-partitioned Graph Condensation (CGC), which refines the node feature matching from the class-to-class paradigm into a novel class-to-node paradigm. Remarkably, this refinement also simplifies the GC optimization as a class partition problem, which can be efficiently solved by any clustering methods. Moreover, CGC incorporates a pre-defined graph structure to enable a closed-form solution for condensed node features, eliminating the back-and-forth gradient descent in existing GC approaches without sacrificing accuracy. Extensive experiments demonstrate that CGC achieves state-of-the-art performance with a more efficient condensation process. For instance, compared with the seminal GC method (i.e., GCond), CGC condenses the largest Reddit graph within 10 seconds, achieving a 2,680X speedup and a 1.4% accuracy increase.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2280283715",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "1849611",
                    "name": "Guanhua Ye"
                },
                {
                    "authorId": "144133815",
                    "name": "Q. Nguyen"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "6af3cba144f11381400d82236e3c6fb22dcab6c9",
            "title": "Acceleration Algorithms in GNNs: A Survey",
            "abstract": "Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph-based tasks. However, their inefficiency in training and inference presents challenges for scaling up to real-world and large-scale graph applications. To address the critical challenges, a range of algorithms have been proposed to accelerate training and inference of GNNs, attracting increasing attention from the research community. In this paper, we present a systematic review of acceleration algorithms in GNNs, which can be categorized into three main topics based on their purpose: training acceleration, inference acceleration, and execution acceleration. Specifically, we summarize and categorize the existing approaches for each main topic, and provide detailed characterizations of the approaches within each category. Additionally, we review several libraries related to acceleration algorithms in GNNs and discuss our Scalable Graph Learning (SGL) library. Finally, we propose promising directions for future research. A complete summary is presented in our GitHub repository: https://github.com/PKU-DAIR/SGL/blob/main/Awsome-GNN-Acceleration.md.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2300236632",
                    "name": "Lu Ma"
                },
                {
                    "authorId": "2300174486",
                    "name": "Zeang Sheng"
                },
                {
                    "authorId": "2268429288",
                    "name": "Xunkai Li"
                },
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2301539938",
                    "name": "Zhezheng Hao"
                },
                {
                    "authorId": "2249513224",
                    "name": "Ling Yang"
                },
                {
                    "authorId": "2277807793",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2277742543",
                    "name": "Bin Cui"
                }
            ]
        },
        {
            "paperId": "92ea98ca67767c05e9a32ed4a9e0937257c9b9cf",
            "title": "Graph Condensation: A Survey",
            "abstract": "The rapid growth of graph data poses significant challenges in storage, transmission, and particularly the training of graph neural networks (GNNs). To address these challenges, graph condensation (GC) has emerged as an innovative solution. GC focuses on synthesizing a compact yet highly representative graph, enabling GNNs trained on it to achieve performance comparable to those trained on the original large graph. The notable efficacy of GC and its broad prospects have garnered significant attention and spurred extensive research. This survey paper provides an up-to-date and systematic overview of GC, organizing existing research into five categories aligned with critical GC evaluation criteria: effectiveness, generalization, efficiency, fairness, and robustness. To facilitate an in-depth and comprehensive understanding of GC, this paper examines various methods under each category and thoroughly discusses two essential components within GC: optimization strategies and condensed graph generation. We also empirically compare and analyze representative GC methods with diverse optimization strategies based on the five proposed GC evaluation criteria. Finally, we explore the applications of GC in various fields, outline the related open-source libraries, and highlight the present challenges and novel insights, with the aim of promoting advancements in future research. The related resources can be found at https://github.com/XYGaoG/Graph-Condensation-Papers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2281470021",
                    "name": "Wei Jiang"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2280283715",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "9510bbc5e3cb8d7b601474ecb2cd32c6db9dc75a",
            "title": "Reliable Node Similarity Matrix Guided Contrastive Graph Clustering",
            "abstract": "Graph clustering, which involves the partitioning of nodes within a graph into disjoint clusters, holds significant importance for numerous subsequent applications. Recently, contrastive learning, known for utilizing supervisory information, has demonstrated encouraging results in deep graph clustering. This methodology facilitates the learning of favorable node representations for clustering by attracting positively correlated node pairs and distancing negatively correlated pairs within the representation space. Nevertheless, a significant limitation of existing methods is their inadequacy in thoroughly exploring node-wise similarity. For instance, some hypothesize that the node similarity matrix within the representation space is identical, ignoring the inherent semantic relationships among nodes. Given the fundamental role of instance similarity in clustering, our research investigates contrastive graph clustering from the perspective of the node similarity matrix. We argue that an ideal node similarity matrix within the representation space should accurately reflect the inherent semantic relationships among nodes, ensuring the preservation of semantic similarities in the learned representations. In response to this, we introduce a new framework, Reliable Node Similarity Matrix Guided Contrastive Graph Clustering (NS4GC), which estimates an approximately ideal node similarity matrix within the representation space to guide representation learning. Our method introduces node-neighbor alignment and semantic-aware sparsification, ensuring the node similarity matrix is both accurate and efficiently sparse. Comprehensive experiments conducted on $8$ real-world datasets affirm the efficacy of learning the node similarity matrix and the superior performance of NS4GC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2313929373",
                    "name": "Yunhui Liu"
                },
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2278776738",
                    "name": "Tieke He"
                },
                {
                    "authorId": "2313993478",
                    "name": "Tao Zheng"
                },
                {
                    "authorId": "2314168612",
                    "name": "Jianhua Zhao"
                },
                {
                    "authorId": "2292126832",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "bd603882603101b9e7cf48a09caf93322dbcb1cf",
            "title": "Challenging Low Homophily in Social Recommendation",
            "abstract": "Social relations are leveraged to tackle the sparsity issue of user-item interaction data in recommendation under the assumption of social homophily. However, social recommendation paradigms predominantly focus on homophily based on user preferences. While social information can enhance recommendations, its alignment with user preferences is not guaranteed, thereby posing the risk of introducing informational redundancy. We empirically discover that social graphs in real recommendation data exhibit low preference-aware homophily, which limits the effect of social recommendation models. To comprehensively extract preference-aware homophily information latent in the social graph, we propose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric framework for enhancing existing graph-based social recommendation models. We adopt Graph Rewiring technique to capture and add highly homophilic social relations, and cut low homophilic (or heterophilic) relations. To better refine the user representations from reliable social relations, we integrate a contrastive learning method into the training of SHaRe, aiming to calibrate the user representations for enhancing the result of Graph Rewiring. Experiments on real-world datasets show that the proposed framework not only exhibits enhanced performances across varying homophily ratios but also improves the performance of existing state-of-the-art (SOTA) social recommendation models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281470021",
                    "name": "Wei Jiang"
                },
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2281788009",
                    "name": "Guandong Xu"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "ccba97a805651d1134c85bc865eb5f00c1e1be46",
            "title": "BIM: Improving Graph Neural Networks with Balanced Influence Maximization",
            "abstract": "The imbalanced data classification problem has aroused lots of concerns from both academia and industry since data imbalance is a widespread phenomenon in many real-world scenarios. Although this problem has been well researched from the view of imbalanced class samples, we further argue that graph neural networks (GNNs) expose a unique source of imbalance from the influenced nodes of different classes of labeled nodes, i.e., labeled nodes are imbalanced in terms of the number of nodes they influenced during the influence propagation in GNNs. To tackle this previously unexplored influence-imbalance issue, we connect social influence maximization with the imbalanced node classification problem and propose balanced influence maximization (BIM). Specifically, BIM greedily assigns the pseudo label to the node which can maximize the number of influenced nodes in GNN training while making the influence of each class more balance. Experimental results on five public datasets demonstrate the effectiveness of our method in relieving the influence-imbalance issue. For example, when training a GCN with an imbalance ratio of 0.1, BIM significantly outperforms the most competitive baseline by 0.6% -9.8% in five public datasets in terms of the F1 score.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108232566",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2249513224",
                    "name": "Ling Yang"
                },
                {
                    "authorId": "2312916415",
                    "name": "Meng Cao"
                },
                {
                    "authorId": "1704261333",
                    "name": "Ping-Chia Huang"
                },
                {
                    "authorId": "2091600962",
                    "name": "Jiulong Shan"
                },
                {
                    "authorId": "2286301391",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "2312744587",
                    "name": "Bin Cui"
                }
            ]
        },
        {
            "paperId": "e1d8244289468fa3350fa9b64ad466599c746ca2",
            "title": "Graph Condensation for Open-World Graph Learning",
            "abstract": "The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications. To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance. Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution. This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes. In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated. Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation. This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it. Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2280283715",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2303406950",
                    "name": "Yayong Li"
                },
                {
                    "authorId": "2302792188",
                    "name": "Xiangguo Sun"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "f20d934631943945f95f82d00a5868cb22f0aabb",
            "title": "RobGC: Towards Robust Graph Condensation",
            "abstract": "Graph neural networks (GNNs) have attracted widespread attention for their impressive capability of graph representation learning. However, the increasing prevalence of large-scale graphs presents a significant challenge for GNN training due to their computational demands, limiting the applicability of GNNs in various scenarios. In response to this challenge, graph condensation (GC) is proposed as a promising acceleration solution, focusing on generating an informative compact graph that enables efficient training of GNNs while retaining performance. Despite the potential to accelerate GNN training, existing GC methods overlook the quality of large training graphs during both the training and inference stages. They indiscriminately emulate the training graph distributions, making the condensed graphs susceptible to noises within the training graph and significantly impeding the application of GC in intricate real-world scenarios. To address this issue, we propose robust graph condensation (RobGC), a plug-and-play approach for GC to extend the robustness and applicability of condensed graphs in noisy graph structure environments. Specifically, RobGC leverages the condensed graph as a feedback signal to guide the denoising process on the original training graph. A label propagation-based alternating optimization strategy is in place for the condensation and denoising processes, contributing to the mutual purification of the condensed graph and training graph. Additionally, as a GC method designed for inductive graph inference, RobGC facilitates test-time graph denoising by leveraging the noise-free condensed graph to calibrate the structure of the test graph. Extensive experiments show that RobGC is compatible with various GC methods, significantly boosting their robustness under different types and levels of graph structural noises.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "1849611",
                    "name": "Guanhua Ye"
                },
                {
                    "authorId": "2280283715",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2257282067",
                    "name": "Bin Cui"
                }
            ]
        },
        {
            "paperId": "f87e9bc3e81e9c3a73787ac507b31ef406718120",
            "title": "LLM-Powered Text Simulation Attack Against ID-Free Recommender Systems",
            "abstract": "The ID-free recommendation paradigm has been proposed to address the limitation that traditional recommender systems struggle to model cold-start users or items with new IDs. Despite its effectiveness, this study uncovers that ID-free recommender systems are vulnerable to the proposed Text Simulation attack (TextSimu) which aims to promote specific target items. As a novel type of text poisoning attack, TextSimu exploits large language models (LLM) to alter the textual information of target items by simulating the characteristics of popular items. It operates effectively in both black-box and white-box settings, utilizing two key components: a unified popularity extraction module, which captures the essential characteristics of popular items, and an N-persona consistency simulation strategy, which creates multiple personas to collaboratively synthesize refined promotional textual descriptions for target items by simulating the popular items. To withstand TextSimu-like attacks, we further explore the detection approach for identifying LLM-generated promotional text. Extensive experiments conducted on three datasets demonstrate that TextSimu poses a more significant threat than existing poisoning attacks, while our defense method can detect malicious text of target items generated by TextSimu. By identifying the vulnerability, we aim to advance the development of more robust ID-free recommender systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2040449292",
                    "name": "Zongwei Wang"
                },
                {
                    "authorId": "2155431892",
                    "name": "Min Gao"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "144133815",
                    "name": "Q. Nguyen"
                },
                {
                    "authorId": "2268756599",
                    "name": "S. Sadiq"
                },
                {
                    "authorId": "2260297841",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "4c27a3dfb43596cacbdba2e6806cd15ff5e0313a",
            "title": "Graph Condensation for Inductive Node Representation Learning",
            "abstract": "Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for inductive representation learning. This enables direct information propagation on the synthetic graph, which is much more efficient than on the original large graph. Specifically, MCond employs an alternating optimization scheme with innovative loss terms from transductive and inductive perspectives, facilitating the mutual promotion between graph condensation and node mapping learning. Extensive experiments demonstrate the efficacy of our approach in inductive inference. On the Reddit dataset, MCond achieves up to 121.5\u00d7 inference speedup and 55.9\u00d7 reduction in storage requirements compared with counterparts based on the original graph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "1490931831",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2183027408",
                    "name": "Yilong Zang"
                },
                {
                    "authorId": "2136776515",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "144133815",
                    "name": "Q. Nguyen"
                },
                {
                    "authorId": "145487536",
                    "name": "Kai Zheng"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                }
            ]
        }
    ]
}