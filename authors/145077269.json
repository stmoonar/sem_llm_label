{
    "authorId": "145077269",
    "papers": [
        {
            "paperId": "05b0db8d0f5bbe31aa0051241babeb913c233932",
            "title": "Same or Different? Diff-Vectors for Authorship Analysis",
            "abstract": "In this article, we investigate the effects on authorship identification tasks (including authorship verification, closed-set authorship attribution, and closed-set and open-set same-author verification) of a fundamental shift in how to conceive the vectorial representations of documents that are given as input to a supervised learner. In \u201cclassic\u201d authorship analysis, a feature vector represents a document, the value of a feature represents (an increasing function of) the relative frequency of the feature in the document, and the class label represents the author of the document. We instead investigate the situation in which a feature vector represents an unordered pair of documents, the value of a feature represents the absolute difference in the relative frequencies (or increasing functions thereof) of the feature in the two documents, and the class label indicates whether the two documents are from the same author or not. This latter (learner-independent) type of representation has been occasionally used before, but has never been studied systematically. We argue that it is advantageous, and that, in some cases (e.g., authorship verification), it provides a much larger quantity of information to the training process than the standard representation. The experiments that we carry out on several publicly available datasets (among which one that we here make available for the first time) show that feature vectors representing pairs of documents (that we here call Diff-Vectors) bring about systematic improvements in the effectiveness of authorship identification tasks, and especially so when training data are scarce (as it is often the case in real-life authorship identification scenarios). Our experiments tackle same-author verification, authorship verification, and closed-set authorship attribution; while DVs are naturally geared for solving the 1st, we also provide two novel methods for solving the 2nd and 3rd that use a solver for the 1st as a building block. The code to reproduce our experiments is open-source and available online.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1379629622",
                    "name": "Silvia Corbara"
                },
                {
                    "authorId": "38218211",
                    "name": "Alejandro Moreo"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "02dba0250e86d1cb806f199086a16ac19fb1684b",
            "title": "A Detailed Overview of LeQua@CLEF 2022: Learning to Quantify",
            "abstract": "LeQua 2022 is a new lab for the evaluation of methods for \u201clearning to quantify\u201d in textual datasets, i.e., for training predictors of the relative frequencies of the classes of interest \ud835\udcb4 = { \ud835\udc66 1 , ..., \ud835\udc66 \ud835\udc5b } in sets of unlabelled textual documents. While these predictions could be easily achieved by first classifying all documents via a text classifier and then counting the numbers of documents assigned to the classes, a growing body of literature has shown this approach to be suboptimal, and has proposed better methods. The goal of this lab is to provide a setting for the comparative evaluation of methods for learning to quantify, both in the binary setting and in the single-label multiclass setting; this is the first time that an evaluation exercise solely dedicated to quantification is organized. For both the binary setting and the single-label multiclass setting, data were provided to participants both in ready-made vector form and in raw document form. In this overview article we describe the structure of the lab, we report the results obtained by the participants in the four proposed tasks and subtasks, and we comment on the lessons that can be learned from these results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699411",
                    "name": "Andrea Esuli"
                },
                {
                    "authorId": "38218211",
                    "name": "Alejandro Moreo"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                },
                {
                    "authorId": "2127993814",
                    "name": "Gianluca Sperduti"
                }
            ]
        },
        {
            "paperId": "0bf04d86b1ad778e95bb369818c9a71880b6695f",
            "title": "Report on the 1st International Workshop on Learning to Quantify (LQ 2021)",
            "abstract": "The 1st International Workshop on Learning to Quantify (LQ 2021 - https://cikmlq2021.github.io/), organized as a satellite event of the 30th ACM International Conference on Knowledge Management (CIKM 2021), took place on two separate days, November 1 and 5, 2021. As the main CIKM 2021 conference, the workshop was held entirely online, due to the COVID-19 pandemic. This report presents a summary of each keynote speech and contributed paper presented in this event, and discusses the issues that were raised during the workshop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258395",
                    "name": "J. J. D. Coz"
                },
                {
                    "authorId": "2066371824",
                    "name": "Pablo Gonz\u00e1lez"
                },
                {
                    "authorId": "38218211",
                    "name": "Alejandro Moreo"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "11f2b350ee1196b884c406809550506b40100b59",
            "title": "Unravelling Interlanguage Facts via Explainable Machine Learning",
            "abstract": "\n Native language identification (NLI) is the task of training (via supervised machine learning) a classifier that guesses the native language of the author of a text. This task has been extensively researched in the last decade, and the performance of NLI systems has steadily improved over the years. We focus on a different facet of the NLI task, i.e. that of analysing the internals of an NLI classifier trained by an explainable machine learning (EML) algorithm, in order to obtain explanations of its classification decisions, with the ultimate goal of gaining insight into which linguistic phenomena \u2018give a speaker\u2019s native language away\u2019. We use this perspective in order to tackle both NLI and a (much less researched) companion task, i.e. guessing whether a text has been written by a native or a non-native speaker. Using three datasets of different provenance (two datasets of English learners\u2019 essays and a dataset of social media posts), we investigate which kind of linguistic traits (lexical, morphological, syntactic, and statistical) are most effective for solving our two tasks, namely, are most indicative of a speaker\u2019s L1; our experiments indicate that the most discriminative features are the lexical ones, followed by the morphological, syntactic, and statistical features, in this order. We also present two case studies, one on Italian and one on Spanish learners of English, in which we analyse individual linguistic traits that the classifiers have singled out as most important for spotting these L1s; we show that the traits identified as most discriminative well align with our intuition, i.e. represent typical patterns of language misuse, underuse, or overuse, by speakers of the given L1. Overall, our study shows that the use of EML can be a valuable tool for the scholar who investigates interlanguage facts and language transfer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38936899",
                    "name": "B. Berti"
                },
                {
                    "authorId": "1699411",
                    "name": "Andrea Esuli"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "2b768ad30497c7f1545f8542ec5eb64d1cd73eb8",
            "title": "Report on the 13th Conference and Labs of the Evaluation Forum (CLEF 2022)",
            "abstract": "This is a report on the thirteenth edition of the Conference and Labs of the Evaluation Forum (CLEF 2022), held on September 5--8, 2022, in Bologna, Italy. CLEF was a four-day hybrid event combining a conference and an evaluation forum. The conference featured keynotes by Benno Stein and Rita Cucchiara, and presentation of peer-reviewed research papers covering a wide range of topics, in addition to many posters. The evaluation forum consisted of fourteen labs: ARQMath, BioASQ, CheckThat!, ChEMU, eRisk, HIPE, iDPP, ImageCLEF, JokeR, LeQua, LifeCLEF, PAN, SimpleText, and Touch\u00e9, addressing a wide range of tasks, media, languages, and ways to go beyond standard test collections. Date: 5--8 September, 2022. Website: https://clef2022.clef-initiative.eu/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "134000266",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "33831241",
                    "name": "M. Esposti"
                },
                {
                    "authorId": "80808662",
                    "name": "G. Faggioli"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                },
                {
                    "authorId": "3046200",
                    "name": "Martin Potthast"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "393fc9cc22b77bf57877fbee932108f2b24833f4",
            "title": "Active Learning and the Saerens-Latinne-Decaestecker Algorithm: An Evaluation",
            "abstract": "The Saerens-Latinne-Decaestecker (SLD) algorithm is a method whose goal is improving the quality of the posterior probabilities (or simply \u201cposteriors\u201d) returned by a probabilistic classifier in scenarios characterized by prior probability shift (PPS) between the training set and the unlabelled (\u201ctest\u201d) set. This is an important task, (a) because posteriors are of the utmost importance in downstream tasks such as, e.g., multiclass classification and cost-sensitive classification, and (b) because PPS is ubiquitous in many applications. In this paper we explore whether using SLD can indeed improve the quality of posteriors returned by a classifier trained via active learning (AL), a class of machine learning (ML) techniques that indeed tend to generate substantial PPS. Specifically, we target AL via relevance sampling (ALvRS) and AL via uncertainty sampling (ALvUS), two AL techniques that are very well-known especially because, due to their low computational cost, are suitable to being applied in scenarios characterized by large datasets. We present experimental results obtained on the RCV1-v2 dataset, showing that SLD fails to deliver better-quality posteriors with both ALvRS and ALvUS, thus contradicting previous findings in the literature, and that this is due not to the amount of PPS that these techniques generate, but to how the examples they prioritize for annotation are distributed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2061607585",
                    "name": "Alessio Molinari"
                },
                {
                    "authorId": "1699411",
                    "name": "Andrea Esuli"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "a0248bf2a4a15c82725fc6534f408c0b44279bf7",
            "title": "Multi-Label Quantification",
            "abstract": "Quantification, variously called supervised prevalence estimation or learning to quantify, is the supervised learning task of generating predictors of the relative frequencies (a.k.a. prevalence values) of the classes of interest in unlabelled data samples. While many quantification methods have been proposed in the past for binary problems and, to a lesser extent, single-label multiclass problems, the multi-label setting (i.e., the scenario in which the classes of interest are not mutually exclusive) remains by and large unexplored. A straightforward solution to the multi-label quantification problem could simply consist of recasting the problem as a set of independent binary quantification problems. Such a solution is simple but na\u00efve, since the independence assumption upon which it rests is, in most cases, not satisfied. In these cases, knowing the relative frequency of one class could be of help in determining the prevalence of other related classes. We propose the first truly multi-label quantification methods, i.e., methods for inferring estimators of class prevalence values that strive to leverage the stochastic dependencies among the classes of interest in order to predict their relative frequencies more accurately. We show empirical evidence that natively multi-label solutions outperform the na\u00efve approaches by a large margin. The code to reproduce all our experiments is available online.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38218211",
                    "name": "Alejandro Moreo"
                },
                {
                    "authorId": "74821134",
                    "name": "M. Francisco"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "16dd060d64deeda18c6c4277cbc9ff14d0f88181",
            "title": "Measuring Fairness Under Unawareness of Sensitive Attributes: A Quantification-Based Approach",
            "abstract": "Algorithms and models are increasingly deployed to inform decisions about people, inevitably affecting their lives. As a consequence, those in charge of developing these models must carefully evaluate their impact on different groups of people and favour group fairness, that is, ensure that groups determined by sensitive demographic attributes, such as race or sex, are not treated unjustly. To achieve this goal, the availability (awareness) of these demographic attributes to those evaluating the impact of these models is fundamental. Unfortunately, collecting and storing these attributes is often in conflict with industry practices and legislation on data minimisation and privacy. For this reason, it can be hard to measure the group fairness of trained models, even from within the companies developing them. In this work, we tackle the problem of measuring group fairness under unawareness of sensitive attributes, by using techniques from quantification, a supervised learning task concerned with directly providing group-level prevalence estimates (rather than individual-level class labels). We show that quantification approaches are particularly suited to tackle the fairness-under-unawareness problem, as they are robust to inevitable distribution shifts while at the same time decoupling the (desirable) objective of measuring group fairness from the (undesirable) side effect of allowing the inference of sensitive attributes of individuals. More in detail, we show that fairness under unawareness can be cast as a quantification problem and solved with proven methods from the quantification literature. We show that these methods outperform previous approaches to measure demographic parity in five experimental protocols, corresponding to important challenges that complicate the estimation of classifier fairness under unawareness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35340264",
                    "name": "Alessandro Fabris"
                },
                {
                    "authorId": "1699411",
                    "name": "Andrea Esuli"
                },
                {
                    "authorId": "38218211",
                    "name": "Alejandro Moreo"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "1d17f25c88e767153125c9c469e1062b6217608d",
            "title": "Learning to Quantify: Methods and Applications (LQ 2021)",
            "abstract": "Learning to Quantify (LQ) is the task of training class prevalence estimators via supervised learning. The task of these estimators is to estimate, given an unlabelled set of data items D and a set of classes C ={c1,...., c|C|}, the prevalence (i.e., relative frequency) of each class c_i in D. LQ is interesting in all applications of classification in which the final goal is not determining which class (or classes) individual unlabelled data items belong to, but estimating the distribution of the unlabelled data items across the classes of interest. Example disciplines whose interest in labelling data items is at the aggregate level (rather than at the individual level) are the social sciences, political science, market research, ecological modelling, and epidemiology. While LQ may in principle be solved by classifying each data item in D and counting how many such items have been labelled with c_i, it has been shown that this \"classify and count'' (CC) method yields suboptimal quantification accuracy. As a result, quantification is now no longer considered a mere byproduct of classification and has evolved as a task of its own. The goal of this workshop is bringing together all researchers interested in methods, algorithms, and evaluation measures and methodologies for LQ, as well as practitioners interested in their practical application to managing large quantities of data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258395",
                    "name": "J. J. D. Coz"
                },
                {
                    "authorId": "2066371824",
                    "name": "Pablo Gonz\u00e1lez"
                },
                {
                    "authorId": "38218211",
                    "name": "Alejandro Moreo"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "253fea74fc97a5a2918c0e34144737f46013f067",
            "title": "Generalized Funnelling: Ensemble Learning and Heterogeneous Document Embeddings for Cross-Lingual Text Classification",
            "abstract": "Funnelling (Fun) is a recently proposed method for cross-lingual text classification (CLTC) based on a two-tier learning ensemble for heterogeneous transfer learning (HTL). In this ensemble method, 1st-tier classifiers, each working on a different and language-dependent feature space, return a vector of calibrated posterior probabilities (with one dimension for each class) for each document, and the final classification decision is taken by a meta-classifier that uses this vector as its input. The meta-classifier can thus exploit class-class correlations, and this (among other things) gives Fun an edge over CLTC systems in which these correlations cannot be brought to bear. In this article, we describe Generalized Funnelling (gFun), a generalization of Fun consisting of an HTL architecture in which 1st-tier components can be arbitrary view-generating functions, i.e., language-dependent functions that each produce a language-independent representation (\u201cview\u201d) of the (monolingual) document. We describe an instance of gFun in which the meta-classifier receives as input a vector of calibrated posterior probabilities (as in Fun) aggregated to other embedded representations that embody other types of correlations, such as word-class correlations (as encoded by Word-Class Embeddings), word-word correlations (as encoded by Multilingual Unsupervised or Supervised Embeddings), and word-context correlations (as encoded by multilingual BERT). We show that this instance of gFun substantially improves over Fun and over state-of-the-art baselines by reporting experimental results obtained on two large, standard datasets for multilingual multilabel text classification. Our code that implements gFun is publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38218211",
                    "name": "Alejandro Moreo"
                },
                {
                    "authorId": "2080435228",
                    "name": "Andrea Pedrotti"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        }
    ]
}