{
    "authorId": "100996634",
    "papers": [
        {
            "paperId": "52dd29bd1d968e5db8661aa0a9e1d3f5d6c8c827",
            "title": "MCIL: Multimodal Counterfactual Instance Learning for Low-resource Entity-based Multimodal Information Extraction",
            "abstract": "Multimodal information extraction (MIE) is a challenging task which aims to extract the structural information in free text coupled with the image for constructing the multimodal knowledge graph. The entity-based MIE tasks are based on the entity information to complete the specific tasks. However, the existing methods only investigated the entity-based MIE tasks under supervised learning with adequate labeled data. In the real-world scenario, collecting enough data and annotating the entity-based samples are time-consuming, and impractical. Therefore, we propose to investigate the entity-based MIE tasks under the low-resource settings. The conventional models are prone to overfitting on limited labeled data, which can result in poor performance. This is because the models tend to learn the bias existing in the limited samples, which can lead them to model the spurious correlations between multimodal features and task labels. To provide a more comprehensive understanding of the bias inherent in multimodal features of MIE samples, we decompose the features into image, entity, and context factors. Furthermore, we investigate the causal relationships between these factors and model performance, leveraging the structural causal model to delve into the correlations between the input features and output labels. Based on this, we propose the multimodal counterfactual instance learning framework to generate the counterfactual instances by the interventions on the limited observational samples. In the framework, we analyze the causal effect of the counterfactual instances and exploit it as a supervisory signal to maximize the effect for reducing the bias and improving the generalization of the model. Empirically, we evaluate the proposed method on the two public MIE benchmark datasets and the experimental results verify the effectiveness of it.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2301654681",
                    "name": "Hongru Wang"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "8c2cf43026879adf5d0faf0ab041fe61a63c73d5",
            "title": "Look before You Leap: Dual Logical Verification for Knowledge-based Visual Question Generation",
            "abstract": "Knowledge-based Visual Question Generation aims to generate visual questions with outside knowledge other than the image. Existing approaches are answer-aware, which incorporate answers into the question-generation process. However, these methods just focus on leveraging the semantics of inputs to propose questions, ignoring the logical coherence among generated questions (Q), images (V), answers (A), and corresponding acquired outside knowledge (K). It results in generating many non-expected questions with low quality, lacking insight and diversity, and some of them are even without any corresponding answer. To address this issue, we inject logical verification into the processes of knowledge acquisition and question generation, which is defined as LV\u02c62-Net. Through checking the logical structure among V, A, K, ground-truth and generated Q twice in the whole KB-VQG procedure, LV\u02c62-Net can propose diverse and insightful knowledge-based visual questions. And experimental results on two commonly used datasets demonstrate the superiority of LV\u02c62-Net. Our code will be released to the public soon.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162443483",
                    "name": "Xumeng Liu"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2240858056",
                    "name": "Xubo Liu"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "50882212",
                    "name": "Shenglong Yu"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "8fdafdd43a224b0d9cfd8fb5bf9cae60a582067e",
            "title": "Bring Invariant to Variant: A Contrastive Prompt-based Framework for Temporal Knowledge Graph Forecasting",
            "abstract": "Temporal knowledge graph forecasting aims to reason over known facts to complete the missing links in the future. Existing methods are highly dependent on the structures of temporal knowledge graphs and commonly utilize recurrent or graph neural networks for forecasting. However, entities that are infrequently observed or have not been seen recently face challenges in learning effective knowledge representations due to insufficient structural contexts. To address the above disadvantages, in this paper, we propose a Contrastive Prompt-based framework with Entity background information for TKG forecasting, which we named CoPET. Specifically, to bring the time-invariant entity background information to time-variant structural information, we employ a dual encoder architecture consisting of a candidate encoder and a query encoder. A contrastive learning framework is used to encourage the query representation to be closer to the candidate representation. We further propose three kinds of trainable time-variant prompts aimed at capturing temporal structural information. Experiments on two datasets demonstrate that our method is effective and stays competitive in inference with limited structural information. Our code is available at https://github.com/qianxinying/CoPET.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2301634520",
                    "name": "Xinying Qian"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "9f5ed0c132df8c2e4a0cea8ba35e1d80337f4a90",
            "title": "LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection",
            "abstract": "Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between activities across sequences with graph structure learning. Moreover, to mitigate the data imbalance problem in ITD, we propose a novel hybrid prediction loss, which integrates self-supervision signals from normal activities and supervision signals from abnormal activities into a unified loss for anomaly detection. We evaluate the performance of LAN on two widely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative experiments demonstrate the superiority of LAN, outperforming 9 state-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD on CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to post-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in AUC on two datasets. Finally, the ablation study, parameter analysis, and compatibility analysis evaluate the impact of each module and hyper-parameter in LAN. The source code can be obtained from https://github.com/Li1Neo/LAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3127329",
                    "name": "Xiangrui Cai"
                },
                {
                    "authorId": "2291167164",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2110510537",
                    "name": "Sihan Xu"
                },
                {
                    "authorId": "2291204340",
                    "name": "Hao Li"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2108993427",
                    "name": "Zheli Liu"
                },
                {
                    "authorId": "2267877841",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "aa89cc8888d0f7334dc0e0bfa6422dc4643be05e",
            "title": "Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation",
            "abstract": "Next Point-of-Interests (POIs) recommendation task aims to provide a dynamic ranking of POIs based on users' current check-in trajectories. The recommendation performance of this task is contingent upon a comprehensive understanding of users' personalized behavioral patterns through Location-based Social Networks (LBSNs) data. While prior studies have adeptly captured sequential patterns and transitional relationships within users' check-in trajectories, a noticeable gap persists in devising a mechanism for discerning specialized behavioral patterns during distinct time slots, such as noon, afternoon, or evening. In this paper, we introduce an innovative data structure termed the ``Mobility Tree'', tailored for hierarchically describing users' check-in records. The Mobility Tree encompasses multi-granularity time slot nodes to learn user preferences across varying temporal periods. Meanwhile, we propose the Mobility Tree Network (MTNet), a multitask framework for personalized preference learning based on Mobility Trees. We develop a four-step node interaction operation to propagate feature information from the leaf nodes to the root node. Additionally, we adopt a multitask training strategy to push the model towards learning a robust representation. The comprehensive experimental results demonstrate the superiority of MTNet over eleven state-of-the-art next POI recommendation models across three real-world LBSN datasets, substantiating the efficacy of time slot preference learning facilitated by Mobility Tree.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292215410",
                    "name": "Tianhao Huang"
                },
                {
                    "authorId": "2292219658",
                    "name": "Xuan Pan"
                },
                {
                    "authorId": "3127329",
                    "name": "Xiangrui Cai"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2267877841",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "0941d750525d6dcea5c4a497ea8d80bd4e7bfba3",
            "title": "Incorporating Object-Level Visual Context for Multimodal Fine-Grained Entity Typing",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2273905684",
                    "name": "Wenbo Fan"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "221e5e14f14537bb65efec79f60aad5010834a66",
            "title": "AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis",
            "abstract": "Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment of a specific aspect can also be interfered by descriptions of other aspects (i.e., textual noise). Considering the aforementioned noises, this paper proposes an Aspect-oriented Method (AoM) to detect aspect-relevant semantic and sentiment information. Specifically, an aspect-aware attention module is designed to simultaneously select textual tokens and image blocks that are semantically related to the aspects. To accurately aggregate sentiment information, we explicitly introduce sentiment embedding into AoM, and use a graph convolutional network to model the vision-text and text-text interaction. Extensive experiments demonstrate the superiority of AoM to existing methods. The source code is publicly released at https://github.com/SilyRab/AoM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219327022",
                    "name": "Ru Zhou"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "2162443483",
                    "name": "Xumeng Liu"
                },
                {
                    "authorId": "50882212",
                    "name": "Shenglong Yu"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "9823e7d0bc60968e0bc793593811092c8a0146ef",
            "title": "Selecting Key Views for Zero-Shot Entity Linking",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                },
                {
                    "authorId": "2273574757",
                    "name": "Wensheng Zhang"
                }
            ]
        },
        {
            "paperId": "d3e35732a193acaa8f61ffdaa26e169708d35c15",
            "title": "From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment",
            "abstract": "Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples into unified textual sequences, and model the EA task as a bi-directional textual entailment task between the sequences of cross-KG entities. Specifically, we feed the sequences of two entities simultaneously into a pre-trained language model (PLM) and propose two kinds of PLM-based entity aligners that model the entailment probability between sequences as the similarity between entities. Our approach captures the unified correlation pattern of two kinds of information between entities, and explicitly models the fine-grained interaction between original entity information. The experiments on five cross-lingual EA datasets show that our approach outperforms the state-of-the-art EA methods and enables the mutual enhancement of the heterogeneous information. Codes are available at https://github.com/OreOZhao/TEA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "50118263",
                    "name": "Yike Wu"
                },
                {
                    "authorId": "3127329",
                    "name": "Xiangrui Cai"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2213318427",
                    "name": "Haiwei Zhang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "f49eb198153c0aa1b33c88351fcaca37d023ca1f",
            "title": "Licon: A Diverse, Controllable and Challenging Linguistic Concept Learning Benchmark",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50882212",
                    "name": "Shenglong Yu"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "2273631986",
                    "name": "Zhengkun Zhang"
                },
                {
                    "authorId": "2219327022",
                    "name": "Ru Zhou"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        }
    ]
}