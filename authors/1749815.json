{
    "authorId": "1749815",
    "papers": [
        {
            "paperId": "0550e63749f0270b5ff7655b50a902f614cfb281",
            "title": "Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies",
            "abstract": "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating that state-of-the-art LLMs constitute a very strong baseline also in challenging tasks that require systematic generalization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171081238",
                    "name": "Flavio Petruzzellis"
                },
                {
                    "authorId": "1915299",
                    "name": "Alberto Testolin"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                }
            ]
        },
        {
            "paperId": "0d7f5023685a228875735ce82b1eed614b202d77",
            "title": "IFH: a Diffusion Framework for Flexible Design of Graph Generative Models",
            "abstract": "Graph generative models can be classified into two prominent families: one-shot models, which generate a graph in one go, and sequential models, which generate a graph by successive additions of nodes and edges. Ideally, between these two extreme models lies a continuous range of models that adopt different levels of sequentiality. This paper proposes a graph generative model, called Insert-Fill-Halt (IFH), that supports the specification of a sequentiality degree. IFH is based upon the theory of Denoising Diffusion Probabilistic Models (DDPM), designing a node removal process that gradually destroys a graph. An insertion process learns to reverse this removal process by inserting arcs and nodes according to the specified sequentiality degree. We evaluate the performance of IFH in terms of quality, run time, and memory, depending on different sequentiality degrees. We also show that using DiGress, a diffusion-based one-shot model, as a generative step in IFH leads to improvement to the model itself, and is competitive with the current state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2316862023",
                    "name": "Samuel Cognolato"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                },
                {
                    "authorId": "2286767454",
                    "name": "Luciano Serafini"
                }
            ]
        },
        {
            "paperId": "224154d2eaffd48d3a7793755c9a17bdfa49ac27",
            "title": "Beyond the Additive Nodes' Convolutions: a Study on High-Order Multiplicative Integration",
            "abstract": "Graph Convolutional Neural Networks (GCNs) compute representations of graph nodes by exploiting convolution operators based on some neighborhood aggregation scheme. These operators are defined by using several stacked Graph Convolutional (GC) layers. They are usually defined as additive building blocks that fuse multiple information streams. However, when considering information integration in sequences, the flow of gradient has been shown to be more robust by adopting the Multiplicative Integration (MI) technique. Because of that, it is worth investigating the impact of MI in Graph Neural Networks. We propose three different GC layers that exploit MI to improve various aspects of the neighborhood aggregation scheme. We report both a theoretical and empirical comparison of our proposals with respect to the most common GC operators for the graph classification task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123088055",
                    "name": "Paolo Frazzetto"
                },
                {
                    "authorId": "32246134",
                    "name": "Luca Pasa"
                },
                {
                    "authorId": "1715584",
                    "name": "Nicol\u00f3 Navarin"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                }
            ]
        },
        {
            "paperId": "41b7c8bc5421a9185555d0f375663c3da0e2171d",
            "title": "Improving Soft Skill Extraction via Data Augmentation and Embedding Manipulation",
            "abstract": "Soft skills (SS) are important for Human Resource Management when recruiting suitable candidates for a job. Nowadays, enterprises aim to automatically extract such information from documents, curriculum vitae (CVs) and job descriptions, to speed up their recruitment process. State-of-the-art Large Language Models (LLMs) have been successful in Natural Language Processing (NLP) by fine-tuning them to the domain-specific task. However, annotated data for the task is very limited and costly to obtain, since it requires domain experts. Moreover, SS consists of complex long entities which are difficult to extract given few annotated examples. As a consequence, the performance of the LLMs on soft skill detection still needs improvement before being used in a real-world context. In this paper, we introduce data augmentation based entity extraction approach which shows promising performance when the entity length is long (i.e more than three tokens). Moreover, we explore the performance of pre-trained LLMs to generate synthetic data for training. The pre-trained models are used to generate contextual augmentation of the baseline dataset. We further analyse the embeddings generated by these models in aiding the extraction process of entities. We develop an Embedding Manipulation (EM) approach to further improve the performance of baseline models. We evaluated our approach on the only publicly available dataset for soft skills (SKILLSPAN), and on three Entity Extraction datasets (GUM, WNUT-2017 and CoNLL-2003) to assess the proposed approach. Empirical evidence shows that the proposed approach allows us to get 6.52% increased F1 over the baseline model for the soft skills.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "102653089",
                    "name": "Muhammad Uzair Ul Haq"
                },
                {
                    "authorId": "123088055",
                    "name": "Paolo Frazzetto"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                },
                {
                    "authorId": "134000266",
                    "name": "Giovanni Da San Martino"
                }
            ]
        },
        {
            "paperId": "53e592b2879969f4cc719f1636fc16d3dd7eaaf2",
            "title": "Physics-Informed Graph Neural Cellular Automata: an Application to Compartmental Modelling",
            "abstract": "The recent outbreak of COVID-19 has spurred global collaborative research efforts to model and forecast the disease to improve preparation and control. Epidemiological models integrate experimental data and expert opinions to understand infection dynamics and control measures. Classical Machine Learning techniques often face challenges such as high data requirements, lack of interpretability, and difficulty integrating domain knowledge. A potential solution is to leverage Physically-Informed Machine Learning (PIML) models, which enhance models by incorporating known physical properties of viral spread. Additionally, epidemiological datasets are best represented as graphs, facilitating the modelling of interactions between individuals. In this paper, we propose a novel, interpretable graph-based PIML technique called SINDy-Graph to model infectious disease dynamics. Our approach is a Graph Cellular Automata architecture that combines the ability to identify dynamics for discovering the differential equations governing the physical phenomena under study using graphs modelling relationships between nodes (individuals). The experimental results demonstrate that integrating domain knowledge ensures better physical plausibility. In addition, our proposed model is easier to train and achieves a lower generalisation error compared to other baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1715584",
                    "name": "Nicol\u00f3 Navarin"
                },
                {
                    "authorId": "123088055",
                    "name": "Paolo Frazzetto"
                },
                {
                    "authorId": "32246134",
                    "name": "Luca Pasa"
                },
                {
                    "authorId": "77998143",
                    "name": "Pietro Verzelli"
                },
                {
                    "authorId": "2320425769",
                    "name": "Filippo Visentin"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                },
                {
                    "authorId": "2320412717",
                    "name": "Cesare Alippi"
                }
            ]
        },
        {
            "paperId": "77338785e65aacf4d378034ff9822c00af45506c",
            "title": "Assessing the Emergent Symbolic Reasoning Abilities of Llama Large Language Models",
            "abstract": "Large Language Models (LLMs) achieve impressive performance in a wide range of tasks, even if they are often trained with the only objective of chatting fluently with users. Among other skills, LLMs show emergent abilities in mathematical reasoning benchmarks, which can be elicited with appropriate prompting methods. In this work, we systematically investigate the capabilities and limitations of popular open-source LLMs on different symbolic reasoning tasks. We evaluate three models of the Llama 2 family on two datasets that require solving mathematical formulas of varying degrees of difficulty. We test a generalist LLM (Llama 2 Chat) as well as two fine-tuned versions of Llama 2 (MAmmoTH and MetaMath) specifically designed to tackle mathematical problems. We observe that both increasing the scale of the model and fine-tuning it on relevant tasks lead to significant performance gains. Furthermore, using fine-grained evaluation measures, we find that such performance gains are mostly observed with mathematical formulas of low complexity, which nevertheless often remain challenging even for the largest fine-tuned models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171081238",
                    "name": "Flavio Petruzzellis"
                },
                {
                    "authorId": "1915299",
                    "name": "Alberto Testolin"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                }
            ]
        },
        {
            "paperId": "d6af363527a4edc215d1f51cf8a18cec04f0ff00",
            "title": "A Neural Rewriting System to Solve Algorithmic Problems",
            "abstract": "Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances. In this work, we focus on formula simplification problems, a class of synthetic benchmarks used to study the systematic generalization capabilities of neural architectures. We propose a modular architecture designed to learn a general procedure for solving nested mathematical formulas by only relying on a minimal set of training examples. Inspired by rewriting systems, a classic framework in symbolic artificial intelligence, we include in the architecture three specialized and interacting modules: the Selector, trained to identify solvable sub-expressions; the Solver, mapping sub-expressions to their values; and the Combiner, replacing sub-expressions in the original formula with the solution provided by the Solver. We benchmark our system against the Neural Data Router, a recent model specialized for systematic generalization, and a state-of-the-art large language model (GPT-4) probed with advanced prompting strategies. We demonstrate that our approach achieves a higher degree of out-of-distribution generalization compared to these alternative approaches on three different types of formula simplification problems, and we discuss its limitations by analyzing its failures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171081238",
                    "name": "Flavio Petruzzellis"
                },
                {
                    "authorId": "1915299",
                    "name": "Alberto Testolin"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                }
            ]
        },
        {
            "paperId": "25e83a88a782c68b4553013579e8744bf983e7b9",
            "title": "Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement",
            "abstract": "Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popular datasets, Flickr30k Entities and ReferIt, shining especially on ReferIt with a 9.6% absolute improvement. Moreover, thanks to the untrained component, it reaches competitive performances just using a small fraction of training examples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2082171598",
                    "name": "Davide Rigoni"
                },
                {
                    "authorId": "47115542",
                    "name": "Luca Parolari"
                },
                {
                    "authorId": "144077615",
                    "name": "L. Serafini"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                },
                {
                    "authorId": "1795847",
                    "name": "Lamberto Ballan"
                }
            ]
        },
        {
            "paperId": "3a329c7097e0add925ae52da501c142a3724a9fb",
            "title": "Topology preserving maps as aggregations for Graph Convolutional Neural Networks",
            "abstract": "In Graph Convolutional Neural Networks, the capability of learning the representation of graph nodes comes at hand when dealing with graph analysis tasks, such as predicting node properties. Furthermore, node-level representations can be aggregated to obtain a single graph-level representation and predictor. This work explores an alternative route for defining the aggregation function compared to existing approaches. We propose a graph aggregator that exploits Generative Topographic Mapping (GTM) to transform a set of node-level representations into a single graph-level one. The integration of GTM in a GCNN pipeline allows to estimate node representation probability densities and projects them in a low-dimensional space, while retaining the information about their mutual similarity and topology. A novel dedicated training procedure is specifically designed to learn from these reduced representations instead of the complete initial data. Experimental results on several graph classification datasets show that this approach achieves competitive predictive performances with respect to the commonly adopted aggregation architectures present in the literature while holding a well-grounded theoretical framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123088055",
                    "name": "Paolo Frazzetto"
                },
                {
                    "authorId": "32246134",
                    "name": "Luca Pasa"
                },
                {
                    "authorId": "1715584",
                    "name": "Nicol\u00f3 Navarin"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                }
            ]
        },
        {
            "paperId": "9d763e9432d3b568f2a3e6f7e375c643b9a0bff5",
            "title": "RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design",
            "abstract": "Identifying molecules that exhibit some pre-specified properties is a difficult problem to solve. In the last few years, deep generative models have been used for molecule generation. Deep Graph Variational Autoencoders are among the most powerful machine learning tools with which it is possible to address this problem. However, existing methods struggle in capturing the true data distribution and tend to be computationally expensive. In this work, we propose RGCVAE, an efficient and effective Graph Variational Autoencoder based on: (i) an encoding network exploiting a new powerful Relational Graph Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE shows state-of-the-art molecule generation performance while being significantly faster to train.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2082171598",
                    "name": "Davide Rigoni"
                },
                {
                    "authorId": "1715584",
                    "name": "Nicol\u00f3 Navarin"
                },
                {
                    "authorId": "1749815",
                    "name": "A. Sperduti"
                }
            ]
        }
    ]
}