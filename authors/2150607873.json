{
    "authorId": "2150607873",
    "papers": [
        {
            "paperId": "3a10c683a7dac0d30d4cecf0a2cf52fe7170d911",
            "title": "Dual Contrastive Transformer for Hierarchical Preference Modeling in Sequential Recommendation",
            "abstract": "Sequential recommender systems (SRSs) aim to predict the subsequent items which may interest users via comprehensively modeling users' complex preference embedded in the sequence of user-item interactions. However, most of existing SRSs often model users' single low-level preference based on item ID information while ignoring the high-level preference revealed by item attribute information, such as item category. Furthermore, they often utilize limited sequence context information to predict the next item while overlooking richer inter-item semantic relations. To this end, in this paper, we proposed a novel hierarchical preference modeling framework to substantially model the complex low- and high-level preference dynamics for accurate sequential recommendation. Specifically, in the framework, a novel dual-transformer module and a novel dual contrastive learning scheme have been designed to discriminatively learn users' low- and high-level preference and to effectively enhance both low- and high-level preference learning respectively. In addition, a novel semantics-enhanced context embedding module has been devised to generate more informative context embedding for further improving the recommendation performance. Extensive experiments on six real-world datasets have demonstrated both the superiority of our proposed method over the state-of-the-art ones and the rationality of our design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150607873",
                    "name": "Chengkai Huang"
                },
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2107924437",
                    "name": "Xianzhi Wang"
                },
                {
                    "authorId": "145095579",
                    "name": "L. Yao"
                }
            ]
        },
        {
            "paperId": "807a1e587fec61f50970c05055243cf959fb09e5",
            "title": "Modeling Temporal Positive and Negative Excitation for Sequential Recommendation",
            "abstract": "Sequential recommendation aims to predict the next item which interests users via modeling their interest in items over time. Most of the existing works on sequential recommendation model users\u2019 dynamic interest in specific items while overlooking users\u2019 static interest revealed by some static attribute information of items, e.g., category, brand. Moreover, existing works often only consider the positive excitation of a user\u2019s historical interactions on his/her next choice on candidate items while ignoring the commonly existing negative excitation, resulting in insufficiently modeling dynamic interest. The overlook of static interest and negative excitation will lead to incomplete interest modeling and thus impedes the recommendation performance. To this end, in this paper, we propose modeling both static interest and negative excitation for dynamic interest to further improve the recommendation performance. Accordingly, we design a novel Static-Dynamic Interest Learning (SDIL) framework featured with a novel Temporal Positive and Negative Excitation Modeling (TPNE) module for accurate sequential recommendation. TPNE is specially designed for comprehensively modeling dynamic interest based on temporal positive and negative excitation learning. Extensive experiments on three real-world datasets show that SDIL can effectively capture both static and dynamic interest and outperforms state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150607873",
                    "name": "Chengkai Huang"
                },
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2107924437",
                    "name": "Xianzhi Wang"
                },
                {
                    "authorId": "145095579",
                    "name": "L. Yao"
                }
            ]
        },
        {
            "paperId": "9530fbd544a7540d71b6142315421c85d6836a2e",
            "title": "Interventional Recommendation with Contrastive Counterfactual Learning for Better Understanding User Preferences",
            "abstract": "Recently, there has been a surging interest in formulating recommendations in the context of causal inference. The studies regard the recommendation as an intervention in causal inference and frame the users\u2019 preferences as interventional effects to improve recommender systems\u2019 generalization. Many studies in the field of causal inference for recommender systems have been focusing on utilizing propensity scores from the causal community that reduce the bias while inducing additional variance. Alternatively, some studies suggest the existence of a set of unbiased data from randomized controlled trials while it requires to satisfy certain assumptions that may be challenging in practice. In this paper, we first design a causal graph representing recommender systems\u2019 data generation and propagation process. Then, we reveal that the underlying exposure mechanism biases the maximum likelihood estimation (MLE) on observational feedback. In order to figure out users\u2019 preferences in terms of causality behind data, we leverage the back-door adjustment and do-calculus, which induces an inter-ventional recommendation model (IREC). Furthermore, considering the confounder may be inaccessible for measurement, we propose a contrastive counterfactual learning method (CCL) for simulating the intervention. In addition, we present two extra novel sampling strategies and show an intriguing finding that sampling from coun-terfactual sets contributes to superior performance. We perform extensive experiments on two real-world datasets to evaluate and analyze the performance of our model IREC-CCL on unbiased test sets. Experimental results demonstrate our model outperforms the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "93925973",
                    "name": "Guanglin Zhou"
                },
                {
                    "authorId": "2150607873",
                    "name": "Chengkai Huang"
                },
                {
                    "authorId": "48283085",
                    "name": "Xiaocong Chen"
                },
                {
                    "authorId": "2106357243",
                    "name": "Lina Yao"
                },
                {
                    "authorId": "3087664",
                    "name": "Xiwei Xu"
                },
                {
                    "authorId": "2109117515",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "2145199748",
                    "name": "Liming Zhu"
                }
            ]
        },
        {
            "paperId": "4c35a1add762b404d9bcc91fff2e301a83feb517",
            "title": "Semi-Online Knowledge Distillation",
            "abstract": "Knowledge distillation is an effective and stable method for model compression via knowledge transfer. Conventional knowledge distillation (KD) is to transfer knowledge from a large and well pre-trained teacher network to a small student network, which is a one-way process. Recently, deep mutual learning (DML) has been proposed to help student networks learn collaboratively and simultaneously. However, to the best of our knowledge, KD and DML have never been jointly explored in a unified framework to solve the knowledge distillation problem. In this paper, we investigate that the teacher model supports more trustworthy supervision signals in KD, while the student captures more similar behaviors from the teacher in DML. Based on these observations, we first propose to combine KD with DML in a unified framework. Furthermore, we propose a Semi-Online Knowledge Distillation (SOKD) method that effectively improves the performance of the student and the teacher. In this method, we introduce the peer-teaching training fashion in DML in order to alleviate the student's imitation difficulty, and also leverage the supervision signals provided by the well-trained teacher in KD. Besides, we also show our framework can be easily extended to feature-based distillation methods. Extensive experiments on CIFAR-100 and ImageNet datasets demonstrate the proposed method achieves state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118809537",
                    "name": "Zhiqiang Liu"
                },
                {
                    "authorId": "2130679830",
                    "name": "Yanxia Liu"
                },
                {
                    "authorId": "2150607873",
                    "name": "Chengkai Huang"
                }
            ]
        },
        {
            "paperId": "a1a952a140833bb85e502a2408b64c88c584ebdb",
            "title": "Improved Knowledge Distillation via Adversarial Collaboration",
            "abstract": "Knowledge distillation has become an important approach to obtain a compact yet effective model. To achieve this goal, a small student model is trained to exploit the knowledge of a large well-trained teacher model. However, due to the capacity gap between the teacher and the student, the student's performance is hard to reach the level of the teacher. Regarding this issue, existing methods propose to reduce the difficulty of the teacher's knowledge via a proxy way. We argue that these proxy-based methods overlook the knowledge loss of the teacher, which may cause the student to encounter capacity bottlenecks. In this paper, we alleviate the capacity gap problem from a new perspective with the purpose of averting knowledge loss. Instead of sacrificing part of the teacher's knowledge, we propose to build a more powerful student via adversarial collaborative learning. To this end, we further propose an Adversarial Collaborative Knowledge Distillation (ACKD) method that effectively improves the performance of knowledge distillation. Specifically, we construct the student model with multiple auxiliary learners. Meanwhile, we devise an adversarial collaborative module (ACM) that introduces attention mechanism and adversarial learning to enhance the capacity of the student. Extensive experiments on four classification tasks show the superiority of the proposed ACKD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118809537",
                    "name": "Zhiqiang Liu"
                },
                {
                    "authorId": "2150607873",
                    "name": "Chengkai Huang"
                },
                {
                    "authorId": "2130679830",
                    "name": "Yanxia Liu"
                }
            ]
        }
    ]
}