{
    "authorId": "2386396",
    "papers": [
        {
            "paperId": "ab1cc6a171fa23e32b4b91b1662c4b5be52e1675",
            "title": "Towards Flexible and Adaptive Neural Process for Cold-Start Recommendation",
            "abstract": "Recommender systems have been widely adopted in various online personal e-commerce applications for improving user experience. A long-standing challenge in recommender systems is how to provide accurate recommendation to users in cold-start situations where only a few user-item interactions can be observed. Recently, meta learning methods provide a promising solution, and most of them follow a way of parameter initialization where predictions can be fast adapted via multiple gradient descent steps. While these meta-learning recommenders promote model performance, how to derive a fundamental paradigm that enables both flexible approximations of complex user interaction distributions and effective task adaptations of global knowledge still remains a critical yet under-explored problem. To this end, we present the Flow-based Adaptive Neural Process (FANP), a new probabilistic meta-learning model where estimating the preference of each user is governed by an underlying stochastic process. Following an encoder-decoder generative framework, FANP is an effective few-shot function estimator that directly maps limited user interactions to a predictive distribution without complicated gradient updates. Through introducing a conditional normalization flow-based encoder, FANP can get rid of the model bias on latent variables and thereby derive more flexible variational distributions. Meanwhile, we propose a task-adaptive mechanism capturing the relevance of different tasks for improving adaptation ability of global knowledge. The learned task-specific and task-relevant representations are simultaneously exploited to generate the decoder parameters via a novel modulation-augmented hypernetwork. FANP is evaluated on both scenario-specific and user-specific cold-start recommendations on various real-world datasets. Extensive experimental results and detailed model analyses demonstrate that our model yields superior performance compared with multiple state-of-the-art meta-learning recommenders.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46519974",
                    "name": "Xixun Lin"
                },
                {
                    "authorId": "2110713858",
                    "name": "Chuan Zhou"
                },
                {
                    "authorId": "153171583",
                    "name": "Jia Wu"
                },
                {
                    "authorId": "8718022",
                    "name": "Lixin Zou"
                },
                {
                    "authorId": "2191655754",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "9310727",
                    "name": "Yanan Cao"
                },
                {
                    "authorId": "2256857762",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                }
            ]
        },
        {
            "paperId": "06f984f447271ef3bb982551d5199757478bcd24",
            "title": "Layout-aware Webpage Quality Assessment",
            "abstract": "Identifying high-quality webpages is fundamental for real-world search engines, which can fulfil users' information need with the less cognitive burden. Early studies of \\emph{webpage quality assessment} usually design hand-crafted features that may only work on particular categories of webpages (e.g., shopping websites, medical websites). They can hardly be applied to real-world search engines that serve trillions of webpages with various types and purposes. In this paper, we propose a novel layout-aware webpage quality assessment model currently deployed in our search engine. Intuitively, layout is a universal and critical dimension for the quality assessment of different categories of webpages. Based on this, we directly employ the meta-data that describes a webpage, i.e., Document Object Model (DOM) tree, as the input of our model. The DOM tree data unifies the representation of webpages with different categories and purposes and indicates the layout of webpages. To assess webpage quality from complex DOM tree data, we propose a graph neural network (GNN) based method that extracts rich layout-aware information that implies webpage quality in an end-to-end manner. Moreover, we improve the GNN method with an attentive readout function, external web categories and a category-aware sampling method. We conduct rigorous offline and online experiments to show that our proposed solution is effective in real search engines, improving the overall usability and user experience.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80658774",
                    "name": "Anfeng Cheng"
                },
                {
                    "authorId": "2108021633",
                    "name": "Yiding Liu"
                },
                {
                    "authorId": "2108793050",
                    "name": "Weibin Li"
                },
                {
                    "authorId": "2203368550",
                    "name": "Qian Dong"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2151325127",
                    "name": "Zhengjie Huang"
                },
                {
                    "authorId": "1718657",
                    "name": "Shikun Feng"
                },
                {
                    "authorId": "2788612",
                    "name": "Zhicong Cheng"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                }
            ]
        },
        {
            "paperId": "105669ec59a58fb2d4dd3021a984af33c227c5ab",
            "title": "Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs",
            "abstract": "Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at: https://github.com/CurryTang/Graph-LLM .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109393101",
                    "name": "Zhikai Chen"
                },
                {
                    "authorId": "2125202063",
                    "name": "Haitao Mao"
                },
                {
                    "authorId": "2145571830",
                    "name": "Hang Li"
                },
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "30580446",
                    "name": "Haifang Wen"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "2b4d0d99d1b210d97ca665239ef3e90d794af488",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "abstract": "Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. To tackle this issue, we Incorporate Implicit Interaction into dual-encoders, and propose I3 retriever. In particular, our implicit interaction paradigm leverages generated pseudo-queries to simulate query-passage interaction, which jointly optimizes with query and passage encoders in an end-to-end manner. It can be fully pre-computed and cached, and its inference process only involves simple dot product operation of the query vector and passage vector, which makes it as efficient as the vanilla dual encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep Learning Datasets, demonstrating the I3 retriever's superiority in terms of both effectiveness and efficiency. Moreover, the proposed implicit interaction is compatible with special pre-training and knowledge distillation for passage retrieval, which brings a new state-of-the-art performance. The codes are available at https://github.com/Deriq-Qian-Dong/III-Retriever.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2203368550",
                    "name": "Qian Dong"
                },
                {
                    "authorId": "2108021633",
                    "name": "Yiding Liu"
                },
                {
                    "authorId": "144922928",
                    "name": "Qingyao Ai"
                },
                {
                    "authorId": "2108590438",
                    "name": "Haitao Li"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "1783406",
                    "name": "Yiqun Liu"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "8093158",
                    "name": "Shaoping Ma"
                }
            ]
        },
        {
            "paperId": "60be11b0c34038d9ee156cbec6c4df5ae5db68b8",
            "title": "Pretrained Language Model based Web Search Ranking: From Relevance to Satisfaction",
            "abstract": "Search engine plays a crucial role in satisfying users' diverse information needs. Recently, Pretrained Language Models (PLMs) based text ranking models have achieved huge success in web search. However, many state-of-the-art text ranking approaches only focus on core relevance while ignoring other dimensions that contribute to user satisfaction, e.g., document quality, recency, authority, etc. In this work, we focus on ranking user satisfaction rather than relevance in web search, and propose a PLM-based framework, namely SAT-Ranker, which comprehensively models different dimensions of user satisfaction in a unified manner. In particular, we leverage the capacities of PLMs on both textual and numerical inputs, and apply a multi-field input that modularizes each dimension of user satisfaction as an input field. Overall, SAT-Ranker is an effective, extensible, and data-centric framework that has huge potential for industrial applications. On rigorous offline and online experiments, SAT-Ranker obtains remarkable gains on various evaluation sets targeting different dimensions of user satisfaction. It is now fully deployed online to improve the usability of our search engine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31917710",
                    "name": "Canjia Li"
                },
                {
                    "authorId": "48631781",
                    "name": "Xiaoyang Wang"
                },
                {
                    "authorId": "2162658764",
                    "name": "Dongdong Li"
                },
                {
                    "authorId": "2108021633",
                    "name": "Yiding Liu"
                },
                {
                    "authorId": "47006228",
                    "name": "Yujie Lu"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2788612",
                    "name": "Zhicong Cheng"
                },
                {
                    "authorId": "2181612436",
                    "name": "Simiu Gu"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                }
            ]
        },
        {
            "paperId": "c56cbec57fb45fc85eeec92805803c5e3386660d",
            "title": "COLTR: Semi-Supervised Learning to Rank With Co-Training and Over-Parameterization for Web Search",
            "abstract": "While <italic>learning to rank</italic> (LTR) has been widely used in web search to prioritize most relevant webpages among the retrieved contents subject to the input queries, the traditional LTR models fail to deliver decent performance due to two main reasons: 1) the lack of well-annotated query-webpage pairs with ranking scores to cover search queries of various popularity, and 2) ill-trained models based on a limited number of training samples with poor generalization performance. To improve the performance of LTR models, tremendous efforts have been done from above two aspects, such as enlarging training sets with pseudo-labels of ranking scores by self-training, or refining the features used for LTR through feature extraction and dimension reduction. Though LTR performance has been marginally increased, we still believe these methods could be further improved in the newly-fashioned \u201cinterpolating regime\u201d. Specifically, instead of lowering the number of features used for LTR models, our work proposes to transform original data with random Fourier feature, so as to over-parameterize the downstream LTR models (e.g., GBRank or LightGBM) with features in ultra-high dimensionality and achieve superb generalization performance. Furthermore, rather than self-training with pseudo-labels produced by the same LTR model in a \u201cself-tuned\u201d fashion, the proposed method incorporates the diversity of prediction results between the listwise and pointwise LTR models while co-training both models with a cyclic labeling-prediction pipeline in a \u201cping-pong\u201d manner. We deploy the proposed <italic><underline>C</underline>o-trained and <underline>O</underline>ver-parameterized <underline>LTR</underline></italic> system <bold>COLTR</bold> at Baidu search and evaluate <bold>COLTR</bold> with a large number of baseline methods. The results show that <bold>COLTR</bold> could achieve <inline-formula><tex-math notation=\"LaTeX\">$\\Delta NDCG_{4}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>\u0394</mml:mi><mml:mi>N</mml:mi><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href=\"kong-ieq1-3270750.gif\"/></alternatives></inline-formula> = 3.64%<inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math><alternatives><mml:math><mml:mo>\u223c</mml:mo></mml:math><inline-graphic xlink:href=\"kong-ieq2-3270750.gif\"/></alternatives></inline-formula>4.92%, compared to baselines, under various ratios of labeled samples. We also conduct a 7-day A/B Test using the realistic web traffics of Baidu Search, where we can still observe significant performance improvement around <inline-formula><tex-math notation=\"LaTeX\">$\\Delta NDCG_{4}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>\u0394</mml:mi><mml:mi>N</mml:mi><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href=\"kong-ieq3-3270750.gif\"/></alternatives></inline-formula> = 0.17%<inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math><alternatives><mml:math><mml:mo>\u223c</mml:mo></mml:math><inline-graphic xlink:href=\"kong-ieq4-3270750.gif\"/></alternatives></inline-formula>0.92% in real-world applications. <bold>COLTR</bold> performs consistently both in online and offline experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110482469",
                    "name": "Yuchen Li"
                },
                {
                    "authorId": "2093122747",
                    "name": "Haoyi Xiong"
                },
                {
                    "authorId": "50621243",
                    "name": "Qingzhong Wang"
                },
                {
                    "authorId": "3254296",
                    "name": "L. Kong"
                },
                {
                    "authorId": "145127257",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "2108564917",
                    "name": "Haifang Li"
                },
                {
                    "authorId": "2143957850",
                    "name": "Jiang Bian"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2116379978",
                    "name": "Guihai Chen"
                },
                {
                    "authorId": "1721158",
                    "name": "D. Dou"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                }
            ]
        },
        {
            "paperId": "cd558da4a936d17eb116780b5aa48ed51330fb08",
            "title": "S2phere: Semi-Supervised Pre-training for Web Search over Heterogeneous Learning to Rank Data",
            "abstract": "While Learning to Rank (LTR) models on top of transformers have been widely adopted to achieve decent performance, it is still challenging to train the model with sufficient data as only an extremely small number of query-webpage pairs could be annotated versus trillions of webpages available online and billions of web search queries everyday. In the meanwhile, industry research communities have released a number of open-source LTR datasets with well annotations but incorporating different designs of LTR features/labels (i.e., heterogeneous domains). In this work, inspired by the recent progress in pre-training transformers for performance advantages, we study the problem of pre-training LTR models using both labeled and unlabeled samples, especially we focus on the use of well-annotated samples in heterogeneous open-source LTR datasets to boost the performance of pre-training. Hereby, we propose S2phere-Semi-Supervised Pre-training with Heterogeneous LTR data strategies for LTR models using both unlabeled and labeled query-webpage pairs across heterogeneous LTR datasets. S2phere consists of a three-step approach: (1) Semi-supervised Feature Extraction Pre-training via Perturbed Contrastive Loss, (2) Cross-domain Ranker Pre-training over Heterogeneous LTR Datasets and (3) End-to-end LTR Fine-tuning via Modular Network Composition. Specifically, given an LTR model composed of a backbone (the feature extractor), a neck (the module to reason the orders) and a head (the predictor of ranking scores), S2phere uses unlabeled/labeled data from the search engine to pre-train the backbone in Step (1) via semi-supervised learning; then Step (2) incorporates multiple open-source heterogeneous LTR datasets to improve pre-training of the neck module as shared parameters of cross-domain learning; and finally, S2phere in Step (3) composes the backbone and neck with a randomly-initialized head into a whole LTR model and fine-tunes the model using search engine data with various learning strategies. Extensive experiments have been done with both offline experiments and online A/B Test on top of Baidu search engine. The comparisons against numbers of baseline algorithms confirmed the advantages of S2phere in producing high-performance LTR models for web-scale search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110482469",
                    "name": "Yuchen Li"
                },
                {
                    "authorId": "2093122747",
                    "name": "Haoyi Xiong"
                },
                {
                    "authorId": "3254296",
                    "name": "L. Kong"
                },
                {
                    "authorId": "50621243",
                    "name": "Qingzhong Wang"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2116379978",
                    "name": "Guihai Chen"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                }
            ]
        },
        {
            "paperId": "d66fe1ade1f2548bdcebd58005cf5fe7153ad9ca",
            "title": "Learning to Tokenize for Generative Retrieval",
            "abstract": "Conventional document retrieval techniques are mainly based on the index-retrieve paradigm. It is challenging to optimize pipelines based on this paradigm in an end-to-end manner. As an alternative, generative retrieval represents documents as identifiers (docid) and retrieves documents by generating docids, enabling end-to-end modeling of document retrieval tasks. However, it is an open question how one should define the document identifiers. Current approaches to the task of defining document identifiers rely on fixed rule-based docids, such as the title of a document or the result of clustering BERT embeddings, which often fail to capture the complete semantic information of a document. We propose GenRet, a document tokenization learning method to address the challenge of defining document identifiers for generative retrieval. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. Three components are included in GenRet: (i) a tokenization model that produces docids for documents; (ii) a reconstruction model that learns to reconstruct a document based on a docid; and (iii) a sequence-to-sequence retrieval model that generates relevant document identifiers directly for a designated query. By using an auto-encoding framework, GenRet learns semantic docids in a fully end-to-end manner. We also develop a progressive training scheme to capture the autoregressive nature of docids and to stabilize training. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets to assess the effectiveness of GenRet. GenRet establishes the new state-of-the-art on the NQ320K dataset. Especially, compared to generative retrieval baselines, GenRet can achieve significant improvements on the unseen documents. GenRet also outperforms comparable baselines on MS MARCO and BEIR, demonstrating the method's generalizability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153198380",
                    "name": "Weiwei Sun"
                },
                {
                    "authorId": "1387839383",
                    "name": "Lingyong Yan"
                },
                {
                    "authorId": "2117203270",
                    "name": "Zheng Chen"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2387872",
                    "name": "Haichao Zhu"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                },
                {
                    "authorId": "1721165",
                    "name": "Zhumin Chen"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "1696030",
                    "name": "M. de Rijke"
                },
                {
                    "authorId": "2780667",
                    "name": "Z. Ren"
                }
            ]
        },
        {
            "paperId": "efaedc173a0e8cafa6c24c3d799e2e4e9a21d340",
            "title": "Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies",
            "abstract": "Recently, a new paradigm called Differentiable Search Index (DSI) has been proposed for document retrieval, wherein a sequence-to-sequence model is learned to directly map queries to relevant document identifiers. The key idea behind DSI is to fully parameterize traditional ''index-retrieve'' pipelines within a single neural model, by encoding all documents in the corpus into the model parameters. In essence, DSI needs to resolve two major questions: (1) how to assign an identifier to each document, and (2) how to learn the associations between a document and its identifier. In this work, we propose a Semantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the area of Cognitive Psychology. Our approach advances original DSI in two ways: (1) For the document identifier, we take inspiration from Elaboration Strategies in human learning. Specifically, we assign each document an Elaborative Description based on the query generation technique, which is more meaningful than a string of integers in the original DSI; and (2) For the associations between a document and its identifier, we take inspiration from Rehearsal Strategies in human learning. Specifically, we select fine-grained semantic features from a document as Rehearsal Contents to improve document memorization. Both the offline and online experiments show improved retrieval performance over prevailing baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166392258",
                    "name": "Yubao Tang"
                },
                {
                    "authorId": "2109960367",
                    "name": "Ruqing Zhang"
                },
                {
                    "authorId": "1777025",
                    "name": "J. Guo"
                },
                {
                    "authorId": "2108313363",
                    "name": "Jiangui Chen"
                },
                {
                    "authorId": "2218182057",
                    "name": "Zuowei Zhu"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "0d0900f2afa1db5c4133907aaaacb21b9a8d86b3",
            "title": "Pre-trained Language Model-based Retrieval and Ranking for Web Search",
            "abstract": "Pre-trained language representation models (PLMs) such as BERT and Enhanced Representation through kNowledge IntEgration (ERNIE) have been integral to achieving recent improvements on various downstream tasks, including information retrieval. However, it is nontrivial to directly utilize these models for the large-scale web search due to the following challenging issues: (1) the prohibitively expensive computations of massive neural PLMs, especially for long texts in the web document, prohibit their deployments in the web search system that demands extremely low latency; (2) the discrepancy between existing task-agnostic pre-training objectives and the ad hoc retrieval scenarios that demand comprehensive relevance modeling is another main barrier for improving the online retrieval and ranking effectiveness; and (3) to create a significant impact on real-world applications, it also calls for practical solutions to seamlessly interweave the resultant PLM and other components into a cooperative system to serve web-scale data. Accordingly, we contribute a series of successfully applied techniques in tackling these exposed issues in this work when deploying the state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the online search engine system. We first present novel practices to perform expressive PLM-based semantic retrieval with a flexible poly-interaction scheme and cost-efficiently contextualize and rank web documents with a cheap yet powerful Pyramid-ERNIE architecture. We then endow innovative pre-training and fine-tuning paradigms to explicitly incentivize the query-document relevance modeling in PLM-based retrieval and ranking with the large-scale noisy and biased post-click behavioral data. We also introduce a series of effective strategies to seamlessly interwoven the designed PLM-based models with other conventional components into a cooperative system. Extensive offline and online experimental results show that our proposed techniques are crucial to achieving more effective search performance. We also provide a thorough analysis of our methodology and experimental results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8718022",
                    "name": "Lixin Zou"
                },
                {
                    "authorId": "1929269",
                    "name": "Weixue Lu"
                },
                {
                    "authorId": "2108021633",
                    "name": "Yiding Liu"
                },
                {
                    "authorId": "22561596",
                    "name": "Hengyi Cai"
                },
                {
                    "authorId": "90368708",
                    "name": "Xiaokai Chu"
                },
                {
                    "authorId": "2113509189",
                    "name": "Dehong Ma"
                },
                {
                    "authorId": "2104450297",
                    "name": "Daiting Shi"
                },
                {
                    "authorId": "2143831300",
                    "name": "Yu Sun"
                },
                {
                    "authorId": "2788612",
                    "name": "Zhicong Cheng"
                },
                {
                    "authorId": "2181612436",
                    "name": "Simiu Gu"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                }
            ]
        }
    ]
}