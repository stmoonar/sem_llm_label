{
    "authorId": "49843300",
    "papers": [
        {
            "paperId": "4f452e6a453f4ccd8f6c1960a8291a0ad8e4c2ee",
            "title": "Exploring the Limitations of Detecting Machine-Generated Text",
            "abstract": "Recent improvements in the quality of the generations by large language models have spurred research into identifying machine-generated text. Systems proposed for the task often achieve high performance. However, humans and machines can produce text in different styles and in different domains, and it remains unclear whether machine generated-text detection models favour particular styles or domains. In this paper, we critically examine the classification performance for detecting machine-generated text by evaluating on texts with varying writing styles. We find that classifiers are highly sensitive to stylistic changes and differences in text complexity, and in some cases degrade entirely to random classifiers. We further find that detection systems are particularly susceptible to misclassify easy-to-read texts while they have high performance for complex texts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1724523030",
                    "name": "Jad Doughman"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2261493094",
                    "name": "Hawau Olamide Toyin"
                },
                {
                    "authorId": "38510157",
                    "name": "Shady Shehata"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                }
            ]
        },
        {
            "paperId": "67f211fa66335dc32be2ec630b85a6f9026fa9de",
            "title": "What Can Natural Language Processing Do for Peer Review?",
            "abstract": "The number of scientific articles produced every year is growing rapidly. Providing quality control over them is crucial for scientists and, ultimately, for the public good. In modern science, this process is largely delegated to peer review -- a distributed procedure in which each submission is evaluated by several independent experts in the field. Peer review is widely used, yet it is hard, time-consuming, and prone to error. Since the artifacts involved in peer review -- manuscripts, reviews, discussions -- are largely text-based, Natural Language Processing has great potential to improve reviewing. As the emergence of large language models (LLMs) has enabled NLP assistance for many new tasks, the discussion on machine-assisted peer review is picking up the pace. Yet, where exactly is help needed, where can NLP help, and where should it stand aside? The goal of our paper is to provide a foundation for the future efforts in NLP for peer-reviewing assistance. We discuss peer review as a general process, exemplified by reviewing at AI conferences. We detail each step of the process from manuscript submission to camera-ready revision, and discuss the associated challenges and opportunities for NLP assistance, illustrated by existing work. We then turn to the big challenges in NLP for peer review as a whole, including data acquisition and licensing, operationalization and experimentation, and ethical issues. To help consolidate community efforts, we create a companion repository that aggregates key datasets pertaining to peer review. Finally, we issue a detailed call for action for the scientific community, NLP and AI researchers, policymakers, and funding bodies to help bring the research in NLP for peer review forward. We hope that our work will help set the agenda for research in machine-assisted scientific quality control in the age of AI, within the NLP community and beyond.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145775250",
                    "name": "Ilia Kuznetsov"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2301019670",
                    "name": "Koen Dercksen"
                },
                {
                    "authorId": "1740602147",
                    "name": "Nils Dycke"
                },
                {
                    "authorId": "2061487530",
                    "name": "Alexander Goldberg"
                },
                {
                    "authorId": "2301019717",
                    "name": "Tom Hope"
                },
                {
                    "authorId": "2324600714",
                    "name": "Dirk Hovy"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                },
                {
                    "authorId": "29891652",
                    "name": "Anne Lauscher"
                },
                {
                    "authorId": "2066411743",
                    "name": "Kevin Leyton-Brown"
                },
                {
                    "authorId": "2237947104",
                    "name": "Sheng Lu"
                },
                {
                    "authorId": "2188374527",
                    "name": "Mausam"
                },
                {
                    "authorId": "2921990",
                    "name": "Margot Mieskes"
                },
                {
                    "authorId": "2190281078",
                    "name": "Aur'elie N'ev'eol"
                },
                {
                    "authorId": "2064506371",
                    "name": "Danish Pruthi"
                },
                {
                    "authorId": "2301070055",
                    "name": "Lizhen Qu"
                },
                {
                    "authorId": "2287451699",
                    "name": "Roy Schwartz"
                },
                {
                    "authorId": "2288092270",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "1794626",
                    "name": "T. Solorio"
                },
                {
                    "authorId": "2301091804",
                    "name": "Jingyan Wang"
                },
                {
                    "authorId": "2279988869",
                    "name": "Xiaodan Zhu"
                },
                {
                    "authorId": "2301020698",
                    "name": "Anna Rogers"
                },
                {
                    "authorId": "2266853981",
                    "name": "Nihar B. Shah"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "b035635e7a715989941af7e1b4c0b1e09e8628df",
            "title": "FRAPPE: FRAming, Persuasion, and Propaganda Explorer",
            "abstract": "The abundance of news sources and the urgent demand for reliable information have led to serious concerns about the threat of misleading information. In this paper, we present FRAPPE, a FRAming, Persuasion, and Propaganda Explorer system. FRAPPE goes beyond conventional news analysis of articles and unveils the intricate linguistic techniques used to shape readers\u2019 opinions and emotions. Our system allows users not only to analyze individual articles for their genre, framings, and use of persuasion techniques, but also to draw comparisons between the strategies of persuasion and framing adopted by a diverse pool of news outlets and countries across multiple languages for different topics, thus providing a comprehensive understanding of how information is presented and manipulated. FRAPPE is publicly accessible at https://frappe.streamlit.app/ and a video explaining our system is available at https://www.youtube.com/watch?v=3RlTfSVnZmk",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2291367379",
                    "name": "Ahmed Sajwani"
                },
                {
                    "authorId": "2293402909",
                    "name": "Alaa Elsetohy"
                },
                {
                    "authorId": "2291366219",
                    "name": "Ali Mekky"
                },
                {
                    "authorId": "2291363086",
                    "name": "Diana Turmakhan"
                },
                {
                    "authorId": "2291361864",
                    "name": "Lara Hassan"
                },
                {
                    "authorId": "2293401556",
                    "name": "Mohamed El Zeftawy"
                },
                {
                    "authorId": "2293401831",
                    "name": "Omar El Herraoui"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2291368505",
                    "name": "Qisheng Liao"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                }
            ]
        },
        {
            "paperId": "c56eab12bd00e2fe28868af21d518044d66df00d",
            "title": "SemEval-2024 Task 8: Multidomain, Multimodel and Multilingual Machine-Generated Text Detection",
            "abstract": "We present the results and the main findings of SemEval-2024 Task 8: Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. The task featured three subtasks. Subtask A is a binary classification task determining whether a text is written by a human or generated by a machine. This subtask has two tracks: a monolingual track focused solely on English texts and a multilingual track. Subtask B is to detect the exact source of a text, discerning whether it is written by a human or generated by a specific LLM. Subtask C aims to identify the changing point within a text, at which the authorship transitions from human to machine. The task attracted a large number of participants: subtask A monolingual (126), subtask A multilingual (59), subtask B (70), and subtask C (30). In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used. For all subtasks, the best systems used LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2058455381",
                    "name": "Petar Ivanov"
                },
                {
                    "authorId": "2265989879",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "2284686859",
                    "name": "Giovanni Puccetti"
                },
                {
                    "authorId": "2284687590",
                    "name": "Thomas Arnold"
                },
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "ea4c0ab66529cac83f0b2b50eaef305da6a297e1",
            "title": "LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection",
            "abstract": "The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs). Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated). This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains. In this paper, we present $\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection. It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished. Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation. This might be useful in some domains like education, where any LLM intervention is usually prohibited. Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains. LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video describing our system is available at https://youtu.be/E8eT_bE7k8c.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2315296727",
                    "name": "Mervat Abassy"
                },
                {
                    "authorId": "2315302985",
                    "name": "Kareem Elozeiri"
                },
                {
                    "authorId": "2315840157",
                    "name": "Alexander Aziz"
                },
                {
                    "authorId": "2315297170",
                    "name": "Minh Ngoc Ta"
                },
                {
                    "authorId": "2309163441",
                    "name": "Raj Vardhan Tomar"
                },
                {
                    "authorId": "2315301202",
                    "name": "Bimarsha Adhikari"
                },
                {
                    "authorId": "2315642904",
                    "name": "Saad El Dine Ahmed"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2315671993",
                    "name": "Zhuohan Xie"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2300660029",
                    "name": "Ekaterina Artemova"
                },
                {
                    "authorId": "51259225",
                    "name": "V. Mikhailov"
                },
                {
                    "authorId": "2308041454",
                    "name": "Rui Xing"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "2300555930",
                    "name": "Hasan Iqbal"
                },
                {
                    "authorId": "2266755049",
                    "name": "Zain Muhammad Mujahid"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "5c577988ccebfea96de86678d04fd94fad367d2e",
            "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
            "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "3422905",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "2087720002",
                    "name": "Bokang Jia"
                },
                {
                    "authorId": "2235818050",
                    "name": "Satheesh Katipomu"
                },
                {
                    "authorId": "49404498",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2203791403",
                    "name": "Samta Kamboj"
                },
                {
                    "authorId": "22171629",
                    "name": "O. Pandit"
                },
                {
                    "authorId": "2235794681",
                    "name": "Rahul Pal"
                },
                {
                    "authorId": "2076256459",
                    "name": "Lalit Pradhan"
                },
                {
                    "authorId": "123838298",
                    "name": "Zainul Mujahid"
                },
                {
                    "authorId": "1380273855",
                    "name": "Massa Baali"
                },
                {
                    "authorId": "2110982198",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "100468503",
                    "name": "Zhengzhong Liu"
                },
                {
                    "authorId": "2235826325",
                    "name": "Andy Hock"
                },
                {
                    "authorId": "77917645",
                    "name": "Andrew Feldman"
                },
                {
                    "authorId": "2235945609",
                    "name": "Jonathan Lee"
                },
                {
                    "authorId": "2064974174",
                    "name": "A. Jackson"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "145465286",
                    "name": "Timothy Baldwin"
                },
                {
                    "authorId": "2064963077",
                    "name": "Eric P. Xing"
                }
            ]
        },
        {
            "paperId": "60730c7baeeabf4ff2fd824effc40bca465b1334",
            "title": "M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries. However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia. In this study, we strive to create automated systems that can detect machine-generated texts and pinpoint potential misuse. We first introduce a large-scale benchmark M4, which is a multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and that there is a lot of room for improvement. We believe that our dataset will enable future research towards more robust approaches to this pressing societal problem. The dataset is available at https://github.com/mbzuai-nlp/M4",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115829571",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2058455381",
                    "name": "Petar Ivanov"
                },
                {
                    "authorId": "2116966710",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "72c62b3a2280e66499d5918fadc3c31474425768",
            "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
            "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document, aiming to facilitate the evaluation of automatic fact-checking systems. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims, with the best F1=0.63 by this annotation solution based on GPT-4. Annotation tool, benchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2266755026",
                    "name": "Revanth Gangi Reddy"
                },
                {
                    "authorId": "2266755049",
                    "name": "Zain Muhammad Mujahid"
                },
                {
                    "authorId": "1943255906",
                    "name": "Arnav Arora"
                },
                {
                    "authorId": "2266754085",
                    "name": "Aleksandr Rubashevskii"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2266841994",
                    "name": "Liangming Pan"
                },
                {
                    "authorId": "2107059981",
                    "name": "Nadav Borenstein"
                },
                {
                    "authorId": "2266754790",
                    "name": "Aditya Pillai"
                },
                {
                    "authorId": "2256988818",
                    "name": "Isabelle Augenstein"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "2a8ec1df3946347bb4f96507d596408d05ce9214",
            "title": "On Smart Gaze Based Annotation of Histopathology Images for Training of Deep Convolutional Neural Networks",
            "abstract": "Unavailability of large training datasets is a bottleneck that needs to be overcome to realize the true potential of deep learning in histopathology applications. Although slide digitization via whole slide imaging scanners has increased the speed of data acquisition, labeling of virtual slides requires a substantial time investment from pathologists. Eye gaze annotations have the potential to speed up the slide labeling process. This work explores the viability and timing comparisons of eye gaze labeling compared to conventional manual labeling for training object detectors. Challenges associated with gaze based labeling and methods to refine the coarse data annotations for subsequent object detection are also discussed. Results demonstrate that gaze tracking based labeling can save valuable pathologist time and delivers good performance when employed for training a deep object detector. Using the task of localization of Keratin Pearls in cases of oral squamous cell carcinoma as a test case, we compare the performance gap between deep object detectors trained using hand-labelled and gaze-labelled data. On average, compared to \u2018Bounding-box\u2019 based hand-labeling, gaze-labeling required 57.6% less time per label and compared to \u2018Freehand\u2019 labeling, gaze-labeling required on average 85% less time per label.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1617883942",
                    "name": "Komal Mariam"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2000619845",
                    "name": "Wajahat Hussain"
                },
                {
                    "authorId": "24463293",
                    "name": "M. Javed"
                },
                {
                    "authorId": "50715158",
                    "name": "A. Kiyani"
                },
                {
                    "authorId": "2229652",
                    "name": "N. Rajpoot"
                },
                {
                    "authorId": "9378653",
                    "name": "S. Khurram"
                },
                {
                    "authorId": "9320951",
                    "name": "H. Khan"
                }
            ]
        }
    ]
}