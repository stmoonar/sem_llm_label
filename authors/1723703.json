{
    "authorId": "1723703",
    "papers": [
        {
            "paperId": "4df8f1b032ca763c9dc73ec45a2d9c57571eb76f",
            "title": "Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence Coverage",
            "abstract": "News media structure their reporting of events or issues using certain perspectives. When describing an incident involving gun violence, for example, some journalists may focus on mental health or gun regulation, while others may emphasize the discussion of gun rights. Such perspectives are called \\say{frames} in communication research. We study, for the first time, the value of combining lead images and their contextual information with text to identify the frame of a given news article. We observe that using multiple modes of information(article- and image-derived features) improves prediction of news frames over any single mode of information when the images are relevant to the frames of the headlines. We also observe that frame image relevance is related to the ease of conveying frames via images, which we call frame concreteness. Additionally, we release the first multimodal news framing dataset related to gun violence in the U.S., curated and annotated by communication researchers. The dataset will allow researchers to further examine the use of multiple information modalities for studying media framing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72437140",
                    "name": "Isidora Chara Tourni"
                },
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "2140488680",
                    "name": "T. Daryanto"
                },
                {
                    "authorId": "2140490515",
                    "name": "Fabian Zhafransyah"
                },
                {
                    "authorId": "2065594140",
                    "name": "Edward Edberg Halim"
                },
                {
                    "authorId": "47801182",
                    "name": "Mona Jalal"
                },
                {
                    "authorId": "2882606",
                    "name": "Boqi Chen"
                },
                {
                    "authorId": "98229497",
                    "name": "Shan-Ching Lai"
                },
                {
                    "authorId": "2142605154",
                    "name": "Hengchang Hu"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "1756038",
                    "name": "P. Ishwar"
                },
                {
                    "authorId": "2129412",
                    "name": "D. Wijaya"
                }
            ]
        },
        {
            "paperId": "6a15a41b6e32a070b8cd899daba5f8fcdde42634",
            "title": "A Lesion-aware Edge-based Graph Neural Network for Predicting Language Ability in Patients with Post-stroke Aphasia",
            "abstract": "We propose a lesion-aware graph neural network (LEGNet) to predict language ability from resting-state fMRI (rs-fMRI) connectivity in patients with post-stroke aphasia. Our model integrates three components: an edge-based learning module that encodes functional connectivity between brain regions, a lesion encoding module, and a subgraph learning module that leverages functional similarities for prediction. We use synthetic data derived from the Human Connectome Project (HCP) for hyperparameter tuning and model pretraining. We then evaluate the performance using repeated 10-fold cross-validation on an in-house neuroimaging dataset of post-stroke aphasia. Our results demonstrate that LEGNet outperforms baseline deep learning methods in predicting language ability. LEGNet also exhibits superior generalization ability when tested on a second in-house dataset that was acquired under a slightly different neuroimaging protocol. Taken together, the results of this study highlight the potential of LEGNet in effectively learning the relationships between rs-fMRI connectivity and language ability in a patient cohort with brain lesions for improved post-stroke aphasia evaluation.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science",
                "Engineering",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2319759803",
                    "name": "Zijian Chen"
                },
                {
                    "authorId": "3730107",
                    "name": "M. Varkanitsa"
                },
                {
                    "authorId": "1756038",
                    "name": "P. Ishwar"
                },
                {
                    "authorId": "2242030160",
                    "name": "Janusz Konrad"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "2242026443",
                    "name": "Swathi Kiran"
                },
                {
                    "authorId": "2242031740",
                    "name": "Archana Venkataraman"
                }
            ]
        },
        {
            "paperId": "702e06f3fc35da86fb4238d36477250f296f7d01",
            "title": "Graph attention-based fusion of pathology images and gene expression for prediction of cancer survival",
            "abstract": "Multimodal machine learning models are being developed to analyze pathology images and other modalities, such as gene expression, to gain clinical and biological in-sights. However, most frameworks for multimodal data fusion do not fully account for the interactions between different modalities. Here, we present an attention-based fusion architecture that integrates a graph representation of pathology images with gene expression data and concomitantly learns from the fused information to predict patient-specific survival. In our approach, pathology images are represented as undirected graphs, and their embeddings are combined with embeddings of gene expression signatures using an attention mechanism to stratify tumors by patient survival. We show that our framework improves the survival prediction of human non-small cell lung cancers, out-performing existing state-of-the-art approaches that lever-age multimodal data. Our framework can facilitate spatial molecular profiling to identify tumor heterogeneity using pathology images and gene expression data, complementing results obtained from more expensive spatial transcriptomic and proteomic technologies.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "9085030",
                    "name": "Yi Zheng"
                },
                {
                    "authorId": "90420656",
                    "name": "Regan D. Conrad"
                },
                {
                    "authorId": "2165662979",
                    "name": "Emily J. Green"
                },
                {
                    "authorId": "2261399152",
                    "name": "Eric J. Burks"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "2261400134",
                    "name": "Jennifer Beane"
                },
                {
                    "authorId": "1887282",
                    "name": "V. Kolachalama"
                }
            ]
        },
        {
            "paperId": "f58207d75838a6f719ec07b5b54ca09ada7bd226",
            "title": "Enhancing Emotion Prediction in News Headlines: Insights from ChatGPT and Seq2Seq Models for Free-Text Generation",
            "abstract": "Predicting emotions elicited by news headlines can be challenging as the task is largely influenced by the varying nature of people\u2019s interpretations and backgrounds. Previous works have explored classifying discrete emotions directly from news headlines. We provide a different approach to tackling this problem by utilizing people\u2019s explanations of their emotion, written in free-text, on how they feel after reading a news headline. Using the dataset BU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the free-text explanations have a strong correlation with the dominant emotion elicited by the headlines. The free-text explanations also contain more sentimental context than the news headlines alone and can serve as a better input to emotion classification models. Therefore, in this work we explored generating emotion explanations from headlines by training a sequence-to-sequence transformer model and by using pretrained large language model, ChatGPT (GPT-4). We then used the generated emotion explanations for emotion classification. In addition, we also experimented with training the pretrained T5 model for the intermediate task of explanation generation before fine-tuning it for emotion classification. Using McNemar\u2019s significance test, methods that incorporate GPT-generated free-text emotion explanations demonstrated significant improvement (P-value < 0.05) in emotion classification from headlines, compared to methods that only use headlines. This underscores the value of using intermediate free-text explanations for emotion prediction tasks with headlines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2154652618",
                    "name": "Ge Gao"
                },
                {
                    "authorId": "2255765211",
                    "name": "Jongin Kim"
                },
                {
                    "authorId": "2132933809",
                    "name": "Sejin Paik"
                },
                {
                    "authorId": "2279841069",
                    "name": "Ekaterina Novozhilova"
                },
                {
                    "authorId": "2301634431",
                    "name": "Yi Liu"
                },
                {
                    "authorId": "2255879906",
                    "name": "Sarah Bonna"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "2129412",
                    "name": "D. Wijaya"
                }
            ]
        },
        {
            "paperId": "395bec35efe0a965c88aac013372967bed23dd2f",
            "title": "Consistent long-term practice leads to consistent improvement: Benefits of self-managed therapy for language and cognitive deficits using a digital therapeutic",
            "abstract": "Background Although speech-language therapy (SLT) is proven to be beneficial to recovery of post-stroke aphasia, delivering sufficiently high amounts of dosage remains a problem in real-world clinical practice. Self-managed SLT was introduced to solve the problem. Previous research showed in a 10-week period, increased dosage frequency could lead to better performance, however, it is uncertain if dosage still affects performance over a longer period of practice time and whether gains can be seen following practice over several months. Objective This study aims to evaluate data from a health app (Constant Therapy) to investigate the relationship between dosage amount and improvements following a 30-week treatment period. Two cohorts of users were analyzed. One was comprised of patients with a consistent average weekly dosage amount and the other cohort was comprised of users whose practice had higher variability. Methods We conducted two analyses with two cohorts of post-stroke patients who used Constant Therapy. The first cohort contains 537 \u201cconsistent\u201d users, while the second cohort contains 2,159. The 30-week practice period was split into three consecutive 10-week practice windows to calculate average dosage amount. In each 10-week practice period, patients were grouped by their average dosage into low (0\u201315\u2005min/week), medium (15\u201340\u2005min/week) and moderate dosage (greater than 40\u2005min/week) groups. Linear mixed-effects models were employed to evaluate if dosage amount was a significant factor affecting performance. Pairwise comparison was also applied to evaluate the slope difference between groups. Results For the consistent cohort, medium (\u03b2\u2009=\u2009.002, t17,700\u2009=\u20097.64, P\u2009<\u2009.001) and moderate (\u03b2\u2009=\u2009.003, t9,297\u2009=\u20097.94, P\u2009<\u2009.001) dosage groups showed significant improvement compared to the low dosage group. The moderate group also showed greater improvement compared to the medium group. For the variable cohort in analysis 2, the same trend was shown in the first two 10-week windows, however, in weeks 21\u201330, the difference was insignificant between low and medium groups (\u03b2\u2009=\u2009.001, t\u2009=\u20091.76, P\u2009=\u2009.078). Conclusions This study showed a higher dosage amount is related to greater therapy outcomes in over 6 months of digital self-managed therapy. It also showed that regardless of the exact pattern of practice, self-managed SLT leads to significant and sustained performance gains.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2118960645",
                    "name": "Hantian Liu"
                },
                {
                    "authorId": "37862934",
                    "name": "Claire Cordella"
                },
                {
                    "authorId": "1756038",
                    "name": "P. Ishwar"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "32459665",
                    "name": "S. Kiran"
                }
            ]
        },
        {
            "paperId": "3e44d5d49022a9fe264fdd6b27aa91f6a62b41d1",
            "title": "Face Readers: The Frontier of Computer Vision and Math Learning",
            "abstract": "The future of AI-assisted individualized learning includes computer vision to inform intelligent tutors and teachers about student affect, motivation and performance. Facial expression recognition is essential in recognizing subtle differences when students ask for hints or fail to solve problems. Facial features and classification labels enable intelligent tutors to predict students\u2019 performance and recommend activities. Videos can capture students\u2019 faces and model their effort and progress; machine learning classifiers can support intelligent tutors to provide interventions. One goal of this research is to support deep dives by teachers to identify students\u2019 individual needs through facial expression and to provide immediate feedback. Another goal is to develop data-directed education to gauge students\u2019 pre-existing knowledge and analyze real-time data that will engage both teachers and students in more individualized and precision teaching and learning. This paper identifies three phases in the process of recognizing and predicting student progress based on analyzing facial features: Phase I: Collecting datasets and identifying salient labels for facial features and student attention/engagement; Phase II: Building and training deep learning models of facial features; and Phase III: Predicting student problem-solving outcome.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3325410",
                    "name": "B. Woolf"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "2149179009",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2141772809",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "145323574",
                    "name": "I. Arroyo"
                },
                {
                    "authorId": "2247909001",
                    "name": "John J. Magee"
                },
                {
                    "authorId": "3408856",
                    "name": "Danielle A. Allessio"
                },
                {
                    "authorId": "2111521639",
                    "name": "William Rebelsky"
                }
            ]
        },
        {
            "paperId": "40b0e4508ac4504149f3f835c353660abacc172c",
            "title": "Age-constrained Ear Recognition: The EICZA Dataset and SASE Baseline Model",
            "abstract": "Using the ear as a biometric identifier, particularly for children in healthcare settings, has an important advantage over using the face - the privacy of the person can be protected better. However, aging and the resulting appearance differences, known to be challenges for face recognition models, have not been addressed for ear recognition yet. To address this limitation, we curated a publicly available dataset, which we call Ears of Infant Cohort in Zambia with Aging (EICZA) 1. The dataset contains 3,330 ear images of 177 subjects, each photographed multiple times between the ages of 6 days and 9 months, when ear growth is most significant. For the task of age-constrained ear recognition, i.e., recognizing a person who has aged since the model was trained, we propose a new ear recognition model, called SASE for Self-Attention-based Sequential Ear image analysis. The model takes a sequence of ear images at early but different ages as input (instead of a single image) and processes them with a feature extraction network and a Transformer encoder. Trained with a large margin cosine loss function, the model is encouraged to learn a feature representation that distinguishes subjects from each other: Our experiments show that accounting for age enables our model to outperform other models that do not in recognizing ears that have grown and look different in later time periods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47844337",
                    "name": "Wenda Qin"
                },
                {
                    "authorId": "47440720",
                    "name": "L. Etter"
                },
                {
                    "authorId": "133666394",
                    "name": "Alinani Simukanga"
                },
                {
                    "authorId": "2266170338",
                    "name": "Christopher J. Gill"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        },
        {
            "paperId": "704d9cec55737e4e49549d28971df9921f9fedeb",
            "title": "ATL-BP: A Student Engagement Dataset and Model for Affect Transfer Learning for Behavior Prediction",
            "abstract": "We propose a video-based transfer learning approach for predicting problem outcomes of students working with an intelligent tutoring system (ITS) by analyzing their faces and gestures. The ability to predict such outcomes enables tutoring systems to adjust interventions and ultimately yield improved student learning. We collected and released a labeled dataset of 2,749 problem-solving interaction samples of 54 students working with an intelligent online math tutor. Our transfer-learning challenge was then to design a representation in the source domain of images obtained from the Internet for facial expression analysis, and transfer this learned representation for human behavior prediction in the domain of webcam videos of students in a classroom environment. We developed a novel facial affect representation and a user-personalized training scheme that unlocks the potential of this representation. We designed several variants of a recurrent neural network that models the temporal structure of video sequences. Our final model, named ATL-BP for Affect Transfer Learning for Behavior Prediction, achieves a relative increase in the mean F-score of 50% over the state-of-the-art method on this new dataset. We also propose an additional set of annotations to predict students\u2019 engagement while solving a specific problem, and present models that can predict such engagement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31601235",
                    "name": "Nataniel Ruiz"
                },
                {
                    "authorId": "2149179009",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "3408856",
                    "name": "Danielle A. Allessio"
                },
                {
                    "authorId": "47801182",
                    "name": "Mona Jalal"
                },
                {
                    "authorId": "1702222",
                    "name": "Ajjen Joshi"
                },
                {
                    "authorId": "145934474",
                    "name": "T. Murray"
                },
                {
                    "authorId": "1703932",
                    "name": "John J. Magee"
                },
                {
                    "authorId": "2186698768",
                    "name": "Kevin Manuel Delgado"
                },
                {
                    "authorId": "1852308",
                    "name": "Vitaly Ablavsky"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "145323574",
                    "name": "I. Arroyo"
                },
                {
                    "authorId": "3325410",
                    "name": "B. Woolf"
                },
                {
                    "authorId": "2141772809",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        },
        {
            "paperId": "9d7acf719d393488dca8a24146d6b578c7265bb7",
            "title": "COVES: A Cognitive-Affective Deep Model that Personalizes Math Problem Difficulty in Real Time and Improves Student Engagement with an Online Tutor",
            "abstract": "A key to personalized online learning is presenting content at an appropriate difficulty level; content that is too difficult can cause frustration and content that is too easy may result in boredom. Appropriate content can improve students' engagement and learning outcome. In this research, we propose a computer vision enhanced problem selector (COVES), a deep learning model to select a personalized difficulty level for each student. A combination of visual information and traditional log data is used to predict student-problem interactions, which are then used to guide problem difficulty selection in real time. COVES was trained on a dataset of fifty-one sixth-grade students interacting with the online math tutor MathSpring. Once COVES was integrated into the tutor, its effectiveness was tested with twenty-two seventh-grade students in controlled experiments. Students who received problems at an appropriate difficulty level, based on real-time predictions of their performance, demonstrated improved engagement with the math tutor. Results indicate that COVES leads to higher mastery of math concepts, better timing, and higher scores, thus providing a positive learning experience for the participants.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149179009",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "3408856",
                    "name": "Danielle A. Allessio"
                },
                {
                    "authorId": "2118406194",
                    "name": "William Lee"
                },
                {
                    "authorId": "2111521639",
                    "name": "William Rebelsky"
                },
                {
                    "authorId": "2128881594",
                    "name": "Frank Sylvia"
                },
                {
                    "authorId": "2261752126",
                    "name": "Tom Murray"
                },
                {
                    "authorId": "2247909001",
                    "name": "John J. Magee"
                },
                {
                    "authorId": "145323574",
                    "name": "I. Arroyo"
                },
                {
                    "authorId": "3325410",
                    "name": "B. Woolf"
                },
                {
                    "authorId": "2141772809",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        },
        {
            "paperId": "c77f672685aa891d5beabf02ce4efc5d4444a5cb",
            "title": "BU-CVKit: Extendable Computer Vision Framework for Species Independent Tracking and Analysis",
            "abstract": "A major bottleneck of interdisciplinary computer vision (CV) research is the lack of a framework that eases the reuse and abstraction of state-of-the-art CV models by CV and non-CV researchers alike. We present here BU-CVKit, a computer vision framework that allows the creation of research pipelines with chainable Processors. The community can create plugins of their work for the framework, hence improving the re-usability, accessibility, and exposure of their work with minimal overhead. Furthermore, we provide MuSeqPose Kit, a user interface for the pose estimation package of BU-CVKit, which automatically scans for installed plugins and programmatically generates an interface for them based on the metadata provided by the user. It also provides software support for standard pose estimation features such as annotations, 3D reconstruction, reprojection, and camera calibration. Finally, we show examples of behavioral neuroscience pipelines created through the sample plugins created for our framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192362391",
                    "name": "Mahir Patel"
                },
                {
                    "authorId": "71084437",
                    "name": "Lucas C. Carstensen"
                },
                {
                    "authorId": "46964309",
                    "name": "Yiwen Gu"
                },
                {
                    "authorId": "1729874",
                    "name": "Michael Hasselmo"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        }
    ]
}