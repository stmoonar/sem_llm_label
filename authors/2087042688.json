{
    "authorId": "2087042688",
    "papers": [
        {
            "paperId": "e466d857367bfc53a2f1137c65ed5bda2718322a",
            "title": "Recent advancements in multimodal human\u2013robot interaction",
            "abstract": "Robotics have advanced significantly over the years, and human\u2013robot interaction (HRI) is now playing an important role in delivering the best user experience, cutting down on laborious tasks, and raising public acceptance of robots. New HRI approaches are necessary to promote the evolution of robots, with a more natural and flexible interaction manner clearly the most crucial. As a newly emerging approach to HRI, multimodal HRI is a method for individuals to communicate with a robot using various modalities, including voice, image, text, eye movement, and touch, as well as bio-signals like EEG and ECG. It is a broad field closely related to cognitive science, ergonomics, multimedia technology, and virtual reality, with numerous applications springing up each year. However, little research has been done to summarize the current development and future trend of HRI. To this end, this paper systematically reviews the state of the art of multimodal HRI on its applications by summing up the latest research articles relevant to this field. Moreover, the research development in terms of the input signal and the output signal is also covered in this manuscript.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                },
                {
                    "authorId": "145356054",
                    "name": "Wen Qi"
                },
                {
                    "authorId": "2108200691",
                    "name": "Jiahao Chen"
                },
                {
                    "authorId": "2197072898",
                    "name": "Chenguang Yang"
                },
                {
                    "authorId": "145144839",
                    "name": "J. Sandoval"
                },
                {
                    "authorId": "50539114",
                    "name": "M. Laribi"
                }
            ]
        },
        {
            "paperId": "0071604cdf29429e0fa3d8a58d66b3874eb5645f",
            "title": "Multimodal Emotion Recognition Based on Feature Fusion",
            "abstract": "In the field of human-computer interaction, human emotion recognition is a challenging problem, and it is also a key link to achieve barrier-free communication between human and machine. At present, most of the emotion recognition algorithms are constructed based on single modal social information, and the recognition results are one-sided and easily disturbed. The recognition accuracy is often difficult to meet the practical requirements after being separated from specific social environment conditions. Based on the above situation and problems, this paper adopts multimodal input and simultaneously includes three modal information of audio, text and facial expression to recognition emotion. Three single modal emotion recognition models are proposed based on three different input information, and the multimodal emotion recognition model are constructed by different feature fusion methods. The experimental results showed that the accuracy of multimodal model on the CH-SIMS dataset was 93.92%. In addition, compared with other emotion recognition models, the effectiveness of the proposed method is verified.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182141797",
                    "name": "Yurui Xu"
                },
                {
                    "authorId": "2157134824",
                    "name": "Xiao Wu"
                },
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "33447300d11ee0a82284963949811c9bfce1f070",
            "title": "A Cybertwin Based Multimodal Network for ECG Patterns Monitoring Using Deep Learning",
            "abstract": "In next-generation network architecture, the Cybertwin drove the sixth generation of cellular networks sixth-generation (6G) to play an active role in many applications, such as healthcare and computer vision. Although the previous sixth-generation (5G) network provides the concept of edge cloud and core cloud, the internal communication mechanism has not been explained with a specific application. This article introduces a possible Cybertwin based multimodal network (beyond 5G) for electrocardiogram (ECG) patterns monitoring during daily activity. This network paradigm consists of a cloud-centric network and several Cybertwin communication ends. The Cybertwin nodes combine support locator/identifier identification, data caching, behavior logger, and communications assistant in the edge cloud. The application focuses on monitoring the ECG patterns during daily activity because few studies analyze them under different motions. We present a novel deep convolutional neural network based human activity recognition classifier to enhance identification accuracy. The healthcare monitoring values and potential clinical medicine are provided by the Cybertwin based network for ECG patterns observing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145356054",
                    "name": "Wen Qi"
                },
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                }
            ]
        },
        {
            "paperId": "7eb8965cdc7c1f7522babec4ea2636f0aa037603",
            "title": "Improved Kernel Correlation Filter Based Moving Target Tracking for Robot Grasping",
            "abstract": "Tracking and grasping moving objects are hot topics in the field of robots, which provides great potential in the industrial scene and human\u2013computer cooperation. Based on the kernel correlation filter and vision 3-D reconstruction, this article proposes a visual-based tracking and grasping method for moving targets. An improved algorithm based on the kernel correlation filter is proposed for object tracking. A scale pool is constructed, and a scale filter is trained to solve the problem of algorithm scale adaptation. At the same time, the judgment mechanism of tracking results, secondary detection, and modification update mechanisms are added to improve the robustness of the algorithm. Combined with an RGB-D camera, the target is reconstructed to obtain the 3-D pose of the target. The tracking and intercepting strategy is adopted to grasp the moving target. The proposed method is proven to have good performance through dataset comparison tests and experiments on real robot systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500421606",
                    "name": "Fang Peng"
                },
                {
                    "authorId": "2180828644",
                    "name": "Qinyi Xu"
                },
                {
                    "authorId": null,
                    "name": "Yifei Li"
                },
                {
                    "authorId": "1383098168",
                    "name": "Maoxi Zheng"
                },
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                }
            ]
        },
        {
            "paperId": "a628912989cd3db396f6c0b7f3078767fa7186ea",
            "title": "Theory, Applications, and Challenges of Cyber-Physical Systems 2021",
            "abstract": "<jats:p />",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                },
                {
                    "authorId": "2172829639",
                    "name": "Bo Xiao"
                },
                {
                    "authorId": "3383265",
                    "name": "Mingchuan Zhou"
                },
                {
                    "authorId": "145356054",
                    "name": "Wen Qi"
                },
                {
                    "authorId": "120851147",
                    "name": "Juan Sandoval"
                },
                {
                    "authorId": "49899378",
                    "name": "S. T. Kim"
                }
            ]
        },
        {
            "paperId": "156e6bfc8dd475119eee12c93eb3ed1c7e8a198e",
            "title": "An Approach for Robotic Leaning Inspired by Biomimetic Adaptive Control",
            "abstract": "How to enable robotic compliant manipulation has become a critical problem in the robotics field. Inspired by a biomimetic adaptive control strategy, this article presents a novel representation model named human-like compliant movement primitives (Hl-CMPs) which could allow a robot to learn human-like compliant behaviors. The state-of-the-art approaches can hardly learn complete compliant profiles for a specific task. Comparatively, our model can encode task-specific parametric movement trajectories, correspondingly associated with dynamic trajectories including both impedance and feedforward force profiles. The compliant profiles are learned based on a biomimetic control strategy derived from the human motor learning in the muscle space, enabling the robot to simultaneously learn the impedance and the force while executing the movement trajectories obtained from human demonstration. Furthermore, both the kinematic and the dynamic profiles are learned in the parametric space, thus enabling the representation of a skill using corresponding parameters (i.e, task-specific parameters). Hl-CMps can allow the robot to automatically learn compliant behaviors in an online manner after kinematic demonstration. Our approach is validated by an insertion task and a cutting task based on a KUKA LBR iiwa robot.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055609343",
                    "name": "Chao Zeng"
                },
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                },
                {
                    "authorId": "50025065",
                    "name": "Yanan Li"
                },
                {
                    "authorId": "2157958778",
                    "name": "Jing Guo"
                },
                {
                    "authorId": "2149303136",
                    "name": "Chenguang Yang"
                }
            ]
        },
        {
            "paperId": "8e9d53cffa92f381d451c50803e5d56c5683cd05",
            "title": "Object Detection and Localization Using Stereo Cameras",
            "abstract": "Camera systems have become increasingly popular because cameras are cheap and easy to deploy. Compared with other depth cameras, the stereo camera is small, and it is easily carried by subjects. Through a fixed baseline, the stereo camera is able to compute depth information. However, the traditional stereo matching algorithm can not compute the depth information on the edge of the image. Meanwhile, due to the large amount in the 3D point cloud, there is no specific numerical relationship between semantic information and depth information. In order to solve this problem, estimating depth and semantic information in an accurate way is required. A deep neural network model is used to predict semantic information and depth at the same time. Further, we propose a robust method to deal with variation brightness and improve performance under actual conditions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120155449",
                    "name": "Haoran Wu"
                },
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                },
                {
                    "authorId": "47909601",
                    "name": "Yueyue Liu"
                },
                {
                    "authorId": "2108917664",
                    "name": "Hongbo Gao"
                }
            ]
        },
        {
            "paperId": "a1ca90047b36acab4a7d63e69716d86795ccbe69",
            "title": "Depth Vision Guided Human Activity Recognition in Surgical Procedure using Wearable Multisensor",
            "abstract": "The increasing complexity of modern surgery rooms brings many challenges. Human activity recognition (HAR) plays a significant part in healthcare, telemedicine, long-term treatment, and even surgery by using wearable inertial sensors or depth cameras. Although the development of artificial intelligence techniques provide various machine learning (ML) methods to identify activities, it is a time-consuming implementation and high work burden to collect and label the large data set. To fascinate efficient data collection and labeling, we propose a novel depth vision-guided HAR architecture to obtain the labels of the collected raw data from the inertial measurement unit (IMU) sensors using depth data automatically. Experimental results show that the novel depth vision-guided interface can be utilized for identifying activities without labeling the data in advance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145356054",
                    "name": "Wen Qi"
                },
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                },
                {
                    "authorId": "2107179960",
                    "name": "Fei Chen"
                },
                {
                    "authorId": "2006174813",
                    "name": "Xuanyi Zhou"
                },
                {
                    "authorId": "1515831509",
                    "name": "Yan Shi"
                },
                {
                    "authorId": "2722437",
                    "name": "G. Ferrigno"
                },
                {
                    "authorId": "46452039",
                    "name": "E. De Momi"
                }
            ]
        },
        {
            "paperId": "c4f52030ee134ac84bbfc5f47e5177f6bc9ca744",
            "title": "Radio Tomographic Imaging with Feedback-Based Sparse Bayesian Learning",
            "abstract": "Radio tomographic imaging (RTI) provides an efficient method to realize device-free localization (DFL) which does not require the target to carry any tags or electronic devices. By the measurement of received signal strength (RSS) between node pairs in a wireless sensor network, the attenuation image caused by the target can be reconstructed. Subsequently, the target location can be extracted from the attenuation image. Sparse Bayesian learning (SBL) can be employed for reconstruction because of the sparseness of the attenuation image. However, the fast SBL degrades in reconstruction performances due to the inaccurate estimation on the noise hyper-parameters. To address this, this paper exploits a feedback-based fast SBL framework both for homogeneous-noise and heterogeneous-noise cases. Theoretical modeling and Bayesian inference procedure are given for this feedback-based framework. Finally, RTI experimental results from three different scenarios demonstrate the effectiveness of the proposed scheme.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118452939",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                },
                {
                    "authorId": "145607942",
                    "name": "Xuemei Guo"
                },
                {
                    "authorId": "1736332",
                    "name": "Guoli Wang"
                }
            ]
        }
    ]
}