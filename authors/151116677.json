{
    "authorId": "151116677",
    "papers": [
        {
            "paperId": "8f07183bf588f551b701fad6bfecf231f3ab78bc",
            "title": "AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs",
            "abstract": "Arabic, with its rich diversity of dialects, remains significantly underrepresented in Large Language Models, particularly in dialectal variations. We address this gap by introducing seven synthetic datasets in dialects alongside Modern Standard Arabic (MSA), created using Machine Translation (MT) combined with human post-editing. We present AraDiCE, a benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects. Additionally, we introduce the first-ever fine-grained benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and Levant regions, providing a novel dimension to LLM evaluation. Our findings demonstrate that while Arabic-specific models like Jais and AceGPT outperform multilingual models on dialectal tasks, significant challenges persist in dialect identification, generation, and translation. This work contributes ~45K post-edited samples, a cultural benchmark, and highlights the importance of tailored training to improve LLM performance in capturing the nuances of diverse Arabic dialects and cultural contexts. We will release the dialectal translation models and benchmarks curated in this study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171367840",
                    "name": "Basel Mousi"
                },
                {
                    "authorId": "145938140",
                    "name": "Nadir Durrani"
                },
                {
                    "authorId": "2311384605",
                    "name": "Fatema Ahmad"
                },
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2305614243",
                    "name": "Tameem Kabbani"
                },
                {
                    "authorId": "6415321",
                    "name": "Fahim Dalvi"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "911a107db0823bfe7059e2afd9420044ed23b6bb",
            "title": "ArAIEval Shared Task: Propagandistic Techniques Detection in Unimodal and Multimodal Arabic Content",
            "abstract": "We present an overview of the second edition of the ArAIEval shared task, organized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In this edition, ArAIEval offers two tasks: (i) detection of propagandistic textual spans with persuasion techniques identification in tweets and news articles, and (ii) distinguishing between propagandistic and non-propagandistic memes. A total of 14 teams participated in the final evaluation phase, with 6 and 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams submitted system description papers. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further provide a brief overview of the participating systems. All datasets and evaluation scripts are released to the research community. We hope this will enable further research on these important tasks in Arabic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "2266856028",
                    "name": "Fatema Ahmed"
                },
                {
                    "authorId": "3378399",
                    "name": "Reem Suwaileh"
                },
                {
                    "authorId": "32267310",
                    "name": "Md. Rafiul Biswas"
                },
                {
                    "authorId": "2034351",
                    "name": "W. Zaghouani"
                },
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "916ed0f604561033d4df141388b691b4d4e65c60",
            "title": "ArMeme: Propagandistic Content in Arabic Memes",
            "abstract": "With the rise of digital communication, memes have become a significant medium for cultural and political expression that is often used to mislead audiences. Identification of such misleading and persuasive multimodal content has become more important among various stakeholders, including social media platforms, policymakers, and the broader society as they often cause harm to individuals, organizations, and/or society. While there has been effort to develop AI-based automatic systems for resource-rich languages (e.g., English), it is relatively little to none for medium to low resource languages. In this study, we focused on developing an Arabic memes dataset with manual annotations of propagandistic content. We annotated ~6K Arabic memes collected from various social media platforms, which is a first resource for Arabic multimodal research. We provide a comprehensive analysis aiming to develop computational tools for their detection. We will make them publicly available for the community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "83264368",
                    "name": "A. Hasnat"
                },
                {
                    "authorId": "2266856028",
                    "name": "Fatema Ahmed"
                },
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                }
            ]
        },
        {
            "paperId": "a33cbc6d75b0dc2a36a359b4e6e6ff085690191f",
            "title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs",
            "abstract": "Natural Question Answering (QA) datasets play a crucial role in evaluating the capabilities of large language models (LLMs), ensuring their effectiveness in real-world applications. Despite the numerous QA datasets that have been developed, there is a notable lack of region-specific datasets generated by native users in their own languages. This gap hinders the effective benchmarking of LLMs for regional and cultural specificities. Furthermore, it also limits the development of fine-tuned models. In this study, we propose a scalable, language-independent framework, NativQA, to seamlessly construct culturally and regionally aligned QA datasets in native languages, for LLM evaluation and tuning. We demonstrate the efficacy of the proposed framework by designing a multilingual natural QA dataset, \\mnqa, consisting of ~64k manually annotated QA pairs in seven languages, ranging from high to extremely low resource, based on queries from native speakers from 9 regions covering 18 topics. We benchmark open- and closed-source LLMs with the MultiNativQA dataset. We also showcase the framework efficacy in constructing fine-tuning data especially for low-resource and dialectally-rich languages. We made both the framework NativQA and MultiNativQA dataset publicly available for the community (https://nativqa.gitlab.io).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2311384605",
                    "name": "Fatema Ahmad"
                },
                {
                    "authorId": "12352363",
                    "name": "Sahinur Rahman Laskar"
                },
                {
                    "authorId": "2311437171",
                    "name": "Sunaya Upadhyay"
                },
                {
                    "authorId": "2155270590",
                    "name": "Vrunda N. Sukhadia"
                },
                {
                    "authorId": "38330039",
                    "name": "Mucahid Kutlu"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "a6b2da2b377016ca509e056823dc7aaf187d6285",
            "title": "BLP-2023 Task 2: Sentiment Analysis",
            "abstract": "We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, only 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a succinct overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the research community, to foster further research in this domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "2257025421",
                    "name": "Anika Anjum"
                },
                {
                    "authorId": "2198439213",
                    "name": "Shudipta Das"
                },
                {
                    "authorId": "2232781563",
                    "name": "Afiyat Anjum"
                }
            ]
        },
        {
            "paperId": "bc70af9248d210663edf22e5fc84ca9313c697b0",
            "title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis",
            "abstract": "The rapid expansion of the digital world has propelled sentiment analysis into a critical tool across diverse sectors such as marketing, politics, customer service, and healthcare. While there have been significant advancements in sentiment analysis for widely spoken languages, low-resource languages, such as Bangla, remain largely under-researched due to resource constraints. Furthermore, the recent unprecedented performance of Large Language Models (LLMs) in various applications highlights the need to evaluate them in the context of low-resource languages. In this study, we present a sizeable manually annotated dataset encompassing 33,606 Bangla news tweets and Facebook comments. We also investigate zero- and few-shot in-context learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering a comparative analysis against fine-tuned models. Our findings suggest that monolingual transformer-based models consistently outperform other models, even in zero and few-shot scenarios. To foster continued exploration, we intend to make this dataset and our research tools publicly available to the broader research community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "2198439213",
                    "name": "Shudipta Das"
                },
                {
                    "authorId": "2232781563",
                    "name": "Afiyat Anjum"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "35324286",
                    "name": "Anika Anjum"
                },
                {
                    "authorId": "2008192067",
                    "name": "Avijit Sarker"
                },
                {
                    "authorId": "7638904",
                    "name": "S. R. H. Noori"
                }
            ]
        },
        {
            "paperId": "8b9f987ef9d31c9cc5d8c6a3816de5b998743d6b",
            "title": "Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet Text",
            "abstract": "The wide use of social media and digital technologies facilitates sharing various news and information about events and activities. Despite sharing positive information misleading and false information is also spreading on social media. There have been efforts in identifying such misleading information both manually by human experts and automatic tools. Manual effort does not scale well due to the high volume of information, containing factual claims, are appearing online. Therefore, automatically identifying check-worthy claims can be very useful for human experts. In this study, we describe our participation in Subtask-1A: Check-worthiness of tweets (English, Dutch and Spanish) of CheckThat! lab at CLEF 2022. We performed standard preprocessing steps and applied different models to identify whether a given text is worthy of fact checking or not. We use the oversampling technique to balance the dataset and applied SVM and Random Forest (RF) with TF-IDF representations. We also used BERT multilingual (BERT-m) and XLM-RoBERTa-base pre-trained models for the experiments. We used BERT-m for the official submissions and our systems ranked as 3rd, 5th, and 12th in Spanish, Dutch, and English, respectively. In further experiments, our evaluation shows that transformer models (BERT-m and XLM-RoBERTa-base) outperform the SVM and RF in Dutch and English languages where a different scenario is observed for Spanish.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2176643875",
                    "name": "Prerona Tarannum"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "7638904",
                    "name": "S. R. H. Noori"
                }
            ]
        },
        {
            "paperId": "9cb96b48ded356968c5273809058e6d9cbb4f4fc",
            "title": "SemEval-2022 Task 3: PreTENS-Evaluating Neural Networks on Presuppositional Semantic Knowledge",
            "abstract": "We report the results of the SemEval 2022 Task 3, PreTENS, on evaluation the acceptability of simple sentences containing constructions whose two arguments are presupposed to be or not to be in an ordered taxonomic relation. The task featured two sub-tasks articulated as: (i) binary prediction task and (ii) regression task, predicting the acceptability in a continuous scale. The sentences were artificially generated in three languages (English, Italian and French). 21 systems, with 8 system papers were submitted for the task, all based on various types of fine-tuned transformer systems, often with ensemble methods and various data augmentation techniques. The best systems reached an F1-macro score of 94.49 (sub-task1) and a Spearman correlation coefficient of 0.80 (sub-task2), with interesting variations in specific constructions and/or languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2713535",
                    "name": "Roberto Zamparelli"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "3460752",
                    "name": "D. Brunato"
                },
                {
                    "authorId": "3007700",
                    "name": "C. Chesi"
                },
                {
                    "authorId": "2187131",
                    "name": "F. Dell\u2019Orletta"
                },
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "35068913",
                    "name": "Giulia Venturi"
                }
            ]
        },
        {
            "paperId": "7646cdbb429ff5e733d84ed1b9059e4a8e116d91",
            "title": "Brain Tumor Analysis Using Deep Neural Network",
            "abstract": "The identification of tumors is one of the most tenacious and emerging fields in medical image processing. A tumor means the unrestricted existence of a bunch of cells in a precise area of the human body which destroys the normal body cells and keeps increasing. In human body, brain tumor is measured as the most common tumor which affects the nervous system, memory functional cells, glands, and membranes that surround the brain and can conduct to a high mortality rate if the affected one is unsuccessful to reach proper medical treatment. For effective treatment, precise and early recognition of the tumors is critical work and also a vital step in diagnosis and treatment preparation for affected one which not only benefits to arise with improved medications but also saves the affected life in due time. This research work uses Magnetic Resonance Imaging (MRI), which is a prominent imaging procedure in terms of brain tumor recognition. For features extraction, segmentation and classification, the proposed research work includes the deep neural network integrated method and Convolutional Neural Network (CNN) to classify the MRI images and an accuracy of about 97.92% has been achieved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2105728112",
                    "name": "Iftekhar Khan"
                },
                {
                    "authorId": "2105768055",
                    "name": "Kuheli Ahsan"
                },
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "1474569456",
                    "name": "A. Sattar"
                }
            ]
        },
        {
            "paperId": "8b4bec25ad7976a2f5f36906bec7a128d535278e",
            "title": "BlackOps at CheckThat!\u00a02021: User Profiles Analyze of Intelligent Detection on Fake Tweets Notebook for PAN",
            "abstract": "An expensive task is fake news detection for recent trends among the concept of misinformation or rumors. In everywhere most of the times information lead or play emergent preface but forthwith misinformation also in everywhere to mislead the peoples mind and activity. Therefore, detecting fake content in any system can be a weapon over fictitious news. In any language cross over the exponential growth of fake news in social sites. Hence, it is the real time process to produce online fake news so that it has been needed to implement an automated technique whenever detect true from false. According to the solution of this approach made a research On English language textual inputs as twitter news from user profiles. At this point, due to accurate analysis for social media we experimented with supervised learning such as Decision tree, Random forest and gradient boosting. In between all the ML classifiers outperformed with 88% detection accuracy that mention the research of detection is more accurate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1786599",
                    "name": "S. Sohan"
                },
                {
                    "authorId": "1999672587",
                    "name": "S. Khushbu"
                },
                {
                    "authorId": "98155861",
                    "name": "Md. Sanzidul Islam"
                },
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                }
            ]
        }
    ]
}