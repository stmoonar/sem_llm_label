{
    "authorId": "51263680",
    "papers": [
        {
            "paperId": "3c2e5eb128af974ce8a21c189c6f06ebb671a2f8",
            "title": "Controlling High-Dimensional Data With Sparse Input",
            "abstract": "We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an ef\ufb01cient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human-interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We pro-pose a model, called Multiple-Instance CVAE (MICVAE), that is speci\ufb01cally designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: ef\ufb01ciency, robustness, and faithfulness. With even a very small number of input values ( ~ 4), MICVAE enables users to improve the quality of the output signi\ufb01-cantly, in terms of listener preference (4:1).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66962453",
                    "name": "Dan-Andrei Iliescu"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                }
            ]
        },
        {
            "paperId": "70dfac2c00f26ea3c372aab8213d8eb8b9ca2062",
            "title": "Ensemble Prosody Prediction For Expressive Speech Synthesis",
            "abstract": "Generating expressive speech with rich and varied prosody continues to be a challenge for Text-to-Speech. Most efforts have focused on sophisticated neural architectures intended to better model the data distribution. Yet, in evaluations it is generally found that no single model is preferred for all input texts. This suggests an approach that has rarely been used before for Text-to-Speech: an ensemble of models.We apply ensemble learning to prosody prediction. We construct simple ensembles of prosody predictors by varying either model architecture or model parameter values.To automatically select amongst the models in the ensemble when performing Text-to-Speech, we propose a novel, and computationally trivial, variance-based criterion. We demonstrate that even a small ensemble of prosody predictors yields useful diversity, which, combined with the proposed selection criterion, outperforms any individual model from the ensemble.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "50161308",
                    "name": "Vivian Hu"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                },
                {
                    "authorId": "2213297845",
                    "name": "Christopher G. R. Wallis"
                },
                {
                    "authorId": "90190727",
                    "name": "Tom\u00e1s G\u00f3mez Ibarrondo"
                },
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "2213305354",
                    "name": "James Leoni"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "7b16c64fb656328e4fac6383ef3c290ff87e26ab",
            "title": "Controllable Prosody Generation with Partial Inputs",
            "abstract": "We address the problem of human-in-the-loop control for generating prosody in the context of text-to-speech synthesis. Controlling prosody is challenging because existing generative models lack an efficient interface through which users can modify the output quickly and precisely. To solve this, we introduce a novel framework whereby the user provides partial inputs and the generative model generates the missing features. We propose a model that is specifically designed to encode partial prosodic features and output complete audio. We show empirically that our model displays two essential qualities of a human-in-the-loop control mechanism: efficiency and robustness. With even a very small number of input values (~4), our model enables users to improve the quality of the output significantly in terms of listener preference (4:1).",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "66962453",
                    "name": "Dan-Andrei Iliescu"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                }
            ]
        },
        {
            "paperId": "446d4d883f790e0433e14ca40fd7b1cf672ace5c",
            "title": "Prosodic Representation Learning and Contextual Sampling for Neural Text-to-Speech",
            "abstract": "In this paper, we introduce Kathaka, a model trained with a novel two-stage training process for neural speech synthesis with contextually appropriate prosody. In Stage I, we learn a prosodic distribution at the sentence level from mel-spectrograms available during training. In Stage II, we propose a novel method to sample from this learnt prosodic distribution using the contextual information available in text. To do this, we use BERT on text, and graph-attention networks on parse trees extracted from text. We show a statistically significant relative improvement of 13.2% in naturalness over a strong baseline when compared to recordings. We also conduct an ablation study on variations of our sampling technique, and show a statistically significant improvement over the baseline in each case.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1664895952",
                    "name": "S. Karlapati"
                },
                {
                    "authorId": "2055662553",
                    "name": "Ammar Abbas"
                },
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                },
                {
                    "authorId": "2118227",
                    "name": "A. Moinet"
                },
                {
                    "authorId": "2058185",
                    "name": "Arnaud Joly"
                },
                {
                    "authorId": "2908898",
                    "name": "Panagiota Karanasou"
                },
                {
                    "authorId": "2242058",
                    "name": "Thomas Drugman"
                }
            ]
        },
        {
            "paperId": "6d3be7f3f4116a2da2c6300e8aaa6a65e836a764",
            "title": "Perception of prosodic variation for speech synthesis using an unsupervised discrete representation of F0",
            "abstract": "In English, prosody adds a broad range of information to segment sequences, from information structure (e.g. contrast) to stylistic variation (e.g. expression of emotion). However, when learning to control prosody in text-to-speech voices, it is not clear what exactly the control is modifying. Existing research on discrete representation learning for prosody has demonstrated high naturalness, but no analysis has been performed on what these representations capture, or if they can generate meaningfully-distinct variants of an utterance. We present a phrase-level variational autoencoder with a multi-modal prior, using the mode centres as \"intonation codes\". Our evaluation establishes which intonation codes are perceptually distinct, finding that the intonation codes from our multi-modal latent model were significantly more distinct than a baseline using k-means clustering. We carry out a follow-up qualitative study to determine what information the codes are carrying. Most commonly, listeners commented on the intonation codes having a statement or question style. However, many other affect-related styles were also reported, including: emotional, uncertain, surprised, sarcastic, passive aggressive, and upset.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                },
                {
                    "authorId": "144490056",
                    "name": "Catherine Lai"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "71e13e9ac00ea102a8b1bacdc25d298072945f99",
            "title": "Camp: A Two-Stage Approach to Modelling Prosody in Context",
            "abstract": "Prosody is an integral part of communication, but remains an open problem in state-of-the-art speech synthesis. There are two major issues faced when modelling prosody: (1) prosody varies at a slower rate compared with other content in the acoustic signal (e.g. segmental information and background noise); (2) determining appropriate prosody without sufficient context is an ill-posed problem. In this paper, we propose solutions to both these issues. To mitigate the challenge of modelling a slow-varying signal, we learn to disentangle prosodic information using a word level representation. To alleviate the ill-posed nature of prosody modelling, we use syntactic and semantic information derived from text to learn a context-dependent prior over our prosodic space. Our context-aware model of prosody (CAMP) outperforms the state-of-the-art technique, closing the gap with natural speech by 26%. We also find that replacing attention with a jointly-trained duration model improves prosody significantly.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                },
                {
                    "authorId": "2118227",
                    "name": "A. Moinet"
                },
                {
                    "authorId": "1664895952",
                    "name": "S. Karlapati"
                },
                {
                    "authorId": "1388601149",
                    "name": "Jaime Lorenzo-Trueba"
                },
                {
                    "authorId": "143733597",
                    "name": "Thomas Merritt"
                },
                {
                    "authorId": "2058185",
                    "name": "Arnaud Joly"
                },
                {
                    "authorId": "2055662553",
                    "name": "Ammar Abbas"
                },
                {
                    "authorId": "2908898",
                    "name": "Panagiota Karanasou"
                },
                {
                    "authorId": "2242058",
                    "name": "Thomas Drugman"
                }
            ]
        },
        {
            "paperId": "8d089d80eea2b6ef695b95d3a0ef8195f7652cd0",
            "title": "Using generative modelling to produce varied intonation for speech synthesis",
            "abstract": "Unlike human speakers, typical text-to-speech (TTS) systems are unable to produce multiple distinct renditions of a given sentence. This has previously been addressed by adding explicit external control. In contrast, generative models are able to capture a distribution over multiple renditions and thus produce varied renditions using sampling. Typical neural TTS models learn the average of the data because they minimise mean squared error. In the context of prosody, taking the average produces flatter, more boring speech: an \"average prosody\". A generative model that can synthesise multiple prosodies will, by design, not model average prosody. We use variational autoencoders (VAEs) which explicitly place the most \"average\" data close to the mean of the Gaussian prior. We propose that by moving towards the tails of the prior distribution, the model will transition towards generating more idiosyncratic, varied renditions. Focusing here on intonation, we investigate the trade-off between naturalness and intonation variation and find that typical acoustic models can either be natural, or varied, but not both. However, sampling from the tails of the VAE prior produces much more varied intonation than the traditional approaches, whilst maintaining the same level of naturalness.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                },
                {
                    "authorId": "1720858",
                    "name": "O. Watts"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "e1b1d2a54928681d7fc0e1e749e7dab540766101",
            "title": "Investigating the Robustness of Sequence-to-Sequence Text-to-Speech Models to Imperfectly-Transcribed Training Data",
            "abstract": "Sequence-to-sequence (S2S) text-to-speech (TTS) models can synthesise high quality speech when large amounts of annotated training data are available. Transcription errors exist in all data and are especially prevalent in found data such as audiobooks. In previous generations of TTS technology, alignment using Hidden Markov Models (HMMs) was widely used to identify and eliminate bad data. In S2S models, the use of attention replaces HMM-based alignment, and there is no explicit mechanism for removing bad data. It is not yet understood how such models deal with transcription errors in the training data. We evaluate the quality of speech from S2S-TTS models when trained on data with imperfect transcripts, simulated using corruption, or provided by an Automatic Speech Recogniser (ASR). We find that attention can skip over extraneous words in the input sequence, providing robustness to insertion errors. But substitutions and deletions pose a problem because there is no ground truth input available to align to the ground truth acoustics during teacher-forced training. We conclude that S2S-TTS systems are only partially robust to training on imperfectlytranscribed data and further work is needed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2058161100",
                    "name": "Jason Fong"
                },
                {
                    "authorId": "2067330734",
                    "name": "Pilar Oplustil Gallegos"
                },
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "8451392694cb685e6f7741fe01ee312ab94ae846",
            "title": "Learning Interpretable Control Dimensions for Speech Synthesis by Using External Data",
            "abstract": "There are many aspects of speech that we might want to control when creating text-to-speech (TTS) systems. We present a general method that enables control of arbitrary aspects of speech, which we demonstrate on the task of emotion control. Current TTS systems use supervised machine learning and are therefore heavily reliant on labelled data. If no labels are available for a desired control dimension, then creating interpretable control becomes challenging. We introduce a method that uses external, labelled data (i.e. not the original data used to train the acoustic model) to enable the control of dimensions that are not labelled in the original data. Adding interpretable control allows the voice to be manually controlled to produce more engaging speech, for applications such as audiobooks. We evaluate our method using a listening test.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                },
                {
                    "authorId": "1720858",
                    "name": "O. Watts"
                },
                {
                    "authorId": "3404365",
                    "name": "S. Ronanki"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        }
    ]
}