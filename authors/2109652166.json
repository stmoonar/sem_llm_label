{
    "authorId": "2109652166",
    "papers": [
        {
            "paperId": "0809c278fcdec2ce297da3a9d6e031fc192263f6",
            "title": "User-friendly Image Editing with Minimal Text Input: Leveraging Captioning and Injection Techniques",
            "abstract": "Recent text-driven image editing in diffusion models has shown remarkable success. However, the existing methods assume that the user's description sufficiently grounds the contexts in the source image, such as objects, background, style, and their relations. This assumption is unsuitable for real-world applications because users have to manually engineer text prompts to find optimal descriptions for different images. From the users' standpoint, prompt engineering is a labor-intensive process, and users prefer to provide a target word for editing instead of a full sentence. To address this problem, we first demonstrate the importance of a detailed text description of the source image, by dividing prompts into three categories based on the level of semantic details. Then, we propose simple yet effective methods by combining prompt generation frameworks, thereby making the prompt engineering process more user-friendly. Extensive qualitative and quantitative experiments demonstrate the importance of prompts in text-driven image editing and our method is comparable to ground-truth prompts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "8310837",
                    "name": "Wooseok Jang"
                },
                {
                    "authorId": "2118020280",
                    "name": "Hyunsung Kim"
                },
                {
                    "authorId": "2173728733",
                    "name": "Junho Kim"
                },
                {
                    "authorId": "30187096",
                    "name": "Yunjey Choi"
                },
                {
                    "authorId": "2596437",
                    "name": "Seung Wook Kim"
                },
                {
                    "authorId": "1625176918",
                    "name": "Gayeong Lee"
                }
            ]
        },
        {
            "paperId": "357c9cbdcd164d86d3a559ef9bab750c996da13b",
            "title": "Classification of Edge-dependent Labels of Nodes in Hypergraphs",
            "abstract": "A hypergraph is a data structure composed of nodes and hyperedges, where each hyperedge is an any-sized subset of nodes. Due to the flexibility in hyperedge size, hypergraphs represent group interactions (e.g., co-authorship by more than two authors) more naturally and accurately than ordinary graphs. Interestingly, many real-world systems modeled as hypergraphs contain edge-dependent node labels, i.e., node labels that vary depending on hyperedges. For example, on co-authorship datasets, the same author (i.e., a node) can be the primary author in a paper (i.e., a hyperedge) but the corresponding author in another paper (i.e., another hyperedge). In this work, we introduce a classification of edge-dependent node labels as a new problem. This problem can be used as a benchmark task for hypergraph neural networks, which recently have attracted great attention, and also the usefulness of edge-dependent node labels has been verified in various applications. To tackle this problem, we propose WHATsNet, a novel hypergraph neural network that represents the same node differently depending on the hyperedges it participates in by reflecting its varying importance in the hyperedges. To this end, WHATsNet models the relations between nodes within each hyperedge, using their relative centrality as positional encodings. In our experiments, we demonstrate that WHATsNet significantly and consistently outperforms ten competitors on six real-world hypergraphs, and we also show successful applications of WHATsNet to (a) ranking aggregation, (b) node clustering, and (c) product return prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047035591",
                    "name": "Minyoung Choe"
                },
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "31888223",
                    "name": "Jaemin Yoo"
                },
                {
                    "authorId": "40553270",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "a311be947f3df61c61cbf3acc3e3f15dee7ee8fa",
            "title": "Diffusion Model for Dense Matching",
            "abstract": "The objective for establishing dense correspondence between paired images consists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, recent approaches have focused on learning the data term with deep neural networks without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, and large displacements. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms. Unlike previous approaches, this is accomplished by leveraging a conditional denoising diffusion model. DiffMatch consists of two main components: conditional denoising diffusion module and cost injection module. We stabilize the training process and reduce memory usage with a stage-wise training strategy. Furthermore, to boost performance, we introduce an inference technique that finds a better path to the accurate matching field. Our experimental results demonstrate significant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component. Project page is available at https://ku-cvlab.github.io/DiffMatch/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146930133",
                    "name": "Jisu Nam"
                },
                {
                    "authorId": "12035749",
                    "name": "Gyuseong Lee"
                },
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "2130270152",
                    "name": "Ines Hyeonsu Kim"
                },
                {
                    "authorId": "2218543875",
                    "name": "Hyoungwon Cho"
                },
                {
                    "authorId": "2109583752",
                    "name": "Seyeong Kim"
                },
                {
                    "authorId": "2596437",
                    "name": "Seung Wook Kim"
                }
            ]
        },
        {
            "paperId": "bee3c3277cdc181fa33d3d847878847606bd982b",
            "title": "How Transitive Are Real-World Group Interactions? - Measurement and Reproduction",
            "abstract": "Many real-world interactions (e.g., researcher collaborations and email communication) occur among multiple entities. These group interactions are naturally modeled as hypergraphs. In graphs, transitivity is helpful to understand the connections between node pairs sharing a neighbor, and it has extensive applications in various domains. Hypergraphs, an extension of graphs, are designed to represent group relations. However, to the best of our knowledge, there has been no examination regarding the transitivity of real-world group interactions. In this work, we investigate the transitivity of group interactions in real-world hypergraphs. We first suggest intuitive axioms as necessary characteristics of hypergraph transitivity measures. Then, we propose a principled hypergraph transitivity measure HyperTrans, which satisfies all the proposed axioms, with a fast computation algorithm Fast-HyperTrans. After that, we analyze the transitivity patterns in real-world hypergraphs distinguished from those in random hypergraphs. Lastly, we propose a scalable hypergraph generator THera. It reproduces the observed transitivity patterns by leveraging community structures, which are pervasive in real-world hypergraphs. Our code and datasets are available at https://github.com/kswoo97/hypertrans.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "2065674134",
                    "name": "Fanchen Bu"
                },
                {
                    "authorId": "2047035591",
                    "name": "Minyoung Choe"
                },
                {
                    "authorId": "31888223",
                    "name": "Jaemin Yoo"
                },
                {
                    "authorId": "40553270",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "e5f3d05a415ff6ae669f7a9ad403352053931c64",
            "title": "Semantic-Preserving Augmentation for Robust Image-Text Retrieval",
            "abstract": "Image-text retrieval is a task to search for the proper textual descriptions of the visual world and vice versa. One challenge of this task is the vulnerability to input image/text corruptions. Such corruptions are often unobserved during the training, and degrade the retrieval model\u2019s decision quality substantially. In this paper, we propose a novel image-text retrieval technique, referred to as robust visual semantic embedding (RVSE), which consists of novel image-based and text-based augmentation techniques called semantic-preserving augmentation for image (SPAug-I) and text (SPAug-T). Since SPAug-I and SPAug-T change the original data in a way that its semantic information is preserved, we enforce the feature extractors to generate semantic-aware embedding vectors regardless of the corruption, improving the model\u2019s robustness significantly. From extensive experiments using benchmark datasets, we show that RVSE outperforms conventional retrieval schemes in terms of image-text retrieval performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "29932533",
                    "name": "Kyuhong Shim"
                },
                {
                    "authorId": "151094839",
                    "name": "L. Nguyen"
                },
                {
                    "authorId": "145500030",
                    "name": "B. Shim"
                }
            ]
        },
        {
            "paperId": "061536432b2273d2968a576218c689ab225a4b1e",
            "title": "Controllable Style Transfer via Test-time Training of Implicit Neural Representation",
            "abstract": "We propose a controllable style transfer framework based on Implicit Neural Representation that pixel-wisely controls the stylized output via test-time training. Unlike traditional image optimization methods that often suffer from unstable convergence and learning-based methods that require intensive training and have limited generalization ability, we present a model optimization framework that optimizes the neural networks during test-time with explicit loss functions for style transfer. After being test-time trained once, thanks to the flexibility of the INR-based model, our framework can precisely control the stylized images in a pixel-wise manner and freely adjust image resolution without further optimization or training. We demonstrate several applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "2161434381",
                    "name": "Youngjo Min"
                },
                {
                    "authorId": "2152904312",
                    "name": "Younghun Jung"
                },
                {
                    "authorId": "2596437",
                    "name": "Seung Wook Kim"
                }
            ]
        },
        {
            "paperId": "13a7a37ea7c1a896b452add3f40b626a8ad0c68a",
            "title": "REVECA - Rich Encoder-decoder framework for Video Event CAptioner",
            "abstract": "We describe an approach used in the Generic Boundary Event Captioning challenge at the Long-Form Video Understanding Workshop held at CVPR 2022. We designed a Rich Encoder\u2013decoder framework for Video Event CAptioner (REVECA) that utilizes spatial and temporal information from the video to generate a caption for the corresponding the event boundary. REVECA uses frame position embedding to incorporate information before and after the event boundary. Furthermore, it employs features extracted using the temporal segment network and temporal-based pairwise difference method to learn temporal information. A semantic segmentation mask for the attentional pooling process is adopted to learn the subject of an event. Finally, LoRA is applied to \ufb01ne-tune the image encoder to enhance the learning ef\ufb01ciency. REVECA yielded an average score of 50.97 on the Kinetics-GEBC test data, which is an improvement of 10.17 over the baseline method. Our code is available in https://github.com/TooTouch/REVECA .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150286722",
                    "name": "Jaehyuk Heo"
                },
                {
                    "authorId": "2171649253",
                    "name": "YongGi Jeong"
                },
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "2164724041",
                    "name": "Jaehee Kim"
                },
                {
                    "authorId": "2994930",
                    "name": "Pilsung Kang"
                }
            ]
        },
        {
            "paperId": "4cf27c45f8eb3d79e00d9aea20a925e9a50f6b76",
            "title": "R2Z2: Detecting Rendering Regressions in Web Browsers through Differential Fuzz Testing",
            "abstract": "A rendering regression is a bug introduced by a web browser where a web page no longer functions as users expect. Such rendering bugs critically harm the usability of web browsers as well as web applications. The unique aspect of rendering bugs is that they affect the presented visual appearance of web pages, but those web pages have no pre-defined correct appearance. Therefore, it is challenging to automatically detect errors in their appearance. In practice, web browser vendors rely on non-trivial and time-prohibitive manual analysis to detect and handle rendering regressions. This paper proposes R2Z2, an automated tool to find rendering regressions. R2Z2 uses the differential fuzz testing approach, which repeatedly compares the rendering results of two different versions of a browser while providing the same HTML as input. If the rendering results are different, R2Z2 further performs cross browser compatibility testing to check if the rendering difference is indeed a rendering regression. After identifying a rendering regression, R2Z2 will perform an in-depth analysis to aid in fixing the regression. Specifically, R2Z2 performs a delta-debugging-like analysis to pinpoint the exact browser source code commit causing the regression, as well as inspecting the rendering pipeline stages to pinpoint which pipeline stage is responsible. We implemented a prototype of R2Z2 particularly targeting the Chrome browser. So far, R2Z2 found 11 previously undiscovered rendering regressions in Chrome, all of which were confirmed by the Chrome developers. Importantly, in each case, R2Z2 correctly reported the culprit commit. Moreover, R2Z2 correctly pin-pointed the culprit rendering pipeline stage in all but one case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110554129",
                    "name": "Suhwan Song"
                },
                {
                    "authorId": "2142813257",
                    "name": "Jaewon Hur"
                },
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "2172020341",
                    "name": "Philip Rogers"
                },
                {
                    "authorId": "2767582",
                    "name": "Byoungyoung Lee"
                }
            ]
        },
        {
            "paperId": "839aca3c850eec2431078abce9cee194daf5db24",
            "title": "Boosted Locality Sensitive Hashing: Discriminative, Efficient, and Scalable Binary Codes for Source Separation",
            "abstract": "We propose a novel adaptive boosting approach to learn discriminative binary hash codes, boosted locality sensitive hashing (BLSH), that can represent audio spectra efficiently. We aim to use the learned hash codes in the single-channel speech denoising task by designing a nearest neighborhood search method that operates in the hashed feature space. To achieve the optimal denoising results given the highly compact binary feature representation, our proposed BLSH algorithm learns simple logistic regressors as the weak learners in an incremental way (i.e., one by one) so that each weak learner is trained to complement the mistake its predecessors have made. Upon testing, their binary classification results transform each spectrum of noisy speech into a bit string, where the bits are ordered based on their significance, adding scalability to the denoising system. Simple bitwise operations calculate Hamming distance to find the $\\boldsymbol{K}$-nearest matching hashed frames in the dictionary of training noisy speech spectra, whose associated ideal binary masks are averaged to estimate the denoising mask for that test mixture. In contrast to the locality sensitive hashing method's random projections, our proposed supervised learning algorithm trains the projections such that the distance between the self-similarity matrix of the hash codes and that of the original spectra is minimized. Likewise, the process conceptually aligns to the Adaboost algorithm, although ours is specialized in learning binary features for source separation rather than classification. Experimental results on speech denoising suggest that the BLSH algorithm learns more discriminative representations than Fourier or mel spectra and the nonlinear kernels derived from them. Our compact binary representation is expected to facilitate model deployment onto resource-constrained environments, where comprehensive models (e.g., deep neural networks) are unaffordable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                }
            ]
        },
        {
            "paperId": "b1754d37749e43ba4e7ed786c528de59122d5d63",
            "title": "LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data",
            "abstract": "Existing techniques for image-to-image translation commonly have suffered from two critical problems: heavy reliance on per-sample domain annotation and/or inability to handle multiple attributes per image. Recent truly-unsupervised methods adopt clustering approaches to easily provide per-sample one-hot domain labels. However, they cannot account for the real-world setting: one sample may have multiple attributes. In addition, the semantics of the clusters are not easily coupled to human understanding. To overcome these, we present LANguage-driven Image-to-image Translation model, dubbed LANIT. We leverage easy-to-obtain candidate attributes given in texts for a dataset: the similarity between images and attributes indicates per-sample domain labels. This formulation naturally enables multi-hot labels so that users can specify the target domain with a set of attributes in language. To account for the case that the initial prompts are inaccurate, we also present prompt learning. We further present domain regularization loss that enforces translated images to be mapped to the corresponding domain. Experiments on several standard benchmarks demonstrate that LANIT achieves comparable or superior performance to existing models. The code is available at github.com/KU-CVLAB/LANIT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2160862678",
                    "name": "Jihye Park"
                },
                {
                    "authorId": "2155236022",
                    "name": "Soohyun Kim"
                },
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "8351571",
                    "name": "Jaejun Yoo"
                },
                {
                    "authorId": "2847986",
                    "name": "Youngjung Uh"
                },
                {
                    "authorId": "2596437",
                    "name": "Seung Wook Kim"
                }
            ]
        }
    ]
}