{
    "authorId": "2018051",
    "papers": [
        {
            "paperId": "150dc1e4307dc5158ea1decd40e8b52c72caae72",
            "title": "LiST-Net: Enhanced Flood Mapping With Lightweight SAR Transformer Network and Dimension-Wise Attention",
            "abstract": "Detecting flood-induced changes using synthetic aperture radar (SAR) is crucial for crisis management and damage assessment. Nevertheless, current methodologies predominantly focus on changes in buildings within optical images, struggling with the complex structures of floods. These structures are marked by widespread speckle noise and are accompanied by an increase in computational cost. These challenges hinder their success in real-world applications, necessitating a novel approach. This article proposes LiST-Net, a lightweight SAR transformer network with dimension-wise attention to improve flood detection accuracy. LiST-Net offers three key advantages. First, the graph neighbor module (GNM) is designed to enhance both detailed information of neighboring pixels and multidate features within the encoder. Second, the dimension-wise interactive attention (DIA) module is proposed to effectively reduce computational complexity while enhancing feature representation. Third, an attentive supervised learning module (ASLM) is incorporated to mitigate noise through a pixel mask gate, allowing change water information to pass through and improving the accuracy of water edge delineation. The effectiveness of LiST-Net is evaluated on two flood detection datasets, S1GFloods and ETCI-2021. Experimental results demonstrate that LiST-Net outperforms existing methods, showcasing a 94.7% improvement in $F1$ and an 88.7% enhancement in intersection over union (IoU) on the S1GFloods datasets, with lower computational costs (11.78G) and fewer parameters (7.34M). This underscores LiST-Net as a promising strategy for precise and effective mapping of floods within SAR images in real-world applications. A public release of the demo code will be available at https://github.com/Tamer-Saleh.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067344277",
                    "name": "T. Saleh"
                },
                {
                    "authorId": "2219396609",
                    "name": "S. Holail"
                },
                {
                    "authorId": "2271461409",
                    "name": "M. Zahran"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "2259003019",
                    "name": "Gui-Song Xia"
                }
            ]
        },
        {
            "paperId": "18ca1e790f986d191d15b12eba12f1b2067af776",
            "title": "LUMNet: Land Use Knowledge Guided Multiscale Network for Height Estimation From Single Remote Sensing Images",
            "abstract": "For estimating ground object height from single remote sensing images, this study proposes a land use knowledge guided multiscale height estimation network (LUMNet), which takes single image and land use data as inputs and produces an estimated height map as an output. First, in the encoder part, the visual geometry group network (VGGNet) is used to extract multilevel deep semantic features from images. Second, the features of the encoder part are fused with those of the decoder part through skip connection with land use knowledge attention weight and dense atrous spatial pyramid pooling (DenseASPP). Third, in the decoder part, the features are decoded using upsampling and convolution, and a joint loss function is constructed to supervise network training. Experiments show that the proposed method achieves the best visualizations and quantitative evaluation results among all the tested methods. For the Vaihingen dataset, the root mean square error (RMSE), mean absolute error (MAE), and zero-mean normalized cross correlation (ZNCC) of LUMNet are 1.523, 0.969, and 0.908, respectively. For the Potsdam dataset, these are 2.158, 1.222, and 0.871, respectively. The source code of LUMNet has been made public at the following link: https://figshare.com/s/eaf206879ade35a61f88.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147242721",
                    "name": "Shouhang Du"
                },
                {
                    "authorId": "2167936575",
                    "name": "Jianghe Xing"
                },
                {
                    "authorId": "2239856281",
                    "name": "Shaoyu Wang"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "2290773272",
                    "name": "Jun Li"
                },
                {
                    "authorId": "2291015072",
                    "name": "Hao Liu"
                }
            ]
        },
        {
            "paperId": "1dd33e32d5ca865208586e0eddc4ffb7042c77a4",
            "title": "TL2GH\u00b2T: Triple-Path Local-to-Global Network With Hybrid Head Transformer for Hyperspectral Change Detection",
            "abstract": "With the aid of transformers, significant progress has been achieved in hyperspectral image change detection (HSI-CD) in recent times. Nonetheless, most contemporary detection methods fail to incorporate diverse diagnostic features extracted from hyperspectral (HS) images. In addition, relying solely on algebraic-based techniques to extract information of difference is insufficient for achieving satisfactory detection performance. In this regard, we propose an innovative triple-path local-to-global network (TL2GN), complemented by a hybrid head transformer (HybridHT), called TL2GH2T, tailored for HSI-CD tasks. To be specific, TL2GH2T first investigates spatial, spectral, and spatial\u2013spectral features from a local-to-global perspective. Then, a novel spatial and spectral token fusion (SSTF) module is developed to integrate the above three tokenized features, producing discriminative features from two HS images separately. Moreover, drawing inspiration from chromosomal crossover mechanisms, we propose a HybridHT. Its goal is to simultaneously learn cross correlation and self-correlation information of bitemporal features from a global perspective, producing highly discriminative distinctions. Our approach, validated through extensive experimentation on four varied HS benchmarks, exhibits exceptional performance in HSI-CD, outperforming contemporary methods in both visual and quantitative evaluations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144172680",
                    "name": "Zhonghao Chen"
                },
                {
                    "authorId": "2249093595",
                    "name": "Yuyang Wang"
                },
                {
                    "authorId": "1842981",
                    "name": "S. K. Roy"
                },
                {
                    "authorId": "9309089",
                    "name": "Hongmin Gao"
                },
                {
                    "authorId": "2248577081",
                    "name": "Yao Ding"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "2238367459",
                    "name": "Zhenfeng Shao"
                },
                {
                    "authorId": "2155845865",
                    "name": "Bing Zhang"
                }
            ]
        },
        {
            "paperId": "54967514282cfa122c84d42957f5c5490bb065e8",
            "title": "Patch Similarity Self-Knowledge Distillation for Cross-View Geo-Localization",
            "abstract": "Cross-view geo-localization is an extremely challenging task due to drastic discrepancies in scene context and object scale between different views. Existing works normally concentrate on aligning the global appearance between two views but underestimate these two discrepancies. In practice, only a small region in the retrieved aerial image can be matched to the whole query ground image (i.e. scene context change). On the other hand, the retrieved aerial images are only able to describe the coarse-grained information but the query ground images can capture the fine-grained details (i.e. object scale change). In this paper, we propose a novel self-distillation framework called Patch Similarity Self-Knowledge Distillation (PaSS-KD), which provides the local and multi-scale knowledge as fine-grained location-related supervision to guide cross-view image feature extraction and representation in a self-enhanced manner. Specifically, we develop an auxiliary image-to-patch retrieval task to explore the scene context change and devise a multi-scale patch partition strategy to sense the object scale change across views. Additionally, our self-distilling framework can be removed to avoid additional computation cost at the inference stage. Extensive experiments show that our method not only achieves the state-of-the-art image retrieval performance on the CVUSA and CVACT benchmarks, but also significantly boosts the fine-grained localization accuracy on the VIGOR dataset. Remarkably, for 10 meter-level localization, we improve the relative accuracy by a factor of $0.8\\times $ and $1.6\\times $ on the VIGOR dataset under same-area and cross-area evaluation, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146039527",
                    "name": "Songlian Li"
                },
                {
                    "authorId": "2268824838",
                    "name": "Min Hu"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "2061178292",
                    "name": "Zhigang Tu"
                }
            ]
        },
        {
            "paperId": "87ab24158dec6a0e25b9ff4e2a2f6a0e8403325e",
            "title": "A Novel LOD Rendering Method With Multilevel Structure-Keeping Mesh Simplification and Fast Texture Alignment for Realistic 3-D Models",
            "abstract": "Fast, high-precision texture maps and high-frame-rate level of detail (LOD) generation for realistic 3-D models are foundational data infrastructures for smart cities. However, LOD generation faces three main issues: reduced model accuracy from mesh simplification, inefficient texture memory utilization, and browsing lag with detail loss. This article proposed a novel LOD rendering method with multilevel structure-keeping mesh simplification and fast texture alignment for realistic 3-D models. First, a multilevel structure-keeping mesh simplification method with mesh segmentation and vertex classification was used to generate a simplified mesh with high-precision structure preservation. Second, a fast texture alignment method was proposed that uses segmentation information and least-squares conformal map (LSCM) parameterization to acquire texture blocks. The method integrated integral images and a precise multitemplate strategy to align texture blocks, to obtain texture maps with high completeness and high occupancy. Finally, by integrating these methods, an LOD generation method with fast multilevel pyramid construction and adaptive tree organization is proposed. This method achieved high-precision multilevel structure keeping, along with a high-occupancy rate of texture maps, facilitating a high frame rate for LOD model construction. Compared with quadratic error function (QEF), quadratic error metrics (QEMs), low-poly, computational geometry algorithms library (CGAL), and Nvdiffrec, the proposed mesh simplification algorithm demonstrated average accuracy improvements of 12.1%, 24.2%, 57.1%, 17.9%, and 3.2%, respectively. Compared with open multi-view environment (OpenMVE), ContextCapture, and Xatlas, the proposed texture alignment algorithm achieved average occupancy improvements of 27.74%, 11.89%, and 4.80%, respectively. Compared with the state-of-the-art ContextCapture and Smart3D, the proposed method for browsing large-scale realistic 3-D models increased the frame rate by 17.3% and 14.4%, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2274724204",
                    "name": "Yingwei Ge"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "2274387506",
                    "name": "Bingxuan Guo"
                },
                {
                    "authorId": "2238367459",
                    "name": "Zhenfeng Shao"
                },
                {
                    "authorId": "2284623233",
                    "name": "Jianya Gong"
                },
                {
                    "authorId": "2281604233",
                    "name": "Deren Li"
                }
            ]
        },
        {
            "paperId": "1fdc07b3152bd0e3213b8122224fc96f3070f2f5",
            "title": "Improved Neural Network with Spatial Pyramid Pooling and Online Datasets Preprocessing for Underwater Target Detection Based on Side Scan Sonar Imagery",
            "abstract": "Fast and high-accuracy detection of underwater targets based on side scan sonar images has great potential for marine fisheries, underwater security, marine mapping, underwater engineering and other applications. The following problems, however, must be addressed when using low-resolution side scan sonar images for underwater target detection: (1) the detection performance is limited due to the restriction on the input of multi-scale images; (2) the widely used deep learning algorithms have a low detection effect due to their complex convolution layer structures; (3) the detection performance is limited due to insufficient model complexity in training process; and (4) the number of samples is not enough because of the bad dataset preprocessing methods. To solve these problems, an improved neural network for underwater target detection\u2014which is based on side scan sonar images and fully utilizes spatial pyramid pooling and online dataset preprocessing based on the You Look Only Once version three (YOLO V3) algorithm\u2014is proposed. The methodology of the proposed approach is as follows: (1) the AlexNet, GoogleNet, VGGNet and the ResNet networks and an adopted YOLO V3 algorithm were the backbone networks. The structure of the YOLO V3 model is more mature and compact and has higher target detection accuracy and better detection efficiency than the other models; (2) spatial pyramid pooling was added at the end of the convolution layer to improve detection performance. Spatial pyramid pooling breaks the scale restrictions when inputting images to improve feature extraction because spatial pyramid pooling enables the backbone network to learn faster at high accuracy; and (3) online dataset preprocessing based on YOLO V3 with spatial pyramid pooling increases the number of samples and improves the complexity of the model to further improve detection process performance. Three-side scan imagery datasets were used for training and were tested in experiments. The quantitative evaluation using Accuracy, Recall, Precision, mAP and F1-Score metrics indicates that: for the AlexNet, GoogleNet, VGGNet and ResNet algorithms, when spatial pyramid pooling is added to their backbone networks, the average detection accuracy of the three sets of data was improved by 2%, 4%, 2% and 2%, respectively, as compared to their original formulations. Compared with the original YOLO V3 model, the proposed ODP+YOLO V3+SPP underwater target detection algorithm model has improved detection performance through the mAP qualitative evaluation index has increased by 6%, the Precision qualitative evaluation index has increased by 13%, and the detection efficiency has increased by 9.34%. These demonstrate that adding spatial pyramid pooling and online dataset preprocessing can improve the target detection accuracy of these commonly used algorithms. The proposed, improved neural network with spatial pyramid pooling and online dataset preprocessing based on the YOLO V3 method achieves the highest scores for underwater target detection results for sunken ships, fish flocks and seafloor topography, with mAP scores of 98%, 91% and 96% for the above three kinds of datasets, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145465764",
                    "name": "Jinrui Li"
                },
                {
                    "authorId": "2122090299",
                    "name": "Libin Chen"
                },
                {
                    "authorId": "2115733546",
                    "name": "Jian Shen"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "2110654417",
                    "name": "Xiaosong Liu"
                },
                {
                    "authorId": "2109230283",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                }
            ]
        },
        {
            "paperId": "521b8383818ee1050fefd05ae9f9df4ef2ff2175",
            "title": "AFDE-Net: Building Change Detection Using Attention-Based Feature Differential Enhancement for Satellite Imagery",
            "abstract": "Building change detection (BCD) from satellite imagery is critical for monitoring urbanization, managing agricultural land, and updating geospatial databases. However, complex variations in building roofs that resemble the background of their surroundings pose challenges for deep-learning-based change detection methods due to their focus on color and texture. Additionally, downsampling can result in the loss of spatial information, leading to incomplete buildings and irregular output boundaries. To address these challenges, a novel Siamese network called AFDE-Net is proposed, which combines differential image features and attention modules using a learnable parameter. The AFDE-Net employs an ensemble spatial-channel attention fusion (ESCAF) module, along with a deep supervision (DS) module, to mitigate the loss of spatial information and refine deep features in high-dimensional inputs. Besides, we have created a new dataset (EGY-BCD) comprising high-resolution and multitemporal satellite images captured in four urban and coastal areas in Egypt to detect building changes. The EGY-BCD dataset includes images with complex types of change, such as tall and dense buildings with roofs that resemble the background of their surroundings, which is a challenge for deep-learning algorithms. The proposed method outperforms other methods on the EGY-BCD dataset with an overall accuracy (OA) of 94.3%, an F1-score of 88.8%, and an mIoU of 86.6%. The datasets and codes will be released at https://github.com/oshholail/EGY-BCD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219396609",
                    "name": "S. Holail"
                },
                {
                    "authorId": "2067344277",
                    "name": "T. Saleh"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                }
            ]
        },
        {
            "paperId": "1fad978638cee07513f3667c25ea7033d63325f9",
            "title": "Editorial on Special Issue \"Techniques and Applications of UAV-Based Photogrammetric 3D Mapping\"",
            "abstract": "Recently, 3D mapping has begun to play an increasingly important role in photogrammetric applications [...]",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3039544",
                    "name": "Wanshou Jiang"
                },
                {
                    "authorId": "49047830",
                    "name": "San Jiang"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                }
            ]
        },
        {
            "paperId": "242292ed86a12f0b91397ac5e0d85e7590538d0b",
            "title": "3D Mesh Pre-Processing Method Based on Feature Point Classification and Anisotropic Vertex Denoising Considering Scene Structure Characteristics",
            "abstract": "3D mesh denoising plays an important role in 3D model pre-processing and repair. A fundamental challenge in the mesh denoising process is to accurately extract features from the noise and to preserve and restore the scene structure features of the model. In this paper, we propose a novel feature-preserving mesh denoising method, which was based on robust guidance normal estimation, accurate feature point extraction and an anisotropic vertex denoising strategy. The methodology of the proposed approach is as follows: (1) The dual weight function that takes into account the angle characteristics is used to estimate the guidance normals of the surface, which improved the reliability of the joint bilateral filtering algorithm and avoids losing the corner structures; (2) The filtered facet normal is used to classify the feature points based on the normal voting tensor (NVT) method, which raised the accuracy and integrity of feature classification for the noisy model; (3) The anisotropic vertex update strategy is used in triangular mesh denoising: updating the non-feature points with isotropic neighborhood normals, which effectively suppressed the sharp edges from being smoothed; updating the feature points based on local geometric constraints, which preserved and restored the features while avoided sharp pseudo features. The detailed quantitative and qualitative analyses conducted on synthetic and real data show that our method can remove the noise of various mesh models and retain or restore the edge and corner features of the model without generating pseudo features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46399019",
                    "name": "Yawen Liu"
                },
                {
                    "authorId": "2329820",
                    "name": "Bingxuan Guo"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "2057145820",
                    "name": "Wei Qiu"
                }
            ]
        },
        {
            "paperId": "4d4e35af98ea76cc0ab680f6c674deb5a40ad497",
            "title": "A Spliced Satellite Optical Camera Geometric Calibration Method Based on Inter-Chip Geometry Constraints",
            "abstract": "When in orbit, spliced satellite optical cameras are affected by various factors that degrade the actual image stitching precision and the accuracy of their data products. This is a major bottleneck in the current remote sensing technology. Previous geometric calibration research has mostly focused on stitched satellite images and has largely ignored the inter-chip relationship among original multi-chip images, resulting in accuracy loss in geometric calibration and subsequent image products. Therefore, in this paper, a novel geometric calibration method is proposed for spliced satellite optical cameras. The integral geometric calibration model was developed on inter-chip geometry constraints among multi-chip images, including the corresponding external and internal calibration models. The proposed approach improves uncontrolled geopositioning accuracy and enhances mosaic precision at the same time. For evaluation, images from the optical butting satellite ZiYuan-3 (ZY-3) and mechanical interleaving satellite Tianhui-1 (TH-1) were used for the experiments. Multiple sets of satellite data of the Songshan Calibration field and other regions were used to evaluate the reliability, stability, and applicability of the calibration parameters. The experiment results found that the proposed method obtains reliable camera alignment angles and interior calibration parameters and generates high-precision seamless mosaic images. The calibration scheme is not only suitable for mechanical interleaving cameras with large geometric displacement among multi-chip images but is also effective for optical butting cameras with minor chip offset. It also significantly improves uncontrolled geopositioning accuracy for both types of spliced satellite images. Moreover, the proposed calibration procedure results in multi-chip satellite images being seamlessly stitched together and mosaic errors within one pixel.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41154933",
                    "name": "Tao Wang"
                },
                {
                    "authorId": "2152820817",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2108372766",
                    "name": "Yongsheng Zhang"
                },
                {
                    "authorId": "2144390938",
                    "name": "Zhenchao Zhang"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "2119042376",
                    "name": "Ying Yu"
                },
                {
                    "authorId": "2034188105",
                    "name": "Longhui Wang"
                }
            ]
        }
    ]
}