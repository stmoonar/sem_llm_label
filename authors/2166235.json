{
    "authorId": "2166235",
    "papers": [
        {
            "paperId": "27db92c7fb163202f143c89f5f81040cf940ffab",
            "title": "Causal Forecasting for Pricing",
            "abstract": "This paper proposes a novel method for demand forecasting in a pricing context. Here, modeling the causal relationship between price as an input variable to demand is crucial because retailers aim to set prices in a (profit) optimal manner in a downstream decision making problem. Our methods bring together the Double Machine Learning methodology for causal inference and state-of-the-art transformer-based forecasting models. In extensive empirical experiments, we show on the one hand that our method estimates the causal effect better in a fully controlled setting via synthetic, yet realistic data. On the other hand, we demonstrate on real-world data that our method outperforms forecasting methods in off-policy settings (i.e., when there's a change in the pricing policy) while only slightly trailing in the on-policy setting.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2276429667",
                    "name": "Douglas Schultz"
                },
                {
                    "authorId": "2118151345",
                    "name": "Johannes Stephan"
                },
                {
                    "authorId": "2218803888",
                    "name": "Julian Sieber"
                },
                {
                    "authorId": "2276432722",
                    "name": "Trudie Yeh"
                },
                {
                    "authorId": "2202626639",
                    "name": "Manuel Kunz"
                },
                {
                    "authorId": "2276429499",
                    "name": "Patrick Doupe"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                }
            ]
        },
        {
            "paperId": "552572fa975344350b8af09009c647b4d4598f8c",
            "title": "Deep Learning based Forecasting: a case study from the online fashion industry",
            "abstract": "Demand forecasting in the online fashion industry is particularly amendable to global, data-driven forecasting models because of the industry's set of particular challenges. These include the volume of data, the irregularity, the high amount of turn-over in the catalog and the fixed inventory assumption. While standard deep learning forecasting approaches cater for many of these, the fixed inventory assumption requires a special treatment via controlling the relationship between price and demand closely. In this case study, we describe the data and our modelling approach for this forecasting problem in detail and present empirical results that highlight the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2202626639",
                    "name": "Manuel Kunz"
                },
                {
                    "authorId": "90947077",
                    "name": "Stefan Birr"
                },
                {
                    "authorId": "8653580",
                    "name": "Mones Raslan"
                },
                {
                    "authorId": "143828252",
                    "name": "L. Ma"
                },
                {
                    "authorId": "49969637",
                    "name": "Zhuguo Li"
                },
                {
                    "authorId": "2118151127",
                    "name": "Adele Gouttes"
                },
                {
                    "authorId": "2218803526",
                    "name": "Mateusz Koren"
                },
                {
                    "authorId": "2298773",
                    "name": "T. Naghibi"
                },
                {
                    "authorId": "2118151345",
                    "name": "Johannes Stephan"
                },
                {
                    "authorId": "104413041",
                    "name": "M. Bulycheva"
                },
                {
                    "authorId": "102686496",
                    "name": "Matthias Grzeschik"
                },
                {
                    "authorId": "2078911543",
                    "name": "Armin Keki'c"
                },
                {
                    "authorId": "30891652",
                    "name": "Michael Narodovitch"
                },
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2218803888",
                    "name": "Julian Sieber"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                }
            ]
        },
        {
            "paperId": "7947d22cb949c9a8e11b381ea5611a792955b844",
            "title": "Deep Non-Parametric Time Series Forecaster",
            "abstract": "This paper presents non-parametric baseline models for time series forecasting. Unlike classical forecasting models, the proposed approach does not assume any parametric form for the predictive distribution and instead generates predictions by sampling from the empirical distribution according to a tunable strategy. By virtue of this, the model is always able to produce reasonable forecasts (i.e., predictions within the observed data range) without fail unlike classical models that suffer from numerical stability on some data distributions. Moreover, we develop a global version of the proposed method that automatically learns the sampling strategy by exploiting the information across multiple related time series. The empirical evaluation shows that the proposed methods have reasonable and consistent performance across all datasets, proving them to be strong baselines to be considered in one's forecasting toolbox.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "26320653",
                    "name": "Syama Sundar Rangapuram"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "2066263165",
                    "name": "Lorenzo Stella"
                },
                {
                    "authorId": "2067154581",
                    "name": "Valentin Flunkert"
                },
                {
                    "authorId": "2276206710",
                    "name": "David Salinas"
                },
                {
                    "authorId": "2276262553",
                    "name": "Yuyang Wang"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                }
            ]
        },
        {
            "paperId": "b71f510bed5d9d0c7ef47838c44bef85b95ee6e2",
            "title": "Coherent Probabilistic Forecasting of Temporal Hierarchies",
            "abstract": "Forecasts at different time granularities are required in practice for addressing various business problems starting from short-term operational to medium-term tactical and to long-term strategic planning. These forecasting problems are usually treated independently by learning different ML models which results in forecasts that are not consistent with the temporal aggregation structure, leading to inef\ufb01cient decision making. While prior work addressed this problem, this typically uses a post-hoc reconciliation strategy, which leads to sub-optimal results and cannot produce probabilistic forecasts. In this paper, we present a global model that produces coherent, probabilistic forecasts for different time granularities by learning joint embeddings for the different aggregation levels with graph neural networks and temporal reconciliation. Temporal reconciliation not only enables consistent decisions for business problems across different planning horizons but also improves the quality of forecasts at \ufb01ner time granularities. A thorough empirical evaluation illustrates the bene\ufb01ts of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26320653",
                    "name": "Syama Sundar Rangapuram"
                },
                {
                    "authorId": "51016973",
                    "name": "Rajbir-Singh Nirwan"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                }
            ]
        },
        {
            "paperId": "02674f6f2798ed6ab012397bbfc4d490f0661eff",
            "title": "Multi-Objective Model Selection for Time Series Forecasting",
            "abstract": "Research on time series forecasting has predominantly focused on developing methods that improve accuracy. However, other criteria such as training time or latency are critical in many real-world applications. We therefore address the question of how to choose an appropriate forecasting model for a given dataset among the plethora of available forecasting methods when accuracy is only one of many criteria. For this, our contributions are two-fold. First, we present a comprehensive benchmark, evaluating 7 classical and 6 deep learning forecasting methods on 44 heterogeneous, publicly available datasets. The benchmark code is open-sourced along with evaluations and forecasts for all methods. These evaluations enable us to answer open questions such as the amount of data required for deep learning models to outperform classical ones. Second, we leverage the benchmark evaluations to learn good defaults that consider multiple objectives such as accuracy and latency. By learning a mapping from forecasting models to performance metrics, we show that our method PARETOSELECT is able to accurately select models from the Pareto front -- alleviating the need to train or evaluate many forecasting models for model selection. To the best of our knowledge, PARETOSELECT constitutes the first method to learn default models in a multi-objective setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2101840120",
                    "name": "Oliver Borchert"
                },
                {
                    "authorId": "144607961",
                    "name": "David Salinas"
                },
                {
                    "authorId": "2067154581",
                    "name": "Valentin Flunkert"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "51249380",
                    "name": "Stephan Gunnemann"
                }
            ]
        },
        {
            "paperId": "3145014e65861f3981a652222daf964bca3bbe62",
            "title": "Diverse Counterfactual Explanations for Anomaly Detection in Time Series",
            "abstract": "Data-driven methods that detect anomalies in times series data are ubiquitous in practice, but they are in general unable to provide helpful explanations for the predictions they make. In this work we propose a model-agnostic algorithm that generates counterfactual ensemble explanations for time series anomaly detection models. Our method generates a set of diverse counterfactual examples, i.e, multiple perturbed versions of the original time series that are not considered anomalous by the detection model. Since the magnitude of the perturbations is limited, these counterfactuals represent an ensemble of inputs similar to the original time series that the model would deem normal. Our algorithm is applicable to any differentiable anomaly detection model. We investigate the value of our method on univariate and multivariate real-world datasets and two deep-learning-based anomaly detection models, under several explainability criteria previously proposed in other data domains such as Validity, Plausibility, Closeness and Diversity. We show that our algorithm can produce ensembles of counterfactual examples that satisfy these criteria and thanks to a novel type of visualisation, can convey a richer interpretation of a model's internal mechanism than existing methods. Moreover, we design a sparse variant of our method to improve the interpretability of counterfactual explanations for high-dimensional time series anomalies. In this setting, our explanation is localised on only a few dimensions and can therefore be communicated more efficiently to the model's user.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2285351482",
                    "name": "Deborah Sulem"
                },
                {
                    "authorId": "2192704",
                    "name": "Michele Donini"
                },
                {
                    "authorId": "2625318",
                    "name": "M. B. Zafar"
                },
                {
                    "authorId": "51055581",
                    "name": "Fran\u00e7ois-Xavier Aubet"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "2155897297",
                    "name": "Sanjiv Das"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "356069daf4d4628559da92c9245aad39e578e090",
            "title": "Multivariate Quantile Function Forecaster",
            "abstract": "We propose Multivariate Quantile Function Forecaster (MQF$^2$), a global probabilistic forecasting method constructed using a multivariate quantile function and investigate its application to multi-horizon forecasting. Prior approaches are either autoregressive, implicitly capturing the dependency structure across time but exhibiting error accumulation with increasing forecast horizons, or multi-horizon sequence-to-sequence models, which do not exhibit error accumulation, but also do typically not model the dependency structure across time steps. MQF$^2$ combines the benefits of both approaches, by directly making predictions in the form of a multivariate quantile function, defined as the gradient of a convex function which we parametrize using input-convex neural networks. By design, the quantile function is monotone with respect to the input quantile levels and hence avoids quantile crossing. We provide two options to train MQF$^2$: with energy score or with maximum likelihood. Experimental results on real-world and synthetic datasets show that our model has comparable performance with state-of-the-art methods in terms of single time step metrics while capturing the time dependency structure.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "49283927",
                    "name": "Kelvin K. Kan"
                },
                {
                    "authorId": "150098869",
                    "name": "Franccois-Xavier Aubet"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "1845861038",
                    "name": "Youngsuk Park"
                },
                {
                    "authorId": "1915597",
                    "name": "Konstantinos Benidis"
                },
                {
                    "authorId": "49418655",
                    "name": "Lars Ruthotto"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                }
            ]
        },
        {
            "paperId": "363d88aa10012b4469670a1d7ba7c8602e6b8e61",
            "title": "Intrinsic Anomaly Detection for Multi-Variate Time Series",
            "abstract": "We introduce a novel, practically relevant variation of the anomaly detection problem in multi-variate time series: intrinsic anomaly detection. It appears in diverse practical scenarios ranging from DevOps to IoT, where we want to recognize failures of a system that operates under the influence of a surrounding environment. Intrinsic anomalies are changes in the functional dependency structure between time series that represent an environment and time series that represent the internal state of a system that is placed in said environment. We formalize this problem, provide under-studied public and new purpose-built data sets for it, and present methods that handle intrinsic anomaly detection. These address the short-coming of existing anomaly detection methods that cannot differentiate between expected changes in the system's state and unexpected ones, i.e., changes in the system that deviate from the environment's influence. Our most promising approach is fully unsupervised and combines adversarial learning and time series representation learning, thereby addressing problems such as label sparsity and subjectivity, while allowing to navigate and improve notoriously problematic anomaly detection data sets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "29938263",
                    "name": "Stephan Rabanser"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "4565995",
                    "name": "Kashif Rasul"
                },
                {
                    "authorId": "2101840120",
                    "name": "Oliver Borchert"
                },
                {
                    "authorId": "4708988",
                    "name": "Richard Kurle"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "1405223072",
                    "name": "Michael Bohlke-Schneider"
                },
                {
                    "authorId": "1967156",
                    "name": "Nicolas Papernot"
                },
                {
                    "authorId": "2067154581",
                    "name": "Valentin Flunkert"
                }
            ]
        },
        {
            "paperId": "3da74e3763e15e5c964d92a736be2822901a0826",
            "title": "On the detrimental effect of invariances in the likelihood for variational inference",
            "abstract": "Variational Bayesian posterior inference often requires simplifying approximations such as mean-field parametrisation to ensure tractability. However, prior work has associated the variational mean-field approximation for Bayesian neural networks with underfitting in the case of small datasets or large model sizes. In this work, we show that invariances in the likelihood function of over-parametrised models contribute to this phenomenon because these invariances complicate the structure of the posterior by introducing discrete and/or continuous modes which cannot be well approximated by Gaussian mean-field distributions. In particular, we show that the mean-field approximation has an additional gap in the evidence lower bound compared to a purpose-built posterior that takes into account the known invariances. Importantly, this invariance gap is not constant; it vanishes as the approximation reverts to the prior. We proceed by first considering translation invariances in a linear model with a single data point in detail. We show that, while the true posterior can be constructed from a mean-field parametrisation, this is achieved only if the objective function takes into account the invariance gap. Then, we transfer our analysis of the linear model to neural networks. Our analysis provides a framework for future work to explore solutions to the invariance problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4708988",
                    "name": "Richard Kurle"
                },
                {
                    "authorId": "3234984",
                    "name": "R. Herbrich"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "49416149",
                    "name": "Bernie Wang"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                }
            ]
        },
        {
            "paperId": "ab84c5286a1b67abca97574710cf1a4562c50a2d",
            "title": "Multivariate Time Series Forecasting with Latent Graph Inference",
            "abstract": "This paper introduces a new approach for Multivariate Time Series forecasting that jointly infers and leverages relations among time series. Its modularity allows it to be integrated with current univariate methods. Our approach allows to trade-off accuracy and computational efficiency gradually via offering on one extreme inference of a potentially fully-connected graph or on another extreme a bipartite graph. In the potentially fully-connected case we consider all pair-wise interactions among time-series which yields the best forecasting accuracy. Conversely, the bipartite case leverages the dependency structure by inter-communicating the N time series through a small set of K auxiliary nodes that we introduce. This reduces the time and memory complexity w.r.t. previous graph inference methods from O(N^2) to O(NK) with a small trade-off in accuracy. We demonstrate the effectiveness of our model in a variety of datasets where both of its variants perform better or very competitively to previous graph inference methods in terms of forecasting accuracy and time efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "73240341",
                    "name": "Victor Garcia Satorras"
                },
                {
                    "authorId": "26320653",
                    "name": "Syama Sundar Rangapuram"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                }
            ]
        }
    ]
}