{
    "authorId": "1390612725",
    "papers": [
        {
            "paperId": "a8cb1c9b1836caa48f771e29dcc0df7840e4ec71",
            "title": "How does the Memorization of Neural Networks Impact Adversarial Robust Models?",
            "abstract": "Recent studies suggest that \"memorization\" is one necessary factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: (a) Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose Benign Adversarial Training (BAT) which can facilitate adversarial training to avoid fitting \"harmful\" atypical samples and fit as more \"benign\" atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show that it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets for image classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2018756699",
                    "name": "Han Xu"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "2108329255",
                    "name": "Wentao Wang"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "1739705",
                    "name": "Anil K. Jain"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "fa061277792ffbde886b99398bc91459724f6eda",
            "title": "Large-Scale Graph Neural Networks: The Past and New Frontiers",
            "abstract": "Graph Neural Networks (GNNs) have gained significant attention in recent years due to their ability to model complex relationships between entities in graph-structured data such as social networks, protein structures, and knowledge graphs. However, due to the size of real-world industrial graphs and the special architecture of GNNs, it is a long-lasting challenge for engineers and researchers to deploy GNNs on large-scale graphs, which significantly limits their applications in real-world applications. In this tutorial, we will cover the fundamental scalability challenges of GNNs, frontiers of large-scale GNNs including classic approaches and some newly emerging techniques, the evaluation and comparison of scalable GNNs, and their large-scale real-world applications. Overall, this tutorial aims to provide a systematic and comprehensive understanding of the challenges and state-of-the-art techniques for scaling GNNs. The summary and discussion on future directions will inspire engineers and researchers to explore new ideas and developments in this rapidly evolving field. The website of this tutorial is available at https://sites.google.com/ncsu.edu/gnnkdd2023tutorial.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066270999",
                    "name": "Rui Xue"
                },
                {
                    "authorId": "2049039664",
                    "name": "Haoyu Han"
                },
                {
                    "authorId": "2187164642",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "2153429147",
                    "name": "Neil Shah"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "0071604cdf29429e0fa3d8a58d66b3874eb5645f",
            "title": "Multimodal Emotion Recognition Based on Feature Fusion",
            "abstract": "In the field of human-computer interaction, human emotion recognition is a challenging problem, and it is also a key link to achieve barrier-free communication between human and machine. At present, most of the emotion recognition algorithms are constructed based on single modal social information, and the recognition results are one-sided and easily disturbed. The recognition accuracy is often difficult to meet the practical requirements after being separated from specific social environment conditions. Based on the above situation and problems, this paper adopts multimodal input and simultaneously includes three modal information of audio, text and facial expression to recognition emotion. Three single modal emotion recognition models are proposed based on three different input information, and the multimodal emotion recognition model are constructed by different feature fusion methods. The experimental results showed that the accuracy of multimodal model on the CH-SIMS dataset was 93.92%. In addition, compared with other emotion recognition models, the effectiveness of the proposed method is verified.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182141797",
                    "name": "Yurui Xu"
                },
                {
                    "authorId": "2157134824",
                    "name": "Xiao Wu"
                },
                {
                    "authorId": "2087042688",
                    "name": "Hang Su"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "458b81d5d39421c4a208666434e4edfe065fab0c",
            "title": "Primary Study on the Face-recognition Framework with anti-spoofing function",
            "abstract": "In pracyical life scene, most access control systems (ACS) cannot solve the problem of various sproofing ways and mask-wearing recognition. In this paper, a new security classification framework based on face recognition is proposed. This framework uses face recognition algorithm with anti-spoofing function. In order to evaluate the performance of the framework, this paper employs the Chinese Academy of Science Institute of Automation-Face Anti-spoofing Datasets (CASIA-FASD) as benchmarks. Performance evaluation indicates that the Half Total Error Rate (HTER) is 9.7%, the Equal Error Rate (EER) is 5.5%. The results demonstrate that this framework has a high anti-spoofing capability and can be employed on the embedded system to complete the mask detection in real-time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157134824",
                    "name": "Xiao Wu"
                },
                {
                    "authorId": "2197879426",
                    "name": "Dekang Zhang"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "6f80d1ade43ae048763d65c6e8e913d9a31de4be",
            "title": "Accepted Tutorials at The Web Conference 2022",
            "abstract": "This paper summarizes the content of the 20 tutorials that have been given at The Web Conference 2022: 85% of these tutorials are lecture style, and 15% of these are hands on.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50271459",
                    "name": "Riccardo Tommasini"
                },
                {
                    "authorId": "2034201368",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "2154990549",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "2108986527",
                    "name": "Hongwei Wang"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "134000266",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "144125621",
                    "name": "M. Schedl"
                },
                {
                    "authorId": "3124784",
                    "name": "E. Lex"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "37386609",
                    "name": "J. Forberg"
                },
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "33122761",
                    "name": "P. Bonte"
                },
                {
                    "authorId": "50535911",
                    "name": "Marco Balduini"
                },
                {
                    "authorId": "2130209106",
                    "name": "Matteo Belcao"
                },
                {
                    "authorId": "1490541824",
                    "name": "Emanuele Della Valle"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "1490931831",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "66442354",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2108941389",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "1380259269",
                    "name": "Jamell Dacon"
                },
                {
                    "authorId": "95104850",
                    "name": "L. Lye"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "2181680391",
                    "name": "Stefan Neumann"
                },
                {
                    "authorId": "35332118",
                    "name": "Bruno Ordozgoiti"
                },
                {
                    "authorId": "2066327465",
                    "name": "S. Razniewski"
                },
                {
                    "authorId": "2064460784",
                    "name": "H. Arnaout"
                },
                {
                    "authorId": "2135976772",
                    "name": "Shrestha Ghosh"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                },
                {
                    "authorId": "2116666963",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2169468461",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "1718694",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "2116441692",
                    "name": "Bang Liu"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "1398926410",
                    "name": "D. Garijo"
                },
                {
                    "authorId": "2284176",
                    "name": "Hans Chalupsky"
                },
                {
                    "authorId": "2628881",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "1637421061",
                    "name": "Ilias Kanellos"
                },
                {
                    "authorId": "1760642",
                    "name": "Dimitris Sacharidis"
                },
                {
                    "authorId": "1768540",
                    "name": "Thanasis Vergoulis"
                },
                {
                    "authorId": "2726036",
                    "name": "Nurendra Choudhary"
                },
                {
                    "authorId": "36724558",
                    "name": "N. Rao"
                },
                {
                    "authorId": "2691095",
                    "name": "Karthik Subbian"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                },
                {
                    "authorId": "51050025",
                    "name": "Friedhelm Victor"
                },
                {
                    "authorId": "1679379",
                    "name": "Bernhard Haslhofer"
                },
                {
                    "authorId": "2055401797",
                    "name": "George Katsogiannis-Meimarakis"
                },
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "28044622",
                    "name": "Shengmin Jin"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "2281410",
                    "name": "R. Zafarani"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "51467955",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "2116710405",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "92633145",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "47561503",
                    "name": "Peng Wu"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "ab13fbbbb97c1cf67bc88bf0d9d4ee401142d9e0",
            "title": "Learning from Imbalanced Crowdsourced Labeled Data",
            "abstract": "Crowdsourcing has proven to be a cost-e\ufb00ective way to meet the demands for labeled training data in supervised deep learning models. However, crowdsourced labels are often inconsistent and noisy due to cognitive and expertise di\ufb00erences among crowd workers. Existing approaches either infer latent true labels from noisy crowdsourced labels or learn a discriminative model directly from the crowdsourced labeled data, assuming the latent true label distribution is class-balanced. Unfortunately, in many real-world applications, the true label distribution typically is imbalanced across classes. Therefore, in this paper, we address the problem of learning from crowdsourced labeled data with an imbalanced true label distribution. We propose a new framework, named \u201cLearning from Imbalanced Crowdsourced Labeled Data\u201d (ICED), which simultaneously infers true labels from imbalanced crowdsourced labeled data and achieves high accuracy on downstream tasks such as classi\ufb01cation. The ICED framework consists of two modules, i.e., a true label inference module and a synthetic data generation module, that augment each other iteratively. Extensive experiments conducted on both synthetic and real-world datasets demonstrate the e\ufb00ectiveness of the ICED framework. Our code is available at https://github.com/wentao-repo/ICED .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108329255",
                    "name": "Wentao Wang"
                },
                {
                    "authorId": "80601470",
                    "name": "J. Thekinen"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "cd08ea42a82ac44374f3c1338469039599678ab8",
            "title": "Enhancing Adversarial Training with Feature Separability",
            "abstract": "Deep Neural Network (DNN) are vulnerable to adversarial attacks. As a countermeasure, adversarial training aims to achieve robustness based on the min-max optimization problem and it has shown to be one of the most effective defense strategies. However, in this work, we found that compared with natural training, adversarial training fails to learn better feature representations for either clean or adversarial samples, which can be one reason why adversarial training tends to have severe overfitting issues and less satisfied generalize performance. Specifically, we observe two major shortcomings of the features learned by existing adversarial training methods:(1) low intra-class feature similarity; and (2) conservative inter-classes feature variance. To overcome these shortcomings, we introduce a new concept of adversarial training graph (ATG) with which the proposed adversarial training with feature separability (ATFS) enables to coherently boost the intra-class feature similarity and increase inter-class feature variance. Through comprehensive experiments, we demonstrate that the proposed ATFS framework significantly improves both clean and robust performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1527096073",
                    "name": "Yaxin Li"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "46485412",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2108329255",
                    "name": "Wentao Wang"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "140f168d8f4e5d110416eb23bf53be7ac4d090cd",
            "title": "Elastic Graph Neural Networks",
            "abstract": "While many existing graph neural networks (GNNs) have been proven to perform $\\ell_2$-based graph smoothing that enforces smoothness globally, in this work we aim to further enhance the local smoothness adaptivity of GNNs via $\\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs (Elastic GNNs) based on $\\ell_1$ and $\\ell_2$-based graph smoothing. In particular, we propose a novel and general message passing scheme into GNNs. This message passing algorithm is not only friendly to back-propagation training but also achieves the desired smoothing properties with a theoretical convergence guarantee. Experiments on semi-supervised learning tasks demonstrate that the proposed Elastic GNNs obtain better adaptivity on benchmark datasets and are significantly robust to graph adversarial attacks. The implementation of Elastic GNNs is available at \\url{https://github.com/lxiaorui/ElasticGNN}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "2112343701",
                    "name": "W. Jin"
                },
                {
                    "authorId": "47009435",
                    "name": "Yao Ma"
                },
                {
                    "authorId": "1527096073",
                    "name": "Yaxin Li"
                },
                {
                    "authorId": "2145497065",
                    "name": "Hua Liu"
                },
                {
                    "authorId": "2108941389",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "2114009138",
                    "name": "Ming Yan"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "3b2f54f38900d0d5b89ef116fbce9b153acf8cf1",
            "title": "Faster than real-time simulation: methods, tools, and applications",
            "abstract": "Real-time simulation enables the understanding of system operating conditions by evaluating simulation models of physical components running synchronized at the real-time wall clock. Leveraging the real-time measurements of comprehensive system models, faster than real-time (FTRT) simulation allows the evaluation of system architectures at speeds faster than real-time. FTRT simulation can assist in predicting the system's behavior efficiently, thus assisting the operation of system processes. Namely, the provided acceleration can be used for improving system scheduling, assessing system vulnerabilities, and predicting system disruptions in real-time systems. The acceleration of simulation times can be achieved by utilizing digital real-time simulators (RTS) and high-performance computing (HPC) architectures. FTRT simulation has been widely used, among others, for the operation, design, and investigation of power system events, building emergency management plans, wildfire prediction, etc. In this paper, we review the existing literature on FTRT simulation and its applications in different disciplines, with a particular focus on power systems. We present existing system modeling approaches, simulation tools and computing frameworks, and stress the importance of FTRT accuracy.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "51231866",
                    "name": "Juan Ospina"
                },
                {
                    "authorId": "1814068574",
                    "name": "Ioannis Zografopoulos"
                },
                {
                    "authorId": "2072265561",
                    "name": "Alonzo Russell"
                },
                {
                    "authorId": "2953860",
                    "name": "Charalambos Konstantinou"
                }
            ]
        },
        {
            "paperId": "47b43c2c24c0305861bb6ba89ff32c8a41814294",
            "title": "Towards the Memorization Effect of Neural Networks in Adversarial Training",
            "abstract": "Recent studies suggest that ``memorization'' is one important factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: (a) Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose Benign Adversarial Training (BAT) which can facilitate adversarial training to avoid fitting ``harmful'' atypical samples and fit as more ``benign'' atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets such as CIFAR100 and Tiny~ImageNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2018756699",
                    "name": "Han Xu"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "2108329255",
                    "name": "Wentao Wang"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "1739705",
                    "name": "Anil K. Jain"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        }
    ]
}