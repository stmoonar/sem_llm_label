{
    "authorId": "2513991",
    "papers": [
        {
            "paperId": "87b264d9d0364968842845c7141838429a94ef47",
            "title": "OpenFact at CheckThat! 2024: Combining Multiple Attack Methods for Effective Adversarial Text Generation",
            "abstract": "This paper presents the experiments and results for the CheckThat! Lab at CLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial Examples (InCrediblAE). The primary objective of this task was to generate adversarial examples in five problem domains in order to evaluate the robustness of widely used text classification methods (fine-tuned BERT, BiLSTM, and RoBERTa) when applied to credibility assessment issues. This study explores the application of ensemble learning to enhance adversarial attacks on natural language processing (NLP) models. We systematically tested and refined several adversarial attack methods, including BERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across various misinformation tasks. By developing modified versions of BERT-Attack and hybrid methods, we achieved significant improvements in attack effectiveness. Our results demonstrate the potential of modification and combining multiple methods to create more sophisticated and effective adversarial attack strategies, contributing to the development of more robust and secure systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                },
                {
                    "authorId": "2598459",
                    "name": "P. Stolarski"
                },
                {
                    "authorId": "3420296",
                    "name": "Milena Str\u00f3\u017cyna"
                },
                {
                    "authorId": "1885809",
                    "name": "Elzbieta Lewanska"
                },
                {
                    "authorId": "2315350192",
                    "name": "Aleksandra Wojewoda"
                },
                {
                    "authorId": "2224421615",
                    "name": "Ewelina Ksi\u0119\u017cniak"
                },
                {
                    "authorId": "2224412752",
                    "name": "Marcin Sawi\u0144ski"
                }
            ]
        },
        {
            "paperId": "98fd8b6162c25a620a5f37a30f780c69c4a80bd4",
            "title": "Measuring Americanization: A Global Quantitative Study of Interest in American Topics on Wikipedia",
            "abstract": "We conducted a global comparative analysis of the coverage of American topics in different language versions of Wikipedia, using over 90 million Wikidata items and 40 million Wikipedia articles in 58 languages. Our study aimed to investigate whether Americanization is more or less dominant in different regions and cultures and to determine whether interest in American topics is universal.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "47560015",
                    "name": "Piotr Konieczny"
                },
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                }
            ]
        },
        {
            "paperId": "cd5bcf7729dd0fdc715519e737aa712c49630f8c",
            "title": "OpenFact at CheckThat!-2023: Head-to-Head GPT vs. BERT - A Comparative Study of Transformers Language Models for the Detection of Check-worthy Claims",
            "abstract": "This paper presents the research findings resulting from experiments conducted as part of the Check-That! Lab Task 1B-English submission at CLEF 2023. The aim of the research was to evaluate the check-worthiness of short texts in English. Various methodologies were employed, including zero-shot, few-shot, and fine-tuning techniques, and different GPT and BERT models were assessed. Given the significant increase in the use of GPT models in recent times, we posed a research question to investigate whether GPT models exhibit notable superiority over BERT models in detecting check-worthy claims. Our findings indicate that fine-tuned BERT models can perform comparably to large language models such as GPT-3 in identifying check-worthy claims for this particular task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2224412752",
                    "name": "Marcin Sawi\u0144ski"
                },
                {
                    "authorId": "3223508",
                    "name": "Krzysztof W\u0119cel"
                },
                {
                    "authorId": "2224421615",
                    "name": "Ewelina Ksi\u0119\u017cniak"
                },
                {
                    "authorId": "3420296",
                    "name": "Milena Str\u00f3\u017cyna"
                },
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                },
                {
                    "authorId": "2598459",
                    "name": "P. Stolarski"
                },
                {
                    "authorId": "2846021",
                    "name": "W. Abramowicz"
                }
            ]
        },
        {
            "paperId": "69cdd3d3f780a5407334475fbf8ced7d663c5787",
            "title": "Reliability in Time: Evaluating the Web Sources of Information on COVID-19 in Wikipedia across Various Language Editions from the Beginning of the Pandemic",
            "abstract": "There are over a billion websites on the Internet that can potentially serve as sources of information on various topics. One of the most popular examples of such an online source is Wikipedia. This public knowledge base is co-edited by millions of users from all over the world. Information in each language version of Wikipedia can be created and edited independently. Therefore, we can observe certain inconsistencies in the statements and facts described therein - depending on language and topic. In accordance with the Wikipedia content authoring guidelines, information in Wikipedia articles should be based on reliable, published sources. So, based on data from such a collaboratively edited encyclopedia, we should also be able to find important sources on specific topics. This effect can be potentially useful for people and organizations. The reliability of a source in Wikipedia articles depends on the context. So the same source (website) may have various degrees of reliability in Wikipedia depending on topic and language version. Moreover, reliability of the same source can change over the time. The purpose of this study is to identify reliable sources on a specific topic - the COVID-19 pandemic. Such an analysis was carried out on real data from Wikipedia within selected language versions and within a selected time period.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                },
                {
                    "authorId": "3223508",
                    "name": "Krzysztof W\u0119cel"
                },
                {
                    "authorId": "2846021",
                    "name": "W. Abramowicz"
                }
            ]
        },
        {
            "paperId": "74d3d0b5a27aec5a2c52b5639b9f84010eb6ed35",
            "title": "Identifying Reliable Sources of Information about Companies in Multilingual Wikipedia",
            "abstract": "For over 21 years Wikipedia has been edited by volunteers from all over the world. Such editors have different education, cultural background and competences. One of the core rules of Wikipedia says, that information in its articles should be based on reliable sources and Wikipedia readers must be able to verify particular facts in text. However, reliability is a subjective concept and a reputation of the same source can be assessed differently depending on a person (or group of persons), language and topic. So each language version of Wikipedia may have own rules or criteria on how the website must be assessed before it can be used as a source in references. At the same time, nowadays there are over 1 billion websites on the Internet and only few developed Wikipedia language versions contain non-exhaustive lists of popular websites with reliability assessment. Additionally, since reputation of the source can be changed during the time, such lists must be updated regularly.This study presents the result of identification of reliable sources of information based on the analysis of over 200 million references that were extracted from over 40 million Wikipedia articles. Using DBpedia and Wikidata we identified articles related to various kinds of companies and found the most important sources of information in this area. This also allows to compare differences of the source reliability between Wikipedia languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                },
                {
                    "authorId": "3223508",
                    "name": "Krzysztof W\u0119cel"
                },
                {
                    "authorId": "2846021",
                    "name": "W. Abramowicz"
                }
            ]
        },
        {
            "paperId": "21798e23fc4af57aace2e76850daf372229ac02c",
            "title": "Towards a Systematic Approach to Sync Factual Data across Wikipedia, Wikidata and External Data Sources",
            "abstract": ". This paper addresses one of the largest and most complex data curation work\ufb02ows in existence: Wikipedia and Wikidata, with a high number of users and curators adding factual information from exter-nal sources via a non-systematic Wiki work\ufb02ow to Wikipedia\u2019s infoboxes and Wikidata items. We present high-level analyses of the current state, the challenges and limitations in this work\ufb02ow and supplement it with a quantitative and semantic analysis of the resulting data spaces by deploying DBpedia\u2019s integration and extraction capabilities. Based on an analysis of millions of references from Wikipedia infoboxes in di\ufb00erent languages, we can \ufb01nd the most important sources which can be used to enrich other knowledge bases with information of better quality. An initial tool is presented, the GlobalFactSync browser, as a prototype to discuss further measures to develop a more systematic approach for data curation in the WikiVerse.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "24163430",
                    "name": "M. Hofer"
                },
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "3223508",
                    "name": "Krzysztof W\u0119cel"
                },
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                }
            ]
        },
        {
            "paperId": "efd10ab832bdb00e7ac28d63938c5d88ae63cf4a",
            "title": "Resilient supply chains 4.0 - a research review",
            "abstract": "The purpose of this study is to illustrate knowledge diffusion in building resilient supply chains (RSCs) in terms of time, geography, and subject matter, and to identify the inherent potential inherent in creating RSCs using technology 4.0. Based on a quantitative and qualitative analysis of theoretical and empirical material, research gaps are identified and a conceptual framework for creating an RSC environment is outlined by juxtaposing individual technologies and dimensions of resilience. To the authors' knowledge, this is the most comprehensive approach given previous model proposals. Among other things, the paper defines the terms \u201cresilient supply chain 4.0\u201d. The obtained research results can serve both business practices and inspire causal research. In the context of market disruption, chain elongation, and globalization of operations, this topic will become more and more important every year. The collected material helps to raise awareness in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9699433",
                    "name": "A. Maryniak"
                },
                {
                    "authorId": "101935214",
                    "name": "Yuliia Bulhakova"
                },
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                }
            ]
        },
        {
            "paperId": "234216d885d92432b1f406a3cb1aa69d1648ad84",
            "title": "Modeling Popularity and Reliability of Sources in Multilingual Wikipedia",
            "abstract": "One of the most important factors impacting quality of content in Wikipedia is presence of reliable sources. By following references, readers can verify facts or find more details about described topic. A Wikipedia article can be edited independently in any of over 300 languages, even by anonymous users, therefore information about the same topic may be inconsistent. This also applies to use of references in different language versions of a particular article, so the same statement can have different sources. In this paper we analyzed over 40 million articles from the 55 most developed language versions of Wikipedia to extract information about over 200 million references and find the most popular and reliable sources. We presented 10 models for the assessment of the popularity and reliability of the sources based on analysis of meta information about the references in Wikipedia articles, page views and authors of the articles. Using DBpedia and Wikidata we automatically identified the alignment of the sources to a specific domain. Additionally, we analyzed the changes of popularity and reliability in time and identified growth leaders in each of the considered months. The results can be used for quality improvements of the content in different languages versions of Wikipedia.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                },
                {
                    "authorId": "3223508",
                    "name": "Krzysztof W\u0119cel"
                },
                {
                    "authorId": "2846021",
                    "name": "W. Abramowicz"
                }
            ]
        },
        {
            "paperId": "31e8dd0c67fcbb959230a017395751a8253fd9f8",
            "title": "Quality assessment of Wikipedia and its sources",
            "abstract": "Slides from the December 2020 Wikimedia Research Showcase: https://www.youtube.com/watch?v=v9Wcc-TeaEYInformation in Wikipedia can be edited in over 300 languages independently. Therefore often the same subject in Wikipedia can be described differently depending on language edition. In order to compare information between them one usually needs to understand each of considered languages. We work on solutions that can help to automate this process. They leverage machine learning and artificial intelligence algorithms. The crucial component, however, is assessment of article quality therefore we need to know how to define and extract different quality measures. This presentation briefly introduces some of the recent activities of Department of Information Systems at Poznan University of Economics and Business related to quality assessment of multilingual content in Wikipedia. In particular, we demonstrate some of the approaches for the reliability assessment of sources in Wikipedia articles. Such solutions can help to enrich various language editions of Wikipedia and other knowledge bases with information of better quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                }
            ]
        },
        {
            "paperId": "56bf3c2223d01ca19aa69e62fba08c5f065422ce",
            "title": "How to Inspect and Measure Data Quality about Scientific Publications: Use Case of Wikipedia and CRIS Databases",
            "abstract": "The quality assurance of publication data in collaborative knowledge bases and in current research information systems (CRIS) becomes more and more relevant by the use of freely available spatial information in different application scenarios. When integrating this data into CRIS, it is necessary to be able to recognize and assess their quality. Only then is it possible to compile a result from the available data that fulfills its purpose for the user, namely to deliver reliable data and information. This paper discussed the quality problems of source metadata in Wikipedia and CRIS. Based on real data from over 40 million Wikipedia articles in various languages, we performed preliminary quality analysis of the metadata of scientific publications using a data quality tool. So far, no data quality measurements have been programmed with Python to assess the quality of metadata from scientific publications in Wikipedia and CRIS. With this in mind, we programmed the methods and algorithms as code, but presented it in the form of pseudocode in this paper to measure the quality related to objective data quality dimensions such as completeness, correctness, consistency, and timeliness. This was prepared as a macro service so that the users can use the measurement results with the program code to make a statement about their scientific publications metadata so that the management can rely on high-quality data when making decisions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40866415",
                    "name": "Otmane Azeroual"
                },
                {
                    "authorId": "2513991",
                    "name": "W\u0142odzimierz Lewoniewski"
                }
            ]
        }
    ]
}