{
    "authorId": "3216767",
    "papers": [
        {
            "paperId": "7bf4ce81f7fcc99c576c0b364a0f1195a8d9a651",
            "title": "FDup framework: A General-purpose solution for Efficient Entity Deduplication of Record Collections",
            "abstract": "Deduplication is a technique aimed at identifying and resolving duplicate metadata records in a collection with a special focus on the performances of the approach. This paper describes FDup(Flat Collections Deduper), a general-purpose software framework supporting a complete deduplication workflow to manage big data record collections: metadata record data model definition, identification of candidate duplicates, identification of duplicates. FDup brings two main innovations: first, it delivers a full deduplication framework in a single easy-to-use software package based on Apache Spark Hadoop framework, where developers can customize the optimal and parallel workflow steps of blocking, sliding windows, and similarity matching function via an intuitive configuration file; second, it introduces a novel approach to improve performance, beyond the known techniques of \u201cblocking\u201d and \u201csliding window\u201d, by introducing a smart similarity-matching function T-match. T-match is engineered as a decision tree that drives the comparisons of the fields of two records as branches of predicates and allows for successful or unsuccessful early exit strategies. The efficacy of the approach is proved by experiments performed over big data collections of metadata records in the OpenAIRE Graph, a known open-access knowledge base in Scholarly communication.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145546470",
                    "name": "M. Bonis"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "2240178920",
                    "name": "Paolo Manghi"
                }
            ]
        },
        {
            "paperId": "48a1a381b52243d2b6f033e42fec4ab2bd3afd16",
            "title": "Reflections on the Misuses of ORCID iDs",
            "abstract": ". Since 2012, the \u201cOpen Researcher and Contributor Identi\ufb01cation Initiative\u201d (ORCID) has been successfully running a worldwide registry, with the aim of unequivocally pinpoint researchers and the body of knowledge they contributed to. In practice, ORCID clients, e.g., publishers, repositories, and CRIS systems, make sure their metadata can refer to iDs in the ORCID registry to associate authors and their work unambiguously. However, the ORCID infrastructure still su\ufb00ers from several \u201cservice misuses\u201d, which put at risk its very mission and should be therefore identi\ufb01ed and tackled. In this paper, we classify and qualitatively document such misuses, occurring from both users (researchers and organisations) of the ORCID registry and the ORCID clients. We conclude providing an outlook and a few recommendations aiming at improving the exploitation of the ORCID infrastructure.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1824207",
                    "name": "Miriam Baglioni"
                },
                {
                    "authorId": "2043406",
                    "name": "Andrea Mannocci"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "39565794",
                    "name": "A. Bardi"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                }
            ]
        },
        {
            "paperId": "671e2509b993820200c7316bfc81b33a0206b525",
            "title": "BIP! DB: A Dataset of Impact Measures for Scientific Publications",
            "abstract": "The growth rate of the number of scientific publications is constantly increasing, creating important challenges in the identification of valuable research and in various scholarly data management applications, in general. In this context, measures which can effectively quantify the scientific impact could be invaluable. In this work, we present BIP! DB, an open dataset that contains a variety of impact measures calculated for a large collection of more than 100 million scientific publications from various disciplines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1768540",
                    "name": "Thanasis Vergoulis"
                },
                {
                    "authorId": "1637421061",
                    "name": "Ilias Kanellos"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "2043406",
                    "name": "Andrea Mannocci"
                },
                {
                    "authorId": "40056258",
                    "name": "Serafeim Chatzopoulos"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "2877400",
                    "name": "Natalia Manola"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                }
            ]
        },
        {
            "paperId": "8df3e55169b9fbecc66b2d9cf32cc0f915cbb94c",
            "title": "DOIBoost - Boosting CrossRef for Research",
            "abstract": ". Research in information science and scholarly communication strongly relies on the availability of openly accessible datasets of scholarly entities metadata and, where possible, their relative payloads. Since such metadata information is scattered across diverse, freely accessible, online resources (e.g. CrossRef, ORCID), researchers in this domain are doomed to struggle with metadata integration problems, in order to produce custom datasets of undocumented and rather obscure provenance. This practice leads to waste of time, duplication of efforts, and typically infringes open science best practices of transparency and reproducibility of science. In this article, we describe how to generate DOIBoost, a metadata collection that enriches CrossRef with inputs from Microsoft Academic Graph, ORCID, and Unpaywall for the purpose of supporting high-quality and robust research experiments, saving times to researchers and enabling their comparison. To this aim, we describe the dataset value and its schema, analyse its actual content, and share the software Toolkit and experimental workflow required to reproduce it. The DOIBoost dataset and Software Toolkit are made openly available via Zenodo.org.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "2043406",
                    "name": "Andrea Mannocci"
                }
            ]
        },
        {
            "paperId": "382512e51925207bdfe100dfc4bed0d8ec1b0879",
            "title": "The data-literature interlinking service: Towards a common infrastructure for sharing data-article links",
            "abstract": "Research data publishing is today widely regarded as crucial for reproducibility, proper assessment of scientific results, and as a way for researchers to get proper credit for sharing their data. However, several challenges need to be solved to fully realize its potential, one of them being the development of a global standard for links between research data and literature. Current linking solutions are mostly based on bilateral, ad hoc agreements between publishers and data centers. These operate in silos so that content cannot be readily combined to deliver a network graph connecting research data and literature in a comprehensive and reliable way. The Research Data Alliance (RDA) Publishing Data Services Working Group (PDS-WG) aims to address this issue of fragmentation by bringing together different stakeholders to agree on a common infrastructure for sharing links between datasets and literature. The paper aims to discuss these issues.,This paper presents the synergic effort of the RDA PDS-WG and the OpenAIRE infrastructure toward enabling a common infrastructure for exchanging data-literature links by realizing and operating the Data-Literature Interlinking (DLI) Service. The DLI Service populates and provides access to a graph of data set-literature links (at the time of writing close to five million, and growing) collected from a variety of major data centers, publishers, and research organizations.,To achieve its objectives, the Service proposes an interoperable exchange data model and format, based on which it collects and publishes links, thereby offering the opportunity to validate such common approach on real-case scenarios, with real providers and consumers. Feedback of these actors will drive continuous refinement of the both data model and exchange format, supporting the further development of the Service to become an essential part of a universal, open, cross-platform, cross-discipline solution for collecting, and sharing data set-literature links.,This realization of the DLI Service is the first technical, cross-community, and collaborative effort in the direction of establishing a common infrastructure for facilitating the exchange of data set-literature links. As a result of its operation and underlying community effort, a new activity, name Scholix, has been initiated involving the technological level stakeholders such as DataCite and CrossRef.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143991305",
                    "name": "A. Burton"
                },
                {
                    "authorId": "145630846",
                    "name": "H. Koers"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "2678026",
                    "name": "Amir Aryani"
                },
                {
                    "authorId": "3086940",
                    "name": "M. Diepenbroek"
                },
                {
                    "authorId": "41198256",
                    "name": "Uwe Schindler"
                }
            ]
        },
        {
            "paperId": "6478a7b3c97564cac2f376fd487d65520188bfbc",
            "title": "The Scholix Framework for Interoperability in Data-Literature Information Exchange",
            "abstract": "The Scholix Framework (SCHOlarly LInk eXchange) is a high level interoperability framework for exchanging information about the links between scholarly literature and data, as well as between datasets. Over the past decade, publishers, data centers, and indexing services have agreed on and implemented numerous bilateral agreements to establish bidirectional links between research data and the scholarly literature. However, because of the considerable differences inherent to these many agreements, there is very limited interoperability between the various solutions. This situation is fueling systemic inefficiencies and limiting the value of these, separated, sets of links. Scholix, a framework proposed by the RDA/WDS Publishing Data Services working group, envisions a universal interlinking service and proposes the technical guidelines of a multi-hub interoperability framework. Hubs are natural collection and aggregation points for data-literature information from their respective communities. Relevant hubs for the communities of data centers, repositories, and journals include DataCite, OpenAIRE, and Crossref, respectively. The framework respects existing community-specific practices while enabling interoperability among the hubs through a common conceptual model, an information model and open exchange protocols. The proposed framework will make research data, and the related literature, easier to find and easier to interpret and reuse, and will provide additional incentives for researchers to share their data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143991305",
                    "name": "A. Burton"
                },
                {
                    "authorId": "2678026",
                    "name": "Amir Aryani"
                },
                {
                    "authorId": "145630846",
                    "name": "H. Koers"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "35047581",
                    "name": "M. Stocker"
                },
                {
                    "authorId": "3086940",
                    "name": "M. Diepenbroek"
                },
                {
                    "authorId": "41198256",
                    "name": "Uwe Schindler"
                },
                {
                    "authorId": "49771643",
                    "name": "M. Fenner"
                }
            ]
        },
        {
            "paperId": "612d843cbe5f54114fe491c84eed2164fe9fa76d",
            "title": "The OpenAIRE Literature Broker Service for Institutional Repositories",
            "abstract": "OpenAIRE is the European infrastructure for Open Access scholarly communication. It populates and provides access to a graph of objects relative to publications, datasets, people, organizations, projects, and funders aggregated from a variety of data sources, such as institutional repositories, data archives, journals, and CRIS systems. Thanks to infrastructure services, objects in the graph are harmonized to achieve semantic homogeneity, de-duplicated to avoid ambiguities, and enriched with missing properties and/or relationships. OpenAIRE data sources interested in enhancing or incrementing their content may benefit in a number of ways from this graph. This paper presents the high-level architecture behind the realization of an institutional repository Literature Broker Service for OpenAIRE. The Service implements a subscription and notification paradigm supporting institutional repositories willing to: (i) learn about publication objects in OpenAIRE that do not appear in their collection but may be pertinent to it, and (ii) learn about extra properties or relationships relative to publication objects in their collection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152784012",
                    "name": "Michele Artini"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "39565794",
                    "name": "A. Bardi"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "2043406",
                    "name": "Andrea Mannocci"
                }
            ]
        },
        {
            "paperId": "21e56182034297c5363039f5b7746c2ec86d7881",
            "title": "TagTick: A tool for annotation tagging over solr indexes",
            "abstract": "\u201cAnnotation tagging\u201d is an important curation action performed by authorized data curators willing to classify according to a common vocabulary an Information Space of potentially heterogeneous objects (e.g. not sharing common classification schemes). To carry out their activities, data curators need annotation tagging tools which allow them to bulk tag or untag large sets of objects in temporary work sessions, where they can experiment in real-time the effect of their actions before making the changes visible to end-users. Real-time temporary bulk tagging is a non trivial feature to implement, which strictly depends on the back-end used to index the Information Space. This demo presents TagTick, a tool which offers to data curators a fully functional annotation tagging environment over full-text index Apache Solr, considered a \u201cde facto standard\u201d in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152784012",
                    "name": "Michele Artini"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "39565794",
                    "name": "A. Bardi"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                }
            ]
        },
        {
            "paperId": "7338b356a47bebb0167a567c0ba4df87791dce9f",
            "title": "The D-NET software toolkit: A framework for the realization, maintenance, and operation of aggregative infrastructures",
            "abstract": "Purpose \u2013 The purpose of this paper is to present the architectural principles and the services of the D-NET software toolkit. D-NET is a framework where designers and developers find the tools for constructing and operating aggregative infrastructures (systems for aggregating data sources with heterogeneous data models and technologies) in a cost-effective way. Designers and developers can select from a variety of D-NET data management services, can configure them to handle data according to given data models, and can construct autonomic workflows to obtain personalized aggregative infrastructures. Design/methodology/approach \u2013 The paper provides a definition of aggregative infrastructures, sketching architecture, and components, as inspired by real-case examples. It then describes the limits of current solutions, which find their lacks in the realization and maintenance costs of such complex software. Finally, it proposes D-NET as an optimal solution for designers and developers willing to realize aggre...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "152784012",
                    "name": "Michele Artini"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "39565794",
                    "name": "A. Bardi"
                },
                {
                    "authorId": "2043406",
                    "name": "Andrea Mannocci"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "145679761",
                    "name": "Leonardo Candela"
                },
                {
                    "authorId": "1773869",
                    "name": "D. Castelli"
                },
                {
                    "authorId": "1769599",
                    "name": "P. Pagano"
                }
            ]
        },
        {
            "paperId": "f33416fbbcd4851d4c64d69e8cd70727d6d0fa93",
            "title": "High-Performance Annotation Tagging over Solr Full-text Indexes",
            "abstract": "In this work, we focus on the problem of \u201cannotation tagging\u201d over Information\u00a0 Spaces of objects stored in a full-text index. In such a scenario, tags are assigned to objects by \u201cdata curator\u201d users with the purpose of classification, while generic end-users will perceive tags as searchable and browsable object properties. To carry out their activities, data curators need \u201cannotation tagging tools\u201d which allow them to \u201cbulk\u201d tag or untag large sets of objects in temporary work sessions, where they can \u201cvirtually\u201d and in \u201creal-time\u201d experiment the effect of their actions before making the changes visible to end-users. The implementation of these tools over full-text indexes is a challenge, since bulk object updates in this context are far from being real-time and in critical cases may slow down index performance. We devised TagTick, a tool which offers to data curators a fully functional annotation tagging environment over the full-text index Apache Solr, regarded as a \u201cde-facto standard\u201d in this area. TagTick consists of a TagTick Virtualizer module, which extends the APIs of Solr to support real-time, virtual, bulk-tagging operations, and a TagTick User Interface module, which offers end-user functionalities for annotation tagging. The tool scales optimally with the number and size of bulk tag operations, without compromising index performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1799502",
                    "name": "P. Manghi"
                },
                {
                    "authorId": "152784012",
                    "name": "Michele Artini"
                },
                {
                    "authorId": "39565794",
                    "name": "A. Bardi"
                },
                {
                    "authorId": "2589488",
                    "name": "Claudio Atzori"
                },
                {
                    "authorId": "3216767",
                    "name": "Sandro La Bruzzo"
                },
                {
                    "authorId": "2299475",
                    "name": "Marko Mikulicic"
                }
            ]
        }
    ]
}