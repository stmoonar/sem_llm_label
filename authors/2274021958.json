{
    "authorId": "2274021958",
    "papers": [
        {
            "paperId": "2bc0e1b782ec7ff56bcd0d117e553f411c7940b0",
            "title": "Prompt Tuning as User Inherent Profile Inference Machine",
            "abstract": "Large Language Models (LLMs) have exhibited significant promise in recommender systems by empowering user profiles with their extensive world knowledge and superior reasoning capabilities. However, LLMs face challenges like unstable instruction compliance, modality gaps, and high inference latency, leading to textual noise and limiting their effectiveness in recommender systems. To address these challenges, we propose UserIP-Tuning, which uses prompt-tuning to infer user profiles. It integrates the causal relationship between user profiles and behavior sequences into LLMs' prompts. And employs expectation maximization to infer the embedded latent profile, minimizing textual noise by fixing the prompt template. Furthermore, A profile quantization codebook bridges the modality gap by categorizing profile embeddings into collaborative IDs, which are pre-stored for online deployment. This improves time efficiency and reduces memory usage. Experiments on four public datasets show that UserIP-Tuning outperforms state-of-the-art recommendation algorithms. Additional tests and case studies confirm its effectiveness, robustness, and transferability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256815726",
                    "name": "Yusheng Lu"
                },
                {
                    "authorId": "2294672261",
                    "name": "Zhaocheng Du"
                },
                {
                    "authorId": "2181637944",
                    "name": "Xiangyang Li"
                },
                {
                    "authorId": "2238104000",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2130051800",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "2262403807",
                    "name": "Yichao Wang"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2262216857",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2309213607",
                    "name": "Yongrui Duan"
                }
            ]
        },
        {
            "paperId": "3be0aae63b6429680d5c28b794be63149b4b4e39",
            "title": "Retrievable Domain-Sensitive Feature Memory for Multi-Domain Recommendation",
            "abstract": "With the increase in the business scale and number of domains in online advertising, multi-domain ad recommendation has become a mainstream solution in the industry. The core of multi-domain recommendation is effectively modeling the commonalities and distinctions among domains. Existing works are dedicated to designing model architectures for implicit multi-domain modeling while overlooking an in-depth investigation from a more fundamental perspective of feature distributions. This paper focuses on features with significant differences across various domains in both distributions and effects on model predictions. We refer to these features as domain-sensitive features, which serve as carriers of domain distinctions and are crucial for multi-domain modeling. Experiments demonstrate that existing multi-domain modeling methods may neglect domain-sensitive features, indicating insufficient learning of domain distinctions. To avoid this neglect, we propose a domain-sensitive feature attribution method to identify features that best reflect domain distinctions from the feature set. Further, we design a memory architecture that extracts domain-specific information from domain-sensitive features for the model to retrieve and integrate, thereby enhancing the awareness of domain distinctions. Extensive offline and online experiments demonstrate the superiority of our method in capturing domain distinctions and improving multi-domain recommendation performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287812381",
                    "name": "Yuang Zhao"
                },
                {
                    "authorId": "2294672261",
                    "name": "Zhaocheng Du"
                },
                {
                    "authorId": "3789712",
                    "name": "Qinglin Jia"
                },
                {
                    "authorId": "2287839449",
                    "name": "Linxuan Zhang"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2262216857",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "3f46e66675a89f35d3991a85ff0556c1533de4d2",
            "title": "Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey",
            "abstract": "Personalized recommendation serves as a ubiquitous channel for users to discover information tailored to their interests. However, traditional recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms. The recent advancements in large multimodal models offer new opportunities and challenges in developing content-aware recommender systems. This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications in enhancing recommender systems. Furthermore, we discuss current open challenges and opportunities for future research in this dynamic domain. We believe that this survey, alongside the curated resources, will provide valuable insights to inspire further advancements in this evolving landscape.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150270469",
                    "name": "Qijiong Liu"
                },
                {
                    "authorId": "2240695630",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2223871084",
                    "name": "Yanting Yang"
                },
                {
                    "authorId": "29969336",
                    "name": "Quanyu Dai"
                },
                {
                    "authorId": "2294672261",
                    "name": "Zhaocheng Du"
                },
                {
                    "authorId": "2187512110",
                    "name": "Xiao-Ming Wu"
                },
                {
                    "authorId": "2265936086",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2163112250",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                }
            ]
        },
        {
            "paperId": "509f2781fb6871cefec5bfa624a9d0754e3031d7",
            "title": "Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment",
            "abstract": "Recent advances in representation learning have demonstrated the significance of multimodal alignment. The Dual Cross-modal Information Disentanglement (DCID) model, utilizing a unified codebook, shows promising results in achieving fine-grained representation and cross-modal generalization. However, it is still hindered by equal treatment of all channels and neglect of minor event information, resulting in interference from irrelevant channels and limited performance in fine-grained tasks. Thus, in this work, We propose a Training-free Optimization of Codebook (TOC) method to enhance model performance by selecting important channels in the unified space without retraining. Additionally, we introduce the Hierarchical Dual Cross-modal Information Disentanglement (H-DCID) approach to extend information separation and alignment to two levels, capturing more cross-modal details. The experiment results demonstrate significant improvements across various downstream tasks, with TOC contributing to an average improvement of 1.70% for DCID on four tasks, and H-DCID surpassing DCID by an average of 3.64%. The combination of TOC and H-DCID further enhances performance, exceeding DCID by 4.43%. These findings highlight the effectiveness of our methods in facilitating robust and nuanced cross-modal learning, opening avenues for future enhancements. The source code and pre-trained models can be accessed at https://github.com/haihuangcode/TOC_H-DCID.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2288977658",
                    "name": "Hai Huang"
                },
                {
                    "authorId": "2111131270",
                    "name": "Yan Xia"
                },
                {
                    "authorId": "72890649",
                    "name": "Shengpeng Ji"
                },
                {
                    "authorId": "2284727737",
                    "name": "Shulei Wang"
                },
                {
                    "authorId": "2284211766",
                    "name": "Hanting Wang"
                },
                {
                    "authorId": "2265944252",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2265936086",
                    "name": "Zhou Zhao"
                }
            ]
        },
        {
            "paperId": "52dc538cf0f0b979105712ae073ca125602cda25",
            "title": "AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising",
            "abstract": "Click-Through Rate (CTR) prediction is a fundamental technique for online advertising recommendation and the complex online competitive auction process also brings many difficulties to CTR optimization. Recent studies have shown that introducing posterior auction information contributes to the performance of CTR prediction. However, existing work doesn't fully capitalize on the benefits of auction information and overlooks the data bias brought by the auction, leading to biased and suboptimal results. To address these limitations, we propose Auction Information Enhanced Framework (AIE) for CTR prediction in online advertising, which delves into the problem of insufficient utilization of auction signals and first reveals the auction bias. Specifically, AIE introduces two pluggable modules, namely Adaptive Market-price Auxiliary Module (AM2) and Bid Calibration Module (BCM), which work collaboratively to excavate the posterior auction signals better and enhance the performance of CTR prediction. Furthermore, the two proposed modules are lightweight, model-agnostic, and friendly to inference latency. Extensive experiments are conducted on a public dataset and an industrial dataset to demonstrate the effectiveness and compatibility of AIE. Besides, a one-month online A/B test in a large-scale advertising platform shows that AIE improves the base model by 5.76% and 2.44% in terms of eCPM and CTR, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290248265",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2115802321",
                    "name": "Chenxu Zhu"
                },
                {
                    "authorId": "2238203237",
                    "name": "Menghui Zhu"
                },
                {
                    "authorId": "2105646417",
                    "name": "Xinyi Dai"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2287873067",
                    "name": "Muyu Zhang"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2262216857",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "6d9c4f406caff94da5846a9be6749ffe3505bda2",
            "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
            "abstract": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290237904",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2298723722",
                    "name": "Mengqun Jin"
                },
                {
                    "authorId": "150270469",
                    "name": "Qijiong Liu"
                },
                {
                    "authorId": "2298269415",
                    "name": "Zexuan Qiu"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2297866189",
                    "name": "Xiu Li"
                }
            ]
        },
        {
            "paperId": "786abaa17b04c4caed22e820fa0ca4390a56cc1a",
            "title": "CELA: Cost-Efficient Language Model Alignment for CTR Prediction",
            "abstract": "Click-Through Rate (CTR) prediction holds a paramount position in recommender systems. The prevailing ID-based paradigm underperforms in cold-start scenarios due to the skewed distribution of feature frequency. Additionally, the utilization of a single modality fails to exploit the knowledge contained within textual features. Recent efforts have sought to mitigate these challenges by integrating Pre-trained Language Models (PLMs). They design hard prompts to structure raw features into text for each interaction and then apply PLMs for text processing. With external knowledge and reasoning capabilities, PLMs extract valuable information even in cases of sparse interactions. Nevertheless, compared to ID-based models, pure text modeling degrades the efficacy of collaborative filtering, as well as feature scalability and efficiency during both training and inference. To address these issues, we propose \\textbf{C}ost-\\textbf{E}fficient \\textbf{L}anguage Model \\textbf{A}lignment (\\textbf{CELA}) for CTR prediction. CELA incorporates textual features and language models while preserving the collaborative filtering capabilities of ID-based models. This model-agnostic framework can be equipped with plug-and-play textual features, with item-level alignment enhancing the utilization of external information while maintaining training and inference efficiency. Through extensive offline experiments, CELA demonstrates superior performance compared to state-of-the-art methods. Furthermore, an online A/B test conducted on an industrial App recommender system showcases its practical effectiveness, solidifying the potential for real-world applications of CELA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223459253",
                    "name": "Xingmei Wang"
                },
                {
                    "authorId": "2281903918",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "2223761014",
                    "name": "Xiaolong Chen"
                },
                {
                    "authorId": "2267408601",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "2236672137",
                    "name": "Xu Huang"
                },
                {
                    "authorId": "2267335798",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2181637944",
                    "name": "Xiangyang Li"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2262216857",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "8654b611548b09ec49816fc874ccdd8abcee1f40",
            "title": "ACE: A Generative Cross-Modal Retrieval Framework with Coarse-To-Fine Semantic Modeling",
            "abstract": "Generative retrieval, which has demonstrated effectiveness in text-to-text retrieval, utilizes a sequence-to-sequence model to directly generate candidate identifiers based on natural language queries. Without explicitly computing the similarity between queries and candidates, generative retrieval surpasses dual-tower models in both speed and accuracy on large-scale corpora, providing new insights for cross-modal retrieval. However, constructing identifiers for multimodal data remains an untapped problem, and the modality gap between natural language queries and multimodal candidates hinders retrieval performance due to the absence of additional encoders. To this end, we propose a pioneering generAtive Cross-modal rEtrieval framework (ACE), which is a comprehensive framework for end-to-end cross-modal retrieval based on coarse-to-fine semantic modeling. We propose combining K-Means and RQ-VAE to construct coarse and fine tokens, serving as identifiers for multimodal data. Correspondingly, we design the coarse-to-fine feature fusion strategy to efficiently align natural language queries and candidate identifiers. ACE is the first work to comprehensively demonstrate the feasibility of generative approach on text-to-image/audio/video retrieval, challenging the dominance of the embedding-based dual-tower architecture. Extensive experiments show that ACE achieves state-of-the-art performance in cross-modal retrieval and outperforms the strong baselines on Recall@1 by 15.27% on average.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2234355048",
                    "name": "Minghui Fang"
                },
                {
                    "authorId": "72890649",
                    "name": "Shengpeng Ji"
                },
                {
                    "authorId": "2199136449",
                    "name": "Jia-li Zuo"
                },
                {
                    "authorId": "2288977658",
                    "name": "Hai Huang"
                },
                {
                    "authorId": "2111131270",
                    "name": "Yan Xia"
                },
                {
                    "authorId": "2265944252",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2191618494",
                    "name": "Xize Cheng"
                },
                {
                    "authorId": "2308224151",
                    "name": "Xiaoda Yang"
                },
                {
                    "authorId": "2287272576",
                    "name": "Wenrui Liu"
                },
                {
                    "authorId": "2303650284",
                    "name": "Gang Wang"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2265936086",
                    "name": "Zhou Zhao"
                }
            ]
        },
        {
            "paperId": "8a56019401b55c76fac5232193fcce3f2f1af702",
            "title": "LightCS: Selecting Quadratic Feature Crosses in Linear Complexity",
            "abstract": "Feature crosses, which represent joint features synthesized by two single features, are critical for deep recommender systems to model sophisticated feature relations. In practice, only a tiny fraction of feature crosses among massive possible ones are informative, while introducing irrelevant or noisy ones may increase online service latency and boost the risk of overfitting. Therefore, picking high-quality feature crosses is essential in practical recommender systems. However, even for selecting quadratic feature crosses, existing algorithms still incur either o(n2) time complexity or o(n2) space complexity, which is inefficient and unscalable in industrial scenarios. In this paper, we present an efficient and accurate quadratic feature cross selection method with both linear time and space complexity. Motivated by the idea of Quasi-Newton methods, we propose to use 2nd-order derivative matrix to evaluate all theoretically possible feature crosses concurrently without the need of constructing them explicitly, where an approximation of 2nd-order gradient is applied to guarantee both low time and space complexity. Furthermore, we decouple the feature crosses' novelty from single features' joint importance. Experiments on two public recommendation datasets and a private dataset validate the efficiency and effectiveness of our method, and it has also become a fundamental feature cross selection tool used by Huawei Ads Platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2294672261",
                    "name": "Zhaocheng Du"
                },
                {
                    "authorId": "2301263203",
                    "name": "Junhao Chen"
                },
                {
                    "authorId": "3789712",
                    "name": "Qinglin Jia"
                },
                {
                    "authorId": "2287881587",
                    "name": "Chuhan Wu"
                },
                {
                    "authorId": "2265944252",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2262216857",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "9856cfeec94e796b950fd02729d0a5171a164546",
            "title": "Multimodal Pretraining and Generation for Recommendation: A Tutorial",
            "abstract": "Personalized recommendation stands as a ubiquitous channel for users to explore information or items aligned with their interests. Nevertheless, prevailing recommendation models predominantly rely on unique IDs and categorical features for user-item matching. While this ID-centric approach has witnessed considerable success, it falls short in comprehensively grasping the essence of raw item contents across diverse modalities, such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, particularly in the realm of multimedia services like news, music, and short-video platforms. The recent surge in pretraining and generation techniques presents both opportunities and challenges in the development of multimodal recommender systems. This tutorial seeks to provide a thorough exploration of the latest advancements and future trajectories in multimodal pretraining and generation techniques within the realm of recommender systems. The tutorial comprises four talks, addressing multimodal pretraining, multimodal fusion, multimodal generation, and presenting successful stories alongside open challenges in the field of recommendation. Our target audience encompasses scholars, practitioners, and other parties interested in this domain. By providing a succinct overview of the field, we aspire to facilitate a swift understanding of multimodal recommendation and foster meaningful discussions on the future development of this evolving landscape.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240695630",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2257593840",
                    "name": "Xin Zhou"
                },
                {
                    "authorId": "2287881587",
                    "name": "Chuhan Wu"
                },
                {
                    "authorId": "2163112250",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                }
            ]
        }
    ]
}