{
    "authorId": "1804885",
    "papers": [
        {
            "paperId": "4feb412574eb5d0b187276069fe6024c22629c0e",
            "title": "The Calibration Gap between Model and Human Confidence in Large Language Models",
            "abstract": "For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                },
                {
                    "authorId": "2187456239",
                    "name": "Heliodoro Tejeda Lemus"
                },
                {
                    "authorId": "2110096539",
                    "name": "Aakriti Kumar"
                },
                {
                    "authorId": "2281033983",
                    "name": "Catarina Bel\u00e9m"
                },
                {
                    "authorId": "2281033266",
                    "name": "Sheer Karny"
                },
                {
                    "authorId": "2281396285",
                    "name": "Xinyue Hu"
                },
                {
                    "authorId": "2281033339",
                    "name": "L. Mayer"
                },
                {
                    "authorId": "1630595717",
                    "name": "P. Smyth"
                }
            ]
        },
        {
            "paperId": "5882749d7392f14e9583973188dbe87703f95227",
            "title": "Perceptions of Linguistic Uncertainty by Language Models and Humans",
            "abstract": "Uncertainty expressions such as ``probably'' or ``highly unlikely'' are pervasive in human language. While prior work has established that there is population-level agreement in terms of how humans interpret these expressions, there has been little inquiry into the abilities of language models to interpret such expressions. In this paper, we investigate how language models map linguistic expressions of uncertainty to numerical responses. Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertainty of another agent about a particular statement, independently of the model's own certainty about that statement. We evaluate both humans and 10 popular language models on a task created to assess these abilities. Unexpectedly, we find that 8 out of 10 models are able to map uncertainty expressions to probabilistic responses in a human-like manner. However, we observe systematically different behavior depending on whether a statement is actually true or false. This sensitivity indicates that language models are substantially more susceptible to bias based on their prior knowledge (as compared to humans). These findings raise important questions and have broad implications for human-AI alignment and AI-AI communication.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281033983",
                    "name": "Catarina Bel\u00e9m"
                },
                {
                    "authorId": "2312328444",
                    "name": "Markelle Kelly"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                },
                {
                    "authorId": "2313369038",
                    "name": "Sameer Singh"
                },
                {
                    "authorId": "1630595717",
                    "name": "P. Smyth"
                }
            ]
        },
        {
            "paperId": "69dbfb9dc0304073d41ef6c22be34b798b6ae8ba",
            "title": "Help me help you: A computational model for goal inference and action planning",
            "abstract": "Helping is an inherently cooperative behavior, but the cognitive mechanisms underlying this behavior remain relatively underexplored. In this paper, we introduce a novel gamified paradigm for understanding a variety of cognitive behaviors associated with helping. Principals are assigned secret goals in a block-based grid (e.g., move all blue blocks to room C), and helpers can either pass their turn or make a move that could help the principal. We show that principals make useful and pragmatic first moves and helpers accumulate evidence over time before initiating a helpful move. We also introduce a preliminary set of computational models based on recursive pragmatic inference and utility maximization that attempt to account for these behavioral findings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109225081",
                    "name": "A. Kumar"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                }
            ]
        },
        {
            "paperId": "78d456cda93d472400b887daefa7351897b678d7",
            "title": "Bayesian Online Learning for Consensus Prediction",
            "abstract": "Given a pre-trained classifier and multiple human experts, we investigate the task of online classification where model predictions are provided for free but querying humans incurs a cost. In this practical but under-explored setting, oracle ground truth is not available. Instead, the prediction target is defined as the consensus vote of all experts. Given that querying full consensus can be costly, we propose a general framework for online Bayesian consensus estimation, leveraging properties of the multivariate hypergeometric distribution. Based on this framework, we propose a family of methods that dynamically estimate expert consensus from partial feedback by producing a posterior over expert and model beliefs. Analyzing this posterior induces an interpretable trade-off between querying cost and classification performance. We demonstrate the efficacy of our framework against a variety of baselines on CIFAR-10H and ImageNet-16H, two large-scale crowdsourced datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2273926383",
                    "name": "Sam Showalter"
                },
                {
                    "authorId": "2074330244",
                    "name": "Alex Boyd"
                },
                {
                    "authorId": "1630595717",
                    "name": "P. Smyth"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                }
            ]
        },
        {
            "paperId": "8665c34a331fb900d9df42c74692dc229b8ca3f6",
            "title": "When Do Drivers Intervene In Autonomous Driving?",
            "abstract": "Autonomous vehicles (AVs) are expected to handle traffic scenarios more safely and efficiently than human drivers. However, it needs to be better understood which AV decisions are perceived to be unsafe or risky by drivers. To investigate drivers' perceived risk, we conducted a driving simulator experiment where participants are driven around by two types of AVs---car and sidewalk mobility---with a driving style that matches the participant's driving style. We developed a computational model that allows us to examine drivers' perceived risk of scenarios when interacting with an AV based on the drivers' interventions. The model allows us to quantify and compare the relative perceived risk of different scenarios for the two mobility types. Our results indicate that 1) drivers perceived higher risk in scenarios where the AV attempts to match the driver's preferred driving style, and 2) different scenarios were perceived as having higher risk across the two mobility types. The ability to quantify the perceived risk of scenarios and an understanding of how perceived risk differs across mobility types will provide critical insights for the design of human-aware mobility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110096539",
                    "name": "Aakriti Kumar"
                },
                {
                    "authorId": "35052727",
                    "name": "K. Akash"
                },
                {
                    "authorId": "91093379",
                    "name": "Shashank Mehrotra"
                },
                {
                    "authorId": "32639375",
                    "name": "Teruhisa Misu"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                }
            ]
        },
        {
            "paperId": "b5aad38122c14f4287c5feca20b7017fda50d7c0",
            "title": "Hybrid forecasting of geopolitical events\u2020",
            "abstract": "Sound decision-making relies on accurate prediction for tangible outcomes ranging from military conflict to disease outbreaks. To improve crowdsourced forecasting accuracy, we developed SAGE, a hybrid forecasting system that combines human and machine generated forecasts. The system provides a platform where users can interact with machine models and thus anchor their judgments on an objective",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052360522",
                    "name": "Daniel M. Benjamin"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "32494669",
                    "name": "A. Abbas"
                },
                {
                    "authorId": "2919118",
                    "name": "A. Abeliuk"
                },
                {
                    "authorId": "2213032239",
                    "name": "Pavel Atanasov"
                },
                {
                    "authorId": "1667696997",
                    "name": "Stephen T. Bennett"
                },
                {
                    "authorId": "52249013",
                    "name": "Andreas Beger"
                },
                {
                    "authorId": "2088006944",
                    "name": "Saurabh Birari"
                },
                {
                    "authorId": "3275470",
                    "name": "D. Budescu"
                },
                {
                    "authorId": "1754926",
                    "name": "Michele Catasta"
                },
                {
                    "authorId": "48898287",
                    "name": "Emilio Ferrara"
                },
                {
                    "authorId": "2213306048",
                    "name": "Lucas Haravitch"
                },
                {
                    "authorId": "115201201",
                    "name": "Mark Himmelstein"
                },
                {
                    "authorId": "144022002",
                    "name": "K. T. Hossain"
                },
                {
                    "authorId": "35633657",
                    "name": "Yuzhong Huang"
                },
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "51892568",
                    "name": "R. Joseph"
                },
                {
                    "authorId": "1702139",
                    "name": "J. Leskovec"
                },
                {
                    "authorId": "50390828",
                    "name": "Akira Matsui"
                },
                {
                    "authorId": "35416435",
                    "name": "Mehrnoosh Mirtaheri"
                },
                {
                    "authorId": "145201124",
                    "name": "Xiang Ren"
                },
                {
                    "authorId": "1840215",
                    "name": "Gleb Satyukov"
                },
                {
                    "authorId": "31928125",
                    "name": "Rajiv Sethi"
                },
                {
                    "authorId": "2116288277",
                    "name": "Amandeep Singh"
                },
                {
                    "authorId": "48523334",
                    "name": "R. Sosi\u010d"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                },
                {
                    "authorId": "2628881",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "2067815167",
                    "name": "M. D. Ward"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                }
            ]
        },
        {
            "paperId": "f1c526d742428076abe5b1dfd2a6690d9bdf2f75",
            "title": "Capturing Humans\u2019 Mental Models of AI: An Item Response Theory Approach",
            "abstract": "Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate\u2019s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people\u2019s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants\u2019 own self-perception. Our results indicate that people expect AI agents\u2019 performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30777651",
                    "name": "M. Kelly"
                },
                {
                    "authorId": "2110096539",
                    "name": "Aakriti Kumar"
                },
                {
                    "authorId": "50860274",
                    "name": "Padhraic Smyth"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                }
            ]
        },
        {
            "paperId": "092791d7d9bc1fb6a286733641a9debb4b773fcc",
            "title": "The Funny Thing About Algorithm Aversion: Investigating Bias Toward AI Humor",
            "abstract": "Though humans should defer to the superior judgement of AI in an increasing number of domains, certain biases prevent us fromdoing so. Understandingwhen and how these biases occur is a central challenge for human-computer interaction. A proposed source of such bias is the perceived subjectivity of tasks. We tested this hypothesis using one of the most subjective tasks possible: Evaluating joke funniness. Across two experiments, we addressed the following: Would people rate jokes as less funny if they believed an AI created them? When asked to rate jokes and guess their likeliest source, participants evaluated jokes attributed to humans as the funniest and those to AI as the least funny. However, when we explicitly framed these same jokes as either human or AI-created, there was no difference in performance-level ratings. These results challenge the notion that task subjectivity always biases users against AI if the source is transparent.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388287012",
                    "name": "Alexander H. Bower"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                }
            ]
        },
        {
            "paperId": "1d514ecf7b786b3207f713ee161ea4788cd022c1",
            "title": "Explaining Algorithm Aversion with Metacognitive Bandits",
            "abstract": "Human-AI collaboration is an increasingly commonplace part of decision-making in real world applications. However, how humans behave when collaborating with AI is not well understood. We develop metacognitive bandits, a computational model of a human\u2019s advice-seeking behavior when working with an AI. The model describes a person\u2019s metacognitive process of deciding when to rely on their own judgment and when to solicit the advice of the AI. It also accounts for the dif\ufb01culty of each trial in making the decision to solicit advice. We illustrate that the metacognitive bandit makes decisions similar to humans in a behavioral experiment. We also demonstrate that algorithm aversion, a widely reported bias, can be explained as the result of a quasi-optimal sequential decision-making process. Our model does not need to assume any prior biases towards AI to produce this behavior.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110096539",
                    "name": "Aakriti Kumar"
                },
                {
                    "authorId": "47651890",
                    "name": "Trisha N Patel"
                },
                {
                    "authorId": "34734570",
                    "name": "A. Benjamin"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                }
            ]
        },
        {
            "paperId": "42adcc94f4cdb9a169d578974984e714833fe0fa",
            "title": "A Critical Review of Network-Based and Distributional Approaches to Semantic Memory Structure and Processes",
            "abstract": "Some of the earliest work on understanding how concepts are organized in memory used a network-based approach, where words or concepts are represented as nodes, and relationships between words are represented by links between nodes. Over the past two decades, advances in network science and graph theoretical methods have led to the development of computational semantic networks. This review provides a modern perspective on how computational semantic networks have proven to be useful tools to investigate the structure of semantic memory as well as search and retrieval processes within semantic memory, to ultimately model performance in a wide variety of cognitive tasks. Regarding representation, the review focuses on the distinctions and similarities between network-based (based on behavioral norms) approaches and more recent distributional (based on natural language corpora) semantic models, and the potential overlap between the two approaches. Capturing the type of relation between concepts appears to be particularly important in this modeling endeavor. Regarding processes, the review focuses on random walk models and the degree to which retrieval processes demand attention in pursuit of given task goals, which dovetails with the type of relation retrieved during tasks. Ultimately, this review provides a critical assessment of how the network perspective can be reconciled with distributional and machine-learning-based perspectives to meaning representation, and describes how cognitive network science provides a useful conceptual toolkit to probe both the structure and retrieval processes within semantic memory.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2109225081",
                    "name": "A. Kumar"
                },
                {
                    "authorId": "1804885",
                    "name": "M. Steyvers"
                },
                {
                    "authorId": "2103103",
                    "name": "D. Balota"
                }
            ]
        }
    ]
}