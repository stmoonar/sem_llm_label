{
    "authorId": "2055335611",
    "papers": [
        {
            "paperId": "f7daa82747a13314c42a1df0c8eb2d4958ac27d6",
            "title": "Tackling Prevalent Conditions in Unsupervised Combinatorial Optimization: Cardinality, Minimum, Covering, and More",
            "abstract": "Combinatorial optimization (CO) is naturally discrete, making machine learning based on differentiable optimization inapplicable. Karalias&Loukas (2020) adapted the probabilistic method to incorporate CO into differentiable optimization. Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization. However, each component confronts unique challenges. First, deriving objectives under various conditions (e.g., cardinality constraints and minimum) is nontrivial. Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding. In this work, we aim to tackle prevalent (i.e., commonly involved) conditions in unsupervised CO. First, we concretize the targets for objective construction and derandomization with theoretical justification. Then, for various conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets. Finally, we apply the derivations to various CO problems. Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t. both optimization quality and speed.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2065674134",
                    "name": "Fanchen Bu"
                },
                {
                    "authorId": "2055335611",
                    "name": "Hyeonsoo Jo"
                },
                {
                    "authorId": "2218932858",
                    "name": "Soo Yong Lee"
                },
                {
                    "authorId": "2301797085",
                    "name": "Sungsoo Ahn"
                },
                {
                    "authorId": "2242112674",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "5ff04aedb26a0f51a91572c5910583fc4bd189dc",
            "title": "Robust Graph Clustering via Meta Weighting for Noisy Graphs",
            "abstract": "How can we find meaningful clusters in a graph robustly against noise edges? Graph clustering (i.e., dividing nodes into groups of similar ones) is a fundamental problem in graph analysis with applications in various fields. Recent studies have demonstrated that graph neural network (GNN) based approaches yield promising results for graph clustering. However, we observe that their performance degenerates significantly on graphs with noise edges, which are prevalent in practice. In this work, we propose MetaGC for robust GNN-based graph clustering. MetaGC employs a decomposable clustering loss function, which can be rephrased as a sum of losses over node pairs. We add a learnable weight to each node pair, and MetaGC adaptively adjusts the weights of node pairs using meta-weighting so that the weights of meaningful node pairs increase and the weights of less-meaningful ones (e.g., noise edges) decrease. We show empirically that MetaGC learns weights as intended and consequently outperforms the state-of-the-art GNN-based competitors, even when they are equipped with separate denoising schemes, on five real-world graphs under varying levels of noise. Our code and datasets are available at https://github.com/HyeonsooJo/MetaGC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055335611",
                    "name": "Hyeonsoo Jo"
                },
                {
                    "authorId": "2065674134",
                    "name": "Fanchen Bu"
                },
                {
                    "authorId": "40553270",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "50e107ed7026084db2da06e9f7a74e6508372944",
            "title": "SSumM: Sparse Summarization of Massive Graphs",
            "abstract": "Given a graph G and the desired size k in bits, how can we summarize G within k bits, while minimizing the information loss? Large-scale graphs have become omnipresent, posing considerable computational challenges. Analyzing such large graphs can be fast and easy if they are compressed sufficiently to fit in main memory or even cache. Graph summarization, which yields a coarse-grained summary graph with merged nodes, stands out with several advantages among graph compression techniques. Thus, a number of algorithms have been developed for obtaining a concise summary graph with little information loss or equivalently small reconstruction error. However, the existing methods focus solely on reducing the number of nodes, and they often yield dense summary graphs, failing to achieve better compression rates. Moreover, due to their limited scalability, they can be applied only to moderate-size graphs. In this work, we propose SSumM, a scalable and effective graph-summarization algorithm that yields a sparse summary graph. SSumM not only merges nodes together but also sparsifies the summary graph, and the two strategies are carefully balanced based on the minimum description length principle. Compared with state-of-the-art competitors, SSumM is (a) Concise: yields up to 11.2X smaller summary graphs with similar reconstruction error, (b) Accurate: achieves up to 4.2X smaller reconstruction error with similarly concise outputs, and (c) Scalable: summarizes 26X larger graphs while exhibiting linear scalability. We validate these advantages through extensive experiments on 10 real-world graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40248036",
                    "name": "Kyuhan Lee"
                },
                {
                    "authorId": "2055335611",
                    "name": "Hyeonsoo Jo"
                },
                {
                    "authorId": "2072690914",
                    "name": "Jihoon Ko"
                },
                {
                    "authorId": "1471444633",
                    "name": "Sungsu Lim"
                },
                {
                    "authorId": "40553270",
                    "name": "Kijung Shin"
                }
            ]
        },
        {
            "paperId": "d125c624dfaa833c7303c0001b8168c5ec9a50c2",
            "title": "Basis Learning Autoencoders for Hybrid Collaborative Filtering in Cold Start Setting",
            "abstract": "In recent years, most recommender systems rely on collaborative filtering (CF) based on matrix factorization (MF) that can predict unknown ratings by completing a rating matrix. However, this approach cannot be used for the cold start where no rating information is available for a given user or item. To address this problem, we develop a new hybrid CF (HCF) technique incorporating CF with content information. The proposed HCF is based on an auto-encoder (AE) consisting of a nonlinear encoder and a linear decoder. This type of AE is called the basis learning AE (BAE), because it can learn the basis of the row space of a sparse input matrix by its encoder. In the proposed scheme, the input to the BAE is a content augmented rating matrix; the BAE learns the basis of the row space of a given rating matrix, which is a subset of the basis of the content augmented rating matrix, and recovers each row of the rating matrix by a linear combination of the learned basis. Unlike most existing HCF schemes, our model does not incorporate additional content-based objective terms; yet extensive experiments on real-world datasets show that the proposed HCF can significantly advance the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49151881",
                    "name": "Kiwon Lee"
                },
                {
                    "authorId": "2055335611",
                    "name": "Hyeonsoo Jo"
                },
                {
                    "authorId": "8879331",
                    "name": "Hyoji Kim"
                },
                {
                    "authorId": "2146305539",
                    "name": "Yong H. Lee"
                }
            ]
        }
    ]
}