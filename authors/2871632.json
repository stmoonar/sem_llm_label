{
    "authorId": "2871632",
    "papers": [
        {
            "paperId": "68719e9df3d4821a1afedf9c18b0148e51800f71",
            "title": "Inference Optimization of Foundation Models on AI Accelerators",
            "abstract": "Powerful foundation models, including large language models (LLMs), with Transformer architectures have ushered in a new era of Generative AI across various industries. Industry and research community have witnessed a large number of new applications, based on those foundation models. Such applications include question and answer, customer services, image and video generation, and code completions, among others. However, as the number of model parameters reaches to hundreds of billions, their deployment incurs prohibitive inference costs and high latency in real-world scenarios. As a result, the demand for cost-effective and fast inference using AI accelerators is ever more higher. To this end, our tutorial offers a comprehensive discussion on complementary inference optimization techniques using AI accelerators. Beginning with an overview of basic Transformer architectures and deep learning system frameworks, we deep dive into system optimization techniques for fast and memory-efficient attention computations and discuss how they can be implemented efficiently on AI accelerators. Next, we describe architectural elements that are key for fast transformer inference. Finally, we examine various model compression and fast decoding strategies in the same context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2296621337",
                    "name": "Youngsuk Park"
                },
                {
                    "authorId": "3207499",
                    "name": "Kailash Budhathoki"
                },
                {
                    "authorId": "2311506033",
                    "name": "Liangfu Chen"
                },
                {
                    "authorId": "2316130157",
                    "name": "Jonas M. K\u00fcbler"
                },
                {
                    "authorId": "2311301305",
                    "name": "Jiaji Huang"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "2296789669",
                    "name": "Jun Huan"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "2296947641",
                    "name": "Yida Wang"
                },
                {
                    "authorId": "2064547804",
                    "name": "George Karypis"
                }
            ]
        },
        {
            "paperId": "535c0642aa2b5178317f082d04f1eb6a37c86bc9",
            "title": "Evaluating the Fairness of Discriminative Foundation Models in Computer Vision",
            "abstract": "We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI\u2019s CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes over ten diverse datasets. We find that fair PCA, a post-processing method for fair representations, works very well for debiasing in most of the aforementioned tasks while incurring only minor loss of performance. However, different debiasing approaches vary in their effectiveness depending on the task. Hence, one should choose the debiasing approach depending on the specific use case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064705296",
                    "name": "Junaid Ali"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "39798982",
                    "name": "F. Wenzel"
                },
                {
                    "authorId": "3207499",
                    "name": "Kailash Budhathoki"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "9b50feef27bb914b3b5a414479aed5f634809a7b",
            "title": "Efficient fair PCA for fair representation learning",
            "abstract": "We revisit the problem of fair principal component analysis (PCA), where the goal is to learn the best low-rank linear approximation of the data that obfuscates demographic information. We propose a conceptually simple approach that allows for an analytic solution similar to standard PCA and can be kernelized. Our methods have the same complexity as standard PCA, or kernel PCA, and run much faster than existing methods for fair PCA based on semidefinite programming or manifold optimization, while achieving similar results.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "2192704",
                    "name": "Michele Donini"
                },
                {
                    "authorId": "145485799",
                    "name": "Chris Russell"
                },
                {
                    "authorId": "2625318",
                    "name": "M. B. Zafar"
                }
            ]
        },
        {
            "paperId": "ed0bf9fe699a6f426f18ea92fe96e368c3d95888",
            "title": "When do Minimax-fair Learning and Empirical Risk Minimization Coincide?",
            "abstract": "Minimax-fair machine learning minimizes the error for the worst-off group. However, empirical evidence suggests that when sophisticated models are trained with standard empirical risk minimization (ERM), they often have the same performance on the worst-off group as a minimax-trained model. Our work makes this counter-intuitive observation concrete. We prove that if the hypothesis class is sufficiently expressive and the group information is recoverable from the features, ERM and minimax-fairness learning formulations indeed have the same performance on the worst-off group. We provide additional empirical evidence of how this observation holds on a wide range of datasets and hypothesis classes. Since ERM is fundamentally easier than minimax optimization, our findings have implications on the practice of fair machine learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "3144230",
                    "name": "R. Chunara"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "06ed4b4bf0ba037b94c67c8233746cdc33dae388",
            "title": "Measuring Fairness of Rankings under Noisy Sensitive Information",
            "abstract": "Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2043975",
                    "name": "Azin Ghazimatin"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                },
                {
                    "authorId": "2034349",
                    "name": "Ziawasch Abedjan"
                },
                {
                    "authorId": "1411129857",
                    "name": "Jacek Golebiowski"
                }
            ]
        },
        {
            "paperId": "1a00b048b2c844d19aaf0d87f78d28cb96be05c8",
            "title": "Individual Preference Stability for Clustering",
            "abstract": "In this paper, we propose a natural notion of individual preference (IP) stability for clustering, which asks that every data point, on average, is closer to the points in its own cluster than to the points in any other cluster. Our notion can be motivated from several perspectives, including game theory and algorithmic fairness. We study several questions related to our proposed notion. We \ufb01rst show that deciding whether a given data set allows for an IP-stable clustering in general is NP-hard. As a result, we explore the design of ef\ufb01cient algorithms for \ufb01nding IP-stable clusterings in some restricted metric spaces. We present a polytime algorithm to \ufb01nd a clustering satisfying exact IP-stability on the real line, and an ef\ufb01cient algorithm to \ufb01nd an IP-stable 2-clustering for a tree metric. We also consider relaxing the stability constraint, i.e., every data point should not be too far from its own cluster compared to any other cluster. For this case, we provide polytime algorithms with different guarantees. We evaluate some of our algorithms and several standard clustering approaches on real data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Saba Ahmadi"
                },
                {
                    "authorId": "144030228",
                    "name": "Pranjal Awasthi"
                },
                {
                    "authorId": "1737147",
                    "name": "S. Khuller"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "144848816",
                    "name": "Jamie Morgenstern"
                },
                {
                    "authorId": "9559852",
                    "name": "Pattara Sukprasert"
                },
                {
                    "authorId": "2347383",
                    "name": "A. Vakilian"
                }
            ]
        },
        {
            "paperId": "1b3142ee576017e5aa34aac94c658f948b75dbcd",
            "title": "Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers",
            "abstract": "Algorithmic fairness is frequently motivated in terms of a trade-off in which overall performance is decreased so as to improve performance on disadvantaged groups where the algorithm would otherwise be less accurate. Contrary to this, we find that applying existing fairness approaches to computer vision improve fairness by degrading the performance of classifiers across all groups (with increased degradation on the best performing groups). Extending the bias-variance decomposition for classification to fairness, we theoretically explain why the majority of fairness methods designed for low capacity models should not be used in settings involving high-capacity models, a scenario common to computer vision. We corroborate this analysis with extensive experimental support that shows that many of the fairness heuristics used in computer vision also degrade performance on the most disadvantaged groups. Building on these insights, we propose an adaptive augmentation strategy that, uniquely, of all methods tested, improves performance for the disadvantaged groups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52306341",
                    "name": "Dominik Zietlow"
                },
                {
                    "authorId": "147521198",
                    "name": "Michael Lohaus"
                },
                {
                    "authorId": "47231927",
                    "name": "Guha Balakrishnan"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "9557137",
                    "name": "Francesco Locatello"
                },
                {
                    "authorId": "1707625",
                    "name": "B. Scholkopf"
                },
                {
                    "authorId": "145485799",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "74270eb578c75d5bdd9c89b047c9f211bd7e15be",
            "title": "Are Two Heads the Same as One? Identifying Disparate Treatment in Fair Neural Networks",
            "abstract": "We show that deep networks trained to satisfy demographic parity often do so through a form of race or gender awareness, and that the more we force a network to be fair, the more accurately we can recover race or gender from the internal state of the network. Based on this observation, we investigate an alternative fairness approach: we add a second classification head to the network to explicitly predict the protected attribute (such as race or gender) alongside the original task. After training the two-headed network, we enforce demographic parity by merging the two heads, creating a network with the same architecture as the original network. We establish a close relationship between existing approaches and our approach by showing (1) that the decisions of a fair classifier are well-approximated by our approach, and (2) that an unfair and optimally accurate classifier can be recovered from a fair classifier and our second head predicting the protected attribute. We use our explicit formulation to argue that the existing fairness approaches, just as ours, demonstrate disparate treatment and that they are likely to be unlawful in a wide range of scenarios under US law.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "147521198",
                    "name": "Michael Lohaus"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "9557137",
                    "name": "Francesco Locatello"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "ce4631ab1fdcaf6320b5e05039fd5ba4535302b7",
            "title": "Score matching enables causal discovery of nonlinear additive noise models",
            "abstract": "This paper demonstrates how to recover causal graphs from the score of the data distribution in non-linear additive (Gaussian) noise models. Using score matching algorithms as a building block, we show how to design a new generation of scalable causal discovery methods. To showcase our approach, we also propose a new efficient method for approximating the score's Jacobian, enabling to recover the causal graph. Empirically, we find that the new algorithm, called SCORE, is competitive with state-of-the-art causal discovery methods while being significantly faster.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1388317622",
                    "name": "Paul Rolland"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "2158168214",
                    "name": "Chris Russel"
                },
                {
                    "authorId": "1707625",
                    "name": "B. Scholkopf"
                },
                {
                    "authorId": "1700657",
                    "name": "D. Janzing"
                },
                {
                    "authorId": "9557137",
                    "name": "Francesco Locatello"
                }
            ]
        },
        {
            "paperId": "86306923f1a696ec657e196dbfbf66aad24f69d8",
            "title": "Backward-Compatible Prediction Updates: A Probabilistic Approach",
            "abstract": "When machine learning systems meet real world applications, accuracy is only one of several requirements. In this paper, we assay a complementary perspective originating from the increasing availability of pre-trained and regularly improving state-of-the-art models. While new improved models develop at a fast pace, downstream tasks vary more slowly or stay constant. Assume that we have a large unlabelled data set for which we want to maintain accurate predictions. Whenever a new and presumably better ML models becomes available, we encounter two problems: (i) given a limited budget, which data points should be re-evaluated using the new model?; and (ii) if the new predictions differ from the current ones, should we update? Problem (i) is about compute cost, which matters for very large data sets and models. Problem (ii) is about maintaining consistency of the predictions, which can be highly relevant for downstream applications; our demand is to avoid negative flips, i.e., changing correct to incorrect predictions. In this paper, we formalize the Prediction Update Problem and present an efficient probabilistic approach as answer to the above questions. In extensive experiments on standard classification benchmark data sets, we show that our method outperforms alternative strategies along key metrics for backward-compatible prediction updates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72803733",
                    "name": "F. Tr\u00e4uble"
                },
                {
                    "authorId": "51135567",
                    "name": "Julius von K\u00fcgelgen"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "9557137",
                    "name": "Francesco Locatello"
                },
                {
                    "authorId": "1707625",
                    "name": "B. Scholkopf"
                },
                {
                    "authorId": "2871555",
                    "name": "Peter Gehler"
                }
            ]
        }
    ]
}