{
    "authorId": "121243468",
    "papers": [
        {
            "paperId": "2781182a38b6303dc5b4cb224579e9a365156bc3",
            "title": "LATEX-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes",
            "abstract": "In this paper, we present LATEX-Numeric - a high-precision fully-automated scalable framework for extracting E-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of active learning. We rely on distant supervision for training data generation, removing dependency on manual labels. One issue with distant supervision is that it leads to incomplete training annotation due to missing attribute values while matching. We propose a multi-task learning architecture to deal with missing labels in the training data, leading to F1 improvement of 9.2% for numeric attributes over state-of-the-art single-task architecture. While multi-task architecture benefits both numeric and non-numeric attributes, we present automated techniques to further improve the numeric attributes extraction models. Numeric attributes require a list of units (or aliases) for better matching with distant supervision. We propose an automated algorithm for alias creation using unstructured text and attribute values, leading to a 20.2% F1 improvement. Extensive experiments on real world datasets for 20 numeric attributes across 5 product categories and 3 English marketplaces show that LATEX-numeric achieves a high F1-score, without any manual intervention, making it suitable for practical applications. Finally we show that the improvements are language-agnostic and LATEX-Numeric achieves 13.9% F1 improvement for 3 non-English languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054297550",
                    "name": "Kartik Mehta"
                },
                {
                    "authorId": "145004446",
                    "name": "I. Oprea"
                },
                {
                    "authorId": "121243468",
                    "name": "Nikhil Rasiwasia"
                }
            ]
        },
        {
            "paperId": "678ee0571346b27692bd82889d1d38fc7250ab64",
            "title": "Scalable Approach for Normalizing E-commerce Text Attributes (SANTA)",
            "abstract": "In this paper, we present SANTA, a scalable framework to automatically normalize E-commerce attribute values (e.g. \u201cWin 10 Pro\u201d) to a fixed set of pre-defined canonical values (e.g. \u201cWindows 10\u201d). Earlier works on attribute normalization focused on fuzzy string matching (also referred as syntactic matching in this paper). In this work, we first perform an extensive study of nine syntactic matching algorithms and establish that \u2018cosine\u2019 similarity leads to best results, showing 2.7% improvement over commonly used Jaccard index. Next, we show that string similarity alone is not sufficient for attribute normalization as many surface forms require going beyond syntactic matching (e.g. \u201c720p\u201d and \u201cHD\u201d are synonyms). While semantic techniques like unsupervised embeddings (e.g. word2vec/fastText) have shown good results in word similarity tasks, we observed that they perform poorly to distinguish between close canonical forms, as these close forms often occur in similar contexts. We propose to learn token embeddings using a twin network with triplet loss. We propose an embedding learning task leveraging raw attribute values and product titles to learn these embeddings in a self-supervised fashion. We show that providing supervision using our proposed task improves over both syntactic and unsupervised embeddings based techniques for attribute normalization. Experiments on a real-world dataset of 50 attributes show that the embeddings trained using our proposed approach obtain 2.3% improvement over best string similarity and 19.3% improvement over best unsupervised embeddings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2105822749",
                    "name": "Ravi Shankar Mishra"
                },
                {
                    "authorId": "2054297550",
                    "name": "Kartik Mehta"
                },
                {
                    "authorId": "121243468",
                    "name": "Nikhil Rasiwasia"
                }
            ]
        },
        {
            "paperId": "86cfd74601cd05aa954817aa14aa4d6c2d47a566",
            "title": "Improving Answer Selection and Answer Triggering using Hard Negatives",
            "abstract": "In this paper, we establish the effectiveness of using hard negatives, coupled with a siamese network and a suitable loss function, for the tasks of answer selection and answer triggering. We show that the choice of sampling strategy is key for achieving improved performance on these tasks. Evaluating on recent answer selection datasets - InsuranceQA, SelQA, and an internal QA dataset, we show that using hard negatives with relatively simple model architectures (bag of words and LSTM-CNN) drives significant performance gains. On InsuranceQA, this strategy alone improves over previously reported results by a minimum of 1.6 points in P@1. Using hard negatives with a Transformer encoder provides a further improvement of 2.3 points. Further, we propose to use quadruplet loss for answer triggering, with the aim of producing globally meaningful similarity scores. We show that quadruplet loss function coupled with the selection of hard negatives enables bag-of-words models to improve F1 score by 2.3 points over previous baselines, on SelQA answer triggering dataset. Our results provide key insights into answer selection and answer triggering tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36212180",
                    "name": "Sawan Kumar"
                },
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "2054297550",
                    "name": "Kartik Mehta"
                },
                {
                    "authorId": "121243468",
                    "name": "Nikhil Rasiwasia"
                }
            ]
        },
        {
            "paperId": "ea7d106575a5f78db7a7b079179f57928324ded1",
            "title": "Perspectives on Becoming an Applied Machine Learning Scientist",
            "abstract": "While becoming a scientist is all about creating a dent in the boundaries of human knowledge, applying that expertise in a corporate setting requires a transformation of the scientist skill set. In both academia and industry, the primary focus is problem solving. In academia, however, it is targeted toward creating a novel solution that pushes the state of the art, while, in industry, the solution is valuable if it leads to the desired change in business metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "121243468",
                    "name": "Nikhil Rasiwasia"
                }
            ]
        }
    ]
}