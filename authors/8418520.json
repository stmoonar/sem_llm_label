{
    "authorId": "8418520",
    "papers": [
        {
            "paperId": "b5e1569e368c560a50933baf5c3adcae5a02ff88",
            "title": "The Sillwood Technologies System for the VoiceMOS Challenge 2022",
            "abstract": "In this paper we describe our entry for the VoiceMOS Challenge 2022 for both the main and out-of-domain (OOD) track of the competition. Our system is based on finetuning pre-trained self-supervised waveform prediction models, while improving its generalisation ability through stochastic weight averaging. Further, we use influence functions to identity possible low-quality data within the training set to further increase our model's performance for the OOD track. Our system ranked 5th and joint 7th for the main track and OOD track, respectively.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                }
            ]
        },
        {
            "paperId": "2b0cd6eb6a0c0031a9c19af66c4acf02c7e4de28",
            "title": "Identification Method of Wheat Cultivars by Using a Convolutional Neural Network Combined with Images of Multiple Growth Periods of Wheat",
            "abstract": "Wheat is a very important food crop for mankind. Many new varieties are bred every year. The accurate judgment of wheat varieties can promote the development of the wheat industry and the protection of breeding property rights. Although gene analysis technology can be used to accurately determine wheat varieties, it is costly, time-consuming, and inconvenient. Traditional machine learning methods can significantly reduce the cost and time of wheat cultivars identification, but the accuracy is not high. In recent years, the relatively popular deep learning methods have further improved the accuracy on the basis of traditional machine learning, whereas it is quite difficult to continue to improve the identification accuracy after the convergence of the deep learning model. Based on the ResNet and SENet models, this paper draws on the idea of the bagging-based ensemble estimator algorithm, and proposes a deep learning model for wheat classification, CMPNet, which is coupled with the tillering period, flowering period, and seed image. This convolutional neural network (CNN) model has a symmetrical structure along the direction of the tensor flow. The model uses collected images of different types of wheat in multiple growth periods. First, it uses the transfer learning method of the ResNet-50, SE-ResNet, and SE-ResNeXt models, and then trains the collected images of 30 kinds of wheat in different growth periods. It then uses the concat layer to connect the output layers of the three models, and finally obtains the wheat classification results through the softmax function. The accuracy of wheat variety identification increased from 92.07% at the seed stage, 95.16% at the tillering stage, and 97.38% at the flowering stage to 99.51%. The model\u2019s single inference time was only 0.0212 s. The model not only significantly improves the classification accuracy of wheat varieties, but also achieves low cost and high efficiency, which makes it a novel and important technology reference for wheat producers, managers, and law enforcement supervisors in the practice of wheat production.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                },
                {
                    "authorId": "2155881055",
                    "name": "Chengzhong Liu"
                },
                {
                    "authorId": "5967453",
                    "name": "Junying Han"
                },
                {
                    "authorId": "2117522626",
                    "name": "Qinglin Lu"
                },
                {
                    "authorId": "2155262641",
                    "name": "Hengxing Wang"
                },
                {
                    "authorId": "2144214894",
                    "name": "Jianhua Zhang"
                },
                {
                    "authorId": "2153645508",
                    "name": "Xuguang Bai"
                },
                {
                    "authorId": "2157965211",
                    "name": "Jianzhong Luo"
                }
            ]
        },
        {
            "paperId": "55e666881f2b51ba94ab504a0131707da4a21ed2",
            "title": "Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis",
            "abstract": "Text does not fully specify the spoken form, so text-to-speech models must be able to learn from speech data that vary in ways not explained by the corresponding text. One way to reduce the amount of unexplained variation in training data is to provide acoustic information as an additional learning signal. When generating speech, modifying this acoustic information enables multiple distinct renditions of a text to be produced. Since much of the unexplained variation is in the prosody, we propose a model that generates speech explicitly conditioned on the three primary acoustic correlates of prosody: $F_{0}$, energy and duration. The model is flexible about how the values of these features are specified: they can be externally provided, or predicted from text, or predicted then subsequently modified. Compared to a model that employs a variational auto-encoder to learn unsupervised latent features, our model provides more interpretable, temporally-precise, and disentangled control. When automatically predicting the acoustic features from text, it generates speech that is more natural than that from a Tacotron 2 model with reference encoder. Subsequent human-in-the-loop modification of the predicted acoustic features can significantly further increase naturalness.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "3330285",
                    "name": "Qinmin Hu"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "23143903",
                    "name": "C. Wallis"
                },
                {
                    "authorId": "51251744",
                    "name": "Marlene Staib"
                },
                {
                    "authorId": "1863936190",
                    "name": "Lorenzo Foglianti"
                },
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "5ca6643b4dde05a51035eb8f91d9a2dfc5964203",
            "title": "ADEPT: A Dataset for Evaluating Prosody Transfer",
            "abstract": "Text-to-speech is now able to achieve near-human naturalness and research focus has shifted to increasing expressivity. One popular method is to transfer the prosody from a reference speech sample. There have been considerable advances in using prosody transfer to generate more expressive speech, but the field lacks a clear definition of what successful prosody transfer means and a method for measuring it. We introduce a dataset of prosodically-varied reference natural speech samples for evaluating prosody transfer. The samples include global variations reflecting emotion and interpersonal attitude, and local variations reflecting topical emphasis, propositional attitude, syntactic phrasing and marked tonicity. The corpus only includes prosodic variations that listeners are able to distinguish with reasonable accuracy, and we report these figures as a benchmark against which text-to-speech prosody transfer can be compared. We conclude the paper with a demonstration of our proposed evaluation methodology, using the corpus to evaluate two text-to-speech models that perform prosody transfer.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "40445010",
                    "name": "C. Wallis"
                },
                {
                    "authorId": "51251744",
                    "name": "Marlene Staib"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "50161308",
                    "name": "Vivian Hu"
                },
                {
                    "authorId": "1863936190",
                    "name": "Lorenzo Foglianti"
                },
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "6a662878e7396f2507eecbe95fbb6162d0bf12ac",
            "title": "Phonological Features for 0-shot Multilingual Speech Synthesis",
            "abstract": "Code-switching---the intra-utterance use of multiple languages---is prevalent across the world. Within text-to-speech (TTS), multilingual models have been found to enable code-switching. By modifying the linguistic input to sequence-to-sequence TTS, we show that code-switching is possible for languages unseen during training, even within monolingual models. We use a small set of phonological features derived from the International Phonetic Alphabet (IPA), such as vowel height and frontness, consonant place and manner. This allows the model topology to stay unchanged for different languages, and enables new, previously unseen feature combinations to be interpreted by the model. We show that this allows us to generate intelligible, code-switched speech in a new language at test time, including the approximation of sounds never seen in training.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "51251744",
                    "name": "Marlene Staib"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "1863936190",
                    "name": "Lorenzo Foglianti"
                },
                {
                    "authorId": "1403266846",
                    "name": "R. Lenain"
                },
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                }
            ]
        },
        {
            "paperId": "c316f7012cc6a459cc6044b7ac384e3c41739fc3",
            "title": "Incremental Text to Speech for Neural Sequence-to-Sequence Models using Reinforcement Learning",
            "abstract": "Modern approaches to text to speech require the entire input character sequence to be processed before any audio is synthesised. This latency limits the suitability of such models for time-sensitive tasks like simultaneous interpretation. Interleaving the action of reading a character with that of synthesising audio reduces this latency. However, the order of this sequence of interleaved actions varies across sentences, which raises the question of how the actions should be chosen. We propose a reinforcement learning based framework to train an agent to make this decision. We compare our performance against that of deterministic, rule-based systems. Our results demonstrate that our agent successfully balances the trade-off between the latency of audio generation and the quality of synthesised audio. More broadly, we show that neural sequence-to-sequence models can be adapted to run in an incremental manner.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "1403266846",
                    "name": "R. Lenain"
                },
                {
                    "authorId": "1863936190",
                    "name": "Lorenzo Foglianti"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "51251744",
                    "name": "Marlene Staib"
                },
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                }
            ]
        }
    ]
}