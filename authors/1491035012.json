{
    "authorId": "1491035012",
    "papers": [
        {
            "paperId": "29632ece419ac9537d8b3caadfa9edd3812392a2",
            "title": "Knowledge-Enhanced Causal Reinforcement Learning Model for Interactive Recommendation",
            "abstract": "Owing to its inherently dynamic nature and economical training cost, offline reinforcement learning (RL) is typically employed to implement an interactive recommender system (IRS). A crucial challenge in offline RL-based IRSs is the data sparsity issue, i.e., it is hard to mine user preferences well from the limited number of user-item interactions. In this article, we propose a knowledge-enhanced causal reinforcement learning model (KCRL) to mitigate data sparsity in IRSs. We make technical extensions to the offline RL framework in terms of the reward function and state representation. Specifically, we first propose a group preference-injected causal user model (GCUM) to learn user satisfaction (i.e., reward) estimation. We introduce beneficial group preference information, namely, the group effect, via causal inference to compensate for incomplete user interests extracted from sparse data. Then, we learn the RL recommendation policy with the reward given by the GCUM. We propose a knowledge-enhanced state encoder (KSE) to generate knowledge-enriched user state representations at each time step, which is assisted by a self-constructed user-item knowledge graph. Extensive experimental results on real-world datasets demonstrate that our model significantly outperforms the baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144536249",
                    "name": "Weizhi Nie"
                },
                {
                    "authorId": "47631470",
                    "name": "Xin-ling Wen"
                },
                {
                    "authorId": "46701354",
                    "name": "J. Liu"
                },
                {
                    "authorId": "1452347263",
                    "name": "Jiawei Chen"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2071128650",
                    "name": "Guoqing Jin"
                },
                {
                    "authorId": "2217839403",
                    "name": "Jing Lu"
                },
                {
                    "authorId": "2118791811",
                    "name": "Anjin Liu"
                }
            ]
        },
        {
            "paperId": "2cc453a9fe20f27f99228ed21cd0b8cbfdb880d9",
            "title": "Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation",
            "abstract": "In the realm of deep learning-based recommendation systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment. This challenge is primarily twofold: reducing the model size while effectively learning user and item representations for efficient recommendations. Despite considerable advancements in model compression and architecture search, prevalent approaches face notable constraints. These include substantial additional computational costs from pre-training/re-training in model compression and an extensive search space in architecture design. Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations. Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for recommendation models. DSL innovatively trains a lightweight sparse model from scratch, periodically evaluating and dynamically adjusting each weight's significance and the model's sparsity distribution during the training. This approach ensures a consistent and minimal parameter budget throughout the full learning lifecycle, paving the way for \"end-to-end\" efficiency from training to inference. Our extensive experimental results underline DSL's effectiveness, significantly reducing training and inference costs while delivering comparable recommendation performance. We give an code link of our work: https://github.com/shuyao-wang/DSL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186629043",
                    "name": "Shuyao Wang"
                },
                {
                    "authorId": "2003767516",
                    "name": "Yongduo Sui"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2282146801",
                    "name": "Zhi Zheng"
                },
                {
                    "authorId": "2282527818",
                    "name": "Hui Xiong"
                }
            ]
        },
        {
            "paperId": "5017c488281fe88dcd6c980791789232ae2315b9",
            "title": "Enhancing Out-of-distribution Generalization on Graphs via Causal Attention Learning",
            "abstract": "In graph classification, attention- and pooling-based graph neural networks (GNNs) predominate to extract salient features from the input graph and support the prediction. They mostly follow the paradigm of \u201clearning to attend,\u201d which maximizes the mutual information between the attended graph and the ground-truth label. However, this paradigm causes GNN classifiers to indiscriminately absorb all statistical correlations between input features and labels in the training data without distinguishing the causal and noncausal effects of features. Rather than emphasizing causal features, the attended graphs tend to rely on noncausal features as shortcuts to predictions. These shortcut features may easily change outside the training distribution, thereby leading to poor generalization for GNN classifiers. In this article, we take a causal view on GNN modeling. Under our causal assumption, the shortcut feature serves as a confounder between the causal feature and prediction. It misleads the classifier into learning spurious correlations that facilitate prediction in in-distribution (ID) test evaluation while causing significant performance drop in out-of-distribution (OOD) test data. To address this issue, we employ the backdoor adjustment from causal theory\u2014combining each causal feature with various shortcut features, to identify causal patterns and mitigate the confounding effect. Specifically, we employ attention modules to estimate the causal and shortcut features of the input graph. Then, a memory bank collects the estimated shortcut features, enhancing the diversity of shortcut features for combination. Simultaneously, we apply the prototype strategy to improve the consistency of intra-class causal features. We term our method as CAL+, which can promote stable relationships between causal estimation and prediction, regardless of distribution changes. Extensive experiments on synthetic and real-world OOD benchmarks demonstrate our method\u2019s effectiveness in improving OOD generalization. Our codes are released at https://github.com/shuyao-wang/CAL-plus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2003767516",
                    "name": "Yongduo Sui"
                },
                {
                    "authorId": "2283170413",
                    "name": "Wenyu Mao"
                },
                {
                    "authorId": "2186629043",
                    "name": "Shuyao Wang"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                },
                {
                    "authorId": "2257036129",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "54ed2db02ab448e888a037b6ca2e31e1b8a5e832",
            "title": "Customizing Language Models with Instance-wise LoRA for Sequential Recommendation",
            "abstract": "Sequential recommendation systems predict a user's next item of interest by analyzing past interactions, aligning recommendations with individual preferences. Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches have applied LLMs to sequential recommendation through language generation paradigms. These methods convert user behavior sequences into prompts for LLM fine-tuning, utilizing Low-Rank Adaptation (LoRA) modules to refine recommendations. However, the uniform application of LoRA across diverse user behaviors sometimes fails to capture individual variability, leading to suboptimal performance and negative transfer between disparate sequences. To address these challenges, we propose Instance-wise LoRA (iLoRA), integrating LoRA with the Mixture of Experts (MoE) framework. iLoRA creates a diverse array of experts, each capturing specific aspects of user preferences, and introduces a sequence representation guided gate function. This gate function processes historical interaction sequences to generate enriched representations, guiding the gating network to output customized expert participation weights. This tailored approach mitigates negative transfer and dynamically adjusts to diverse behavior patterns. Extensive experiments on three benchmark datasets demonstrate the effectiveness of iLoRA, highlighting its superior performance compared to existing methods in capturing user-specific preferences and improving recommendation accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316524399",
                    "name": "Xiaoyu Kong"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2153659066",
                    "name": "An Zhang"
                },
                {
                    "authorId": "2258731846",
                    "name": "Leheng Sheng"
                },
                {
                    "authorId": "2316661312",
                    "name": "Hui Lin"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "5d458edb21c0d70c63018043cac7c1d0de1fd813",
            "title": "Reinforced Prompt Personalization for Recommendation with Large Language Models",
            "abstract": "Designing effective prompts can empower LLMs to understand user preferences and provide recommendations by leveraging LLMs' intent comprehension and knowledge utilization capabilities. However, existing research predominantly concentrates on task-wise prompting, developing fixed prompt templates composed of four patterns (i.e., role-playing, history records, reasoning guidance, and output format) and applying them to all users for a given task. Although convenient, task-wise prompting overlooks individual user differences, leading to potential mismatches in capturing user preferences. To address it, we introduce the concept of instance-wise prompting to personalize discrete prompts for individual users and propose Reinforced Prompt Personalization (RPP) to optimize the four patterns in prompts using multi-agent reinforcement learning (MARL). To boost efficiency, RPP formulates prompt personalization as selecting optimal sentences holistically across the four patterns, rather than optimizing word-by-word. To ensure the quality of prompts, RPP meticulously crafts diverse expressions for each of the four patterns, considering multiple analytical perspectives for specific recommendation tasks. In addition to RPP, our proposal of RPP+ aims to enhance the scalability of action space by dynamically refining actions with LLMs throughout the iterative process. We evaluate the effectiveness of RPP/RPP+ in ranking tasks over various datasets. Experimental results demonstrate the superiority of RPP/RPP+ over traditional recommender models, few-shot methods, and other prompt-based methods, underscoring the significance of instance-wise prompting for LLMs in recommendation tasks and validating the effectiveness of RPP/RPP+. Our code is available at https://github.com/maowenyu-11/RPP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283170413",
                    "name": "Wenyu Mao"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2312880149",
                    "name": "Weijian Chen"
                },
                {
                    "authorId": "2265123543",
                    "name": "Chongming Gao"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "6f918de95b8a3619e6eb00dbc16a3da43d6fa571",
            "title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization",
            "abstract": "This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low-quality data points, and pairwise noise, which encompasses erroneous data pair associations that affect preference rankings. Utilizing Distributionally Robust Optimization (DRO), we enhance DPO's resilience to these types of noise. Our theoretical insights reveal that DPO inherently embeds DRO principles, conferring robustness to pointwise noise, with the regularization coefficient $\\beta$ playing a critical role in its noise resistance. Extending this framework, we introduce Distributionally Robustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing against worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr. DPO allows for fine-tuned control over data pair reliability, providing a strategic balance between exploration and exploitation in noisy training environments. Empirical evaluations demonstrate that Dr. DPO substantially improves the quality of generated text and response accuracy in preference datasets, showcasing enhanced performance in both noisy and noise-free settings. The code is available at https://github.com/junkangwu/Dr_DPO.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260622979",
                    "name": "Junkang Wu"
                },
                {
                    "authorId": "2298950582",
                    "name": "Yuexiang Xie"
                },
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2259837759",
                    "name": "Jiawei Chen"
                },
                {
                    "authorId": "2237951623",
                    "name": "Jinyang Gao"
                },
                {
                    "authorId": "2265900833",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "8d90a0813617b90d90b9cc7cde91211bde496a47",
            "title": "Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients",
            "abstract": "Medication recommendation systems have gained significant attention in healthcare as a means of providing tailored and effective drug combinations based on patients' clinical information. However, existing approaches often suffer from fairness issues, as recommendations tend to be more accurate for patients with common diseases compared to those with rare conditions. In this paper, we propose a novel model called Robust and Accurate REcommendations for Medication (RAREMed), which leverages the pretrain-finetune learning paradigm to enhance accuracy for rare diseases. RAREMed employs a transformer encoder with a unified input sequence approach to capture complex relationships among disease and procedure codes. Additionally, it introduces two self-supervised pre-training tasks, namely Sequence Matching Prediction (SMP) and Self Reconstruction (SR), to learn specialized medication needs and interrelations among clinical codes. Experimental results on two real-world datasets demonstrate that RAREMed provides accurate drug sets for both rare and common disease patients, thereby mitigating unfairness in medication recommendation systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293453301",
                    "name": "Zihao Zhao"
                },
                {
                    "authorId": "2293673637",
                    "name": "Yi Jing"
                },
                {
                    "authorId": "2275173839",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2265123543",
                    "name": "Chongming Gao"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "9b2ae5c6be3af83384cc3de3160ef38ef4a78ae0",
            "title": "Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization",
            "abstract": "Graph out-of-distribution (OOD) generalization remains a major challenge in graph learning since graph neural networks (GNNs) often suffer from severe performance degradation under distribution shifts. Invariant learning, aiming to extract invariant features across varied distributions, has recently emerged as a promising approach for OOD generation. Despite the great success of invariant learning in OOD problems for Euclidean data (i.e., images), the exploration within graph data remains constrained by the complex nature of graphs. Existing studies, such as data augmentation or causal intervention, either suffer from disruptions to invariance during the graph manipulation process or face reliability issues due to a lack of supervised signals for causal parts. In this work, we propose a novel framework, called Invariant Graph Learning based on Information bottleneck theory (InfoIGL), to extract the invariant features of graphs and enhance models' generalization ability to unseen distributions. Specifically, InfoIGL introduces a redundancy filter to compress task-irrelevant information related to environmental factors. Cooperating with our designed multi-level contrastive learning, we maximize the mutual information among graphs of the same class in the downstream classification tasks, preserving invariant features for prediction to a great extent. An appealing feature of InfoIGL is its strong generalization ability without depending on supervised signal of invariance. Experiments on both synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance under OOD generalization for graph classification tasks. The source code is available at https://github.com/maowenyu-11/InfoIGL.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2283170413",
                    "name": "Wenyu Mao"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2314889892",
                    "name": "Haoyang Liu"
                },
                {
                    "authorId": "2003767516",
                    "name": "Yongduo Sui"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                }
            ]
        },
        {
            "paperId": "9bc15396818f63e319333d9f087120a4be8f9cae",
            "title": "Graph Contrastive Learning With Negative Propagation for Recommendation",
            "abstract": "Previous recommendation models build interest embeddings heavily relying on the observed interactions and optimize the embeddings with a contrast between the interactions and randomly sampled negative instances. To our knowledge, the negative interest signals remain unexplored in interest encoding, which merely serves losses for backpropagation. Besides, the sparse undifferentiated interactions inherently bring implicit bias in revealing users\u2019 interests, leading to suboptimal interest prediction. The negative interest signals would be a piece of promising evidence to support detailed interest modeling. In this work, we propose a perturbed graph contrastive learning with negative propagation (PCNP) for recommendation, which introduces negative interest to assist interest modeling in a contrastive learning (CL) architecture. An auxiliary channel of negative interest learning generates a contrastive graph by negative sampling and propagates complementary embeddings of users and items to encode negative signals. The proposed PCNP contrasts positive and negative embeddings to promote interest modeling for recommendation. Extensive experiments demonstrate the capability of PCNP using two-level CL to alleviate interaction sparsity and bias issues for recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159003695",
                    "name": "Meishan Liu"
                },
                {
                    "authorId": "2980051",
                    "name": "Meng Jian"
                },
                {
                    "authorId": "2275528397",
                    "name": "Yulong Bai"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2241927662",
                    "name": "Lifang Wu"
                }
            ]
        },
        {
            "paperId": "b5bb10b374b52a830e96e3c427d8486def7d882c",
            "title": "Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning",
            "abstract": "Conventional recommender systems (RSs) face challenges in precisely capturing users' fine-grained preferences. Large language models (LLMs) have shown capabilities in commonsense reasoning and leveraging external tools that may help address these challenges. However, existing LLM-based RSs suffer from hallucinations, misalignment between the semantic space of items and the behavior space of users, or overly simplistic control strategies (e.g., whether to rank or directly present existing results). To bridge these gap, we introduce ToolRec, a framework for LLM-empowered recommendations via tool learning that uses LLMs as surrogate users, thereby guiding the recommendation process and invoking external tools to generate a recommendation list that aligns closely with users' nuanced preferences. We formulate the recommendation process as a process aimed at exploring user interests in attribute granularity. The process factors in the nuances of the context and user preferences. The LLM then invokes external tools based on a user's attribute instructions and probes different segments of the item pool. We consider two types of attribute-oriented tools: rank tools and retrieval tools. Through the integration of LLMs, ToolRec enables conventional recommender systems to become external tools with a natural language interface. Extensive experiments verify the effectiveness of ToolRec, particularly in scenarios that are rich in semantic content.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109814746",
                    "name": "Yuyue Zhao"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2303792476",
                    "name": "Wei Tang"
                },
                {
                    "authorId": "2303281488",
                    "name": "Dingxian Wang"
                },
                {
                    "authorId": "2265490493",
                    "name": "M. D. Rijke"
                }
            ]
        }
    ]
}