{
    "authorId": "2216605916",
    "papers": [
        {
            "paperId": "1b59fe8d168f6b7c762ade018041ac09f438eeee",
            "title": "What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation",
            "abstract": "Vision-Language Models (VLMs) have gained community-spanning prominence due to their ability to integrate visual and textual inputs to perform complex tasks. Despite their success, the internal decision-making processes of these models remain opaque, posing challenges in high-stakes applications. To address this, we introduce NOTICE, the first Noise-free Text-Image Corruption and Evaluation pipeline for mechanistic interpretability in VLMs. NOTICE incorporates a Semantic Minimal Pairs (SMP) framework for image corruption and Symmetric Token Replacement (STR) for text. This approach enables semantically meaningful causal mediation analysis for both modalities, providing a robust method for analyzing multimodal integration within models like BLIP. Our experiments on the SVO-Probes, MIT-States, and Facial Expression Recognition datasets reveal crucial insights into VLM decision-making, identifying the significant role of middle-layer cross-attention heads. Further, we uncover a set of ``universal cross-attention heads'' that consistently contribute across tasks and modalities, each performing distinct functions such as implicit image segmentation, object inhibition, and outlier inhibition. This work paves the way for more transparent and interpretable multimodal systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171105220",
                    "name": "Michal Golovanevsky"
                },
                {
                    "authorId": "2166313068",
                    "name": "William Rudman"
                },
                {
                    "authorId": "2216605916",
                    "name": "Vedant Palit"
                },
                {
                    "authorId": "2268844528",
                    "name": "Ritambhara Singh"
                },
                {
                    "authorId": "2262215315",
                    "name": "Carsten Eickhoff"
                }
            ]
        },
        {
            "paperId": "360e4cbd79690f75756900dcb4818cf177071995",
            "title": "Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust",
            "abstract": "A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs. This has significant application-level user trust implications as stochastic patterns without a strong sense of meaning cannot be trusted in high-stakes applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2066796687",
                    "name": "Tarun Garg"
                },
                {
                    "authorId": "2216605916",
                    "name": "Vedant Palit"
                },
                {
                    "authorId": "2187301061",
                    "name": "Yuxin Zi"
                },
                {
                    "authorId": "2068096390",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "d494727306a375e524c4c4c8cc1a2dc1845cc4b7",
            "title": "Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP",
            "abstract": "Mechanistic interpretability seeks to understand the neural mechanisms that enable specific behaviors in Large Language Models (LLMs) by leveraging causality-based methods. While these approaches have identified neural circuits that copy spans of text, capture factual knowledge, and more, they remain unusable for multimodal models since adapting these tools to the vision-language domain requires considerable architectural changes. In this work, we adapt a unimodal causal tracing tool to BLIP to enable the study of the neural mechanisms underlying image-conditioned text generation. We demonstrate our approach on a visual question answering dataset, highlighting the causal relevance of later layer representations for all tokens. Furthermore, we release our BLIP causal tracing tool as open source to enable further experimentation in vision-language mechanistic interpretability by the community. Our code is available at this URL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216605916",
                    "name": "Vedant Palit"
                },
                {
                    "authorId": "1471734043",
                    "name": "Rohan Pandey"
                },
                {
                    "authorId": "1575802390",
                    "name": "Aryaman Arora"
                },
                {
                    "authorId": "28130078",
                    "name": "P. Liang"
                }
            ]
        }
    ]
}