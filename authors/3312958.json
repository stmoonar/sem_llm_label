{
    "authorId": "3312958",
    "papers": [
        {
            "paperId": "38af6ea1957ac76848d735c207b34fd9971b3722",
            "title": "DAViD: Domain Adaptive Visually-Rich Document Understanding with Synthetic Insights",
            "abstract": "Visually-Rich Documents (VRDs), encompassing elements like charts, tables, and references, convey complex information across various fields. However, extracting information from these rich documents is labor-intensive, especially given their inconsistent formats and domain-specific requirements. While pretrained models for VRD Understanding have progressed, their reliance on large, annotated datasets limits scalability. This paper introduces the Domain Adaptive Visually-rich Document Understanding (DAViD) framework, which utilises machine-generated synthetic data for domain adaptation. DAViD integrates fine-grained and coarse-grained document representation learning and employs synthetic annotations to reduce the need for costly manual labelling. By leveraging pretrained models and synthetic data, DAViD achieves competitive performance with minimal annotated datasets. Extensive experiments validate DAViD's effectiveness, demonstrating its ability to efficiently adapt to domain-specific VRDU tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166956722",
                    "name": "Yihao Ding"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "2323844149",
                    "name": "Zechuan Li"
                },
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                }
            ]
        },
        {
            "paperId": "c22fea067a866f823286c20b3d087e06a01af7e7",
            "title": "PEACH: Pretrained-embedding Explanation Across Contextual and Hierarchical Structure",
            "abstract": "In this work, we propose a novel tree-based explanation technique, PEACH (Pretrained-embedding Explanation Across Contextual and Hierarchical Structure), that can explain how text-based documents are classified by using any pretrained contextual embeddings in a tree-based human-interpretable manner. Note that PEACH can adopt any contextual embeddings of the PLMs as a training input for the decision tree. \n\nUsing the proposed PEACH, we perform a comprehensive analysis of several contextual embeddings on nine different NLP text classification benchmarks. This analysis demonstrates the flexibility of the model by appling several PLM contextual embeddings, its attribute selections, scaling, and clustering methods. Furthermore, we show the utility of explanations by visualising the feature selection and important trend of text classification via human-interpretable word-cloud-based trees, which clearly identify model mistakes and assist in dataset debugging. Besides interpretability, PEACH outperforms or is similar to those from pretrained models. Code and Appendix are in https://github.com/adlnlp/peach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162737605",
                    "name": "Feiqi Cao"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                }
            ]
        },
        {
            "paperId": "473008e002aed063257afbd9988e5cc483dec35e",
            "title": "PDFVQA: A New Dataset for Real-World VQA on PDF Documents",
            "abstract": "Document-based Visual Question Answering examines the document understanding of document images in conditions of natural language questions. We proposed a new document-based VQA dataset, PDF-VQA, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages. We also propose a new graph-based VQA model that explicitly integrates the spatial and hierarchically structural relationships between different document elements to boost the document structural understanding. The performances are compared with several baselines over different question types and tasks\\footnote{The full dataset will be released after paper acceptance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166956722",
                    "name": "Yihao Ding"
                },
                {
                    "authorId": "39209233",
                    "name": "Siwen Luo"
                },
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                }
            ]
        },
        {
            "paperId": "e48e4cf40ae9cc1c7f68e36952f62440a6a8f7b9",
            "title": "Form-NLU: Dataset for the Form Natural Language Understanding",
            "abstract": "Compared to general document analysis tasks, form document structure understanding and retrieval are challenging. Form documents are typically made by two types of authors; A form designer, who develops the form structure and keys, and a form user, who fills out form values based on the provided keys. Hence, the form values may not be aligned with the form designer's intention (structure and keys) if a form user gets confused. In this paper, we introduce Form-NLU, the first novel dataset for form structure understanding and its key and value information extraction, interpreting the form designer's intent and the alignment of user-written value on it. It consists of 857 form images, 6k form keys and values, and 4k table keys and values. Our dataset also includes three form types: digital, printed, and handwritten, which cover diverse form appearances and layouts. We propose a robust positional and logical relation-based form key-value information extraction framework. Using this dataset, Form-NLU, we first examine strong object detection models for the form layout understanding, then evaluate the key information extraction task on the dataset, providing fine-grained results for different types of forms and keys. Furthermore, we examine it with the off-the-shelf pdf layout extraction tool and prove its feasibility in real-world cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166956722",
                    "name": "Yihao Ding"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "2213332565",
                    "name": "Jiabin Huang"
                },
                {
                    "authorId": "2212942973",
                    "name": "Kaixuan Ren"
                },
                {
                    "authorId": "2115606197",
                    "name": "Xingxian Luo"
                },
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                }
            ]
        },
        {
            "paperId": "560e0114a023bdfd99eb60eb4d9d555a348600a0",
            "title": "PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals",
            "abstract": "We propose a PiggyBack, a Visual Question Answering platform that allows users to apply the state-of-the-art visual-language pretrained models easily. We integrate visual-language models, pretrained by HuggingFace, an open-source API platform of deep learning technologies; however, it cannot be runnable without programming skills or deep learning understanding. Hence, our PiggyBack supports an easy-to-use browser-based user interface with several deep-learning visual language pretrained models for general users and domain experts. The PiggyBack includes the following benefits: Portability due to web-based and thus runs on almost any platform, A comprehensive data creation and processing technique, and ease of use on visual language pretrained models. The demo video can be found at https://youtu.be/iz44RZ1lF4s.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2179013400",
                    "name": "Zhihao Zhang"
                },
                {
                    "authorId": "39209233",
                    "name": "Siwen Luo"
                },
                {
                    "authorId": "2108169214",
                    "name": "Junyi Chen"
                },
                {
                    "authorId": "2164243615",
                    "name": "Sijia Lai"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                }
            ]
        },
        {
            "paperId": "a5107bb4ff932e72afa2497268e9646a53885629",
            "title": "V-Doc : Visual questions answers with Documents",
            "abstract": "We propose V-Doc, a question-answering tool using document images and PDF, mainly for researchers and general non-deep learning experts looking to generate, process, and understand the document visual question answering tasks. The V-Doc supports generating and using both extractive and abstractive question-answer pairs using documents images. The extractive QA selects a subset of tokens or phrases from the document contents to predict the answers, while the abstractive QA recognises the language in the content and generates the answer based on the trained model. Both aspects are crucial to understanding the documents, especially in an image format. We include a detailed scenario of question generation for the abstractive QA task. V-Doc supports a wide range of datasets and models, and is highly extensible through a declarative, framework-agnostic platform.11Data and demo video: https://github.com/usydnlp/vdoc",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166956722",
                    "name": "Yihao Ding"
                },
                {
                    "authorId": "2166912856",
                    "name": "Zhe Huang"
                },
                {
                    "authorId": "2166827160",
                    "name": "Runlin Wang"
                },
                {
                    "authorId": "2179703699",
                    "name": "Yanhang Zhang"
                },
                {
                    "authorId": "2167589253",
                    "name": "Xianru Chen"
                },
                {
                    "authorId": "2166916005",
                    "name": "Yuzhong Ma"
                },
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                }
            ]
        },
        {
            "paperId": "e710e0af0eb843d400ae34ff13081039ff24eb7e",
            "title": "A comparative study of online and face-to-face embedded systems learning course",
            "abstract": "Embedded Systems are integrated into our daily life in many ways. Embedded systems education requires at least two major components: (1) a teacher with expertise in both hardware and software; (2) a hands-on experimental environment. Recent developments in educational tool kits for learning embedded systems, such as Ardu-EZ, increase the feasibility of teaching embedded systems online; but conclusive evidence about the efficiency of teaching embedded systems online is still very rare. This paper presents a comparative study of face-to-face and online delivery modes of an embedded systems course with a focus on students' learning experience. During our experiment, an embedded systems course was taught in both face-to-face and online modes with identical content. The results show that online learning mode can be a viable alternative to the traditional face-to-face course for embedded systems education if required components such as student-teacher interaction, sufficient independent learning skills for the students, well-designed teaching materials, and tangible support are provided.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "32032101",
                    "name": "Shouvojit Sarker"
                },
                {
                    "authorId": "1796502",
                    "name": "Leonie Ellis"
                },
                {
                    "authorId": "1976441",
                    "name": "B. Kang"
                }
            ]
        },
        {
            "paperId": "a0e05882013bf522027c66ec136e225deb687fe3",
            "title": "Constructing an initial knowledge base for medical domain expert system using induct RDR",
            "abstract": "This paper describes how we build an initial knowledge-base of ripple-down rules (RDR) in medical domain. In medical domain, all decisions are made by the domain experts. Increasing a complexity of disease and various symptoms, there are some attempts to introduce an expert system in medical domain these days. To construct the expert system, it needs to extract the expert's knowledge. To do that, we use ripple-down rules (RDR) which allows experts to modify their knowledge base directly because it provides a systematic approach to do that. We also use Induct RDR which builds a knowledge base from existing data to reduce experts' burden of adding their knowledge from the bottom up. The expert system should produce multiple comments from a test set, which is multiple classification problem. However, Induct RDR only deals with a single classification problem. To handle this problem, we divide a test set into 18 categories which is almost the single classification problem and apply Induct RDR to each category independently. Using this approach, we can improve the missing rate about 70% compared to an approach not dividing into several categories.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3361229",
                    "name": "Jonghwan Hyeon"
                },
                {
                    "authorId": "1980224",
                    "name": "Kyojoong Oh"
                },
                {
                    "authorId": "2107906682",
                    "name": "You Jin Kim"
                },
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                },
                {
                    "authorId": "1976441",
                    "name": "B. Kang"
                },
                {
                    "authorId": "145530103",
                    "name": "Ho\u2010Jin Choi"
                }
            ]
        },
        {
            "paperId": "dac3a0f5f86aaf3d1dfdbc9696ea1e9437e4ba15",
            "title": "What issue spread on the web: analyze the web trends",
            "abstract": "On the web, a lot of services enable people to obtain, share, and disseminate information. There are three main types of services that help people to obtain information in real time: search engines, social networking services (SNS), and Internet-news-services. People use the web to share or disseminate information. News companies display their articles to the public, and individuals post and share their personal interests on their blogs or social network sites. People try to find information by using search engines as well. As these activities are conducted in social contexts, they convey social trends. Those services also provide an additional service that displays the list of the most discussed topic-keywords in each service. In this paper, there are two main goals: First of all, we research on the characteristics of the popular events/issues among users in online services, including search engine, social media, and online news services and examine how the events/issues diffuse among those three services on the web. To achieve this goal, we collected social issue keywords from three different services, Google Trends, Twitter Trending Topics, Google News Top Stories. First, we focus on the characteristics of the popular issues among users in each service. Then, we provide the quantitative study on the issue diffusion among three different services manifests and disappearances of trends are different for each service.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1976441",
                    "name": "B. Kang"
                },
                {
                    "authorId": "2109233894",
                    "name": "Do Hyeong Kim"
                },
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                }
            ]
        }
    ]
}