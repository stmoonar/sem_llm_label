{
    "authorId": "2046170685",
    "papers": [
        {
            "paperId": "a2ed954a00672eb97dca028b921bd47b82267cb0",
            "title": "On the Ordering of Pooled Web Pages, Gold Assessments, and Bronze Assessments",
            "abstract": "The present study leverages a recent opportunity we had to create a new English web search test collection for the NTCIR-16 We Want Web (WWW-4) task, which concluded in June 2022. More specifically, through the test collection construction effort, we examined two factors that may affect the relevance assessments of depth-k pools, which in turn may affect the relative evaluation of different IR systems. The first factor is the document ordering strategy for the assessors, namely, prioritisation (PRI) and randomisation (RND). PRI is a method that has been used in NTCIR tasks for over a decade; it ranks the pooled documents by a kind of pseudorelevance for the assessors. The second factor is assessor type, i.e., Gold or Bronze. Gold assessors are the topic creators and therefore they \u201cknow\u201d which documents are (highly) relevant and which are not; Bronze assessors are not the topic creators and may lack sufficient knowledge about the topics. We believe that our study is unique in that the authors of this article served as the Gold assessors when creating the WWW-4 test collection, which enabled us to closely examine why Bronze assessments differ from the Gold ones. Our research questions examine assessor efficiency (RQ1), inter-assessor agreement (RQ2), system ranking similarity with different qrels files (RQ3), system ranking robustness to the choice of test topics (RQ4), and the reasons why Bronze assessors tend to be more liberal than Gold assessors (RQ5). The most remarkable of our results are as follows: First, in the comparisons for RQ1 through RQ4, it turned out that what may matter more than the document ordering strategy (PRI vs. RND) and the assessor type (Gold vs. Bronze) is how well-motivated and/or well-trained the Bronze assessors are. Second, regarding RQ5, of the documents originally judged nonrelevant by the Gold assessors contrary to the Bronze assessors in our experiments, almost one half were truly relevant according to the Gold assessors\u2019 own reconsiderations. This result suggests that even Gold assessors are far from perfect; budget permitting, it may be beneficial to hire highly motivated Bronze assessors in addition to Gold assessors so they can complement each other.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725544",
                    "name": "T. Sakai"
                },
                {
                    "authorId": "2054575809",
                    "name": "Sijie Tao"
                },
                {
                    "authorId": "2118768400",
                    "name": "Nuo Chen"
                },
                {
                    "authorId": "2135349819",
                    "name": "Yujing Li"
                },
                {
                    "authorId": "1954475",
                    "name": "Maria Maistro"
                },
                {
                    "authorId": "1381470019",
                    "name": "Zhumin Chu"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                }
            ]
        },
        {
            "paperId": "a6d085e0644d61c199eb5996cd0adebe1b778401",
            "title": "How Discriminative Are Your Qrels? How To Study the Statistical Significance of Document Adjudication Methods",
            "abstract": "Creating test collections for offline retrieval evaluation requires human effort to judge documents' relevance. This expensive activity motivated much work in developing methods for constructing benchmarks with fewer assessment costs. In this respect, adjudication methods actively decide both which documents and the order in which experts review them, in order to better exploit the assessment budget or to lower it. Researchers evaluate the quality of those methods by measuring the correlation between the known gold ranking of systems under the full collection and the observed ranking of systems under the lower-cost one. This traditional analysis ignores whether and how the low-cost judgements impact on the statistically significant differences among systems with respect to the full collection. We fill this void by proposing a novel methodology to evaluate how the low-cost adjudication methods preserve the pairwise significant differences between systems as the full collection. In other terms, while traditional approaches look for stability in answering the question \"is system A better than system B?\", our proposed approach looks for stability in answering the question \"is system A significantly better than system B?\", which is the ultimate questions researchers need to answer to guarantee the generalisability of their results. Among other results, we found that the best methods in terms of ranking of systems correlation do not always match those preserving statistical significance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2073115031",
                    "name": "David Otero"
                },
                {
                    "authorId": "1898621",
                    "name": "Javier Parapar"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                }
            ]
        },
        {
            "paperId": "d2aeedf086c83806d1ed99ddc76d7d9db90978d1",
            "title": "Report from Dagstuhl Seminar 23031: Frontiers of Information Access Experimentation for Research and Education",
            "abstract": "This report documents the program and the outcomes of Dagstuhl Seminar 23031 ``Frontiers of Information Access Experimentation for Research and Education'', which brought together 37 participants from 12 countries. The seminar addressed technology-enhanced information access (information retrieval, recommender systems, natural language processing) and specifically focused on developing more responsible experimental practices leading to more valid results, both for research as well as for scientific education. The seminar brought together experts from various sub-fields of information access, namely IR, RS, NLP, information science, and human-computer interaction to create a joint understanding of the problems and challenges presented by next generation information access systems, from both the research and the experimentation point of views, to discuss existing solutions and impediments, and to propose next steps to be pursued in the area in order to improve not also our research methods and findings but also the education of the new generation of researchers and developers. The seminar featured a series of long and short talks delivered by participants, who helped in setting a common ground and in letting emerge topics of interest to be explored as the main output of the seminar. This led to the definition of five groups which investigated challenges, opportunities, and next steps in the following areas: reality check, i.e. conducting real-world studies, human-machine-collaborative relevance judgment frameworks, overcoming methodological challenges in information retrieval and recommender systems through awareness and education, results-blind reviewing, and guidance for authors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39367482",
                    "name": "Christine Bauer"
                },
                {
                    "authorId": "1750995",
                    "name": "Ben Carterette"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "1703148",
                    "name": "N. Fuhr"
                }
            ]
        },
        {
            "paperId": "2b768ad30497c7f1545f8542ec5eb64d1cd73eb8",
            "title": "Report on the 13th Conference and Labs of the Evaluation Forum (CLEF 2022)",
            "abstract": "This is a report on the thirteenth edition of the Conference and Labs of the Evaluation Forum (CLEF 2022), held on September 5--8, 2022, in Bologna, Italy. CLEF was a four-day hybrid event combining a conference and an evaluation forum. The conference featured keynotes by Benno Stein and Rita Cucchiara, and presentation of peer-reviewed research papers covering a wide range of topics, in addition to many posters. The evaluation forum consisted of fourteen labs: ARQMath, BioASQ, CheckThat!, ChEMU, eRisk, HIPE, iDPP, ImageCLEF, JokeR, LeQua, LifeCLEF, PAN, SimpleText, and Touch\u00e9, addressing a wide range of tasks, media, languages, and ways to go beyond standard test collections. Date: 5--8 September, 2022. Website: https://clef2022.clef-initiative.eu/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "134000266",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "33831241",
                    "name": "M. Esposti"
                },
                {
                    "authorId": "80808662",
                    "name": "G. Faggioli"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                },
                {
                    "authorId": "3046200",
                    "name": "Martin Potthast"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "59d21c5faee2df1617bd779e416cccf4fdf97559",
            "title": "Extracting a Unified Database from Heterogeneous Bills of Materials via Data Integration Approaches",
            "abstract": "Since the beginning of the information era, manufacturing companies have collected a huge amount of data and acknowledged the importance of proper collection and curation policies. Nevertheless, an open problem is the exploitation of historical and legacy data. Such data is heterogeneous: it has often been accumulated throughout decades by different curators with different degrees of attention toward the data. In this work, we focus on the historical data collected by Bedeschi S.p.A., an international company dealing with the design and production of turn-key solutions for Bulk Material Handling, Container Logistics and Bricks. Our task is to extract knowledge, available under the form of partially structured Bills of Materials (BoMs), to distil a unified database over a mediated schema that can be used by designers and sales department to rapidly have a global overview of the products developed over the years. We present a feasibility study investigating the performance of several traditional approaches of data integration applied to our specific task. In particular, we treat BoMs as sources that need to be aggregated in the unified mediated schema. We achieved satisfactory results indicating the direction that the company should follow to properly extract knowledge from their historical data. and historical datasets unexploitable automatically. Our work stems from a specific industrial need: exploiting historical data to help the design and sales departments by providing them with a unifying overview of all the products built over the years. We exploit historical data collected by Bedeschi S.p.A, an international company specialized in the production of turn-key solutions for Bulk Material Handling, Container Logistics and Bricks. We aim at extracting information from a set of heterogeneous semi-structured Bills of Materials (BoMs) to organize",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3640923",
                    "name": "L. D. Lucia"
                },
                {
                    "authorId": "80808662",
                    "name": "G. Faggioli"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "2182554233",
                    "name": "Fabio Orobelli"
                }
            ]
        },
        {
            "paperId": "854d279b43dce01ce06a26a8ac8988baa17a4824",
            "title": "Towards Feature Selection for Ranking and Classification Exploiting Quantum Annealers",
            "abstract": "Feature selection is a common step in many ranking, classification, or prediction tasks and serves many purposes. By removing redundant or noisy features, the accuracy of ranking or classification can be improved and the computational cost of the subsequent learning steps can be reduced. However, feature selection can be itself a computationally expensive process. While for decades confined to theoretical algorithmic papers, quantum computing is now becoming a viable tool to tackle realistic problems, in particular special-purpose solvers based on the Quantum Annealing paradigm. This paper aims to explore the feasibility of using currently available quantum computing architectures to solve some quadratic feature selection algorithms for both ranking and classification. The experimental analysis includes 15 state-of-the-art datasets. The effectiveness obtained with quantum computing hardware is comparable to that of classical solvers, indicating that quantum computers are now reliable enough to tackle interesting problems. In terms of scalability, current generation quantum computers are able to provide a limited speedup over certain classical algorithms and hybrid quantum-classical strategies show lower computational cost for problems of more than a thousand features.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "51237994",
                    "name": "Maurizio Ferrari Dacrema"
                },
                {
                    "authorId": "2164384274",
                    "name": "Fabio Moroni"
                },
                {
                    "authorId": "2124653987",
                    "name": "Riccardo Nembrini"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "80808662",
                    "name": "G. Faggioli"
                },
                {
                    "authorId": "1709519",
                    "name": "P. Cremonesi"
                }
            ]
        },
        {
            "paperId": "dc9a3afe7053407a186a287cec4f4a1d4d80fd99",
            "title": "How Do You Test a Test?: A Multifaceted Examination of Significance Tests",
            "abstract": "We examine three statistical significance tests -- a recently proposed ANOVA model and two baseline tests -- using a suite of measures to determine which is better suited for offline evaluation. We apply our analysis to both the runs of a whole TREC track and also to the runs submitted by six participant groups. The former reveals test behavior in the heterogeneous settings of a large-scale offline evaluation initiative; the latter, almost overlooked in past work (to the best of our knowledge), reveals what happens in the much more restricted case of variants of a single system, i.e. the typical context in which companies and research groups operate. We find the ANOVA test strikingly consistent in large-scale settings, but worryingly inconsistent in some participant experiments. Of greater concern, the participant only experiments show one of our baseline tests (a test widely used in research) can produce a substantial number of inconsistent results. We discuss the implications of this inconsistency for possible publication bias.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "144721996",
                    "name": "M. Sanderson"
                }
            ]
        },
        {
            "paperId": "e6643c2a945e3ad878652cfde05a3c4d782f7cd0",
            "title": "Corrected Evaluation Results of the NTCIR WWW-2, WWW-3, and WWW-4 English Subtasks",
            "abstract": "Unfortunately, the official English (sub)task results reported in the NTCIR-14 WWW-2, NTCIR-15 WWW-3, and NTCIR-16 WWW-4 overview papers are incorrect due to noise in the official qrels files; this paper reports results based on the corrected qrels files. The noise is due to a fatal bug in the backend of our relevance assessment interface. More specifically, at WWW-2, WWW-3, and WWW-4, two versions of pool files were created for each English topic: a PRI (\"prioritised\") file, which uses the NTCIRPOOL script to prioritise likely relevant documents, and a RND (\"randomised\") file, which randomises the pooled documents. This was done for the purpose of studying the effect of document ordering for relevance assessors. However, the programmer who wrote the interface backend assumed that a combination of a topic ID and a document rank in the pool file uniquely determines a document ID; this is obviously incorrect as we have two versions of pool files. The outcome is that all the PRI-based relevance labels for the WWW-2 test collection are incorrect (while all the RND-based relevance labels are correct), and all the RND-based relevance labels for the WWW-3 and WWW-4 test collections are incorrect (while all the PRI-based relevance labels are correct). This bug was finally discovered at the NTCIR-16 WWW-4 task when the first seven authors of this paper served as Gold assessors (i.e., topic creators who define what is relevant) and closely examined the disagreements with Bronze assessors (i.e., non-topic-creators; non-experts). We would like to apologise to the WWW participants and the NTCIR chairs for the inconvenience and confusion caused due to this bug.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725544",
                    "name": "T. Sakai"
                },
                {
                    "authorId": "2054575809",
                    "name": "Sijie Tao"
                },
                {
                    "authorId": "1954475",
                    "name": "Maria Maistro"
                },
                {
                    "authorId": "1381470019",
                    "name": "Zhumin Chu"
                },
                {
                    "authorId": "2135349819",
                    "name": "Yujing Li"
                },
                {
                    "authorId": null,
                    "name": "Nuo Chen"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "2143183255",
                    "name": "Junjie Wang"
                },
                {
                    "authorId": "144526707",
                    "name": "I. Soboroff"
                },
                {
                    "authorId": "46399371",
                    "name": "Y. Liu"
                }
            ]
        },
        {
            "paperId": "e91f7145a3c96480103f8d1138619551dbc5ac8e",
            "title": "Comparing ANOVA Approaches to Detect Significantly Different IR Systems",
            "abstract": "The ultimate goal of the evaluation is to understand when two IR systems are (significantly) different. To this end, many comparison procedures have been developed over time. However, to date, most reproducibility efforts focused just on reproducing systems and algorithms, almost fully neglecting to investigate the reproducibility of the methods we use to compare our systems. In this paper, we focus on methods based on ANalysis Of VAriance (ANOVA), which explicitly model the data in terms of different contributing effects, allowing us to obtain a more accurate estimate of significant differences. In this context, we compare statistical analysis methods based on \u201ctraditional\u201d ANOVA (t ANOVA ) to those based on a bootstrapped version of ANOVA (b ANOVA ) and those performing multiple comparisons relying on a more conservative Family-wise Error Rate (FWER) controlling approach to those relying on a more lenient False Discovery Rate (FDR) controlling approach. Our findings highlight that, compared to the t ANOVA approaches, b ANOVA presents greater statistical power, at the cost of lower stability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80808662",
                    "name": "G. Faggioli"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                }
            ]
        },
        {
            "paperId": "e96522de8bc54a59d702143b1633eab803727a27",
            "title": "FullBrain: a Social E-learning Platform",
            "abstract": "We present FullBrain, a social e-learning platform where students share and track their knowledge. FullBrain users can post notes, ask questions and share learning resources in dedicated course and concept spaces. We detail two components of FullBrain: a SIR system equipped with query autocomplete and query autosuggestion, and a Leaderboard module to improve user experience. We analyzed the day-to-day users' usage of the SIR system, measuring a time-to-complete a request below 0.11s, matching or exceeding our UX targets. Moreover, we performed stress tests which lead the way for more detailed analysis. Through a preliminary user study and log data analysis, we observe that 97% of the users' activity is directed to the top 4 positions in the leaderboard.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051749166",
                    "name": "M. Biasini"
                },
                {
                    "authorId": "2051748443",
                    "name": "Vittorio Carmignani"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "2051748428",
                    "name": "Panagiotis Filianos"
                },
                {
                    "authorId": "1954475",
                    "name": "Maria Maistro"
                },
                {
                    "authorId": "1695690",
                    "name": "Giorgio Maria Di Nunzio"
                }
            ]
        }
    ]
}