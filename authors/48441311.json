{
    "authorId": "48441311",
    "papers": [
        {
            "paperId": "3bce52a911cda47f4822fcc0f679c8234572d4fb",
            "title": "Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering",
            "abstract": "An open-domain question answering (QA) system usually follows a retrieve-then-read paradigm, in which a retriever is used to retrieve relevant passages from a large corpus, and then a reader generates answers based on the retrieved passages and the original question. In this paper, we propose a simple and novel mutual learning framework to improve the performance of retrieve-then-read-style models via an intermediate module named the knowledge selector, which we train with reinforcement learning. The key benefits of our proposed intermediate module are: 1) no requirement for additional annotated question-passage pairs; 2) improvements in both retrieval and QA performance, as well as computational efficiency, compared to prior competitive retrieve-then-read models; 3) with no finetuning, improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119264432",
                    "name": "Dingmin Wang"
                },
                {
                    "authorId": "2110991956",
                    "name": "Qiuyuan Huang"
                },
                {
                    "authorId": "2287808304",
                    "name": "Matthew Jackson"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "0819c1e60c13b9797f937282d06b54d252d9d6ec",
            "title": "Segment Everything Everywhere All at Once",
            "abstract": "In this work, we present SEEM, a promptable and interactive model for segmenting everything everywhere all at once in an image, as shown in Fig.1. In SEEM, we propose a novel decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal segmentation interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata: i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks; iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from decoder to image features; and iv) Semantic-awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. Notably, our single SEEM model achieves competitive performance across interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity for generalization to novel prompts or their combinations, rendering it a readily universal image segmentation interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1882962264",
                    "name": "Xueyan Zou"
                },
                {
                    "authorId": "120157163",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2315254849",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2152978390",
                    "name": "Feng Li"
                },
                {
                    "authorId": "50703697",
                    "name": "Linjie Li"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "144756076",
                    "name": "Yong Jae Lee"
                }
            ]
        },
        {
            "paperId": "0ad0e3e965318d83f53f8d5980f3a3d4cc81922f",
            "title": "Pre-training Transformers for Knowledge Graph Completion",
            "abstract": "Learning transferable representation of knowledge graphs (KGs) is challenging due to the heterogeneous, multi-relational nature of graph structures. Inspired by Transformer-based pretrained language models' success on learning transferable representation for texts, we introduce a novel inductive KG representation model (iHT) for KG completion by large-scale pre-training. iHT consists of a entity encoder (e.g., BERT) and a neighbor-aware relational scoring function both parameterized by Transformers. We first pre-train iHT on a large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art results on matched evaluations, with a relative improvement of more than 25% in mean reciprocal rank over previous SOTA models. When further fine-tuned on smaller KGs with either entity and relational shifts, pre-trained iHT representations are shown to be transferable, significantly improving the performance on FB15K-237 and WN18RR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2296722484",
                    "name": "Sanxing Chen"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "46522098",
                    "name": "Xiaodong Liu"
                },
                {
                    "authorId": "49097406",
                    "name": "Jian Jiao"
                },
                {
                    "authorId": "2114167577",
                    "name": "Yangfeng Ji"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "103ee4ea6dd56890c517dadc07c6bd8f4d29a359",
            "title": "Bridging Discrete and Backpropagation: Straight-Through and Beyond",
            "abstract": "Backpropagation, the cornerstone of deep learning, is limited to computing gradients for continuous variables. This limitation poses challenges for problems involving discrete latent variables. To address this issue, we propose a novel approach to approximate the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose ReinMax, which achieves second-order accuracy by integrating Heun's method, a second-order numerical method for solving ODEs. ReinMax does not require Hessian or other second-order derivatives, thus having negligible computation overheads. Extensive experimental results on various tasks demonstrate the superiority of ReinMax over the state of the art. Implementations are released at https://github.com/microsoft/ReinMax.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109392217",
                    "name": "Liyuan Liu"
                },
                {
                    "authorId": "2113540861",
                    "name": "Chengyu Dong"
                },
                {
                    "authorId": "46522098",
                    "name": "Xiaodong Liu"
                },
                {
                    "authorId": "2116416822",
                    "name": "Bin Yu"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e",
            "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
            "abstract": "Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2887562",
                    "name": "Pan Lu"
                },
                {
                    "authorId": "1780690",
                    "name": "Baolin Peng"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "1947267",
                    "name": "Michel Galley"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "39092098",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "145380991",
                    "name": "Song-Chun Zhu"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "179237bc5fb46f34ef936b1552600bf3521c3c64",
            "title": "Differentiable Tree Operations Promote Compositional Generalization",
            "abstract": "In the context of structure-to-structure transformation tasks, learning sequences of discrete symbolic operations poses significant challenges due to their non-differentiability. To facilitate the learning of these symbolic sequences, we introduce a differentiable tree interpreter that compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. We present a novel Differentiable Tree Machine (DTM) architecture that integrates our interpreter with an external memory and an agent that learns to sequentially select tree operations to execute the target transformation in an end-to-end manner. With respect to out-of-distribution compositional generalization on synthetic semantic parsing and language generation tasks, DTM achieves 100% while existing baselines such as Transformer, Tree Transformer, LSTM, and Tree2Tree LSTM achieve less than 30%. DTM remains highly interpretable in addition to its perfect performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46182659",
                    "name": "Paul Soulos"
                },
                {
                    "authorId": "2157840220",
                    "name": "J. E. Hu"
                },
                {
                    "authorId": "14310569",
                    "name": "Kate McCurdy"
                },
                {
                    "authorId": "104375103",
                    "name": "Yunmo Chen"
                },
                {
                    "authorId": "2097712360",
                    "name": "Roland Fernandez"
                },
                {
                    "authorId": "1748557",
                    "name": "P. Smolensky"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "1cd56940219a968a04eb61b8bc53f4cd3f9822d4",
            "title": "Is Self-Repair a Silver Bullet for Code Generation?",
            "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689656957",
                    "name": "Theo X. Olausson"
                },
                {
                    "authorId": "1827015",
                    "name": "J. Inala"
                },
                {
                    "authorId": "2144523164",
                    "name": "Chenglong Wang"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "1389870240",
                    "name": "Armando Solar-Lezama"
                }
            ]
        },
        {
            "paperId": "2b0d96dccd07ebe8feb90951fe90d3aa81741097",
            "title": "Understand and Modularize Generator Optimization in ELECTRA-style Pretraining",
            "abstract": "Despite the effectiveness of ELECTRA-style pretraining, their performance is dependent on the careful selection of the model size for the auxiliary generator, leading to high trial-and-error costs. In this paper, we present the first systematic study of this problem. Our theoretical investigation highlights the importance of controlling the generator capacity in ELECTRA-style training. Meanwhile, we found it is not handled properly in the original ELECTRA design, leading to the sensitivity issue. Specifically, since adaptive optimizers like Adam will cripple the weighing of individual losses in the joint optimization, the original design fails to control the generator training effectively. To regain control over the generator, we modularize the generator optimization by decoupling the generator optimizer and discriminator optimizer completely, instead of simply relying on the weighted objective combination. Our simple technique reduced the sensitivity of ELECTRA training significantly and obtains considerable performance gain compared to the original design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113540861",
                    "name": "Chengyu Dong"
                },
                {
                    "authorId": "2109392217",
                    "name": "Liyuan Liu"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "46522098",
                    "name": "Xiaodong Liu"
                }
            ]
        },
        {
            "paperId": "2eb617e1242e8f0856f2e7b36073461cb741570d",
            "title": "Semantic-SAM: Segment and Recognize Anything at Any Granularity",
            "abstract": "In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity. Our model offers two key advantages: semantic-awareness and granularity-abundance. To achieve semantic-awareness, we consolidate multiple datasets across three granularities and introduce decoupled classification for objects and parts. This allows our model to capture rich semantic information. For the multi-granularity capability, we propose a multi-choice learning scheme during training, enabling each click to generate masks at multiple levels that correspond to multiple ground-truth masks. Notably, this work represents the first attempt to jointly train a model on SA-1B, generic, and part segmentation datasets. Experimental results and visualizations demonstrate that our model successfully achieves semantic-awareness and granularity-abundance. Furthermore, combining SA-1B training with other segmentation tasks, such as panoptic and part segmentation, leads to performance improvements. We will provide code and a demo for further exploration and evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152978390",
                    "name": "Feng Li"
                },
                {
                    "authorId": "2315254849",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2075416446",
                    "name": "Pei Sun"
                },
                {
                    "authorId": "1882962264",
                    "name": "Xueyan Zou"
                },
                {
                    "authorId": "150301258",
                    "name": "Siyi Liu"
                },
                {
                    "authorId": "120157163",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2109738542",
                    "name": "Chun-yue Li"
                },
                {
                    "authorId": "1452981772",
                    "name": "Lei Zhang"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "34e1a8a75bf6f35084ac6d714a136f39d02c649e",
            "title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
            "abstract": "Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it very efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1395101702",
                    "name": "Zelalem Gero"
                },
                {
                    "authorId": "145229121",
                    "name": "Chandan Singh"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1947267",
                    "name": "Michel Galley"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        }
    ]
}