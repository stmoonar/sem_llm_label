{
    "authorId": "2060638599",
    "papers": [
        {
            "paperId": "3a20d70116ffc4f8edab97b13638771a6f22f9ba",
            "title": "Data poisoning attacks on off-policy policy evaluation methods",
            "abstract": "Off-policy Evaluation (OPE) methods are a crucial tool for evaluating policies in high-stakes domains such as healthcare, where exploration is often infeasible, unethical, or expensive. However, the extent to which such methods can be trusted under adversarial threats to data quality is largely unexplored. In this work, we make the first attempt at investigating the sensitivity of OPE methods to marginal adversarial perturbations to the data. We design a generic data poisoning attack framework leveraging influence functions from robust statistics to carefully construct perturbations that maximize error in the policy value estimates. We carry out extensive experimentation with multiple healthcare and control datasets. Our results demonstrate that many existing OPE methods are highly prone to generating value estimates with large errors when subject to data poisoning attacks, even for small adversarial perturbations. These findings question the reliability of policy values derived using OPE methods and motivate the need for developing OPE methods that are statistically robust to train-time data poisoning attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35597739",
                    "name": "Elita Lobo"
                },
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "2096a2a97b83cf334a280f6f85e9e89d620729ff",
            "title": "Interpretable Causal Inference for Analyzing Wearable, Sensor, and Distributional Data",
            "abstract": "Many modern causal questions ask how treatments affect complex outcomes that are measured using wearable devices and sensors. Current analysis approaches require summarizing these data into scalar statistics (e.g., the mean), but these summaries can be misleading. For example, disparate distributions can have the same means, variances, and other statistics. Researchers can overcome the loss of information by instead representing the data as distributions. We develop an interpretable method for distributional data analysis that ensures trustworthy and robust decision-making: Analyzing Distributional Data via Matching After Learning to Stretch (ADD MALTS). We (i) provide analytical guarantees of the correctness of our estimation strategy, (ii) demonstrate via simulation that ADD MALTS outperforms other distributional data analysis methods at estimating treatment effects, and (iii) illustrate ADD MALTS' ability to verify whether there is enough cohesion between treatment and control units within subpopulations to trustworthily estimate treatment effects. We demonstrate ADD MALTS' utility by studying the effectiveness of continuous glucose monitors in mitigating diabetes risks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "148072390",
                    "name": "Srikar Katta"
                },
                {
                    "authorId": "144881140",
                    "name": "Harsh Parikh"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "6974508",
                    "name": "A. Volfovsky"
                }
            ]
        },
        {
            "paperId": "6464b5c1896852d66c419d9c09e2fbaff007829e",
            "title": "A Double Machine Learning Approach to Combining Experimental and Observational Data",
            "abstract": "Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one of these assumptions is violated, we provide semiparametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. Through comparative analyses, we show our framework's superiority over existing data fusion methods. The practical utility of our approach is further exemplified by three real-world case studies, underscoring its potential for widespread application in empirical research.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Economics"
            ],
            "authors": [
                {
                    "authorId": "72588620",
                    "name": "Marco Morucci"
                },
                {
                    "authorId": "1515535466",
                    "name": "Vittorio Orlandi"
                },
                {
                    "authorId": "144881140",
                    "name": "Harsh Parikh"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "6974508",
                    "name": "A. Volfovsky"
                }
            ]
        },
        {
            "paperId": "8d24ad82a1bfcd6aa15932d604cec4599681e071",
            "title": "Missing Values and Imputation in Healthcare Data: Can Interpretable Machine Learning Help?",
            "abstract": "Missing values are a fundamental problem in data science. Many datasets have missing values that must be properly handled because the way missing values are treated can have large impact on the resulting machine learning model. In medical applications, the consequences may affect healthcare decisions. There are many methods in the literature for dealing with missing values, including state-of-the-art methods which often depend on black-box models for imputation. In this work, we show how recent advances in interpretable machine learning provide a new perspective for understanding and tackling the missing value problem. We propose methods based on high-accuracy glass-box Explainable Boosting Machines (EBMs) that can help users (1) gain new insights on missingness mechanisms and better understand the causes of missingness, and (2) detect -- or even alleviate -- potential risks introduced by imputation algorithms. Experiments on real-world medical datasets illustrate the effectiveness of the proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143786389",
                    "name": "Zhi Chen"
                },
                {
                    "authorId": "145176257",
                    "name": "S. Tan"
                },
                {
                    "authorId": "2335853",
                    "name": "Urszula Chajewska"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "2066161176",
                    "name": "R. Caruana"
                }
            ]
        },
        {
            "paperId": "f7c43ccd2511d215e947f0369472514d2ee592f9",
            "title": "Safe and Interpretable Estimation of Optimal Treatment Regimes",
            "abstract": "Recent statistical and reinforcement learning methods have significantly advanced patient care strategies. However, these approaches face substantial challenges in high-stakes contexts, including missing data, inherent stochasticity, and the critical requirements for interpretability and patient safety. Our work operationalizes a safe and interpretable framework to identify optimal treatment regimes. This approach involves matching patients with similar medical and pharmacological characteristics, allowing us to construct an optimal policy via interpolation. We perform a comprehensive simulation study to demonstrate the framework's ability to identify optimal policies even in complex settings. Ultimately, we operationalize our approach to study regimes for treating seizures in critically ill patients. Our findings strongly support personalized treatment strategies based on a patient's medical history and pharmacological features. Notably, we identify that reducing medication doses for patients with mild and brief seizure episodes while adopting aggressive treatment for patients in intensive care unit experiencing intense seizures leads to more favorable outcomes.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144881140",
                    "name": "Harsh Parikh"
                },
                {
                    "authorId": "107613423",
                    "name": "Quinn Lanners"
                },
                {
                    "authorId": "15016330",
                    "name": "Zade Akras"
                },
                {
                    "authorId": "2246993089",
                    "name": "Sahar F. Zafar"
                },
                {
                    "authorId": "144293787",
                    "name": "M. Westover"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "6974508",
                    "name": "A. Volfovsky"
                }
            ]
        },
        {
            "paperId": "30adb68b84e6974027610aecced25fc62a825ab1",
            "title": "Why Interpretable Causal Inference is Important for High-Stakes Decision Making for Critically Ill Patients and How To Do It",
            "abstract": "the effects of and They there are patients for high-dimensional observational causal inference analysis, and randomized controlled trials cannot ethically be conducted. However, mechanistic knowledge is available, including how drugs are absorbed into the body, and the combination of this knowledge with the limited data could potentially suf\ufb01ce \u2013 if we knew how to combine them. In this work, we present a framework for interpretable estimation of causal effects for critically ill patients under exactly these complex conditions: interactions between drugs and observations over time, patient data sets that are not large, and mechanistic knowledge that can substitute for lack of data. Our framework incorporates pharmacokinetics and pharmacodynamics with interpretable matching methods to adjust for confounders such as patients\u2019 drug response, medical history, and demographic variables. We apply this framework to an extremely important problem affecting critically ill patients, namely the effect of seizures and other potentially harmful electrical events in the brain (called epileptiform activity \u2013 EA) on outcomes. EA is a key indicator of whether the patient will suffer long term severe neurological disability or death. Given the high-stakes involved, and the high noise in the data, interpretability is critical for troubleshooting such complex problems. Interpretablity of our matched groups allowed neurologists to perform chart review to verify the quality of our causal analysis. For instance, our work indicates that a patient who experiences a high level of seizure-like activity (75% high EA burden), and is untreated for a six-hour window, has, on average, a 16.7% increased chance of adverse outcomes such as severe brain damage, lifetime disability, or death. We were also able to show that patients with mild-but-long-lasting EA (average EA burden \u2265 50%) have their risk of an adverse outcome increased by 11.2%. This information is essential to any neurologist who treats critically-ill patients.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8458200",
                    "name": "Harshel G. Parikh"
                },
                {
                    "authorId": "2054927173",
                    "name": "Kentaro Hoffman"
                },
                {
                    "authorId": "3286128",
                    "name": "Haoqi Sun"
                },
                {
                    "authorId": "3223669",
                    "name": "Wendong Ge"
                },
                {
                    "authorId": "47684945",
                    "name": "J. Jing"
                },
                {
                    "authorId": "40948549",
                    "name": "Rajesh Amerineni"
                },
                {
                    "authorId": "2146017696",
                    "name": "Lin Liu"
                },
                {
                    "authorId": "49991208",
                    "name": "Jimeng Sun"
                },
                {
                    "authorId": "5826067",
                    "name": "Sahar F. Zafar"
                },
                {
                    "authorId": "4008147",
                    "name": "A. Struck"
                },
                {
                    "authorId": "6974508",
                    "name": "A. Volfovsky"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "144293787",
                    "name": "M. Westover"
                }
            ]
        },
        {
            "paperId": "0233ac5fe6ce3d6327dfbc8edc577a36f7800cd4",
            "title": "O-098 Embryo selection using Artificial Intelligence (AI): Epistemic and ethical considerations",
            "abstract": "\n \n \n What are the epistemic and ethical considerations of clinically implementing Artificial Intelligence (AI) algorithms in embryo selection?\n \n \n \n AI embryo selection algorithms used to date are \u201cblack-box\u201d models with significant epistemic and ethical issues, and there are no trials assessing their clinical effectiveness.\n \n \n \n The innovation of time-lapse imaging offers the potential to generate vast quantities of data for embryo assessment. Computer Vision allows image data to be analysed using algorithms developed via machine learning which learn and adapt as they are exposed to more data. Most algorithms are developed using neural networks and are uninterpretable (or \u201cblack box\u201d). Uninterpretable models are either too complicated to understand or proprietary, in which case comprehension is impossible for outsiders. In the IVF context, these outsiders include doctors, embryologists and patients, which raises ethical questions for its use in embryo selection.\n \n \n \n We performed a scoping review of articles evaluating AI for embryo selection in IVF. We considered the epistemic and ethical implications of current approaches.\n \n \n \n We searched Medline, Embase, ClinicalTrials.gov and the EU Clinical Trials Register for full text papers evaluating AI for embryo selection using the following key words: artificial intelligence* OR AI OR neural network* OR machine learning OR support vector machine OR automatic classification AND IVF OR in vitro fertilisation OR embryo*, as well as relevant MeSH and Emtree terms for Medline and Embase respectively.\n \n \n \n We found no trials evaluating clinical effectiveness either published or registered. We found efficacy studies which looked at 2 types of outcomes \u2013 accuracy for predicting pregnancy or live birth and agreement with embryologist evaluation. Some algorithms were shown to broadly differentiate well between \u201cgood-\u201d and \u201cpoor-\u201d quality embryos but not between embryos of similar quality, which is the clinical need. Almost universally, the AI models were opaque (\u201cblack box\u201d) in that at least some part of the process was uninterpretable.\n \u201cBlack box\u201d models are problematic for epistemic and ethical reasons.\n Epistemic concerns include information asymmetries between algorithm developers and doctors, embryologists and patients; the risk of biased prediction caused by known and/or unknown confounders during the training process; difficulties in real-time error checking due to limited interpretability; the economics of buying into commercial proprietary models, brittle to variation in the treatment process; and an overall difficulty troubleshooting.\n Ethical pitfalls include the risk of misrepresenting patient values; concern for the health and well-being of future children; the risk of disvaluing disability; possible societal implications; and a responsibility gap, in the event of adverse events.\n \n \n \n Our search was limited to the two main medical research databases. Although we checked article references for more publications, we were less likely to identify studies that were not indexed in Medline or Embase, especially if they were not cited in studies identified in our search.\n \n \n \n It is premature to implement AI for embryo selection outside of a clinical trial. AI for embryo selection is potentially useful, but must be done carefully and transparently, as the epistemic and ethical issues are significant. We advocate for the use of interpretable AI models to overcome these issues.\n \n \n \n not applicable\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "81436719",
                    "name": "M. Afnan"
                },
                {
                    "authorId": "2668536",
                    "name": "Yanhe Liu"
                },
                {
                    "authorId": "2146849053",
                    "name": "Vincent Conitzer"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "2052067082",
                    "name": "Abhishek Mishra"
                },
                {
                    "authorId": "4159060",
                    "name": "J. Savulescu"
                }
            ]
        },
        {
            "paperId": "3b991488f8c412b279cd11dcce7fbd5d95b3a6aa",
            "title": "Ethical Implementation of Artificial Intelligence to Select Embryos in In Vitro Fertilization",
            "abstract": "AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "81436719",
                    "name": "M. Afnan"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "1749906",
                    "name": "Vincent Conitzer"
                },
                {
                    "authorId": "4159060",
                    "name": "J. Savulescu"
                },
                {
                    "authorId": "2052067082",
                    "name": "Abhishek Mishra"
                },
                {
                    "authorId": "2668536",
                    "name": "Yanhe Liu"
                },
                {
                    "authorId": "4710281",
                    "name": "M. Afnan"
                }
            ]
        }
    ]
}