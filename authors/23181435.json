{
    "authorId": "23181435",
    "papers": [
        {
            "paperId": "122dbf4c968a448fb2dde6c5c4a10f2697364531",
            "title": "ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media",
            "abstract": "Considerable advancements have been made to tackle the misrepresentation of information derived from reference articles in the domains of fact-checking and faithful summarization. However, an unaddressed aspect remains - the identification of social media posts that manipulate information within associated news articles. This task presents a significant challenge, primarily due to the prevalence of personal opinions in such posts. We present a novel task, identifying manipulation of news on social media, which aims to detect manipulation in social media posts and identify manipulated or inserted information. To study this task, we have proposed a data collection schema and curated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and corresponding articles. Our analysis demonstrates that this task is highly challenging, with large language models (LLMs) yielding unsatisfactory performance. Additionally, we have developed a simple yet effective basic model that outperforms LLMs significantly on the ManiTweet dataset. Finally, we have conducted an exploratory analysis of human-written tweets, unveiling intriguing connections between manipulation and the domain and factuality of news articles, as well as revealing that manipulated sentences are more likely to encapsulate the main story or consequences of a news outlet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1420116116",
                    "name": "Kung-Hsiang Huang"
                },
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "145590324",
                    "name": "K. McKeown"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "43971299f110516dea7715da2fcb4eec33f5284c",
            "title": "PDSum: Prototype-driven Continuous Summarization of Evolving Multi-document Sets Stream",
            "abstract": "Summarizing text-rich documents has been long studied in the literature, but most of the existing efforts have been made to summarize a static and predefined multi-document set. With the rapid development of online platforms for generating and distributing text-rich documents, there arises an urgent need for continuously summarizing dynamically evolving multi-document sets where the composition of documents and sets is changing over time. This is especially challenging as the summarization should be not only effective in incorporating relevant, novel, and distinctive information from each concurrent multi-document set, but also efficient in serving online applications. In this work, we propose a new summarization problem, Evolving Multi-Document sets stream Summarization (EMDS), and introduce a novel unsupervised algorithm PDSum with the idea of prototype-driven continuous summarization. PDSum builds a lightweight prototype of each multi-document set and exploits it to adapt to new documents while preserving accumulated knowledge from previous documents. To update new summaries, the most representative sentences for each multi-document set are extracted by measuring their similarities to the prototypes. A thorough evaluation with real multi-document sets streams demonstrates that PDSum outperforms state-of-the-art unsupervised multi-document summarization algorithms in EMDS in terms of relevance, novelty, and distinctiveness and is also robust to various evaluation settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3396235",
                    "name": "Susik Yoon"
                },
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "5c0148e2ee4efc33731335fac26780a3fb79e16c",
            "title": "Zero-shot Faithful Factual Error Correction",
            "abstract": "Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans\u2019 ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. Our zero-shot framework outperforms fully-supervised approaches, as demonstrated by experiments on the FEVER and SciFact datasets, where our outputs are shown to be more faithful. More importantly, the decomposability nature of our framework inherently provides interpretability. Additionally, to reveal the most suitable metrics for evaluating factual error corrections, we analyze the correlation between commonly used metrics with human judgments in terms of three different dimensions regarding intelligibility and faithfulness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1420116116",
                    "name": "Kung-Hsiang Huang"
                },
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "6532fff00ca6ebeee71069f7cbcd9fbc96114fc2",
            "title": "Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization",
            "abstract": "Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data. In this work, we propose TempoSum, a novel benchmark that contains data samples from 2010 to 2022, to understand the temporal generalization ability of abstractive summarization models. Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Moreover, existing faithfulness enhancement methods cannot reliably improve the faithfulness of summarization models on future data. Finally, we discuss several recommendations to the research community on how to evaluate and improve the temporal generalization capability of text summarization models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "114557127",
                    "name": "C. Cheang"
                },
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "1758353",
                    "name": "Derek F. Wong"
                },
                {
                    "authorId": "1390611971",
                    "name": "Xuebo Liu"
                },
                {
                    "authorId": "2186153870",
                    "name": "Zhao Li"
                },
                {
                    "authorId": "2145377854",
                    "name": "Yanming Sun"
                },
                {
                    "authorId": "2007543818",
                    "name": "Shudong Liu"
                },
                {
                    "authorId": "1774304",
                    "name": "Lidia S. Chao"
                }
            ]
        },
        {
            "paperId": "9429398c52f712e3cd9a92c83bcc7f37c1bf1378",
            "title": "Measuring the Effect of Influential Messages on Varying Personas",
            "abstract": "Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. We further evaluate the SOTA neural language models with our dataset. The empirical results suggest that the included persona attributes are helpful for the performance of all response dimensions. Our analysis shows that the best-performing models are capable of predicting responses that are consistent with the personas, and as a byproduct, the task formulation also enables many interesting applications in the analysis of social network groups and their opinions, such as the discovery of extreme opinion groups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118831287",
                    "name": "Chenkai Sun"
                },
                {
                    "authorId": "2109377974",
                    "name": "Jinning Li"
                },
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "1736467",
                    "name": "ChengXiang Zhai"
                },
                {
                    "authorId": "2072976571",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "b5e78cf49a93bba76981ad16ca040da62d3fa72b",
            "title": "Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization",
            "abstract": "Existing factual consistency evaluation approaches for text summarization provide binary predictions and limited insights into the weakness of summarization systems. Therefore, we propose the task of fine-grained inconsistency detection, the goal of which is to predict the fine-grained types of factual errors in a summary. Motivated by how humans inspect factual inconsistency in summaries, we propose an interpretable fine-grained inconsistency detection model, FineGrainFact, which explicitly represents the facts in the documents and summaries with semantic frames extracted by semantic role labeling, and highlights the related semantic frames to predict inconsistency. The highlighted semantic frames help verify predicted error types and correct inconsistent summaries. Experiment results demonstrate that our model outperforms strong baselines and provides evidence to support or refute the summary.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "145653969",
                    "name": "Qi Zeng"
                },
                {
                    "authorId": "2072975828",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "e5641c189e1c4bae73308fadd88839ffa73f382b",
            "title": "Scientific Opinion Summarization: Paper Meta-review Generation Dataset, Methods, and Evaluation",
            "abstract": "Opinions in scientific research papers can be divergent, leading to controversies among reviewers. However, most existing datasets for opinion summarization are centered around product reviews and assume that the analyzed opinions are non-controversial, failing to account for the variability seen in other contexts such as academic papers, political debates, or social media discussions. To address this gap, we propose the task of scientific opinion summarization, where research paper reviews are synthesized into meta-reviews. To facilitate this task, we introduce the ORSUM dataset covering 15,062 paper meta-reviews and 57,536 paper reviews from 47 conferences. Furthermore, we propose the Checklist-guided Iterative Introspection approach, which breaks down scientific opinion summarization into several stages, iteratively refining the summary under the guidance of questions from a checklist. Our experiments show that (1) human-written summaries do not always satisfy all necessary criteria such as depth of discussion, and identifying consensus and controversy for the specific domain, and (2) the combination of task decomposition and iterative self-refinement shows strong potential for enhancing the opinions and can be applied to other complex text generation using black-box LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145653969",
                    "name": "Qi Zeng"
                },
                {
                    "authorId": "2192788718",
                    "name": "Mankeerat Sidhu"
                },
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "2153516659",
                    "name": "Lu Wang"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "3a356a3a48b952837c79c229531ad93a72bc62df",
            "title": "SumREN: Summarizing Reported Speech about Events in News",
            "abstract": "A primary objective of news articles is to establish the factual record for an event, frequently achieved by conveying both the details of the specified event (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and how people reacted to it (i.e., reported statements). However, existing work on news summarization almost exclusively focuses on the event details. In this work, we propose the novel task of summarizing the reactions of different speakers, as expressed by their reported statements, to a given event. To this end, we create a new multi-document summarization benchmark, SumREN, comprising 745 summaries of reported statements from various public figures obtained from 633 news articles discussing 132 events. We propose an automatic silver-training data generation approach for our task, which helps smaller models like BART achieve GPT-3 level performance on this task. Finally, we introduce a pipeline-based framework for summarizing reported speech, which we empirically show to generate summaries that are more abstractive and factual than baseline query-focused summarization approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113583612",
                    "name": "R. Reddy"
                },
                {
                    "authorId": "3053247",
                    "name": "Heba Elfardy"
                },
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "50044599",
                    "name": "Kevin Small"
                },
                {
                    "authorId": "2072975828",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "77e0f1461cb82a1ef6c91d82af395642baa6d18c",
            "title": "Grounding Commands for Autonomous Vehicles via Layer Fusion with Region-specific Dynamic Layer Attention",
            "abstract": "Grounding a command to the visual environment is an essential ingredient for interactions between autonomous vehicles and humans. In this work, we study the problem of language grounding for autonomous vehicles, which aims to localize a region in a visual scene according to a natural language command from a passenger. Prior work only employs the top layer representations of a vision-and-language pretrained model to predict the region referred to by the command. However, such a method omits the useful features encoded in other layers, and thus results in inadequate understanding of the input scene and command. To tackle this limitation, we present the first layer fusion approach for this task. Since different visual regions may require distinct types of features to disambiguate them from each other, we further propose the region-specific dynamic (RSD) layer attention to adaptively fuse the multimodal information across layers for each region. Extensive experiments on the Talk2Car benchmark demonstrate that our approach helps predict more accurate regions and outperforms state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "30453927",
                    "name": "M. Guo"
                },
                {
                    "authorId": "3008909",
                    "name": "Chengguang Xu"
                }
            ]
        },
        {
            "paperId": "80217f99eefaca813adcffec6968bcd28a66b8bd",
            "title": "PLANET: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation",
            "abstract": "Despite recent progress of pre-trained language models on generating fluent text, existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow. In this work, we propose PLANET, a novel generation framework leveraging autoregressive self-attention mechanism to conduct content planning and surface realization dynamically. To guide the generation of output sentences, our framework enriches the Transformer decoder with latent representations to maintain sentence-level semantic plans grounded by bag-of-words. Moreover, we introduce a new coherence-based contrastive learning objective to further improve the coherence of output. Extensive experiments are conducted on two challenging long-form text generation tasks including counterargument generation and opinion article generation. Both automatic and human evaluations show that our method significantly outperforms strong baselines and generates more coherent texts with richer contents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111374759",
                    "name": "Zhe Hu"
                },
                {
                    "authorId": "23181435",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": null,
                    "name": "Jiachen Liu"
                },
                {
                    "authorId": "2107521158",
                    "name": "Xinyan Xiao"
                },
                {
                    "authorId": "2149181702",
                    "name": "Hua Wu"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        }
    ]
}