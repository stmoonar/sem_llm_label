{
    "authorId": "2139143386",
    "papers": [
        {
            "paperId": "1de16e78babf5e5928c0e4cf7da3c7fc29e25551",
            "title": "Efficient Time-Series Clustering through Sparse Gaussian Modeling",
            "abstract": "In this work, we consider the problem of shape-based time-series clustering with the widely used Dynamic Time Warping (DTW) distance. We present a novel two-stage framework based on Sparse Gaussian Modeling. In the first stage, we apply Sparse Gaussian Process Regression and obtain a sparse representation of each time series in the dataset with a logarithmic (in the original length T) number of inducing data points. In the second stage, we apply k-means with DTW Barycentric Averaging (DBA) to the sparsified dataset using a generalization of DTW, which accounts for the fact that each inducing point serves as a representative of many original data points. The asymptotic running time of our Sparse Time-Series Clustering framework is \u03a9(T2/log2T) times faster than the running time of applying k-means to the original dataset because sparsification reduces the running time of DTW from \u0398(T2) to \u0398(log2T). Moreover, sparsification tends to smoothen outliers and particularly noisy parts of the original time series. We conduct an extensive experimental evaluation using datasets from the UCR Time-Series Classification Archive, showing that the quality of clustering computed by our Sparse Time-Series Clustering framework is comparable to the clustering computed by the standard k-means algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273195757",
                    "name": "Dimitris Fotakis"
                },
                {
                    "authorId": "68973905",
                    "name": "Panagiotis Patsilinakos"
                },
                {
                    "authorId": "2139143386",
                    "name": "Eleni Psaroudaki"
                },
                {
                    "authorId": "2185714704",
                    "name": "Michalis Xefteris"
                }
            ]
        },
        {
            "paperId": "7442bc53a1bb7916b33b7ea3323084b836ed04b7",
            "title": "GLANCE: Global Actions in a Nutshell for Counterfactual Explainability",
            "abstract": "Counterfactual explanations have emerged as an important tool to understand, debug, and audit complex machine learning models. To offer global counterfactual explainability, state-of-the-art methods construct summaries of local explanations, offering a trade-off among conciseness, counterfactual effectiveness, and counterfactual cost or burden imposed on instances. In this work, we provide a concise formulation of the problem of identifying global counterfactuals and establish principled criteria for comparing solutions, drawing inspiration from Pareto dominance. We introduce innovative algorithms designed to address the challenge of finding global counterfactuals for either the entire input space or specific partitions, employing clustering and decision trees as key components. Additionally, we conduct a comprehensive experimental evaluation, considering various instances of the problem and comparing our proposed algorithms with state-of-the-art methods. The results highlight the consistent capability of our algorithms to generate meaningful and interpretable global counterfactual explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298897005",
                    "name": "Ioannis Emiris"
                },
                {
                    "authorId": "2273195757",
                    "name": "Dimitris Fotakis"
                },
                {
                    "authorId": "40397337",
                    "name": "G. Giannopoulos"
                },
                {
                    "authorId": "2303655763",
                    "name": "Dimitrios Gunopulos"
                },
                {
                    "authorId": "3434260",
                    "name": "Loukas Kavouras"
                },
                {
                    "authorId": "2297771041",
                    "name": "Kleopatra Markou"
                },
                {
                    "authorId": "2139143386",
                    "name": "Eleni Psaroudaki"
                },
                {
                    "authorId": "150081157",
                    "name": "D. Rontogiannis"
                },
                {
                    "authorId": "1760642",
                    "name": "Dimitris Sacharidis"
                },
                {
                    "authorId": "2220631614",
                    "name": "Nikolaos Theologitis"
                },
                {
                    "authorId": "2082489302",
                    "name": "Dimitrios Tomaras"
                },
                {
                    "authorId": "2220631616",
                    "name": "Konstantinos Tsopelas"
                }
            ]
        },
        {
            "paperId": "f3586434ae779e452416e9035fdc23c9000e39cc",
            "title": "Fairness in Ranking: Robustness through Randomization without the Protected Attribute",
            "abstract": "There has been great interest in fairness in machine learning, especially in relation to classification problems. In ranking-related problems, such as in online advertising, recommender systems, and HR automation, much work on fairness remains to be done. Two complications arise: first, the protected attribute may not be available in many applications. Second, there are multiple measures of fairness of rankings, and optimization-based methods utilizing a single measure of fairness of rankings may produce rankings that are unfair with respect to other measures. In this work, we propose a randomized method for post-processing rankings, which do not require the availability of the protected attribute. In an extensive numerical study, we show the robustness of our methods with respect to P-Fairness and effectiveness with respect to Normalized Discounted Cumulative Gain (NDCG) from the baseline ranking, improving on previously proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293722300",
                    "name": "Andrii Kliachkin"
                },
                {
                    "authorId": "2139143386",
                    "name": "Eleni Psaroudaki"
                },
                {
                    "authorId": "2298966095",
                    "name": "Jakub Marecek"
                },
                {
                    "authorId": "2273195757",
                    "name": "Dimitris Fotakis"
                }
            ]
        },
        {
            "paperId": "4b531e2e65ae2f8e989342fbe8e41d2822584be8",
            "title": "Fairness Aware Counterfactuals for Subgroups",
            "abstract": "In this work, we present Fairness Aware Counterfactuals for Subgroups (FACTS), a framework for auditing subgroup fairness through counterfactual explanations. We start with revisiting (and generalizing) existing notions and introducing new, more refined notions of subgroup fairness. We aim to (a) formulate different aspects of the difficulty of individuals in certain subgroups to achieve recourse, i.e. receive the desired outcome, either at the micro level, considering members of the subgroup individually, or at the macro level, considering the subgroup as a whole, and (b) introduce notions of subgroup fairness that are robust, if not totally oblivious, to the cost of achieving recourse. We accompany these notions with an efficient, model-agnostic, highly parameterizable, and explainable framework for evaluating subgroup fairness. We demonstrate the advantages, the wide applicability, and the efficiency of our approach through a thorough experimental evaluation of different benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3434260",
                    "name": "Loukas Kavouras"
                },
                {
                    "authorId": "2220631616",
                    "name": "Konstantinos Tsopelas"
                },
                {
                    "authorId": "40397337",
                    "name": "G. Giannopoulos"
                },
                {
                    "authorId": "1760642",
                    "name": "Dimitris Sacharidis"
                },
                {
                    "authorId": "2139143386",
                    "name": "Eleni Psaroudaki"
                },
                {
                    "authorId": "2220631614",
                    "name": "Nikolaos Theologitis"
                },
                {
                    "authorId": "150081157",
                    "name": "D. Rontogiannis"
                },
                {
                    "authorId": "143995221",
                    "name": "Dimitris Fotakis"
                },
                {
                    "authorId": "1804042",
                    "name": "I. Emiris"
                }
            ]
        },
        {
            "paperId": "5234bf3c06826dc294a381ff9e6b2c695b7620ab",
            "title": "Label Ranking through Nonparametric Regression",
            "abstract": "Label Ranking (LR) corresponds to the problem of learning a hypothesis that maps features to rankings over a \ufb01nite set of labels. We adopt a nonparametric regression approach to LR and obtain theoretical performance guarantees for this fundamental practical problem. We introduce a generative model for Label Ranking, in noiseless and noisy nonparametric regression settings, and provide sample complexity bounds for learning algorithms in both cases. In the noiseless setting, we study the LR problem with full rankings and provide computationally ef\ufb01cient algorithms using decision trees and random forests in the high-dimensional regime. In the noisy setting, we consider the more general cases of LR with incomplete and partial rankings from a statistical viewpoint and obtain sample complexity bounds using the One-Versus-One approach of multiclass classi\ufb01cation. Finally, we complement our theoretical contributions with experiments, aiming to understand how the input regression noise affects the observed output.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143995221",
                    "name": "Dimitris Fotakis"
                },
                {
                    "authorId": "1796255018",
                    "name": "Alkis Kalavasis"
                },
                {
                    "authorId": "2139143386",
                    "name": "Eleni Psaroudaki"
                }
            ]
        }
    ]
}