{
    "authorId": "2117075272",
    "papers": [
        {
            "paperId": "230f56377100ae1dc433c72c5ca96b1467a04612",
            "title": "GraphRCG: Self-Conditioned Graph Generation",
            "abstract": "Graph generation generally aims to create new graphs that closely align with a specific graph distribution. Existing works often implicitly capture this distribution through the optimization of generators, potentially overlooking the intricacies of the distribution itself. Furthermore, these approaches generally neglect the insights offered by the learned distribution for graph generation. In contrast, in this work, we propose a novel self-conditioned graph generation framework designed to explicitly model graph distributions and employ these distributions to guide the generation process. We first perform self-conditioned modeling to capture the graph distributions by transforming each graph sample into a low-dimensional representation and optimizing a representation generator to create new representations reflective of the learned distribution. Subsequently, we leverage these bootstrapped representations as self-conditioned guidance for the generation process, thereby facilitating the generation of graphs that more accurately reflect the learned distributions. We conduct extensive experiments on generic and molecular graph datasets across various fields. Our framework demonstrates superior performance over existing state-of-the-art graph generation methods in terms of graph quality and fidelity to training data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2264369617",
                    "name": "Zhen Tan"
                },
                {
                    "authorId": "2290239259",
                    "name": "Xinyu Zhao"
                },
                {
                    "authorId": "2276535128",
                    "name": "Tianlong Chen"
                },
                {
                    "authorId": "2258336609",
                    "name": "Huan Liu"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "2df14dafc8486ff51575f8327159da1a021054b5",
            "title": "Large Language Models for Data Annotation: A Survey",
            "abstract": "Data annotation generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to automate the complicated process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, we uniquely focus on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment, and LLM-Generated Annotations Utilization. Furthermore, this survey includes an in-depth taxonomy of data types that LLMs can annotate, a comprehensive review of learning strategies for models utilizing LLM-generated annotations, and a detailed discussion of the primary challenges and limitations associated with using LLMs for data annotation. Serving as a key guide, this survey aims to assist researchers and practitioners in exploring the potential of the latest LLMs for data annotation, thereby fostering future advancements in this critical field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2264369617",
                    "name": "Zhen Tan"
                },
                {
                    "authorId": "2161635474",
                    "name": "Dawei Li"
                },
                {
                    "authorId": "2284862132",
                    "name": "Alimohammad Beigi"
                },
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2257208347",
                    "name": "Ruocheng Guo"
                },
                {
                    "authorId": "2144903149",
                    "name": "Amrita Bhattacharjee"
                },
                {
                    "authorId": "2036355404",
                    "name": "Bohan Jiang"
                },
                {
                    "authorId": "145084368",
                    "name": "Mansooreh Karami"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                },
                {
                    "authorId": "2140175677",
                    "name": "Lu Cheng"
                },
                {
                    "authorId": "2258336609",
                    "name": "Huan Liu"
                }
            ]
        },
        {
            "paperId": "31c43f4d3ba6ea711b613dcd792cd62c13acf2d2",
            "title": "A Benchmark for Fairness-Aware Graph Learning",
            "abstract": "Fairness-aware graph learning has gained increasing attention in recent years. Nevertheless, there lacks a comprehensive benchmark to evaluate and compare different fairness-aware graph learning methods, which blocks practitioners from choosing appropriate ones for broader real-world applications. In this paper, we present an extensive benchmark on ten representative fairness-aware graph learning methods. Specifically, we design a systematic evaluation protocol and conduct experiments on seven real-world datasets to evaluate these methods from multiple perspectives, including group fairness, individual fairness, the balance between different fairness criteria, and computational efficiency. Our in-depth analysis reveals key insights into the strengths and limitations of existing methods. Additionally, we provide practical guidance for applying fairness-aware graph learning methods in applications. To the best of our knowledge, this work serves as an initial step towards comprehensively understanding representative fairness-aware graph learning methods to facilitate future advancements in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123918726",
                    "name": "Yushun Dong"
                },
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2301468051",
                    "name": "Zhenyu Lei"
                },
                {
                    "authorId": "2262086932",
                    "name": "Zaiyi Zheng"
                },
                {
                    "authorId": "2273765881",
                    "name": "Jing Ma"
                },
                {
                    "authorId": "2127380428",
                    "name": "Chen Chen"
                },
                {
                    "authorId": "2290320001",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "3d3095e5a4e1a8f0e0a94feecf6e551bb97df93f",
            "title": "Few-shot Knowledge Graph Relational Reasoning via Subgraph Adaptation",
            "abstract": "Few-shot Knowledge Graph (KG) Relational Reasoning aims to predict unseen triplets (i.e., query triplets) for rare relations in KGs, given only several triplets of these relations as references (i.e., support triplets). This task has gained significant traction due to the widespread use of knowledge graphs in various natural language processing applications. Previous approaches have utilized meta-training methods and manually constructed meta-relation sets to tackle this task. Recent efforts have focused on edge-mask-based methods, which exploit the structure of the contextualized graphs of target triplets (i.e., a subgraph containing relevant triplets in the KG). However, existing edge-mask-based methods have limitations in extracting insufficient information from KG and are highly influenced by spurious information in KG. To overcome these challenges, we propose SAFER (Subgraph Adaptation for Few-shot Relational Reasoning), a novel approach that effectively adapts the information in contextualized graphs to various subgraphs generated from support and query triplets to perform the prediction. Specifically, SAFER enables the extraction of more comprehensive information from support triplets while minimizing the impact of spurious information when predicting query triplets. Experimental results on three prevalent datasets demonstrate the superiority of our proposed framework SAFER.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261678191",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2127380597",
                    "name": "Chen Chen"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "3dea23e10eeff848f7352b17bbc1fdce38112acc",
            "title": "Knowledge Graph-Enhanced Large Language Models via Path Selection",
            "abstract": "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261678191",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "123918726",
                    "name": "Yushun Dong"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "3e56598ae064345a9689ab39ac0c65b208afc3f4",
            "title": "PyGDebias: A Python Library for Debiasing in Graph Learning",
            "abstract": "Graph-structured data is ubiquitous among a plethora of real-world applications. However, as graph learning algorithms have been increasingly deployed to help decision-making, there has been rising societal concern in the bias these algorithms may exhibit. In certain high-stake decision-making scenarios, the decisions made may be life-changing for the involved individuals. Accordingly, abundant explorations have been made to mitigate the bias for graph learning algorithms in recent years. However, there still lacks a library to collectively consolidate existing debiasing techniques and help practitioners to easily perform bias mitigation for graph learning algorithms. In this paper, we present PyGDebias, an open-source Python library for bias mitigation in graph learning algorithms. As the first comprehensive library of its kind, PyGDebias covers 13 popular debiasing methods under common fairness notions together with 26 commonly used graph datasets. In addition, PyGDebias also comes with comprehensive performance benchmarks and well-documented API designs for both researchers and practitioners. To foster convenient accessibility, PyGDebias is released under a permissive BSD-license together with performance benchmarks, API documentation, and use examples at https://github.com/yushundong/PyGDebias.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123918726",
                    "name": "Yushun Dong"
                },
                {
                    "authorId": "2301468051",
                    "name": "Zhenyu Lei"
                },
                {
                    "authorId": "2262086932",
                    "name": "Zaiyi Zheng"
                },
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2273765881",
                    "name": "Jing Ma"
                },
                {
                    "authorId": "2301215253",
                    "name": "Alex Jing Huang"
                },
                {
                    "authorId": "2127380428",
                    "name": "Chen Chen"
                },
                {
                    "authorId": "1737121128",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "50d65e1bf5cee30b24c3872afc6aabae87f44e66",
            "title": "\"Glue pizza and eat rocks\" - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models",
            "abstract": "Retrieval-Augmented Generative (RAG) models enhance Large Language Models (LLMs) by integrating external knowledge bases, improving their performance in applications like fact-checking and information searching. In this paper, we demonstrate a security threat where adversaries can exploit the openness of these knowledge bases by injecting deceptive content into the retrieval database, intentionally changing the model's behavior. This threat is critical as it mirrors real-world usage scenarios where RAG systems interact with publicly accessible knowledge bases, such as web scrapings and user-contributed data pools. To be more realistic, we target a realistic setting where the adversary has no knowledge of users' queries, knowledge base data, and the LLM parameters. We demonstrate that it is possible to exploit the model successfully through crafted content uploads with access to the retriever. Our findings emphasize an urgent need for security measures in the design and deployment of RAG systems to prevent potential manipulation and ensure the integrity of machine-generated content.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2264369617",
                    "name": "Zhen Tan"
                },
                {
                    "authorId": "2038670428",
                    "name": "Chengshuai Zhao"
                },
                {
                    "authorId": "11064745",
                    "name": "Raha Moraffah"
                },
                {
                    "authorId": "2267393864",
                    "name": "Yifan Li"
                },
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                },
                {
                    "authorId": "2276535128",
                    "name": "Tianlong Chen"
                },
                {
                    "authorId": "2258336609",
                    "name": "Huan Liu"
                }
            ]
        },
        {
            "paperId": "6090b400fff46f14e1062dc12953b4b1837db494",
            "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
            "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Benchmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive evaluation strategy for the bias in LLMs. Our experiments demonstrate that the levels of bias vary across these dimensions, thereby providing guidance for the development of specific bias mitigation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2309310448",
                    "name": "Peng Wang"
                },
                {
                    "authorId": "2310189642",
                    "name": "Tong Zhou"
                },
                {
                    "authorId": "123918726",
                    "name": "Yushun Dong"
                },
                {
                    "authorId": "2309805899",
                    "name": "Zhen Tan"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "827b1fcc89920d5e8efa75f513355508596d2112",
            "title": "Understanding and Modeling Job Marketplace with Pretrained Language Models",
            "abstract": "Job marketplace is a heterogeneous graph composed of interactions among members (job-seekers), companies, and jobs. Understanding and modeling job marketplace can benefit both job seekers and employers, ultimately contributing to the greater good of the society. However, existing graph neural network (GNN)-based methods have shallow understandings of the associated textual features and heterogeneous relations. To address the above challenges, we propose PLM4Job, a job marketplace foundation model that tightly couples pretrained language models (PLM) with job market graph, aiming to fully utilize the pretrained knowledge and reasoning ability to model member/job textual features as well as various member-job relations simultaneously. In the pretraining phase, we propose a heterogeneous ego-graph-based prompting strategy to model and aggregate member/job textual features based on the topological structure around the target member/job node, where entity type embeddings and graph positional embeddings are introduced accordingly to model different entities and their heterogeneous relations. Meanwhile, a proximity-aware attention alignment strategy is designed to dynamically adjust the attention of the PLM on ego-graph node tokens in the prompt, such that the attention can be better aligned with job marketplace semantics. Extensive experiments at LinkedIn demonstrate the effectiveness of PLM4Job.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2264480350",
                    "name": "Liang Wu"
                },
                {
                    "authorId": "2134483590",
                    "name": "Binchi Zhang"
                },
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2170992709",
                    "name": "Qilnli Guo"
                },
                {
                    "authorId": "2264620213",
                    "name": "Liangjie Hong"
                },
                {
                    "authorId": "2284861972",
                    "name": "Luke Simon"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "8a29eaf87faaf27f712c1bcb4203565d0baeb4a1",
            "title": "Safety in Graph Machine Learning: Threats and Safeguards",
            "abstract": "Graph Machine Learning (Graph ML) has witnessed substantial advancements in recent years. With their remarkable ability to process graph-structured data, Graph ML techniques have been extensively utilized across diverse applications, including critical domains like finance, healthcare, and transportation. Despite their societal benefits, recent research highlights significant safety concerns associated with the widespread use of Graph ML models. Lacking safety-focused designs, these models can produce unreliable predictions, demonstrate poor generalizability, and compromise data confidentiality. In high-stakes scenarios such as financial fraud detection, these vulnerabilities could jeopardize both individuals and society at large. Therefore, it is imperative to prioritize the development of safety-oriented Graph ML models to mitigate these risks and enhance public confidence in their applications. In this survey paper, we explore three critical aspects vital for enhancing safety in Graph ML: reliability, generalizability, and confidentiality. We categorize and analyze threats to each aspect under three headings: model threats, data threats, and attack threats. This novel taxonomy guides our review of effective strategies to protect against these threats. Our systematic review lays a groundwork for future research aimed at developing practical, safety-centered Graph ML models. Furthermore, we highlight the significance of safe Graph ML practices and suggest promising avenues for further investigation in this crucial area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "123918726",
                    "name": "Yushun Dong"
                },
                {
                    "authorId": "2134483590",
                    "name": "Binchi Zhang"
                },
                {
                    "authorId": "2276579472",
                    "name": "Zihan Chen"
                },
                {
                    "authorId": "2194727743",
                    "name": "Xingbo Fu"
                },
                {
                    "authorId": "2302556971",
                    "name": "Yinhan He"
                },
                {
                    "authorId": "2276579917",
                    "name": "Cong Shen"
                },
                {
                    "authorId": "2117879943",
                    "name": "Chuxu Zhang"
                },
                {
                    "authorId": "144539424",
                    "name": "N. Chawla"
                },
                {
                    "authorId": "2276482171",
                    "name": "Jundong Li"
                }
            ]
        }
    ]
}