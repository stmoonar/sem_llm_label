{
    "authorId": "2784644",
    "papers": [
        {
            "paperId": "0b6e5b414b71f83681a5382240795586a73c975c",
            "title": "Research on the Uncertainty of Landslide Susceptibility Prediction Using Various Data-Driven Models and Attribute Interval Division",
            "abstract": "Two significant uncertainties that are crucial for landslide susceptibility prediction modeling are attribute interval numbers (AIN) division of continuous landslide impact factors in frequency ratio analysis and various susceptibility prediction models. Five continuous landslide impact factor interval attribute classifications (4, 8, 12, 16, 20) and three data-driven models (deep belief networks (DBN), random forest (RF), and neural network (back propagation (BP)) were used for a total of fifteen different scenarios of landslide susceptibility prediction studies in order to investigate the effects of these two factors on modeling and perform a landslide susceptibility index uncertainty analysis (including precision evaluation and statistical law). The findings indicate that: (1) The results demonstrate that for the same model, as the interval attribute value rises from 4 to 8 and finally to 20, the forecast accuracy of landslide susceptibility initially increases gradually, then progressively grows until stable. (2) The DBN model, followed by the RF and BP models, provides the highest prediction accuracy for the same interval attribute value. (3) AIN = 20 and DBN models have the highest prediction accuracy under 15 combined conditions, while AIN = 4 and BP models have the lowest. The accuracy and efficiency of landslide susceptibility modeling are higher when the AIN = 8 and DBN models are combined. (4) The landslide susceptibility index uncertainty predicted by the deeper learning model and the bigger interval attribute value is comparatively low, which is more in line with the real landslide probability distribution features. The conditions that the environmental component attribute interval is divided into eight parts and DBN models are used allow for the efficient and accurate construction of the landslide susceptibility prediction model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072727626",
                    "name": "Yin Xing"
                },
                {
                    "authorId": "2144354321",
                    "name": "Yang Chen"
                },
                {
                    "authorId": "2048367952",
                    "name": "Saipeng Huang"
                },
                {
                    "authorId": "2113723004",
                    "name": "Wei Xie"
                },
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "103989059",
                    "name": "Yunfei Xiang"
                }
            ]
        },
        {
            "paperId": "1406c1a0c9d0e855e5bf16b8303f8cde86e8fb2c",
            "title": "Online Noisy Continual Relation Learning",
            "abstract": "Recent work for continual relation learning has achieved remarkable progress. However, most existing methods only focus on tackling catastrophic forgetting to improve performance in the existing setup, while continually learning relations in the real-world must overcome many other challenges. One is that the data possibly comes in an online streaming fashion with data distributions gradually changing and without distinct task boundaries. Another is that noisy labels are inevitable in real-world, as relation samples may be contaminated by label inconsistencies or labeled with distant supervision. In this work, therefore, we propose a novel continual relation learning framework that simultaneously addresses both online and noisy relation learning challenges. Our framework contains three key modules: (i) a sample separated online purifying module that divides the online data stream into clean and noisy samples, (ii) a self-supervised online learning module that circumvents inferior training signals caused by noisy data, and (iii) a semi-supervised offline finetuning module that ensures the participation of both clean and noisy samples. Experimental results on FewRel, TACRED and NYT-H with real-world noise demonstrate that our framework greatly outperforms the combinations of the state-of-the-art online continual learning and noisy label learning methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "91119265",
                    "name": "Guozheng Li"
                },
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "88233596",
                    "name": "Qiqing Luo"
                },
                {
                    "authorId": "2108217685",
                    "name": "Yanhe Liu"
                },
                {
                    "authorId": "1596819256",
                    "name": "Wenjun Ke"
                }
            ]
        },
        {
            "paperId": "25d8e3c541f996e366d8fa48cba248ca330e8a78",
            "title": "AerialVLN: Vision-and-Language Navigation for UAVs",
            "abstract": "Recently emerged Vision-and-Language Navigation (VLN) tasks have drawn significant attention in both computer vision and natural language processing communities. Existing VLN tasks are built for agents that navigate on the ground, either indoors or outdoors. However, many tasks require intelligent agents to carry out in the sky, such as UAV-based goods delivery, traffic/security patrol, and scenery tour, to name a few. Navigating in the sky is more complicated than on the ground because agents need to consider the flying height and more complex spatial relationship reasoning. To fill this gap and facilitate research in this field, we propose a new task named AerialVLN, which is UAV-based and towards outdoor environments. We develop a 3D simulator rendered by near-realistic pictures of 25 city-level scenarios. Our simulator supports continuous navigation, environment extension and configuration. We also proposed an extended baseline model based on the widely-used cross-modal-alignment (CMA) navigation methods. We find that there is still a significant gap between the baseline model and human performance, which suggests AerialVLN is a new challenging task. Dataset and code is available at https://github.com/AirVLN/AirVLN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192800622",
                    "name": "Shubo Liu"
                },
                {
                    "authorId": "2108878660",
                    "name": "Hongsheng Zhang"
                },
                {
                    "authorId": "35653798",
                    "name": "Yuankai Qi"
                },
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "2108155662",
                    "name": "Yaning Zhang"
                },
                {
                    "authorId": "2167223085",
                    "name": "Qi Wu"
                }
            ]
        },
        {
            "paperId": "2e0aed964a845ad6818cd5cb26d501c41c2bee4c",
            "title": "HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation",
            "abstract": "Recent works attempt to employ pre-training in Vision-and-Language Navigation (VLN). However, these methods neglect the importance of historical contexts or ignore predicting future actions during pre-training, limiting the learning of visual-textual correspondence and the capability of decision-making. To address these problems, we present a history-enhanced and order-aware pre-training with the complementing fine-tuning paradigm (HOP+) for VLN. Specifically, besides the common Masked Language Modeling (MLM) and Trajectory-Instruction Matching (TIM) tasks, we design three novel VLN-specific proxy tasks: Action Prediction with History (APH) task, Trajectory Order Modeling (TOM) task and Group Order Modeling (GOM) task. APH task takes into account the visual perception trajectory to enhance the learning of historical knowledge as well as action prediction. The two temporal visual-textual alignment tasks, TOM and GOM further improve the agent's ability to order reasoning. Moreover, we design a memory network to address the representation inconsistency of history context between the pre-training and the fine-tuning stages. The memory network effectively selects and summarizes historical information for action prediction during fine-tuning, without costing huge extra computation consumption for downstream VLN tasks. HOP+ achieves new state-of-the-art performance on four downstream VLN tasks (R2R, REVERIE, RxR, and NDH), which demonstrates the effectiveness of our proposed method.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80526284",
                    "name": "Yanyuan Qiao"
                },
                {
                    "authorId": "35653798",
                    "name": "Yuankai Qi"
                },
                {
                    "authorId": "1612421029",
                    "name": "Yicong Hong"
                },
                {
                    "authorId": "2116731898",
                    "name": "Zheng Yu"
                },
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "1715610",
                    "name": "Qi Wu"
                }
            ]
        },
        {
            "paperId": "4243ba22834129c254543198b7f36d364a1a0eb2",
            "title": "BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting",
            "abstract": "Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists of two off-the-shelf methods for improving PLMs\u2019 early exiting. We first address the issues of training a multi-exit PLM, the backbone model for early exiting. We propose the novel architecture of block-wise bypasses, which can alleviate the conflicts in jointly training multiple intermediate classifiers and thus improve the overall performances of multi-exit PLM while introducing negligible additional flops to the model. Second, we propose a novel divergence-based early exiting (DGE) mechanism, which obtains early exiting signals by comparing the predicted distributions of two adjacent layers\u2019 exits. Extensive experiments on three proprietary datasets and three GLUE benchmark tasks demonstrate that our method can obtain a better speedup-performance trade-off than the existing baseline methods.\\footnote{Code will be made publicly available to the research community upon acceptance.}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152348673",
                    "name": "Wei Zhu"
                },
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "2072724069",
                    "name": "Yuan Ni"
                },
                {
                    "authorId": "2052157466",
                    "name": "G. Xie"
                },
                {
                    "authorId": "2145748428",
                    "name": "Xiaoling Wang"
                }
            ]
        },
        {
            "paperId": "4dfd36369761b0b99f1097bac69dd6ce849dabed",
            "title": "Towards Incremental NER Data Augmentation via Syntactic-aware Insertion Transformer",
            "abstract": "Named entity recognition (NER) aims to locate and classify named entities in natural language texts. Most existing high-performance NER models employ a supervised paradigm, which requires a large quantity of high-quality annotated data during training. In order to help NER models perform well in few-shot scenarios, data augmentation approaches attempt to build extra data by means of random editing or by using end-to-end generation with PLMs.\n\nHowever, these methods focus on only the fluency of generated sentences, ignoring the syntactic correlation between the new and raw sentences. Such uncorrelation also brings low diversity and inconsistent labeling of synthetic samples. To fill this gap, we present SAINT (Syntactic-Aware InsertioN Transformer), a hard-constraint controlled text generation model that incorporates syntactic information. The proposed method operates by inserting new tokens between existing entities in a parallel manner. During insertion procedure, new tokens will be added taking both semantic and syntactic factors into account. Hence the resulting sentence can retain the syntactic correctness with respect to the raw data. Experimental results on two benchmark datasets, i.e., Ontonotes and Wikiann, demonstrate the comparable performance of SAINT over the state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2034348705",
                    "name": "Wenjun Ke"
                },
                {
                    "authorId": "103303283",
                    "name": "Zongkai Tian"
                },
                {
                    "authorId": "50384171",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "3223390",
                    "name": "Jinhua Gao"
                },
                {
                    "authorId": "2066043434",
                    "name": "Rui Qi"
                }
            ]
        },
        {
            "paperId": "56fa65d8dc41708082f9b2ef7752c49cee9ebe01",
            "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation",
            "abstract": "Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting.While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM\u2019s predictions or faithfully justify the decisions. In this work, we propose SCOTT, a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that while yielding comparable performance, our method leads to a more faithful model than baselines. Further analysis shows that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "8492168",
                    "name": "Zhengyang Wang"
                },
                {
                    "authorId": "2146249169",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "1921742",
                    "name": "Yifan Gao"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "2115257544",
                    "name": "Xiang Ren"
                }
            ]
        },
        {
            "paperId": "574278a1b7955c9a56d65241e40410ff9899a88d",
            "title": "PasCore: A Chinese Overlapping Relation Extraction Model Based on Global Pointer Annotation Strategy",
            "abstract": "Recent work for extracting relations from texts has achieved excellent performance. However, existing studies mainly focus on simple relation extraction, these methods perform not well on overlapping triple problem because the tags of shared entities would conflict with each other. Especially, overlapping entities are common and indispensable in Chinese. To address this issue, this paper proposes PasCore, which utilizes a global pointer annotation strategy for overlapping relation extraction in Chinese. PasCore first obtains the sentence vector via general pre-training model encoder, and uses classifier to predicate relations. Subsequently, it uses global pointer annotation strategy for head entity annotation, which uses global tags to label the start and end positions of the entities. Finally, PasCore integrates the relation, head entity and its type to mark the tail entity. Furthermore, PasCore performs conditional layer normalization to fuse features, which connects all stages and greatly enriches the association between relations and entities. Experimental results on both Chinese and English real-world datasets demonstrate that PasCore outperforms strong baselines on relation extraction and, especially, shows superior performance on overlapping relation extraction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "2153624361",
                    "name": "Jiafeng Xie"
                },
                {
                    "authorId": "2229014483",
                    "name": "Xiye Chen"
                },
                {
                    "authorId": "91119265",
                    "name": "Guozheng Li"
                },
                {
                    "authorId": "2157337679",
                    "name": "Wei Li"
                }
            ]
        },
        {
            "paperId": "79852c8ff53da89fed92d1c94e6070e2eb46fa3e",
            "title": "Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning",
            "abstract": "\u2014With the rise in popularity of video-based social media, new categories of videos are constantly being generated, creating an urgent need for robust incremental learning techniques for video understanding. One of the biggest challenges in this task is catastrophic forgetting, where the network tends to forget previously learned data while learning new categories. To overcome this issue, knowledge distillation is a widely used technique for rehearsal-based video incremental learning that involves transferring important information on similarities among different categories to enhance the student model. Therefore, it is preferable to have a strong teacher model to guide the students. However, the limited performance of the network itself and the occurrence of catastrophic forgetting can result in the teacher network making inaccurate predictions for some memory ex-emplars, ultimately limiting the student network\u2019s performance. Based on these observations, we propose a teacher agent capable of generating stable and accurate soft labels to replace the output of the teacher model. This method circumvents the problem of knowledge misleading caused by inaccurate predictions of the teacher model and avoids the computational overhead of loading the teacher model for knowledge distillation. Extensive experiments demonstrate the advantages of our method, yielding significant performance improvements while utilizing only half the resolution of video clips in the incremental phases as input compared to recent state-of-the-art methods. Moreover, our method surpasses the performance of joint training when employing four times the number of samples in episodic memory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8038247",
                    "name": "Shengqin Jiang"
                },
                {
                    "authorId": "2218607741",
                    "name": "Yaoyu Fang"
                },
                {
                    "authorId": "9726614",
                    "name": "Haokui Zhang"
                },
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "35653798",
                    "name": "Yuankai Qi"
                },
                {
                    "authorId": "2218639381",
                    "name": "Qingshan Liu"
                }
            ]
        },
        {
            "paperId": "7d6ae8b62cc8bcefc3e0d50be5b592d96254d930",
            "title": "Multimodal Short Video Rumor Detection System Based on Contrastive Learning",
            "abstract": "With the rise of short video platforms as prominent channels for news dissemination, major platforms in China have gradually evolved into fertile grounds for the proliferation of fake news. However, distinguishing short video rumors poses a significant challenge due to the substantial amount of information and shared features among videos, resulting in homogeneity. To address the dissemination of short video rumors effectively, our research group proposes a methodology encompassing multimodal feature fusion and the integration of external knowledge, considering the merits and drawbacks of each algorithm. The proposed detection approach entails the following steps: (1) creation of a comprehensive dataset comprising multiple features extracted from short videos; (2) development of a multimodal rumor detection model: first, we employ the Temporal Segment Networks (TSN) video coding model to extract video features, followed by the utilization of Optical Character Recognition (OCR) and Automatic Speech Recognition (ASR) to extract textual features. Subsequently, the BERT model is employed to fuse textual and video features; (3) distinction is achieved through contrast learning: we acquire external knowledge by crawling relevant sources and leverage a vector database to incorporate this knowledge into the classification output. Our research process is driven by practical considerations, and the knowledge derived from this study will hold significant value in practical scenarios, such as short video rumor identification and the management of social opinions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108795831",
                    "name": "Yuxing Yang"
                },
                {
                    "authorId": "51068641",
                    "name": "Junhao Zhao"
                },
                {
                    "authorId": "2116422314",
                    "name": "Siyi Wang"
                },
                {
                    "authorId": "2214586409",
                    "name": "Xiangyu Min"
                },
                {
                    "authorId": "2784644",
                    "name": "Peifeng Wang"
                },
                {
                    "authorId": "2145330648",
                    "name": "Haizhou Wang"
                }
            ]
        }
    ]
}