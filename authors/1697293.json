{
    "authorId": "1697293",
    "papers": [
        {
            "paperId": "871bd6989b889dcc266775f25d933789d6aa21bb",
            "title": "Cost-Effective Traffic Scheduling and Resource Allocation for Edge Service Provisioning",
            "abstract": "The multi-access edge computing (MEC) paradigm has emerged as a critical solution to address the exponential growth in mobile web services and devices. By implementing an edge-based service provisioning system (EPS) with servers located at the network\u2019s edge, both transmission and computation efficiency can be significantly enhanced. Nevertheless, it is also essential to carefully consider the resource allocation for services, the traffic management of requests, and the path arrangement for data delivery to ensure the cost-effective operation of the EPS. Therefore, we investigate and quantify the relationship between the performance and cost of the EPS in this paper, and model the cost-effective service provisioning problem as a multi-phase convex optimization problem. An online algorithm whose name is RDC based on the Lyapunov framework is proposed to decompose this problem into several sub-problems.Additionally, a heuristic approach that partitions edge servers into several clusters, called RDC-NeP and based on RDC, has also been proposed to reduce computational complexity. A series of experiments were conducted to evaluate the proposed approach. The results demonstrate that RDC can effectively balance expense and performance, while RDC-NeP significantly simplifies the processing of RDC when the problem scale increases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34812540",
                    "name": "Zhengzhe Xiang"
                },
                {
                    "authorId": "2149513491",
                    "name": "Yuhang Zheng"
                },
                {
                    "authorId": "2629922",
                    "name": "Zengwei Zheng"
                },
                {
                    "authorId": "145590434",
                    "name": "Shuiguang Deng"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                },
                {
                    "authorId": "1691109",
                    "name": "S. Dustdar"
                }
            ]
        },
        {
            "paperId": "0ab21134f2812ab3d12607b73494fa5c4ea4304d",
            "title": "Exploiting big.LITTLE Batteries for Software Defined Management on Mobile Devices",
            "abstract": "Battery service time is a critical constraint on the availability and functionality of mobile devices. Equipping larger batteries may mitigate such deficiency yet it raises the challenges on thermal limit and physical size. Changing the battery chemistry is another solution, which however usually benefits the energy efficiency of only part of the applications, depending on their software behaviors. To address the challenges of battery energy efficiency and heat dissipation in limited physical space, we propose CAPMAN, a management framework that jointly optimizes the cooling and active power management in a smartphone, a typical mobile device, equipped with a hybrid battery pack. We establish the framework with three components. First, we abstract the correlation among the batteries, devices and software into a finite state machine model, whose state transitions can be triggered by actions like system calls and user activities. Second, we propose a battery scheduling algorithm that determines the more suitable battery for cooling/active power use, with respect to the dynamic software behaviors and their impact on the hardware states, based on a Markov decision process (MDP). Third, we design a facility for joint cooling and active power management by coordinating TECs and batteries. With the three major designs, CAPMAN realizes software defined management that schedules heterogeneous batteries and TEC cooling in a timely manner. In addition, CAPMAN provides an online algorithm with a proved O(1.05)-competitiveness performance. With a pair of big.LITTLE batteries, we prototype CAPMAN on multiple popular smartphones and a PYNQ development board. The evaluation with real-world workloads shows that compared to the current mainstream, CAPMAN can achieve 114 percent longer battery service time under skewed loads; compared to the state-of-the-practice baselines, CAPMAN shows 55 percent performance gain and 53 percent less energy use on average. Those results approve that big.LITTLE batteries with sophisticated software defined management is an effective way to prolong the battery service times on mobile devices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47047681",
                    "name": "Zichen Xu"
                },
                {
                    "authorId": "2116575614",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "50010573",
                    "name": "Wenli Zheng"
                },
                {
                    "authorId": "2115768992",
                    "name": "Yuhao Wang"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                }
            ]
        },
        {
            "paperId": "1b432a3036b06df5cd2fd14ad7af6568191e89e5",
            "title": "Adaptive Resource Efficient Microservice Deployment in Cloud-Edge Continuum",
            "abstract": "User-facing services are now evolving towards the microservice architecture where a service is built by connecting multiple microservice stages. Since the entire service is heavy, the microservice architecture shows the opportunity to only offload some microservice stages to the edge devices that are close to the end users. However, emerging techniques often result in the violation of Quality-of-Service (QoS) of microservice-based services in cloud-edge continuum, as they do not consider the communication overhead or the resource contention between microservices and external co-located tasks. We propose Nautilus, a runtime system that effectively deploys microservice-based user-facing services in cloud-edge continuum. Nautilus ensures the QoS of microservice-based user-facing services while minimizing the required computational resources, which is comprised of a communication-aware microservice mapper, a contention-aware resource manager and an IO-sensitive and load-aware microservice migration scheduler. The mapper divides the microservice graph into multiple partitions based on the communication overhead and maps the partitions to appropriate nodes. On each node, the resource manager determines the optimal resource allocation for its microservices based on reinforcement learning that may capture the complex contention behaviors. Once the microservices are suffered from external IO pressure, the IO-sensitive microservice scheduler migrates the critical one to idle nodes. Furthermore, when the load of microservices changes dynamically, the load-aware microservice scheduler migrates microservices from busy nodes to idle ones to ensure the QoS goal of the entire service. Our experimental results show that Nautilus can guarantee the required QoS target under external shared resources contention while the state-of-the-art suffers from QoS violations. Meanwhile, Nautilus reduces the computational resource usage by 23.9% and the network bandwidth usage by 53.4%, while achieving the required 99%-ile latency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "101731246",
                    "name": "Kaihua Fu"
                },
                {
                    "authorId": "39340691",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "1596812259",
                    "name": "Quan Chen"
                },
                {
                    "authorId": "2288376751",
                    "name": "Deze Zeng"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                }
            ]
        },
        {
            "paperId": "2729eb66fc3c09b928a937c53f1a7d2332946335",
            "title": "Preference-Aware Edge Server Placement in the Internet of Things",
            "abstract": "While it is well understood that edge computing can significantly facilitate IoT-related applications by deploying edge servers close to IoT devices, it also faces many challenges with numerous IoT devices connected and interacted. One of the most important issues is how to efficiently deploy edge servers under a certain budget with the explosive growth of data scale and user base. Existing studies for edge server placement fail to consider user\u2019s query preferences since individual users may be interested in events in particular regions and are keen to receive up-to-date data streams that originate in regions of interest. In this article, we present a preference-aware edge server placement approach that offers better workload distribution in terms of both minimizing query latency and balancing the load of edge servers. To achieve this, we formulate edge server placement with multiobjective optimization as a <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-center problem and design two progressive approaches. We first propose quadratic integer programming (QIP) for small-scale data sets. Since the <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-center problem is an NP-hard problem, we thus propose a heuristic algorithm named TAKG (TAbu search with <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula>-means and Genetic algorithm) for large-scale data sets. To evaluate the utility of the proposed models, we have conducted a comprehensive evaluation on a large data set that is collected by more than 1900 IoT devices during 30 days. Experimental results indicate our approaches outperform all baselines significantly in terms of both query latency and load balancing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16098577",
                    "name": "Yuanyi Chen"
                },
                {
                    "authorId": "97594531",
                    "name": "Yih-Yeong Lin"
                },
                {
                    "authorId": "2629922",
                    "name": "Zengwei Zheng"
                },
                {
                    "authorId": "92720938",
                    "name": "Pengfei Yu"
                },
                {
                    "authorId": "144980022",
                    "name": "Jiaxing Shen"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                }
            ]
        },
        {
            "paperId": "4d1a9ea0368aad53975e2fe34dc62528f9f5822e",
            "title": "FaaSFlow: enable efficient workflow execution for function-as-a-service",
            "abstract": "Serverless computing (Function-as-a-Service) provides fine-grain resource sharing by running functions (or Lambdas) in containers. Data-dependent functions are required to be invoked following a pre-defined logic, which is known as serverless workflows. However, our investigation shows that the traditional master-worker based workflow execution architecture performs poorly in serverless context. One significant overhead results from the master-side workflow schedule pattern, with which the functions are triggered in the master node and assigned to worker nodes for execution. Besides, the data movement between workers also reduces the throughput. To this end, we present a worker-side workflow schedule pattern for serverless workflow execution. Following the design, we implement FaaSFlow to enable efficient workflow execution in the serverless context. Besides, we propose an adaptive storage library FaaStore that enables fast data transfer between functions on the same node without through the database. Experiment results show that FaaSFlow effectively mitigates the workflow scheduling overhead by 74.6% on average and data transmission overhead by 95% at most. When the network bandwidth fluctuates, FaaSFlow-FaaStore reduces the throughput degradation by 23.0%, and is able to multiply the utilization of network bandwidth by 1.5X-4X.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118275662",
                    "name": "Zijun Li"
                },
                {
                    "authorId": "2175971206",
                    "name": "Yushi Liu"
                },
                {
                    "authorId": "9221797",
                    "name": "Linsong Guo"
                },
                {
                    "authorId": "1596812259",
                    "name": "Quan Chen"
                },
                {
                    "authorId": "2149023479",
                    "name": "Jiagan Cheng"
                },
                {
                    "authorId": "50010573",
                    "name": "Wenli Zheng"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                }
            ]
        },
        {
            "paperId": "4f82a34a9a0a2e82912f0f670c41d10e70e080d7",
            "title": "QoS-awareness of Microservices with Excessive Loads via Inter-Datacenter Scheduling",
            "abstract": "User-facing applications often experience excessive loads and are shifting towards microservice software architecture. While the local datacenter may not have enough resources to host the excessive loads, a reasonable solution is moving some microservices of the applications to remote datacenters. However, it is nontrivial to identify the appropriate migration decision, as the microservices show different characteristics, and the local datacenter also shows different resource contention situations. We therefore propose ELIS, an inter-datacenter scheduling system that ensures the required Quality-of-Service (QoS) of the microservice application with excessive loads, while minimizing the resource usage of the remote datacenter. ELIS comprises a resource manager and a reward-based microservice migrator. The resource manager finds the near-optimal resource configurations for different microservices to minimize resource usage while ensuring QoS. The microservice migrator migrates some microservices to remote datacenters when local resources cannot afford the excessive loads. Our experimental results show that ELIS ensures the required QoS of user-facing applications at excessive loads. Meanwhile, it reduces overall/remote resource usage by 13.1% and 58.1% on average, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143892434",
                    "name": "Jiuchen Shi"
                },
                {
                    "authorId": "2110132117",
                    "name": "Jiawen Wang"
                },
                {
                    "authorId": "101731246",
                    "name": "Kaihua Fu"
                },
                {
                    "authorId": "1596812259",
                    "name": "Quan Chen"
                },
                {
                    "authorId": "2288376751",
                    "name": "Deze Zeng"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                }
            ]
        },
        {
            "paperId": "5cc95fe5faaa43ac1bcc37164df6587b767a66cc",
            "title": "GRPU: An Efficient Graph-based Cross-Rack Parallel Update Scheme for Cloud Storage Systems",
            "abstract": "Erasure coding (EC) has been widely used in cloud storage systems to provide both high reliability and low storage cost. Previous literatures show that the cross-rack update operations are prevalent for many applications in erasure-coded cloud storage systems, which introduces significant I/O amplification, load imbalance and high latency. Several existing methods have been proposed to mitigate these problems. However, they ignore the correlations among chunks when performing data placement. Thus numerous stripes and racks participate in the update leading to extra I/Os and cross-rack traffic. Moreover, they don\u2019t take into account the parallelism of network transmission which loses the potential update performance gains.To address the issues, we propose a novel Graph-based cross-Rack Parallel Update (GRPU) scheme to improve the update performance for erasure-coded cloud storage systems. The key idea of GRPU is to place the correlated chunks in the same stripe and rack, and transmit the chunks in parallel based on the network distance. The data placement and transmission paths selection are guided by two kinds of graphs. To demonstrate the effectiveness of GRPU, we conduct several experiments in a local cluster. The results show that, compared to the state-of-the-art methods, GRPU reduces the cross-rack traffic by up to 34.66% and the average response time by up to 61.69%, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130914522",
                    "name": "Ranhao Jia"
                },
                {
                    "authorId": "2153490077",
                    "name": "Haiwei Deng"
                },
                {
                    "authorId": "2112678271",
                    "name": "Yunfei Gu"
                },
                {
                    "authorId": "2197234789",
                    "name": "Huangzhen Xue"
                },
                {
                    "authorId": "3070171",
                    "name": "Chentao Wu"
                },
                {
                    "authorId": "2232328",
                    "name": "Shiyi Li"
                },
                {
                    "authorId": "2149869431",
                    "name": "Jie Li"
                },
                {
                    "authorId": "1684808",
                    "name": "Guangtao Xue"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                }
            ]
        },
        {
            "paperId": "6382d8f987b8f03722d339168d402731f5716f12",
            "title": "Tapping into NFV Environment for Opportunistic Serverless Edge Function Deployment",
            "abstract": "Even with Network Function Virtualization (NFV), many commodity network servers have spare cycles. Despite that they are small and irregularly occur, spare cycles are fit for deploying short-lived serverless computing functions at the network edge. In this work, we perform detailed analyses of the benefits and limitations of co-locating serverless functions on NFV-ready servers. We propose <italic>NEMO</italic>, a novel platform that enables efficient serverless edge function deployment in the NFV environment. NEMO can intelligently harvest spare cycles of network functions to warm up the serverless functions and speed up the function invocation in an agile manner. Besides, NEMO can judiciously manage the thread conflict in a resource-limited environment. We build a prototype of NEMO. Our thorough evaluations show that NEMO can harvest up to 41% spare cycles and achieve about 12.5<inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math><alternatives><mml:math><mml:mo>\u223c</mml:mo></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3132776.gif\"/></alternatives></inline-formula>25X performance improvement compared with straightforward co-location.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156144870",
                    "name": "Lu Zhang"
                },
                {
                    "authorId": "1643328669",
                    "name": "Weiqi Feng"
                },
                {
                    "authorId": "46651954",
                    "name": "Chao Li"
                },
                {
                    "authorId": "2087625687",
                    "name": "Xiaofeng Hou"
                },
                {
                    "authorId": "1914624198",
                    "name": "Pengyu Wang"
                },
                {
                    "authorId": "2152447086",
                    "name": "Jing Wang"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                }
            ]
        },
        {
            "paperId": "724f8c0d4af3a8ef83612a51fd5c876783eeb2cc",
            "title": "Efficient Trustworthiness Management for Malicious User Detection in Big Data Collection",
            "abstract": "Data collection in big data is an effective way to aggregate information that the collector is interested in. However, there is no assurance for the data that the users provide. Since collector does not have the ability to check the authenticity of every piece of information, the trustworthiness of users participated in the collection become important. In this paper, we design an efficient approach to calculate users\u2019 trustworthiness in data collection for big data context. We divide the trustworthiness into familiarity trustworthiness and similarity trustworthiness, and study the influences of user actions on trustworthiness. To prevent malicious users from raising their trustworthiness and providing false information that may mislead final results, we also design a security queue to record users\u2019 historical trust information, so that we can detect malicious users with high accuracy. Simulation results show that our model can sensitively resist the malicious actions of users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115937175",
                    "name": "Jiahui Yu"
                },
                {
                    "authorId": "28014924",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "143667002",
                    "name": "Peng Li"
                },
                {
                    "authorId": "144304984",
                    "name": "Rui Xia"
                },
                {
                    "authorId": "2115741856",
                    "name": "Song Guo"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                }
            ]
        },
        {
            "paperId": "9f54e47b56aa9aca23de3c82a653066c21f3a1ed",
            "title": "Integrated Power Anomaly Defense: Towards Oversubscription-Safe Data Centers",
            "abstract": "Energy storage devices (e.g., batteries) are critical components for high-availability data center infrastructure today. Without resilient energy management of these devices, existing power-hungry data centers are largely unguarded targets for cyber criminals. Particularly for some of today's scale-out data centers, power infrastructure oversubscription unavoidably taxes the data center's backup energy resources (i.e., UPS), leaving very little room for dealing with power emergency. As a result, an attacker could manipulate the computing system to generate peak power demand and disrupt power-constrained server racks. This article aims at protecting data centers from malicious loads that seek to drain precious energy backup, overload server racks and compromise workload performance. We term such load as Elusive Power Peak (EPP) and demonstrate its basic three-phase attacking model. To defend against EPP, we propose IPAD, a remediation solution build on integrated software and hardware mechanisms. IPAD not only increases the attacking cost considerably by hiding vulnerable server racks from visible power peaks, but also strengthens the last line of defense against hidden power spikes with fine-grained power control strategy. We show that IPAD can effectively raise the bar of power-related attack, with reasonable design overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087625687",
                    "name": "Xiaofeng Hou"
                },
                {
                    "authorId": "46651954",
                    "name": "Chao Li"
                },
                {
                    "authorId": "2012701950",
                    "name": "Jinghang Yang"
                },
                {
                    "authorId": "50010573",
                    "name": "Wenli Zheng"
                },
                {
                    "authorId": "1768287",
                    "name": "Xiaoyao Liang"
                },
                {
                    "authorId": "1697293",
                    "name": "M. Guo"
                }
            ]
        }
    ]
}