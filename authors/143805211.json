{
    "authorId": "143805211",
    "papers": [
        {
            "paperId": "11295ac04371b91f095afad9d2001e591df6441a",
            "title": "Background Prompting for Improved Object Depth",
            "abstract": "Estimating the depth of objects from a single image is a valuable task for many vision, robotics, and graphics applications. However, current methods often fail to produce accurate depth for objects in diverse scenes. In this work, we propose a simple yet effective Background Prompting strategy that adapts the input object image with a learned background. We learn the background prompts only using small-scale synthetic object datasets. To infer object depth on a real image, we place the segmented object into the learned background prompt and run off-the-shelf depth networks. Background Prompting helps the depth networks focus on the foreground object, as they are made invariant to background variations. Moreover, Background Prompting minimizes the domain gap between synthetic and real object images, leading to better sim2real generalization than simple finetuning. Results on multiple synthetic and real datasets demonstrate consistent improvements in real object depths for a variety of existing depth networks. Code and optimized background prompts can be found at: https://mbaradad.github.io/depth_prompt.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47402054",
                    "name": "Manel Baradad"
                },
                {
                    "authorId": "2167749913",
                    "name": "Yuanzhen Li"
                },
                {
                    "authorId": "39578349",
                    "name": "Forrester Cole"
                },
                {
                    "authorId": "144544291",
                    "name": "Michael Rubinstein"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                },
                {
                    "authorId": "1768236",
                    "name": "W. Freeman"
                },
                {
                    "authorId": "2131639924",
                    "name": "Varun Jampani"
                }
            ]
        },
        {
            "paperId": "1ad85b1a902cd37c855dbedf5bd17c536628b611",
            "title": "Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning",
            "abstract": "In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3978031",
                    "name": "Tongzhou Wang"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                },
                {
                    "authorId": "2094770",
                    "name": "Phillip Isola"
                },
                {
                    "authorId": "2111672235",
                    "name": "Amy Zhang"
                }
            ]
        },
        {
            "paperId": "41f4dc53ed4550653d07cb3d5472a6f0af3115b7",
            "title": "Open-vocabulary Panoptic Segmentation with Embedding Modulation",
            "abstract": "Open-vocabulary image segmentation is attracting increasing attention due to its critical applications in the real world. Traditional closed-vocabulary segmentation methods are not able to characterize novel objects, whereas several recent open-vocabulary attempts obtain unsatisfactory results, i.e., notable performance reduction on the closedvocabulary and massive demand for extra data. To this end, we propose OPSNet, an omnipotent and data-efficient framework for Open-vocabulary Panoptic Segmentation. Specifically, the exquisitely designed Embedding Modulation module, together with several meticulous components, enables adequate embedding enhancement and information exchange between the segmentation model and the visual-linguistic well-aligned CLIP encoder, resulting in superior segmentation performance under both open- and closed-vocabulary settings with much fewer need of additional data. Extensive experimental evaluations are conducted across multiple datasets (e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various circumstances, where the proposed OPSNet achieves state-of-theart results, which demonstrates the effectiveness and generality of the proposed approach. The project page is https://opsnet-page.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145309103",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2133436855",
                    "name": "Shuang Li"
                },
                {
                    "authorId": "153317808",
                    "name": "S. Lim"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                },
                {
                    "authorId": "3459894",
                    "name": "Hengshuang Zhao"
                }
            ]
        },
        {
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15394275",
                    "name": "Yilun Du"
                },
                {
                    "authorId": "145015904",
                    "name": "Shuang Li"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                },
                {
                    "authorId": "1763295",
                    "name": "J. Tenenbaum"
                },
                {
                    "authorId": "2316241372",
                    "name": "Igor Mordatch"
                }
            ]
        },
        {
            "paperId": "5e2bceb56f116e98baf7e418208057bc0e1c1861",
            "title": "ConceptFusion: Open-set Multimodal 3D Mapping",
            "abstract": "Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts. We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today's foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping. For more information, visit our project page https://concept-fusion.github.io or watch our 5-minute explainer video https://www.youtube.com/watch?v=rkXgws8fiDs",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7434636",
                    "name": "Krishna Murthy Jatavallabhula"
                },
                {
                    "authorId": "2028784496",
                    "name": "Ali Kuwajerwala"
                },
                {
                    "authorId": "15899518",
                    "name": "Qiao Gu"
                },
                {
                    "authorId": "2273329716",
                    "name": "Mohd. Omama"
                },
                {
                    "authorId": "144799987",
                    "name": "Tao Chen"
                },
                {
                    "authorId": "145015904",
                    "name": "Shuang Li"
                },
                {
                    "authorId": "2006798846",
                    "name": "Ganesh Iyer"
                },
                {
                    "authorId": "29793812",
                    "name": "Soroush Saryazdi"
                },
                {
                    "authorId": "1585516855",
                    "name": "Nikhil Varma Keetha"
                },
                {
                    "authorId": "9102722",
                    "name": "A. Tewari"
                },
                {
                    "authorId": "1763295",
                    "name": "J. Tenenbaum"
                },
                {
                    "authorId": "2147315384",
                    "name": "Celso M. de Melo"
                },
                {
                    "authorId": "2053997870",
                    "name": "M. Krishna"
                },
                {
                    "authorId": "3198259",
                    "name": "L. Paull"
                },
                {
                    "authorId": "2162768",
                    "name": "F. Shkurti"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                }
            ]
        },
        {
            "paperId": "65422006c040947afac35bb2fcf05d2b8f702827",
            "title": "NOPA: Neurally-guided Online Probabilistic Assistance for Building Socially Intelligent Home Assistants",
            "abstract": "In this work, we study how to build socially intelligent robots to assist people in their homes. In particular, we focus on assistance with online goal inference, where robots must simultaneously infer humans' goals and how to help them achieve those goals. Prior assistance methods either lack the adaptivity to adjust helping strategies (i.e., when and how to help) in response to uncertainty about goals or the scalability to conduct fast inference in a large goal space. Our NOPA (Neurally-guided Online Probabilistic Assistance) method addresses both of these challenges. NOPA consists of (1) an online goal inference module combining neural goal proposals with inverse planning and particle filtering for robust inference under uncertainty, and (2) a helping planner that discovers valuable subgoals to help with and is aware of the uncertainty in goal inference. We compare NOPA against multiple baselines in a new embodied AI assistance challenge: Online Watch-And-Help, in which a helper agent needs to simultaneously watch a main agent's action, infer its goal, and help perform a common household task faster in realistic virtual home environments. Experiments show that our helper agent robustly updates its goal inference and adapts its helping plans to the changing level of uncertainty.11Code and a supplementary video are available at https://www.tshu.io/online_watch_and_help.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143872936",
                    "name": "Xavier Puig"
                },
                {
                    "authorId": "1844358",
                    "name": "Tianmin Shu"
                },
                {
                    "authorId": "1763295",
                    "name": "J. Tenenbaum"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                }
            ]
        },
        {
            "paperId": "73003cb9f5c1b30cd495f31f70323b1b17b24ff3",
            "title": "FluidLab: A Differentiable Environment for Benchmarking Complex Fluid Manipulation",
            "abstract": "Humans manipulate various kinds of fluids in their everyday life: creating latte art, scooping floating objects from water, rolling an ice cream cone, etc. Using robots to augment or replace human labors in these daily settings remain as a challenging task due to the multifaceted complexities of fluids. Previous research in robotic fluid manipulation mostly consider fluids governed by an ideal, Newtonian model in simple task settings (e.g., pouring). However, the vast majority of real-world fluid systems manifest their complexities in terms of the fluid's complex material behaviors and multi-component interactions, both of which were well beyond the scope of the current literature. To evaluate robot learning algorithms on understanding and interacting with such complex fluid systems, a comprehensive virtual platform with versatile simulation capabilities and well-established tasks is needed. In this work, we introduce FluidLab, a simulation environment with a diverse set of manipulation tasks involving complex fluid dynamics. These tasks address interactions between solid and fluid as well as among multiple fluids. At the heart of our platform is a fully differentiable physics simulator, FluidEngine, providing GPU-accelerated simulations and gradient calculations for various material types and their couplings. We identify several challenges for fluid manipulation learning by evaluating a set of reinforcement learning and trajectory optimization methods on our platform. To address these challenges, we propose several domain-specific optimization schemes coupled with differentiable physics, which are empirically shown to be effective in tackling optimization problems featured by fluid system's non-convex and non-smooth properties. Furthermore, we demonstrate reasonable sim-to-real transfer by deploying optimized trajectories in real-world settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060151696",
                    "name": "Zhou Xian"
                },
                {
                    "authorId": "2184192853",
                    "name": "Bo Zhu"
                },
                {
                    "authorId": "74498275",
                    "name": "Zhenjia Xu"
                },
                {
                    "authorId": "1693704",
                    "name": "H. Tung"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                },
                {
                    "authorId": "1705557",
                    "name": "Katerina Fragkiadaki"
                },
                {
                    "authorId": "2056157586",
                    "name": "Chuang Gan"
                }
            ]
        },
        {
            "paperId": "83397788e30473ad5a996a062f7c62bb1122ad52",
            "title": "Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos",
            "abstract": "Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are directly estimated from noisy real-world impact sound examples without sophisticated setup and learned residual parameters that interpret the sound environment via neural networks. We further implement a novel diffusion model with specific training and inference strategies to combine physics priors and visual information for impact sound synthesis. Experimental results show that our model outperforms several existing systems in generating realistic impact sounds. Lastly, the physics-based representations are fully interpretable and transparent, thus allowing us to perform sound editing flexibly. We encourage the readers visit our project page11https://sukun1045.github.io/video-physics-sound-diffusion/ to watch demo videos with the audio turned on to experience the result.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2065701609",
                    "name": "Kun Su"
                },
                {
                    "authorId": "9566679",
                    "name": "Kaizhi Qian"
                },
                {
                    "authorId": "2003419",
                    "name": "Eli Shlizerman"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                },
                {
                    "authorId": "2056157586",
                    "name": "Chuang Gan"
                }
            ]
        },
        {
            "paperId": "850b71c61d9fa4a153eee6e2c912c16ecc459b7a",
            "title": "Debiasing Vision-Language Models via Biased Prompts",
            "abstract": "Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8551209",
                    "name": "Ching-Yao Chuang"
                },
                {
                    "authorId": "2131639924",
                    "name": "Varun Jampani"
                },
                {
                    "authorId": "2167749913",
                    "name": "Yuanzhen Li"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                },
                {
                    "authorId": "2594093",
                    "name": "S. Jegelka"
                }
            ]
        },
        {
            "paperId": "a42b68b9fbd4f9c3382b54f1387547eb56d011ac",
            "title": "Generalizing Dataset Distillation via Deep Generative Prior",
            "abstract": "Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthe-size a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite a recent upsurge of progress in the field, existing dataset dis-tillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pretrained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "19296006",
                    "name": "George Cazenavette"
                },
                {
                    "authorId": "3978031",
                    "name": "Tongzhou Wang"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                },
                {
                    "authorId": "1763086",
                    "name": "Alexei A. Efros"
                },
                {
                    "authorId": "2436356",
                    "name": "Jun-Yan Zhu"
                }
            ]
        }
    ]
}