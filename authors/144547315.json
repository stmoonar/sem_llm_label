{
    "authorId": "144547315",
    "papers": [
        {
            "paperId": "71debf888acd57bb1baa4c146f31e58c66ea51af",
            "title": "On the Interactions of Structural Constraints and Data Resources for Structured Prediction",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1929423",
                    "name": "Zhisong Zhang"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        },
        {
            "paperId": "72cce47fd053bf916314d89a8174726c58c05e02",
            "title": "Towards Open-Domain Twitter User Profile Inference",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4428136",
                    "name": "Haoyang Wen"
                },
                {
                    "authorId": "123034558",
                    "name": "Zhenxin Xiao"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                }
            ]
        },
        {
            "paperId": "90df9c6924425d7366d16731a34bfa4e52ad5b2e",
            "title": "What\u2019s the Meaning of Superhuman Performance in Today\u2019s NLU?",
            "abstract": "In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140370472",
                    "name": "Simone Tedeschi"
                },
                {
                    "authorId": "3461596",
                    "name": "Johan Bos"
                },
                {
                    "authorId": "72836788",
                    "name": "T. Declerck"
                },
                {
                    "authorId": "144002335",
                    "name": "Jan Hajic"
                },
                {
                    "authorId": "2064295987",
                    "name": "Daniel Hershcovich"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "145542037",
                    "name": "Alexander Koller"
                },
                {
                    "authorId": "1755311",
                    "name": "Simon Krek"
                },
                {
                    "authorId": "2265382",
                    "name": "Steven Schockaert"
                },
                {
                    "authorId": "2082372",
                    "name": "Rico Sennrich"
                },
                {
                    "authorId": "2362276",
                    "name": "Ekaterina Shutova"
                },
                {
                    "authorId": "1733928",
                    "name": "Roberto Navigli"
                }
            ]
        },
        {
            "paperId": "ba31ccac5fe5ea151727e8427e78bb300c35f899",
            "title": "Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training",
            "abstract": "In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1929423",
                    "name": "Zhisong Zhang"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        },
        {
            "paperId": "c5a0a92fb521c7d1399cf26e63a6ffb806d1b291",
            "title": "A Textual Dataset for Situated Proactive Response Selection",
            "abstract": "Recent data-driven conversational models are able to return fluent, consistent, and informative responses to many kinds of requests and utterances in task-oriented scenarios.However, these responses are typically limited to just the immediate local topic instead of being wider-ranging and proactively taking the conversation further, for example making suggestions to help customers achieve their goals. This inadequacy reflects a lack of understanding of the interlocutor\u2019s situation and implicit goal. To address the problem, we introduce a task of proactive response selection based on situational information. We present a manually-curated dataset of 1.7k English conversation examples that include situational background information plus for each conversation a set of responses, only some of which are acceptable in the situation. A responsive and informed conversation system should select the appropriate responses and avoid inappropriate ones; doing so demonstrates the ability to adequately understand the initiating request and situation. Our benchmark experiments show that this is not an easy task even for strong neural models, offering opportunities for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145671279",
                    "name": "Naoki Otani"
                },
                {
                    "authorId": "50007145",
                    "name": "J. Araki"
                },
                {
                    "authorId": "2109893608",
                    "name": "Hyeongsik Kim"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        },
        {
            "paperId": "cc1a8ba1ba85f9e975b4b52bc5661c5c2a932ec0",
            "title": "Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation",
            "abstract": "We present PeerSum, a novel dataset for generating meta-reviews of scientific papers. The meta-reviews can be interpreted as abstractive summaries of reviews, multi-turn discussions and the paper abstract. These source documents have rich inter-document relationships with an explicit hierarchical conversational structure, cross-references and (occasionally) conflicting information. To introduce the structural inductive bias into pre-trained language models, we introduce Rammer ( Relationship-aware Multi-task Meta-review Generator), a model that uses sparse attention based on the conversational structure and a multi-task training objective that predicts metadata features (e.g., review ratings). Our experimental results show that Rammer outperforms other strong baseline models in terms of a suite of automatic evaluation metrics. Further analyses, however, reveal that RAMMER and other models struggle to handle conflicts in source documents of PeerSum, suggesting meta-review generation is a challenging task and a promising avenue for further research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143544766",
                    "name": "Miao Li"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "1800564",
                    "name": "Jey Han Lau"
                }
            ]
        },
        {
            "paperId": "0cff19c296456a6effc115af0907808d7bcf1138",
            "title": "DualGCN: Exploring Syntactic and Semantic Information for Aspect-Based Sentiment Analysis",
            "abstract": "The task of aspect-based sentiment analysis aims to identify sentiment polarities of given aspects in a sentence. Recent advances have demonstrated the advantage of incorporating the syntactic dependency structure with graph convolutional networks (GCNs). However, their performance of these GCN-based methods largely depends on the dependency parsers, which would produce diverse parsing results for a sentence. In this article, we propose a dual GCN (DualGCN) that jointly considers the syntax structures and semantic correlations. Our DualGCN model mainly comprises four modules: 1) SynGCN: instead of explicitly encoding syntactic structure, the SynGCN module uses the dependency probability matrix as a graph structure to implicitly integrate the syntactic information; 2) SemGCN: we design the SemGCN module with multihead attention to enhance the performance of the syntactic structure with the semantic information; 3) Regularizers: we propose orthogonal and differential regularizers to precisely capture semantic correlations between words by constraining attention scores in the SemGCN module; and 4) Mutual BiAffine: we use the BiAffine module to bridge relevant information between the SynGCN and SemGCN modules. Extensive experiments are conducted compared with up-to-date pretrained language encoders on two groups of datasets, one including Restaurant14, Laptop14, and Twitter and the other including Restaurant15 and Restaurant16. The experimental results demonstrate that the parsing results of various dependency parsers affect their performance of the GCN-based models. Our DualGCN model achieves superior performance compared with the state-of-the-art approaches. The source code and preprocessed datasets are provided and publicly available on GitHub (see https://github.com/CCChenhao997/DualGCN-ABSA).",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2462591",
                    "name": "Ruifan Li"
                },
                {
                    "authorId": "47666864",
                    "name": "Haoxing Chen"
                },
                {
                    "authorId": "39825530",
                    "name": "Fangxiang Feng"
                },
                {
                    "authorId": "1755773",
                    "name": "Zhanyu Ma"
                },
                {
                    "authorId": "38542466",
                    "name": "Xiaojie Wang"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        },
        {
            "paperId": "3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f",
            "title": "A Survey of Active Learning for Natural Language Processing",
            "abstract": "In this work, we provide a literature review of active learning (AL) for its applications in natural language processing (NLP). In addition to a fine-grained categorization of query strategies, we also investigate several other important aspects of applying AL to NLP problems. These include AL for structured prediction tasks, annotation cost, model learning (especially with deep neural models), and starting and stopping AL. Finally, we conclude with a discussion of related topics and future directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1929423",
                    "name": "Zhisong Zhang"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        },
        {
            "paperId": "51e549e1bb49032ea24bfe74dfa30d52c1172ae4",
            "title": "Counterfactual Data Augmentation improves Factuality of Abstractive Summarization",
            "abstract": "Abstractive summarization systems based on pretrained language models often generate coherent but factually inconsistent sentences. In this paper, we present a counterfactual data augmentation approach where we augment data with perturbed summaries that increase the training data diversity. Specifically, we present three augmentation approaches based on replacing (i) entities from other and the same category and (ii) nouns with their corresponding WordNet hypernyms. We show that augmenting the training data with our approach improves the factual correctness of summaries without significantly affecting the ROUGE score. We show that in two commonly used summarization datasets (CNN/Dailymail and XSum), we improve the factual correctness by about 2.5 points on average",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "2944868",
                    "name": "Siamak Shakeri"
                },
                {
                    "authorId": "1790831",
                    "name": "C. D. Santos"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "2152948655",
                    "name": "Chung-Ching Chang"
                }
            ]
        },
        {
            "paperId": "748a2700ec11f51560a69ec05c67ca9f97014be7",
            "title": "EvEntS ReaLM: Event Reasoning of Entity States via Language Models",
            "abstract": "This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3373316",
                    "name": "Evangelia Spiliopoulou"
                },
                {
                    "authorId": "51152502",
                    "name": "Artidoro Pagnoni"
                },
                {
                    "authorId": "3312309",
                    "name": "Yonatan Bisk"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        }
    ]
}