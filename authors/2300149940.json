{
    "authorId": "2300149940",
    "papers": [
        {
            "paperId": "1e3b64ef8d1b55a22dd3560aaaa7bb20cd4104f8",
            "title": "Large Language Models for Mobility in Transportation Systems: A Survey on Forecasting Tasks",
            "abstract": "Mobility analysis is a crucial element in the research area of transportation systems. Forecasting traffic information offers a viable solution to address the conflict between increasing transportation demands and the limitations of transportation infrastructure. Predicting human travel is significant in aiding various transportation and urban management tasks, such as taxi dispatch and urban planning. Machine learning and deep learning methods are favored for their flexibility and accuracy. Nowadays, with the advent of large language models (LLMs), many researchers have combined these models with previous techniques or applied LLMs to directly predict future traffic information and human travel behaviors. However, there is a lack of comprehensive studies on how LLMs can contribute to this field. This survey explores existing approaches using LLMs for mobility forecasting problems. We provide a literature review concerning the forecasting applications within transportation systems, elucidating how researchers utilize LLMs, showcasing recent state-of-the-art advancements, and identifying the challenges that must be overcome to fully leverage LLMs in this domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2300399012",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2300127756",
                    "name": "Yujie Sun"
                },
                {
                    "authorId": "2300283769",
                    "name": "Zepu Wang"
                },
                {
                    "authorId": "2300149940",
                    "name": "Yuqi Nie"
                },
                {
                    "authorId": "2300130847",
                    "name": "Xiaobo Ma"
                },
                {
                    "authorId": "2216789869",
                    "name": "Peng Sun"
                },
                {
                    "authorId": "2300132879",
                    "name": "Ruolin Li"
                }
            ]
        },
        {
            "paperId": "2cb3044ef42c7ee022a988864028b80ce977072c",
            "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts",
            "abstract": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2322999629",
                    "name": "Xiaoming Shi"
                },
                {
                    "authorId": "2322609248",
                    "name": "Shiyu Wang"
                },
                {
                    "authorId": "2300149940",
                    "name": "Yuqi Nie"
                },
                {
                    "authorId": "2322812784",
                    "name": "Dianqi Li"
                },
                {
                    "authorId": "2323439377",
                    "name": "Zhou Ye"
                },
                {
                    "authorId": "2312901184",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                }
            ]
        },
        {
            "paperId": "c30abb2ad76fcbfd4e6dc8881850b591d3434a3e",
            "title": "Foundation Models for Time Series Analysis: A Tutorial and Survey",
            "abstract": "Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advances in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored for time series analysis. This survey aims to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either application or pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future exploration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2262445381",
                    "name": "Haomin Wen"
                },
                {
                    "authorId": "2300149940",
                    "name": "Yuqi Nie"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2292142150",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "db9accd4e74a2c48887d572350b75ba663c679f1",
            "title": "Unlocking the Power of LSTM for Long Term Time Series Forecasting",
            "abstract": "Traditional recurrent neural network architectures, such as long short-term memory neural networks (LSTM), have historically held a prominent role in time series forecasting (TSF) tasks. While the recently introduced sLSTM for Natural Language Processing (NLP) introduces exponential gating and memory mixing that are beneficial for long term sequential learning, its potential short memory issue is a barrier to applying sLSTM directly in TSF. To address this, we propose a simple yet efficient algorithm named P-sLSTM, which is built upon sLSTM by incorporating patching and channel independence. These modifications substantially enhance sLSTM's performance in TSF, achieving state-of-the-art results. Furthermore, we provide theoretical justifications for our design, and conduct extensive comparative and analytical experiments to fully validate the efficiency and superior performance of our model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307607703",
                    "name": "Yaxuan Kong"
                },
                {
                    "authorId": "2300283769",
                    "name": "Zepu Wang"
                },
                {
                    "authorId": "2300149940",
                    "name": "Yuqi Nie"
                },
                {
                    "authorId": "2316586016",
                    "name": "Tian Zhou"
                },
                {
                    "authorId": "2279077978",
                    "name": "Stefan Zohren"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2216789869",
                    "name": "Peng Sun"
                },
                {
                    "authorId": "2307076087",
                    "name": "Qingsong Wen"
                }
            ]
        }
    ]
}