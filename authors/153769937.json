{
    "authorId": "153769937",
    "papers": [
        {
            "paperId": "8d3cc62fbe79b280a9084a43b295a4c77f7092ad",
            "title": "Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception",
            "abstract": "We present Integrated Multimodal Perception (IMP), a simple and scalable multimodal multi-task training and modeling approach. IMP integrates multimodal inputs including image, video, text, and audio into a single Transformer encoder with minimal modality-specific components. IMP makes use of a novel design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) for efficient model and task scaling. We conduct extensive empirical studies and reveal the following key insights: 1) Performing gradient descent updates by alternating on diverse modalities, loss functions, and tasks, with varying input resolutions, efficiently improves the model. 2) Sparsification with MoE on a single modality-agnostic encoder substantially improves the performance, outperforming dense models that use modality-specific encoders or additional fusion layers and greatly mitigates the conflicts between modalities. IMP achieves competitive performance on a wide range of downstream tasks including video classification, image classification, image-text, and video-text retrieval. Most notably, we train a sparse IMP-MoE-L variant focusing on video tasks that achieves new state-of-the-art in zero-shot video classification: 77.0% on Kinetics-400, 76.8% on Kinetics-600, and 68.3% on Kinetics-700, improving the previous state-of-the-art by +5%, +6.7%, and +5.8%, respectively, while using only 15% of their total training computational cost.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": "51208108",
                    "name": "D. Kondratyuk"
                },
                {
                    "authorId": "2115350367",
                    "name": "Yin Cui"
                },
                {
                    "authorId": "32225472",
                    "name": "Rachel Hornung"
                },
                {
                    "authorId": "3154495",
                    "name": "H. Wang"
                },
                {
                    "authorId": "2595180",
                    "name": "Hartwig Adam"
                }
            ]
        },
        {
            "paperId": "28630034bb29760df01ab033b743e30b37f336ae",
            "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
            "abstract": "Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145309103",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2059199",
                    "name": "Soravit Changpinyo"
                },
                {
                    "authorId": "8797855",
                    "name": "A. Piergiovanni"
                },
                {
                    "authorId": "31148950",
                    "name": "Piotr Padlewski"
                },
                {
                    "authorId": "3821662",
                    "name": "Daniel M. Salz"
                },
                {
                    "authorId": "7685850",
                    "name": "Sebastian Goodman"
                },
                {
                    "authorId": "3213962",
                    "name": "Adam Grycner"
                },
                {
                    "authorId": "40608942",
                    "name": "Basil Mustafa"
                },
                {
                    "authorId": "39611591",
                    "name": "Lucas Beyer"
                },
                {
                    "authorId": "144629422",
                    "name": "Alexander Kolesnikov"
                },
                {
                    "authorId": "1794202",
                    "name": "J. Puigcerver"
                },
                {
                    "authorId": "2066767241",
                    "name": "Nan Ding"
                },
                {
                    "authorId": "1996199677",
                    "name": "Keran Rong"
                },
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": "2159632445",
                    "name": "Gaurav Mishra"
                },
                {
                    "authorId": "2692973",
                    "name": "Linting Xue"
                },
                {
                    "authorId": "2904055",
                    "name": "Ashish V. Thapliyal"
                },
                {
                    "authorId": "40518045",
                    "name": "James Bradbury"
                },
                {
                    "authorId": "7987770",
                    "name": "Weicheng Kuo"
                },
                {
                    "authorId": "2678444",
                    "name": "Mojtaba Seyedhosseini"
                },
                {
                    "authorId": "2082436672",
                    "name": "Chao Jia"
                },
                {
                    "authorId": "143990191",
                    "name": "Burcu Karagol Ayan"
                },
                {
                    "authorId": "145814174",
                    "name": "C. Riquelme"
                },
                {
                    "authorId": "2079614268",
                    "name": "A. Steiner"
                },
                {
                    "authorId": "145426908",
                    "name": "A. Angelova"
                },
                {
                    "authorId": "2743563",
                    "name": "Xiaohua Zhai"
                },
                {
                    "authorId": "2815290",
                    "name": "N. Houlsby"
                },
                {
                    "authorId": "1737285",
                    "name": "Radu Soricut"
                }
            ]
        },
        {
            "paperId": "a5f4018833c6327b6dcb1f4001fcbe2e76743f3b",
            "title": "Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization",
            "abstract": "Self-supervised pre-training recently demonstrates success on large-scale multimodal data, and state-of-the-art contrastive learning methods often enforce the feature consistency from cross-modality inputs, such as video/audio or video/text pairs. Despite its convenience to formulate and leverage in practice, such cross-modality alignment (CMA) is only a weak and noisy supervision, since two modalities can be semantically misaligned even they are temporally aligned. For example, even in the commonly adopted instructional videos, a speaker can sometimes refer to something that is not visually present in the current frame; and the semantic misalignment would only be more unpredictable for the raw videos from the internet. We conjecture that might cause conflicts and biases among modalities, and may hence prohibit CMA from scaling up to training with larger and more heterogeneous data. This paper first verifies our conjecture by observing that, even in the latest VATT pre-training using only instructional videos, there exist strong gradient conflicts between different CMA losses within the same video, audio, text triplet, indicating them as the noisy source of supervision. We then propose to harmonize such gradients, via two techniques: (i) cross-modality gradient realignment: modifying different CMA loss gradients for each sample triplet, so that their gradient directions are more aligned; and (ii) gradient-based curriculum learning: leveraging the gradient conflict information on an indicator of sample noisiness, to develop a curriculum learning strategy to prioritize training on less noisy sample triplets. Applying those techniques to pre-training VATT on the HowTo100M dataset, we consistently improve its performance on different downstream tasks. Moreover, we are able to scale VATT pre-training to more complicated non-narrative Youtube8M dataset to further improve the state-of-the-arts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14737712",
                    "name": "Junru Wu"
                },
                {
                    "authorId": "2119123595",
                    "name": "Yi Liang"
                },
                {
                    "authorId": "2168024244",
                    "name": "Feng Han"
                },
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": "2969311",
                    "name": "Zhangyang Wang"
                },
                {
                    "authorId": "2117762585",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "f0524b3005720bcff886bcb0227f7f0dd924ff07",
            "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
            "abstract": "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": null,
                    "name": "Linagzhe Yuan"
                },
                {
                    "authorId": "47519958",
                    "name": "Rui Qian"
                },
                {
                    "authorId": "1764944",
                    "name": "Wei-Hong Chuang"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "50355189",
                    "name": "Yin Cui"
                },
                {
                    "authorId": "40206014",
                    "name": "Boqing Gong"
                }
            ]
        },
        {
            "paperId": "8c28c5195cb92cc633f82e401e0860560c1314fe",
            "title": "Estimating and interpreting nonlinear receptive field of sensory neural responses with deep neural network models",
            "abstract": "Our understanding of nonlinear stimulus transformations by neural circuits is hindered by the lack of comprehensive yet interpretable computational modeling frameworks. Here, we propose a data-driven approach based on deep neural networks to directly model arbitrarily nonlinear stimulus-response mappings. Reformulating the exact function of a trained neural network as a collection of stimulus-dependent linear functions enables a locally linear receptive field interpretation of the neural network. Predicting the neural responses recorded invasively from the auditory cortex of neurosurgical patients as they listened to speech, this approach significantly improves the prediction accuracy of auditory cortical responses, particularly in nonprimary areas. Moreover, interpreting the functions learned by neural networks uncovered three distinct types of nonlinear transformations of speech that varied considerably from primary to nonprimary auditory regions. The ability of this framework to capture arbitrary stimulus-response mappings while maintaining model interpretability leads to a better understanding of cortical processing of sensory signals.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1473699167",
                    "name": "Menoua Keshishian"
                },
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": "2634845",
                    "name": "B. Khalighinejad"
                },
                {
                    "authorId": "2083119",
                    "name": "J. Herrero"
                },
                {
                    "authorId": "31593564",
                    "name": "A. Mehta"
                },
                {
                    "authorId": "1686269",
                    "name": "N. Mesgarani"
                }
            ]
        },
        {
            "paperId": "c2b4d96db34bd472e84c9234838cc4e808eb1ba9",
            "title": "Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language",
            "abstract": "Neuro-symbolic representations have proved effective in learning structure information in vision and language. In this paper, we propose a new model architecture for learning multi-modal neuro-symbolic representations for video captioning. Our approach uses a dictionary learning-based method of learning relations between videos and their paired text descriptions. We refer to these relations as relative roles and leverage them to make each token role-aware using attention. This results in a more structured and interpretable architecture that incorporates modality-specific inductive biases for the captioning task. Intuitively, the model is able to learn spatial, temporal, and cross-modal relations in a given pair of video and text. The disentanglement achieved by our proposal gives the model more capacity to capture multi-modal structures which result in captions with higher quality for videos. Our experiments on two established video captioning datasets verifies the effectiveness of the proposed approach based on automatic metrics. We further conduct a human evaluation to measure the grounding and relevance of the generated captions and observe consistent improvement for the proposed model. The codes and trained models can be found at this https URL",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": "2542427",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "120157163",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "1845230025",
                    "name": "Sudha Rao"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "2097712360",
                    "name": "Roland Fernandez"
                },
                {
                    "authorId": "1748557",
                    "name": "P. Smolensky"
                },
                {
                    "authorId": "1800422",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "c9bb243bfcb41be3a40492e5768ba806054ec988",
            "title": "Estimating and interpreting nonlinear receptive fields of sensory responses with deep neural network models",
            "abstract": "Sensory processing by neural circuits includes numerous nonlinear transformations that are critical to perception. Our understanding of these nonlinear mechanisms, however, is hindered by the lack of a comprehensive and interpretable computational framework that can model and explain nonlinear signal transformations. Here, we propose a data-driven framework based on deep neural network regression models that can directly learn any nonlinear stimulus-response mapping. A key component of this approach is an analysis method that reformulates the exact function of the trained neural network as a collection of stimulus-dependent linear functions. This locally linear receptive field interpretation of the network function enables straightforward comparison with conventional receptive field models and uncovers nonlinear encoding properties. We demonstrate the efficacy of this framework by predicting the neural responses recorded invasively from the auditory cortex of neurosurgical patients as they listened to speech. Our method significantly improves the prediction accuracy of auditory cortical responses particularly in nonprimary areas. Moreover, interpreting the functions learned by neural networks uncovered three distinct types of nonlinear transformations of speech that varied considerably in primary and nonprimary auditory regions. By combining two desired properties of a computational sensory-response model; the ability to capture arbitrary stimulus-response mappings and maintaining model interpretability, this data-driven method can lead to better neurophysiological models of the sensory processing.",
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1473699167",
                    "name": "Menoua Keshishian"
                },
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": "2634845",
                    "name": "B. Khalighinejad"
                },
                {
                    "authorId": "2083119",
                    "name": "J. Herrero"
                },
                {
                    "authorId": "31593564",
                    "name": "A. Mehta"
                },
                {
                    "authorId": "1686269",
                    "name": "N. Mesgarani"
                }
            ]
        },
        {
            "paperId": "cc17571a5ae906cd473ace3abe9f4cb01b543bc5",
            "title": "GAIA at SM-KBP 2019 - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System",
            "abstract": "Extraction and Hypothesis Generation System Manling Li, Ying Lin, Ananya Subburathinam, Spencer Whitehead, Xiaoman Pan, Di Lu, Qingyun Wang, Tongtao Zhang, Lifu Huang, Heng Ji 1 University of Illinois at Urbana-Champaign hengji@illinois.edu Alireza Zareian, Hassan Akbari, Brian Chen, Bo Wu, Emily Allaway, Shih-Fu Chang, Kathleen McKeown 2 Columbia University sc250@columbia.edu, kathy@cs.columbia.edu Yixiang Yao, Jennifer Chen, Eric Berquist, Kexuan Sun, Xujun Peng, Ryan Gabbard Marjorie Freedman, Pedro Szekely, T.K. Satish Kumar 3 Information Sciences Institute, University of Southern California mrf@isi.edu Arka Sadhu, Ram Nevatia University of Southern California nevatia@usc.edu Miguel Rodriguez5, Yifan Wang5, Yang Bai5, Ali Sadeghian4, Daisy Zhe Wang5 5 University of Florida daisyw@ufl.edu",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3361240",
                    "name": "Manling Li"
                },
                {
                    "authorId": "49417338",
                    "name": "Ying Lin"
                },
                {
                    "authorId": "3393606",
                    "name": "Ananya Subburathinam"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "152347526",
                    "name": "Di Lu"
                },
                {
                    "authorId": "1786863",
                    "name": "Qingyun Wang"
                },
                {
                    "authorId": "2111626",
                    "name": "Tongtao Zhang"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2778637",
                    "name": "Alireza Zareian"
                },
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "46208659",
                    "name": "Emily Allaway"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "145590324",
                    "name": "K. McKeown"
                },
                {
                    "authorId": "27039165",
                    "name": "Yixiang Yao"
                },
                {
                    "authorId": "2115897093",
                    "name": "Jennifer Chen"
                },
                {
                    "authorId": "14993211",
                    "name": "E. Berquist"
                },
                {
                    "authorId": "35329068",
                    "name": "Kexuan Sun"
                },
                {
                    "authorId": "2109627387",
                    "name": "Xujun Peng"
                },
                {
                    "authorId": "50543673",
                    "name": "Ryan Gabbard"
                },
                {
                    "authorId": "2052513135",
                    "name": "Marjorie Freedman"
                },
                {
                    "authorId": "2628881",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "2066758459",
                    "name": "T. K. S. Kumar"
                },
                {
                    "authorId": "2064142411",
                    "name": "Arka Sadhu"
                },
                {
                    "authorId": "1694832",
                    "name": "R. Nevatia"
                },
                {
                    "authorId": "144078631",
                    "name": "Miguel E. Rodr\u00edguez"
                },
                {
                    "authorId": "2115569383",
                    "name": "Yifan Wang"
                },
                {
                    "authorId": "1471403524",
                    "name": "Yang Bai"
                },
                {
                    "authorId": "51283807",
                    "name": "A. Sadeghian"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        },
        {
            "paperId": "2718cd594d2aa09315da52594877cd71d377dfcf",
            "title": "Multi-Level Multimodal Common Semantic Space for Image-Phrase Grounding",
            "abstract": "We address the problem of phrase grounding by learning a multi-level common semantic space shared by the textual and visual modalities. We exploit multiple levels of feature maps of a Deep Convolutional Neural Network, as well as contextualized word and sentence embeddings extracted from a character-based language model. Following dedicated non-linear mappings for visual features at each level, word, and sentence embeddings, we obtain multiple instantiations of our common semantic space in which comparisons between any target text and the visual content is performed with cosine similarity. We guide the model by a multi-level multimodal attention mechanism which outputs attended visual features at each level. The best level is chosen to be compared with text content for maximizing the pertinence scores of image-sentence pairs of the ground truth. Experiments conducted on three publicly available datasets show significant performance gains (20%-60% relative) over the state-of-the-art in phrase localization and set a new performance record on those datasets. We provide a detailed ablation study to show the contribution of each element of our approach and release our code on GitHub.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": "35862299",
                    "name": "Svebor Karaman"
                },
                {
                    "authorId": "1754397",
                    "name": "Surabhi Bhargava"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "1856025",
                    "name": "Carl Vondrick"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "60d3ad4894912e72cd5ff36c928cd65087f06483",
            "title": "GAIA - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System",
            "abstract": "Tongtao Zhang, Ananya Subburathinam, Ge Shi, Lifu Huang, Di Lu, Xiaoman Pan, Manling Li, Boliang Zhang, Qingyun Wang, Spencer Whitehead, Heng Ji 1 Rensselaer Polytechnic Institute jih@rpi.edu Alireza Zareian, Hassan Akbari, Brian Chen, Ruiqi Zhong, Steven Shao, Emily Allaway, Shih-Fu Chang, Kathleen McKeown 2 Columbia University sc250@columbia.edu, kathy@cs.columbia.edu Dongyu Li, Xin Huang, Kexuan Sun, Xujun Peng, Ryan Gabbard, Marjorie Freedman, Mayank Kejriwal, Ram Nevatia, Pedro Szekely, T.K. Satish Kumar 3 Information Sciences Institute, University of Southern California mrf@isi.edu Ali Sadeghian4, Giacomo Bergami4, Sourav Dutta4, Miguel Rodriguez4, Daisy Zhe Wang4 4 University of Florida daisyw@ufl.edu",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111626",
                    "name": "Tongtao Zhang"
                },
                {
                    "authorId": "3393606",
                    "name": "Ananya Subburathinam"
                },
                {
                    "authorId": "2067725506",
                    "name": "Ge Shi"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                },
                {
                    "authorId": "152347526",
                    "name": "Di Lu"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "3361240",
                    "name": "Manling Li"
                },
                {
                    "authorId": "38629264",
                    "name": "Boliang Zhang"
                },
                {
                    "authorId": "1786863",
                    "name": "Qingyun Wang"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2778637",
                    "name": "Alireza Zareian"
                },
                {
                    "authorId": "153769937",
                    "name": "Hassan Akbari"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "51011000",
                    "name": "Ruiqi Zhong"
                },
                {
                    "authorId": "46465420",
                    "name": "Steven Shao"
                },
                {
                    "authorId": "46208659",
                    "name": "Emily Allaway"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "145590324",
                    "name": "K. McKeown"
                },
                {
                    "authorId": "2115198771",
                    "name": "Dongyu Li"
                },
                {
                    "authorId": "2152663974",
                    "name": "Xin Huang"
                },
                {
                    "authorId": "35329068",
                    "name": "Kexuan Sun"
                },
                {
                    "authorId": "2109627387",
                    "name": "Xujun Peng"
                },
                {
                    "authorId": "50543673",
                    "name": "Ryan Gabbard"
                },
                {
                    "authorId": "2052513135",
                    "name": "Marjorie Freedman"
                },
                {
                    "authorId": "2683157",
                    "name": "M. Kejriwal"
                },
                {
                    "authorId": "1694832",
                    "name": "R. Nevatia"
                },
                {
                    "authorId": "2628881",
                    "name": "Pedro A. Szekely"
                },
                {
                    "authorId": "2066758459",
                    "name": "T. K. S. Kumar"
                },
                {
                    "authorId": "51283807",
                    "name": "A. Sadeghian"
                },
                {
                    "authorId": "49351295",
                    "name": "Giacomo Bergami"
                },
                {
                    "authorId": "50757978",
                    "name": "S. Dutta"
                },
                {
                    "authorId": "144078631",
                    "name": "Miguel E. Rodr\u00edguez"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        }
    ]
}