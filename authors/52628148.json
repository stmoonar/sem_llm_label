{
    "authorId": "52628148",
    "papers": [
        {
            "paperId": "049029a6ce83710c565939359047bf25affabffd",
            "title": "Powerful A/B-Testing Metrics and Where to Find Them",
            "abstract": "Online controlled experiments, colloquially known as A/B-tests, are the bread and butter of real-world recommender system evaluation. Typically, end-users are randomly assigned some system variant, and a plethora of metrics are then tracked, collected, and aggregated throughout the experiment. A North Star metric (e.g. long-term growth or revenue) is used to assess which system variant should be deemed superior. As a result, most collected metrics are supporting in nature, and serve to either (i) provide an understanding of how the experiment impacts user experience, or (ii) allow for confident decision-making when the North Star metric moves insignificantly (i.e. a false negative or type-II error). The latter is not straightforward: suppose a treatment variant leads to fewer but longer sessions, with more views but fewer engagements; should this be considered a positive or negative outcome? The question then becomes: how do we assess a supporting metric's utility when it comes to decision-making using A/B-testing? Online platforms typically run dozens of experiments at any given time. This provides a wealth of information about interventions and treatment effects that can be used to evaluate metrics' utility for online evaluation. We propose to collect this information and leverage it to quantify type-I, type-II, and type-III errors for the metrics of interest, alongside a distribution of measurements of their statistical power (e.g. $z$-scores and $p$-values). We present results and insights from building this pipeline at scale for two large-scale short-video platforms: ShareChat and Moj; leveraging hundreds of past experiments to find online metrics with high statistical power.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "2278433198",
                    "name": "Shubham Baweja"
                },
                {
                    "authorId": "2269468940",
                    "name": "Neeti Pokharna"
                },
                {
                    "authorId": "2269471595",
                    "name": "Aleksei Ustimenko"
                }
            ]
        },
        {
            "paperId": "292c34ff0dd1582e2d00ba6482009c445064367e",
            "title": "\u0394-OPE: Off-Policy Estimation with Pairs of Policies",
            "abstract": "The off-policy paradigm casts recommendation as a counterfactual decision-making task, allowing practitioners to unbiasedly estimate online metrics using offline data. This leads to effective evaluation metrics, as well as learning procedures that directly optimise online success. Nevertheless, the high variance that comes with unbiasedness is typically the crux that complicates practical applications. An important insight is that the difference between policy values can often be estimated with significantly reduced variance, if said policies have positive covariance. This allows us to formulate a pairwise off-policy estimation task: $\\Delta\\text{-}{\\rm OPE}$. $\\Delta\\text{-}{\\rm OPE}$ subsumes the common use-case of estimating improvements of a learnt policy over a production policy, using data collected by a stochastic logging policy. We introduce $\\Delta\\text{-}{\\rm OPE}$ methods based on the widely used Inverse Propensity Scoring estimator and its extensions. Moreover, we characterise a variance-optimal additive control variate that further enhances efficiency. Simulated, offline, and online experiments show that our methods significantly improve performance for both evaluation and learning tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "2269471595",
                    "name": "Aleksei Ustimenko"
                }
            ]
        },
        {
            "paperId": "2d8cd0b72363a515c3089480fdfbd70bbf637b8e",
            "title": "Variance Reduction in Ratio Metrics for Efficient Online Experiments",
            "abstract": "Online controlled experiments, such as A/B-tests, are commonly used by modern tech companies to enable continuous system improvements. Despite their paramount importance, A/B-tests are expensive: by their very definition, a percentage of traffic is assigned an inferior system variant. To ensure statistical significance on top-level metrics, online experiments typically run for several weeks. Even then, a considerable amount of experiments will lead to inconclusive results (i.e. false negatives, or type-II error). The main culprit for this inefficiency is the variance of the online metrics. Variance reduction techniques have been proposed in the literature, but their direct applicability to commonly used ratio metrics (e.g. click-through rate or user retention) is limited. In this work, we successfully apply variance reduction techniques to ratio metrics on a large-scale short-video platform: ShareChat. Our empirical results show that we can either improve A/B-test confidence in 77% of cases, or can retain the same level of confidence with 30% fewer data points. Importantly, we show that the common approach of including as many covariates as possible in regression is counter-productive, highlighting that control variates based on Gradient-Boosted Decision Tree predictors are most effective. We discuss the practicalities of implementing these methods at scale and showcase the cost reduction they beget.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2278433198",
                    "name": "Shubham Baweja"
                },
                {
                    "authorId": "2269468940",
                    "name": "Neeti Pokharna"
                },
                {
                    "authorId": "2269471595",
                    "name": "Aleksei Ustimenko"
                },
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                }
            ]
        },
        {
            "paperId": "6f5ba48c8d50d9a0e7c7325bdeb60b5dd52e717a",
            "title": "Multi-Objective Recommendation via Multivariate Policy Learning",
            "abstract": "Real-world recommender systems often need to balance multiple objectives when deciding which recommendations to present to users. These include behavioural signals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g. diversity, fairness). Scalarisation methods are commonly used to handle this balancing task, where a weighted average of per-objective reward signals determines the final score used for ranking. Naturally, how these weights are computed exactly, is key to success for any online platform. We frame this as a decision-making task, where the scalarisation weights are actions taken to maximise an overall North Star reward (e.g. long-term user retention or growth). We extend existing policy learning methods to the continuous multivariate action domain, proposing to maximise a pessimistic lower bound on the North Star reward that the learnt policy will yield. Typical lower bounds based on normal approximations suffer from insufficient coverage, and we propose an efficient and effective policy-dependent correction for this. We provide guidance to design stochastic data collection policies, as well as highly sensitive reward signals. Empirical observations from simulations, offline and online experiments highlight the efficacy of our deployed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "2000976308",
                    "name": "Jatin Mandav"
                },
                {
                    "authorId": "2225233752",
                    "name": "Ivan Potapov"
                },
                {
                    "authorId": "2299941331",
                    "name": "Nakul Agarwal"
                },
                {
                    "authorId": "2299943728",
                    "name": "Sourabh Vaid"
                },
                {
                    "authorId": "2269717809",
                    "name": "Wenzhe Shi"
                },
                {
                    "authorId": "2269471595",
                    "name": "Aleksei Ustimenko"
                }
            ]
        },
        {
            "paperId": "82db0ef24f9c637668cf30145cb18c40b1a2f485",
            "title": "Learning-to-Rank with Nested Feedback",
            "abstract": "Many platforms on the web present ranked lists of content to users, typically optimized for engagement-, satisfaction- or retention- driven metrics. Advances in the Learning-to-Rank (LTR) research literature have enabled rapid growth in this application area. Several popular interfaces now include nested lists, where users can enter a 2nd-level feed via any given 1st-level item. Naturally, this has implications for evaluation metrics, objective functions, and the ranking policies we wish to learn. We propose a theoretically grounded method to incorporate 2nd-level feedback into any 1st-level ranking model. Online experiments on a large-scale recommendation system confirm our theoretical findings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223763954",
                    "name": "Hitesh Sagtani"
                },
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "2269471595",
                    "name": "Aleksei Ustimenko"
                }
            ]
        },
        {
            "paperId": "ee002fbee64cbaeedfbe4376531b89f79771e5ec",
            "title": "Learning Metrics that Maximise Power for Accelerated A/B-Tests",
            "abstract": "Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent. We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "2269471595",
                    "name": "Aleksei Ustimenko"
                }
            ]
        },
        {
            "paperId": "0acb68fba43d6fde2d65bee9593bd2f8526552bb",
            "title": "On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top-n Recommendation",
            "abstract": "Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years. Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR. Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order. Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric's inherent assumptions are violated. This statement no longer holds for its normalised variant, suggesting that nDCG's practical utility may be limited.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "2225233752",
                    "name": "Ivan Potapov"
                },
                {
                    "authorId": "146211662",
                    "name": "Aleksei Ustimenko"
                }
            ]
        },
        {
            "paperId": "42f4ca818ebc4913cf00b26f64175e2c732411bb",
            "title": "On Gradient Boosted Decision Trees and Neural Rankers: A Case-Study on Short-Video Recommendations at ShareChat",
            "abstract": "Practitioners who wish to build real-world applications that rely on ranking models, need to decide which modelling paradigm to follow. This is not an easy choice to make, as the research literature on this topic has been shifting in recent years. In particular, whilst Gradient Boosted Decision Trees (GBDTs) have reigned supreme for more than a decade, the flexibility of neural networks has allowed them to catch up, and recent works report accuracy metrics that are on par. Nevertheless, practical systems require considerations beyond mere accuracy metrics to decide on a modelling approach. This work describes our experiences in balancing some of the trade-offs that arise, presenting a case study on a short-video recommendation application. We highlight We believe these findings are of relevance to researchers in both academia and industry, and hope they can inspire practitioners who need to make similar modelling choices in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "2223763954",
                    "name": "Hitesh Sagtani"
                },
                {
                    "authorId": "2269471877",
                    "name": "Himanshu Doi"
                },
                {
                    "authorId": "2269471736",
                    "name": "Rasul Karimov"
                },
                {
                    "authorId": "2269468940",
                    "name": "Neeti Pokharna"
                },
                {
                    "authorId": "9159754",
                    "name": "D. Kalim"
                },
                {
                    "authorId": "2269471595",
                    "name": "Aleksei Ustimenko"
                },
                {
                    "authorId": "2269473646",
                    "name": "Christopher Green"
                },
                {
                    "authorId": "2269717809",
                    "name": "Wenzhe Shi"
                },
                {
                    "authorId": "2243336832",
                    "name": "Rishabh Mehrotra"
                }
            ]
        },
        {
            "paperId": "676ea4c1a264c11055cb385f1ac892ff790a8a04",
            "title": "Practical Bandits: An Industry Perspective",
            "abstract": "The bandit paradigm provides a unified modeling framework for problems that require decision-making under uncertainty. Because many business metrics can be viewed as rewards (a.k.a. utilities) that result from actions, bandit algorithms have seen a large and growing interest from industrial applications, such as search, recommendation and advertising. Indeed, with the bandit lens comes the promise of direct optimisation for the metrics we care about. Nevertheless, the road to successfully applying bandits in production is not an easy one. Even when the action space and rewards are well-defined, practitioners still need to make decisions regarding multi-arm or contextual approaches, on- or off-policy setups, delayed or immediate feedback, myopic or long-term optimisation, etc. To make matters worse, industrial platforms typically give rise to large action spaces in which existing approaches tend to break down. The research literature on these topics is broad and vast, but this can overwhelm practitioners, whose primary aim is to solve practical problems, and therefore need to decide on a specific instantiation or approach for each project. This tutorial will take a step towards filling that gap between the theory and practice of bandits. Our goal is to present a unified overview of the field and its existing terminology, concepts and algorithms---with a focus on problems relevant to industry. We hope our industrial perspective will help future practitioners who wish to leverage the bandit paradigm for their application.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "81269354",
                    "name": "Bram van den Akker"
                },
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "2155508209",
                    "name": "Ying Li"
                },
                {
                    "authorId": "2085850",
                    "name": "Ben London"
                },
                {
                    "authorId": "46340159",
                    "name": "Zahra Nazari"
                },
                {
                    "authorId": "48331451",
                    "name": "Devesh Parekh"
                }
            ]
        },
        {
            "paperId": "c287b4d2428dd7f8eb207661bfa80544e59b13a4",
            "title": "A Probabilistic Position Bias Model for Short-Video Recommendation Feeds",
            "abstract": "Modern web-based platforms often show ranked lists of recommendations to users, in an attempt to maximise user satisfaction or business metrics. Typically, the goal of such systems boils down to maximising the exposure probability \u2014conversely, minimising the rank\u2014 for items that are deemed \u201creward-maximising\u201d according to some metric of interest. This general framing comprises music or movie streaming applications, as well as e-commerce, restaurant or job recommendations, and even web search. Position bias or user models can be used to estimate exposure probabilities for each use-case, specifically tailored to how users interact with the presented rankings. A unifying factor in these diverse problem settings is that typically only one or several items will be engaged with (clicked, streamed, purchased, et cetera) before a user leaves the ranked list. Short-video feeds on social media platforms diverge from this general framing in several ways, most notably that users do not tend to leave the feed after, for example, liking a post. Indeed, seemingly infinite feeds invite users to scroll further down the ranked list. For this reason, existing position bias or user models tend to fall short in such settings, as they do not accurately capture users\u2019 interaction modalities. In this work, we propose a novel and probabilistically sound personalised position bias model for feed recommendations. We focus on a 1st-level feed in a hierarchical structure, where users may enter a 2nd-level feed via any given 1st-level item. We posit that users come to the platform with a given scrolling budget that is drawn according to a discrete power-law distribution, and show how the survival function of said distribution can be used to obtain closed-form estimates for personalised exposure probabilities. Empirical insights gained through data from a large-scale social media platform show how our probabilistic position bias model more accurately captures empirical exposure than existing models, and paves the way for improved unbiased evaluation and learning-to-rank.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52628148",
                    "name": "Olivier Jeunen"
                }
            ]
        }
    ]
}