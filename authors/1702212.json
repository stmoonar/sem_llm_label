{
    "authorId": "1702212",
    "papers": [
        {
            "paperId": "7368bef6b6ddcdf7bf35350491bf69e6574324c0",
            "title": "Learning Approximation Sets for Exploratory Queries",
            "abstract": "In data exploration, executing complex non-aggregate queries over large databases can be time-consuming. Our paper introduces a novel approach to address this challenge, focusing on finding an optimized subset of data, referred to as the approximation set, for query execution. The goal is to maximize query result quality while minimizing execution time. We formalize this problem as Approximate Non-Aggregates Query Processing (ANAQP) and establish its NP-completeness. To tackle this, we propose an approximate solution using advanced Reinforcement Learning architecture, termed ASQP-RL. This approach overcomes challenges related to the large action space and the need for generalization beyond a known query workload. Experimental results on two benchmarks demonstrate the superior performance of ASQP-RL, outperforming baselines by 30% in accuracy and achieving efficiency gains of 10-35X. Our research sheds light on the potential of reinforcement learning techniques for advancing data management tasks. Experimental results on two benchmarks show that ASQP-RL significantly outperforms the baselines both in terms of accuracy (30% better) and efficiency (10-35X). This research provides valuable insights into the potential of RL techniques for future advancements in data management tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282994605",
                    "name": "Susan B. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                },
                {
                    "authorId": "50479435",
                    "name": "Gal Zeevi"
                }
            ]
        },
        {
            "paperId": "8051a66ebf8248a522dbe5209fa2f6a66d9b66e1",
            "title": "Cost-Effective LLM Utilization for Machine Learning Tasks over Tabular Data",
            "abstract": "Classic machine learning (ML) models excel in modeling tabular datasets but lack broader world knowledge due to the absence of pre-training, an area where Large Language Models (LLMs) stand out. This paper presents an effective method that bridges the gap, leveraging LLMs to enrich tabular data to enhance the performance of classical ML models. Despite the previously limited success of direct LLM application to tabular tasks due to their high computational demands, our approach selectively enriches datasets with essential world knowledge, balancing performance improvement with cost-effectiveness. This work advances the capabilities of traditional ML models and opens new avenues for research at the convergence of classical ML and LLMs, marking the onset of a new era in cost-effective data enrichment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305381231",
                    "name": "Yael Einy"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "8f5fa93bc809c2a230214cfba036afcca2a45406",
            "title": "ASQP-RL Demo: Learning Approximation Sets for Exploratory Queries",
            "abstract": "We demonstrate the Approximate Selection Query Processing (ASQP-RL) system, which uses Reinforcement Learning to select a subset of a large external dataset to process locally in a notebook during data exploration. Given a query workload over an external database and notebook memory size, the system translates the workload to select-project-join (non-aggregate) queries and finds a subset of each relation such that the data subset - called the approximation set - fits into the notebook memory and maximizes query result quality. The data subset can then be loaded into the notebook, and rapidly queried by the analyst. Our demonstration shows how ASQP-RL can be used during data exploration and achieve comparable results to external queries over the large dataset at significantly reduced query times. It also shows how ASQP-RL can be used for aggregation queries, achieving surprisingly good results compared to state-of-the-art techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282994605",
                    "name": "Susan B. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                },
                {
                    "authorId": "50479435",
                    "name": "Gal Zeevi"
                }
            ]
        },
        {
            "paperId": "a3a9f5aa8d371e4f1fb514be6cc345524221671d",
            "title": "LINX: A Language Driven Generative System for Goal-Oriented Automated Data Exploration",
            "abstract": "Data exploration is a challenging process in which users examine a dataset by iteratively employing a series of queries. While in some cases the user explores a new dataset to become familiar with it, more often, the exploration process is conducted with a specific analysis goal or question in mind. To assist users in exploring a new dataset, Automated Data Exploration (ADE) systems have been devised in previous work. These systems aim to auto-generate a full exploration session, containing a sequence of queries that showcase interesting elements of the data. However, existing ADE systems are often constrained by a predefined objective function, thus always generating the same session for a given dataset. Therefore, their effectiveness in goal-oriented exploration, in which users need to answer specific questions about the data, are extremely limited. To this end, this paper presents LINX, a generative system augmented with a natural language interface for goal-oriented ADE. Given an input dataset and an analytical goal described in natural language, LINX generates a personalized exploratory session that is relevant to the user's goal. LINX utilizes a Large Language Model (LLM) to interpret the input analysis goal, and then derive a set of specifications for the desired output exploration session. These specifications are then transferred to a novel, modular ADE engine based on Constrained Deep Reinforcement Learning (CDRL), which can adapt its output according to the specified instructions. To validate LINX's effectiveness, we introduce a new benchmark dataset for goal-oriented exploration and conduct an extensive user study. Our analysis underscores LINX's superior capability in producing exploratory notebooks that are significantly more relevant and beneficial than those generated by existing solutions, including ChatGPT, goal-agnostic ADE, and commercial systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219551725",
                    "name": "Tavor Lipman"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                },
                {
                    "authorId": "51174907",
                    "name": "Tomer Wolfson"
                },
                {
                    "authorId": "2305484452",
                    "name": "Oz Zafar"
                }
            ]
        },
        {
            "paperId": "f0f61b81865675b41bc466ad76f95e6b3f4586e4",
            "title": "TabEE: Tabular Embeddings Explanations",
            "abstract": "Tabular embedding methods have become increasingly popular due to their effectiveness in improving the results of various tasks, including classic databases tasks and machine learning predictions. However, most current methods treat these embedding models as \"black boxes\" making it difficult to understand the insights captured by the models. Our research proposes a novel approach to interpret these models, aiming to provide local and global explanations for the original data and detect potential flaws in the embedding models. The proposed solution is appropriate for every tabular embedding algorithm, as it fits the black box view of the embedding model. Furthermore, we propose methods for comparing different embedding models, which can help identify data biases that might impact the models' credibility without the user's knowledge. Our approach is evaluated on multiple datasets and multiple embeddings, demonstrating that our proposed explanations provide valuable insights into the behavior of tabular embedding methods. By making these models more transparent, we believe our research will contribute to the development of more effective and reliable embedding methods for a wide range of applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293867432",
                    "name": "Roni Copul"
                },
                {
                    "authorId": "32915630",
                    "name": "Nave Frost"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                }
            ]
        },
        {
            "paperId": "080445f828610db80b0eedeed93e526301e8ac5f",
            "title": "Efficiently Archiving Photos under Storage Constraints",
            "abstract": "Our ability to collect data is rapidly outstripping our ability to effectively store and use it. Organizations are therefore facing tough decisions of what data to archive (or dispose of) to effectively meet their business goals. We address this general problem in the context of image data (photos) by proposing which photos to archive to meet an online storage budget. The decision is based on factors such as usage patterns and their relative importance, the quality and size of a photo, the relevance of a photo for a usage pattern, the similarity between different photos, as well as policy requirements of what photos must be retained. We formalize the photo archival problem, analyze its complexity, and give two approximation algorithms. One algorithm comes with an optimal approximation guarantee and another, more scalable, algorithm that comes with both worst-case and data-dependent guarantees. Based on these algorithms we implemented an end-to-end system, PHOcus, and discuss how to automatically derive the inputs for this system in many settings. An extensive experimental study based on public as well as private datasets demonstrates the effectiveness and efficiency of PHOcus. Furthermore, a user study using business analysts in a real e-commerce application shows that it can save a tremendous amount of human effort and yield unexpected insights.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2184486608",
                    "name": "May Shoshan"
                }
            ]
        },
        {
            "paperId": "934d27f239411b26117bc3474b7b72b6850fe93d",
            "title": "ATENA-PRO: Generating Personalized Exploration Notebooks with Constrained Reinforcement Learning",
            "abstract": "One of the most common, helpful practices of data scientists, when starting the exploration of a given dataset, is to examine existing data exploration notebooks prepared by other data analysts or scientists. These notebooks contain curated sessions of contextually-related query operations that together demonstrate interesting hypotheses and conjectures on the data. Unfortunately,relevant such notebooks, that had been prepared on the same dataset, and in light of thesame analysis task, are often nonexistent or unavailable. In this work, we describe ATENA-PRO, a framework for auto-generating such relevant, personalized exploratory sessions. Using a novel specification language, users first describe their desired output notebook. Our language contains dedicated constructs for contextually connecting future output queries. These specifications are then used as input for a Deep Reinforcement Learning (DRL) engine, which auto-generates the personalized notebook. Our DRL engine relies on an existing, general-purpose, DRL framework for data exploration. However, augmenting the generic framework with user specifications requires overcoming a difficult sparsity challenge, as only a small portion of the possible sessions may be compliant with the specifications. Inspired by solutions for constrained reinforcement learning, we devise a compound, flexible reward scheme as well as specification-aware neural network architecture. Our experimental evaluation shows that the combination of these components allows ATENA-PRO to consistently generate interesting, personalized exploration sessions for various analysis tasks and datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219551725",
                    "name": "Tavor Lipman"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "0ed9572639c6e042039cfe20be49dd70c31120c0",
            "title": "Classifier Construction Under Budget Constraints",
            "abstract": "Search mechanisms over large assortments of items are central to the operation of many platforms. As users commonly express filtering conditions based on item properties that are not initially stored, companies must derive the missing information by training and applying binary classifiers. Choosing which classifiers to construct is however not trivial, since classifiers differ in construction costs and range of applicability. Previous work has considered the problem of selecting a classifier set of minimum construction cost, but this has been done under the (often unrealistic) assumption that the available budget is unlimited and allows to support all search queries. In practice, budget constraints require prioritizing some queries over others. To capture this consideration, we study in this work a more general model that allows assigning to each search query a score that models how important it is to compute its result set and examine the optimization problem of selecting a classifier set, whose cost is within the budget, that maximizes the overall score of the queries it can answer. We show that this generalization is likely much harder to approximate complexity-wise, even assuming limited special cases. Nevertheless, we devise a heuristic algorithm, whose effectiveness is demonstrated in our experimental study over real-world data, consisting of a public dataset and datasets provided by a large e-commerce company that include costs and scores derived by business analysts. Finally, we show that our methods are applicable also for related problems in practical settings where there is some flexibility in determining the budget.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                }
            ]
        },
        {
            "paperId": "164716602ccb0f4e8af9ec07e02d817b6890ac64",
            "title": "FEDEX: An Explainability Framework for Data Exploration Steps",
            "abstract": "When exploring a new dataset, Data Scientists often apply analysis queries, look for insights in the resulting dataframe, and repeat to apply further queries. We propose in this paper a novel solution that assists data scientists in this laborious process. In a nutshell, our solution pinpoints the most interesting (sets of) rows in each obtained dataframe. Uniquely, our definition of interest is based on the contribution of each row to the interestingness of different columns of the entire dataframe, which, in turn, is defined using standard measures such as diversity and exceptionality. Intuitively, interesting rows are ones that explain why (some column of) the analysis query result is interesting as a whole. Rows are correlated in their contribution and so the interesting score for a set of rows may not be directly computed based on that of individual rows. We address the resulting computational challenge by restricting attention to semantically-related sets, based on multiple notions of semantic relatedness; these sets serve as more informative explanations. Our experimental study across multiple real-world datasets shows the usefulness of our system in various scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "32466098",
                    "name": "Amir Gilad"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2184781246",
                    "name": "Amit Mualem"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "32101c4d567240a00ee5027a5f8b403682a8a295",
            "title": "Automated Category Tree Construction in E-Commerce",
            "abstract": "Category trees play a central role in many web applications, enabling browsing-style information access. Building trees that reflect users' dynamic interests is, however, a challenging task, carried out by taxonomists. This manual construction leads to outdated trees as it is hard to keep track of market trends. While taxonomists can identify candidate categories, i.e. sets of items with a shared label, most such categories cannot simultaneously exist in the tree, as platforms set a bound on the number of categories an item may belong to. To address this setting, we formalize the problem of constructing a tree where the categories are maximally similar to desirable candidate categories while satisfying combinatorial requirements and provide a model that captures practical considerations. In previous work, we proved inapproximability bounds for this model. Nevertheless, in this work we provide two heuristic algorithms, and demonstrate their effectiveness over datasets from real-life e-commerce platforms, far exceeding the worst-case bounds. We also identify a natural special case, for which we devise a solution with tight approximation guarantees. Moreover, we explain how our approach facilitates continual updates, maintaining consistency with an existing tree. Finally, we propose to include in the input candidate categories derived from result sets to recent search queries to reflect dynamic user interests and trends.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114853627",
                    "name": "Uri Avron"
                },
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1830456932",
                    "name": "Ido Guy"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "35fdb7311a59301e974f718ab86f4a92ece8dda2",
            "title": "On the Hardness of Category Tree Construction",
            "abstract": "Category trees, or taxonomies, are rooted trees where each node, called a category, corresponds to a set of related items. The construction of taxonomies has been studied in various domains, including e-commerce, document management, and question answering. Multiple algorithms for automating construction have been proposed, employing a variety of clustering approaches and crowdsourcing. However, no formal model to capture such categorization problems has been devised, and their complexity has not been studied. To address this, we propose in this work a combinatorial model that captures many practical settings and show that the aforementioned empirical approach has been warranted, as we prove strong inapproximability bounds for various problem variants and special cases when the goal is to produce a categorization of the maximum utility. In our model, the input is a set of n weighted item sets that the tree would ideally contain as categories. Each category, rather than perfectly match the corresponding input set, is allowed to exceed a given threshold for a given similarity function. The goal is to produce a tree that maximizes the total weight of the sets for which it contains a matching category. A key parameter is an upper bound on the number of categories an item may belong to, which produces the hardness of the problem, as initially each item may be contained in an arbitrary number of input sets. For this model, we prove inapproximability bounds, of order \u02dc\u0398( \u221a n ) or \u02dc\u0398( n ), for various problem variants and special cases, loosely justifying the aforementioned heuristic approach. Our work includes reductions based on parameterized randomized constructions that highlight how various problem parameters and properties of the input may affect the hardness. Moreover, for the special case where the category must be identical to the corresponding input set, we devise an algorithm whose approximation guarantee depends solely on a more granular parameter, allowing improved worst-case guarantees. Finally, we also generalize our results to DAG-based and non-hierarchical categorization",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "2114853627",
                    "name": "Uri Avron"
                },
                {
                    "authorId": "1830456932",
                    "name": "Ido Guy"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "3f0b3ff01db3cec1e3ad1881d3e3a21e5287fb6f",
            "title": "PHOcus: Efficiently Archiving Photos",
            "abstract": "Our ability to collect data is rapidly outstripping our ability to effectively store and use it. Organizations are therefore facing tough decisions of what data to archive (or dispose of) to effectively meet their business goals. PHOcus addresses this problem in the context of image data (photos) by proposing which photos to archive to meet an online storage budget. The decision is based on factors such as usage patterns and their relative importance, the quality and size of a photo, the relevance of a photo for a usage pattern, the similarity between different photos, as well as policy requirements of what photos must be retained. We formalize the photo archival problem and give an efficient algorithm with an optimal approximation guarantee. We then demonstrate our system, PHOcus, on an e-commerce application as well as with personal photos on a smartphone, and discuss how many of the inputs to the problem can be automatically obtained.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2184486608",
                    "name": "May Shoshan"
                }
            ]
        },
        {
            "paperId": "80b95b5fb0d5d536e69325ff7e10fb18baf28b62",
            "title": "Selecting Sub-tables for Data Exploration",
            "abstract": "Data scientists frequently examine the raw content of large tables when exploring an unknown dataset. In such cases, small subsets of the full tables (sub-tables) that accurately capture table contents are useful. We present a framework which, given a large data table T, creates a sub-table of small, fixed dimensions by selecting a subset of T\u2019s rows and projecting them over a subset of T\u2019s columns. The question is: Which rows and columns should be selected to yield an informative sub-table?Our first contribution is an informativeness metric for sub-tables with two complementary dimensions: cell coverage, which measures how well the sub-table captures prominent data patterns in T, and diversity. We use association rules as the patterns captured by sub-tables, and show that computing optimal sub-tables directly using this metric is infeasible. We then develop an efficient algorithm that indirectly accounts for association rules using table embedding. The resulting framework produces sub-tables for the full table as well as for the results of queries over the table, enabling the user to quickly understand results and determine subsequent queries. Experimental results show that high-quality sub-tables can be efficiently computed, and verify the soundness of our metrics as well as the usefulness of selected sub-tables through user studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "960c8d6d368b08e3c701d1027dd402e9a7dc797e",
            "title": "Disposal by Design",
            "abstract": "The \ufb02ood of data that has enabled breakthroughs in medicine, commerce, transportation, science and society also threatens to overwhelm our storage capacities and our privacy. Due to the volume of data and growth of regulations governing its maintenance and use, it is essential to develop automatic disposal techniques to manage this \ufb02ood. We present a vision for automating data disposal \u2013 disposal by design \u2013 which takes into account processing constraints, regulatory constraints as well as storage constraints, and give three concrete examples which address aspects of this vision. Two of the examples address current needs in e-commerce, while the third suggests how to use machine learning to \ufb01nd summaries of relational data. We then discuss the research challenges that remain to provide a holistic solution to disposal by design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "b3333559178dc9247ab2f103abf60518dd234d3e",
            "title": "OREO: Detection of Cherry-picked Generalizations",
            "abstract": "Data analytics often make sense of large data sets by generalization: aggregating from the detailed data to a more general context. Given a dataset, misleading generalizations can sometimes be drawn from a cherry-picked level of aggregation to obscure substantial subgroups that oppose the generalization. Our goal is to detect and explain cherry-picked generalizations by refining the corresponding aggregate queries. We demonstrate OREO, a system to compute a support score of the given statement to quantify the quality of the generalization; that is, whether the aggregated result is an accurate reflection of the data. To better understand the resulting score, our system also identifies significant counterexamples and alternative statements that better represent the data at hand. We will demonstrate the utility of OREO for investigating generalizations, by interacting with the VLDB'22 participants who will use the OREO interface for statement validation and explanation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117033724",
                    "name": "Yin Lin"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                },
                {
                    "authorId": "2330778",
                    "name": "Y. Moskovitch"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "e8ca453e854fbbee016bee8da08796f7f58e7fcf",
            "title": "SubTab: Data Exploration with Informative Sub-Tables",
            "abstract": "We demonstrate SubTab, a framework for creating small, informative sub-tables of large data tables to speed up data exploration. Given a table with n rows and m columns where n and m are large, SubTab creates a sub-table T_sub with k<n rows and l<m columns, i.e. a subset of k rows of the table projected over a subset of l columns. The rows and columns are chosen as representatives of prominent data patterns within and across columns in the input table. SubTab can also be used for query results, enabling the user to quickly understand the results and determine subsequent queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "289b9014e2a6ecea000ef9de1638f16192810ee0",
            "title": "Exploring Ratings in Subjective Databases",
            "abstract": "Subjective data links people to content items and reflects who likes or dislikes what. The valuable information this data contains is virtually infinite and satisfies various information needs. Yet, as of today, dedicated tools to explore this data are lacking. In this paper, we develop a framework for Subjective Data Exploration (SDE). Our solution enables the joint exploration of items, people, and people's opinions on items, in a guided multi-step process where each step aggregates the most useful and diverse trends in the form of rating maps. Because of the large search space of possible rating maps, we leverage pruning strategies based on confidence intervals and multi-armed bandits. Our large-scale experiments with human subjects and real datasets, demonstrate the need for dedicated SDE frameworks and the effectiveness and efficiency of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "3599f8afd848fe48dbbfbd8427e622e2939234dd",
            "title": "Fixing Wikipedia Interlinks Using Revision History Patterns",
            "abstract": "Wikipedia, the web-based free content encyclopedia project, is one of the most popular websites on the Web. Its \u201copen-door\" policy, allowing anyone to edit, has made Wikipedia the largest and possibly the best encyclopedia in the world. At the same time, the continuously evolving content, constantly updated by a large number of uncoordinated users, renders the maintenance of a clean, consistent encyclopedia an extremely challenging task. The goal of the W I C LEAN (WC) system presented in this paper is to assist Wikipedia editors in this difficult task. Specifically, we focus on the correctness of Wikipedia inter-links that point from one article (entity) to another. Such inter-links form a key component of the structured part of Wikipedia and their correctness is critical for coherent browsing. Given an entity type of interest, our highly parallelizable algorithm identifies relevant edit patterns across revision histories of Wikipedia entities of related types, along with time windows in which partial edits are tolerable. The discovered patterns/windows are then used by WC to alert Wikipedia editors on past edits that appear to be incomplete, as well as to provide users with on-line assistance as they update the encyclopedia. Our experiments with real-life Wikipedia data demonstrate the efficiency and effectiveness of WC in identifying actual errors in a variety of Wikipedia entity types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                }
            ]
        },
        {
            "paperId": "39e90ed2b1826c41fb5cda7baa3fb53b2489dd15",
            "title": "ConCaT: Construction of Category Trees from Search Queries in E-Commerce",
            "abstract": "Category trees play a central role in e-commerce platforms, enabling browsing-style information access. Building category trees that reflect users' dynamic information needs is a challenging task, mostly carried out by in-house taxonomists. This manual construction often leads to trees that are lacking or outdated since it is hard to keep track of market trends, seasonal changes, holidays, and special events.To support a browsing experience that better matches the user information needs, and to considerably reduce the manual work performed by taxonomists, we propose CONCAT - a system that leverages the demand-based nature of the query paradigm to automatically build a category tree that is maximally similar to the result sets for search queries. We demonstrate the effectiveness of CONCAT on real-world data, taken from a large e-commerce platform, by interacting with the ICDE'21 participants who act both as the consumers and the taxonomists.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114853627",
                    "name": "Uri Avron"
                },
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1830456932",
                    "name": "Ido Guy"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "96f1046053d51dd7da3d5c7ba6cd2590f9e8a68d",
            "title": "On Detecting Cherry-picked Generalizations",
            "abstract": "Generalizing from detailed data to statements in a broader context is often critical for users to make sense of large data sets. Correspondingly, poorly constructed generalizations might convey misleading information even if the statements are technically supported by the data. For example, a cherry-picked level of aggregation could obscure substantial sub-groups that oppose the generalization. We present a framework for detecting and explaining cherry-picked generalizations by refining aggregate queries. We present a scoring method to indicate the appropriateness of the generalizations. We design efficient algorithms for score computation. For providing a better understanding of the resulting score, we also formulate practical explanation tasks to disclose significant counterexamples and provide better alternatives to the statement. We conduct experiments using real-world data sets and examples to show the effectiveness of our proposed evaluation metric and the efficiency of our algorithmic framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117033724",
                    "name": "Yin Lin"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                },
                {
                    "authorId": "2330778",
                    "name": "Y. Moskovitch"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "a2075624ff5a38857aa568c8f8f4f02a4fbe894c",
            "title": "Multi-Objective Influence Maximization",
            "abstract": "Influence Maximization (IM) is the problem of finding a set of influential users in a social network, so that their aggregated influence is maximized. The classic IM problem focuses on the single objective of maximizing the overall number of influenced users . While this serves the goal of reaching a large audience, users often have multiple specific sub-populations they would like to reach within a single campaign, and consequently multiple influence maximization objectives. As we show, maximizing the influence over one group may come at the cost of significantly reducing the influence over the others. To address this, we propose IM-Balanced , a system that allows users to explicitly declare the desired balance between the objectives. IM-Balanced employs a refined notion of the classic IM problem, called Multi-Objective IM, where all objectives except one are turned into constraints, and the remaining objective is optimized subject to these constraints. We prove Multi-Objective IM to be harder to approximate than the original IM problem, and correspondingly provide two complementary approximation algorithms, each suit-ing a different prioritization pertaining to the inherent trade-off between the objectives. In our experiments we compare our solutions both to existing IM algorithms as well as to alternative approaches, demonstrating the advantages of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "b5343536b7d77c8e6f0c969ebb5951a1be9b2665",
            "title": "SubDEx: Exploring Ratings in Subjective Databases",
            "abstract": "We demonstrate SubDEx, a dedicated framework for Subjective Data Exploration (SDE). SubDEx enables the joint exploration of items, people, and people's opinions on items, in a guided multi-step process where each step aggregates the most useful and diverse trends in the form of rating maps. Because of the large search space of possible rating maps, we leverage pruning strategies to enable interactive running times. We demonstrate the need for a dedicated SDE framework and the effectiveness and efficiency of our approach, by interacting with the ICDE'21 participants who will act as data analysts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "c86bb7461e3252d3639b71a32f3eb4a06ddafc35",
            "title": "Improving Constrained Search Results By Data Melioration",
            "abstract": "The problem of finding an item-set of maximal aggregated utility that satisfies a set of constraints is at the cornerstone of many search applications. Its classical definition assumes that all the information needed to verify the constraints is explicitly given. However, in real-world databases, the data available on items is often partial. Hence, adequately answering constrained search queries requires the completion of this missing information. A common approach to complete missing data is to employ Machine Learning (ML)-based inference. However, such methods are naturally error-prone. More accurate data can be obtained by asking humans to complete missing information. But, as the number of items in the repository is vast, limiting human effort is crucial. To this end, we introduce the Probabilistic Constrained Search (PCS) problem, which identifies a bounded-size item-set whose data completion is likely to be highly beneficial, as these items are expected to belong to the result set of the constrained search queries in question. We prove PCS to be hard to approximate, and consequently propose a best-effort PTIME heuristic to solve it. We demonstrate the effectiveness and efficiency of our algorithm over real-world datasets and scenarios, showing that our algorithm significantly improves the result sets of constrained search queries, in terms of both utility and constraints satisfaction probability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1830456932",
                    "name": "Ido Guy"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "0bbc326b647ba29dbddd600be42f835663d4aa0c",
            "title": "Minimization of Classifier Construction Cost for Search Queries",
            "abstract": "Search over massive sets of items is the cornerstone of many modern applications. Users express a set of properties and expect the system to retrieve qualifying items. A common difficulty, however, is that the information on whether an item satisfies the search criteria is not explicitly recorded in the repository. Instead, it may be general knowledge or \"hidden\" in a picture/description, leading to incomplete search results. To overcome this problem, companies build dedicated classifiers that determine which items satisfy the given criteria. However, building classifiers requires volumes of high-quality labeled training data. Since the costs of training classifiers for different subsets of properties can vastly differ, the choice of which classifiers to train has great monetary significance. The goal of our research is to devise effective algorithms to choose which classifiers one should train to address a given query load while minimizing the cost. Previous work considered a simplified model with uniform classifier costs, and queries with two properties. We remove these restrictions in our model. We prove NP-hard inapproximability bounds and devise several algorithms with approximation guarantees. Moreover, we identify a common special case for which we provide an exact algorithm. Our experiments, performed over real-life datasets, demonstrate the effectiveness and efficiency of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1726029516",
                    "name": "Gefen Morami"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "0f10f8884e86efbab165733aeed5a5fc8fd394d1",
            "title": "Inventory Reduction via Maximal Coverage in E-Commerce",
            "abstract": "Many e-commerce platforms serve as an intermediary between companies and consumers, receiving a commission per purchase. To increase sales, these platforms tend to offer as many items as possible. However, in many situations a reduced subset of the items should be offered for sale, e.g., when opening an express delivery branch, starting operations in a new region, or disposing of redundant items to improve data quality and decrease maintenance costs. In all these cases it is imperative to select a reduced inventory which maximally covers consumer needs. A na\u00efve, yet popular, solution is to focus on the top selling items. This however ignores the hidden relations between items, and in particular the tendency of shoppers to buy, in the absence of an item they are looking for, a satisfying alternative. In this paper we introduce the Preference Cover problem , and investigate its application to practical inventory reduction. Given a large set of items, a bound on the number of items that can be retained and consumer preferences in terms of items popularity and suitability as alternatives, the goal is to select a reduced inventory which maximizes the likelihood of a purchase. We first model the problem via a dedicated weighted directed graph which captures the relevant information, then study two problem variants, which differ in their interpretation of the probabilistic dependencies be-tween consumer preferences. We prove both variants are NP-hard, and characterize their approximation hardness. Since in the practical application the overall number of items and the bound on the reduced item set are very large - in the order of magnitude of millions - we propose a highly parallelizable and scalable algo-rithm along with approximation guarantees. Finally, we present an end-to-end solution that fits the real-world e-commerce application, and provide an extensive set of experiments demonstrating the efficiency and effectiveness of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "1126777e338c5557642f167952257b1609c7cef9",
            "title": "Contribution Maximization in Probabilistic Datalog",
            "abstract": "The use of probabilistic datalog programs has been recently advocated for applications that involve recursive computation and uncertainty. While using such programs allows for a flexible knowledge derivation, it makes the analysis of query results a challenging task. Particularly, given a set O of output tuples and a number k, one would like to understand which k-size subset of the input tuples have contributed the most to the derivation of O. This is useful for multiple tasks, such as identifying the critical sources of errors and understanding surprising results. Previous works have mainly focused on the quantification of tuples contribution to a query result in non-recursive SQL queries, very often disregarding probabilistic inference. To quantify the contribution in probabilistic datalog programs, one must account for the recursive relations between input and output data, and the uncertainty. To this end, we formalize the Contribution Maximization (CM) problem. We then reduce CM to the well-studied Influence Maximization (IM) problem, showing that we can harness techniques developed for IM to our setting. However, we show that such na\u00efve adoption results in poor performance. To overcome this, we propose an optimized algorithm which injects a refined variant of the classic Magic Sets technique, integrated with a sampling method, into IM algorithms, achieving a significant saving of space and execution time. Our experiments demonstrate the effectiveness of our algorithm, even where the na\u00efve approach is infeasible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2330778",
                    "name": "Y. Moskovitch"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "133b3275e2642f558165698a1cef37047f884c6c",
            "title": "Toward Recommendation for Upskilling: Modeling Skill Improvement and Item Difficulty in Action Sequences",
            "abstract": "How can recommender systems help people improve their skills? As a first step toward recommendation for the upskilling of users, this paper addresses the problems of modeling the improvement of user skills and the difficulty of items in action sequences where users select items at different times. We propose a progression model that uses latent variables to learn the monotonically non-decreasing progression of user skills. Once this model is trained with the given sequence data, we leverage it to find a statistical solution to the item difficulty estimation problem, where we assume that users usually select items within their skill capacity. Experiments on five datasets (four from real domains, and one generated synthetically) revealed that (1) our model successfully captured the progression of domain-dependent skills; (2) multi-faceted item features helped to learn better models that aligned well with the ground-truth skill and difficulty levels in the synthetic dataset; (3) the learned models were practically useful to predict items and ratings in action sequences; and (4) exploiting the dependency structure of our skill model for parallel computation made the training process more efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49697753",
                    "name": "Kazutoshi Umemoto"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1716799",
                    "name": "M. Kitsuregawa"
                }
            ]
        },
        {
            "paperId": "279e5511cf067fa0760b77a8ff46d3720393c688",
            "title": "Towards Autonomous, Hands-Free Data Exploration",
            "abstract": "Exploratory Data Analysis (EDA) is an important yet dif-\ufb01cult task, currently performed by expert users, as it requires deep understanding of the data domain as well as profound analytical skills. In this work we make the case for the Hands-Free EDA (HFE) paradigm, in which the exploratory process is automatically conducted, requiring little or no human input - as in watching a \u201cvideo\u201d presenting selected highlights of the dataset. To that end, we suggest an end-to-end visionary system architecture, coupled with a prototype implementation. Our preliminary experimental results demonstrate that HFE is achievable, and leads the way for improvement and optimization research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67011698",
                    "name": "Ori Bar El"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "86c0107bb37599ab1f69c6a4695a03105e7b8bfb",
            "title": "Automatically Generating Data Exploration Sessions Using Deep Reinforcement Learning",
            "abstract": "Exploratory Data Analysis (EDA) is an essential yet highly demanding task. To get a head start before exploring a new dataset, data scientists often prefer to view existing EDA notebooks -- illustrative, curated exploratory sessions, on the same dataset, that were created by fellow data scientists who shared them online. Unfortunately, such notebooks are not always available (e.g., if the dataset is new or confidential). To address this, we present ATENA, a system that takes an input dataset and auto-generates a compelling exploratory session, presented in an EDA notebook. We shape EDA into a control problem, and devise a novel Deep Reinforcement Learning (DRL) architecture to effectively optimize the notebook generation. Though ATENA uses a limited set of EDA operations, our experiments show that it generates useful EDA notebooks, allowing users to gain actual insights.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67011698",
                    "name": "Ori Bar El"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "a4eb1fa1de9721471a551ae69db7a331186afdde",
            "title": "MC3: A System for Minimization of Classifier Construction Cost",
            "abstract": "Search mechanisms over massive sets of items are the cornerstone of many modern applications, particularly in e-commerce websites. Consumers express in search queries a set of properties, and expect the system to retrieve qualifying items. A common difficulty, however, is that the information on whether or not an item satisfies the search criteria is sometimes not explicitly recorded in the repository. Instead, it may be considered as general knowledge or \"hidden\" in a picture/description, thereby leading to incomplete search results. To overcome these problems companies invest in building dedicated classifiers that determine whether an item satisfies the given search criteria. However, building classifiers typically incurs non-trivial costs due to the required volumes of high-quality labeled training data. In this demo, we introduce MC3, a real-time system that helps data analysts decide which classifiers to construct to minimize the costs of answering a set of search queries. MC3 is interactive and facilitates real-time analysis, by providing detailed classifiers impact information. We demonstrate the effectiveness of MC3 on real-world data and scenarios taken from a large e-commerce system, by interacting with the SIGMOD'20 audience members who act as analysts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1726029516",
                    "name": "Gefen Morami"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "bb8e1e77cd08e7259e428b2fffb41b14b886fb8b",
            "title": "The Seattle Report on Database Research",
            "abstract": "Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254232",
                    "name": "D. Abadi"
                },
                {
                    "authorId": "1728318",
                    "name": "A. Ailamaki"
                },
                {
                    "authorId": "34752743",
                    "name": "D. Andersen"
                },
                {
                    "authorId": "2740804",
                    "name": "Peter D. Bailis"
                },
                {
                    "authorId": "1718134",
                    "name": "M. Balazinska"
                },
                {
                    "authorId": "1737944",
                    "name": "P. Bernstein"
                },
                {
                    "authorId": "1687211",
                    "name": "P. Boncz"
                },
                {
                    "authorId": "145647476",
                    "name": "S. Chaudhuri"
                },
                {
                    "authorId": "144385783",
                    "name": "Alvin Cheung"
                },
                {
                    "authorId": "3030274",
                    "name": "A. Doan"
                },
                {
                    "authorId": "46514930",
                    "name": "Luna Dong"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "1695576",
                    "name": "J. Hellerstein"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1691108",
                    "name": "Donald Kossmann"
                },
                {
                    "authorId": "1746961",
                    "name": "Tim Kraska"
                },
                {
                    "authorId": "34839826",
                    "name": "S. Krishnamurthy"
                },
                {
                    "authorId": "1733290",
                    "name": "V. Markl"
                },
                {
                    "authorId": "143973671",
                    "name": "S. Melnik"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "50224767",
                    "name": "C. Mohan"
                },
                {
                    "authorId": "143993045",
                    "name": "Thomas Neumann"
                },
                {
                    "authorId": "1508134967",
                    "name": "Beng Chin Ooi"
                },
                {
                    "authorId": "34075852",
                    "name": "F. Ozcan"
                },
                {
                    "authorId": "2070427422",
                    "name": "Jignesh M. Patel"
                },
                {
                    "authorId": "1774210",
                    "name": "Andrew Pavlo"
                },
                {
                    "authorId": "2036796951",
                    "name": "Raluca Popa"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "1803218",
                    "name": "Christopher R\u00e9"
                },
                {
                    "authorId": "145345023",
                    "name": "M. Stonebraker"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                }
            ]
        },
        {
            "paperId": "bbd3de0687a5c853e6088e844719f34d563800ec",
            "title": "CONCIERGE",
            "abstract": "The problem of finding an item-set of maximal aggregated utility that satisfies a set of constraints is at the cornerstone of many e-commerce applications. Its classical definition assumes that all the information needed to verify the constraints is explicitly given. In practice, however, the data available in e-commerce databases on the items is often partial. Hence, adequately answering constrained search queries requires the completion of this missing information. A common approach to complete missing data is to employ Machine Learning (ML) algorithms. However, ML is naturally error-prone. More accurate data can be obtained by asking the items' sellers to complete missing data. But as the number of items in the repository is huge, asking sellers about all items is prohibitively expensive. CONCIERGE, our presented system, assists the e-commerce platform in identifying a bounded-size set of items whose data should be manually completed, as these items are expected to contribute the most to the constrained search queries in question. We demonstrate the effectiveness of our system on real-world data and scenarios taken from a large e-commerce system by interacting with the VLDB'20 participants who act as both analysts and the sellers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1830456932",
                    "name": "Ido Guy"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "c403c8a98ce82cec20f396e4978534fed9b4a4d1",
            "title": "Personal insights for altering decisions of tree-based ensembles over time",
            "abstract": "Machine Learning models are prevalent in critical human-related decision making, such as resume filtering and loan applications. Refused individuals naturally ask what could change the decision, should they reapply. This question is hard for the model owner to answer: first, the model is typically complex and not easily interpretable; second, models may be updated periodically; and last, attributes of the individual seeking approval are apt to change in time. While each of these challenges have been extensively studied in isolation, their conjunction has not. To this end, we propose a novel framework that allows users to devise a plan of action to individuals in presence of Machine Learning classification, where both the ML model and the user properties are expected to change over time. Our technical solution is currently confined to a particular yet important class of models, namely those of tree-based ensembles (Random Forests, Gradient Boosted trees). In this setting it uniquely combines state-of-the-art solutions for single model interpretation, domain adaptation techniques for predicting future models, and constraint databases to represent and query the space of possible actions. We devise efficient algorithms that leverage these foundations in a novel solution, and experimentally show that they are effective in proposing useful and actionable steps leading to the desired classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32915630",
                    "name": "Nave Frost"
                },
                {
                    "authorId": "134504611",
                    "name": "Naama Boer"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "e5c051168cc5dd7c69b37d7b4be47c86e0f59803",
            "title": "ExplainED",
            "abstract": "Exploratory Data Analysis (EDA) is an essential yet highly demanding task. To get a head start before exploring a new dataset, data scientists often prefer to view existing EDA notebooks - illustrative exploratory sessions that were created by fellow data scientists who examined the same dataset and shared their notebooks via online platforms. Unfortunately, creating an illustrative, well-documented notebook is cumbersome and time-consuming, therefore users sometimes share their notebook without explaining their exploratory steps and their results. Such notebooks are difficult to follow and to understand. To address this, we present ExplainED, a system that automatically attaches explanations to views in EDA notebooks. ExplainED analyzes each view in order to detect what elements thereof are particularly interesting, and produces a corresponding textual explanation. The explanations are generated by first evaluating the interestingness of the given view using several measures capturing different interestingness facets, then computing the Shapely values of the elements in the view, w.r.t. the interestingness measure yielding the highest score. These Shapely values are then used to guide the generation of the textual explanation. We demonstrate the usefulness of the explanations generated by ExplainED on real-life, undocumented EDA notebooks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "32466098",
                    "name": "Amir Gilad"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "eb3b2da083353e7e938da06e8641f05b8e3be96f",
            "title": "Automating Exploratory Data Analysis via Machine Learning: An Overview",
            "abstract": "Exploratory Data Analysis (EDA) is an important initial step for any knowledge discovery process, in which data scientists interactively explore unfamiliar datasets by issuing a sequence of analysis operations (e.g. filter, aggregation, and visualization). Since EDA is long known as a difficult task, requiring profound analytical skills, experience, and domain knowledge, a plethora of systems have been devised over the last decade in order to facilitate EDA. In particular, advancements in machine learning research have created exciting opportunities, not only for better facilitating EDA, but to fully automate the process. In this tutorial, we review recent lines of work for automating EDA. Starting from recommender systems for suggesting a single exploratory action, going through kNN-based classifiers and active-learning methods for predicting users' interestingness preferences, and finally to fully automating EDA using state-of-the-art methods such as deep reinforcement learning and sequence-to-sequence models. We conclude the tutorial with a discussion on the main challenges and open questions to be dealt with in order to ultimately reduce the manual effort required for EDA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "186e0ce1c604a11f4f9dfb654512b27d72124ab4",
            "title": "Query Driven Data Labeling with Experts: Why Pay Twice?",
            "abstract": "Data has become a major priority for customer facing businesses of all sizes. Companies put a lot of effort and money into storing, cleaning, organizing, enriching and processing data to better meet user needs. Usually in large scale systems such as big e-commerce sites these tasks involve machine learning methods, relying on training data annotated by domain experts. Since domain experts are an expensive resource in terms of monetary costs and latency, it is desired to design algorithms that minimize the interaction with them. In this paper we address the problem of minimizing the number of annotation tasks with respect to a set of queries. We present a dedicated algorithm based on efficient labeling, that dictates the strategy for constructing a minimal set of classifiers sufficing to answer all queries. Our approach not only reduces monetary costs and latency, but also avoids data redundancy and saves storage space. We first consider a typical scenario of two expressions per query, and further discuss the challenges of extending our approach to multiple expressions. We examine two common models: batch and stream configurations, and devise offline and online algorithms, respectively. We analyze the number of annotations, and demonstrate the efficiency and effectiveness of our algorithm on a real-world dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15332639",
                    "name": "Eyal Dushkin"
                },
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "1c244f7a22fd7d7c216bc2ad9b53f400964e1c7d",
            "title": "ProvCite: Provenance-based Data Citation",
            "abstract": "\n As research products expand to include structured datasets, the challenge arises of how to automatically generate citations to the results of arbitrary queries against such datasets. Previous work explored this problem in the context of\n conjunctive\n queries and views using a Rewriting-Based Model (RBM). However, an increasing number of scientific queries are\n aggregate,\n e.g. statistical summaries of the underlying data, for which the RBM cannot be easily extended. In this paper, we show how a Provenance-Based Model (PBM) can be leveraged to 1) generate citations to conjunctive as well as aggregate queries and views; 2) associate citations with individual result tuples to enable arbitrary subsets of the result set to be cited (\n fine-grained citations\n ); and 3) be optimized to return citations in\n acceptable time.\n Our implementation of PBM in ProvCite shows that it not only handles a larger class of queries and views than RBM, but can outperform it when restricted to conjunctive views in some cases.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3037003",
                    "name": "Yinjun Wu"
                },
                {
                    "authorId": "2517099",
                    "name": "Abdussalam Alawini"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "33d8ccd0c5758723b368b2703c1117123b1a5cac",
            "title": "Boosting SimRank with Semantics",
            "abstract": "The problem of estimating the similarity of a pair of nodes in an information network draws extensive interest in numerous fields, e.g., social networks and recommender systems. In this work we revisit SimRank, a popular andwell studied similarity measure for information networks, that quantifies the similarity of two nodes based on the similarity of their neighbors. SimRank\u2019s popularity stems from its simple, declarative definition and its efficient, scalable computation. However, despite its wide adaptation, it has been observed that for many applications SimRank may yield inaccurate similarity estimations, due to the fact that it focuses on the network structure and ignores the semantics conveyed in the node/edge labels. Therefore, the question that we ask is can SimRank be enriched with semantics while preserving its",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "47a10e80c0c64775cb8ec3a35ad8520f7061ad2a",
            "title": "PODIUM: Probabilistic Datalog Analysis via Contribution Maximization",
            "abstract": "The use of probabilistic datalog programs has been advocated for applications that involve recursive computation and uncertainty. While using such programs allows for a flexible knowledge derivation, it makes the analysis of query results a challenging task. Particularly, given a set O of output tuples and a number k, one would like to understand which k-size subset of the input tuples has affected the most the derivation of O. This is useful for multiple tasks, such as identifying critical sources of errors and understanding surprising results. To this end, we formalize the Contribution Maximization problem and present an efficient algorithm to solve it. Our algorithm injects a refined variant of the classic Magic Sets technique, integrated with a sampling method, into top-performing algorithms for the well-studied Influence Maximization problem. We propose to demonstrate our solution in a system called PODIUM. We will demonstrate the usefulness of PODIUM using real-life data and programs, and illustrate the effectiveness of our algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2330778",
                    "name": "Y. Moskovitch"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "49f369b42702b04cda3645498306362e3d304beb",
            "title": "Predicting \"What is Interesting\" by Mining Interactive-Data-Analysis Session Logs",
            "abstract": "Assessing the interestingness of data analysis actions has been the subject of extensive previous work, and a multitude of inter-estingness measures have been devised, each capturing a different facet of the broad concept. While such measures are a core component in many analysis platforms (e.g., for ranking association rules, recommending visualizations, and query formulation), choosing the most adequate measure for a specific analysis task or an application domain is known to be a difficult task. In this work we focus on the choice of interestingness measures particularly for Interactive Data Analysis (IDA), where users examine datasets by performing sessions of analysis actions. Our goal is to determine the most suitable interestingness measure that adequately captures the user\u2019s current interest at each step of an interactive analysis session . We propose a novel solution that is based on the mining of IDA session logs. First, we perform an offline analysis of the logs, and identify unique characteristics of interestingness in IDA sessions. We then define a classification problem and build a predictive model that can select the best measure for a given a state of a user session. Our experimental evaluation, performed over real-life session logs, demonstrates the sensibility and adequacy of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "88050170",
                    "name": "Chai Ozeri"
                }
            ]
        },
        {
            "paperId": "56330ad4b36bb273336de3489a7bf42fe86ee9b0",
            "title": "Getting Rid of Data",
            "abstract": "We are experiencing an amazing data-centered revolution. Incredible amounts of data are collected, integrated, and analyzed, leading to key breakthroughs in science and society. This well of knowledge, however, is at a great risk if we do not dispense with some of the data flood. First, the amount of generated data grows exponentially and already at 2020 is expected to be more than twice the available storage. Second, even disregarding storage constraints, uncontrolled data retention risks privacy and security, as recognized, e.g., by the recent EU Data Protection reform. Data disposal policies must be developed to benefit and protect organizations and individuals. Retaining the knowledge hidden in the data while respecting storage, processing, and regulatory constraints is a great challenge. The difficulty stems from the distinct, intricate requirements entailed by each type of constraint, the scale and velocity of data, and the constantly evolving needs. While multiple data sketching, summarization, and deletion techniques were developed to address specific aspects of the problem, we are still very far from a comprehensive solution. Every organization has to battle the same tough challenges with ad hoc solutions that are application-specific and rarely sharable. In this article, we will discuss the logical, algorithmic, and methodological foundations required for the systematic disposal of large-scale data, for constraints enforcement and for the development of applications over the retained information. In particular, we will overview relevant related work, highlighting new research challenges and potential reuse of existing techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "5970fbe980618812ab94ad643f69424fa019f295",
            "title": "Just in Time: Personal Temporal Insights for Altering Model Decisions",
            "abstract": "The interpretability of complex Machine Learning models is coming to be a critical social concern, as they are increasingly used in human-related decision-making processes such as resume filtering or loan applications. Individuals receiving an undesired classification are likely to call for an explanation - preferably one that specifies what they should do in order to alter that decision when they reapply in the future. Existing work focuses on a single ML model and a single point in time, whereas in practice, both models and data evolve over time: an explanation for an application rejection in 2018 may be irrelevant in 2019 since in the meantime both the model and the applicant's data can change. To this end, we propose a novel framework that provides users with insights and plans for changing their classification in particular future time points. The solution is based on combining state-of-the-art algorithms for (single) model explanations, ones for predicting future models, and database-style querying of the obtained explanations. We propose to demonstrate the usefulness of our solution in the context of loan applications, and interactively engage the audience in computing and viewing suggestions tailored for applicants based on their unique characteristic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "134504611",
                    "name": "Naama Boer"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "32915630",
                    "name": "Nave Frost"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "6d3d9970eea4abd348f14bf15648547e265db7ec",
            "title": "Declarative User Selection with Soft Constraints",
            "abstract": "In applications with large userbases such as crowdsourcing, social networks or recommender systems, selecting users is a common and challenging task. Different applications require different policies for selecting users, and implementing such policies is applicationspecific and laborious. To this end, we introduce a novel declarative framework that abstracts common components of the user selection problem, while allowing for domain-specific tuning. The framework is based on an ontology view of user profiles, with respect to which we define a query language for policy specification. Our language extends SPARQL with means for capturing soft constraints which are essential for worker selection. At the core of our query engine is then a novel efficient algorithm for handling these constraints. Our experimental study on real-life data indicates the effectiveness and flexibility of our approach, showing in particular that it outperforms existing task-specific solutions in prominent user selection tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "7dc5a0dbcaa2c2974d30b3c893b59db6e43897c4",
            "title": "SimMeme: A Search Engine for Internet Memes",
            "abstract": "As more and more social network users interact through Internet Memes, an emerging popular type of captioned images, there is a growing need for users to quickly retrieve the right Meme for a given situation. As opposed conventional image search, visually similar Memes may reflect different concepts. Intent is sometimes captured by user annotations (e.g., tags), but these are often incomplete and ambiguous. Thus, a deeper analysis of the relations among Memes is required for an accurate, custom search. To address this problem, we present SimMeme, a Meme-dedicated search engine. SimMeme uses a generic graph-based data model that aligns various types of information about the Memes with a semantic ontology. A novel similarity measure that effectively considers all incorporated data is employed and serves as the foundation of our system. Our experimental results achieve using common evaluation metrics and crowd feedback, over a large repository of real-life annotated Memes, show that in the task of Meme retrieval, SimMeme outperforms state-of-the-art solutions for image retrieval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "987530728898409ff8838679133753d9b3b0a65e",
            "title": "WiClean: A System for Fixing Wikipedia Interlinks Using Revision History Patterns",
            "abstract": "We present W I C LEAN , a Wikipedia plug-in that supports the iden-ti\ufb01cation and cleaning of certain types of errors in Wikipedia inter-links. The system mines update patterns in Wikipedia revision logs, identi\ufb01es the common time frames in which they occur",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "148050714",
                    "name": "Stephan Goldberg"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                }
            ]
        },
        {
            "paperId": "9a662cb48549937bcecb390fd8f53e17092963f5",
            "title": "ReducE-Comm: Effective Inventory Reduction System for E-Commerce",
            "abstract": "Many e-commerce platforms serve as an intermediary between companies/manufacturers and consumers, receiving a commission per purchase. To increase revenue, such sites tend to offer a wide variety of items. However, in many situations a smaller subset of the items should be selected and offered for sale, e.g., when opening an express branch or expanding to a new region, or when maintenance costs become prohibitive and redundant items should be disposed of. In all these cases selecting a reduced inventory which covers most consumer needs is an important goal. In this demo we introduce ReducE-Comm - a highly parallelizable and scalable system that given a large set of items, a bound on the number of items that can be supported and information about consumer preferences/items relationships, allows to select a subset of the items which maximizes the likelihood of a purchase. Our system is interactive and facilitates real-time analysis, by providing detailed per-item impact statistics. We demonstrate the effectiveness of ReducE-Comm on real-world data and scenarios taken from a large e-commerce system, by interacting with the CIKM'19 audience who act as analysts aiming to intelligently reduce the inventory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "9d50962c22f8fe8c1704ec2c6b421ac0a89b2023",
            "title": "ATENA: An Autonomous System for Data Exploration Based on Deep Reinforcement Learning",
            "abstract": "Exploratory Data Analysis (EDA), is an important yet challenging task, that requires profound analytical skills and familiarity with the data domain. While Deep Reinforcement Learning (DRL) is nowadays used to solve AI challenges previously considered to be intractable, to our knowledge such solutions have not yet been applied to EDA. In this work we present ATENA, an autonomous system capable of exploring a given dataset by executing a meaningfulsequence of EDA operations. ATENA uses a novel DRL architecture, and learns to perform EDA operations by independently interacting with the dataset, without any training data or human assistance. We demonstrate ATENA in the context ofcyber security log analysis, where the audience is invited to partake in a data exploration challenge: explore real-life network logs, assisted by ATENA, in order to reveal underlying security attacks hidden in the data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67011698",
                    "name": "Ori Bar El"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "1a59db6cbf465f0333aa656927131408b4ab5538",
            "title": "Cleaning Data with Constraints and Experts",
            "abstract": "Popular techniques for data cleaning use integrity constraints to identify errors in the data and to automatically resolve them, e.g. by using predefined priorities among possible updates and finding a minimal repair that will resolve violations. Such automatic solutions however cannot ensure precision of the repairs since they do not have enough evidence about the actual errors and may in fact lead to wrong results with respect to the ground truth. It has thus been suggested to use domain experts to examine the potential updates and choose which should be applied to the database. However, the sheer volume of the databases and the large number of possible updates that may resolve a given constraint violation, may make such a manual examination prohibitory expensive. The goal of the DANCE system presented here is to help to optimize the experts work and reduce as much as possible the number of questions (updates verification) they need to address. Given a constraint violation, our algorithm identifies the suspicious tuples whose update may contribute (directly or indirectly) to the constraint resolution, as well as the possible dependencies among them. Using this information it builds a graph whose nodes are the suspicious tuples and whose weighted edges capture the likelihood of an error in one tuple to occur and affect the other. PageRank-style algorithm then allows us to identify the most beneficial tuples to ask about first. Incremental graph maintenance is used to assure interactive response time. We implemented our solution in the DANCE system and show its effectiveness and efficiency through a comprehensive suite of experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50811632",
                    "name": "A. Assadi"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "4adec2380dac312f998c564964d4986cf9345544",
            "title": "Machine Learning to Data Management: A Round Trip",
            "abstract": "With the emergence of machine learning (ML) techniques in database research, ML has already proved a tremendous potential to dramatically impact the foundations, algorithms, and models of several data management tasks, such as error detection, data cleaning, data integration, and query inference. Part of the data preparation, standardization, and cleaning processes, such as data matching and deduplication for instance, could be automated by making a ML model \"learn\" and predict the matches routinely. Data integration can also benefit from ML as the data to be integrated can be sampled and used to design the data integration algorithms. After the initial manual work to setup the labels, ML models can start learning from the new incoming data that are being submitted for standardization, integration, and cleaning. The more data supplied to the model, the better the ML algorithm can perform and deliver accurate results. Therefore, ML is more scalable compared to traditional and time-consuming approaches. Nevertheless, many ML algorithms require an out-of-the-box tuning and their parameters and scope are often not adapted to the problem at hand. To make an example, in cleaning and integration processes, the window sizes of values used for the ML models cannot be arbitrarily chosen and require an adaptation of the learning parameters. This tutorial will survey the recent trend of applying machine learning solutions to improve data management tasks and establish new paradigms to sharpen data error detection, cleaning, and integration at the data instance level, as well as at schema, system, and user levels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265210126",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2275254532",
                    "name": "Angela Bonifati"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "5dcda941cc9a909ca1fa9c77c71dbeb56c82b3ce",
            "title": "Preserving Privacy of Fraud Detection Rule Sharing Using Intel's SGX",
            "abstract": "The collaboration of financial institutes against fraudsters is a promising path for reducing resource investments and increasing coverage. Yet, such collaboration is held back by two somewhat conflicting challenges: effective knowledge sharing and limiting leakage of private information. While the censorship of private information is likely to reduce knowledge sharing effectiveness, the generalization of private information to a desired degree can potentially allow, on one hand, to limit the leakage, and on the other hand, to reveal some properties of the private information that can be beneficial for sharing. In this demo we present a system that allows knowledge sharing via effective adaptation of fraud detection rules while preserving privacy. The system uses taxonomies to generalize concrete values appearing in fraud detection rules to higher level concepts which conform to some privacy/utility requirements set by the owner. Our demonstration will engage the CIKM'18 audience by showing that private information can be abstracted to enforce privacy while maintaining its usage by (partially) trusted allies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "2762090",
                    "name": "Yehonatan Ginzberg"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "7321a996c0b31c5d468622566fd97e2cde2e5fd5",
            "title": "Deep Reinforcement-Learning Framework for Exploratory Data Analysis",
            "abstract": "Deep Reinforcement Learning (DRL) is unanimously considered as a breakthrough technology, used in solving a growing number of AI challenges previously considered to be intractable. In this work, we aim to set the ground for employing DRL techniques in the context of Exploratory Data Analysis (EDA), an important yet challenging, that is critical in many application domains. We suggest an end-to-end framework architecture, coupled with an initial implementation of each component. The goal of this short paper is to encourage the exploration of DRL models and techniques for facilitating a full-fledged, autonomous solution for EDA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "8089fff3a5818d3b1d09c4ee1dcbdac9ce4515eb",
            "title": "Top-k Sorting Under Partial Order Information",
            "abstract": "We address the problem of sorting the top-k elements of a set, given a predefined partial order over the set elements. Our means to obtain missing order information is via a comparison operator that interacts with a crowd of domain experts to determine the order between two unordered items. The practical motivation for studying this problem is the common scenario where elements cannot be easily compared by machines and thus human experts are harnessed for this task. As some initial partial order is given, our goal is to optimally exploit it in order to minimize the domain experts work. The problem lies at the intersection of two well-studied problems in the theory and crowdsourcing communities:full sorting under partial order information and top-k sorting with no prior partial order information. As we show, resorting to one of the existing state-of-the-art algorithms in these two problems turns out to be extravagant in terms of the number of comparisons performed by the users. In light of this, we present a dedicated algorithm for top-k sorting that aims to minimize the number of comparisons by thoroughly leveraging the partial order information. We examine two possible interpretations of the comparison operator, taken from the theory and crowdsourcing communities, and demonstrate the efficiency and effectiveness of our algorithm for both of them. We further demonstrate the utility of our algorithm, beyond identifying the top-k elements in a dataset, as a vehicle to improve the performance of Learning-to-Rank algorithms in machine learning context. We conduct a comprehensive experimental evaluation in both synthetic and real-world settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15332639",
                    "name": "Eyal Dushkin"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "981d1cabd0992b9bf7f234c3b3935ca69e24f531",
            "title": "Explaining Queries Over Web Tables to Non-experts",
            "abstract": "Designing a reliable natural language (NL) interface for querying tables has been a longtime goal of researchers in both the data management and natural language processing (NLP) communities. Such an interface receives as input an NL question, translates it into a formal query, executes the query and returns the results. Errors in the translation process are not uncommon, and users typically struggle to understand whether their query has been mapped correctly. We address this problem by explaining the obtained formal queries to non-expert users. We introduce novel query explanations that provide a graphic representation of the query cell-based provenance (in its execution on a given table). Our solution augments a state-of-the-art NL interface over web tables, enhancing it in both its training and deployment phase. Experiments, including a user study conducted on Amazon Mechanical Turk, show our solution to improve both the correctness and reliability of the NL interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1750652",
                    "name": "Jonathan Berant"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1786843",
                    "name": "A. Globerson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "51174907",
                    "name": "Tomer Wolfson"
                }
            ]
        },
        {
            "paperId": "9dfa79cf0734504522afe4e3d37b4d47c4dd3769",
            "title": "GOLDRUSH: Rule Sharing System for Fraud Detection",
            "abstract": "Fraud detection rules, written by domain experts, are often employed by financial companies to enhance their machine learning-based mechanisms for accurate detection of fraudulent transactions. Accurate rule writing is a challenging task where domain experts spend significant effort and time. A key observation is that much of this difficulty originates from the fact that experts typically work as \"lone rangers\" or in isolated groups to define the rules, or work on detecting frauds in one context in isolation from frauds that occur in another context. However, in practice there is a lot of commonality in what different experts are trying to achieve.\n In this demo, we present the GOLDRUSH system, which facilitates knowledge sharing via effective adaptation of fraud detection rules from one context to another. GOLDRUSH abstracts the possible semantic interpretations of each of the conditions in the rules in one context and adapts them to the target context. Efficient algorithms are used to identify the most effective rule adaptations w.r.t a given cost-benefit metric. We showcase GOLDRUSH through a reenactment of a real-life fraud detection event. Our demonstration will engage the VLDB'18 audience, allowing them to play the role of experts collaborating in the fight against financial frauds.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51189408",
                    "name": "Ariel Jarovsky"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                }
            ]
        },
        {
            "paperId": "a7307bf463f2911eea021d952a8d6b9d8eff1dcb",
            "title": "Interactive Rule Refinement for Fraud Detection",
            "abstract": "Credit card frauds are unauthorized transactions that are made or attempted by a person or an organization that is not authorized by the card holders. Fraud with general-purpose cards (credit, debit cards etc.) is a billion dollar industry and companies are therefore investing significant efforts in identifying and preventing them. It is typical to deploy mining and machine learning-based techniques to derive rules. However, such rules may not always capture the semantic reasons underlying the frauds that occur. For this reason, credit card companies often employ domain experts to manually specify rules that exploit general or domain knowledge for improving the detection process. Over time, however, as new (fraudulent and legitimate) transactions arrive, these rules need to be updated and refined to capture the evolving (fraud and legitimate) activity patterns. The goal of the RUDOLF system described in this paper is to guide and assist domain experts in this challenging task. RUDOLF automatically determines the \u201cbest\u201d adaptation to existing rules to capture all fraudulent transactions and, respectively, omit all legitimate transactions. The proposed modifications can then be further refined by users and the process can be repeated until they are satisfied with the resulting rules. We show that the problem of identifying the best candidate adaptation is NP-hard in general and present PTIME heuristic algorithms for determining the set of rules to adapt. We have implemented our algorithms in RUDOLF and show, through experiments on real-life datasets, the effectiveness and efficiency of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                }
            ]
        },
        {
            "paperId": "d12a5204bffb18517ff03c6ffb539ba224e6dc8b",
            "title": "Next-Step Suggestions for Modern Interactive Data Analysis Platforms",
            "abstract": "Modern Interactive Data Analysis (IDA) platforms, such as Kibana, Splunk, and Tableau, are gradually replacing traditional OLAP/SQL tools, as they allow for easy-to-use data exploration, visualization, and mining, even for users lacking SQL and programming skills. Nevertheless, data analysis is still a di cult task, especially for non-expert users. To that end we present REACT, a recommender system designed for modern IDA platforms. In these platforms, analysis sessions interweave high-level actions of multiple types and operate over diverse datasets . REACT identifies and generalizes relevant (previous) sessions to generate personalized next-action suggestions to the user. We model the user's analysis context using a generic tree based model, where the edges represent the user's recent actions, and the nodes represent their result \"screens\". A dedicated context-similarity metric is employed for efficient indexing and retrieval of relevant candidate next-actions. These are then generalized to abstract actions that convey common fragments, then adapted to the specific user context. To prove the utility of REACT we performed an extensive online and offline experimental evaluation over real-world analysis logs from the cyber security domain, which we also publish to serve as a benchmark dataset for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "e0b84766c40c66b687e3dfd92fae1a10479c5e37",
            "title": "Rule Sharing for Fraud Detection via Adaptation",
            "abstract": "Writing rules to capture precisely fraudulent transactions is a challenging task where domain experts spend significant effort and time. A key observation is that much of this difficulty originates from the fact that such experts typically work as \"lone rangers\" or in isolated groups, or work on detecting frauds in one context in isolation from frauds that occur in another context. However, in practice there is a lot of commonality in what different experts are trying to achieve. In this paper, we present the GOLDRUSH system, which facilitates knowledge sharing via effective adaptation of fraud detection rules from one context to another. GOLDRUSH abstracts the possible semantic interpretations of each of the conditions in the rules at the source context and adapts them to the target context. Efficient algorithms are used to identify the most effective rule adaptations w.r.t a given cost-benefit metric. Our extensive set of experiments, based on real-world financial datasets, demonstrate the efficiency and effectiveness of our solution, both in terms of the accuracy of the fraud detection and the actual money saved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51189408",
                    "name": "Ariel Jarovsky"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                }
            ]
        },
        {
            "paperId": "ff45f5d488ffa97643a0de24d5177d648df4df57",
            "title": "IM Balanced: Influence Maximization Under Balance Constraints",
            "abstract": "Influence Maximization (IM) is the problem of finding a set of influential users in a social network, so that their aggregated influence is maximized. IM has natural applications in viral marketing and has been the focus of extensive recent research. One critical problem, however, is that while existing IM algorithms serve the goal of reaching a large audience, they may obliviously focus on certain well-connected populations, at the expense of key demographics, creating an undesirable imbalance, an illustration of a broad phenomenon referred to as algorithmic discrimination. Indeed, we demonstrate an inherent trade-off between two objectives: (1) maximizing the overall influence and (2) maximizing influence over a predefined \"protected\" demographic, with the optimal balance between the two being open to different interpretations. To this end, we present IM-Balanced, a system enabling end users to declaratively specify the desired trade-off between these objectives w.r.t. an emphasized population. IM-Balanced provides theoretical guarantees for the proximity to the optimal solution in terms of both objectives and ensures an efficient, scalable computation via careful adaptation of existing state-of-the-art IM algorithms. Our demonstration illustrates the effectiveness of our approach through real-life viral marketing scenarios in an academic social network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                },
                {
                    "authorId": "50479435",
                    "name": "Gal Zeevi"
                }
            ]
        },
        {
            "paperId": "27396008da229a2d03cb5c45264287dc551bcb9f",
            "title": "7 Secrets That My Mother Didn't Tell Me",
            "abstract": "What does it take to be a good researcher? And, is it different when you are a women? These are questions that many of us are wondering about throughout our career. Being honored with a VLDB Women in Database Research Award, I would like to share with you in this talk some of the secrets to successful research that I have learned over the years. These secrets highlight some of the fundamental research directions that I have taken. No less importantly, they explain how I successfully got to work on them, both personally and professionally.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "3877a906c28fb985af46d19f47743c0d3d91c75b",
            "title": "SimMeme: Semantic-Based Meme Search",
            "abstract": "With the proliferation of social image-sharing applications, image search becomes an increasingly common activity. In this work, we focus on a particular class of images that convey semantic meaning beyond the visual appearance, and whose search presents particular challenges. A prominent example is Memes, an emerging popular type of captioned pictures, which we will use in this demo to demonstrate our solution. Unlike in conventional image-search, visually similar Memes may reflect different concepts. The intent is sometimes captured by user annotations, but these too are often incomplete and ambiguous. Thus, a deeper analysis of the semantic relations among Memes is required for an accurate search. To address this problem, we present SimMeme, a semantic aware search engine for Memes. SimMeme uses a generic graph-based data model that aligns all the information available about the Memes with a semantic ontology. A novel similarity measure that interweaves common image, textual, structural and semantic similarities into one holistic measure is employed to effectively answer user queries. We will demonstrate the operation of SimMeme over a large repository of real-life annotated Memes which we have constructed by web crawling and crowd annotations, allowing users to appreciate the quality of the search results as well as the execution efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9835815",
                    "name": "M. Ekron"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "4308967390fd768122a6904fa8987a8eb9d79183",
            "title": "A Model for Fine-Grained Data Citation",
            "abstract": "An increasing amount of information is being collected in structured, evolving, curated databases, driving the question of how information extracted from such datasets via queries should be cited. Unlike traditional research products, such books and journals, which have a \ufb01xed granularity, data citation is a challenge because the granularity varies. Di\ufb00erent portions of the database, with varying granularity, may have di\ufb00erent citations. Furthermore, there are an in\ufb01nite number of queries over a database, each accessing and generating di\ufb00erent subsets of the database, so we cannot hope to explicitly attach a citation to every possible result set and/or query. We present the novel problem of automatically generating citations for general queries over a relational database, and explore a solution based on a set of citation views , each of which attaches a citation to a view of the database. Citation views are then used to automatically construct citations for general queries. Our approach draws inspiration from re-sults in two areas, query rewriting using views and database provenance and combines them in a robust model. We then discuss open issues in developing a practical solution to this challenging problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1688094",
                    "name": "G. Silvello"
                }
            ]
        },
        {
            "paperId": "477fd30bb227ea356e42a64cb4b2931cd9fb5e55",
            "title": "Top-k Querying of Unknown Values under Order Constraints (Extended Version)",
            "abstract": "Many practical scenarios make it necessary to evaluate top-k queries over data items with partially unknown values. This paper considers a setting where the values are taken from a numerical domain, and where some partial order constraints are given over known and unknown values: under these constraints, we assume that all possible worlds are equally likely. Our work is the first to propose a principled scheme to derive the value distributions and expected values of unknown items in this setting, with the goal of computing estimated top-k results by interpolating the unknown values from the known ones. We study the complexity of this general task, and show tight complexity bounds, proving that the problem is intractable, but can be tractably approximated. We then consider the case of tree-shaped partial orders, where we show a constructive PTIME solution. We also compare our problem setting to other top-k definitions on uncertain data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2441482",
                    "name": "Antoine Amarilli"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "6740c1fe20b4cc9e9c13da376a864ec0ab3c4b7f",
            "title": "Research Directions for Principles of Data Management (Dagstuhl Perspectives Workshop 16151)",
            "abstract": "The area of Principles of Data Management (PDM) has made crucial contributions to the development of formal frameworks for understanding and managing \ndata and knowledge. This work has involved a rich cross-fertilization between \nPDM and other disciplines in mathematics and computer science, including logic, complexity theory, and knowledge representation. We anticipate on-going expansion of PDM research as the technology and applications involving data management continue to grow and evolve. In particular, the lifecycle of Big Data Analytics raises a wealth of challenge areas that PDM can help with. \n \nIn this report we identify some of the most important research directions where the PDM community has the potential to make significant contributions. This is done from three perspectives: potential practical relevance, results already obtained, and research questions that appear surmountable in the short and medium term.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "144658846",
                    "name": "M. Arenas"
                },
                {
                    "authorId": "35106192",
                    "name": "P. Barcel\u00f3"
                },
                {
                    "authorId": "1764889",
                    "name": "Meghyn Bienvenu"
                },
                {
                    "authorId": "1733846",
                    "name": "Diego Calvanese"
                },
                {
                    "authorId": "2053464159",
                    "name": "C. David"
                },
                {
                    "authorId": "144837668",
                    "name": "R. Hull"
                },
                {
                    "authorId": "1691955",
                    "name": "Eyke H\u00fcllermeier"
                },
                {
                    "authorId": "1679226",
                    "name": "B. Kimelfeld"
                },
                {
                    "authorId": "1681226",
                    "name": "L. Libkin"
                },
                {
                    "authorId": "144352362",
                    "name": "W. Martens"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "3166093",
                    "name": "Filip Murlak"
                },
                {
                    "authorId": "1786248",
                    "name": "F. Neven"
                },
                {
                    "authorId": "145174132",
                    "name": "Magdalena Ortiz"
                },
                {
                    "authorId": "1736032",
                    "name": "T. Schwentick"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "11254357",
                    "name": "Jianwen Su"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "1765187",
                    "name": "V. Vianu"
                },
                {
                    "authorId": "144744111",
                    "name": "K. Yi"
                }
            ]
        },
        {
            "paperId": "6dcfb0b2c7cae7673437cba87255df5d12ce866e",
            "title": "The Smart Crowd - Learning from the Ones Who Know (Invited Talk)",
            "abstract": "One of the foremost challenges for information technology over the last few years has been to explore, understand, and extract useful information from large amounts of data. Some particular tasks such as annotating data or matching entities have been outsourced to human workers for many years. But the last few years have seen the rise of a new research field called crowdsourcing that aims at delegating a wide range of tasks to human workers, building formal frameworks, and improving the efficiency of these processes. \nIn order to provide sound scientific foundations for crowdsourcing and support the development of efficient crowd sourcing processes, adequate formal models and algorithms must be defined. In particular, the models must formalize unique characteristics of crowd-based settings, such as the knowledge of the crowd and crowd-provided data; the interaction with crowd members; the inherent inaccuracies and disagreements in crowd answers; and evaluation metrics that capture the cost and effort of the crowd. \nClearly, what may be achieved with the help of the crowd depends heavily on the properties and knowledge of the given crowd. In this talk we will focus on knowledgeable crowds. We will examine the use of such crowds, and in particular domain experts, for assisting solving data management problems. Specifically we will consider three dimensions of the problem: (1) How domain experts can help in improving the data itself, e.g. by gathering missing data and improving the quality of existing data, (2) How they can assist in gathering meta-data that facilitate improved data processing, and (3) How can we find and identify the most relevant crowd for a given data management task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "85e3e134eda54c2520adfdcd8abe3d0485467ff9",
            "title": "DANCE: Data Cleaning with Constraints and Experts",
            "abstract": "We demonstrate DANCE, a system that assists domain experts in the efficient resolution of integrity constraints violation. DANCE is demonstrated on the UEFA Champions League database, employing the ICDE'17 audience for effective data cleaning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50811632",
                    "name": "A. Assadi"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "898e5b4ad9cbb86e5165bccc4f52deb520efc73f",
            "title": "Data Citation: A Computational Challenge",
            "abstract": "Data citation is an interesting computational challenge, whose solution draws on several well-studied problems in database theory: query answering using views, and provenance. We describe the problem, suggest an approach to its solution, and highlight several open research problems, both practical and theoretical.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1688094",
                    "name": "G. Silvello"
                }
            ]
        },
        {
            "paperId": "df965cb7ab6dee75186471676586033c41592913",
            "title": "Research Directions for Principles of Data Management (Abridged)",
            "abstract": "In April 2016, a community of researchers working in the area of Principles of Data Management (PDM) joined in a workshop at the Dagstuhl Castle in Germany. The workshop was organized jointly by the Executive Committee of the ACM Symposium on Principles of Database Systems (PODS) and the Council of the International Conference on Database Theory (ICDT). The mission of the workshop was to identify and explore some of the most important research directions that have high relevance to society and to Computer Science today, and where the PDM community has the potential to make significant contributions. This article presents a summary of the report created by the workshop [4]. That report describes the family of research directions that the workshop focused on from three perspectives: potential practical relevance, results already obtained, and research questions that appear surmountable in the short and medium term. The report organizes the identified research challenges for PDM around seven core themes, namely Managing Data at Scale, Multi-model Data, Uncertain Information, Knowledge-enriched Data, Data Management and Machine Learning, Process and Data, and Ethics and Data Management. Since new challenges in PDM arise all the time, we note that this list of themes is not intended to be exclusive.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "144658846",
                    "name": "M. Arenas"
                },
                {
                    "authorId": "35106192",
                    "name": "P. Barcel\u00f3"
                },
                {
                    "authorId": "1764889",
                    "name": "Meghyn Bienvenu"
                },
                {
                    "authorId": "1733846",
                    "name": "Diego Calvanese"
                },
                {
                    "authorId": "2053464159",
                    "name": "C. David"
                },
                {
                    "authorId": "144837668",
                    "name": "R. Hull"
                },
                {
                    "authorId": "1691955",
                    "name": "Eyke H\u00fcllermeier"
                },
                {
                    "authorId": "1679226",
                    "name": "B. Kimelfeld"
                },
                {
                    "authorId": "1681226",
                    "name": "L. Libkin"
                },
                {
                    "authorId": "144352362",
                    "name": "W. Martens"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "3166093",
                    "name": "Filip Murlak"
                },
                {
                    "authorId": "1786248",
                    "name": "F. Neven"
                },
                {
                    "authorId": "145174132",
                    "name": "Magdalena Ortiz"
                },
                {
                    "authorId": "1736032",
                    "name": "T. Schwentick"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "11254357",
                    "name": "Jianwen Su"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "1765187",
                    "name": "V. Vianu"
                },
                {
                    "authorId": "144744111",
                    "name": "K. Yi"
                }
            ]
        },
        {
            "paperId": "477a3c1c69ff045ad7abdfb7977e1eb8b936782f",
            "title": "Filtering With the Crowd: CrowdScreen Revisited",
            "abstract": "Filtering a set of items, based on a set of properties that can be verified by humans, is a common application of CrowdSourcing. When the workers are error-prone, each item is presented to multiple users, to limit the probability of misclassification. Since the Crowd is a relatively expensive resource, minimizing the number of questions per item may naturally result in big savings. Several algorithms to address this minimization problem have been presented in the CrowdScreen framework by Parameswaran et al. However, those algorithms do not scale well and therefore cannot be used in scenarios where high accuracy is required in spite of high user error rates. The goal of this paper is thus to devise algorithms that can cope with such situations. To achieve this, we provide new theoretical insights to the problem, then use them to develop a new efficient algorithm. We also propose novel optimizations for the algorithms of CrowdScreen that improve their scalability. We complement our theoretical study by an experimental evaluation of the algorithms on a large set of synthetic parameters as well as real-life crowdsourcing scenarios, demonstrating the advantages of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2516234",
                    "name": "Beno\u00eet Groz"
                },
                {
                    "authorId": "1734847429",
                    "name": "Ezra Levin"
                },
                {
                    "authorId": "1904578",
                    "name": "I. Meilijson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "53748fe355a6bc85fe5765142075d903f5c09111",
            "title": "PROX: Approximated Summarization of Data Provenance",
            "abstract": "Many modern applications involve collecting large amounts of data from multiple sources, and then aggregating and manipulating it in intricate ways. The complexity of such applications, combined with the size of the collected data, makes it difficult to understand the application logic and how information was derived. Data provenance has been proven helpful in this respect in different contexts; however, maintaining and presenting the full and exact provenance may be infeasible, due to its size and complex structure. For that reason, we introduce the notion of approximated summarized provenance, where we seek a compact representation of the provenance at the possible cost of information loss. Based on this notion, we have developed PROX, a system for the management, presentation and use of data provenance for complex applications. We propose to demonstrate PROX in the context of a movies rating crowd-sourcing system, letting participants view provenance summarization and use it to gain insights on the application and its underlying data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2627111",
                    "name": "Eleanor Ainy"
                },
                {
                    "authorId": "1807924",
                    "name": "P. Bourhis"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "807a54deb2ef6b980570360e50d78568fe0aa730",
            "title": "REACT: Context-Sensitive Recommendations for Data Analysis",
            "abstract": "Data analysis may be a difficult task, especially for non-expert users, as it requires deep understanding of the investigated domain and the particular context. In this demo we present REACT, a system that hooks to the analysis UI and provides the users with personalized recommendations of analysis actions. By matching the current user session to previous sessions of analysts working with the same or other data sets, REACT is able to identify the potentially best next analysis actions in the given user context. Unlike previous work that mainly focused on individual components of the analysis work, REACT provides a holistic approach that captures a wider range of analysis action types by utilizing novel notions of similarity in terms of the individual actions, the analyzed data and the entire analysis workflow. We demonstrate the functionality of REACT, as well as its effectiveness through a digital forensics scenario where users are challenged to detect cyber attacks in real life data achieved from honeypot servers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "91def56c0137eeab9e1c9b35cbca47ff1a066edd",
            "title": "Letter from the Special Issue Editor",
            "abstract": "Over the past years there has been a growing recognition of the increasing role of people in the data life cycle. People massively contribute data and share opinions, people also extensively analyze data and consume the derived information. In this issue, we have a slate of very interesting articles discussing the different roles of human in data management. In particular, we discuss three complementary aspects. First, we consider the role of people as crowdsourcing workers, assisting in data generation and in data-centric computation and analysis. The focus here is on devising methods for cost-effective, meaningful usage of human capabilities and knowledge. Second, we consider the role of people as data analysts. The focus here is on interactive data analysis and mining and the development of tools that facilitate such effective interaction. Finally, we consider the role of people as data consumers and acknowledge the importance of considering ethics in many aspects of data creation, access, and usage. The focus here is on finding new ways for maximizing the benefits of massive data while nevertheless safeguarding the privacy and integrity of citizens and societies. We start with three papers that investigate different aspects of crowdsourcing. In \" Toward Worker-Centric Crowdsourcing \" , Amer-Yahia and Roy argue that accounting for human factors, in particular workers' characteristics , in task assignment benefits both workers and requesters, and discuss new opportunities raised by worker-centric crowdsourcing. In the second paper, \" Spatial Crowdsourcing: Challenges and Opportunities \" , Chen and Shahabi consider spatial aspects of crowdsourcing, in particular those related to the usage of mobile devices and users, and discuss the challenges and opportunities related to this important setting. In \" Optimizing Open-Ended Crowdsourcing: The Next Frontier in Crowdsourced Data Management \" , Parameswaran, Sarma and Venkataraman examine open-ended crowdsourcing and survey existing work on formally reasoning about and optimizing this important, but relatively understudied class of crowdsourcing. The next three papers focus on interactive data analysis. In \" Interactive Data Exploration via Machine Learning Models \" , Papaemmanouil, Diao, Dimitriadou and Peng examine how learning-based exploration techniques can automatically steer the user towards interesting data areas, based on relevance feedback on database samples. In \" Runtime Support for Human-in-the-Loop Feature Engineering Systems \" , Anderson, Antenucci and Cafarella take a closer look at feature selection for machine learning and discuss two projects that accelerate feature engineering by applying domain insights to engineer high-impact features. \u2026",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "9315cf87a57a346c416a98984b8bf82d3b85e679",
            "title": "The Beckman report on database research",
            "abstract": "Database researchers paint big data as a defining challenge. To make the most of the enormous opportunities at hand will require focusing on five research areas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254232",
                    "name": "D. Abadi"
                },
                {
                    "authorId": "144947410",
                    "name": "R. Agrawal"
                },
                {
                    "authorId": "1728318",
                    "name": "A. Ailamaki"
                },
                {
                    "authorId": "1718134",
                    "name": "M. Balazinska"
                },
                {
                    "authorId": "1737944",
                    "name": "P. Bernstein"
                },
                {
                    "authorId": "1703347",
                    "name": "M. Carey"
                },
                {
                    "authorId": "145647476",
                    "name": "S. Chaudhuri"
                },
                {
                    "authorId": "2056947015",
                    "name": "Jeffrey Dean"
                },
                {
                    "authorId": "3030274",
                    "name": "A. Doan"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "143614516",
                    "name": "J. Gehrke"
                },
                {
                    "authorId": "145704391",
                    "name": "L. Haas"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "1695576",
                    "name": "J. Hellerstein"
                },
                {
                    "authorId": "1684197",
                    "name": "Y. Ioannidis"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1691108",
                    "name": "Donald Kossmann"
                },
                {
                    "authorId": "144478906",
                    "name": "S. Madden"
                },
                {
                    "authorId": "144156242",
                    "name": "S. Mehrotra"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "5151034",
                    "name": "J. Naughton"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "1733290",
                    "name": "V. Markl"
                },
                {
                    "authorId": "2072569",
                    "name": "Christopher Olston"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "1803218",
                    "name": "Christopher R\u00e9"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "145345023",
                    "name": "M. Stonebraker"
                },
                {
                    "authorId": "2058106369",
                    "name": "Todd Walter"
                },
                {
                    "authorId": "1737896",
                    "name": "J. Widom"
                }
            ]
        },
        {
            "paperId": "a3172056e81001f9fcccceb1c9a7af6904227747",
            "title": "Rudolf: Interactive Rule Refinement System for Fraud Detection",
            "abstract": "Credit card frauds are unauthorized transactions that are made or attempted by a person or an organization that is not authorized by the card holders. In addition to machine learning-based techniques, credit card companies often employ domain experts to manually specify rules that exploit domain knowledge for improving the detection process. Over time, however, as new (fraudulent and legitimate) transaction arrive, these rules need to be updated and refined to capture the evolving (fraud and legitimate) activity patterns. The goal of the RUDOLF system that is demonstrated here is to guide and assist domain experts in this challenging task. \n \nRUDOLF automatically determines a best set of candidate adaptations to existing rules to capture all fraudulent transactions and, respectively, omit all legitimate transactions. The proposed modifications can then be further refined by domain experts based on their domain knowledge, and the process can be repeated until the experts are satisfied with the resulting rules. Our experimental results on real-life datasets demonstrate the effectiveness and efficiency of our approach. We showcase RUDOLF with two demonstration scenarios: detecting credit card frauds and network attacks. Our demonstration will engage the VLDB audience by allowing them to play the role of a security expert, a credit card fraudster, or a network attacker.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                }
            ]
        },
        {
            "paperId": "c496f56a7c2c34b4738190c3a6deb380e55b98f2",
            "title": "Foundations of Data Management (Dagstuhl Perspectives Workshop 16151)",
            "abstract": "In this Workshop we have explored the degree to which principled foundations are crucial to the long-term success and effectiveness of the new generation of data management paradigms and applications, and investigated what forms of research need to be pursued to develop and advance these foundations. \n \nThe workshop brought together specialists from the existing database theory community, and from adjoining areas, particularly from various subdisciplines within the Big Data community, to understand the challenge areas that might be resolved through principled foundations and mathematical theory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144658846",
                    "name": "M. Arenas"
                },
                {
                    "authorId": "144837668",
                    "name": "R. Hull"
                },
                {
                    "authorId": "144352362",
                    "name": "W. Martens"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1736032",
                    "name": "T. Schwentick"
                }
            ]
        },
        {
            "paperId": "c5837d5bfd3ab0794a51cdee234ec62e819b0108",
            "title": "December: A Declarative Tool for Crowd Member Selection",
            "abstract": "Adequate crowd selection is an important factor in the success of crowdsourcing platforms, increasing the quality and relevance of crowd answers and their performance in different tasks. The optimal crowd selection can greatly vary depending on properties of the crowd and of the task. To this end, we present December, a declarative platform with novel capabilities for flexible crowd selection. December supports the personalized selection of crowd members via a dedicated query language Member-QL. This language enables specifying and combining common crowd selection criteria such as properties of a crowd member's profile and history, similarity between profiles in specific aspects and relevance of the member to a given task. This holistic, customizable approach differs from previous work that has mostly focused on dedicated algorithms for crowd selection in specific settings. To allow efficient query execution, we implement novel algorithms in December based on our generic, semantically-aware definitions of crowd member similarity and expertise. \n \nWe demonstrate the effectiveness of December and Member-QL by using the VLDB community as crowd members and allowing conference participants to choose from among these members for different purposes and in different contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                },
                {
                    "authorId": "1403679857",
                    "name": "Brit Youngmann"
                }
            ]
        },
        {
            "paperId": "ca68bb43ff3a3f1dec89e3f5aecb76ef01f16ff1",
            "title": "An Efficient MapReduce Cube Algorithm for Varied DataDistributions",
            "abstract": "Data cubes allow users to discover insights from their data and are commonly used in data analysis. While very useful, the data cube is expensive to compute, in particular when the input relation is very large. To address this problem, we consider cube computation in MapReduce, the popular paradigm for distributed big data processing, and present an efficient algorithm for computing cubes over large data sets. We show that our new algorithm consistently performs better than the previous solutions. In particular, existing techniques for cube computation in MapReduce suffer from sensitivity to the distribution of the input data and their performance heavily depends on whether or not, and how exactly, the data is skewed. In contrast, the cube algorithm that we present here is resilient and significantly outperforms previous solutions for varying data distributions. At the core of our solution is a dedicated data structure called the Skews and Partitions Sketch (SP-Sketch for short). The SP-Sketch is compact in size and fast to compute, and records all needed information for identifying skews and effectively partitioning the workload between the machines. Our algorithm uses the sketch to speed up computation and minimize communication overhead. Our theoretical analysis and thorough experimental study demonstrate the feasibility and efficiency of our solution, including comparisons to state of the art tools for big data processing such as Pig and Hive.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1399478998",
                    "name": "Eyal Altshuler"
                }
            ]
        },
        {
            "paperId": "03f50c025e16075d9d0a9e9b280b4a456bc7b62d",
            "title": "Managing General and Individual Knowledge in Crowd Mining Applications",
            "abstract": "Crowd mining frameworks combine general knowledge, which can refer to an ontology or information in a database, with individual knowledge obtained from the crowd, which captures habits and preferences. To account for such mixed knowledge, along with user interaction and optimization issues, such frameworks must employ a complex process of reasoning, automatic crowd task generation and result analysis. In this paper, we describe a generic architecture for crowd mining applications. This architecture allows us to examine and compare the components of existing crowdsourcing systems and point out extensions required by crowd mining. It also highlights new research challenges and potential reuse of existing techniques/components. We exemplify this for the OASSIS project and for other prominent crowdsourcing frameworks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2287536",
                    "name": "Anna Kukliansky"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "16708d0e9bd3da8cc20285cbe1d82a4eb6799e90",
            "title": "Approximated Summarization of Data Provenance",
            "abstract": "Many modern applications involve collecting large amounts of data from multiple sources, and then aggregating and manipulating it in intricate ways. The complexity of such applications, combined with the size of the collected data, makes it difficult to understand how the resulting information was derived. Data provenance has proven helpful in this respect, however, maintaining and presenting the full and exact provenance information may be infeasible due to its size and complexity. We therefore introduce the notion of approximated summarized provenance, which provides a compact representation of the provenance at the possible cost of information loss. Based on this notion, we present a novel provenance summarization algorithm which, based on the semantics of the underlying data and the intended use of provenance, outputs a summary of the input provenance. Experiments measure the conciseness and accuracy of the resulting provenance summaries, and improvement in provenance usage time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2627111",
                    "name": "Eleanor Ainy"
                },
                {
                    "authorId": "1807924",
                    "name": "P. Bourhis"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "2e25e6bf7b1d8c3492d3ad09092b364995d74644",
            "title": "NL2CM: A Natural Language Interface to Crowd Mining",
            "abstract": "The joint processing of general data, which can refer to objective data such as geographical locations, with individual data, which is related to the habits and opinions of individuals, is required in many real-life scenarios. For this purpose, crowd mining platforms combine searching knowledge bases for general data, with mining the crowd for individual, unrecorded data. Existing such platforms require queries to be stated in a formal language. To bridge the gap between na\u00efve users, who are not familiar with formal query languages, and crowd mining platforms, we develop NL2CM, a prototype system which translates natural language (NL) questions into well-formed crowd mining queries. The mix of general and individual information needs raises unique challenges. In particular, the different types of needs must be identified and translated into separate query parts. To account for these challenges, we develop new, dedicated modules and embed them within the modular and easily extensible architecture of NL2CM. Some of the modules interact with the user during the translation process to resolve uncertainties and complete missing data. We demonstrate NL2CM by translating questions of the audience, in different domains, into NL2CM, a crowd mining query language which is based on SPARQL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "2287536",
                    "name": "Anna Kukliansky"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "31a40d471842f4c35c59c0b0579d8d8cf56a3264",
            "title": "A Natural Language Interface for Querying General and Individual Knowledge",
            "abstract": "Many real-life scenarios require the joint analysis of general knowledge, which includes facts about the world, with individual knowledge, which relates to the opinions or habits of individuals. Recently developed crowd mining platforms, which were designed for such tasks, are a major step towards the solution. However, these platforms require users to specify their information needs in a formal, declarative language, which may be too complicated for naive users. To make the joint analysis of general and individual knowledge accessible to the public, it is desirable to provide an interface that translates the user questions, posed in natural language (NL), into the formal query languages that crowd mining platforms support. \n \nWhile the translation of NL questions to queries over conventional databases has been studied in previous work, a setting with mixed individual and general knowledge raises unique challenges. In particular, to support the distinct query constructs associated with these two types of knowledge, the NL question must be partitioned and translated using different means; yet eventually all the translated parts should be seamlessly combined to a well-formed query. To account for these challenges, we design and implement a modular translation framework that employs new solutions along with state-of-the art NL parsing tools. The results of our experimental study, involving real user questions on various topics, demonstrate that our framework provides a high-quality translation for many questions that are not handled by previous translation tools.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "2287536",
                    "name": "Anna Kukliansky"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "4508afd43a616e62627a5f5c6089bb3b3629518f",
            "title": "Dismantling Complicated Query Attributes with Crowd",
            "abstract": "We study the problem of query evaluation with the help of the crowd, when the value of the queried attributes is not available in the database and is also hard for the crowd to estimate. Rather than asking users directly about these attributes, we propose a novel alternative approach that first uses the crowd to dismantle the query attributes into finer related ones (whose value estimation is easier), then assemble them to yield better estimation for the query attributes. We show that it is sometimes beneficial not to only dismantle the query attributes themselves, but rather to continue dismantling newly discovered attributes. We provide a careful statistical analysis to estimate the potential benefit (and cost) of dismantling each of the so-far-discovered attributes. Building on this analysis, we present an e\u21b5ective algorithm that balances between attributes dismantling and obtaining essential statistics about them (for estimating properties like \u201cdi culty\u201d and \u201ccontribution\u201d of attributes) to decide how many crowd members should be asked about each attribute and how the answers should be assembled together. A thorough experimental analysis demonstrates the feasibility and e\u21b5ectiveness of the approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2231434",
                    "name": "Matan Laadan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "56c8626fe79893a60a0a6593b94f5369f3c86e50",
            "title": "QOCO: A Query Oriented Data Cleaning System with Oracles",
            "abstract": "As key decisions are often made based on information contained in a database, it is important for the database to be as complete and correct as possible. For this reason, many data cleaning tools have been developed to automatically resolve inconsistencies in databases. However, data cleaning tools provide only best-effort results and usually cannot eradicate all errors that may exist in a database. Even more importantly, existing data cleaning tools do not typically address the problem of determining what information is missing from a database. \n \nTo tackle these problems, we present QOCO, a novel query oriented cleaning system that leverages materialized views that are defined by user queries as a trigger for identifying the remaining incorrect/missing information. Given a user query, QOCO interacts with domain experts (which we model as oracle crowds) to identify potentially wrong or missing answers in the result of the user query, as well as determine and correct the wrong data that is the cause for the error(s). We will demonstrate QOCO over a World Cup Games database, and illustrate the interaction between QOCO and the oracles. Our demo audience will play the role of oracles, and we show how QOCO's underlying operations and optimization mechanisms can effectively prune the search space and minimize the number of questions that need to be posed to accelerate the cleaning process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29626000",
                    "name": "M. Bergman"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                }
            ]
        },
        {
            "paperId": "6763a2216338ba29b36783f66cb615067f6e748b",
            "title": "From Good Research to Good Papers: A Personal Journey",
            "abstract": "BIO Tova Milo received her Ph.D. degree in Computer Science from the Hebrew University, Jerusalem, in 1992. After graduating she worked at the INRIA research institute in Paris and at University of Toronto and returned to Israel in 1995, joining the School of Computer Science at Tel Aviv university, where she is now a full Professor. She is the head of the Database research group and holds the Chair of Information Management. She served as the Head of the Computer Science Department from 2011-2014. Her research focuses on advanced database applications such as data integration, XML and semi-structured information, Data-centered Business Processes and Crowd-sourcing, studying both theoretical and practical aspects. Tova served as the Program Chair of several international conferences, including PODS, VLDB, ICDT, XSym, and WebDB, and as a member of the VLDB Endowment and the ICDT executive board. She serves as the chair of the PODS Executive Committee and an editor of TODS and the Logical Methods in Computer Science Journal. Tova has received grants from the Israel Science Foundation, the US-Israel Binational Science Foundation, the Israeli and French Ministry of Science and the European Union. She is an ACM Fellow, a member of Academia Europaea, and a recipient of the 2010 ACM PODS Alberto O. Mendelzon Test-of-Time Award and of the prestigious EU ERC Advanced Investigators grant.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "69a2d8434a0b392e67d8ca2e26ba94b40161f715",
            "title": "On the Complexity of Evaluating Order Queries with the Crowd",
            "abstract": "One of the foremost challenges for information technology over the last few years has been to explore, understand, and extract useful information from large amounts of data. Some particular tasks such as annotating data or matching entities have been outsourced to human workers for many years. But the last few years have seen the rise of a new research field called crowdsourcing that aims at delegating a wide range of tasks to human workers, building formal frameworks, and improving the efficiency of these processes. The database community has thus been suggesting algorithms to process traditional data manipulation operators with the crowd, such as joins or filtering. This is even more useful when comparing the underlying \u201ctuples\u201d is a subjective decision \u2013 e.g., when they are photos, text, or simply noisy data with different variations and interpretations \u2013 and can presumably be done better and faster by humans than by machines. The problems considered in this article aim to retrieve a subset of preferred items from a set of items by delegating pairwise comparison operations to the crowd. The most obvious example is finding the maximum of a set of items (called max). We also consider two natural generalizations of the max problem:",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2516234",
                    "name": "Beno\u00eet Groz"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "69a80667199860c28ac218e6105466de9407d223",
            "title": "Data-driven crowdsourcing: Management, mining, and applications",
            "abstract": "In this 3-hour tutorial, we present the landscape of recent developments in data management and mining research, and survey a selected set of state-of-the-art works that significantly extended existing database reserach in order to incorporate and exploit the novel notion of \u201ccrowdsourcing\u201d in a creative fashion. In particular, three speakers take turns to present the topics of human-powered database operations, crowdsourced data mining, and the application of crowdsourcing in social media, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "7fd7aef3f79229657e510e794b83da9f464908c2",
            "title": "Skyline Queries with Noisy Comparisons",
            "abstract": "We study in this paper the computation of skyline queries - a popular tool for multicriteria data analysis - in the presence of noisy input. Motivated by crowdsourcing applications, we present the first algorithms for skyline evaluation in a computation model where the input data items can only be compared through noisy comparisons. In this model comparisons may return wrong answers with some probability, and confidence can be increased through independent repetitions of a comparison. Our goal is to minimize the number of comparisons required for computing or verifying a candidate skyline, while returning the correct answer with high probability. We design output-sensitive algorithms, namely algorithms that take advantage of the potentially small size of the skyline, and analyze the number of comparison rounds of our solutions. We also consider the problem of predicting the most likely skyline given some partial information in the form of noisy comparisons, and show that optimal prediction is computationally intractable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2516234",
                    "name": "Beno\u00eet Groz"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "a252c711233b24c17f07c575b13a2c135fa7c0c8",
            "title": "Query-Oriented Data Cleaning with Oracles",
            "abstract": "As key decisions are often made based on information contained in a database, it is important for the database to be as complete and correct as possible. For this reason, many data cleaning tools have been developed to automatically resolve inconsistencies in databases. However, data cleaning tools provide only best-effort results and usually cannot eradicate all errors that may exist in a database. Even more importantly, existing data cleaning tools do not typically address the problem of determining what information is missing from a database. To overcome the limitations of existing data cleaning techniques, we present QOCO, a novel query-oriented system for cleaning data with oracles. Under this framework, incorrect (resp. missing) tuples are removed from (added to) the result of a query through edits that are applied to the underlying database, where the edits are derived by interacting with domain experts which we model as oracle crowds. We show that the problem of determining minimal interactions with oracle crowds to derive database edits for removing (adding) incorrect (missing) tuples to the result of a query is NP-hard in general and present heuristic algorithms that interact with oracle crowds. Finally, we implement our algorithms in our prototype system QOCO and show that it is effective and efficient through a comprehensive suite of experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29626000",
                    "name": "M. Bergman"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                }
            ]
        },
        {
            "paperId": "c003ee7c2e4f28256eb445caf77ece001f021e74",
            "title": "Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems",
            "abstract": "It is our great pleasure to welcome you to the 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (PODS 2015), held in Melbourne, Victoria, Australia, on May 31 -- June 4, 2015, in conjunction with the 2015 ACM SIGMOD International Conference on Management of Data. Since the first edition of the symposium in 1982, the PODS papers are distinguished by a rigorous approach to widely diverse problems in data management, often bringing to bear techniques from a variety of different areas, including computational logic, finite model theory, computational complexity, algorithm design and analysis, programming languages, and artificial intelligence. The PODS Symposia study data management challenges in a variety of application contexts, including more recently probabilistic data, streaming data, graph data, information retrieval, ontology and semantic web, and data-driven processes and systems. PODS has a tradition of being the premier international conference on the theoretical and foundational aspects of mdata management, and the interested reader is referred to the PODS web pages at http://www.sigmod.org/thepods- pages/ for information on the history of this conference series. \n \nThis year's symposium continues this tradition, but in addition the PODS Executive Committee decided to broaden the scope of PODS, and to explicitly invite for submission papers providing original, substantial contributions in one or more of the following categories: a) deep theoretical exploration of topical areas central to data management; b) new formal frameworks that aim at providing the basis for deeper theoretical investigation of important emerging issues in data management; and c) validation of theoretical approaches from the lens of practical applicability in data management. This volume contains the proceedings of PODS 2015, which include an abstract for the keynote address by Michael I. Johnson (University of California, Berkeley), papers based on two invited tutorials by Todd J. Green (LogicBlox, USA) and Graham Cormode (University of Warwick, UK), and 25 contributions that were selected by the Program Committee for presentation at the symposium. \n \nThis year, PODS experimented for the first time with two submission cycles, where the first cycle allowed also for papers to be revised and resubmitted. For the first cycle, 29 papers were submitted, 4 of which were directly selected for inclusion in the proceedings, and 7 were invited for a resubmission after a revision. The quality of most of the revised papers increased substantially with respect to the first submission, and 6 of those in the end were selected for the proceedings. For the second cycle, 51 papers were submitted, 15 of which were selected, resulting in 25 papers selected overall from a total number of 80 submissions. Most of the 25 accepted papers are extended abstracts. While all submissions have been reviewed by at least four Program Committee members, they have not been formally referred. It is expected that much of the research described in these papers will be published in a more polished and detailed form in scientific journals. \n \nWith respect to the three categories mentioned above, of the 80 submissions (resp., 25 accepted papers), 47 (resp., 19) were classified by the authors in category (a), 28 (resp., 5) in category (b), and only 6 (resp., 3) in category (c). The categories are non-exclusive, and classification was not mandatory; indeed, several papers were classified in more than one category, and for 13 (resp., 3) submissions, no category was specified. \n \nAn important task for the Program Committee has been the selection of the PODS 2015 Best Paper Award. The committee selected the paper \"Parallel-Correctness and Transferability for Conjunctive Queries\" by Tom J. Ameloot, Gaetano Geck, Bas Ketsman, Frank Neven and Thomas Schwentick \n \nOn behalf of the committee, we would like to extend our sincere congratulations to the authors. Since 2008, PODS assigns the ACM PODS Alberto O. Mendelzon Test-of-Time Award to a paper or a small number of papers published in the PODS proceedings ten years prior that had the most impact over the intervening decade. This year's committee, consisting of Dan Suciu (chair), Foto Afrati, and Frank Neven, selected the following two papers. Our warmest congratulations to their authors! \"XPath Satisfiability in the Presence of DTDs\" by Michael Benedikt, Wenfei Fan, and Floris Geerts \"Views and Queries: Determinacy and Rewriting\" by Luc Segoufin and Victor Vianu.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1733846",
                    "name": "Diego Calvanese"
                }
            ]
        },
        {
            "paperId": "e796f8b3f0646066173fb5d1ce840bc5357e2a65",
            "title": "Foundations of Crowd Data Sourcing",
            "abstract": "Crowdsourcing techniques are very powerful when harnessed for the purpose of collecting and managing data. In order to provide sound scientific foundations for crowdsourcing and support the development of efficient crowdsourcing processes, adequate formal models must be defined. In particular, the models must formalize unique characteristics of crowd-based settings, such as the knowledge of the crowd and crowd-provided data; the interaction with crowd members; the inherent inaccuracies and disagreements in crowd answers; and evaluation metrics that capture the cost and effort of the crowd. In this paper, we review the foundational challenges in modeling crowd-based data sourcing, for its two main tasks, namely, harvesting data and processing it with the help of the crowd. For each of the two task types, we dive into the details of one foundational line of work, analyzing its model and reviewing the theoretical results established using this model, such as complexity bounds and efficient algorithms. We also overview a broader spectrum of work on crowd data sourcing, and highlight directions for further research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "eaaad9fb8fe11967bcb64a38078ff60a3743e9e0",
            "title": "SampleClean: Fast and Reliable Analytics on Dirty Data",
            "abstract": "An important obstacle to accurate data analytics is dirty data in the form of missing, duplicate, incorrect, or inconsistent values. In the SampleClean project, we have developed a new suite of techniques to estimate the results of queries when only a sample of data can be cleaned. Some forms of data corruption, such as duplication, can affect sampling probabilities, and thus, new techniques have to be designed to ensure correctness of the approximate query results. We first describe our initial project on computing statistically bounded estimates of sum, count, and avg queries from samples of cleaned data. We subsequently explored how the same techniques could apply to other problems in database research, namely, materialized view maintenance. To avoid expensive incremental maintenance, we maintain only a sample of rows in a view, and then leverage SampleClean to approximate aggregate query results. Finally, we describe our work on a gradient-descent algorithm that extends the key ideas to the increasingly common Machine Learning-based analytics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40428703",
                    "name": "S. Krishnan"
                },
                {
                    "authorId": "1743709",
                    "name": "Jiannan Wang"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "144344283",
                    "name": "Ken Goldberg"
                },
                {
                    "authorId": "1746961",
                    "name": "Tim Kraska"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "48144872",
                    "name": "Eugene Wu"
                }
            ]
        },
        {
            "paperId": "23ddaf4c6194af2bc6b53ba217bfedb0a2927381",
            "title": "Approximated Provenance for Complex Applications",
            "abstract": "Many applications now involve the collection of large amounts of data from multiple users, and then aggregating and manipulating it in intricate ways. The complexity of such applications, combined with the size of the collected data, makes it difficult to understand how information was derived, and consequently difficult to asses its credibility, to optimize and debug its derivation, etc. Provenance has been helpful in achieving such goals in different contexts, and we illustrate its potential for novel complex applications such as those performing crowd-sourcing. Maintaining (and presenting) the full and exact provenance information may be infeasible for such applications, due to the size of the provenance and its complex structure. We propose some initial directions towards addressing this challenge, through the notion of approximated provenance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2627111",
                    "name": "Eleanor Ainy"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "2635e1ff129ac64b08ec6be89c3878684e309740",
            "title": "Ontology Assisted Crowd Mining",
            "abstract": "We present OASSIS (for Ontology ASSISted crowd mining), a prototype system which allows users to declaratively specify their information needs, and mines the crowd for answers. The answers that the system computes are concise and relevant, and represent frequent, significant data patterns. The system is based on (1) a generic model that captures both ontological knowledge, as well as the individual knowledge of crowd members from which frequent patterns are mined; (2) a query language in which users can specify their information needs and types of data patterns they seek; and (3) an efficient query evaluation algorithm, for mining semantically concise answers while minimizing the number of questions posed to the crowd. We will demonstrate OASSIS using a couple of real-life scenarios, showing how users can formulate and execute queries through the OASSIS UI and how the relevant data is mined from the crowd.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "4cf37e72c105e0da55d4494e21db430d68a996d5",
            "title": "The Beckman Report on Database Research",
            "abstract": "Every few years a group of database researchers meets to discuss the state of database research, its impact on practice, and important new directions. This report summarizes the discussion and conclusions of the eighth such meeting, held October 14- 15, 2013 in Irvine, California. It observes that Big Data has now become a defining challenge of our time, and that the database research community is uniquely positioned to address it, with enormous opportunities to make transformative impact. To do so, the report recommends significantly more attention to five research areas: scalable big/fast data infrastructures; coping with diversity in the data management landscape; end-to-end processing and understanding of data; cloud services; and managing the diverse roles of people in the data life cycle.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254232",
                    "name": "D. Abadi"
                },
                {
                    "authorId": "144947410",
                    "name": "R. Agrawal"
                },
                {
                    "authorId": "1728318",
                    "name": "A. Ailamaki"
                },
                {
                    "authorId": "1718134",
                    "name": "M. Balazinska"
                },
                {
                    "authorId": "1737944",
                    "name": "P. Bernstein"
                },
                {
                    "authorId": "1703347",
                    "name": "M. Carey"
                },
                {
                    "authorId": "145647476",
                    "name": "S. Chaudhuri"
                },
                {
                    "authorId": "2056947015",
                    "name": "Jeffrey Dean"
                },
                {
                    "authorId": "3030274",
                    "name": "A. Doan"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "143614516",
                    "name": "J. Gehrke"
                },
                {
                    "authorId": "145704391",
                    "name": "L. Haas"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "1695576",
                    "name": "J. Hellerstein"
                },
                {
                    "authorId": "1684197",
                    "name": "Y. Ioannidis"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1691108",
                    "name": "Donald Kossmann"
                },
                {
                    "authorId": "144478906",
                    "name": "S. Madden"
                },
                {
                    "authorId": "144156242",
                    "name": "S. Mehrotra"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "5151034",
                    "name": "J. Naughton"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "1733290",
                    "name": "V. Markl"
                },
                {
                    "authorId": "2072569",
                    "name": "Christopher Olston"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "2114485554",
                    "name": "C. R\u00e9"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "145345023",
                    "name": "M. Stonebraker"
                },
                {
                    "authorId": "2058106369",
                    "name": "Todd Walter"
                },
                {
                    "authorId": "1737896",
                    "name": "J. Widom"
                }
            ]
        },
        {
            "paperId": "828b31e25f3077779c4f2ab115196f9b9e30bee7",
            "title": "Top-k and Clustering with Noisy Comparisons",
            "abstract": "We study the problems of max/top-k and clustering when the comparison operations may be performed by oracles whose answer may be erroneous. Comparisons may either be of type or of value: given two data elements, the answer to a type comparison is \u201cyes\u201d if the elements have the same type and therefore belong to the same group (cluster); the answer to a value comparison orders the two data elements. We give efficient algorithms that are guaranteed to achieve correct results with high probability, analyze the cost of these algorithms in terms of the total number of comparisons (i.e., using a fixed-cost model), and show that they are essentially the best possible. We also show that fewer comparisons are needed when values and types are correlated, or when the error model is one in which the error decreases as the distance between the two elements in the sorted order increases. Finally, we examine another important class of cost functions, concave functions, which balances the number of rounds of interaction with the oracle with the number of questions asked of the oracle. Results of this article form an important first step in providing a formal basis for max/top-k and clustering queries in crowdsourcing applications, that is, when the oracle is implemented using the crowd. We explain what simplifying assumptions are made in the analysis, what results carry to a generalized crowdsourcing setting, and what extensions are required to support a full-fledged model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "c6850e82b368bbfcaa7aa2e24d5c1e8cf8646172",
            "title": "Circuits for Datalog Provenance",
            "abstract": "The annotation of the results of database queries with provenance information has many applications. This paper studies provenance for datalog queries. We start by considering provenance representation by (positive) Boolean expressions, as pioneered in the theories of incomplete and probabilistic databases. We show that even for linear datalog programs the representation of provenance using Boolean expressions incurs a super-polynomial size blowup in data complexity. We address this with an approach that is novel in provenance studies, showing that we can construct in PTIME poly-size (data complexity) provenance representations as Boolean circuits. Then we present optimization techniques that embed the construction of circuits into seminaive datalog evaluation, and further reduce the size of the circuits. We also illustrate the usefulness of our approach in multiple application domains such as query evaluation in probabilistic databases, and in deletion propagation. Next, we study the possibility of extending the circuit approach to the more general framework of semiring annotations introduced in earlier work. We show that for a large and useful class of provenance semirings, we can construct in PTIME poly-size circuits that capture the provenance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "d4307fc98506b3fb86298380ebe1fd849e7ee792",
            "title": "OASSIS: query driven crowd mining",
            "abstract": "Crowd data sourcing is increasingly used to gather information from the crowd and to obtain recommendations. In this paper, we explore a novel approach that broadens crowd data sourcing by enabling users to pose general questions, to mine the crowd for potentially relevant data, and to receive concise, relevant answers that represent frequent, significant data patterns. Our approach is based on (1) a simple generic model that captures both ontological knowledge as well as the individual history or habits of crowd members from which frequent patterns are mined; (2) a query language in which users can declaratively specify their information needs and the data patterns of interest; (3) an efficient query evaluation algorithm, which enables mining semantically concise answers while minimizing the number of questions posed to the crowd; and (4) an implementation of these ideas that mines the crowd through an interactive user interface. Experimental results with both real-life crowd and synthetic data demonstrate the feasibility and effectiveness of the approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "dcabb31822f026e294971f9051e856053cc11e22",
            "title": "Answering regular path queries on workflow provenance",
            "abstract": "This paper proposes a novel approach for efficiently evaluating regular path queries over provenance graphs of workflows that may include recursion. The approach assumes that an execution g of a workflow G is labeled with query-agnostic reachability labels using an existing technique. At query time, given g, G and a regular path query R, the approach decomposes R into a set of subqueries R1, ..., Rk that are safe for G. For each safe subquery Ri, G is rewritten so that, using the reachability labels of nodes in g, whether or not there is a path which matches Ri between two nodes can be decided in constant time. The results of each safe subquery are then composed, possibly with some small unsafe remainder, to produce an answer to R. The approach results in an algorithm that significantly reduces the number of subqueries k over existing techniques by increasing their size and complexity, and that evaluates each subquery in time bounded by its input and output size. Experimental results demonstrate the benefit of this approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107884501",
                    "name": "Xiaocheng Huang"
                },
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "f55b345477fee491ad68b4b2797499a34e06291e",
            "title": "A sample-and-clean framework for fast and accurate query processing on dirty data",
            "abstract": "In emerging Big Data scenarios, obtaining timely, high-quality answers to aggregate queries is difficult due to the challenges of processing and cleaning large, dirty data sets. To increase the speed of query processing, there has been a resurgence of interest in sampling-based approximate query processing (SAQP). In its usual formulation, however, SAQP does not address data cleaning at all, and in fact, exacerbates answer quality problems by introducing sampling error. In this paper, we explore an intriguing opportunity. That is, we explore the use of sampling to actually improve answer quality. We introduce the Sample-and-Clean framework, which applies data cleaning to a relatively small subset of the data and uses the results of the cleaning process to lessen the impact of dirty data on aggregate query answers. We derive confidence intervals as a function of sample size and show how our approach addresses error bias. We evaluate the Sample-and-Clean framework using data from three sources: the TPC-H benchmark with synthetic noise, a subset of the Microsoft academic citation index and a sensor data set. Our results are consistent with the theoretical confidence intervals and suggest that the Sample-and-Clean framework can produce significant improvements in accuracy compared to query processing without data cleaning and speed compared to data cleaning without sampling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1743709",
                    "name": "Jiannan Wang"
                },
                {
                    "authorId": "40428703",
                    "name": "S. Krishnan"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "144344283",
                    "name": "Ken Goldberg"
                },
                {
                    "authorId": "1746961",
                    "name": "Tim Kraska"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "308eb455c8f0a407a4d8f4f7820e1c4eac2864b9",
            "title": "The ACM PODS Alberto O. Mendelzon test-of-time award 2013",
            "abstract": "In 2007, the PODS Executive Committee decided to establish a Test-of-Time Award, named after the late Alberto O. Mendelzon, in recognition of his scientific legacy, and his service and dedication to the database community. Mendelzon was an international leader in database theory, whose pioneering and fundamental work has inspired and influenced both database theoreticians and practitioners, and continues to be applied in a variety of advanced settings. He served the database community in many ways; in particular, he served as the General Chair of the PODS conference, and was instrumental in bringing together the PODS and SIGMOD conferences. He also was an outstanding educator, who guided the research of numerous doctoral students and postdoctoral fellows. The Award is to be awarded each year to a paper or a small number of papers published in the PODS proceedings ten years prior, that had the most impact (in terms of research, methodology, or transfer of practice) over the intervening decade. The decision was approved by SIGMOD and ACM. The funds for the Award were contributed by IBM Toronto.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1750856",
                    "name": "Michael Benedikt"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1682274",
                    "name": "D. V. Gucht"
                }
            ]
        },
        {
            "paperId": "45b03b85ee9f91b54d5d7baec01e3079693b2dd4",
            "title": "In the Mood4: recommendation by examples",
            "abstract": "Traditional recommender systems generate personalized recommendations based on a profile that they create for each user. We argue here that such profiles are often too coarse to capture the current user's state of mind and desire. For example, a serious user that usually prefers documentary features may, at the end of a long and tiring conference, be in the mood for a lighter entertaining movie, not captured by her usual profile. As communicating one's state of mind to a system in (key)words may be difficult, we present in this demo Mood4 - a novel plug-in for recommender systems, which allows users to describe their current desire/mood through examples. Mood4 utilizes the user's examples to refine the recommendations generated by a given recommender system, considering several, possibly competing, desired properties of the recommended items set (rating, diversity, coverage). The system uses a novel algorithm, based on a simple geometric representation of the items, which allows for efficient processing and the generation of suitable recommendations even in the absence of semantic information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1950494",
                    "name": "Rubi Boim"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "6495514d150da4e6e65b973e0e53d1749724bb07",
            "title": "Caravan: Provisioning for What-If Analysis",
            "abstract": "Problems of what-if analysis (such as hypothetical deletions, insertions, and modifications) over complex analysis queries are increasingly commonplace, e.g., in forming a business strategy or looking for causal relationships in science. Here, data analysts are typically interested only in task-specific views of the data, and they expect to be able to interactively manipulate the data in a natural and seamless way \u2014 possibly on a phone or tablet, and possibly via a spreadsheet or similar interface without having to carry the full machinery of a DBMS. The Caravan system enables what-if analysis: fast, lightweight, interactive exploration of alternative answers, within views computed over large-scale distributed data sources. Our novel approach is based on creating dedicated provisioned autonomous representations, or PARs. PARs are compiled out of the data, initial analysis queries and user-specified what-if scenarios. They allow rapid evaluation of what-if scenarios without accessing the original data or performing complex query operations. Importantly, the size of PARs is governed by the parameters of the what-if analysis and is proportional to the size of the initial query answer rather than the typically much larger source data. Consequently, many what-if analysis tasks performed through PAR evaluations can be done autonomously, on limited-resource devices. We describe our model and architecture, demonstrate preliminary performance results, and present several open implementation and optimization issues.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1804315",
                    "name": "Z. Ives"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "6ca9ba6a24263ef84a5370cbc7efca69fe2144d0",
            "title": "Crowd mining",
            "abstract": "Harnessing a crowd of Web users for data collection has recently become a wide-spread phenomenon. A key challenge is that the human knowledge forms an open world and it is thus difficult to know what kind of information we should be looking for. Classic databases have addressed this problem by data mining techniques that identify interesting data patterns. These techniques, however, are not suitable for the crowd. This is mainly due to properties of the human memory, such as the tendency to remember simple trends and summaries rather than exact details.\n Following these observations, we develop here for the first time the foundations of crowd mining. We first define the formal settings. Based on these, we design a framework of generic components, used for choosing the best questions to ask the crowd and mining significant patterns from the answers. We suggest general implementations for these components, and test the resulting algorithm's performance on benchmarks that we designed for this purpose. Our algorithm consistently outperforms alternative baseline algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "2096288317",
                    "name": "Yael Grossman"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "7282173a31cb283c5b05ffd2440cd8c2c17699d4",
            "title": "Answering Planning Queries with the Crowd",
            "abstract": "Recent research has shown that crowd sourcing can be used effectively to solve problems that are difficult for computers, e.g., optical character recognition and identification of the structural configuration of natural proteins. In this paper we propose to use the power of the crowd to address yet another difficult problem that frequently occurs in a daily life - answering planning queries whose output is a sequence of objects/actions, when the goal, i.e, the notion of \"best output\", is hard to formalize. For example, planning the sequence of places/attractions to visit in the course of a vacation, where the goal is to enjoy the resulting vacation the most, or planning the sequence of courses to take in an academic schedule planning, where the goal is to obtain solid knowledge of a given subject domain. Such goals may be easily understandable by humans, but hard or even impossible to formalize for a computer. \n \nWe present a novel algorithm for efficiently harnessing the crowd to assist in answering such planning queries. The algorithm builds the desired plans incrementally, choosing at each step the 'best' questions so that the overall number of questions that need to be asked is minimized. We prove the algorithm to be optimal within its class and demonstrate experimentally its effectiveness and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145325898",
                    "name": "Haim Kaplan"
                },
                {
                    "authorId": "3158592",
                    "name": "Ilia Lotosh"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "7d62e3ee01ea384007f95c3e28db4f3e0dad8526",
            "title": "Understanding Local Structure in Ranked Datasets",
            "abstract": "Ranked data is ubiquitous in real-world applications. Rankings arise naturally when users express preferences about products and services, when voters cast ballots in elections, when funding pro- posals are evaluated based on their merits and university depart- ments based on their reputation, or when genes are ordered based on their expression levels under various experimental conditions. We observe that ranked data exhibits interesting local structure, representing agreement of subsets of rankers over subsets of items. Being able to model, identify and describe such structure is im- portant, because it enables novel kinds of analysis with the poten- tial of making ground-breaking impact, but is challenging to do effectively and efficiently. We argue for the use of fundamental data management principles such as declarativeness and incremen- tal evaluation, in combination with state-of-the-art machine learn- ing and data mining techniques, for addressing the effectiveness and efficiency challenges. We describe the key ingredients of a so- lution, and propose aroadmap towards aframework that will enable robust and efficient analysis of large ranked datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "40076282",
                    "name": "Marie Jacob"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "8d81357855a607d9be4f9131339a88c6f9b98453",
            "title": "CrowdPlanr: Planning made easy with crowd",
            "abstract": "Recent research has shown that crowd sourcing can be used effectively to solve problems that are difficult for computers, e.g., optical character recognition and identification of the structural configuration of natural proteins [1]. In this demo we propose to use the power of the crowd to address yet another difficult problem that frequently occurs in a daily life-planning a sequence of actions, when the goal is hard to formalize. For example, planning the sequence of places/attractions to visit in the course of a vacation, where the goal is to enjoy the resulting vacation the most, or planning the sequence of courses to take in an academic schedule planning, where the goal is to obtain solid knowledge of a given subject domain. Such goals may be easily understandable by humans, but hard or even impossible to formalize for a computer. We present a novel algorithm for efficiently harnessing the crowd to assist in solving such planning problems. The algorithm builds the desired plans incrementally, optimally choosing at each step the `best' questions so that the overall number of questions that need to be asked is minimized. We demonstrate the effectiveness of our solution in CrowdPlanr, a system for vacation travel planning. Given a destination, dates, preferred activities and other constraints CrowdPlanr employs the crowd to build a vacation plan (sequence of places to visit) that is expected to maximize the \u201cenjoyment\u201d of the vacation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3158592",
                    "name": "Ilia Lotosh"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "aa3ace22c3f6b895bb5dd4813fe231c61526bd1e",
            "title": "Using the crowd for top-k and group-by queries",
            "abstract": "Group-by and top-k are fundamental constructs in database queries. However, the criteria used for grouping and ordering certain types of data -- such as unlabeled photos clustered by the same person ordered by age -- are difficult to evaluate by machines. In contrast, these tasks are easy for humans to evaluate and are therefore natural candidates for being crowd-sourced.\n We study the problem of evaluating top-k and group-by queries using the crowd to answer either type or value questions. Given two data elements, the answer to a type question is \"yes\" if the elements have the same type and therefore belong to the same group or cluster; the answer to a value question orders the two data elements. The assumption here is that there is an underlying ground truth, but that the answers returned by the crowd may sometimes be erroneous. We formalize the problems of top-k and group-by in the crowd-sourced setting, and give efficient algorithms that are guaranteed to achieve good results with high probability. We analyze the crowd-sourced cost of these algorithms in terms of the total number of type and value questions, and show that they are essentially the best possible. We also show that fewer questions are needed when values and types are correlated, or when the error model is one in which the error decreases as the distance between the two elements in the sorted order increases.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "d342f39018a27b336dded07e7e3f957509d1ab22",
            "title": "On the Complexity of Mining Itemsets from the Crowd Using Taxonomies",
            "abstract": "We study the problem of frequent itemset mining in domains where data is not recorded in a conventional database but only exists in human knowledge. We provide examples of such scenarios, and present a crowdsourcing model for them. The model uses the crowd as an oracle to find out whether an itemset is frequent or not, and relies on a known taxonomy of the item domain to guide the search for frequent itemsets. In the spirit of data mining with oracles, we analyze the complexity of this problem in terms of (i) crowd complexity, that measures the number of crowd questions required to identify the frequent itemsets; and (ii) computational complexity, that measures the computational effort required to choose the questions. We provide lower and upper complexity bounds in terms of the size and structure of the input taxonomy, as well as the size of a concise description of the output itemsets. We also provide constructive algorithms that achieve the upper bounds, and consider more efficient variants for practical situations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2441482",
                    "name": "Antoine Amarilli"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "f4577c8850ae8e348570af0fe6f5869b522d61bb",
            "title": "CrowdMiner: Mining association rules from the crowd",
            "abstract": "This demo presents CrowdMiner, a system enabling the mining of interesting data patterns from the crowd. While traditional data mining techniques have been used extensively for finding patterns in classic databases, they are not always suitable for the crowd, mainly because humans tend to remember only simple trends and summaries rather than exact details. To address this, CrowdMiner employs a novel crowd-mining algorithm, designed specifically for this context. The algorithm iteratively chooses appropriate questions to ask the crowd, while aiming to maximize the knowledge gain at each step. We demonstrate CrowdMiner through a Well-Being portal, constructed interactively by mining the crowd, and in particular the conference participants, for common health related practices and trends.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "2096288317",
                    "name": "Yael Grossman"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "1c77c523c64cfcf43f6ef8bab6785700b40c913b",
            "title": "A propagation model for provenance views of public/private workflows",
            "abstract": "We study the problem of concealing functionality of a proprietary or private module when provenance information is shown over repeated executions of a workflow which contains both public and private modules. Our approach is to use provenance views to hide carefully chosen subsets of data over all executions of the workflow to ensure \u0393-privacy: for each private module and each input x, the module's output f(x) is indistinguishable from \u0393--1 other possible values given the visible data in the workflow executions. We show that \u0393-privacy cannot be achieved simply by combining solutions for individual private modules; data hiding must also be propagated through public modules. We then examine how much additional data must be hidden and when it is safe to stop propagating data hiding. The answer depends strongly on the workflow topology as well as the behavior of public modules on the visible data. In particular, for a class of workflows (which include the common tree and chain workflows), taking private solutions for each private module, augmented with a public closure that is upstream-downstream safe, ensures \u0393-privacy. We define these notions formally and show that the restrictions are necessary. We also study the related optimization problems of minimizing the amount of hidden data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "371fc532e70dc5a72c49eb3503ae1f707f38ea7d",
            "title": "Asking the Right Questions in Crowd Data Sourcing",
            "abstract": "Crowd-based data sourcing is a new and powerful data procurement paradigm that engages Web users to collectively contribute information. In this work, we target the problem of gathering data from the crowd in an economical and principled fashion. We present Ask It!, a system that allows interactive data sourcing applications to effectively determine which questions should be directed to which users for reducing the uncertainty about the collected data. Ask It! uses a set of novel algorithms for minimizing the number of probing (questions) required from the different users. We demonstrate the challenge and our solution in the context of a multiple-choice question game played by the ICDE'12 attendees, targeted to gather information on the conference's publications, authors and colleagues.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1950494",
                    "name": "Rubi Boim"
                },
                {
                    "authorId": "2201956",
                    "name": "Ohad Greenshpan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "1763100",
                    "name": "N. Polyzotis"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                }
            ]
        },
        {
            "paperId": "6ce69895aa52473659120a5d0227c4af82fdafee",
            "title": "Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database Systems",
            "abstract": "First, a brief overview of the contents of this volume, the proceedings of the thirty-first ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2012). The proceedings includes an overview of the keynote address by Surajit Chaudhuri along with two papers based on invited tutorials, one by Michael Mahoney and one by Benjamin Pierce. There are 26 research papers that were selected by the Program Committee, out of 101 submissions with authors from over 25 countries across the world. Out of the 26 accepted papers, the program committee selected the paper Worst Case Optimal Join Algorithms by Hung Q. Ngo, Ely Porat, Christopher Re and Atri Rudra for the PODS 2012 Best Paper Award. In addition, the announcement of the 2012 ACM PODS Alberto O. Mendelzon Test-of-Time Award appears in the proceedings, given to Containment and Equivalence for an XPath Fragment by Gerome Miklau and Dan Suciu. The latter paper originally appeared in the proceedings of PODS 2002. Congratulations to the authors of these papers. \n \nThe review process was grueling, and involved enormous effort from a large group of researchers. This year PODS experimented with the use of an External Review Committee, consisting of distinguished experts in areas of particular interest to PODS, in addition to our core Program Committee. We relied heavily on the Easychair system for management of all aspects of the review process, and we are extremely grateful to Andrei Voronkov for his help in adding and modifying new features to support the External Review Committee. All members of the Program Committee, External Review Committee, and the additional external referees deserve thanks for their work -- both for producing the selection of papers that appear here in the proceedings, and for providing high-quality feedback to all authors of submitted papers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1733846",
                    "name": "Diego Calvanese"
                }
            ]
        },
        {
            "paperId": "8122f642979584897c416d1c89e8619f726bbbb9",
            "title": "On Provenance Minimization",
            "abstract": "Provenance information has been proved to be very effective in capturing the computational process performed by queries, and has been used extensively as the input to many advanced data management tools (e.g., view maintenance, trust assessment, or query answering in probabilistic databases).\n We observe here that while different (set-)equivalent queries may admit different provenance expressions when evaluated on the same database, there is always some part of these expressions that is common to all. We refer to this part as the core provenance. In addition to being informative, the core provenance is also useful as a compact input to the aforementioned data management tools. We formally define the notion of core provenance. We study algorithms that, given a query, compute an equivalent (called p-minimal) query that for every input database, the provenance of every result tuple is the core provenance. We study such algorithms for queries of varying expressive power (namely conjunctive queries with disequalities and unions thereof). Finally, we observe that, in general, one would not want to require database systems to execute a specific p-minimal query, but instead to be able to find, possibly off-line, the core provenance of a given tuple in the output (computed by an arbitrary equivalent query), without reevaluating the query. We provide algorithms for such direct computation of the core provenance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "89ae66624bf21e9be92c81bb08ed24708ec65938",
            "title": "Finding optimal probabilistic generators for XML collections",
            "abstract": "We study the problem of, given a corpus of XML documents and its schema, finding an optimal (generative) probabilistic model, where optimality here means maximizing the likelihood of the particular corpus to be generated. Focusing first on the structure of documents, we present an efficient algorithm for finding the best generative probabilistic model, in the absence of constraints. We further study the problem in the presence of integrity constraints, namely key, inclusion, and domain constraints. We study in this case two different kinds of generators. First, we consider a continuation-test generator that performs, while generating documents, tests of schema satisfiability; these tests prevent from generating a document violating the constraints but, as we will see, they are computationally expensive. We also study a restart generator that may generate an invalid document and, when this is the case, restarts and tries again. Finally, we consider the injection of data values into the structure, to obtain a full XML document. We study different approaches for generating these values.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "8ab34f454d4a9020ea040f2c826955400e25ff2d",
            "title": "Declarative platform for data sourcing games",
            "abstract": "Harnessing a crowd of users for the collection of mass data (data sourcing) has recently become a wide-spread practice. One effective technique is based on games as a tool that attracts the crowd to contribute useful facts. We focus here on the data management layer of such games, and observe that the development of this layer involves challenges such as dealing with probabilistic data, combined with recursive manipulation of this data. These challenges are difficult to address using current declarative data management framework works, and we thus propose here a novel such framework, and demonstrate its usefulness in expressing different aspects in the data management of Trivia-like games. We have implemented a system prototype with our novel data management framework at its core, and we highlight key issues in the system design, as well as our experimentations that indicate the usefulness and scalability of the approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "2201956",
                    "name": "Ohad Greenshpan"
                },
                {
                    "authorId": "2076948442",
                    "name": "Boris Kostenko"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "8d3e1d7a2395473d860440fe0764473feef54a15",
            "title": "Auto-completion learning for XML",
            "abstract": "Editing an XML document manually is a complicated task. While many XML editors exist in the market, we argue that some important functionalities are missing in all of them. Our goal is to makes the editing task simpler and faster. We present ALEX (Auto-completion Learning Editor for XML), an editor that assists the users by providing intelligent auto-completion suggestions. These suggestions are adapted to the user needs, simply by feeding ALEX with a set of example XML documents to learn from. The suggestions are also guaranteed to be compliant with a given XML schema, possibly including integrity constraints. To fulfill this challenging goal, we rely on novel, theoretical foundations by us and others, which are combined here in a system for the first time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "b46be9e73c67b1ab5dd333b3ac3b31351826d97d",
            "title": "Labeling Workflow Views with Fine-Grained Dependencies",
            "abstract": "This paper considers the problem of efficiently answering reachability queries over views of provenance graphs, derived from executions of workflows that may include recursion. Such views include composite modules and model fine-grained dependencies between module inputs and outputs. A novel view-adaptive dynamic labeling scheme is developed for efficient query evaluation, in which view specifications are labeled statically (i.e. as they are created) and data items are labeled dynamically as they are produced during a workflow execution. Although the combination of fine-grained dependencies and recursive workflows entail, in general, long (linear-size) data labels, we show that for a large natural class of workflows and views, labels are compact (logarithmic-size) and reachability queries can be evaluated in constant time. Experimental results demonstrate the benefit of this approach over the state-of-the-art technique when applied for labeling multiple views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "b7069dd4891ad152cd6d169f4ddb2bba4ff8c38f",
            "title": "Mob data sourcing",
            "abstract": "Crowdsourcing is an emerging paradigm that harnesses a mass of users to perform various types of tasks. We focus in this tutorial on a particular form of crowdsourcing, namely crowd (or mob) datasourcing whose goal is to obtain, aggregate or process data. We overview crowd datasourcing solutions in various contexts, explain the need for a principled solution, describe advances towards achieving such a solution, and highlight remaining gaps.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "c2fa2338e1042dcc9342b8b7bab83f46c7719b63",
            "title": "Business Processes: A Database Perspective",
            "abstract": "While classic data management focuses on the data itself, research on Business Processes also considers the context in which this data is generated and manipulated, namely the processes, users, and goals that this data serves. This provides the analysts a better perspective of the organizational needs centered around the data. As such, this research is of fundamental importance. Much of the success of database systems in the last decade is due to the beauty and elegance of the relational model and its declarative query languages, combined with a rich spectrum of underlying evaluation and optimization techniques, and efficient implementations. Much like the case for traditional database research, elegant modeling and rich underlying technology are likely to be highly beneficiary for the Business Process owners and their users; both can benefit from easy formulation and analysis of the processes. While there have been many important advances in this research in recent years, there is still much to be desired: specifically, there have been many works that focus on the processes behavior (flow), and many that focus on its data, but only very few works have dealt with both the state-of-the-art in a database approach to Business Process modeling and analysis, the progress towards a holistic flow-and-data framework for these tasks, and highlight the current gaps and research directions. Table of Contents: Introduction / Modeling / Querying Business Processes / Other Issues / Conclusion",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "1bc75fb34cac01115e92d5b667800378f75cefff",
            "title": "Diversification and refinement in collaborative filtering recommender",
            "abstract": "This paper considers a popular class of recommender systems that are based on Collaborative Filtering (CF) and proposes a novel technique for diversifying the recommendations that they give to users. Items are clustered based on a unique notion of priority-medoids that provides a natural balance between the need to present highly ranked items vs. highly diverse ones. Our solution estimates items diversity by comparing the rankings that different users gave to the items, thereby enabling diversification even in common scenarios where no semantic information on the items is available. It also provides a natural zoom-in mechanism to focus on items (clusters) of interest and recommending diversified similar items. We present DiRec a plug-in that implements the above concepts and allows CF Recommender systems to diversify their recommendations. We illustrate the operation of DiRec in the context of a movie recommendation system and present a thorough experimental study that demonstrates the effectiveness of our recommendation diversification technique and its superiority over previous solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1950494",
                    "name": "Rubi Boim"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "20894deb609ff804f5877a56461141bdf7a82114",
            "title": "Proceedings of the 14th International Conference on Database Theory",
            "abstract": "The papers in this volume were presented at the 14th International Conference on Database Theory (ICDT'11), held in Uppsala, Sweden, March 21--23, 2011. Starting at 2009, ICDT is being held jointly with the EDBT (Extending Database Technology) conference. EDBT took place on March 22--24, 2011. \n \nIn response to the Call for Papers, 54 submissions were received by the submission deadline of August 14, 2010. All were submitted electronically through CMT. CMT was also used for the Program Committee deliberations, which were held completely electronically. The Program Committee selected 22 papers for presentation. The ICDT Awards Committee, chaired by Alin Deutsch, selected the paper \"Efficient Reasoning about Data Trees via Integer Linear Programming\", by Clair David, Leonid Libkin and Tony Tan, for the ICDT Best Paper Award, and the paper \"Querying Probabilistic Business Processes for Sub-Flows\", by Daniel Deutch, for the ICDT Best Student Paper Award. \n \nIn addition, there were four ICDT/EDBT keynote speakers: Dan Suciu, Jeff Ullman, Susan Davidson and Gerhard Weikum. Furthermore, Kathleen Fisher gave an invited ICDT tutorial.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "20a040df14a4656d49b6764e3a8c174c36158206",
            "title": "Methods for boosting recommender systems",
            "abstract": "Online shopping has grown rapidly over the past few years. Besides the convenience of shopping directly from ones home, an important advantage of e-commerce is the great variety of items that online stores offer. However, with such a large number of items, it becomes harder for vendors to determine which items are more relevant for a given user. Recommender Systems are programs that attempt to assist in such scenarios by presenting the user a small subset of items which she is likely to find interesting. We consider in this work a popular class of such systems that are based on Collaborative Filtering (CF for short). CF is the process of predicting user ratings to items based on previous ratings of (similar) users to (similar) items. The objective of this research is to develop new algorithms and methods for boosting CF based Recommender Systems. Specifically, we focus on the following four challenges: (1) improving the quality of the predictions that such systems provide; (2) devising new methods for choosing the recommended items to be presented to the users; (3) improving the efficiency of CF algorithms and related data structures; (4) incorporating recommendation algorithms in different application domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1950494",
                    "name": "Rubi Boim"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "2d33cc316f29dcfaebf334026743a5af00e46ba1",
            "title": "Labeling recursive workflow executions on-the-fly",
            "abstract": "This paper presents a compact labeling scheme for answering reachability queries over workflow executions. In contrast to previous work, our scheme allows nodes (processes and data) in the execution graph to be labeled on-the-fly, i.e., in a dynamic fashion. In this way, reachability queries can be answered as soon as the relevant data is produced. We first show that, in general, for workflows that contain recursion, dynamic labeling of executions requires long (linear-size) labels. Fortunately, most real-life scientific workflows are linear recursive, and for this natural class we show that dynamic, yet compact (logarithmic-size) labeling is possible. Moreover, our scheme labels the executions in linear time, and answers any reachability query in constant time. We also show that linear recursive workflows are, in some sense, the largest class of workflows that allow compact, dynamic labeling schemes. Interestingly, the empirical evaluation, performed over both real and synthetic workflows, shows that our proposed dynamic scheme outperforms the state-of-the-art static scheme for large executions, and creates labels that are shorter by a factor of almost 3.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "2f1ac823d3763e5fc9d9bd8926554da0372b36f3",
            "title": "Enabling Privacy in Provenance-Aware Workflow Systems",
            "abstract": "A new paradigm for creating and correcting scientific analyses is emerging, that of provenance-aware workflow systems. In such systems, repositories of workflow specifications and of provenance graphs that represent their executions will be made available as part of scientific information sharing. This will allow users to search and query both workflow specifications and their provenance graphs: Scientists who wish to perform new analyses may search workflow repositories to find specifications of interest to reuse or modify. They may also search provenance information to understand the meaning of a workflow, or to debug a specification. Finding erroneous or suspect data, a user may then ask provenance queries to determine what downstream data might have been affected, or to understand how the process failed that led to creating the data. With the increased amount of available provenance information, there is a need to efficiently search and query scientific workflows and their executions. However, workflow authors or owners may wish to keep some information in the repository confidential. For example, intermediate data within an execution may contain sensitive information, such as a social security number, a medical record, or financial information about an individual. Although users with the appropriate access level may be allowed to see such confidential data, making it available to all users, even for scientific purposes, is an unacceptable breach of privacy. Beyond data privacy, a module itself may be proprietary, and hiding its description may not be enough: users without the appropriate access level should not be able to infer its behavior if they are allowed to see the inputs and outputs of the module. Finally, details of how certain modules in the workflow are connected may be proprietary, and so showing how data is passed between modules may reveal too much of the structure of the workflow. There is thus an inherent tradeoff between the utility of the information provided in response to a search/query and the privacy guarantees that",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                },
                {
                    "authorId": "2118047969",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "51b3c682b27deb7459928d01400432bfb52c00e9",
            "title": "A quest for beauty and wealth (or, business processes for database researchers)",
            "abstract": "While classic data management focuses on the data itself, research on Business Processes considers also the context in which this data is generated and manipulated, namely the processes, the users, and the goals that this data serves. This allows the analysts a better perspective of the organizational needs centered around the data. As such, this research is of fundamental importance. Much of the success of database systems in the last decade is due to the beauty and elegance of the relational model and its declarative query languages, combined with a rich spectrum of underlying evaluation and optimization techniques, and efficient implementations. This, in turn, has lead to an economic wealth for both the users and vendors of database systems. Similar beauty and wealth are sought for in the context of Business Processes. Much like the case for traditional database research, elegant modeling and rich underlying technology are likely to bring economic wealth for the Business Process owners and their users; both can benefit from easy formulation and analysis of the processes. While there have been many important advances in this research in recent years, there is still much to be desired: specifically, there have been many works that focus on the processes behavior (flow), and many that focus on its data, but only very few works have dealt with both. We will discuss here the important advantages of a holistic flow-and-data framework for Business Processes, the progress towards such a framework, and highlight the current gaps and research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "5a79d82f52280eef49bcadb03c41e777078fb105",
            "title": "Making interval-based clustering rank-aware",
            "abstract": "In online applications, such as online dating, users often query and rank large collections of structured items. Top results tend to be homogeneous, which hinders data exploration. For example, a dating website user who is looking for a partner between 20 and 40 years old, and who sorts the matches by income from higher to lower, will see a large number of matches in their late 30s who hold an MBA degree and work in the financial industry, before seeing any matches in different age groups and walks of life. An alternative to presenting results in a ranked list is to find clusters in the result space, identified by a combination of attributes that correlate with rank. Such clusters may describe matches between 35 and 40 with an MBA, matches between 25 and 30 who work in the software industry, etc., allowing for data exploration of ranked results.\n We refer to the problem of finding such clusters as rank-aware interval-based clustering and argue that it is not addressed by standard clustering algorithms. We formally define the problem and, to solve it, propose a novel measure of locality, together with a family of clustering quality measures appropriate for this application scenario. These ingredients may be used by a variety of clustering algorithms, and we present BARAC, a particular subspace-clustering algorithm that enables rank-aware interval-based clustering in domains with heterogeneous attributes. We validate the effectiveness of our approach with a large-scale user study, and perform an extensive experimental evaluation of efficiency, demonstrating that our methods are practical on the large scale. Our evaluation is performed on large datasets from Yahoo! Personals, a leading online dating site, and on restaurant data from Yahoo! Local.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "7cc425ee84d48c847a121e2966b92440a86e9aca",
            "title": "A Fine-Grained Workflow Model with Provenance-Aware Security Views",
            "abstract": "In this paper we propose a fine-grained workflow model, based on context-free graph grammars, in which the dependency relation between the inputs and outputs of a module is explicitly specified as a bipartite graph. Using this model, we develop an access control mechanism that supports provenance-aware security views. Our security model not only protects sensitive data and modules from unauthorized access, but also provides the flexibility to expose correct or partially correct data dependency relationships within the provenance information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "88ee7b6bfd7530ebad8ed4b6a306df4989050aee",
            "title": "DiRec: Diversified recommendations for semantic-less Collaborative Filtering",
            "abstract": "In this demo we present DiRec, a plug-in that allows Collaborative Filtering (CF) Recommender systems to diversify the recommendations that they present to users. DiRec estimates items diversity by comparing the rankings that different users gave to the items, thereby enabling diversification even in common scenarios where no semantic information on the items is available. Items are clustered based on a novel notion of priority-medoids that provides a natural balance between the need to present highly ranked items vs. highly diverse ones. We demonstrate the operation of DiRec in the context of a movie recommendation system. We show the advantage of recommendation diversification and its feasibility even in the absence of semantic information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1950494",
                    "name": "Rubi Boim"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "cd3cd7f71d6b7ef5a6ede863237caef7c789789d",
            "title": "Using Markov Chain Monte Carlo to play Trivia",
            "abstract": "We introduce in this Demonstration a system called Trivia Masster that generates a very large Database of facts in a variety of topics, and uses it for question answering. The facts are collected from human users (the \u201ccrowd\u201d); the system motivates users to contribute to the Database by using a Trivia Game, where users gain points based on their contribution. A key challenge here is to provide a suitable Data Cleaning mechanism that allows to identify which of the facts (answers to Trivia questions) submitted by users are indeed correct / reliable, and consequently how many points to grant users, how to answer questions based on the collected data, and which questions to present to the Trivia players, in order to improve the data quality. As no existing single Data Cleaning technique provides a satisfactory solution to this challenge, we propose here a novel approach, based on a declarative framework for defining recursive and probabilistic Data Cleaning rules. Our solution employs an algorithm that is based on Markov Chain Monte Carlo Algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "2201956",
                    "name": "Ohad Greenshpan"
                },
                {
                    "authorId": "2076948442",
                    "name": "Boris Kostenko"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "d6a6a60d62ae47a44dc730ff56dd745982399be1",
            "title": "Deriving probabilistic databases with inference ensembles",
            "abstract": "Many real-world applications deal with uncertain or missing data, prompting a surge of activity in the area of probabilistic databases. A shortcoming of prior work is the assumption that an appropriate probabilistic model, along with the necessary probability distributions, is given. We address this shortcoming by presenting a framework for learning a set of inference ensembles, termed meta-rule semi-lattices, or MRSL, from the complete portion of the data. We use the MRSL to infer probability distributions for missing data, and demonstrate experimentally that high accuracy is achieved when a single attribute value is missing per tuple. We next propose an inference algorithm based on Gibbs sampling that accurately predicts the probability distribution for multiple missing values. We also develop an optimization that greatly improves performance of multi-attribute inference for collections of tuples, while maintaining high accuracy. Finally, we develop an experimental framework to evaluate the efficiency and accuracy of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "d72a7149c7dbb6bcc51df92531187cc9fa3923c0",
            "title": "On the optimality of top-k algorithms for Interactive Web Applications",
            "abstract": "In an interactive Web application, the application state changes according to user choices/actions. To assist users in their interaction with such applications, there is a need to provide them with recommendations for the top-k (according to some ranking metric) interactions. These recommendations must be continually updated, as the user interacts with the application, to be consistent with the actual choices she makes. Efficiency of computation is critical here to provide fast response time and a pleasant user experience. This paper establishes formal foundations for measuring the optimality of top-k algorithms of the aforementioned type, i.e. how well they perform relative to other algorithms, with respect to all possible input instances. We define several intuitive notions of optimality in this setting, analyze the fundamental difficulties in obtaining optimal algorithms, and identify conditions under which such algorithms exist.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "ef75f2cd244f89812152df76b006ee72810ad7db",
            "title": "Putting Lipstick on Pig: Enabling Database-style Workflow Provenance",
            "abstract": "Workflow provenance typically assumes that each module is a \"black-box\", so that each output depends on all inputs (coarse-grained dependencies). Furthermore, it does not model the internal state of a module, which can change between repeated executions. In practice, however, an output may depend on only a small subset of the inputs (fine-grained dependencies) as well as on the internal state of the module. We present a novel provenance framework that marries database-style and workflow-style provenance, by using Pig Latin to expose the functionality of modules, thus capturing internal state and fine-grained dependencies. A critical ingredient in our solution is the use of a novel form of provenance graph that models module invocations and yields a compact representation of fine-grained workflow provenance. It also enables a number of novel graph transformation operations, allowing to choose the desired level of granularity in provenance querying (ZoomIn and ZoomOut), and supporting \"what-if\" workflow analytic queries. We implemented our approach in the Lipstick system and developed a benchmark in support of a systematic performance evaluation. Our results demonstrate the feasibility of tracking and querying fine-grained workflow provenance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "f029a8cfd29ab63fb1024d238505b113fdc5f57e",
            "title": "Optimal Probabilistic Generators for XML Corpora",
            "abstract": "We study the problem of, given a corpus of XML documents and its schema, finding an optimal probabilistic model (optimality meaning maximizing the likelihood of the corpus to be generated). We present an efficient algorithm for finding the best probabilistic model, in absence of constraints. We further study the problem in presence of integrity constraints (key, inclusion, and domain constraints) and consider in this case two different kinds of generators: a continuation-test generator that performs, while generating, some tests of schema satisfiability; these tests allow avoiding the violation of constraints (but as we show, are costly to implement), and a restart generator that may generate an invalid document and then restart and try again. R\u00e9sum\u00e9 \u00c9tant donn\u00e9s un corpus de documents XML et son sch\u00e9ma, nous \u00e9tudions le probl\u00e8me de d\u00e9terminer un mod\u00e8le probabiliste optimal (maximisant les chances de g\u00e9n\u00e9rer ce corpus). Nous montrons comment obtenir le mod\u00e8le probabiliste optimal, en l\u2019absence de contraintes. Nous \u00e9tudions aussi le probl\u00e8me en pr\u00e9sence de contraintes d\u2019int\u00e9grit\u00e9 (cl\u00e9s, inclusions et contraintes de domaine) et consid\u00e9rons dans ce cas deux types de g\u00e9n\u00e9rateurs : (i) des g\u00e9n\u00e9rateur bas\u00e9s sur des tests de continuation qui \u00e9vitent de g\u00e9n\u00e9rer des documents invalides au prix de tests co\u00fbteux ; et (ii) des g\u00e9n\u00e9rateurs qui produisent des documents jusqu\u2019\u00e0 en obtenir un qui soit valide au prix d\u2019un nombre potentiellement tr\u00e8s grand de productions de documents inutiles.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "4a6f98182e52aad53a6fcf204659626c22c6a6b5",
            "title": "Improved recommendations via (more) collaboration",
            "abstract": "We consider in this paper a popular class of recommender systems that are based on Collaborative Filtering (CF for short). CF is the process of predicting customer ratings to items based on previous ratings of (similar) users to (similar) items, and is typically used by a single organization, using its own customer ratings.\n We argue here that a multi-organization collaboration, even for organizations operating in different subject domains, can greatly improve the quality of the recommendations that the individual organizations provide to their users. To substantiate this claim, we present C2F (Collaborative CF), a recommender system that retains the simplicity and efficiency of classical CF, while allowing distinct organizations to collaborate and boost their recommendations. C2F employs CF in a distributed fashion that improves the quality of the generated recommendations, while minimizing the amount of data exchanged between the collaborating parties. Key ingredient of the solution are succinct signatures that can be computed locally for items (users) in a given organization and suffice for identifying similar items (users) in the collaborating organizations. We show that the use of such compact signatures not only reduces data exchange but also allows to speed up, by over 50%, the recommendations computation time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1950494",
                    "name": "Rubi Boim"
                },
                {
                    "authorId": "145325898",
                    "name": "Haim Kaplan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "145109224",
                    "name": "R. Rubinfeld"
                }
            ]
        },
        {
            "paperId": "592906fcd0e47c9de4f9ec273eee2254f9fb744b",
            "title": "BP-Ex: a uniform query engine for business process execution traces",
            "abstract": "Many enterprises nowadays use business processes, based on the BPEL standard, to achieve their goals. Analyzing the execution of such processes is critical for enforcing business policies and meeting efficiency and reliability goals.\n The BP-Ex system presented in this demo is an important component of BP-Suite, a novel tools suite based on the BPEL standard, which offers a uniform, query-based, user-friendly interface for BP analysis. BP-suite allows to gracefully combine the analysis of process specifications, monitoring of run time behavior, and posteriorly querying of execution traces (logs), for a comprehensive process management. BP-Ex is the BP-Suite query engine for process execution traces. The goal of this demo is to highlight the particular challenges that had to be addressed to support the suite's uniform, intuitive query interface, over (possibly very large) execution traces, and to demonstrate the novel optimization techniques that had to be developed for that.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2070408634",
                    "name": "Eran Balan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2536912",
                    "name": "Tal Sterenzy"
                }
            ]
        },
        {
            "paperId": "682bc464bb16e748ab6b9c509834224785f8a5e9",
            "title": "On models and query languages for probabilistic processes",
            "abstract": "Probabilistic processes appear naturally in various contexts, with applications to Business Processes, XML data management and more. Many models for specifying and querying such processes exist in the literature; a main goal of research in this area is to design models that are expressive enough to capture real-life processes and analysis tasks, but at the same time allow for efficient query evaluation. We depict the model established in [13, 16, 17, 18], and claim that it achieves a good balance between expressivity and query evaluation complexity. We compare and contrast the model with other common models for probabilistic processes, highlighting the different choices made in models design and their effect on expressivity and incurred complexity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "af0b30fdc335b476bf6ce91525d7558626c508e6",
            "title": "Provenance views for module privacy",
            "abstract": "Scientific workflow systems increasingly store provenance information about the module executions used to produce a data item, as well as the parameter settings and intermediate data items passed between module executions. However, authors/owners of workflows may wish to keep some of this information confidential. In particular, a module may be proprietary, and users should not be able to infer its behavior by seeing mappings between all data inputs and outputs. The problem we address in this paper is the following: Given a workflow, abstractly modeled by a relation R, a privacy requirement ? and costs associated with data. The owner of the workflow decides which data (attributes) to hide, and provides the user with a view R' which is the projection of R over attributes which have not been hidden. The goal is to minimize the cost of hidden data while guaranteeing that individual modules are ?-private. We call this the Secure-View problem. We formally define the problem, study its complexity, and offer algorithmic solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1715972",
                    "name": "Debmalya Panigrahi"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "b904f338e5d0659f95f957d1667198ae8ffdd756",
            "title": "On probabilistic fixpoint and Markov chain query languages",
            "abstract": "We study highly expressive query languages such as datalog, fixpoint, and while-languages on probabilistic databases. We generalize these languages such that computation steps (e.g. datalog rules) can fire probabilistically. We define two possible semantics for such query languages, namely inflationary semantics where the results of each computation step are added to the current database and noninflationary queries that induce a random walk in-between database instances. We then study the complexity of exact and approximate query evaluation under these semantics.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "145624228",
                    "name": "Christoph E. Koch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "bee8a1ea5e2283c79794526daab15e9b0190c0d1",
            "title": "Navigating in complex mashed-up applications",
            "abstract": "Mashups integrate a set of Web-services and data sources, often referred to as mashlets. We study in this paper a common scenario where these mashlets are components of larger Web-Applications. In this case, integration of mash-lets yields a set of inter-connected applications, referred to as Mashed-up Applications (abbr. MashAPP). While interactions between the mashlets enrich the individual applications, they also render navigation within them more intricate for the user, as actions in one application may affect others. To assist users in their navigation through MashAPPs we provide a solution based on a simple, generic model for MashAPPs and navigation flows within them. Queries over the model allow users to describe navigation flows of interest, and an effective query evaluation algorithm provides users with recommendations on how to navigate within the MashAPP. The model and algorithms serve as a basis for the COMPASS system, built on top of the Mashup Server.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "2201956",
                    "name": "Ohad Greenshpan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "ca5f66238233e3a663610d8547923e8a650c622b",
            "title": "Navigating through Mashed-up Applications with COMPASS",
            "abstract": "Mashups integrate a set of complementary Web-services and data sources, often referred to as mashlets. We consider here a common scenario where the integrated mashlets are part of larger Web-applications, and their integration yields a set of inter-connected applications. We refer to them as Mashed-up Applications (abbr. MashAPP). The inter-connections between the mashlets enrich the individual Web-applications, but at the same time make the user navigation within them more intricate as actions in one application may affect others. To address this difficulty, we present COMPASS, a system that assists users in their navigation through MashAPPs. The system employs a novel top-k algorithm to propose users the most effective navigation paths for their specified goals. The suggestions are continually adapted to choices taken by the users while navigating.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "2201956",
                    "name": "Ohad Greenshpan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "cf62470e7fb2ca3735f454251e5b24350726abb2",
            "title": "Optimal top-k query evaluation for weighted business processes",
            "abstract": "A Business Process (BP for short) consists of a set of activities that achieve some business goal when combined in a flow. Among all the (maybe infinitely many) possible execution flows of a BP, analysts are often interested in identifying flows that are \"most important\", according to some weight metric. This paper studies the following problem: given a specification of such a BP, a weighting function over BP execution flows, a query, and a number k, identify the k flows with the highest weight among those satisfying the query. We provide here, for the first time, a provably optimal algorithm for identifying the top-k weighted flows of a given BP, and use it for efficient top-k query evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1763100",
                    "name": "N. Polyzotis"
                },
                {
                    "authorId": "38868057",
                    "name": "T. Yam"
                }
            ]
        },
        {
            "paperId": "0c464953e5ce3eb19d2f0652c108d6c595c1e41f",
            "title": "Optimized union of non-disjoint distributed data sets",
            "abstract": "In a variety of applications, ranging from data integration to distributed query evaluation, there is a need to obtain sets of data items from several sources (peers) and compute their union. As these sets often contain common data items, avoiding the transmission of redundant information is essential for effective union computation. In this paper we define the notion of optimal union plans for non-disjoint data sets residing on distinct peers, and present efficient algorithms for computing and executing such optimal plans.\n Our algorithms avoid redundant data transmission and optimally exploit the network bandwidth capabilities. A challenge in the design of optimal plans is the lack of a complete map of the distribution of the data items among peers. We analyze the information required for optimal planning and propose novel techniques to obtain compact, cheap to communicate, description of the data sources. We then exploit it for efficient union computation with reasonable accuracy. We demonstrate experimentally the superiority of our approach over the common naive union computation, showing it improves the performance by an order of magnitude.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2850314",
                    "name": "I. Dar"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2172576",
                    "name": "Elad Verbin"
                }
            ]
        },
        {
            "paperId": "47da5bf6304375d0e6d3969e897e506101507a0d",
            "title": "Querying Future and Past in Business Processes",
            "abstract": "A business process (BP for short) consists of a group of business activities undertaken in pursuit of some particular goal. Analysis of BPs bears two main flavors, namely analysis of future and past executions. We intuitively explain these analysis goals and the models and algorithms employed to achieve them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "73c29a312c2d658086dc87b48254da6b662821a1",
            "title": "Autocompletion for Mashups",
            "abstract": "A mashup is a Web application that integrates data, computation and UI elements provided by several components into a single tool. The concept originated from the understanding that there is an increasing number of applications available on the Web and a growing need to combine them in order to meet user requirements. This paper presents MatchUp, a system that supports rapid, on-demand, intuitive development of mashups, based on a novel autocompletion mechanism. The key observation guiding the development of MatchUp is that mashups developed by different users typically share common characteristics; they use similar classes of mashup components and glue them together in a similar manner. MatchUp exploits these similarities to recommend useful completions (missing components and connections between them) for a user's partial mashup specification. The user is presented with a ranking of the recommendations from which she can choose and refine according to her needs. This paper presents the data model and ranking metric underlying our novel autocompletion mechanism. It introduces an efficient top-k ranking algorithm that is at the core of the MatchUp system and that is formally proved to be optimal in some natural sense. We also experimentally demonstrate the efficiency of our algorithm and the effectiveness of our proposal for rapid mashup construction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2201956",
                    "name": "Ohad Greenshpan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1763100",
                    "name": "N. Polyzotis"
                }
            ]
        },
        {
            "paperId": "94e75328f7731a6f36164f94717e6a1dc3bdbca9",
            "title": "MatchUp: Autocompletion for Mashups",
            "abstract": "A mashup is a Web application that integrates data, computation and GUI provided by several systems into a unique tool. The concept originated from the understanding that the number of applications available on the Web and the need for combining them to meet user requirements, are growing very rapidly. This demo presents MatchUp, a system that supports rapid, on-demand, intuitive development of mashups, based on a novel autocompletion mechanism. The key observation guiding the development of MatchUp is that mashups developed by different users typically share common characteristics; they use similar classes of mashup components and glue them together in a similar manner. MatchUp exploits these similarities to predict, given a user's partial mashup specification, what are the most likely potential completions (missing components and connection between them) for the specification. Using a novel ranking algorithm, users are then offered top-k completions from which they choose and refine according to their needs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "2201956",
                    "name": "Ohad Greenshpan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1763100",
                    "name": "N. Polyzotis"
                }
            ]
        },
        {
            "paperId": "9b46838d7ad90cdce046b4856aa07543d4a158f4",
            "title": "Querying DAG-shaped Execution Traces Through Views",
            "abstract": "The question whether a given set of views, deflned by queries, can be used to answer another query, arises in several contexts such as query optimization, data integration and semantic caching [24, 10, 12]. This paper studies a speciflc instance of this problem, where the queried data has the shape of a DAG (Directed Acyclic Graph) and the query language uses DAG patterns to retrieve portions of the data graph that are of interest. Our study is motivated by a particular application domain concerning the analysis of Web-based Business Processes (BPs for short). Such DAGs / DAG patterns are the standard way to model / query BP execution traces [3]. Previous research considered tree-shaped XML data and (general) graph-shaped Semi-Structured data. We show that the particular DAG shape of BP execution traces makes the problem easier than for general graphs, yet harder than for XML trees. Speciflcally, we show which combinations of DAG classes and query features allow for PTIME query answering algorithms and which lead to NP-complete problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1408021176",
                    "name": "M. Ben-Ari"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2172576",
                    "name": "Elad Verbin"
                }
            ]
        },
        {
            "paperId": "ef255fa3154e9fb7b172ce7b071f32b70dab2cbb",
            "title": "TOP-K projection queries for probabilistic business processes",
            "abstract": "A Business Process (BP) consists of some business activities undertaken by one or more organizations in pursuit of some business goal. Tools for querying and analyzing BP specifications are extremely valuable for companies. In particular, given a BP specification, identifying the top-k flows that are most likely to occur in practice, out of those satisfying a given query criteria, is crucial for various applications such as personalized advertizement and BP web-site design.\n This paper studies, for the first time, top-k query evaluation for queries with projection in this context. We analyze the complexity of the problem for different classes of distribution functions for the flows likelihood, and provide efficient (PTIME) algorithms whenever possible. Furthermore, we show an interesting application of our algorithms to the analysis of BP execution traces (logs), for recovering missing information about the run-time process behavior, that has not been recorded in the logs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "f9b3d33c42b2cafaed0c9deee980ad5f96697c01",
            "title": "Evaluating TOP-K Queries over Business Processes",
            "abstract": "A Business Process (BP) consists of some business activities undertaken by one or more organizations in pursuit of some business goal. Tools for querying and analyzing BP specifications are extremely valuable for companies as they allow to optimize the BP, identify potential problems, and reduce operational costs. In particular, given a BP specification, identifying the top-k execution flows that are most likely to occur in practice out of those satisfying the query criteria, is crucial for various applications. To address this need, we introduce in this paper the notion of {\\em likelihood} for BP execution flows, and study top-k query evaluation (finding the $k$ most likely matches) for queries over BP specifications. We analyze the complexity of query evaluation in this context and present novel algorithms for computing top-k query results. To our knowledge, this is the first paper that studies such top-k query evaluation for BP specifications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "ff47ce50ca83187179b2df71146a2821bb2b7334",
            "title": "Goal-Oriented Web-site Navigation for On-line Shoppers",
            "abstract": "Web-sites for on-line shopping typically offer a vast number of product options and combinations thereof. While this is very useful, it often makes the navigation in the site and the identification of the \"ideal\" purchase (where the notion of ideal differs among users) a confusing, non-trivial experience. This demonstration presents ShopIT (ShoppIng assitanT), a system that assists on-line shoppers by suggesting the most effective navigation paths for their specified criteria and preferences. The suggestions are continually adapted to choices/decisions taken by the users while navigating. ShopITis based on a set of novel, adaptive, provably optimal algorithms for TOP-K query evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "38868057",
                    "name": "T. Yam"
                }
            ]
        },
        {
            "paperId": "09e2115a53cd68f6cf07a2dc94d643e6f27a7d6f",
            "title": "BP-Mon: query-based monitoring of BPEL business processes",
            "abstract": "A Business Process (BP for short) consists of some business activities undertaken by one or more organizations in pursuit of some particular goal. It often interacts with other BPs of the same or other organizations and the software implementing it is rather complex. Two complementary instruments facilitate the design, development, and management of this complex software. The first is the use of standards. In particular, the recent BPEL standard (Business Process Execution Language [5]) provides an XML-based language to describe the operational logic and execution flow of the BP, as well as the interfaces it exposes to other BPs. A BP specification written in BPEL can be automatically compiled into an actual code that implements the BP, and can be executed on a BPEL server. The second instrument is the use of supporting BP management tools for (1) designing the BP BPEL specifications, (2) analyzing the design, (3) monitoring the BPs at run time, and (4) analyzing, posteriorly, the process execution traces (logs). Together they provide an essential infrastructure for companies to design business processes, optimize them, reduce operational costs, and ultimately increase competitiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "143658928",
                    "name": "Anat Eyal"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "3092109",
                    "name": "Alon Pilberg"
                }
            ]
        },
        {
            "paperId": "83b1004ce8b0bc4dfd8d593e97f20046ad310e2f",
            "title": "Type inference and type checking for queries on execution traces",
            "abstract": "This paper studies, for the first time, the management of type information for an important class of semi-structured data: nested DAGs (Directed Acyclic Graphs) that describe execution traces of business processes (BPs for short). Specifically, we consider here type inference and type checking for queries over BP execution traces. The queries that we consider select portions of the traces that are of interest to the user; the types describe the possible shape of the execution traces in the input/output of the query. \n \nWe formally define and characterize here three common classes of BP execution traces and their respective notions of type inference and type checking. We study the complexity of the two problems for query languages of varying expressive power and present efficient type inference/checking algorithms whenever possible. Our analysis offers a nearly complete picture of which combinations of trace classes and query features lead to PTIME algorithms and which to NP-complete or undecidable problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "af646376d9955c1e2101df0f915de62080110239",
            "title": "Querying and monitoring distributed business processes",
            "abstract": "A business process (BP for short) consists of a group of business activities undertaken by one or more organizations in pursuit of some particular goal. It usually operates in a cross-organization, distributed environment and the software implementing it is fairly complex. Standards facilitate the design, deployment, and execution of BPs. In particular, the recent BPEL standard (Business Process Execution Language), provides an XML-based language to describe the interface between the participants in a process, as well as the full operational logic of the process and its execution flow. BPEL specifications are automatically compiled into executable code that implements the described BP and runs on a BPEL application server. Processes execution is traced, and their run-time behavior can be recorded in standard XML formats.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                }
            ]
        },
        {
            "paperId": "b439194a5195be0d7a9b6f4c3108734b8af8d342",
            "title": "Enriching topic-based publish-subscribe systems with related content",
            "abstract": "This demonstration presents RMFinder (Related Messages Finder), a system that retains the simplicity and efficiency of topic-based P2P pub-sub, while providing a richer service where users can automatically receive all messages related to those in the topics to which they are subscribed. RMFinder is based on a novel, dynamic, distributed clustering algorithm, that takes advantage of similarities between topic messages to group topics together, into topic-clusters. The clusters adjust automatically to shifts in the focus of the messages published by the topics, as well as to changes in the users interest, and allow for an effective delivery of related messages with minimal overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1950494",
                    "name": "Rubi Boim"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "dc4117ebe8cdb331c6b59d6acf38fbc80725b0a6",
            "title": "Modeling the mashup space",
            "abstract": "We introduce a formal model for capturing the notion of mashup in its globality. The basic component in our model is the mashlet. A mashlet may query data sources, import other mashlets, use external Web services, and specify complex interaction patterns between its components. A mashlet state is modeled by a set of relations and its logic specified by datalog-style active rules. We are primarily concerned with changes in a mashlet state relations and rules. The interactions with users and other applications, as well as the consequent effects on the mashlets composition and behavior, are captured by streams of changes. The model facilitates dynamic mashlets composition, interaction and reuse, and captures the fundamental behavioral aspects of mashups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "2201956",
                    "name": "Ohad Greenshpan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "dd0e46c7381791aca26fc8d2eef16207e031242a",
            "title": "Querying web-based applications under models of uncertainty",
            "abstract": "Many businesses offer their services to customers via Web-based application interfaces. Reasoning about execution flows of such applications is extremely valuable for companies. Such reasoning must often operate under terms of uncertainty and partial information, due to partial tracing, effects of unknown external parameters, and more. The objectives of this research are (1) to define models for capturing Web application executions, with partial information and uncertainly of various flavors, (2) to design algorithms that allow for efficient reasoning over applications/execution traces under these models, and (3) to provide practical implementations that exploit these sound theoretical foundations for effective optimization of Web applications. We identify a restricted class of models that capture realistic scenarios, while allowing for an efficient query-based applications analysis. Hardness results indicate the necessity of such restricted models. We describe these results, highlight open problems, and consider directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "f2c870ec8b97014c0081ff1a1eb4aff447141b89",
            "title": "Querying Business Processes Under Models of Uncertainty.",
            "abstract": "A process for producing spherical graphite cast iron characterized by arranging a required amount of sphering alloy blocks in the hollow part for making a product of a casting mold and then pouring a melt into the casting mold; and a process for producing compound castings consisting of spherical graphite cast iron and ordinary cast iron characterized by arranging a required amount of sphering alloy blocks in the hollow part for making a product and requiring the strength of the product of a casting mold and pouring an ordinary cast iron melt into the casting mold.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "03ab16bd0863321284d17211c712e973e80062a3",
            "title": "Reasoning about XML update constraints",
            "abstract": "We introduce in this paper a class of constraints for describing howan XML document can evolve, namely XML update constraints. For these constraints, we study the implication problem, giving algorithms and complexity results for constraints of varying expressive power. Besides classical constraint implication, we also consider an instance-based approach. More precisely, we study implication with respect to a current tree instance, resulting from a series of unknown updates. The main motivation of our work is reasoning about data integrity under update restrictions in contexts where owners may lose control over their data, such as in publishing or exchange.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2772242",
                    "name": "Bogdan Cautis"
                },
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "47ed7d1e41a68fa0038606beb2ad846624ce16e9",
            "title": "Boosting topic-based publish-subscribe systems with dynamic clustering",
            "abstract": "We consider in this paper a class of Publish-Subscribe (pub-sub) systems called topic-based systems, where users subscribe to topics and are notified on events that belong to those subscribed topics. With the recent flourishing of RSS news syndication, these systems are regaining popularity and are raising new challenging problems.\n In most of the modern topics-based systems, the events in each topic are delivered to the subscribers via a supporting, distributed, data structure (typically a multicast tree). Since peers in the network may come and go frequently, this supporting structure must be continuously maintained so that \"holes\" do not disrupt the events delivery. The dissemination of events in each topic thus incurs two main costs: (1) the actual transmission cost for the topic events,and (2) the maintenance cost for its supporting structure. This maintenance overhead becomes particularly dominating when a pub-sub system supports a large number of topics with moderate event frequency; a typical scenario in nowadays news syndication scene.\n The goal of this paper is to devise a method for reducing this maintenance overhead to the minimum. Our aim is not to invent yet another topic-based pub-sub system, but rather to develop a generic technique for better utilization of existing platforms. Our solution is based on a novel distributed clustering algorithm that utilizes correlations between user subscriptions to dynamically group topics together, into virtual topics (called topic-clusters), andt hereby unifies their supporting structures and reduces costs. Our technique continuously adapts the topic-clusters and the user subscriptions to the system state, and incurs only very minimal overhead. We have implemented our solution in the Tamara pub-sub system. Our experimental study shows this approach to be extremely effective, improving the performance by an order of magnitude.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2845879",
                    "name": "Tal Zur"
                },
                {
                    "authorId": "2172576",
                    "name": "Elad Verbin"
                }
            ]
        },
        {
            "paperId": "ab136fc5d1e575f4e34c5cd81e9ce6a800f132c0",
            "title": "Monitoring Business Processes with Queries",
            "abstract": "Many enterprises nowadays use business processes, based on the BPEL standard, to achieve their goals. These are complex, often distributed, processes. Monitoring the execution of such processes for interesting patterns is critical for enforcing business policies and meeting efficiency and reliability goals. BP-Mon (Business Processes Monitoring) is a novel query language for monitoring business processes, that allows users to visually define monitoring tasks and associated reports, using a simple intuitive interface, similar to those used for designing BPEL processes. We describe here the BP-Mon language and its underlying formal model. We also present the language implementation and describe our novel optimization techniques. An important feature of the implementation is that BP-Mon queries are translated to BPEL processes that run on the same execution engine as the monitored processes. Our experiments indicate that this approach incurs very minimal overhead, hence is a practical and efficient approach to monitoring.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "143658928",
                    "name": "Anat Eyal"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "3092109",
                    "name": "Alon Pilberg"
                }
            ]
        },
        {
            "paperId": "d6a9733a4d2a015764b01c212e593e3c60e8ce25",
            "title": "On-Line Index Selection for Shifting Workloads",
            "abstract": "This paper introduces COLT (continuous on-line tuning), a novel framework that continuously monitors the workload of a database system and enriches the existing physical design with a set of effective indices. The key idea behind COLT is to gather performance statistics at different levels of detail and to carefully allocate profiling resources to the most promising candidate configurations. Moreover, COLT uses effective heuristics to self-regulate its own performance, lowering its overhead when the system is well tutted and being more aggressive when the workload shifts and it becomes necessary to re-tune the system. We describe an implementation of the proposed framework in the PostgreSQL database system and evaluate its performance experimentally. Our results validate the effectiveness of COLT and demonstrate its ability to modify the system configuration in response to changes in the query load.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2607487",
                    "name": "Karl Schnaitter"
                },
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1763100",
                    "name": "N. Polyzotis"
                }
            ]
        },
        {
            "paperId": "0234fefbfe64eaa8f560b1640df40de1bcd2c20b",
            "title": "Compact Labeling Scheme for Ancestor Queries",
            "abstract": "We consider the following problem. Given a rooted tree $T$, label the nodes of $T$ in the most compact way such that, given the labels of two nodes $u$ and $v$, one can determine in constant time, by looking only at the labels, whether $u$ is ancestor of $v$. The best known labeling scheme is rather straightforward and uses labels of length at most $2\\log_2 n$ bits each, where $n$ is the number of nodes in the tree. Our main result in this paper is a labeling scheme with maximum label length $\\log_2 n + \\Oh(\\sqrt{\\log n})$. Our motivation for studying this problem is enhancing the performance of web search engines. In the context of this application each indexed document is a tree, and the labels of all trees are maintained in main memory. Therefore even small improvements in the maximum label length are important.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "2071337",
                    "name": "Stephen Alstrup"
                },
                {
                    "authorId": "145325898",
                    "name": "Haim Kaplan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1870769",
                    "name": "Theis Rauhe"
                }
            ]
        },
        {
            "paperId": "1bc4089c516553b5363c7e2ae5a1615893b6547b",
            "title": "Digital signatures for modifiable collections",
            "abstract": "The common assumption about digital signatures is that they disallow any kind of modification on signed data. However, a more flexible approach is often needed and has been advocated lately, one in which some restricted modifications may still occur, without invalidating the data. This is made possible by offering signatures which are homomorphic with respect to some operation on the message domain. Starting from the signature(s) of some data instance(s), computed by the data owner, anybody else can derive the signature corresponding to a new data instance, if obtained only via some accepted operation from the previous one(s). More, updated signatures should be indistinguishable from the ones computed by the data owner and this updating step should be applicable as many times as needed. This paper deals with the signing of insert-only collections, in which element insertions are accepted but no removals should occur. Newly inserted elements do not have to be signed or known by the initial signer. We propose two techniques: one which transposes the insert-only problem into a delete-only one (which is already solved), and another technique based on zero-knowledge proofs. We also give performance measures and discuss applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "2772242",
                    "name": "Bogdan Cautis"
                },
                {
                    "authorId": "1742404",
                    "name": "A. Fiat"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "3fdcb63f3bac5d8a68bf93fc3272b8023c6c302c",
            "title": "Querying business processes",
            "abstract": "We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (Business Process Execution Language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers(peers).We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "143658928",
                    "name": "Anat Eyal"
                },
                {
                    "authorId": "2280824",
                    "name": "Simon Kamenkovich"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "79cc4b07f10f182d0a0ec27e98c52d1647a15f5f",
            "title": "COLT: continuous on-line tuning",
            "abstract": "The physical schema of a database plays a critical role in performance. Self-tuning is a cost-effective and elegant solution to optimize the physical configuration for the characteristics of the query load. Existing techniques operate in an off-line fashion, by choosing a fixed configuration that is tailored to a subset of the query load. The generated configurations therefore ignore any temporal patterns that may exist in the actual load submitted to the system.This demonstration introduces COLT (Continuous On-Line Tuning), a novel self-tuning framework that continuously monitors the incoming queries and adjusts the system configuration in order to maximize query performance. The key idea behind COLT is to gather performance statistics at different levels of detail and to carefully allocate profiling resources to the most promising candidate configurations. Moreover, COLT uses effective heuristics to regulate its own performance, lowering its overhead when the system is well-tuned, and being more aggressive when the workload shifts and it becomes necessary to re-tune the system. We present a specialization of COLT to the important problem of selecting an effective set of relational indices for the current query load. Our demonstration will use an implementation of our proposed framework in the PostgreSQL database system, showing the internal operation of COLT and the adaptive selection of indices as we vary the query load of the server.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2607487",
                    "name": "Karl Schnaitter"
                },
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1763100",
                    "name": "N. Polyzotis"
                }
            ]
        },
        {
            "paperId": "242b6661277d3e0d88fa26b44396b0abb739d2fd",
            "title": "Regular rewriting of active XML and unambiguity",
            "abstract": "We consider here the exchange of Active XML (AXML) data, i.e., XML documents where some of the data is given explicitly while other parts are given only intensionally as calls to Web services. Peers exchanging AXML data agree on a data exchange schema that specifies in particular which parts of the data are allowed to be intensional. Before sending a document, a peer may need to rewrite it to match the agreed data exchange schema, by calling some of the services and materializing their data. Previous works showed that the rewriting problem is undecidable in the general case and of high complexity in some restricted cases. We argue here that this difficulty is somewhat artificial. Indeed, we study what we believe to be a more adequate, from a practical view point, rewriting problem that is (1) in the spirit of standard 1-unambiguity constraints imposed on XML schema and (2) can be solved by a single pass over the document with a computational device not stronger than a finite state automation. Following previous works, we focus on the core of the problem, i.e., on the problem on words. The results may be extended to (A)XML trees in a straightforward manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1792867",
                    "name": "Omar Benjelloun"
                }
            ]
        },
        {
            "paperId": "2be322b5ebe3ea699e3be6aa80e8ba6d04949df8",
            "title": "Diagnosis of asynchronous discrete event systems: datalog to the rescue!",
            "abstract": "We consider query optimization techniques for data intensive P2P applications. We show how to adapt an old technique from deductive databases, namely Query-Sub-Query (QSQ), to a setting where autonomous and distributed peers share large volumes of interelated data.We illustrate the technique with an important telecommunication problem, the diagnosis of distributed telecom systems. We show that (i) the problem can be modeled using Datalog programs, and (ii) it can benefit from the large battery of optimization techniques developed for Datalog. In particular, we show that a simple generic use of the extension of QSQ achieves an optimization as good as that previously provided by dedicated diagnosis algorithms. Furthermore, we show that it allows solving efficiently a much larger class of system analysis problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "3050129",
                    "name": "Z. Abrams"
                },
                {
                    "authorId": "145549513",
                    "name": "S. Haar"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "53787afda56e017362fd2db9cce25d0645b6fc45",
            "title": "EDOS: Environment for the Development and Distribution of Open Source Software",
            "abstract": "The open-source software community is now comprised of a very large and growing number of contributors and users. The GNU/Linux operating system for instance has an estimated 18 million users worldwide and its contributing developers can be counted by thousands. The critical mass of contributors taking part in various opensource projects has helped to ensure high quality for open source software. However, despite the achievements of the open-source software industry, there are issues in the production of large scale open-source software (OSS) such as the GNU/Linux operating system that have to be addressed as the numbers of users, of contributors, and of available applications grow. EDOS is a European project supported by IST started October 2004 and ending in 2007, whose objective is to provide a new generation of methodologies, theoretical models, technical tools and quality models specifically tailored to OSS engineering and to software distribution over the Internet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "49392754",
                    "name": "X. Leroy"
                },
                {
                    "authorId": "2927100",
                    "name": "B. Vrdoljak"
                },
                {
                    "authorId": "1743244",
                    "name": "C. Bryce"
                },
                {
                    "authorId": "9381770",
                    "name": "R. D. Cosmo"
                },
                {
                    "authorId": "2995000",
                    "name": "K. Dittrich"
                },
                {
                    "authorId": "102330758",
                    "name": "St\u00e9phane Fermigier"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "50178459",
                    "name": "A. Sagi"
                },
                {
                    "authorId": "114458141",
                    "name": "Yotam Shtossel"
                },
                {
                    "authorId": "2578347",
                    "name": "St\u00e9phane Lauri\u00e8re"
                },
                {
                    "authorId": "115810989",
                    "name": "Fr\u00e9d\u00e9ric Lepied"
                },
                {
                    "authorId": "145961536",
                    "name": "Radu Pop"
                },
                {
                    "authorId": "46994801",
                    "name": "E. Pant\u00f2"
                },
                {
                    "authorId": "118008758",
                    "name": "Jean-Paul Smets"
                }
            ]
        },
        {
            "paperId": "8b8d59da21c475ab287f28a904dc639044134e33",
            "title": "Active XML and Data Activation",
            "abstract": "The field of distributed data management [13] has centered for many years around the relational model. More recently, the Web has made the world wide and intranet publication of data much simpler, by relying on HTML, Web browsers, plain-text search engines and query forms. However, the management of distributed information remained cumbersome. The situation is dramatically improving today with the introduction of XML [18] and Web services [21]. Together, these two standards provide an infrastructure for distributed computing at large, independent of any platform, system or programming language, i.e., the appropriate framework for distributed management of information. We discuss here Active XML (AXML, for short), a declarative framework that harnesses these emerging standards for the integration and management of distributed data. An AXML document is an XML document where some of the data is given explicitly, while other portions are given only intensionally by means of embedded calls to Web services. By calling the services, one can obtain up-to-date information. In particular, AXML provides control over the activation of service calls both from the client side (pull) or from the server side (push). It should be noted that the idea of mixing data and code is not new, e.g., stored procedures in relational systems [15], method calls in object-oriented databases [10], and queries in scripting languages such as PHP. The novelty is that since both XML and Web services are standards, AXML documents can be universally understood, and therefore can be universally exchanged. In the present paper, we focus on the idea of \u201cactivating\u201d portions of static data, e.g., transforming an XML document into an Active XML document. As we will see, a wide range of XML sub-documents are candidates for being activated. Furthermore, one may want to activate data for a number of different motivations. In some cases, it may be to introduce new functionalities, e.g., to provide monitoring over the data; in other cases, it may be for performance reasons, e.g., to lower the cost of refresh. Activation may be performed either manually (by the application designer) or automatically (e.g., by a Web server).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "08ef11b5702cad8cc7de079b48c211ac3f6d8234",
            "title": "Database and XML Technologies: Second International XML Database Symposium, XSym 2004, Toronto, Canada, August 29-30, 2004, Proceedings",
            "abstract": "Keynote Speech.- Building an Extensible XQuery Engine: Experiences with Galax (Extended Abstract).- XQuery Processing.- A Light but Formal Introduction to XQuery.- XML Query Processing Using a Schema-Based Numbering Scheme.- Implementing Memoization in a Streaming XQuery Processor.- Searching, Ranking, and Mapping XML Documents.- XQuery Processing with Relevance Ranking.- Information Preservation in XML-to-Relational Mappings.- A Signature-Based Approach for Efficient Relationship Search on XML Data Collections.- XML Constraints Checking and Correcting.- Correctors for XML Data.- Incremental Constraint Checking for XML Documents.- EReX: A Conceptual Model for XML.- XML Processing.- A Runtime System for XML Transformations in Java.- Teaching Relational Optimizers About XML Processing.- Adjustable Transaction Isolation in XML Database Management Systems.- Clustering, Indexing, Statistics.- Fractional XSketch Synopses for XML Databases.- Flexible Workload-Aware Clustering of XML Documents.- XIST: An XML Index Selection Tool.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1773883",
                    "name": "Zohra Bellahsene"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1734650",
                    "name": "M. Rys"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "1730127",
                    "name": "R. Unland"
                }
            ]
        },
        {
            "paperId": "5399bf23a5a79ed0555c23a9bdf3d90c7d9e1650",
            "title": "Positive active XML",
            "abstract": "The increasing popularity of XML and Web services have given rise to a new generation of documents, called Active XML documents (AXML), where some of the data is given explicitly while other parts are given intensionally, by means of embedded calls to Web services. Web services in this context can exchange intensional information, using AXML documents as parameters and results.The goal of this paper is to provide a formal foundation for this new generation of AXML documents and services, and to study fundamental issues they raise. We focus on Web services that are (1) monotone and (2) defined declaratively as conjunctive queries over AXML documents. We study the semantics of documents and queries, the confluence of computations, termination and lazy query evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1792867",
                    "name": "Omar Benjelloun"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "9d57c0cf2db6e2b76beba1e3825477b5de146de9",
            "title": "Lazy query evaluation for Active XML",
            "abstract": "In this paper, we study query evaluation on Active XML documents (AXML for short), a new generation of XML documents that has recently gained popularity. AXML documents are XML documents whose content is given partly extensionally, by explicit data elements, and partly intensionally, by embedded calls to Web services, which can be invoked to generate data.A major challenge in the efficient evaluation of queries over such documents is to detect which calls may bring data that is relevant for the query execution, and to avoid the materialization of irrelevant information. The problem is intricate, as service calls may be embedded anywhere in the document, and service invocations possibly return data containing calls to new services. Hence, the detection of relevant calls becomes a continuous process. Also, a good analysis must take the service signatures into consideration.We formalize the problem, and provide algorithms to solve it. We also present an implementation that is compliant with XML and Web services standards, and is used as part of the ActiveXML system. Finally, we experimentally measure the performance gains obtained by a careful filtering of the service calls to be triggered.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1792867",
                    "name": "Omar Benjelloun"
                },
                {
                    "authorId": "2772242",
                    "name": "Bogdan Cautis"
                },
                {
                    "authorId": "1739309",
                    "name": "I. Manolescu"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "35266437",
                    "name": "N. Preda"
                }
            ]
        },
        {
            "paperId": "c28aa3b1b744c2c8eefc8d22762561c092cc3a34",
            "title": "Active XML, Security and Access Control",
            "abstract": "XML and Web services are revolutioning the automatic management of distributed information, somewhat in the same way that HTML, Web browsers and search engines modified human access to world wide information. We argue in this paper that the combination of XML and Web services allows for a novel distributed data management paradigm, where the exchanged information mixes materialized and intensional, active, information. We illustrate the flexibility of this approach by presenting Active XML, a language that is based on embedding Web service calls in XML data. We focus on two particular issues, namely security and access control.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1792867",
                    "name": "Omar Benjelloun"
                },
                {
                    "authorId": "2772242",
                    "name": "Bogdan Cautis"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "2532366b10b9af28e29d6c440c95cc7f2d2ad89d",
            "title": "Dynamic XML documents with distribution and replication",
            "abstract": "The advent of XML as a universal exchange format, and of Web services as a basis for distributed computing, has fostered the apparition of a new class of documents: dynamic XML documents. These are XML documents where some data is given explicitly while other parts are given only intensionally by means of embedded calls to web services that can be called to generate the required information. By the sole presence of Web services, dynamic documents already include inherently some form of distributed computation. A higher level of distribution that also allows (fragments of) dynamic documents to be distributed and/or replicated over several sites is highly desirable in today's Web architecture, and in fact is also relevant for regular (non dynamic) documents.The goal of this paper is to study new issues raised by the distribution and replication of dynamic XML data. Our study has originated in the context of the Active XML system [1, 3, 22] but the results are applicable to many other systems supporting dynamic XML data. Starting from a data model and a query language, we describe a complete framework for distributed and replicated dynamic XML documents. We provide a comprehensive cost model for query evaluation and show how it applies to user queries and service calls. Finally, we describe an algorithm that, for a given peer, chooses data and services that the peer should replicate to improve the efficiency of maintaining and querying its dynamic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "2355686",
                    "name": "G. Cobena"
                },
                {
                    "authorId": "1739309",
                    "name": "I. Manolescu"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "8402e992a0a754a211c8964733c05eb8e85f99ed",
            "title": "Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems",
            "abstract": "This volume contains the proceedings of the Twenty-second ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2003), held at San Diego, California on June 9-11, 2003 in conjunction with the 2003 ACM SIGMOD International Conference on Management of Data and as part of the Federated Computing Research Conference FCRC 2003. It consists of a paper based on the invited talk by Rick Hull, a paper based on the invited tutorial by Rakesh Agrawal, and 27 contributed papers that were selected by the program committee for presentation at the symposium.The contributed papers were selected from 136 submissions. Most of the papers are \"extended abstracts\" and are preliminary reports on work in progress. While they have been read by program committee members, they have not been formally refereed. It is expected that much of the research described in these papers will be published in detail in computer science journals.The program committee selected \\An Information-Theoretic Approach to Normal Forms for Relational and XML Data\" by Marcelo Arenas and Leonid Libkin for the PODS 2003 Best Paper Award and \"Algorithms for Data Migration with Cloning\" by Samir Khuller, Yoo-Ah Kim, Yung-Chun (Justin) Wan for the PODS 2003 Best Newcomer Award. Warmest congratulations to the authors of these papers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275620446",
                    "name": "Frank Neven"
                },
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "d2c5fdc7c8aba6ecfbbeb20249ca6dd33957067e",
            "title": "Exchanging intensional XML data",
            "abstract": "XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice of whether or not to materialize the intensional data (i.e., to invoke the embedded calls) before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This article addresses the problem of guiding this materialization process.We argue that---like for regular XML data---schemas (\u00e0 la DTD and XML Schema) can be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real-life standards for XML data, schemas, and Web services, and is used in the Active XML system. We illustrate the usefulness of this approach through a real-life application for peer-to-peer news exchange.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1747227",
                    "name": "B. Amann"
                },
                {
                    "authorId": "1792867",
                    "name": "Omar Benjelloun"
                },
                {
                    "authorId": "3129193",
                    "name": "Frederic Dang Ngoc"
                }
            ]
        },
        {
            "paperId": "f5fe2cb61d562acc0623a228cfe56766bed3951f",
            "title": "Exchanging intensional XML data",
            "abstract": "XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice to materialize the intensional data (i.e. to invoke the embedded calls) or not, before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This paper addresses the problem of guiding this materialization process.We argue that, just like for regular XML data, schemas (ala DTD and XML Schema) may be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real life standards for XML data, schemas, and Web services, and is used in the Active XML system [3, 1].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1747227",
                    "name": "B. Amann"
                },
                {
                    "authorId": "1792867",
                    "name": "Omar Benjelloun"
                },
                {
                    "authorId": "3129193",
                    "name": "Frederic Dang Ngoc"
                }
            ]
        },
        {
            "paperId": "1b307fd9c566e5d20d1dbe8a625ba3327bcdacda",
            "title": "A comparison of labeling schemes for ancestor queries",
            "abstract": "Motivated by a recent application in XML search engines we study the problem of labeling the nodes of a tree (XML file) such that given the labels of two nodes one can determine whether one node is an ancestor of the other. We describe several new prefix-based labeling schemes, where an ancestor query roughly amounts to testing whether one label is a prefix of the other. We compare our new schemes to a simple interval-based scheme currently used by search engines, as well as, to schemes with the best theoretical guarantee on the maximum label length. We performed our experimental evaluation on real XML data and on some families of random trees.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145325898",
                    "name": "Haim Kaplan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "33829265",
                    "name": "Ronen Shabo"
                }
            ]
        },
        {
            "paperId": "c1f35d7fe10a55c8418ad600a9272ae21e862070",
            "title": "Web services and data integration",
            "abstract": "The developments of XML [4] and Web services [3] arechanging radically the art of data integration. We brieflydescribe Web services and consider some of their impacton data integration. We argue that XML and Web servicesprovide the proper infrastructure for data integration at theWeb scale. This is illustrated by some work going on atINRIA on Active XML, that is XML extended by allowingthe embedding of calls to Web services.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1792867",
                    "name": "Omar Benjelloun"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "d4592ef8e4ff24cde2ffb430413836f2ec46f1fe",
            "title": "Labeling dynamic XML trees",
            "abstract": "We present algorithms to label the nodes of an XML tree which is subject to insertions and deletions of nodes. The labeling is done such that (1) we label each node immediately when it is inserted and this label remains unchanged, and (2) from a pair of labels alone, we can decide whether one node is an ancestor of the other. This problem arises in the context of XML databases that support queries on the structure of the documents as well us on the changes made to the documents over time. We prove that our algorithms assign the shortest possible labels (up to a constant factor) which satisfy these requirements.We also consider the same problem when \"clues\" that provide guarantees on possible future insertions are given together with newly inserted nodes. Such clues can be derived from the DTD or from statistics on similar XML trees. We present algorithms that use the clues to assign shorter labels. We also prove that the length of our labels is close to the minimum possible.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287512689",
                    "name": "Edith Cohen"
                },
                {
                    "authorId": "145325898",
                    "name": "Haim Kaplan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "817851db639804f34484d1060f65790251886225",
            "title": "XML with data values: typechecking revisited",
            "abstract": "We investigate the type checking problem for XML queries: statically verifying that every answer to a query conforms to a given output DTD, for inputs satisfying a given input DTD. This problem had been studied by a subset of the authors in a simplified framework that captured the structure of XML documents but ignored data values. We revisit here the type checking problem in the more realistic case when data values are present in documents and tested by queries. In this extended framework, type checking quickly becomes undecidable. However, it remains decidable for large classes of queries and DTDs of practical interest. The main contribution of the present paper is to trace a fairly tight boundary of decidability for type checking with data values. The complexity of type checking in the decidable cases is also considered.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1734327",
                    "name": "N. Alon"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1786248",
                    "name": "F. Neven"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "1765187",
                    "name": "V. Vianu"
                }
            ]
        },
        {
            "paperId": "89c9807e028de21f57ae3b2e5f3bc6d53dd21479",
            "title": "Compact labeling schemes for ancestor queries",
            "abstract": "We consider the following problem. Give a rooted tree T, label the nodes of T in the most compact way such that given the labels of two nodes one can determine in constant time, by looking only at the labels, if one node is an ancestor of the other. The best known labeling scheme is rather straightforward and uses labels of size at most 2 log n, where n is the number of vertices In the tree. Our main result in this paper is a labeling scheme with maximum label size close to 3/2 log n.\nOur motivation for studying this problem is enhancing the performance of Web search engines. In the context of this application each indexed document is a tree and the labels of all trees are maintained in main memory. Therefore even small improvements in the maximum label size are important.\nThere are no lower bounds known for this problem except for an obvious lower bound of log n that follows from the fact that different vertices must have different labels. The question whether one can find even shorter labels remains an intriguing open question.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "145325898",
                    "name": "Haim Kaplan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "d974dca23e30ddea70bac2f6fec85f00f9129c97",
            "title": "Typechecking XML views of relational databases",
            "abstract": "Motivated by the need to export relational databases as XML data in the context of the World Wide Web, we investigate the type-checking problem for transformations of relational data into tree data (i.e. XML). The problem consists of statically verifying that the output of every transformation belongs to a given output tree language (specified for XML by a document type definition), for input databases satisfying given integrity constraints. The type-checking problem is parameterized by the class of formulas defining the transformation, the class of output tree languages and the class of integrity constraints. While undecidable in its most general formulation, the type-checking problem has many special cases of practical interest that turn out to be decidable. The main contribution of this paper is to trace a fairly tight boundary of decidability for type-checking in this framework. In the decidable cases, we examine the complexity and show lower and upper bounds. We also exhibit a practically appealing restriction for which type-checking is in PTIME.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1734327",
                    "name": "N. Alon"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1786248",
                    "name": "F. Neven"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "1765187",
                    "name": "V. Vianu"
                }
            ]
        },
        {
            "paperId": "8c9879ef6bfd14afe5b05b5e943034646e75ae99",
            "title": "Typechecking for XML transformers",
            "abstract": "We study the typechecking problem for XML transformers: given an XML transformation program and a DTD for the input XML documents, check whether every result of the program conforms to a specified output DTD. We model XML transformers using a novel device called a k-pebble transducer, that can express most queries without data-value joins in XML-QL, XSLT, and other XML query languages. Types are modeled by regular tree languages, a nobust extension of DTDs. The main result of the paper is that typechecking for k-pebble transducers is decidable. Consequently, typechecking can be performed for a broad range of XML transformation languages, including XML-QL and a fragment of XSLT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                },
                {
                    "authorId": "1765187",
                    "name": "V. Vianu"
                }
            ]
        },
        {
            "paperId": "20dd934a841f775ed18e0d2f8e887f363cbb9099",
            "title": "Querying network directories",
            "abstract": "Heirarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal profiles, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way for superior to what conventional relational or object-oriented databases offer. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated \u201cqueries\u201d involve navigational access.\nIn this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1778524",
                    "name": "D. Vista"
                }
            ]
        },
        {
            "paperId": "5259eeb5ab2a65f178a3dfa7cc533f45d0c0485c",
            "title": "Type inference for queries on semistructured data",
            "abstract": "We study the problem of type checking and type inference for queries over semistructured data. Introducing a novel traces technique, we show that the problem is difficult in general (NP-complete), but can be solved in PTIME for many practical cases, including, in particular, queries over XML data. Besides being interesting by itself, we show that type inference and the related traces technique have several important applications, facilitating query formulation, optimization, and verification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                }
            ]
        },
        {
            "paperId": "5db10738e5f9a69c4014acf224fddeba404a4e50",
            "title": "Active Views for Electronic Commerce",
            "abstract": "Electronic commerce is emerging as a major Web-supported application. In\nthis paper we argue that database technology can, and should, provide the back-bone\nfor a wide range of such applications. More precisely, we present here the\nActiveViews system, which, relaying on an extensive use of database features in-cluding\nviews, active rules (triggers), and enhanced mechanisms for notification,\naccess control and logging/tracing of users activities, provides the needed basis for\nelectronic commerce.\nBased on the emerging XML standards (DOM, query languages for XML,\netc.), the system offers a novel declarative view specification language, describing\nthe relevant data and activities of all actors (e.g. vendors and clients) participat-ing\nin electronic commerce activities . Then, acting as an application generator,\nthe system generates an actual, possibly customized, Web application that allows\nusers to perform the given set of controlled activities and to work interactively on\nthe specified data in a standard distributed environment.\nAlthough closely related to workflow management systems, a major difference\nhere is the importance we give to data. While workflow systems give declarative\nmeans for specifying the operations flow, the data involved is typically described\nin a very abstract manner, often disconnected from the description of the flow\nitself. In contrast, our approach promotes the data, acknowledging its importance\nfor optimization coherence and analysis of applications.\nThe ActiveView system is developed at INRIA on top of Ardent s XML repository\nand Java.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1747227",
                    "name": "B. Amann"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "143658928",
                    "name": "Anat Eyal"
                },
                {
                    "authorId": "2727167",
                    "name": "L. Mignet"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "84368db88954915e2903fdcd796a5f62d2dc4da6",
            "title": "Tools for Data Translation and Integration",
            "abstract": "A broad spectrum of data is available on the Web in distinct heterogeneous sources, stored under different formats. As the number of systems that utilize this data grows, the importance of data conversion mechanisms increases greatly. We present here an overview of a French-Israeli research project aimed at developing tools to simplify the intricate task of data translation. The solution is based on a middleware data model to which various data sources are mapped, and a declarative language for specifying translations within the middleware model. A complementary schema-based mechanism is used to automate some of the translation. Some particular aspects of the solution are detailed in [3, 7, 10].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "49269381",
                    "name": "P. Mogilevsky"
                },
                {
                    "authorId": "1753075",
                    "name": "J\u00e9r\u00f4me Sim\u00e9on"
                },
                {
                    "authorId": "10272731",
                    "name": "Sagit Zohar"
                }
            ]
        },
        {
            "paperId": "b28e256d605be3ff423eac69963b55633d40edf9",
            "title": "An Algebra for Pomsets",
            "abstract": "We study languages for manipulating partially ordered structures with duplicates (e.g. trees, lists). As a general framework, we consider the pomset (partially ordered multiset) datatype. We introduce an algebra for pomsets, which generalizes traditional algebras for (nested) sets, bags and lists. This paper is motivated by the study of the impact of different language primitives on the expressive power. We show that the use of partially ordered types increases the expressive power significantly. Surprisingly, it turns out that the algebra when restricted to both unordered (bags) and totally ordered (lists) intermediate types, yields the same expressive power as fixpoint logic with counting on relational databases. It therefore constitutes a rather robust class of relational queries. On the other hand, we obtain a characterization of PTIME queries on lists by considering only totally ordered types.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257284605",
                    "name": "St\u00e9phane Grumbach"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "d23f6b54dc453832ec69d9d0a1307b96df9a86d0",
            "title": "XML Repository and Active Views Demonstration",
            "abstract": "The goal of this demonstration is to present the main features of (i) Axielle, an XML repository developed by Ardent Software [3] on top of the O2 object-oriented DBMS and (ii) the ActiveView system which has been built by the Verso project at INRIA [1] on top of Axielle. The demonstration is based on a simple electronic commerce application which will be described in Section 2. Electronic commerce is emerging as a major Web-supported application. It involves handling and exchange of data (e.g. product catalogs, yellow pages, etc.) and must provide (i) database functionalities (query language, transactions, concurrency control, distribution and recovery) for the efficient management of large data volumes and hundreds of users as well as (ii) standard data storage and exchange formats (e.g. XML, SGML) for the easy integration of existing software and data. The ActiveView system combined with the Axielle XML repository enables a fast deployment of electronic commerce applications based on a new high-level declarative specification language (AVL), advanced database technology (object-oriented data model, XML query language, notifications), Web standards (HTTP, HTML) and other Internet compliant",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "2094231873",
                    "name": "V. Aguilera"
                },
                {
                    "authorId": "3249888",
                    "name": "S. Ailleret"
                },
                {
                    "authorId": "1747227",
                    "name": "B. Amann"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "47063736",
                    "name": "B. Hills"
                },
                {
                    "authorId": "2446297",
                    "name": "Fr\u00e9d\u00e9ric Hubert"
                },
                {
                    "authorId": "33614954",
                    "name": "J. Mamou"
                },
                {
                    "authorId": "144173226",
                    "name": "A. Marian"
                },
                {
                    "authorId": "2727167",
                    "name": "L. Mignet"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "35240742",
                    "name": "C. Santos"
                },
                {
                    "authorId": "2080598640",
                    "name": "B. Tessier"
                },
                {
                    "authorId": "1685821",
                    "name": "A. Vercoustre"
                }
            ]
        },
        {
            "paperId": "3124576c847070eba73d819cee90c7329cf3a909",
            "title": "Algebras for Querying Text Regions: Expressive Power and Optimization",
            "abstract": "There is a significant amount of interest in combining and extending database and information retrieval technologies to manage textual data. The challenge is becoming more relevant due to increased availability of documents in digital form. Document data has a natural hierarchical structure, which may be made explicit due to the use of markup conventions (as with SGML). An important aspect of managing structured and semistructured textual data consists of supporting the efficient retrieval of text components based both on their content and on their structure. In this paper we study issues related to the expressive power and optimization of a class of algebras that support combining string (or pattern) searches with queries on the hierarchical structure of the text. Theregion algebrastudied is a set-at-a-time algebra for manipulatingtext regions(substrings of the text) that supports finding out nesting and ordering properties of the text regions. This algebra is part of the language in use in commercial text retrieval systems and can form the basis for supporting SQL-like access to textual data. By presenting a close relationship between the region algebra and the monadic first order theory of finite binary trees, we show that queries in the algebra can be optimized, in the sense that equivalence to less expensive expressions can be tested. This optimization can be difficult (co-NP-hard in the general case), but there is an important class of queries that can be optimized in polynomial time. On the negative side, we show that the language is incapable of capturing some important properties of the text structure, related to the nesting and ordering of text regions. We conclude by suggesting possible extensions to increase the expressive power of the language and consider one such example.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1735847",
                    "name": "M. Consens"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "e1a647721a518c6f4f3ff8bc9122294432745361",
            "title": "Using Schema Matching to Simplify Heterogeneous Data Translation",
            "abstract": "A broad spectrum of data is available on the Web in distinct heterogeneous sources, and stored under different formats. As the number of systems that utilize this heterogeneous data grows, the importance of data translation and conversion mechanisms increases greatly. In this paper we present a new translation system, based on schema-matching, aimed at simplifying the intricate task of data conversion. We observe that in many cases the schema of the data in the source system is very similar to that of the target system. In such cases, much of the translation work can be done automatically, based on the schemas similarity. This saves a lot of effort for the user, limiting the amount of programming needed. We define common schema and data models, in which schemas and data (resp.) from many common models can be represented. Using a rule-based method, the source schema is compared with the target one, and each component in the source schema is matched with a corresponding component in the target schema. Then, based on the matching achieved, data instances of the source schema can be translated to instances of the target schema. We show that our schema-based translation system allows a convenient specification and customization of data conversions, and can be easily combined with the traditional data-based translation languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "10272731",
                    "name": "Sagit Zohar"
                }
            ]
        },
        {
            "paperId": "47aefcaf646671709ca94f9e3cf3892a52e681ab",
            "title": "Comparison of Functional and Predicative Query Paradigms",
            "abstract": "In the last decade many extensions of the relational model were proposed, and basic properties of the relational model were investigated in their contexts. In particular, the equivalence of calculus and algebra, and the relative expressive power of other related languages were explored. This paper investigates this subject in a general framework, independently of any specific data type constructors that may exist in specific models, with the goal of making explicit the conditions that enable translation between query languages. The framework is based on a combination the well-founded approach of deductive programs and the initial algebra approach of algebraic specifications. The latter does not support negation (i.e., disequations); hence our combination contributes to the theory of algebraic specifications. Given the framework, we present the predicative and functional approaches to database description and querying. The first leads to the calculus and deductive approaches, the second to several algebras and, also, to generalizations that allow restricted definition by equations. We extend the notions of domain independence to our framework. We then present various sufficient conditions for the calculus and (some) algebra to be equivalent. We also compare the expressive power of algebras and more general languages to several deductive languages, under stratified and well-founded semantics. Finally, we define safety conditions and prove similar results for safe versions of the languages.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "5c7854d035e61729b94f45fdfe117ae8447f8ee0",
            "title": "Formal models of Web queries",
            "abstract": "Abstract We present a new formal model of query and computation on the Web. We focus on two important aspects that distinguish the access to Web data from the access to a standard database system: the navigational nature of the access and the lack of concurrency control. We show that these two issues have significant effects on the computability of queries. To illustrate the ideas and how they can be used in practice for designing appropriate Web query languages, we consider a particular query language, the Web calculus, an abstraction and extension of the practical Web query language WebSQL.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1682406",
                    "name": "A. Mendelzon"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "f754d77a5c49b5bed0110140ff41ffa794665b53",
            "title": "A signature technique for similarity-based queries",
            "abstract": "Jagadish et al. (see Proc. ACM SIGACT-SIGMOD-SIGART PODS, p.36-45, 1995) developed a general framework for posing queries based on similarity. The framework enables a formal definition of the notion of similarity for an application domain of choice, and then its use in queries to perform similarity-based search. We adapt this framework to the specialized domain of real-valued sequences. (Although some of the ideas we present are applicable to other types of data as well). In particular we focus on whole-match queries. By whole-match query we mean the case where the user has to specify the whole sequence. Similarity-based search can be computationally very expensive. The computation cost depends heavily on the length of sequences being compared. To make such similarity testing feasible on large data sets, we propose the use of a signature based technique. In a nutshell, our approach is to \"shrink\" the data sequences into signatures, and search the signatures instead of the real sequences, with further comparison being required only when a possible match is indicated. Being shorter, signatures can usually be compared much faster than the original sequences. In addition, signatures are usually easier to index. For such a signature-based technique to be effective one has to assure that (1) the signature comparison is fast, and (2) the signature comparison gives few false alarms, and no false dismissals. We obtain measures of goodness for our technique. The technique is illustrated with a couple of very different examples.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1682406",
                    "name": "A. Mendelzon"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "fe81942e6a196f0515aed5181a3f105b1c2de943",
            "title": "Displaying data in multidimensional relevance space with 2D visualization maps",
            "abstract": "The paper introduces a tool for visualizing a multidimensional relevance space. Abstractly, the information to be displayed consists of a large number of objects, a set of features that are likely to be of interest to the user, and some function that measures the relevance level of every object to the various features. The goal is to provide the user with a concise and comprehensible visualization of that information. For the type of applications concentrated on, the exact relevance measures of the objects are not significant. This enables accuracy to be traded for a clearer display. The idea is to \"flatten\" the multidimensionality of the feature space into a 2D \"relevance map\", capturing the inter-relations among the features, without causing too many ambiguous interpretations of the results. To better reflect the nature of the data and to resolve the ambiguity the authors refine the given set of features and introduce the notion of composed features. The layout of the map is then obtained by grading it according to a set of rules and using a simulated annealing algorithm which optimizes the layout with respect to these rules. The technique proposed has been implemented and tested, in the context of visualizing the result of a Web search, in the RMAP (Relevance Map) prototype system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2076050620",
                    "name": "Jackie Assa"
                },
                {
                    "authorId": "1388323541",
                    "name": "D. Cohen-Or"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "4a801f7015a7ecd8b0495c7c33916b073ca20456",
            "title": "On genericity and parametricity (extended abstract)",
            "abstract": "Genericity is a property of queries expressed in declarative query languages. The idea, first expressed in [2, 7], is that data values are uninterpreted, hence a query should be invariant under isomorphisms (i.e. element renaming) of dat abases. This has been generalized, following [10], to account for preservation of constants. This concept of genericity is intimately related to classical relational database research, that considered databases as constructed over an abstract domain of uninterpreted elements. However, the scope of database models has vastly expanded in the last decade. Multiple domains, use of domain functions and predicates, user-defined types, and bulk types \u2014 these are essential components of recent data models. Genericity is central to many results in the theory of databases, yet little attention has been paid to it in the development of the new models. This paper tries to amend this situation. We show in the paper that there is a rich structure of genericit y concepts, and argue that genericity provides insight about the relationships between the properties of data structures used in a data",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1403678547",
                    "name": "Paula Ta-Shma"
                }
            ]
        },
        {
            "paperId": "eb359c2c1c89d4e91e60c27b328a720fb8e942f7",
            "title": "Query languages for bags: expressive power and complexity",
            "abstract": "One of the characteristics of advanced database systems is that they must handle data which no longer fits the traditional relational style. The present column summarizes recent results on one such extension, where databases handle bags (multisets) rather than just sets. The results illustrate an important research direction in Database Theory. Abstract Most database theory focused on investigating databases containing sets of tuples. In practice databases often implement relations using bags, i.e. sets with duplicates. In this paper we study how database query languages are affected by the use of duplicates. We consider query l~nguages that are simple extensions of the (nested) relational algebra~ and investigate their resulting expressive power and complexity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257284605",
                    "name": "St\u00e9phane Grumbach"
                },
                {
                    "authorId": "2248406361",
                    "name": "Leonid Libkin"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "2248413946",
                    "name": "Limsoon Wong"
                }
            ]
        },
        {
            "paperId": "75550ac94bb5e6afab26f215866aa6e6a63fb803",
            "title": "Subtyping in OODBs",
            "abstract": "One of the central concepts supported by object-oriented databases is isa relationship. Its intuitive simplicity is deceptive. In reality, this term denotes several concepts, such as subtyping, subset relationships, and inheritance of structure and/or behavior. Each of these is non-trivial, and their interaction may be quite subtle. This paper deals with subtyping and its properties, in the context of a model that allows arbitrary data types and type constructors (but no function constructor). For simplicity a model based on the algebraic specification approach is used, which partially explains the inability to deal with the function constructor. Two intuitive ideas about subtyping are generalized and investigated: first, that the set of elements associated with a subtype is a subset of the set associated with supertype, and second, that each element of the subtype may be used in any place where an element of the supertype is expected. A generalized subtyping relation among abstract data types is defined, and its properties are investigated. In particular, it is shown that often there exist many possible subtyping relations among two types and that testing for some desirable properties, such as correctness, is in general undecidable. The notion of natural subtyping relation (that is related to parametric abstract data types) is introduced and shown to have a simple correctness proof. The paper concludes with a generalized type checking algorithm.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "aa6a1ab3516d100ebca527454d18059c6e77492c",
            "title": "A database interface for file update",
            "abstract": "Database systems are concerned with structured data. Unfortunately,data is still often available in an unstructured manner (e.g., infiles) even when it does have a strong internal structure (e.g.,electronic documents or programs). In a previous paper [2], wefocussed on the use of high-level query languages to access suchfiles and developed optimization techniques to do so. In thispaper, we consider how structured data stored in files can beupdated using database update languages.The interest of using database languages to manipulate files istwofold. First, it opens database systems toexternal data. This concerns data residing infiles or data transiting on communication channels and possiblycoming from other databases [2]. Secondly, it provides high levelquery/update facilities to systems that usually rely on veryprimitive linguistic support. (See [6] for recent works in thisdirection). Similar motivations appear in [4, 5, 7, 8, 11, 12, 13,14, 15, 17, 19, 20, 21]\nIn a previous paper, we introduced the notion of structuringschemas as a mean of providing a database view on structured dataresiding in a file. A structuring schema consists of a grammartogether with semantic actions (in a database language). We alsoshowed how queries on files expressed in ahigh-level query language (O2-SQL [3]) couldbe evaluated efficiently using variations of standard databaseoptimization techniques. The problem of update was mentioned therebut remained largely unexplored. This is the topic of the presentpaper.\nWe argue that updates on files can be expressed convenientlyusing high-level database update languages that work on thedatabase view of the file. The key problem is how to propagate anupdate specified on the database (here a view) to the file (herethe physical storage). As a first step, we propose anaive way of update propagation: the databaseview of the file is materialized; the update is performed on thedatabase; the database is \"unparsed\" to produce an updated file.For this, we develop an unparsing technique. Theproblems that we meet while developing this technique are relatedto the well-known view update problem. ( See, for instance [9, 10,16, 23].) The technique relies on the existence of an inversemapping from the database to the file. We show that the existenceof such an inverse mapping results from the use of restrictedstructuring schemas.\nThe naive technique presents two majordrawbacks. It is inefficient: it entails intense data constructionand unparsing, most of which dealing with data not involved in theupdate. It may result in information loss: information in the file,that is not recorded in the database, may be lost in the process.The major contribution of this paper is a combination of techniquesthat allows to minimize both the data construction and theunparsing work. First, we briefly show how optimization techniquesfrom [2] can be used to focus on the relevant portion of thedatabase and to avoid constructing the entire database. Then weshow that for a class of structuring schemas satisfying alocality condition, it is possible to carefullycircumscribe the unparsing.\nSome of the results in the paper are negative. They should notcome as a surprise since we are dealing with complex theoreticalfoundations: language theory (for parsing and unparsing), andfirst-order logic (for database languages). However, we do presentpositive results for particular classes of structuring schemas. Webelieve that the restrictions imposed on these schemas are veryacceptable in practice. (For instance, all \"real\" examples ofstructuring schemas that we examined arelocal.)\nThe paper is organized as follows. In Section 2, we present theupdate problem and the structuring schemas; in Section 3, a naivetechnique for update propagation and the unparsing technique.Section 4 introduces a locality condition, and presents a moreefficient technique for propagating updates in local structuringschemas. The last section is a conclusion.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "fe9b2df19285e502a3cffe5aeeff0a8441f525e5",
            "title": "Algebras for querying text regions (extended abstract)",
            "abstract": "There is a significant amount of interest in combining and extending database and information retrieval technologies to manage textual data. The challenge is becoming more relevant due to the increased availability of documents in digital form. Document data has a natural hierarchical structure, which may be made explicit due to the use of markup conventions (as it is the case with SGML). An important aspect of managing structured and semi-structured textual data consists of supporting the efficient retrieval of text components based both on their content and structure. In this paper we study issues related to the expressive power and optimization of a class of algebras that support combining string (or pattern) searches with queries on the hierarchical structure of the text. The region algebra studied is a set-at-a-time algebra for manipulating -tezt regions (substrings of the text) that supports finding out nesting and ordering properties of the text regions. The region algebra is part of the language in use in commercial text retrieval systems, and can be implemented very efficiently. The results in this work are obtained by showing a close relationship between the region algebra and the monadic first order theory of binary trees. We show that queries in the algebra can be optimized, but the optimization can be difficult (Co-NP-Hard in the general case, although there is an important class of queries that can be optimized in polynomial time). On the negative side, we show that the language is incapable of capturing some important properties of the text structure, related to the nesting and ordering of text regions. We conclude by suggesting possible extensions *Research done while the author was at the University of Toronto. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice ISgiven that copying is by permission of the Association of Computing Machinery.To copy otherwise, or to republish, requires a fee and/or specific permission, PODS \u201995 San Jose CA USA (3 1995 ACM 0-89791 -730-8/95/0005.. $3.50 Tova Mile* Department of Computer Science",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1735847",
                    "name": "M. Consens"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "ffad1d8e486dc86519b4e5d697a6186122a313e9",
            "title": "Similarity-based queries",
            "abstract": "We develop a domain-independent framework for defining qneries in terms of similarity of objects. Our framework has three components: a pattern language, a transformation rule language, and a query language. The pattern language specifies classes of objects, the transformation rule language defines similarity by specifying the similarity-preserving transformations, and the whole package is wrapped in a general query language. The framework can be \u201ctuned\u201d to the needs of a specific application domain, such as time sequences, molecules, text strings or images, by the choice of these languages. We demonstrate the framework by presenting a specific instance on a specific domain \u2013 the domain of sequences. We start with sequences over a finite alphabet, and then consider sequences over infinite ordered domains. The basic pattern language weuseis regular expressions, and the query language is calculus-based. We show that even when the pattern/query languages chosen are not too powerful, the approximation framework obtained is very strong. We study the properties of the framework, and in particular present expressive power and complexity results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1682406",
                    "name": "A. Mendelzon"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "6617fd96c4b1b9287290b2a613d479604229d3ab",
            "title": "More on Updating the File",
            "abstract": "We present a framework which allows to view information stored in les as if it were an integral part of the database system. We study how high level database update languages can be used to update les, and show that such updates can be executed ee-ciently. The update technique is based on unparsing-a new tool for mapping database objects to strings. We investigate the conditions under which the use of unparsing is possible and eecient. Furthermore, we show that unparsing combined with a previously developed query optimization technique provides an eecient update mechanism.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "6687667d71497a857efd21c06e7681ba30744b17",
            "title": "Object migration",
            "abstract": "We study a mechanism that supports the migration of objects from one class of an OODB to another, thereby enabling us to model the same object playing different roles throughout its lifetime. Object migration may introduce typing conflicts due to the different typing constraints imposed by the classes. We present a coercion-like adaptation process that automatically resolves these conflicts. The process combines re-classification of objects and modification of attributes. We study the computational complexity of the problem, and show that the adaptation process can be performed efficiently in databases with covariant schemas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682406",
                    "name": "A. Mendelzon"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "145457528",
                    "name": "E. Waller"
                }
            ]
        },
        {
            "paperId": "8de7391b4929b4ec4edef6995e4be812680588a7",
            "title": "Optimizing queries on files",
            "abstract": "We present a framework which allows the user to access and manipulate data uniformly, regardless of whether it resides in a database or in the file system (or in both). A key issue is the performance of the system. We show that text indexing, combined with newly developed optimization techniques, can be used to provide an efficient high level interface to information stored in files. Furthermore, using these techniques, some queries can be evaluated significantly faster than in standard database implementations. We also study the tradeoff between efficiency and the amount of indexing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1735847",
                    "name": "M. Consens"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "89f8a1e47d69c1a2236d09d45584b2d85adb7594",
            "title": "Querying and Updating the File",
            "abstract": "We show how structured data stored in files can benefit from standard database technology and in particular be queried and updated using database languages. We introduce the notion of structuring schema which consists of a grammar annotated with database programs and of a database schema. We study the translation from structured strings to databases, and the converse. We adapt optimization techniques from relational databases to our context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "b8ff6c465c5d1c5ad5f520c6a7470230178bc1e3",
            "title": "Towards tractable algebras for bags",
            "abstract": "Bags, i.e. sets with duplicates, are often used to implement relations in database systems. In this paper we study the expressive power of algebras for manipulating bags. The algebra we present is a simple extension of the nested relation algebra. Our aim is to investigate how the use of bags in the language extends its expressive power, and increases its complexity. We consider two main issues, namely (i) the relationship between the depth of bag nesting and the expressive power, and (ii) the relationship between the algebraic operations, and their complexity and expressive power. We show that the bag algebra is more expressive than the nested relation algebra (at all levels of nesting), and that the difference may be subtle. We establish a hierarchy based on the structure of algebra expressions. This hierarchy is shown to be highly related to the properties of the powerset operator.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1710280",
                    "name": "S. Grumbach"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "d7a87d0e432260420b1e6d99b44a1656aa55477c",
            "title": "On the power of algebras with recursion",
            "abstract": "We consider the relationship between the deductive and the functional/algebraic query language paradigms. Previous works considered this subject for a non-recursive algebra, or an algebra with a fixed point operation, and the corresponding class of deductive queries is that defined by stratified programs. We consider here algebraic languages extended by general recursive definitions. We also consider languages that allow non-restricted use of negation. It turns out that recursion and negation in the algebraic paradigm need to be studied together. The semantics used for the comparison is the valid semantics, although other well-known declarative semantics can also be used to derive similar results. We show that the class of queries expressed by general deduction with negation can be captured using algebra with recursive definitions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "cc05d970fb07cbdaa0c98516ba51d58dd2324df7",
            "title": "Functional and predictive programming in OODB's",
            "abstract": "The object oriented paradigm offers a rich set of data structuring and manipulation facilities. Object oriented data models generalize the nested relations/complex object models, that support the construction of complex values using trqde, set and other constructors, and those that support ADT definitions [2, 3, 4, 9, 10, 17, 20]. Recently, much attention is paid to declarative languages for these models. The relational model supports three query language paradigms: algebra, calculus and logic programming. These have been extended in the late 80\u2019s to the nested relations/complex object models, by adding operations and predicates for dealing with sets (such as z c S) and new algebraic operations such as nest, unnest, powerset [1]. Extensions of these paradigms to full object-oriented models are now being investigated [6, 13, 16]. A lot of work has been done on comparing the expressive power of languages for the relational model; the most famous is Codd\u2019s result about the equivalence of the algebra and the calculus. Many of these results have been re-examined in the context of complex object models, and in particular, the algebra-calculus",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "af37c68e01d47b1160ba00a06af6623e42fe2328",
            "title": "A Model for Active Object Oriented Databases",
            "abstract": "This paper presents a logical model for an active object oriented dntahaae. The main idea is to use standard concepts of OODB such aa encapsulation and inheritance, In particular triggers are special met,hods encapsulated in the appropriate object. The execution model uses nested transactions. The model is shown to he flexihle and to generalize previous proposals,",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        }
    ]
}