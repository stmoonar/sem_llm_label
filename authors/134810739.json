{
    "authorId": "134810739",
    "papers": [
        {
            "paperId": "962467abfde04bfa97a66bffad9d1b32618d4e40",
            "title": "Image-Text Rematching for News Items using Optimized Embeddings and CNNs in MediaEval NewsImages 2021",
            "abstract": "Finding a matching image for a news article is a core problem in the creation of traditional and online newspapers. The task of image-text matching has thus become a vibrant research area in computer science. The performance of state-of-the-art image retrieval systems on various benchmarks is excellent. However, they all rely on datasets with a detailed textual description of the images or on very large training collections. In this work, we optimize image-text matching algorithms for a small dataset based on the data of a single newspaper. Our optimized processing pipeline and the computed configurations reach precise results. The evaluation results obtained in the MediaEval NewsImages benchmark significantly outperforming the algorithms from previous years.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "134810739",
                    "name": "Tom S\u00fchr"
                },
                {
                    "authorId": "2181915130",
                    "name": "Ajay Madhavanr"
                },
                {
                    "authorId": "1752764126",
                    "name": "Nasim Jamshidi Avanaki"
                },
                {
                    "authorId": "2181915117",
                    "name": "Ren\u00e9 Berk"
                },
                {
                    "authorId": "47735361",
                    "name": "A. Lommatzsch"
                }
            ]
        },
        {
            "paperId": "0186f849e38e54579856ef5b1a39458e800389c3",
            "title": "Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring",
            "abstract": "Ranking algorithms are being widely employed in various online hiring platforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has demonstrated that ranking algorithms employed by these platforms are prone to a variety of undesirable biases, leading to the proposal of fair ranking algorithms (e.g., Det-Greedy) which increase exposure of underrepresented candidates. However, there is little to no work that explores whether fair ranking algorithms actually improve real world outcomes (e.g., hiring decisions) for underrepresented groups. Furthermore, there is no clear understanding as to how other factors (e.g., job context, inherent biases of the employers) may impact the efficacy of fair ranking in practice. In this work, we analyze various sources of gender biases in online hiring platforms, including the job context and inherent biases of employers and establish how these factors interact with ranking algorithms to affect hiring decisions. To the best of our knowledge, this work makes the first attempt at studying the interplay between the aforementioned factors in the context of online hiring. We carry out a large-scale user study simulating online hiring scenarios with data from TaskRabbit, a popular online freelancing site. Our results demonstrate that while fair ranking algorithms generally improve the selection rates of underrepresented minorities, their effectiveness relies heavily on the job contexts and candidate profiles.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "134810739",
                    "name": "Tom S\u00fchr"
                },
                {
                    "authorId": "67233765",
                    "name": "Sophie Hilgard"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "2c03652190f4f1dd8f23b52e693c170d29934937",
            "title": "A Note on the Significance Adjustment for FA*IR with Two Protected Groups",
            "abstract": "In this report we provide an improvement of the significance adjustment from the FA*IR algorithm in Zehlike et al. [2], which did not work for very short rankings in combination with a low minimum proportion p for the protected group. We show how the minimum number of protected candidates per ranking position can be calculated exactly and provide a mapping from the continuous space of significance levels (\u03b1) to a discrete space of tables, which allows us to find \u03b1corr using a binary search heuristic. In this report we describe a correction of the significance adjustment procedure from [2], which did not work for very small k and \u03b1 . For binomial distributions, i.e. where only one protected and one non-protected group is present, the inverse CDF can be stored as a simple table, which we compute using Algorithm 1. We will call such a table mTable. Algorithm 1: Algorithm ConstructMTable computes the data structure to efficiently verify or construct a ranking that satisfies binomial ranked group fairness. input :k , the size of the ranking to produce; p , the expected proportion of protected elements; \u03b1corr, the significance for each individual test. output :mTable: A list that contains the minimum number of protected candidates required at each position of a ranking of size k . 1 mTable\u2190 [k] // list of size k 2 for i \u2190 1 to k do 3 mTable[i] \u2190 F\u22121 (i, p, \u03b1corr) // the inverse binomial cdf 4 end 5 return mTable Table 1 shows an example of mTables for different k and p , using \u03b1 = 0.1. For instance, for p = 0.5 we see that at least 1 candidate from the protected group is needed in the top 4 positions, and 2 protected candidates in the top 7 positions. Figure 1 shows that we need a correction for \u03b1 as we are testing multiple hypothesis in the ranked group fairness test, namely k of them (note that the scale is logarithmic). In the following, we show that the special case of having only one protected group offers possibilities for verifying ranked group fairness efficiently. A key advantage of considering just one protected group, rather than multiple, is that we can calculate the exact failure probability Pfail (i.e. a fair ranking gets rejected by the ranked group fairness test), which results in an efficient binary search for \u03b1corr. First we introduce the necessary notation for the binomial case and describe how we calculate the exact Pfail. Then we show that we can divide the continuum of possible \u03b1 values in discrete . This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. \u00a9 2020 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License. XXXX-XXXX/2020/12-ART https://doi.org/10.1145/nnnnnnn.nnnnnnn ar X iv :2 01 2. 12 79 5v 1 [ cs .I R ] 2 3 D ec 2 02 0",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                },
                {
                    "authorId": "134810739",
                    "name": "Tom S\u00fchr"
                },
                {
                    "authorId": "2065333595",
                    "name": "C. Castillo"
                }
            ]
        },
        {
            "paperId": "1a6a5be4251e82a810d67bd295f91d8136635331",
            "title": "Two-Sided Fairness for Repeated Matchings in Two-Sided Markets: A Case Study of a Ride-Hailing Platform",
            "abstract": "Ride hailing platforms, such as Uber, Lyft, Ola or DiDi, have traditionally focused on the satisfaction of the passengers, or on boosting successful business transactions. However, recent studies provide a multitude of reasons to worry about the drivers in the ride hailing ecosystem. The concerns range from bad working conditions and worker manipulation to discrimination against minorities. With the sharing economy ecosystem growing, more and more drivers financially depend on online platforms and their algorithms to secure a living. It is pertinent to ask what a fair distribution of income on such platforms is and what power and means the platform has in shaping these distributions. In this paper, we analyze job assignments of a major taxi company and observe that there is significant inequality in the driver income distribution. We propose a novel framework to think about fairness in the matching mechanisms of ride hailing platforms. Specifically, our notion of fairness relies on the idea that, spread over time, all drivers should receive benefits proportional to the amount of time they are active in the platform. We postulate that by not requiring every match to be fair, but rather distributing fairness over time, we can achieve better overall benefit for the drivers and the passengers. We experiment with various optimization problems and heuristics to explore the means of achieving two-sided fairness, and investigate their caveats and side-effects. Overall, our work takes the first step towards rethinking fairness in ride hailing platforms with an additional emphasis on the well-being of drivers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "134810739",
                    "name": "Tom S\u00fchr"
                },
                {
                    "authorId": "31908706",
                    "name": "Asia J. Biega"
                },
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "2676839",
                    "name": "Abhijnan Chakraborty"
                }
            ]
        },
        {
            "paperId": "96dfd1064dcb4382e213c83f7abb587d4261ef7d",
            "title": "FairSearch: A Tool For Fairness in Ranked Search Results",
            "abstract": "Ranked search results and recommendations have become the main mechanism by which we find content, products, places, and people online. With hiring, selecting, purchasing, and dating being increasingly mediated by algorithms, rankings may determine business opportunities, education, access to benefits, and even social success. It is therefore of societal and ethical importance to ask whether search results can demote, marginalize, or exclude individuals of unprivileged groups or promote products with undesired features. In this paper we present FairSearch, the first fair open source search API to provide fairness notions in ranked search results. We implement two well-known algorithms from the literature, namely FA*IR (Zehlike et al., 9) and DELTR (Zehlike and Castillo, 10) and provide them as stand-alone libraries in Python and Java. Additionally we implement interfaces to Elasticsearch for both algorithms, a well-known search engine API based on Apache Lucene. The interfaces use the aforementioned Java libraries and enable search engine developers who wish to ensure fair search results of different styles to easily integrate DELTR and FA*IR into their existing Elasticsearch environment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                },
                {
                    "authorId": "134810739",
                    "name": "Tom S\u00fchr"
                },
                {
                    "authorId": "2065333595",
                    "name": "C. Castillo"
                },
                {
                    "authorId": "2456123",
                    "name": "Ivan Kitanovski"
                }
            ]
        }
    ]
}