{
    "authorId": "2155897297",
    "papers": [
        {
            "paperId": "3145014e65861f3981a652222daf964bca3bbe62",
            "title": "Diverse Counterfactual Explanations for Anomaly Detection in Time Series",
            "abstract": "Data-driven methods that detect anomalies in times series data are ubiquitous in practice, but they are in general unable to provide helpful explanations for the predictions they make. In this work we propose a model-agnostic algorithm that generates counterfactual ensemble explanations for time series anomaly detection models. Our method generates a set of diverse counterfactual examples, i.e, multiple perturbed versions of the original time series that are not considered anomalous by the detection model. Since the magnitude of the perturbations is limited, these counterfactuals represent an ensemble of inputs similar to the original time series that the model would deem normal. Our algorithm is applicable to any differentiable anomaly detection model. We investigate the value of our method on univariate and multivariate real-world datasets and two deep-learning-based anomaly detection models, under several explainability criteria previously proposed in other data domains such as Validity, Plausibility, Closeness and Diversity. We show that our algorithm can produce ensembles of counterfactual examples that satisfy these criteria and thanks to a novel type of visualisation, can convey a richer interpretation of a model's internal mechanism than existing methods. Moreover, we design a sparse variant of our method to improve the interpretability of counterfactual explanations for high-dimensional time series anomalies. In this setting, our explanation is localised on only a few dimensions and can therefore be communicated more efficiently to the model's user.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2285351482",
                    "name": "Deborah Sulem"
                },
                {
                    "authorId": "2192704",
                    "name": "Michele Donini"
                },
                {
                    "authorId": "2625318",
                    "name": "M. B. Zafar"
                },
                {
                    "authorId": "51055581",
                    "name": "Fran\u00e7ois-Xavier Aubet"
                },
                {
                    "authorId": "2113062",
                    "name": "Jan Gasthaus"
                },
                {
                    "authorId": "2166235",
                    "name": "Tim Januschowski"
                },
                {
                    "authorId": "2155897297",
                    "name": "Sanjiv Das"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                }
            ]
        },
        {
            "paperId": "43bb493df73db5bc407c80dddb213b6c9450083f",
            "title": "More Than Words: Towards Better Quality Interpretations of Text Classifiers",
            "abstract": "The large size and complex decision mechanisms of state-of-the-art text classifiers make it difficult for humans to understand their predictions, leading to a potential lack of trust by the users. These issues have led to the adoption of methods like SHAP and Integrated Gradients to explain classification decisions by assigning importance scores to input tokens. However, prior work, using different randomization tests, has shown that interpretations generated by these methods may not be robust. For instance, models making the same predictions on the test set may still lead to different feature importance rankings. In order to address the lack of robustness of token-based interpretability, we explore explanations at higher semantic levels like sentences. We use computational metrics and human subject studies to compare the quality of sentence-based interpretations against token-based ones. Our experiments show that higher-level feature attributions offer several advantages: 1) they are more robust as measured by the randomization tests, 2) they lead to lower variability when using approximation-based methods like SHAP, and 3) they are more intelligible to humans in situations where the linguistic coherence resides at a higher granularity level. Based on these findings, we show that token-based interpretability, while being a convenient first choice given the input interfaces of the ML models, is not the most effective one in all situations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2625318",
                    "name": "M. B. Zafar"
                },
                {
                    "authorId": "144906813",
                    "name": "Philipp Schmidt"
                },
                {
                    "authorId": "2192704",
                    "name": "Michele Donini"
                },
                {
                    "authorId": "2824663",
                    "name": "C. Archambeau"
                },
                {
                    "authorId": "2170760",
                    "name": "F. Biessmann"
                },
                {
                    "authorId": "2155897297",
                    "name": "Sanjiv Das"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                }
            ]
        },
        {
            "paperId": "ce1cc24b687715d844cd3848ec174d0ed6acd87c",
            "title": "Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud",
            "abstract": "Understanding the predictions made by machine learning (ML) models and their potential biases remains a challenging and labor-intensive task that depends on the application, the dataset, and the specific model. We present Amazon SageMaker Clarify, an explainability feature for Amazon SageMaker that launched in December 2020, providing insights into data and ML models by identifying biases and explaining predictions. It is deeply integrated into Amazon SageMaker, a fully managed service that enables data scientists and developers to build, train, and deploy ML models at any scale. Clarify supports bias detection and feature importance computation across the ML lifecycle, during data preparation, model evaluation, and post-deployment monitoring. We outline the desiderata derived from customer input, the modular architecture, and the methodology for bias and explanation computations. Further, we describe the technical challenges encountered and the tradeoffs we had to make. For illustration, we discuss two customer use cases. We present our deployment results including qualitative customer feedback and a quantitative evaluation. Finally, we summarize lessons learned, and discuss best practices for the successful adoption of fairness and explanation tools in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40356669",
                    "name": "Michaela Hardt"
                },
                {
                    "authorId": "2121291892",
                    "name": "Xiaoguang Chen"
                },
                {
                    "authorId": "2149479776",
                    "name": "Xiaoyi Cheng"
                },
                {
                    "authorId": "2192704",
                    "name": "Michele Donini"
                },
                {
                    "authorId": "2077674005",
                    "name": "J. Gelman"
                },
                {
                    "authorId": "2121225230",
                    "name": "Satish Gollaprolu"
                },
                {
                    "authorId": "2121290242",
                    "name": "John He"
                },
                {
                    "authorId": "74650780",
                    "name": "Pedro Larroy"
                },
                {
                    "authorId": "2110789758",
                    "name": "Xinyu Liu"
                },
                {
                    "authorId": "2121255038",
                    "name": "Nick McCarthy"
                },
                {
                    "authorId": "47745269",
                    "name": "Ashish M. Rathi"
                },
                {
                    "authorId": "2121124106",
                    "name": "Scott Rees"
                },
                {
                    "authorId": "2052548461",
                    "name": "Ankit Siva"
                },
                {
                    "authorId": "2121244925",
                    "name": "ErhYuan Tsai"
                },
                {
                    "authorId": "2121256262",
                    "name": "Keerthan Vasist"
                },
                {
                    "authorId": "2077676587",
                    "name": "Pinar Yilmaz"
                },
                {
                    "authorId": "2625318",
                    "name": "M. B. Zafar"
                },
                {
                    "authorId": "2155897297",
                    "name": "Sanjiv Das"
                },
                {
                    "authorId": "2069854336",
                    "name": "Kevin Haas"
                },
                {
                    "authorId": "48162916",
                    "name": "Tyler Hill"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                }
            ]
        }
    ]
}