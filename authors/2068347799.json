{
    "authorId": "2068347799",
    "papers": [
        {
            "paperId": "34a6f05b9f8bb8222fa7d02cf5f0ad544ffcfaf7",
            "title": "Can I Be of Further Assistance? Using Unstructured Knowledge Access to Improve Task-oriented Conversational Modeling",
            "abstract": "Most prior work on task-oriented dialogue systems are restricted to limited coverage of domain APIs. However, users oftentimes have requests that are out of the scope of these APIs. This work focuses on responding to these beyond-API-coverage user turns by incorporating external, unstructured knowledge sources. Our approach works in a pipelined manner with knowledge-seeking turn detection, knowledge selection, and response generation in sequence. We introduce novel data augmentation methods for the first two steps and demonstrate that the use of information extracted from dialogue context improves the knowledge selection and end-to-end performances. Through experiments, we achieve state-of-the-art performance for both automatic and human evaluation metrics on the DSTC9 Track 1 benchmark dataset, validating the effectiveness of our contributions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "655c250b36ae706cc9db5038d32c85080d4f70fe",
            "title": "Adversarial Contrastive Pre-training for Protein Sequences",
            "abstract": "Recent developments in Natural Language Processing (NLP) demonstrate that large-scale, self-supervised pre-training can be extremely beneficial for downstream tasks. These ideas have been adapted to other domains, including the analysis of the amino acid sequences of proteins. However, to date most attempts on protein sequences rely on direct masked language model style pre-training. In this work, we design a new, adversarial pre-training method for proteins, extending and specializing similar advances in NLP. We show compelling results in comparison to traditional MLM pre-training, though further development is needed to ensure the gains are worth the significant computational cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41153596",
                    "name": "Matthew B. A. McDermott"
                },
                {
                    "authorId": "47714231",
                    "name": "Brendan Yap"
                },
                {
                    "authorId": "2047429512",
                    "name": "Harry Hsu"
                },
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1679873",
                    "name": "Peter Szolovits"
                }
            ]
        },
        {
            "paperId": "6c88f39d65d56fd362cc7ea19be9b320ef678796",
            "title": "Heterogeneous Network Embedding for Deep Semantic Relevance Match in E-commerce Search",
            "abstract": "Result relevance prediction is an essential task of e-commerce search engines to boost the utility of search engines and ensure smooth user experience. The last few years eyewitnessed a flurry of research on the use of Transformer-style models and deep text-match models to improve relevance. However, these two types of models ignored the inherent bipartite network structures that are ubiquitous in e-commerce search logs, making these models ineffective. We propose in this paper a novel Second-order Relevance, which is fundamentally different from the previous First-order Relevance, to improve result relevance prediction. We design, for the first time, an end-to-end First-and-Second-order Relevance prediction model for e-commerce item relevance. The model is augmented by the neighborhood structures of bipartite networks that are built using the information of user behavioral feedback, including clicks and purchases. To ensure that edges accurately encode relevance information, we introduce external knowledge generated from BERT to refine the network of user behaviors. This allows the new model to integrate information from neighboring items and queries, which are highly relevant to the focus query-item pair under consideration. Results of offline experiments showed that the new model significantly improved the prediction accuracy in terms of human relevance judgment. An ablation study showed that the First-and-Second-order model gained a 4.3% average gain over the First-order model. Results of an online A/B test revealed that the new model derived more commercial benefits compared to the base model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39789747",
                    "name": "Ziyang Liu"
                },
                {
                    "authorId": "2113279258",
                    "name": "Zhaomeng Cheng"
                },
                {
                    "authorId": "2131218",
                    "name": "Yunjiang Jiang"
                },
                {
                    "authorId": "2053234459",
                    "name": "Yue Shang"
                },
                {
                    "authorId": "2055448780",
                    "name": "Wei Xiong"
                },
                {
                    "authorId": "1752741172",
                    "name": "Sulong Xu"
                },
                {
                    "authorId": "2052143728",
                    "name": "Bo Long"
                },
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                }
            ]
        },
        {
            "paperId": "051b1049e0e0cf1e0620065e7c69f908e98e9ece",
            "title": "Hooks in the Headline: Learning to Generate Headlines with Controlled Styles",
            "abstract": "Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles. We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers. With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework. We also introduced a novel parameter sharing scheme to further disentangle the style from text. Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait. The attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68%, even outperforming human-written references.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "8752221",
                    "name": "Zhijing Jin"
                },
                {
                    "authorId": "10638646",
                    "name": "Joey Tianyi Zhou"
                },
                {
                    "authorId": "2065295357",
                    "name": "Lisa Orii"
                },
                {
                    "authorId": "1679873",
                    "name": "Peter Szolovits"
                }
            ]
        },
        {
            "paperId": "1d2d6dee8be881e58ecd45801eba962d89d3983f",
            "title": "Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation",
            "abstract": "State-of-the-art neural machine translation (NMT) systems are data-hungry and perform poorly on domains with little supervised data. As data collection is expensive and infeasible in many cases, unsupervised domain adaptation methods are needed. We apply an Iterative Back Translation (IBT) training scheme on in-domain monolingual data, which repeatedly uses a Transformer-based NMT model to create in-domain pseudo-parallel sentence pairs in one translation direction on the fly and then use them to train the model in the other direction. Evaluated on three domains of German-to-English translation task with no supervised data, this simple technique alone (without any out-of-domain parallel data) can already surpass all previous domain adaptation methods---up to +9.48 BLEU over the strongest previous method, and up to +27.77 BLEU over the unadapted baseline. Moreover, given available supervised out-of-domain data on German-to-English and Romanian-to-English language pairs, we can further enhance the performance and obtain up to +19.31 BLEU improvement over the strongest baseline, and +47.69 BLEU increment against the unadapted model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "8752221",
                    "name": "Zhijing Jin"
                },
                {
                    "authorId": "10638646",
                    "name": "Joey Tianyi Zhou"
                },
                {
                    "authorId": "1679873",
                    "name": "Peter Szolovits"
                }
            ]
        },
        {
            "paperId": "1f199b82c8eca83e275ffba50a4be7b6a3488637",
            "title": "A Simple Baseline to Semi-Supervised Domain Adaptation for Machine Translation",
            "abstract": "State-of-the-art neural machine translation (NMT) systems are data-hungry and perform poorly on new domains with no supervised data. As data collection is expensive and infeasible in many cases, domain adaptation methods are needed. In this work, we propose a simple but effect approach to the semi-supervised domain adaptation scenario of NMT, where the aim is to improve the performance of a translation model on the target domain consisting of only non-parallel data with the help of supervised source domain data. This approach iteratively trains a Transformer-based NMT model via three training objectives: language modeling, back-translation, and supervised translation. We evaluate this method on two adaptation settings: adaptation between specific domains and adaptation from a general domain to specific domains, and on two language pairs: German to English and Romanian to English. With substantial performance improvement achieved---up to +19.31 BLEU over the strongest baseline, and +47.69 BLEU improvement over the unadapted model---we present this method as a simple but tough-to-beat baseline in the field of semi-supervised domain adaptation for NMT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "8752221",
                    "name": "Zhijing Jin"
                },
                {
                    "authorId": "10638646",
                    "name": "Joey Tianyi Zhou"
                },
                {
                    "authorId": "1679873",
                    "name": "Peter Szolovits"
                }
            ]
        },
        {
            "paperId": "2aaa9ed6ba7646684afbbb42bd3cce863e357678",
            "title": "Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis",
            "abstract": "Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards a specific aspect in the text. However, existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-target aspects. To solve this problem, we develop a simple but effective approach to enrich ABSA test sets. Specifically, we generate new examples to disentangle the confounding sentiments of the non-target aspects from the target aspect's sentiment. Based on the SemEval 2014 dataset, we construct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the aspect robustness of ABSA models. Over 92% data of ARTS show high fluency and desired sentiment on all aspects by human evaluation. Using ARTS, we analyze the robustness of nine ABSA models, and observe, surprisingly, that their accuracy drops by up to 69.73%. We explore several ways to improve aspect robustness, and find that adversarial training can improve models' performance on ARTS by up to 32.85%. Our code and new test set are available at this https URL",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151054",
                    "name": "Xiaoyu Xing"
                },
                {
                    "authorId": "8752221",
                    "name": "Zhijing Jin"
                },
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "46270259",
                    "name": "Bingning Wang"
                },
                {
                    "authorId": "1409702669",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "42b8944b51be279ec12d06408e646a8d52d54b3d",
            "title": "Deep Learning for Text Style Transfer: A Survey",
            "abstract": "Text style transfer is an important task in natural language generation, which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing, and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this article, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of this task.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "8752221",
                    "name": "Zhijing Jin"
                },
                {
                    "authorId": "2749311",
                    "name": "Zhiting Hu"
                },
                {
                    "authorId": "1712417",
                    "name": "Olga Vechtomova"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "44bd6111520ebb7323e834d14aa8b2fe4e4b3b79",
            "title": "Deep Learning for Text Attribute Transfer: A Survey.",
            "abstract": "Driven by the increasingly larger deep learning models, neural language generation (NLG) has enjoyed unprecedentedly improvement and is now able to generate a diversity of human-like texts on demand, granting itself the capability of serving as a human writing assistant. Text attribute transfer is one of the most important NLG tasks, which aims to control certain attributes that people may expect the texts to possess, such as sentiment, tense, emotion, political position, etc. It has a long history in Natural Language Processing but recently gains much more attention thanks to the promising performance brought by deep learning models. In this article, we present a systematic survey on these works for neural text attribute transfer. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze around 65 representative works in a comprehensive way. Overall, we have covered the task formulation, existing datasets and metrics for model development and evaluation, and all methods developed over the last several years. We reveal that existing methods are indeed based on a combination of several loss functions with each of which serving a certain goal. Such a unique perspective we provide could shed light on the design of new methods. We conclude our survey with a discussion on open issues that need to be resolved for better future development.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "8752221",
                    "name": "Zhijing Jin"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "592046d0d45c1fd823ff6b2a4c4162a96d7ecff8",
            "title": "Augmenting NLP models using Latent Feature Interpolations",
            "abstract": "Models with a large number of parameters are prone to over-fitting and often fail to capture the underlying input distribution. We introduce Emix, a data augmentation method that uses interpolations of word embeddings and hidden layer representations to construct virtual examples. We show that Emix shows significant improvements over previously used interpolation based regularizers and data augmentation techniques. We also demonstrate how our proposed method is more robust to sparsification. We highlight the merits of our proposed methodology by performing thorough quantitative and qualitative assessments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054966158",
                    "name": "Amit Jindal"
                },
                {
                    "authorId": "2029671030",
                    "name": "Arijit Ghosh Chowdhury"
                },
                {
                    "authorId": "150010392",
                    "name": "Aniket Didolkar"
                },
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "1753278",
                    "name": "R. Shah"
                }
            ]
        }
    ]
}