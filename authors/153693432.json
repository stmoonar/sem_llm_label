{
    "authorId": "153693432",
    "papers": [
        {
            "paperId": "0383e049e98c9eedbc61be728d4ef037300bbedf",
            "title": "Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach",
            "abstract": "In the past decades, recommender systems have attracted much attention in both research and industry communities, and a large number of studies have been devoted to developing effective recommendation models. Basically speaking, these models mainly learn the underlying user preference from historical behavior data, and then estimate the user-item matching relationships for recommendations. Inspired by the recent progress on large language models (LLMs), we take a different approach to developing the recommendation models, considering recommendation as instruction following by LLMs. The key idea is that the preferences or needs of a user can be expressed in natural language descriptions (called instructions), so that LLMs can understand and further execute the instruction for fulfilling the recommendation task. Instead of using public APIs of LLMs, we instruction tune an open-source LLM (3B Flan-T5-XL), in order to better adapt LLMs to recommender systems. For this purpose, we first design a general instruction format for describing the preference, intention, task form and context of a user in natural language. Then we manually design 39 instruction templates and automatically generate a large amount of user-personalized instruction data (252K instructions) with varying types of preferences and intentions. To demonstrate the effectiveness of our approach, we instantiate the instruction templates into several widely-studied recommendation (or search) tasks, and conduct extensive experiments on these tasks with real-world datasets. Experiment results show that the proposed approach can outperform several competitive baselines, including the powerful GPT-3.5, on these evaluation tasks. Our approach sheds light on developing more user-friendly recommender systems, in which users can freely communicate with the system and obtain more accurate recommendations via natural language instructions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120518257",
                    "name": "Junjie Zhang"
                },
                {
                    "authorId": "3360722",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "151472453",
                    "name": "Yupeng Hou"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "4950224",
                    "name": "Leyu Lin"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        },
        {
            "paperId": "0b220041eb83c23b7b10d32a5d08c0309d528071",
            "title": "Large Language Models for Information Retrieval: A Survey",
            "abstract": "As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1900406",
                    "name": "Yutao Zhu"
                },
                {
                    "authorId": "51162759",
                    "name": "Huaying Yuan"
                },
                {
                    "authorId": "2109464614",
                    "name": "Shuting Wang"
                },
                {
                    "authorId": "1830383266",
                    "name": "Jiongnan Liu"
                },
                {
                    "authorId": "2229460959",
                    "name": "Wenhan Liu"
                },
                {
                    "authorId": "2057946598",
                    "name": "Chenlong Deng"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        },
        {
            "paperId": "14e9f752dfabb8ce34e310ed26c26a3f72a00e74",
            "title": "Learning to Perturb for Contrastive Learning of Unsupervised Sentence Representations",
            "abstract": "Recently, contrastive learning has been shown effective in fine-tuning pre-trained language models (PLM) to learn sentence representations, which incorporates perturbations into unlabeled sentences to augment semantically related positive examples for training. However, previous works mostly adopt heuristic perturbation methods that are independent of the sentence representations. Since the perturbations are unaware of the goal or process of sentence representation learning during training, it is likely to lead to sub-optimal augmentations for conducting constrative learning. To address this issue, we propose a new framework L2P-CSR that adopts a learnable perturbation strategy for improving contrastive learning of sentence representations. In our L2P-CSR, we design a safer perturbation mechanism that only weakens the influence of tokens and features on the sentence representation, which avoids dramatically changing the semantics of the sentence representations. Besides, we devise a gradient-based algorithm to generate adaptive perturbations specially for the dynamically updated sentence representation during training. Such a way is more capable of augmenting high-quality examples that guide the sentence representation learning. Extensive experiments on diverse sentence-related tasks show that our approach outperforms competitive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1423651904",
                    "name": "Kun Zhou"
                },
                {
                    "authorId": "2116568362",
                    "name": "Yuanhang Zhou"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        },
        {
            "paperId": "190140670d224fe934a519e47be196eec9db3f56",
            "title": "Contrastive Learning for User Sequence Representation in Personalized Product Search",
            "abstract": "Providing personalization in product search has attracted increasing attention in both industry and research communities. Most existing personalized product search methods model users' individual search interests based on their historical search logs to generate personalized search results. However, the search logs may be sparse or noisy in the real scenario, which is difficult for existing methods to learn accurate and robust user representations. To address this issue, we propose a contrastive learning framework CoPPS that aims to learn high-quality user representations for personalized product search. Specifically, we design three data augmentation and contrastive learning strategies to construct self-supervision signals from the original search behaviours. The contrastive learning tasks utilize an external knowledge graph and exploit the correlations within and between user sequences, thereby facilitating the discovery of more meaningful search patterns and ultimately enhancing the quality of personalized search. Experimental results on the public Amazon datasets verify the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2227491450",
                    "name": "Shitong Dai"
                },
                {
                    "authorId": "1830383266",
                    "name": "Jiongnan Liu"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "49528487",
                    "name": "Hong Wang"
                },
                {
                    "authorId": "2146017790",
                    "name": "Lin Liu"
                },
                {
                    "authorId": "143947042",
                    "name": "Bo Long"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        },
        {
            "paperId": "1d24f26cfde2b39849db4119d2044b071b2431c1",
            "title": "Towards a More User-Friendly and Easy-to-Use Benchmark Library for Recommender Systems",
            "abstract": "In recent years, the reproducibility of recommendation models has become a severe concern in recommender systems. In light of this challenge, we have previously released a unified, comprehensive and efficient recommendation library called RecBole, attracting much attention from the research community. With the increasing number of users, we have received a number of suggestions and update requests. This motivates us to make further improvements on our library, so as to meet the user requirements and contribute to the research community. In this paper, we present a significant update of RecBole, making it more user-friendly and easy-to-use as a comprehensive benchmark library for recommendation. More specifically, the highlights of this update are summarized as: (1) we include more benchmark models and datasets, improve the benchmark framework in terms of data processing, training and evaluation, and release reproducible configurations to benchmark the recommendation models; (2) we upgrade the user friendliness of our library by providing more detailed documentation and well-organized frequently asked questions, and (3) we propose several development guidelines for the open-source library developers. These extensions make it much easier to reproduce the benchmark results and stay up-to-date with the recent advances on recommender systems. Our update is released at the link: https://github.com/RUCAIBox/RecBole.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167464968",
                    "name": "Lanling Xu"
                },
                {
                    "authorId": "2152251597",
                    "name": "Zhen Tian"
                },
                {
                    "authorId": "2404181",
                    "name": "Gaowei Zhang"
                },
                {
                    "authorId": "2120518257",
                    "name": "Junjie Zhang"
                },
                {
                    "authorId": "2152509786",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2223753455",
                    "name": "Bowen Zheng"
                },
                {
                    "authorId": "2209136299",
                    "name": "Yifan Li"
                },
                {
                    "authorId": "144010962",
                    "name": "Jiakai Tang"
                },
                {
                    "authorId": "2223760889",
                    "name": "Zeyu Zhang"
                },
                {
                    "authorId": "151472453",
                    "name": "Yupeng Hou"
                },
                {
                    "authorId": "1471329930",
                    "name": "Xingyu Pan"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "2144230136",
                    "name": "Xu Chen"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        },
        {
            "paperId": "1f70ca7c9cc9257a16b9705c60b20ed1fc4d872c",
            "title": "Improving Conversational Recommendation Systems via Counterfactual Data Simulation",
            "abstract": "Conversational recommender systems~(CRSs) aim to provide recommendation services via natural language conversations. Although a number of approaches have been proposed for developing capable CRSs, they typically rely on sufficient training data for training. Since it is difficult to annotate recommendation-oriented dialogue datasets, existing CRS approaches often suffer from the issue of insufficient training due to the scarcity of training data. To address this issue, in this paper, we propose a CounterFactual data simulation approach for CRS, named CFCRS, to alleviate the issue of data scarcity in CRSs. Our approach is developed based on the framework of counterfactual data augmentation, which gradually incorporates the rewriting to the user preference from a real dialogue without interfering with the entire conversation flow. To develop our approach, we characterize user preference and organize the conversation flow by the entities involved in the dialogue, and design a multi-stage recommendation dialogue simulator based on a conversation flow language model. Under the guidance of the learned user preference and dialogue schema, the flow language model can produce reasonable, coherent conversation flows, which can be further realized into complete dialogues. Based on the simulator, we perform the intervention at the representations of the interacted entities of target users, and design an adversarial training method with a curriculum schedule that can gradually optimize the data augmentation strategy. Extensive experiments show that our approach can consistently boost the performance of several competitive CRSs, and outperform other data augmentation methods, especially when the training data is limited. Our code is publicly available at https://github.com/RUCAIBox/CFCRS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72541556",
                    "name": "Xiaolei Wang"
                },
                {
                    "authorId": "1423651904",
                    "name": "Kun Zhou"
                },
                {
                    "authorId": "2109887979",
                    "name": "Xinyu Tang"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "2218966021",
                    "name": "Fan Pan"
                },
                {
                    "authorId": "2106400572",
                    "name": "Zhao Cao"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        },
        {
            "paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773",
            "title": "Evaluating Object Hallucination in Large Vision-Language Models",
            "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2209136299",
                    "name": "Yifan Li"
                },
                {
                    "authorId": "2111895473",
                    "name": "Yifan Du"
                },
                {
                    "authorId": "1423651904",
                    "name": "Kun Zhou"
                },
                {
                    "authorId": "1823719",
                    "name": "Jinpeng Wang"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        },
        {
            "paperId": "32f65dd751e5c491d83c2f46e41f50afa1c6d2bb",
            "title": "Scaling Law of Large Sequential Recommendation Models",
            "abstract": "Scaling of neural networks has recently shown great potential to improve the model capacity in various fields. Specifically, model performance has a power-law relationship with model size or data size, which provides important guidance for the development of large-scale models. However, there is still limited understanding on the scaling effect of user behavior models in recommender systems, where the unique data characteristics (e.g. data scarcity and sparsity) pose new challenges to explore the scaling effect in recommendation tasks. In this work, we focus on investigating the scaling laws in large sequential recommendation models. Specially, we consider a pure ID-based task formulation, where the interaction history of a user is formatted as a chronological sequence of item IDs. We don't incorporate any side information (e.g. item text), because we would like to explore how scaling law holds from the perspective of user behavior. With specially improved strategies, we scale up the model size to 0.8B parameters, making it feasible to explore the scaling effect in a diverse range of model sizes. As the major findings, we empirically show that scaling law still holds for these trained models, even in data-constrained scenarios. We then fit the curve for scaling law, and successfully predict the test loss of the two largest tested model scales. Furthermore, we examine the performance advantage of scaling effect on five challenging recommendation tasks, considering the unique issues (e.g. cold start, robustness, long-term preference) in recommender systems. We find that scaling up the model size can greatly boost the performance on these challenging tasks, which again verifies the benefits of large recommendation models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267502484",
                    "name": "Gaowei Zhang"
                },
                {
                    "authorId": "151472453",
                    "name": "Yupeng Hou"
                },
                {
                    "authorId": "2266804776",
                    "name": "Hongyu Lu"
                },
                {
                    "authorId": "2266788301",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "2257376413",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        },
        {
            "paperId": "33da18c377ae04202dd5bcfd0964fb42698aafa8",
            "title": "Spatio-Temporal Branching for Motion Prediction using Motion Increments",
            "abstract": "Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and achieves complementary cross-domain knowledge learning through knowledge distillation. Our approach effectively reduces noise interference and provides more expressive information for characterizing motion by separately extracting temporal and spatial features. We evaluate our approach on standard HMP benchmarks and outperform state-of-the-art methods in terms of prediction accuracy. Code is available at https://github.com/JasonWang959/STPMP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46583614",
                    "name": "Jiexin Wang"
                },
                {
                    "authorId": "2110334261",
                    "name": "Yujie Zhou"
                },
                {
                    "authorId": "2059455684",
                    "name": "Wenwen Qiang"
                },
                {
                    "authorId": "2226443563",
                    "name": "Ying Ba"
                },
                {
                    "authorId": "2100573353",
                    "name": "Bing Su"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        },
        {
            "paperId": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
            "abstract": "Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs).To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance.By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task.In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM.Besides, we also propose the iterative self-prompting strategy to further improve the generation quality.Experimental results on 7 datasets show that our approach can outperform competitive NAR methods, and even surpass autoregressive methods.Our code and data are released at https://github.com/RUCAIBox/DiffusionNAT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1423651904",
                    "name": "Kun Zhou"
                },
                {
                    "authorId": "2209136299",
                    "name": "Yifan Li"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        }
    ]
}