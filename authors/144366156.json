{
    "authorId": "144366156",
    "papers": [
        {
            "paperId": "ca2f63950685a97e5ab6b8e6b2db78a8995e94a2",
            "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization",
            "abstract": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158815576",
                    "name": "Shankar Kanthara"
                },
                {
                    "authorId": "2158811816",
                    "name": "Rixie Tiffany Ko Leong"
                },
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                },
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "5a28b5b0d0b90bf9e5b8066f5e61f3cee672130c",
            "title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation",
            "abstract": "Advanced large-scale neural language models have led to significant success in many language generation tasks. However, the most commonly used training objective, Maximum Likelihood Estimation (MLE), has been shown problematic, where the trained model prefers using dull and repetitive phrases. In this work, we introduce ScaleGrad, a modification straight to the gradient of the loss function, to remedy the degeneration issue of the standard MLE objective. By directly maneuvering the gradient information, ScaleGrad makes the model learn to use novel tokens. Empirical results show the effectiveness of our method not only in open-ended generation, but also in directed generation tasks. With the simplicity in architecture, our method can serve as a general training objective that is applicable to most of the neural text generation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                },
                {
                    "authorId": "2109601302",
                    "name": "Simeng Han"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "91e265040ba64f765db09a3143ce23ef5b626a53",
            "title": "Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling",
            "abstract": "Given the claims of improved text generation quality across various pre-trained neural models, we consider the coherence evaluation of machine generated text to be one of the principal applications of coherence models that needs to be investigated. Prior work in neural coherence modeling has primarily focused on devising new architectures for solving the permuted document task. We instead use a basic model architecture and show significant improvements over state of the art within the same training regime. We then design a harder self-supervision objective by increasing the ratio of negative samples within a contrastive learning setup, and enhance the model further through automatic hard negative mining coupled with a large global negative queue encoded by a momentum encoder. We show empirically that increasing the density of negative samples improves the basic model, and using a global negative queue further improves and stabilizes the model while training with hard negative samples. We evaluate the coherence model on task-independent test sets that resemble real-world applications and show significant improvements in coherence evaluations of downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35640774",
                    "name": "Prathyusha Jwalapuram"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                }
            ]
        },
        {
            "paperId": "d90a49f8cb0df04a1afdc9c87181cb33f82c4ba2",
            "title": "Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks",
            "abstract": "Although coherence modeling has come a long way in developing novel models, their evaluation on downstream applications for which they are purportedly developed has largely been neglected. With the advancements made by neural approaches in applications such as machine translation (MT), summarization and dialog systems, the need for coherence evaluation of these tasks is now more crucial than ever. However, coherence models are typically evaluated only on synthetic tasks, which may not be representative of their performance in downstream applications. To investigate how representative the synthetic tasks are of downstream use cases, we conduct experiments on benchmarking well-known traditional and neural coherence models on synthetic sentence ordering tasks, and contrast this with their performance on three downstream applications: coherence evaluation for MT and summarization, and next utterance prediction in retrieval-based dialog. Our results demonstrate a weak correlation between the model performances in the synthetic tasks and the downstream applications, motivating alternate training and evaluation methods for coherence models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "6838342",
                    "name": "Tasnim Mohiuddin"
                },
                {
                    "authorId": "35640774",
                    "name": "Prathyusha Jwalapuram"
                },
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "0dcaae82ea7e49e084d5a872cd0d2178db1d8c99",
            "title": "Hierarchical Pointer Net Parsing",
            "abstract": "Transition-based top-down parsing with pointer networks has achieved state-of-the-art results in multiple parsing tasks, while having a linear time complexity. However, the decoder of these parsers has a sequential structure, which does not yield the most appropriate inductive bias for deriving tree structures. In this paper, we propose hierarchical pointer network parsers, and apply them to dependency and sentence-level discourse parsing tasks. Our results on standard benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods and setting a new state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145314839",
                    "name": "Linlin Liu"
                },
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "3226782",
                    "name": "Simeng Han"
                },
                {
                    "authorId": "1996394",
                    "name": "Lidong Bing"
                }
            ]
        },
        {
            "paperId": "a1f449ed8912d08876049bdbbb60d0b8a3416c33",
            "title": "A Unified Linear-Time Framework for Sentence-Level Discourse Parsing",
            "abstract": "We propose an efficient neural framework for sentence-level discourse analysis in accordance with Rhetorical Structure Theory (RST). Our framework comprises a discourse segmenter to identify the elementary discourse units (EDU) in a text, and a discourse parser that constructs a discourse tree in a top-down fashion. Both the segmenter and the parser are based on Pointer Networks and operate in linear time. Our segmenter yields an F1 score of 95.4%, and our parser achieves an F1 score of 81.7% on the aggregated labeled (relation) metric, surpassing previous approaches by a good margin and approaching human agreement on both tasks (98.3 and 83.0 F1).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "35640774",
                    "name": "Prathyusha Jwalapuram"
                },
                {
                    "authorId": "31773000",
                    "name": "M Saiful Bari"
                }
            ]
        },
        {
            "paperId": "d0144c692777bb7ae73aaa15c7114289e468e801",
            "title": "Resurrecting Submodularity in Neural Abstractive Summarization",
            "abstract": "Submodularity is a desirable property for a variety of objectives in summarization in terms of content selection where the encode-decoder framework is deficient. We propose `diminishing attentions', a class of novel attention mechanisms that are architecturally simple yet empirically effective to improve the coverage of neural abstractive summarization by exploiting the properties of submodular functions. Without adding any extra parameters to the Pointer-Generator baseline, our attention mechanism yields significant improvements in ROUGE scores and generates summaries of better quality. Our method within the Pointer-Generator framework outperforms the recently proposed Transformer model for summarization while using only 5 times less parameters. Our method also achieves state-of-the-art results in abstractive summarization when applied to the encoder-decoder attention in the Transformer model initialized with BERT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3226782",
                    "name": "Simeng Han"
                },
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        }
    ]
}