{
    "authorId": "49365095",
    "papers": [
        {
            "paperId": "86e0fa0ae02d40d6310fe7897b8b8bbb429101a4",
            "title": "Substance or Style: What Does Your Image Embedding Know?",
            "abstract": "Probes are small networks that predict properties of underlying data from embeddings, and they provide a targeted, effective way to illuminate the information contained in embeddings. While analysis through the use of probes has become standard in NLP, there has been much less exploration in vision. Image foundation models have primarily been evaluated for semantic content. Better understanding the non-semantic information in popular embeddings (e.g., MAE, SimCLR, or CLIP) will shed new light both on the training algorithms and on the uses for these foundation models. We design a systematic transformation prediction task and measure the visual content of embeddings along many axes, including image style, quality, and a range of natural and artificial transformations. Surprisingly, six embeddings (including SimCLR) encode enough non-semantic information to identify dozens of transformations. We also consider a generalization task, where we group similar transformations and hold out several for testing. We find that image-text models (CLIP and ALIGN) are better at recognizing new examples of style transfer than masking-based models (CAN and MAE). Overall, our results suggest that the choice of pre-training algorithm impacts the types of information in the embedding, and certain models are better than others for non-semantic downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3125805",
                    "name": "Cyrus Rashtchian"
                },
                {
                    "authorId": "144382278",
                    "name": "Charles Herrmann"
                },
                {
                    "authorId": "3340602",
                    "name": "Chun-Sung Ferng"
                },
                {
                    "authorId": "38534744",
                    "name": "Ayan Chakrabarti"
                },
                {
                    "authorId": "1707347",
                    "name": "Dilip Krishnan"
                },
                {
                    "authorId": "3232265",
                    "name": "Deqing Sun"
                },
                {
                    "authorId": "50270386",
                    "name": "Da-Cheng Juan"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                }
            ]
        },
        {
            "paperId": "a17c973490fba187c0cad5889df6de88cd44a37c",
            "title": "ACM WSDM 2023 Report",
            "abstract": "The 16th ACM International Conference on Web Search and Data Mining (WSDM 2023) was successfully held on 2/27 - 3/3/2023 at Carlton Hotel, Singapore. It was held as in-person conference that also featured rich virtual elements. The five-day event began with tutorials and Industry Day. The middle three days were dedicated to keynote speeches, single-session presentations, poster reception, and demo sessions. The final day was event-filled with workshops, WSDM Cup Day, Doctoral Consortium, and Smart City Day. This brief report provides an overview of WSDM 2023 with organization and program details and statistics from Conference Chairs and Program Committee Chairs, as well as a message from the WSDM Steering Committee Chair. Further details can be found at https://www.wsdm-conference.org/2023/. At the conclusion of this report, we express our deep gratitude to authors, volunteers, special interest groups (SIGs), steering committee, sponsors and supporters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3309003",
                    "name": "Hady W. Lauw"
                },
                {
                    "authorId": "144078686",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2221129113",
                    "name": "Luo Si"
                },
                {
                    "authorId": "1839624",
                    "name": "Evimaria Terzi"
                },
                {
                    "authorId": "1701195",
                    "name": "Panayiotis Tsaparas"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                }
            ]
        },
        {
            "paperId": "3df333c386f3fbf1b206d79dfc7052cece8fc160",
            "title": "ACM WSDM 2022 report",
            "abstract": "The 15th ACM International Conference on Web Search and Data Mining (WSDM 2022) was successfully held on 2/21 - 2/25/2022 online. It was originally planned to be held in Tempe, Arizona and changed to a virtual format due to the surge of Omicron. It was a five-day event with the middle 3 days dedicated to keynote speeches, single-session presentations, poster sessions, and sponsor talks. This brief report provides an overview of WSDM 2022 with organization and program details and statistics from Conference Chairs and Program Committee Chairs, as well as a message from the WSDM Steering Committee Chair to call for future WSDM conferences in the US (2024) and Europe (2025). Further details can be found at https://www.wsdm-conference.org/2022/. At the end of this report, we express our deep gratitude to authors, volunteers, special interest groups (SIGs), steering committee, and sponsors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720972",
                    "name": "K. Candan"
                },
                {
                    "authorId": "38746648",
                    "name": "Huan Liu"
                },
                {
                    "authorId": "3255268",
                    "name": "L. Akoglu"
                },
                {
                    "authorId": "2143918656",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                }
            ]
        },
        {
            "paperId": "b1f10c0c9739ee65f7de00c99cd188928a4fba90",
            "title": "An Efficient Simulation-Based Travel Demand Calibration Algorithm for Large-Scale Metropolitan Traffic Models",
            "abstract": "Metropolitan scale vehicular traffic modeling is used by a variety of private and public sector urban mobility stakeholders to inform the design and operations of road networks. High-resolution stochastic traffic simulators are increasingly used to describe detailed demand-supply interactions. The design of efficient calibration techniques remains a major challenge. This paper considers a class of high-dimensional calibration problems known as origin-destination (OD) calibration. We formulate the problem as a continuous simulation-based optimization problem. Our proposed algorithm builds upon recent metamodel methods that tackle the simulation-based problem by solving a sequence of approximate analytical optimization problems, which rely on the use of analytical network models. In this paper, we formulate a network model defined as a system of linear equations, the dimension of which scales linearly with the number of roads with field data and independently of the dimension of the route choice set. This makes the approach suitable for large-scale metropolitan networks. The approach has enhanced efficiency compared with past metamodel formulations that are based on systems of nonlinear, rather than linear, equations. It also has enhanced efficiency compared to traditional calibration methods that resort to simulation-based estimates of traffic assignment matrices, while the proposed approach uses analytical approximations of these matrices. We benchmark the approach considering a peak period Salt Lake City case study and calibrate based on field vehicular count data. The new formulation yields solutions with good performance and is suitable for large-scale road networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50780571",
                    "name": "N. Arora"
                },
                {
                    "authorId": "2109142265",
                    "name": "Yi-Fan Chen"
                },
                {
                    "authorId": "144113440",
                    "name": "S. Ganapathy"
                },
                {
                    "authorId": "2110766528",
                    "name": "Yechen Li"
                },
                {
                    "authorId": "1781337",
                    "name": "Ziheng Lin"
                },
                {
                    "authorId": "144319532",
                    "name": "C. Osorio"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                },
                {
                    "authorId": "150136774",
                    "name": "Iveel Tsogsuren"
                }
            ]
        },
        {
            "paperId": "baf7515d291ec434868e64f18e7e20aadfc4b27b",
            "title": "CARLS: Cross-platform Asynchronous Representation Learning System",
            "abstract": "In this work, we propose CARLS, a novel framework for augmenting the capacity of existing deep learning frameworks by enabling multiple components -- model trainers, knowledge makers and knowledge banks -- to concertedly work together in an asynchronous fashion across hardware platforms. The proposed CARLS is particularly suitable for learning paradigms where model training benefits from additional knowledge inferred or discovered during training, such as node embeddings for graph neural networks or reliable pseudo labels from model predictions. We also describe three learning paradigms -- semi-supervised learning, curriculum learning and multimodal learning -- as examples that can be scaled up efficiently by CARLS. One version of CARLS has been open-sourced and available for download at: https://github.com/tensorflow/neural-structured-learning/tree/master/research/carls",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2828701",
                    "name": "Chun-Ta Lu"
                },
                {
                    "authorId": "2111186994",
                    "name": "Yun Zeng"
                },
                {
                    "authorId": "50270386",
                    "name": "Da-Cheng Juan"
                },
                {
                    "authorId": "2051969653",
                    "name": "Yicheng Fan"
                },
                {
                    "authorId": "2144241452",
                    "name": "Zhe Li"
                },
                {
                    "authorId": "16026105",
                    "name": "Jan Dlabal"
                },
                {
                    "authorId": "2109060286",
                    "name": "Yi-Ting Chen"
                },
                {
                    "authorId": "2064867189",
                    "name": "Arjun Gopalan"
                },
                {
                    "authorId": "3283826",
                    "name": "Allan Heydon"
                },
                {
                    "authorId": "3340602",
                    "name": "Chun-Sung Ferng"
                },
                {
                    "authorId": "2105843952",
                    "name": "Reah Miyara"
                },
                {
                    "authorId": "2270670157",
                    "name": "Ariel Fuxman"
                },
                {
                    "authorId": "73777526",
                    "name": "Futang Peng"
                },
                {
                    "authorId": "2110121852",
                    "name": "Zhen Li"
                },
                {
                    "authorId": "2066508193",
                    "name": "Tom Duerig"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                }
            ]
        },
        {
            "paperId": "0951bc326fc316132428f5fd7485a6ce30cf52cf",
            "title": "Surprise: Result List Truncation via Extreme Value Theory",
            "abstract": "Work in information retrieval has largely been centered around ranking and relevance: given a query, return some number of results ordered by relevance to the user. The problem of result list truncation, or where to truncate the ranked list of results, however, has received less attention despite being crucial in a variety of applications. Such truncation is a balancing act between the overall relevance, or usefulness of the results, with the user cost of processing more results. Result list truncation can be challenging because relevance scores are often not well-calibrated. This is particularly true in large-scale IR systems where documents and queries are embedded in the same metric space and a query's nearest document neighbors are returned during inference. Here, relevance is inversely proportional to the distance between the query and candidate document, but what distance constitutes relevance varies from query to query and changes dynamically as more documents are added to the index. In this work, we propose Surprise scoring, a statistical method that leverages the Generalized Pareto Distribution that arises in extreme value theory to produce interpretable and calibrated relevance scores at query time using nothing more than the ranked scores. We demonstrate its effectiveness on the result list truncation task across image, text, and IR datasets and compare it to both classical and recent baselines. We draw connections to hypothesis testing and p-values.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "11774695",
                    "name": "Dara Bahri"
                },
                {
                    "authorId": "72117196",
                    "name": "Che Zheng"
                },
                {
                    "authorId": "144447820",
                    "name": "Yi Tay"
                },
                {
                    "authorId": "1680617",
                    "name": "Donald Metzler"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                }
            ]
        },
        {
            "paperId": "0f5e11d1ec5960be06e3d7f59b796c25edf824eb",
            "title": "Graph Autoencoders with Deconvolutional Networks",
            "abstract": "Recent studies have indicated that Graph Convolutional Networks (GCNs) act as a \\emph{low pass} filter in spectral domain and encode smoothed node representations. In this paper, we consider their opposite, namely Graph Deconvolutional Networks (GDNs) that reconstruct graph signals from smoothed node representations. We motivate the design of Graph Deconvolutional Networks via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a \\emph{high pass} filter and may amplify the noise. Based on the proposed GDN, we further propose a graph autoencoder framework that first encodes smoothed graph representations with GCN and then decodes accurate graph signals with GDN. We demonstrate the effectiveness of the proposed method on several tasks including unsupervised graph-level representation , social recommendation and graph generation",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118372693",
                    "name": "Jia Li"
                },
                {
                    "authorId": "2117902572",
                    "name": "Tomas Yu"
                },
                {
                    "authorId": "50270386",
                    "name": "Da-Cheng Juan"
                },
                {
                    "authorId": "2064867189",
                    "name": "Arjun Gopalan"
                },
                {
                    "authorId": "144488755",
                    "name": "Hong Cheng"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                }
            ]
        },
        {
            "paperId": "2961ba3cffb20b1f1d6db0ecd2d632b46735c2ab",
            "title": "Reverse Engineering Configurations of Neural Text Generation Models",
            "abstract": "Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways. It is therefore desirable to develop a deeper understanding of the fundamental properties of such models. The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area. To this end, the extent and degree to which these artifacts surface in generated text is still unclear. In the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated some piece of text. Specifically, we conduct an extensive suite of diagnostic tests to observe whether modeling choices (e.g., sampling methods, top-k probabilities, model architectures, etc.) leave detectable artifacts in the text they generate. Our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone. This suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144447820",
                    "name": "Yi Tay"
                },
                {
                    "authorId": "11774695",
                    "name": "Dara Bahri"
                },
                {
                    "authorId": "72117196",
                    "name": "Che Zheng"
                },
                {
                    "authorId": "3202382",
                    "name": "Clifford Brunk"
                },
                {
                    "authorId": "1680617",
                    "name": "Donald Metzler"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                }
            ]
        },
        {
            "paperId": "477037aa3a0fe4a25e48120f8f3a9e99360ae087",
            "title": "BusTr: Predicting Bus Travel Times from Real-Time Traffic",
            "abstract": "We present BusTr, a machine-learned model for translating road traffic forecasts into predictions of bus delays, used by Google Maps to serve the majority of the world's public transit systems where no official real-time bus tracking is provided. We demonstrate that our neural sequence model improves over DeepTTE, the state-of-the-art baseline, both in performance (-30% MAPE) and training stability. We also demonstrate significant generalization gains over simpler models, evaluated on longitudinal data to cope with a constantly evolving world.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145511936",
                    "name": "Richard Barnes"
                },
                {
                    "authorId": "2620680",
                    "name": "S. Buthpitiya"
                },
                {
                    "authorId": "2115139432",
                    "name": "James Cook"
                },
                {
                    "authorId": "1983135",
                    "name": "Alex Fabrikant"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                },
                {
                    "authorId": "2534225",
                    "name": "Fangzhou Xu"
                }
            ]
        },
        {
            "paperId": "727e89a98628d0935a339e0c4d328c8c6307cdb5",
            "title": "Adversarial Robustness Across Representation Spaces",
            "abstract": "Adversarial robustness corresponds to the susceptibility of deep neural networks to imperceptible perturbations made at test time. In the context of image tasks, many algorithms have been proposed to make neural networks robust to adversarial perturbations made to the input pixels. These perturbations are typically measured in an \u2113p norm. However, robustness often holds only for the specific attack used for training. In this work we extend the above setting to consider the problem of training of deep neural networks that can be made simultaneously robust to perturbations applied in multiple natural representations spaces. For the case of image data, examples include the standard pixel representation as well as the representation in the discrete cosine transform (DCT) basis. We design a theoretically sound algorithm with formal guarantees for the above problem. Furthermore, our guarantees also hold when the goal is to require robustness with respect to multiple \u2113p norm based attacks. We then derive an efficient practical implementation and demonstrate the effectiveness of our approach on standard datasets for image classification.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144030228",
                    "name": "Pranjal Awasthi"
                },
                {
                    "authorId": "2110954041",
                    "name": "George Yu"
                },
                {
                    "authorId": "3340602",
                    "name": "Chun-Sung Ferng"
                },
                {
                    "authorId": "49365095",
                    "name": "A. Tomkins"
                },
                {
                    "authorId": "50270386",
                    "name": "Da-Cheng Juan"
                }
            ]
        }
    ]
}