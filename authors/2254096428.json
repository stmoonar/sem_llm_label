{
    "authorId": "2254096428",
    "papers": [
        {
            "paperId": "2755b231c6a00a67c8a4bfad4efa847b897dba33",
            "title": "HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for Long-Term Forecasting",
            "abstract": "Time series forecasting is crucial and challenging in the real world. The recent surge in interest regarding time series foundation models, which cater to a diverse array of downstream tasks, is noteworthy. However, existing methods often over-look the multi-scale nature of time series, an aspect crucial for precise forecasting. To bridge this gap, we propose HiMTM, a hierarchical multi-scale masked time series modeling method designed for long-term forecasting. Specifically, it comprises four integral components: (1) hierarchical multi-scale transformer (HMT) to capture temporal information at different scales; (2) decoupled encoder-decoder (DED) forces the encoder to focus on feature extraction, while the decoder to focus on pretext tasks; (3) multi-scale masked reconstruction (MMR) provides multi-stage supervision signals for pre-training; (4) cross-scale attention fine-tuning (CSA-FT) to capture dependencies be-tween different scales for forecasting. Collectively, these components enhance multi-scale feature extraction capabilities in masked time series modeling and contribute to improved prediction accuracy. We conduct extensive experiments on 7 main-stream datasets to prove that HiMTM has obvious advantages over contemporary self-supervised and end-to-end learning methods. The effectiveness of HiMTM is further showcased by its application in the industry of natural gas demand forecasting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2044475378",
                    "name": "Shubao Zhao"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2153615411",
                    "name": "Zhaoxiang Hou"
                },
                {
                    "authorId": "2051970229",
                    "name": "Che-Sheng Yang"
                },
                {
                    "authorId": "2155450576",
                    "name": "Zengxiang Li"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2259354682",
                    "name": "Yi Wang"
                }
            ]
        },
        {
            "paperId": "2cb3044ef42c7ee022a988864028b80ce977072c",
            "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts",
            "abstract": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2322999629",
                    "name": "Xiaoming Shi"
                },
                {
                    "authorId": "2322609248",
                    "name": "Shiyu Wang"
                },
                {
                    "authorId": "2300149940",
                    "name": "Yuqi Nie"
                },
                {
                    "authorId": "2322812784",
                    "name": "Dianqi Li"
                },
                {
                    "authorId": "2323439377",
                    "name": "Zhou Ye"
                },
                {
                    "authorId": "2312901184",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                }
            ]
        },
        {
            "paperId": "ade46150fbb93b4e473f2fafbe39dfbb3346ee94",
            "title": "A Survey on Diffusion Models for Time Series and Spatio-Temporal Data",
            "abstract": "The study of time series is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2303519564",
                    "name": "Yiyuan Yang"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2262445381",
                    "name": "Haomin Wen"
                },
                {
                    "authorId": "2152737103",
                    "name": "Chaoli Zhang"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2253908414",
                    "name": "Lintao Ma"
                },
                {
                    "authorId": "2259354682",
                    "name": "Yi Wang"
                },
                {
                    "authorId": "2275745358",
                    "name": "Cheng-Ming Liu"
                },
                {
                    "authorId": "2298971233",
                    "name": "Bin Yang"
                },
                {
                    "authorId": "2298925809",
                    "name": "Zenglin Xu"
                },
                {
                    "authorId": "2298896985",
                    "name": "Jiang Bian"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "c30abb2ad76fcbfd4e6dc8881850b591d3434a3e",
            "title": "Foundation Models for Time Series Analysis: A Tutorial and Survey",
            "abstract": "Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advances in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored for time series analysis. This survey aims to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either application or pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future exploration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2262445381",
                    "name": "Haomin Wen"
                },
                {
                    "authorId": "2300149940",
                    "name": "Yuqi Nie"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2292142150",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "d622d8b2d5adc4b638a73b105686840e264dfb8f",
            "title": "Position: What Can Large Language Models Tell Us about Time Series Analysis",
            "abstract": "Time series analysis is essential for comprehending the complexities inherent in various realworld systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2304517120",
                    "name": "Yifan Zhang"
                },
                {
                    "authorId": "2301489384",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2304920924",
                    "name": "Kexin Zhang"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2298971233",
                    "name": "Bin Yang"
                },
                {
                    "authorId": "2304515949",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2303259263",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "d945eae22457648e191742c37bcad88a114ce703",
            "title": "HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling with Self-Distillation for Long-Term Forecasting",
            "abstract": "Time series forecasting is a critical and challenging task in practical application. Recent advancements in pre-trained foundation models for time series forecasting have gained significant interest. However, current methods often overlook the multi-scale nature of time series, which is essential for accurate forecasting. To address this, we propose HiMTM, a hierarchical multi-scale masked time series modeling with self-distillation for long-term forecasting. HiMTM integrates four key components: (1) hierarchical multi-scale transformer (HMT) to capture temporal information at different scales; (2) decoupled encoder-decoder (DED) that directs the encoder towards feature extraction while the decoder focuses on pretext tasks; (3) hierarchical self-distillation (HSD) for multi-stage feature-level supervision signals during pre-training; and (4) cross-scale attention fine-tuning (CSA-FT) to capture dependencies between different scales for downstream tasks. These components collectively enhance multi-scale feature extraction in masked time series modeling, improving forecasting accuracy. Extensive experiments on seven mainstream datasets show that HiMTM surpasses state-of-the-art self-supervised and end-to-end learning methods by a considerable margin of 3.16-68.54\\%. Additionally, HiMTM outperforms the latest robust self-supervised learning method, PatchTST, in cross-domain forecasting by a significant margin of 2.3\\%. The effectiveness of HiMTM is further demonstrated through its application in natural gas demand forecasting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2044475378",
                    "name": "Shubao Zhao"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2153615411",
                    "name": "Zhaoxiang Hou"
                },
                {
                    "authorId": "2051970229",
                    "name": "Che-Sheng Yang"
                },
                {
                    "authorId": "2155450576",
                    "name": "Zengxiang Li"
                },
                {
                    "authorId": "2248198673",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2259354682",
                    "name": "Yi Wang"
                }
            ]
        },
        {
            "paperId": "f3ca1515d610ec8811d3b4f9a209eae099523048",
            "title": "Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective",
            "abstract": "In long-term time series forecasting (LTSF) tasks, an increasing number of models have acknowledged that discrete time series originate from continuous dynamic systems and have attempted to model their dynamical structures. Recognizing the chaotic nature of real-world data, our model, \\textbf{\\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes non-parametric Phase Space Reconstruction embedding and the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets with only one-twelfth of the parameters compared to PatchTST.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2284730172",
                    "name": "Jiaxi Hu"
                },
                {
                    "authorId": "2284735864",
                    "name": "Yuehong Hu"
                },
                {
                    "authorId": "2262453292",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                }
            ]
        },
        {
            "paperId": "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
            "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
            "abstract": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2255363760",
                    "name": "Shiyu Wang"
                },
                {
                    "authorId": "2253908414",
                    "name": "Lintao Ma"
                },
                {
                    "authorId": "2237992280",
                    "name": "Zhixuan Chu"
                },
                {
                    "authorId": "2253786576",
                    "name": "James Y. Zhang"
                },
                {
                    "authorId": "2119204984",
                    "name": "X. Shi"
                },
                {
                    "authorId": "2254173316",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2256011160",
                    "name": "Yuan-Fang Li"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "5b038c1a93967072cc76689fd805e756f804cc42",
            "title": "Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook",
            "abstract": "Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to equip practitioners with the knowledge to develop applications and further research in this underexplored domain. We primarily categorize the existing literature into two major clusters: large models for time series analysis (LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further classify research based on model scopes (i.e., general vs. domain-specific) and application areas/tasks. We also provide a comprehensive collection of pertinent resources, including datasets, model assets, and useful tools, categorized by mainstream applications. This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2152737103",
                    "name": "Chaoli Zhang"
                },
                {
                    "authorId": "2149919635",
                    "name": "Siqiao Xue"
                },
                {
                    "authorId": "2258781263",
                    "name": "Xue Wang"
                },
                {
                    "authorId": "2253786576",
                    "name": "James Y. Zhang"
                },
                {
                    "authorId": "2259354682",
                    "name": "Yi Wang"
                },
                {
                    "authorId": "2259822635",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2284734617",
                    "name": "Xiaoli Li"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2258722690",
                    "name": "Vincent S. Tseng"
                },
                {
                    "authorId": "2149514422",
                    "name": "Yu Zheng"
                },
                {
                    "authorId": "2260428467",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2258712531",
                    "name": "Hui Xiong"
                }
            ]
        }
    ]
}