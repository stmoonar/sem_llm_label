{
    "authorId": "136806596",
    "papers": [
        {
            "paperId": "945c6856e64226440915250125ecf79b7540b6da",
            "title": "A Self-Encoder for Learning Nearest Neighbors",
            "abstract": "We present the self-encoder, a neural network trained to guess the identity of each data sample. Despite its simplicity, it learns a very useful representation of data, in a self-supervised way. Specifically, the self-encoder learns to distribute the data samples in the embedding space so that they are linearly separable from one another. This induces a geometry where two samples are close in the embedding space when they are not easy to differentiate. The self-encoder can then be combined with a nearest-neighbor classifier or regressor for any subsequent supervised task. Unlike regular nearest neighbors, the predictions resulting from this encoding of data are invariant to any scaling of features, making any preprocessing like min-max scaling not necessary. The experiments show the efficiency of the approach, especially on heterogeneous data mixing numerical features and categorical features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "136806596",
                    "name": "Armand Boschin"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                },
                {
                    "authorId": "10836493",
                    "name": "Marc Jeanmougin"
                }
            ]
        },
        {
            "paperId": "87852047f1b76c66038d383cb2ebbe99423bc970",
            "title": "Combining Embeddings and Rules for Fact Prediction (Invited Paper)",
            "abstract": "Knowledge bases are typically incomplete, meaning that they are missing information that we would expect to be there. Recent years have seen two main approaches to guess missing facts: Rule Mining and Knowledge Graph Embeddings. The first approach is symbolic",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "136806596",
                    "name": "Armand Boschin"
                },
                {
                    "authorId": "1772598",
                    "name": "Nitisha Jain"
                },
                {
                    "authorId": "2166240706",
                    "name": "Gurami Keretchashvili"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "9fa28468f15daf74659e5ea91782a5f4fcabf819",
            "title": "Enriching Wikidata with Semantified Wikipedia Hyperlinks",
            "abstract": ". We propose a novel approach to enrich Wikidata with the textual content of Wikipedia. Speci\ufb01cally, we leverage knowledge graph (KG) embedding models to classify the hyperlinks between Wikipedia articles and predict the corresponding facts. For instance, we would like to complete the triple ( Berlin , *, Germany ) with the relation capital of , given a hyperlink from Berlin to Germany in Wikipedia. While existing KG embedding models can be used for this task of relation prediction, they were not explicitly designed for it and their performance is not satisfactory. In this paper, we propose two methods that greatly improve the performance of these models on this task: \ufb01rst, a new negative sampling method that balances the roles of entities and relations during training; second, a method to exploit the types of entities in the selection of candidate relations. We obtain accuracy scores as high as 94% on the popular FB15k237 dataset and 75% on WDV5, an extraction of Wikidata. The e\ufb03ciency of the approach is illustrated on some Wikipedia pages, where new facts unknown to Wikidata are predicted by our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "136806596",
                    "name": "Armand Boschin"
                },
                {
                    "authorId": "1774132",
                    "name": "T. Bonald"
                }
            ]
        },
        {
            "paperId": "46b5198a535dfcaf1cc7d57d471ad9ec050e46cf",
            "title": "TorchKGE: Knowledge Graph Embedding in Python and PyTorch",
            "abstract": "TorchKGE is a Python module for knowledge graph (KG) embedding relying solely on PyTorch. This package provides researchers and engineers with a clean and efficient API to design and test new models. It features a KG data structure, simple model interfaces and modules for negative sampling and model evaluation. Its main strength is a very fast evaluation module for the link prediction task, a central application of KG embedding. Various KG embedding models are also already implemented. Special attention has been paid to code efficiency and simplicity, documentation and API consistency. It is distributed using PyPI under BSD license. Source code and pointers to documentation and deployment can be found at this https URL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "136806596",
                    "name": "Armand Boschin"
                }
            ]
        },
        {
            "paperId": "bc85a73f4174c543a1e57359db4c38457f3d82b7",
            "title": "WikiDataSets : Standardized sub-graphs from WikiData",
            "abstract": "Developing new ideas and algorithms in the fields of graph processing and relational learning requires public datasets. While Wikidata is the largest open source knowledge graph, involving more than fifty million entities, it is larger than needed in many cases and even too large to be processed easily. Still, it is a goldmine of relevant facts and relations. Using this knowledge graph is time consuming and prone to task specific tuning which can affect reproducibility of results. Providing a unified framework to extract topic-specific subgraphs solves this problem and allows researchers to evaluate algorithms on common datasets. This paper presents various topic-specific subgraphs of Wikidata along with the generic Python code used to extract them. These datasets can help develop new methods of knowledge graph processing and relational learning.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "136806596",
                    "name": "Armand Boschin"
                }
            ]
        }
    ]
}