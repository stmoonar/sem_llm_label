{
    "authorId": "1699159",
    "papers": [
        {
            "paperId": "06acfd640246ed6fa41855a6750d1cec588846e6",
            "title": "BEDRF: Bidirectional Edge Diffraction Response Function for Interactive Sound Propagation",
            "abstract": "We introduce bidirectional edge diffraction response function (BEDRF), a new approach to model wave diffraction around edges with path tracing. The diffraction part of the wave is expressed as an integration on path space, and the wave-edge interaction is expressed using only the localized information around points on the edge similar to a bidirectional scattering distribution function (BSDF) for visual rendering. For an infinite single wedge, our model generates the same result as the analytic solution. Our approach can be easily integrated into interactive geometric sound propagation algorithms that use path tracing to compute specular and diffuse reflections. Our resulting propagation algorithm can approximate complex wave propagation phenomena involving high-order diffraction, and is able to handle dynamic, deformable objects and moving sources and listeners. We highlight the performance of our approach in different scenarios to generate smooth auralization.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2064672792",
                    "name": "Chunxiao Cao"
                },
                {
                    "authorId": "2152222348",
                    "name": "Zili An"
                },
                {
                    "authorId": "143815979",
                    "name": "Zhong Ren"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "1423651904",
                    "name": "Kun Zhou"
                }
            ]
        },
        {
            "paperId": "0c1a5211f2d7b454526c8b78edb433f21f7cda1d",
            "title": "Dynamic EM Ray Tracing for Large Urban Scenes with Multiple Receivers",
            "abstract": "Radio applications are increasingly being used in urban environments for cellular radio systems and safety applications that use vehicle-vehicle, and vehicle-to-infrastructure. We present a novel ray tracing-based radio propagation algorithm that can handle large urban scenes with hundreds or thousands of dynamic objects and receivers. Our approach is based on the use of coherence-based techniques that exploit spatial and temporal coherence for efficient wireless propagation and radio network planning. Our formulation also utilizes channel coherence which is used to determine the effectiveness of the propagation model within a certain time in dynamically generated paths; and spatial consistency which is used to estimate the similarity and accuracy of changes in a dynamic environment with varying propagation models and blocking obstacles. We highlight the performance of our simulator in large urban traffic scenes with an area of $2 \\ast 2 km ^{2}$ and more than 10,000 users and devices. We evaluate the accuracy by comparing the results with discrete model simulations performed using WinProp. In practice, our approach scales linearly with the area of the urban environment and the number of dynamic obstacles or receivers.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2258928756",
                    "name": "Ruichen Wang"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "30bd42c1ea5d41d4976a3011d7f6d98d2f423169",
            "title": "Listen2Scene: Interactive material-aware binaural sound propagation for reconstructed 3D scenes",
            "abstract": "We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for indoor 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network can handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function for the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acoustic effect in 0.1 milliseconds on an NVIDIA GeForce RTX 2080 Ti GPU. We have evaluated the accuracy of our approach with binaural acoustic effects generated using an interactive geometric sound propagation algorithm and captured real acoustic effects /real-world recordings. We also performed a perceptual evaluation and observed that the audio rendered by our approach is more plausible than audio rendered using prior learning-based and geometric-based sound propagation algorithms. We quantitatively evaluated the accuracy of our approach using statistical acoustic parameters, and energy decay curves. The demo videos, code and dataset are available online1.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "67106893",
                    "name": "Anton Ratnarajah"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "31714c09fe640e54f83d6a7fb2d022309ae0b0a8",
            "title": "Real-Time Decentralized Navigation of Nonholonomic Agents Using Shifted Yielding Areas",
            "abstract": "We present a lightweight, decentralized algorithm for navigating multiple nonholonomic agents through challenging environments with narrow passages. Our key idea is to allow agents to yield to each other in large open areas instead of narrow passages, to increase the success rate of conventional decentralized algorithms. At pre-processing time, our method computes a medial axis for the freespace. A reference trajectory is then computed and projected onto the medial axis for each agent. During run time, when an agent senses other agents moving in the opposite direction, our algorithm uses the medial axis to estimate a Point of Impact (POI) as well as the available area around the POI. If the area around the POI is not large enough for yielding behaviors to be successful, we shift the POI to nearby large areas by modulating the agent's reference trajectory and traveling speed. We evaluate our method on a row of 4 environments with up to 15 robots, and we find our method incurs a marginal computational overhead of 10\u201330 ms on average, achieving real-time performance. Afterward, our planned reference trajectories can be tracked using local navigation algorithms to achieve up to a 100% higher success rate over local navigation algorithms alone.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109334694",
                    "name": "Liang He"
                },
                {
                    "authorId": "1831485",
                    "name": "Zherong Pan"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "32524aa3ae8522542753ed7e6f4cca3970e4acab",
            "title": "Can an Embodied Agent Find Your \u201cCat-shaped Mug\u201d? LLM-Based Zero-Shot Object Navigation",
            "abstract": "We present language-guided exploration (LGX), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of large language models (LLMs) for this task by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1395945114",
                    "name": "Vishnu Sashank Dorbala"
                },
                {
                    "authorId": "2057888241",
                    "name": "James F. Mullen"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "3e14227862ae21ec691794b8586d6c695c5feb75",
            "title": "RE-MOVE: An Adaptive Policy Design for Robotic Navigation Tasks in Dynamic Environments via Language-Based Feedback",
            "abstract": "Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (REquest help and MOVE on) to adapt already trained policy to real-time changes in the environment without re-training via utilizing a language-based feedback. The proposed approach essentially boils down to addressing two main challenges of (1) when to ask for feedback and, if received, (2) how to incorporate feedback into trained policies. RE-MOVE incorporates an epistemic uncertainty-based framework to determine the optimal time to request instructions-based feedback. For the second challenge, we employ a zero-shot learning natural language processing (NLP) paradigm with efficient, prompt design and leverage state-of-the-art GPT-3.5, Llama-2 language models. To show the efficacy of the proposed approach, we performed extensive synthetic and real-world evaluations in several test-time dynamic navigation scenarios. Utilizing RE-MOVE result in up to 80% enhancement in the attainment of successful goals, coupled with a reduction of 13.50% in the normalized trajectory length, as compared to alternative approaches, particularly in demanding real-world environments with perceptual challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49081354",
                    "name": "Souradip Chakraborty"
                },
                {
                    "authorId": "123689410",
                    "name": "K. Weerakoon"
                },
                {
                    "authorId": "2126871763",
                    "name": "Prithvi Poddar"
                },
                {
                    "authorId": "2390456",
                    "name": "Pratap Tokekar"
                },
                {
                    "authorId": "3387859",
                    "name": "A. S. Bedi"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "47f47127ea50a4cc4fbd86afd132e50d532b28aa",
            "title": "AZTR: Aerial Video Action Recognition with Auto Zoom and Temporal Reasoning",
            "abstract": "We propose a novel approach for aerial video action recognition. Our method is designed for videos captured using UAVs and can run on edge or mobile devices. We present a learning-based approach that uses customized auto zoom to automatically identify the human target and scale it appropriately. This makes it easier to extract the key features and reduces the computational overhead. We also present an efficient temporal reasoning algorithm to capture the action information along the spatial and temporal domains within a controllable computational cost. Our approach has been implemented and evaluated both on the desktop with high-end GPUs and on the low power Robotics RB5 Platform for robots and drones. In practice, we achieve $6.1-7.4 \\%$ improvement over SOTA in Top-1 accuracy on the RoCoG-v2 dataset, 8.3-10.4% improvement on the UAV-Human dataset and 3.2% improvement on the Drone Action dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108203610",
                    "name": "Xijun Wang"
                },
                {
                    "authorId": "2210731669",
                    "name": "Ruiqi Xian"
                },
                {
                    "authorId": "94955378",
                    "name": "Tianrui Guan"
                },
                {
                    "authorId": "2147315384",
                    "name": "Celso M. de Melo"
                },
                {
                    "authorId": "50842268",
                    "name": "Stephen M. Nogar"
                },
                {
                    "authorId": "2718563",
                    "name": "Aniket Bera"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "4891844a910c25507f432f3d435930c57fa6c196",
            "title": "Human Trajectory Forecasting with Explainable Behavioral Uncertainty",
            "abstract": "Human trajectory forecasting helps to understand and predict human behaviors, enabling applications from social robots to self-driving cars, and therefore has been heavily investigated. Most existing methods can be divided into model-free and model-based methods. Model-free methods offer superior prediction accuracy but lack explainability, while model-based methods provide explainability but cannot predict well. Combining both methodologies, we propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM, where a behavior SDE model is combined with Bayesian neural networks (BNNs). While the NNs provide superior predictive power, the SDE offers strong explainability with quantifiable uncertainty in behavior and observation. We show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy, compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to drastically different scenes with different environments and crowd densities (~ 20 times higher than the testing data). Finally, BNSP-SFM can provide predictions with confidence to better explain potential causes of behaviors. The code will be released upon acceptance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2178356020",
                    "name": "Jiangbei Yue"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "2149698569",
                    "name": "He Wang"
                }
            ]
        },
        {
            "paperId": "501ba262e08d467d435a1bf75623726ecab7e556",
            "title": "CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition",
            "abstract": "We present CrossLoc3D, a novel 3D place recognition method that solves a large-scale point matching problem in a cross-source setting. Cross-source point cloud data corresponds to point sets captured by depth sensors with different accuracies or from different distances and perspectives. We address the challenges in terms of developing 3D place recognition methods that account for the representation gap between points captured by different sources. Our method handles cross-source data by utilizing multi-grained features and selecting convolution kernel sizes that correspond to most prominent features. Inspired by the diffusion models, our method uses a novel iterative refinement process that gradually shifts the embedding spaces from different sources to a single canonical space for better metric learning. In addition, we present CS-Campus3D, the first 3D aerial-ground cross-source dataset consisting of point cloud data from both aerial and ground LiDAR scans. The point clouds in CS-Campus3D have representation gaps and other features like different views, point densities, and noise patterns. We show that our CrossLoc3D algorithm can achieve an improvement of 4.74% - 15.37% in terms of the top 1 average recall on our CS-Campus3D benchmark and achieves performance comparable to state-of-the-art 3D place recognition method on the Oxford RobotCar. The code and CS-Campus3D benchmark will be available at github.com/rayguan97/crossloc3d.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94955378",
                    "name": "Tianrui Guan"
                },
                {
                    "authorId": "2213416066",
                    "name": "Aswath Muthuselvam"
                },
                {
                    "authorId": "2213420087",
                    "name": "Montana Hoover"
                },
                {
                    "authorId": "2108203610",
                    "name": "Xijun Wang"
                },
                {
                    "authorId": "2118673500",
                    "name": "Jing Liang"
                },
                {
                    "authorId": "73769017",
                    "name": "A. Sathyamoorthy"
                },
                {
                    "authorId": "3428879",
                    "name": "D. Conover"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "5bf7813028e533989dba3ba59d61f0e14fc0a8de",
            "title": "MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition",
            "abstract": "We present a novel approach for action recognition in UAV videos. Our formulation is designed to handle occlusion and viewpoint changes caused by the movement of a UAV. We use the concept of mutual information to compute and align the regions corresponding to human action or motion in the temporal domain. This enables our recognition model to learn from the key features associated with the motion. We also propose a novel frame sampling method that uses joint mutual information to acquire the most informative frame sequence in UAV videos. We have integrated our approach with X3D and evaluated the performance on multiple datasets. In practice, we achieve 18.9% improvement in Top-1 accuracy over current state-of-the-art methods on UAV-Human [30], 7.3% improvement on Drone-Action [41], and 7.16% improvement on NEC Drones [7]. The code is available at https://github.com/Ricky-Xian/MITFAS",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2210731669",
                    "name": "Ruiqi Xian"
                },
                {
                    "authorId": "2108203610",
                    "name": "Xijun Wang"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        }
    ]
}