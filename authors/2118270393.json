{
    "authorId": "2118270393",
    "papers": [
        {
            "paperId": "0c65dfacc858102af978debc10b56536fa186f20",
            "title": "Urania: Visualizing Data Analysis Pipelines for Natural Language-Based Data Exploration",
            "abstract": "Exploratory Data Analysis (EDA) is an essential yet tedious process for examining a new dataset. To facilitate it, natural language interfaces (NLIs) can help people intuitively explore the dataset via data-oriented questions. However, existing NLIs primarily focus on providing accurate answers to questions, with few offering explanations or presentations of the data analysis pipeline used to uncover the answer. Such presentations are crucial for EDA as they enhance the interpretability and reliability of the answer, while also helping users understand the analysis process and derive insights. To fill this gap, we introduce Urania, a natural language interactive system that is able to visualize the data analysis pipelines used to resolve input questions. It integrates a natural language interface that allows users to explore data via questions, and a novel data-aware question decomposition algorithm that resolves each input question into a data analysis pipeline. This pipeline is visualized in the form of a datamation, with animated presentations of analysis operations and their corresponding data changes. Through two quantitative experiments and expert interviews, we demonstrated that our data-aware question decomposition algorithm outperforms the state-of-the-art technique in terms of execution accuracy, and that Urania can help people explore datasets better. In the end, we discuss the observations from the studies and the potential future works.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118270393",
                    "name": "Yi Guo"
                },
                {
                    "authorId": "2059201609",
                    "name": "Nana Cao"
                },
                {
                    "authorId": "47099153",
                    "name": "Xiaoyu Qi"
                },
                {
                    "authorId": "144911687",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2064971223",
                    "name": "Danqing Shi"
                },
                {
                    "authorId": "2155699322",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2152520175",
                    "name": "Qing Chen"
                },
                {
                    "authorId": "69863469",
                    "name": "D. Weiskopf"
                }
            ]
        },
        {
            "paperId": "21ad0d9f734cf88f387eea01bda14ca9448219e7",
            "title": "MusicJam: Visualizing Music Insights via Generated Narrative Illustrations",
            "abstract": "Visualizing the insights of the invisible music is able to bring listeners an enjoyable and immersive listening experience, and therefore has attracted much attention in the field of information visualization. Over the past decades, various music visualization techniques have been introduced. However, most of them are manually designed by following the visual encoding rules, thus shown in form of a graphical visual representation whose visual encoding schema is usually taking effort to understand. Recently, some researchers use figures or illustrations to represent music moods, lyrics, and musical features, which are more intuitive and attractive. However, in these techniques, the figures are usually pre-selected or statically generated, so they cannot precisely convey insights of different pieces of music. To address this issue, in this paper, we introduce MusicJam, a music visualization system that is able to generate narrative illustrations to represent the insight of the input music. The system leverages a novel generation model designed based on GPT-2 to generate meaningful lyrics given the input music and then employs the stable diffusion model to transform the lyrics into coherent illustrations. Finally, the generated results are synchronized and rendered as an MP4 video accompanied by the input music. We evaluated the proposed lyric generation model by comparing it to the baseline models and conducted a user study to estimate the quality of the generated illustrations and the final music videos. The results showed the power of our technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2203326901",
                    "name": "Chuer Chen"
                },
                {
                    "authorId": "2163708412",
                    "name": "Nan Cao"
                },
                {
                    "authorId": "2152079717",
                    "name": "Jiani Hou"
                },
                {
                    "authorId": "2118270393",
                    "name": "Yi Guo"
                },
                {
                    "authorId": "2108095969",
                    "name": "Yulei Zhang"
                },
                {
                    "authorId": "2119036803",
                    "name": "Yang Shi"
                }
            ]
        },
        {
            "paperId": "4e10e2e77d2dee62bc8496adc4753b973ce497ce",
            "title": "Datamator: An Intelligent Authoring Tool for Creating Datamations via Data Query Decomposition",
            "abstract": "Datamation is designed to animate an analysis pipeline step by step, which is an intuitive and effective way to interpret the results from data analysis. However, creating a datamation is not easy. A qualified datamation needs to not only provide a correct analysis result but also ensure that the data flow and animation are coherent. Existing animation authoring tools focus on either leveraging algorithms to automatically generate an animation based on user-provided charts or building graphical user interfaces to provide a programming-free authoring environment for users. None of them are able to help users translate an analysis task into a series of data operations to form an analysis pipeline and visualize them as a datamation. To fill this gap, we introduce Datamator, an intelligent authoring tool developed to support datamation design and generation. It leverages a novel data query decomposition model to allow users to generate an initial datamation by simply inputting a data query in natural language. The initial datamation can be refined via rich interactions and a feedback mechanism is utilized to update the decomposition model based on user knowledge and preferences. Our system produces an animated sequence of visualizations driven by a set of low-level data actions. It supports unit visualizations, which provide a mapping from each data item to a unique visual mark. We demonstrate the effectiveness of Datamator via a series of evaluations including case studies, performance validation, and a controlled user study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118270393",
                    "name": "Yi Guo"
                },
                {
                    "authorId": "2059201609",
                    "name": "Nana Cao"
                },
                {
                    "authorId": "1410503485",
                    "name": "Ligan Cai"
                },
                {
                    "authorId": "2134152070",
                    "name": "Yanqiu Wu"
                },
                {
                    "authorId": "69863469",
                    "name": "D. Weiskopf"
                },
                {
                    "authorId": "2064971223",
                    "name": "Danqing Shi"
                },
                {
                    "authorId": "2152520175",
                    "name": "Qing Chen"
                }
            ]
        },
        {
            "paperId": "c831f34da8cfa38c08861e6914fe3b83196bd96b",
            "title": "Chart2Vec: A Universal Embedding of Context-Aware Visualizations",
            "abstract": "The advances in AI-enabled techniques have accelerated the creation and automation of visualizations in the past decade. However, presenting visualizations in a descriptive and generative format remains a challenge. Moreover, current visualization embedding methods focus on standalone visualizations, neglecting the importance of contextual information for multi-view visualizations. To address this issue, we propose a new representation model, Chart2Vec, to learn a universal embedding of visualizations with context-aware information. Chart2Vec aims to support a wide range of downstream visualization tasks such as recommendation and storytelling. Our model considers both structural and semantic information of visualizations in declarative specifications. To enhance the context-aware capability, Chart2Vec employs multi-task learning on both supervised and unsupervised tasks concerning the cooccurrence of visualizations. We evaluate our method through an ablation study, a user study, and a quantitative comparison. The results verified the consistency of our embedding method with human cognition and showed its advantages over existing methods.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152520175",
                    "name": "Qing Chen"
                },
                {
                    "authorId": "2261944198",
                    "name": "Ying Chen"
                },
                {
                    "authorId": "152570557",
                    "name": "Wei Shuai"
                },
                {
                    "authorId": "2220099450",
                    "name": "Ruishi Zou"
                },
                {
                    "authorId": "2118270393",
                    "name": "Yi Guo"
                },
                {
                    "authorId": "2124913650",
                    "name": "Jiazhe Wang"
                },
                {
                    "authorId": "2059201609",
                    "name": "Nana Cao"
                }
            ]
        },
        {
            "paperId": "82451ff3ac88da3c206f75abcbfe7389a4d89268",
            "title": "Talk2Data: A Natural Language Interface for Exploratory Visual Analysis via Question Decomposition",
            "abstract": "Through a natural language interface (NLI) for exploratory visual analysis, users can directly \u201cask\u201d analytical questions about the given tabular data. This process greatly improves user experience and lowers the technical barriers of data analysis. Existing techniques focus on generating a visualization from a concrete question. However, complex questions, requiring multiple data queries and visualizations to answer, are frequently asked in data exploration and analysis, which cannot be easily solved with the existing techniques. To address this issue, in this article, we introduce Talk2Data, a natural language interface for exploratory visual analysis that supports answering complex questions. It leverages an advanced deep-learning model to resolve complex questions into a series of simple questions that could gradually elaborate on the users\u2019 requirements. To present answers, we design a set of annotated and captioned visualizations to represent the answers in a form that supports interpretation and narration. We conducted an ablation study and a controlled user study to evaluate the Talk2Data\u2019s effectiveness and usefulness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118270393",
                    "name": "Yi Guo"
                },
                {
                    "authorId": "2064971223",
                    "name": "Danqing Shi"
                },
                {
                    "authorId": "2151671022",
                    "name": "Mingjuan Guo"
                },
                {
                    "authorId": "2134152070",
                    "name": "Yanqiu Wu"
                },
                {
                    "authorId": "2059201609",
                    "name": "Nana Cao"
                },
                {
                    "authorId": "2152520175",
                    "name": "Qing Chen"
                }
            ]
        },
        {
            "paperId": "82b40f6e8dde3822e17853df3d6bf2cfa4d14eff",
            "title": "Survey on Visual Analysis of Event Sequence Data",
            "abstract": "Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2118270393",
                    "name": "Yi Guo"
                },
                {
                    "authorId": "30518075",
                    "name": "Shunan Guo"
                },
                {
                    "authorId": "50845699",
                    "name": "Zhuochen Jin"
                },
                {
                    "authorId": "1767832274",
                    "name": "Smiti Kaul"
                },
                {
                    "authorId": "1714488",
                    "name": "D. Gotz"
                },
                {
                    "authorId": "144313415",
                    "name": "Nan Cao"
                }
            ]
        },
        {
            "paperId": "922b0dd9950720ee9ae5b7b503a7b15061b7a624",
            "title": "FALCON: seamless access to meeting data from the inbox and calendar",
            "abstract": "We present a system that supports seamless access to information contained in recorded meetings from the cornerstone points of a knowledge worker's daily life: mailbox and calendar. The solution supports granular search of meeting content from an enterprise email system and automatically displays recordings of meetings related to the message the user is currently viewing. Additionally thumbnail summaries of the meetings are added to the user's calendar entries after the meetings have taken place. Lastly our system supports easy sharing of videos associated with recorded meetings through the use of hot-linked thumbnail summaries which can be sent via email.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48962187",
                    "name": "Peter Bjellerup"
                },
                {
                    "authorId": "2798366",
                    "name": "Karl J. Cama"
                },
                {
                    "authorId": "2970595",
                    "name": "Mukundan Desikan"
                },
                {
                    "authorId": "2118270393",
                    "name": "Yi Guo"
                },
                {
                    "authorId": "37493415",
                    "name": "Ajinkya Kale"
                },
                {
                    "authorId": "3853032",
                    "name": "J. Lai"
                },
                {
                    "authorId": "3286358",
                    "name": "Nizar Lethif"
                },
                {
                    "authorId": "47789894",
                    "name": "Jie Lu"
                },
                {
                    "authorId": "7179616",
                    "name": "Mercan Topkara"
                },
                {
                    "authorId": "3042544",
                    "name": "Stephan H. Wissel"
                }
            ]
        },
        {
            "paperId": "ea8897d44a95c200f02f15170c1581765ad715cc",
            "title": "Automatic Learning of Semantic Region Models for Event Recognition",
            "abstract": "The semantic structure of scene is important information used for interpretation of object behavior or event detection in video surveillance system. In this paper, we propose an automatic method for learning models of semantic region by analyzing the trajectories of moving objects in the scene. First, the trajectory is encoded to represent both the position of the object and its instantaneous velocity. Then, the hierarchical clustering algorithm is applied to cluster the trajectories according to different spatial and velocity distributions. In each cluster, trajectories are spatially close, have similar velocities of motion and represent one type of activity pattern. Based on the trajectory clusters, the statistical models of semantic region in the scene are generated by estimating the density and velocity distributions of each type of activity pattern. Finally, using the proposed semantic region models, anomalous activities are detected in two scenes. Experimental results demonstrate the effectiveness of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112351376",
                    "name": "Lei Gao"
                },
                {
                    "authorId": "46651287",
                    "name": "C. Li"
                },
                {
                    "authorId": "2118270393",
                    "name": "Yi Guo"
                },
                {
                    "authorId": "46335589",
                    "name": "Z. Xiong"
                }
            ]
        }
    ]
}