{
    "authorId": "66848311",
    "papers": [
        {
            "paperId": "31e45d1bf65b76b1805ac54bdb204a3a39c6caf8",
            "title": "CAPER: Enhancing Career Trajectory Prediction using Temporal Knowledge Graph and Ternary Relationship",
            "abstract": "The problem of career trajectory prediction (CTP) aims to predict one's future employer or job position. While several CTP methods have been developed for this problem, we posit that none of these methods (1) jointly considers the mutual ternary dependency between three key units (i.e., user, position, and company) of a career and (2) captures the characteristic shifts of key units in career over time, leading to an inaccurate understanding of the job movement patterns in the labor market. To address the above challenges, we propose a novel solution, named as CAPER, that solves the challenges via sophisticated temporal knowledge graph (TKG) modeling. It enables the utilization of a graph-structured knowledge base with rich expressiveness, effectively preserving the changes in job movement patterns. Furthermore, we devise an extrapolated career reasoning task on TKG for a realistic evaluation. The experiments on a real-world career trajectory dataset demonstrate that CAPER consistently and significantly outperforms four baselines, two recent TKG reasoning methods, and five state-of-the-art CTP methods in predicting one's future companies and positions-i.e., on average, yielding 6.80% and 34.58% more accurate predictions, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2553299",
                    "name": "Yeon-Chang Lee"
                },
                {
                    "authorId": "2265949175",
                    "name": "JaeHyun Lee"
                },
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "2265952497",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2237948017",
                    "name": "Sang-Wook Kim"
                }
            ]
        },
        {
            "paperId": "62ac45c894b38a9ec6ca089d1bea292281089d04",
            "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection",
            "abstract": "High-quality text generation capability of recent Large Language Models (LLMs) causes concerns about their misuse (e.g., in massive generation/spread of disinformation). Machine-generated text (MGT) detection is important to cope with such threats. However, it is susceptible to authorship obfuscation (AO) methods, such as paraphrasing, which can cause MGTs to evade detection. So far, this was evaluated only in monolingual settings. Thus, the susceptibility of recently proposed multilingual detectors is still unknown. We fill this gap by comprehensively benchmarking the performance of 10 well-known AO methods, attacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10 $\\times$ 37 $\\times$ 11 = 4,070 combinations). We also evaluate the effect of data augmentation on adversarial robustness using obfuscated texts. The results indicate that all tested AO methods can cause evasion of automated detection in all tested languages, where homoglyph attacks are especially successful. However, some of the AO methods severely damaged the text, making it no longer readable or easily recognizable by humans (e.g., changed language, weird characters).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260109257",
                    "name": "Dominik Macko"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "2260072705",
                    "name": "Jason Samuel Lucas"
                },
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "66674465",
                    "name": "Nafis Irtiza Tripto"
                },
                {
                    "authorId": "2279666194",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "e629d5398bdd73d9924692e1bfd193c323bf063b",
            "title": "Fake Resume Attacks: Data Poisoning on Online Job Platforms",
            "abstract": "While recent studies have exposed various vulnerabilities incurred from data poisoning attacks in many web services, little is known about the vulnerability on online professional job platforms (e.g., LinkedIn and Indeed). In this work, first time, we demonstrate the critical vulnerabilities found in the common Human Resources (HR) task of matching job seekers and companies on online job platforms. Capitalizing on the unrestricted format and contents of job seekers' resumes and easy creation of accounts on job platforms, we demonstrate three attack scenarios: (1) company promotion attack to increase the likelihood of target companies being recommended, (2) company demotion attack to decrease the likelihood of target companies being recommended, and (3) user promotion attack to increase the likelihood of certain users being matched to certain companies. To this end, we develop an end-to-end \"fake resume\" generation framework, titled FRANCIS, that induces systematic prediction errors via data poisoning. Our empirical evaluation on real-world datasets reveals that data poisoning attacks can markedly skew the results of matchmaking between job seekers and companies, regardless of underlying models, with vulnerability amplified in proportion to poisoning intensity. These findings suggest that the outputs of various services from job platforms can be potentially hacked by malicious users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "2287321418",
                    "name": "Thanh Tran"
                },
                {
                    "authorId": "2279666194",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "b91645729a769c09eddda2efe2512e2f6a750723",
            "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation",
            "abstract": "Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (.i.e, generating large-scale harmful and misleading content). To combat this emerging risk of LLMs, we propose a novel\"Fighting Fire with Fire\"(F3) strategy that harnesses modern LLMs' generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260072705",
                    "name": "Jason Samuel Lucas"
                },
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "2159644449",
                    "name": "Jooyoung Lee"
                },
                {
                    "authorId": "40408676",
                    "name": "Shaurya Rohatgi"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "7d411f0c6eec36c0cc3d480501979f95ff8ae6e6",
            "title": "JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning",
            "abstract": "In online job marketplaces, it is important to establish a well-defined job title taxonomy for various downstream tasks (e.g., job recommendation, users\u2019 career analysis, and turnover prediction). Job Title Normalization (JTN) is such a cleaning step to classify user-created non-standard job titles into normalized ones. However, solving the JTN problem is non-trivial with challenges: (1) semantic similarity of different job titles, (2) non-normalized user-created job titles, and (3) large-scale and long-tailed job titles in real-world applications. To this end, we propose a novel solution, named JAMES, that constructs three unique embeddings (i.e., graph, contextuat, and syntactic) of a target job title to effectively capture its various traits. We further propose a multi-aspect co-attention mechanism to attentively combine these embeddings, and employ neural logical reasoning representations to collaboratively estimate similarities between messy job titles and normalized job titles in a reasoning space. To evaluate JAMES, we conduct comprehensive experiments against ten competing models on a large-scale real-world dataset with over 350,000 job titles. Our experimental results show that JAMES significantly outperforms the best baseline by 10.06% in Precision@10 and by 17.52% in NDCG@10, respectively. To further facilitate the acquisition of normalized job titles for job-domain applications, our JAMES API is available at: https://tinyurl.con JAMES-job-title-mapping.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "2115733400",
                    "name": "J. Shen"
                },
                {
                    "authorId": "2090288772",
                    "name": "Thanh-Binh Tran"
                },
                {
                    "authorId": "2155650328",
                    "name": "Hamoon Ekhtiari"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "ca830fd7d0ef10bad2a2545782ce0f58311f4817",
            "title": "JAMES: Job Title Mapping with Multi-Aspect Embeddings and Reasoning",
            "abstract": "One of the most essential tasks needed for various downstream tasks in career analytics (e.g., career trajectory analysis, job mobility prediction, and job recommendation) is Job Title Mapping ( JTM ), where the goal is to map user-created (noisy and non-standard) job titles to predefined and standard job titles. However, solving JTM is domain-specific and non-trivial due to its inherent challenges: (1) user-created job titles are messy, (2) different job titles often overlap their job requirements, (3) job transition trajectories are inconsistent, and (4) the number of job titles in real world applications is large-scale. Toward this JTM problem, in this work, we propose a novel solution, named as JAMES , that constructs three unique embeddings of a target job title: topological, semantic, and syntactic embeddings, together with multi-aspect co-attention. In addition, we employ logical reasoning representations to collaboratively estimate similarities between messy job titles and standard job titles in the reasoning space. We conduct comprehensive experiments against ten competing models on the large-scale real-world dataset with more than 350,000 job titles. Our results show that JAMES significantly outperforms the best baseline by 10.06% in Precision@10 and by 17.52% in NDCG@10, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "2115733400",
                    "name": "J. Shen"
                },
                {
                    "authorId": "2155650328",
                    "name": "Hamoon Ekhtiari"
                },
                {
                    "authorId": "2090288772",
                    "name": "Thanh-Binh Tran"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "f829674dceb91b76c9350b90d9432530eb1f7ca1",
            "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education",
            "abstract": "Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q&A, and knowledge tracing, to demonstrate the superiority of MathBERT over BASE BERT. Our experiments show that MathBERT outperforms prior best methods by 1.2-22% and BASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific vocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT pre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT vocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the participated leaning platforms: Stride, Inc, a commercial educational resource provider, and ASSISTments.org, a free online educational platform. We release MathBERT for public usage at: https://github.com/tbs17/MathBERT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115733400",
                    "name": "J. Shen"
                },
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "35596962",
                    "name": "Ethan Prihar"
                },
                {
                    "authorId": "1686529",
                    "name": "N. Heffernan"
                },
                {
                    "authorId": "7916525",
                    "name": "Xintao Wu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "7175aca623487c341e1ca03955469512548a4da2",
            "title": "A Comparison of Entity Matching Methods between English and Japanese Katakana",
            "abstract": "Japanese Katakana is one component of the Japanese writing system and is used to express English terms, loanwords, and onomatopoeia in Japanese characters based on the phonemes. The main purpose of this research is to find the best entity matching methods between English and Katakana. We built two research questions to clarify which types of entity matching systems works better than others. The first question is what transliteration should be used for conversion. We need to transliterate English or Katakana terms into the same form in order to compute the string similarity. We consider five conversions that transliterate English to Katakana directly, Katakana to English directly, English to Katakana via phoneme, Katakana to English via phoneme, and both English and Katakana to phoneme. The second question is what should be used for the similarity measure at entity matching. To investigate the problem, we choose six methods, which are Overlap Coefficient, Cosine, Jaccard, Jaro-Winkler, Levenshtein, and the similarity of the phoneme probability predicted by RNN. Our results show that 1) matching using phonemes and conversion of Katakana to English works better than other methods, and 2) the similarity of phonemes outperforms other methods while other similarity score is changed depending on data and models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "151019643",
                    "name": "Hideki Awashima"
                },
                {
                    "authorId": "2835969",
                    "name": "H. Oiwa"
                }
            ]
        },
        {
            "paperId": "982fd481f76dc15475db67cc7459a85d8adf6d23",
            "title": "Discovery of User Preferences from Big Geospatial Data Using Topic Models",
            "abstract": "It is becoming common to apply location information for actual problems such as automatic routing, creating tour, and the prediction of user activity. In many of these cases, they use a geodata that is based on a user\u2019s self check-in. In this study, to focus on a user\u2019s daily behavior deeply, we use a large volume of geospatial data, which is detected regularly and automatically from a cell phone\u2019s GPS system and the volume of data is totally different from the self check-in based. We can comprehend users activity in more detail than self check-in based data. The main purpose of this research is to extract and interpret user preferences from big geospatial data. Because the auto-detected data is big and spatial, it is necessary to process and analyze it for transformation to a value. Firstly, we extract the spots where people visited and stayed from raw geodata. Next, we categorize spots using the spot information to reduce the dimension. Finally, we interpret user preferences by applying a topic model to the matrix of users and the categorized spot. Our results show that we can interpret user preferences as topics from big geospatial data using dimension reduction and a topic model. In addition, we verify a performance of methods that use the topics to predict purchase. We found that the models that use topics outperforms others on the real prediction problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "66487297",
                    "name": "Shota Katsumata"
                },
                {
                    "authorId": "2074249991",
                    "name": "Yusuke Fukasawa"
                }
            ]
        }
    ]
}