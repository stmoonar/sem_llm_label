{
    "authorId": "1409858224",
    "papers": [
        {
            "paperId": "1163c29ae6a26b1cd8d85280740410e99001a959",
            "title": "Self-Supervised Graph Neural Networks via Diverse and Interactive Message Passing",
            "abstract": "By interpreting Graph Neural Networks (GNNs) as the message passing from the spatial perspective, their success is attributed to Laplacian smoothing. However, it also leads to serious over-smoothing issue by stacking many layers. Recently, many efforts have been paid to overcome this issue in semi-supervised learning. Unfortunately, it is more serious in unsupervised node representation learning task due to the lack of supervision information. Thus, most of the unsupervised or self-supervised GNNs often employ \\textit{one-layer GCN} as the encoder. Essentially, the over-smoothing issue is caused by the over-simplification of the existing message passing, which possesses two intrinsic limits: blind message and uniform passing. In this paper, a novel Diverse and Interactive Message Passing (DIMP) is proposed for self-supervised learning by overcoming these limits. Firstly, to prevent the message from blindness and make it interactive between two connected nodes, the message is determined by both the two connected nodes instead of the attributes of one node. Secondly, to prevent the passing from uniformness and make it diverse over different attribute channels, different propagation weights are assigned to different elements in the message. To this end, a natural implementation of the message in DIMP is the element-wise product of the representations of two connected nodes. From the perspective of numerical optimization, the proposed DIMP is equivalent to performing an overlapping community detection via expectation-maximization (EM). Both the objective function of the community detection and the convergence of EM algorithm guarantee that DMIP can prevent from over-smoothing issue. Extensive evaluations on node-level and graph-level tasks demonstrate the superiority of DIMP on improving performance and overcoming over-smoothing issue.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2145775206",
                    "name": "Cheng Chen"
                },
                {
                    "authorId": "1409858224",
                    "name": "Weixun Li"
                },
                {
                    "authorId": "2661834",
                    "name": "Bingxin Niu"
                },
                {
                    "authorId": "6413344",
                    "name": "Junhua Gu"
                },
                {
                    "authorId": "2109150586",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "6983c9a2c1d73556a6bf3e53ffe463a57dfe9505",
            "title": "HCL: Improving Graph Representation with Hierarchical Contrastive Learning",
            "abstract": "Contrastive learning has emerged as a powerful tool for graph representation learning. However, most contrastive learning methods learn features of graphs with fixed coarse-grained scale, which might underestimate either local or global information. To capture more hierarchical and richer representation, we propose a novel Hierarchical Contrastive Learning (HCL) framework that explicitly learns graph representation in a hierarchical manner. Specifically, HCL includes two key components: a novel adaptive Learning to Pool (L2Pool) method to construct more reasonable multi-scale graph topology for more comprehensive contrastive objective, a novel multi-channel pseudo-siamese network to further enable more expressive learning of mutual information within each scale. Comprehensive experimental results show HCL achieves competitive performance on 12 datasets involving node classification, node clustering and graph classification. In addition, the visualization of learned representation reveals that HCL successfully captures meaningful characteristics of graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152811266",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "1409858224",
                    "name": "Weixun Li"
                },
                {
                    "authorId": "2054057703",
                    "name": "Changyu Hou"
                },
                {
                    "authorId": "2109887784",
                    "name": "Xin Tang"
                },
                {
                    "authorId": "148407220",
                    "name": "Yixuan Qiao"
                },
                {
                    "authorId": "2182292096",
                    "name": "Rui Fang"
                },
                {
                    "authorId": "81229965",
                    "name": "Pengyong Li"
                },
                {
                    "authorId": "2088487395",
                    "name": "Peng Gao"
                },
                {
                    "authorId": "2052381718",
                    "name": "Guowang Xie"
                }
            ]
        }
    ]
}