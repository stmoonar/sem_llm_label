{
    "authorId": "47775705",
    "papers": [
        {
            "paperId": "41a34a072ef7a9dc0a08be32798df2e463caf7ce",
            "title": "Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach",
            "abstract": "We present a new task setting for attribute mining on e-commerce products, serving as a practical solution to extract open-world attributes without extensive human intervention. Our supervision comes from a high-quality seed attribute set bootstrapped from existing resources, and we aim to expand the attribute vocabulary of existing seed types, and also to discover any new attribute types automatically. A new dataset is created to support our setting, and our approach Amacer is proposed specifically to tackle the limited supervision. Especially, given that no direct supervision is available for those unseen new attributes, our novel formulation exploits self-supervised heuristic and unsupervised latent attributes, which attains implicit semantic signals as additional supervision by leveraging product context. Experiments suggest that our approach surpasses various baselines by 12 F1, expanding attributes of existing types significantly by up to 12 times, and discovering values from 39% new types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "2047145237",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "2157096355",
                    "name": "Xian Li"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "4724587",
                    "name": "Jinho D. Choi"
                }
            ]
        },
        {
            "paperId": "e6d317bfe096974813993d62cde183d2fcdccce0",
            "title": "Exploring a Multi-Layered Cross-Genre Corpus of Document-Level Semantic Relations",
            "abstract": "This paper introduces a multi-layered cross-genre corpus, annotated for coreference resolution, causal relations, and temporal relations, comprising a variety of genres, from news articles and children\u2019s stories to Reddit posts. Our results reveal distinctive genre-specific characteristics at each layer of annotation, highlighting unique challenges for both annotators and machine learning models. Children\u2019s stories feature linear temporal structures and clear causal relations. In contrast, news articles employ non-linear temporal sequences with minimal use of explicit causal or conditional language and few first-person pronouns. Lastly, Reddit posts are author-centered explanations of ongoing situations, with occasional meta-textual reference. Our annotation schemes are adapted from existing work to better suit a broader range of text types. We argue that our multi-layered cross-genre corpus not only reveals genre-specific semantic characteristics but also indicates a rich contextual interplay between the various layers of semantic information. Our MLCG corpus is shared under the open-source Apache 2.0 license.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1578160338",
                    "name": "Gregor Williamson"
                },
                {
                    "authorId": "2182246411",
                    "name": "Angela Cao"
                },
                {
                    "authorId": "2214097056",
                    "name": "Yingying Chen"
                },
                {
                    "authorId": "2152299195",
                    "name": "Yuxin Ji"
                },
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "4724587",
                    "name": "Jinho D. Choi"
                }
            ]
        },
        {
            "paperId": "f8166f4ac707e5f763360f4488f8aac396e233a6",
            "title": "An Edge Server Placement Algorithm Based on Graph Convolution Network",
            "abstract": "Efficient edge server placement techniques select optimal locations for edge servers to improve network and energy performance. However, without prior knowledge of users' resource demands, the possibilities of optimal location for these edge servers within a network are vast, which is a challenging problem. On the other hand, most existing techniques ignore the influence of user mobility on user's resources demand, violation of low-latency, and high energy consumption of 5G networks. Therefore, this article addresses the edge server placement problem using network traffic to estimate the user resource demands. We first use a network traffic prediction model based on Graph Convolution Network to generate network traffic distribution. Second, the problem of edge server placement is formulated as a constraint optimization problem that places edge servers strategically to balance energy and latency. Searching randomly through many possible solutions and selecting the most descriptive optimal solutions can be time-consuming. Therefore, we used the particle swarm optimization (PSO) algorithm to optimize network delay and energy consumption, especially for high mobility areas. Experimental results are obtained to compare the performance of the proposed algorithm with existing methods. We evaluate the algorithm based on the real dataset from Shenzhen, Futian District. The results show our proposed algorithm averagely reduces edge servers' total cost and overloaded numbers by 23.98% and 52.71%, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988193",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "2173688280",
                    "name": "Zebang Feng"
                },
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "2111286678",
                    "name": "Qian Huang"
                },
                {
                    "authorId": "2193855252",
                    "name": "Yinsheng Zhou"
                },
                {
                    "authorId": "1712773",
                    "name": "Weizhe Zhang"
                },
                {
                    "authorId": "49375635",
                    "name": "Rahul Yadav"
                }
            ]
        },
        {
            "paperId": "31ce5f641dd3fc8898babe4754e3c9c231423289",
            "title": "Number Entity Recognition",
            "abstract": "Numbers are essential components of text, like any other word tokens, from which natural language processing (NLP) models are built and deployed. Though numbers are typically not accounted for distinctly in most NLP tasks, there is still an underlying amount of numeracy already exhibited by NLP models. In this work, we attempt to tap this potential of state-of-the-art NLP models and transfer their ability to boost performance in related tasks. Our proposed classi\ufb01cation of numbers into entities helps NLP models perform well on several tasks, including a handcrafted Fill-In-The-Blank (FITB) task and on question answering using joint embeddings, outperforming the BERT and RoBERTa baseline classi\ufb01cation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "19178540",
                    "name": "Dhanasekar Sundararaman"
                },
                {
                    "authorId": "48051210",
                    "name": "Vivek Subramanian"
                },
                {
                    "authorId": "1700522",
                    "name": "Guoyin Wang"
                },
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "145006560",
                    "name": "L. Carin"
                }
            ]
        },
        {
            "paperId": "91a196e70c6ba45ac0f5808364535d1e2033564c",
            "title": "RescoreBERT: Discriminative Speech Recognition Rescoring With Bert",
            "abstract": "Second-pass rescoring is an important component in automatic speech recognition (ASR) systems that is used to improve the outputs from a first-pass decoder by implementing a lattice rescoring or n-best re-ranking. While pretraining with a masked language model (MLM) objective has received great success in various natural language understanding (NLU) tasks, it has not gained traction as a rescoring model for ASR. Specifically, training a bidirectional model like BERT on a discriminative objective such as minimum WER (MWER) has not been explored. Here we show how to train a BERT-based rescoring model with MWER loss, to incorporate the improvements of a discriminative loss into fine-tuning of deep bidirectional pretrained models for ASR. Specifically, we propose a fusion strategy that incorporates the MLM into the discriminative training process to effectively distill knowledge from a pretrained model. We further propose an alternative discriminative loss. This approach, which we call RescoreBERT, reduces WER by 6.6%/3.4% relative on the LibriSpeech clean/other test sets over a BERT baseline without discriminative objective. We also evaluate our method on an internal dataset from a conversational agent and find that it reduces both latency and WER (by 3 to 8% relative) over an LSTM rescoring model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "2112579196",
                    "name": "Yile Gu"
                },
                {
                    "authorId": "2051209",
                    "name": "J. Kolehmainen"
                },
                {
                    "authorId": "144165565",
                    "name": "Haidar Khan"
                },
                {
                    "authorId": "36881920",
                    "name": "Ankur Gandhe"
                },
                {
                    "authorId": "3070896",
                    "name": "A. Rastrow"
                },
                {
                    "authorId": "1762744",
                    "name": "A. Stolcke"
                },
                {
                    "authorId": "2575459",
                    "name": "I. Bulyko"
                }
            ]
        },
        {
            "paperId": "9aa080f8ef7425c7ccc56585212a410fe8e6fd02",
            "title": "Improving Downstream Task Performance by Treating Numbers as Entities",
            "abstract": "Numbers are essential components of text, like any other word tokens, from which natural language processing (NLP) models are built and deployed. Though numbers are typically not accounted for distinctly in most NLP tasks, there is still an underlying amount of numeracy already exhibited by NLP models. For instance, in named entity recognition (NER), numbers are not treated as an entity with distinct tags. In this work, we attempt to tap the potential of state-of-the-art language models and transfer their ability to boost performance in related downstream tasks dealing with numbers. Our proposed classification of numbers into entities helps NLP models perform well on several tasks, including a handcrafted Fill-In-The-Blank (FITB) task and on question answering, using joint embeddings, outperforming the BERT and RoBERTa baseline classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "19178540",
                    "name": "Dhanasekar Sundararaman"
                },
                {
                    "authorId": "48051210",
                    "name": "Vivek Subramanian"
                },
                {
                    "authorId": "2107926840",
                    "name": "Guoyin Wang"
                },
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "145006560",
                    "name": "L. Carin"
                }
            ]
        },
        {
            "paperId": "a93c42672af584f12c953296918aac7827c0b8b9",
            "title": "Modeling Task Interactions in Document-Level Joint Entity and Relation Extraction",
            "abstract": "We target on the document-level relation extraction in an end-to-end setting, where the model needs to jointly perform mention extraction, coreference resolution (COREF) and relation extraction (RE) at once, and gets evaluated in an entity-centric way. Especially, we address the two-way interaction between COREF and RE that has not been the focus by previous work, and propose to introduce explicit interaction namely Graph Compatibility (GC) that is specifically designed to leverage task characteristics, bridging decisions of two tasks for direct task interference. Our experiments are conducted on DocRED and DWIE; in addition to GC, we implement and compare different multi-task settings commonly adopted in previous work, including pipeline, shared encoders, graph propagation, to examine the effectiveness of different interactions. The result shows that GC achieves the best performance by up to 2.3/5.1 F1 improvement over the baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "4724587",
                    "name": "Jinho D. Choi"
                }
            ]
        },
        {
            "paperId": "db2e4817315dc9394a766f3d9a98d8c4f69dba3d",
            "title": "Online Coreference Resolution for Dialogue Processing: Improving Mention-Linking on Real-Time Conversations",
            "abstract": "This paper suggests a direction of coreference resolution for online decoding on actively generated input such as dialogue, where the model accepts an utterance and its past context, then finds mentions in the current utterance as well as their referents, upon each dialogue turn. A baseline and four incremental updated models adapted from the mention linking paradigm are proposed for this new setting, which address different aspects including the singletons, speaker-grounded encoding and cross-turn mention contextualization. Our approach is assessed on three datasets: Friends, OntoNotes, and BOLT. Results show that each aspect brings out steady improvement, and our best models outperform the baseline by over 10%, presenting an effective system for this setting. Further analysis highlights the task characteristics, such as the significance of addressing the mention recall.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "4724587",
                    "name": "Jinho D. Choi"
                }
            ]
        },
        {
            "paperId": "1297f087e4d539cf7b322641f98e4a15a90b6bc1",
            "title": "Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation",
            "abstract": "Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target languages, combined with uncertainty estimation in the process to select high-quality silver labels. Three different uncertainties are adapted and analyzed specifically for the cross lingual transfer: Language Heteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty (EVI). We evaluate our framework with uncertainties on two cross-lingual tasks including Named Entity Recognition (NER) and Natural Language Inference (NLI) covering 40 languages in total, which outperforms the baselines significantly by 10 F1 for NER on average and 2.5 accuracy for NLI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "144180429",
                    "name": "F. Chen"
                },
                {
                    "authorId": "4724587",
                    "name": "Jinho D. Choi"
                }
            ]
        },
        {
            "paperId": "5a829129e8357ba14143a30c93d971941ae9d255",
            "title": "Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-Sentence Dependency Graph",
            "abstract": "We target the task of cross-lingual Machine Reading Comprehension (MRC) in the direct zero-shot setting, by incorporating syntactic features from Universal Dependencies (UD), and the key features we use are the syntactic relations within each sentence. While previous work has demonstrated effective syntax-guided MRC models, we propose to adopt the inter-sentence syntactic relations, in addition to the rudimentary intra-sentence relations, to further utilize the syntactic dependencies in the multi-sentence input of the MRC task. In our approach, we build the Inter-Sentence Dependency Graph (ISDG) connecting dependency trees to form global syntactic relations across sentences. We then propose the ISDG encoder that encodes the global dependency graph, addressing the inter-sentence relations via both one-hop and multi-hop dependency paths explicitly. Experiments on three multilingual MRC datasets (XQuAD, MLQA, TyDiQA-GoldP) show that our encoder that is only trained on English is able to improve the zero-shot performance on all 14 test sets covering 8 languages, with up to 3.8 F1 / 5.2 EM improvement on-average, and 5.2 F1 / 11.2 EM on certain languages. Further analysis shows the improvement can be attributed to the attention on the cross-linguistically consistent syntactic path. Our code is available at https://github.com/lxucs/multilingual-mrc-isdg.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47775705",
                    "name": "Liyan Xu"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2991105",
                    "name": "Bo Zong"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "2090567",
                    "name": "Jingchao Ni"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "4724587",
                    "name": "Jinho D. Choi"
                }
            ]
        }
    ]
}