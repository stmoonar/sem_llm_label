{
    "authorId": "49702692",
    "papers": [
        {
            "paperId": "080445f828610db80b0eedeed93e526301e8ac5f",
            "title": "Efficiently Archiving Photos under Storage Constraints",
            "abstract": "Our ability to collect data is rapidly outstripping our ability to effectively store and use it. Organizations are therefore facing tough decisions of what data to archive (or dispose of) to effectively meet their business goals. We address this general problem in the context of image data (photos) by proposing which photos to archive to meet an online storage budget. The decision is based on factors such as usage patterns and their relative importance, the quality and size of a photo, the relevance of a photo for a usage pattern, the similarity between different photos, as well as policy requirements of what photos must be retained. We formalize the photo archival problem, analyze its complexity, and give two approximation algorithms. One algorithm comes with an optimal approximation guarantee and another, more scalable, algorithm that comes with both worst-case and data-dependent guarantees. Based on these algorithms we implemented an end-to-end system, PHOcus, and discuss how to automatically derive the inputs for this system in many settings. An extensive experimental study based on public as well as private datasets demonstrates the effectiveness and efficiency of PHOcus. Furthermore, a user study using business analysts in a real e-commerce application shows that it can save a tremendous amount of human effort and yield unexpected insights.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2184486608",
                    "name": "May Shoshan"
                }
            ]
        },
        {
            "paperId": "3f0b3ff01db3cec1e3ad1881d3e3a21e5287fb6f",
            "title": "PHOcus: Efficiently Archiving Photos",
            "abstract": "Our ability to collect data is rapidly outstripping our ability to effectively store and use it. Organizations are therefore facing tough decisions of what data to archive (or dispose of) to effectively meet their business goals. PHOcus addresses this problem in the context of image data (photos) by proposing which photos to archive to meet an online storage budget. The decision is based on factors such as usage patterns and their relative importance, the quality and size of a photo, the relevance of a photo for a usage pattern, the similarity between different photos, as well as policy requirements of what photos must be retained. We formalize the photo archival problem and give an efficient algorithm with an optimal approximation guarantee. We then demonstrate our system, PHOcus, on an e-commerce application as well as with personal photos on a smartphone, and discuss how many of the inputs to the problem can be automatically obtained.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2184486608",
                    "name": "May Shoshan"
                }
            ]
        },
        {
            "paperId": "465d898d97ee6f827c177cb0321c7b3687abbc29",
            "title": "Provenance-based Model Maintenance: Implications for Privacy",
            "abstract": "With the increasing need for Machine-learning-as-a-service (MaaS) online systems, effectively maintaining and reusing machine learning models in light of changes to the underlying data has become a big concern. In particular, it is extremely challenging to refresh existing models after the removal of training samples, which is called \u201cmachine unlearning\u201d. Addressing this challenge not only requires an ef\ufb01cient solution, but must comply with emerging privacy issues, e.g. GDPR, which implies that the removed samples must be fully erased from the models so that they cannot be leaked to an adversary. We review two provenance-based solutions, PrIU and DeltaGrad, and show how they can guard against \u201cmodel inversion attacks\", which reconstruct the removed training samples from the updated models after the unlearning process. Since PrIU and DeltaGrad support a limited class of models, we envision a system that can unlearn general models in an ef\ufb01cient and secure manner and outline possible technical challenges for building this system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145899612",
                    "name": "Yinjun Wu"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "80b95b5fb0d5d536e69325ff7e10fb18baf28b62",
            "title": "Selecting Sub-tables for Data Exploration",
            "abstract": "Data scientists frequently examine the raw content of large tables when exploring an unknown dataset. In such cases, small subsets of the full tables (sub-tables) that accurately capture table contents are useful. We present a framework which, given a large data table T, creates a sub-table of small, fixed dimensions by selecting a subset of T\u2019s rows and projecting them over a subset of T\u2019s columns. The question is: Which rows and columns should be selected to yield an informative sub-table?Our first contribution is an informativeness metric for sub-tables with two complementary dimensions: cell coverage, which measures how well the sub-table captures prominent data patterns in T, and diversity. We use association rules as the patterns captured by sub-tables, and show that computing optimal sub-tables directly using this metric is infeasible. We then develop an efficient algorithm that indirectly accounts for association rules using table embedding. The resulting framework produces sub-tables for the full table as well as for the results of queries over the table, enabling the user to quickly understand results and determine subsequent queries. Experimental results show that high-quality sub-tables can be efficiently computed, and verify the soundness of our metrics as well as the usefulness of selected sub-tables through user studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "960c8d6d368b08e3c701d1027dd402e9a7dc797e",
            "title": "Disposal by Design",
            "abstract": "The \ufb02ood of data that has enabled breakthroughs in medicine, commerce, transportation, science and society also threatens to overwhelm our storage capacities and our privacy. Due to the volume of data and growth of regulations governing its maintenance and use, it is essential to develop automatic disposal techniques to manage this \ufb02ood. We present a vision for automating data disposal \u2013 disposal by design \u2013 which takes into account processing constraints, regulatory constraints as well as storage constraints, and give three concrete examples which address aspects of this vision. Two of the examples address current needs in e-commerce, while the third suggests how to use machine learning to \ufb01nd summaries of relational data. We then discuss the research challenges that remain to provide a holistic solution to disposal by design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "12427017",
                    "name": "Shay Gershtein"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                }
            ]
        },
        {
            "paperId": "c7f5403305c426922fc2bf2f62b61caf5dfa41fb",
            "title": "ShapGraph: An Holistic View of Explanations through Provenance Graphs and Shapley Values",
            "abstract": "Explaining query results is an essential tool for enhancing the transparency and quality of data processing, and has been extensively studied in recent years. In particular, Data Provenance -- the tracking of transformations that data undergoes in query evaluation -- has been shown to be a key component of explanations. A hurdle that remains is that data provenance itself is often too large and complex to be presented in its entirety. To that end, we propose to leverage novel advancements on quantifying and computing the contributions of individual input tuples to query answers, based on the game-theoretic notion of the Shapley value. Our proposed prototype solution, called ShapGraph, combines the global view of explanations through provenance graphs with a local quantification of contributions through Shapley values. The graphical interface allows users to switch between and combine these two views to obtain a deeper understanding of the most influential parts of the database and how they interact to yield query answers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "32915630",
                    "name": "Nave Frost"
                },
                {
                    "authorId": "1679226",
                    "name": "B. Kimelfeld"
                },
                {
                    "authorId": "2105843773",
                    "name": "Omer Koren"
                },
                {
                    "authorId": "51917893",
                    "name": "Mika\u00ebl Monet"
                }
            ]
        },
        {
            "paperId": "e8ca453e854fbbee016bee8da08796f7f58e7fcf",
            "title": "SubTab: Data Exploration with Informative Sub-Tables",
            "abstract": "We demonstrate SubTab, a framework for creating small, informative sub-tables of large data tables to speed up data exploration. Given a table with n rows and m columns where n and m are large, SubTab creates a sub-table T_sub with k<n rows and l<m columns, i.e. a subset of k rows of the table projected over a subset of l columns. The rows and columns are chosen as representatives of prominent data patterns within and across columns in the input table. SubTab can also be used for query results, enabling the user to quickly understand the results and determine subsequent queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382659970",
                    "name": "Kathy Razmadze"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "22648ebe656e898bbe3142582fe6d28974f3eddd",
            "title": "CHEF: A Cheap and Fast Pipeline for Iteratively Cleaning Label Uncertainties",
            "abstract": "\n High-quality labels are expensive to obtain for many machine learning tasks, such as medical image classification tasks. Therefore, probabilistic (weak) labels produced by weak supervision tools are used to seed a process in which influential samples with weak labels are identified and cleaned by several human annotators to improve the model performance. To lower the overall cost and computational overhead of this process, we propose a solution called CHEF (CHEap and Fast label cleaning), which consists of the following three components. First, to reduce the cost of human annotators, we use INFL, which prioritizes the\n most influential\n training samples for cleaning and provides cleaned labels to save the cost of one human annotator. Second, to accelerate the sample selector phase and the model constructor phase, we use Increm-INFL to\n incrementally\n produce influential samples, and DeltaGrad-L to\n incrementally\n update the model. Third, we redesign the typical label cleaning pipeline so that human annotators iteratively clean smaller batch of samples rather than one big batch of samples. This yields better overall model performance and enables possible early termination when the expected model performance has been achieved. Extensive experiments show that our approach gives good model prediction performance while achieving significant speed-ups.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3037003",
                    "name": "Yinjun Wu"
                },
                {
                    "authorId": "4726675",
                    "name": "James Weimer"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "90f292fd8f0f2e321f3829b8cfee7cd7f72ace6e",
            "title": "Dynamic Gaussian Mixture based Deep Generative Model For Robust Forecasting on Sparse Multivariate Time Series",
            "abstract": "Forecasting on sparse multivariate time series (MTS) aims to model the predictors of future values of time series given their incomplete past, which is important for many emerging applications. However, most existing methods process MTS\u2019s individually, and do not leverage the dynamic distributions underlying the MTS\u2019s, leading to sub-optimal results when the sparsity is high. To address this challenge, we propose a novel generative model, which tracks the transition of latent clusters, instead of isolated feature representations, to achieve robust modeling. It is characterized by a newly designed dynamic Gaussian mixture distribution, which captures the dynamics of clustering structures, and is used for emitting time series. The generative model is parameterized by neural networks. A structured inference network is also designed for enabling inductive analysis. A gating mechanism is further introduced to dynamically tune the Gaussian mixture distributions. Extensive experimental results on a variety of real-life datasets demonstrate the effectiveness of our method.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3037003",
                    "name": "Yinjun Wu"
                },
                {
                    "authorId": "2090567",
                    "name": "Jingchao Ni"
                },
                {
                    "authorId": "92186360",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "2991105",
                    "name": "Bo Zong"
                },
                {
                    "authorId": "2451800",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "c9a05b859454810c6474129cda0393f6d588ad22",
            "title": "Solon: Communication-efficient Byzantine-resilient Distributed Training via Redundant Gradients",
            "abstract": "There has been a growing need to provide Byzantine-resilience in distributed model training. Existing robust distributed learning algorithms focus on developing sophisticated robust aggregators at the parameter servers, but pay less attention to balancing the communication cost and robustness. In this paper, we propose Solon, an algorithmic framework that exploits gradient redundancy to provide communication efficiency and Byzantine robustness simultaneously. Our theoretical analysis shows a fundamental trade-off among computational load, communication cost, and Byzantine robustness. We also develop a concrete algorithm to achieve the optimal trade-off, borrowing ideas from coding theory and sparse recovery. Empirical experiments on various datasets demonstrate that Solon provides significant speedups over existing methods to achieve the same accuracy, over 10 times faster than Bulyan and 80% faster than Draco. We also show that carefully designed Byzantine attacks break Signum and Bulyan, but do not affect the successful convergence of Solon.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119322954",
                    "name": "Lingjiao Chen"
                },
                {
                    "authorId": "21023662",
                    "name": "Leshang Chen"
                },
                {
                    "authorId": "2109798334",
                    "name": "Hongyi Wang"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2694895",
                    "name": "Edgar Dobriban"
                }
            ]
        },
        {
            "paperId": "3ec4972a97344b846628c6efdc72a61be2ba2801",
            "title": "PrIU: A Provenance-Based Approach for Incrementally Updating Regression Models",
            "abstract": "The ubiquitous use of machine learning algorithms brings new challenges to traditional database problems such as incremental view update. Much effort is being put in better understanding and debugging machine learning models, as well as in identifying and repairing errors in training datasets. Our focus is on how to assist these activities when they have to retrain the machine learning model after removing problematic training samples in cleaning or selecting different subsets of training data for interpretability. This paper presents an efficient provenance-based approach, PrIU, and its optimized version, PrIU-opt, for incrementally updating model parameters without sacrificing prediction accuracy. We prove the correctness and convergence of the incrementally updated model parameters, and validate it experimentally. Experimental results show that up to two orders of magnitude speed-ups can be achieved by PrIU-opt compared to simply retraining the model from scratch, yet obtaining highly similar models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3037003",
                    "name": "Yinjun Wu"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "bd9ecc05a12563445a2ef4fe758a39d7f2bcda0d",
            "title": "DeltaGrad: Rapid retraining of machine learning models",
            "abstract": "Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3037003",
                    "name": "Yinjun Wu"
                },
                {
                    "authorId": "2694895",
                    "name": "Edgar Dobriban"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "e4982b0ed2156f44be59bf2afecc64e221103c6a",
            "title": "Provenance for Probabilistic Logic Programs",
            "abstract": "Despite the emergence of probabilistic logic programming (PLP) languages for data driven applications, there are currently no debugging tools based on provenance for PLP programs. In this paper, we propose a novel provenance model and system, called P3 (Provenance for Probabilistic logic Programs) for analyzing PLP programs. P3 enables four types of provenance queries: traditional explanation queries, queries for finding the set of most important derivations within an approximate error, top-K most influential queries, and modification queries that enable us to modify tuple probabilities with fewest modifications to program or input data. We apply these queries into real-world scenarios and present theoretical analysis and practical algorithms for such queries. We have developed a prototype of P3, and our evaluation on real-world data demonstrates that the system can support a wide-range of provenance queries with explainable results. Moreover, the system maintains provenance and execute queries efficiently with low overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145318413",
                    "name": "Shaobo Wang"
                },
                {
                    "authorId": "96059079",
                    "name": "Huilong Lyu"
                },
                {
                    "authorId": "2144155560",
                    "name": "Jiachi Zhang"
                },
                {
                    "authorId": "120930856",
                    "name": "Chenyuan Wu"
                },
                {
                    "authorId": "2109080981",
                    "name": "Xinyi Chen"
                },
                {
                    "authorId": "33779522",
                    "name": "Wenchao Zhou"
                },
                {
                    "authorId": "35206168",
                    "name": "B. T. Loo"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2127379668",
                    "name": "Chen Chen"
                }
            ]
        },
        {
            "paperId": "f003d2339a25039f6544c945863a2b716d160e90",
            "title": "Data Provenance for Attributes: Attribute Lineage",
            "abstract": "In this paper we define a new kind of data provenance for database management systems, called attribute lineage for SPJRU queries, building on previous works on data provenance for tuples. We take inspiration from the classical lineage, metadata that enables users to discover which tuples in the input are used to produce a tuple in the output. Attribute lineage is instead defined as the set of all cells in the input database that are used by the query to produce one cell in the output. It is shown that attribute lineage is more informative than simple lineage and we discuss potential new applications for this new metadata.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51214929",
                    "name": "Dennis Dosso"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1688094",
                    "name": "G. Silvello"
                }
            ]
        },
        {
            "paperId": "1c244f7a22fd7d7c216bc2ad9b53f400964e1c7d",
            "title": "ProvCite: Provenance-based Data Citation",
            "abstract": "\n As research products expand to include structured datasets, the challenge arises of how to automatically generate citations to the results of arbitrary queries against such datasets. Previous work explored this problem in the context of\n conjunctive\n queries and views using a Rewriting-Based Model (RBM). However, an increasing number of scientific queries are\n aggregate,\n e.g. statistical summaries of the underlying data, for which the RBM cannot be easily extended. In this paper, we show how a Provenance-Based Model (PBM) can be leveraged to 1) generate citations to conjunctive as well as aggregate queries and views; 2) associate citations with individual result tuples to enable arbitrary subsets of the result set to be cited (\n fine-grained citations\n ); and 3) be optimized to return citations in\n acceptable time.\n Our implementation of PBM in ProvCite shows that it not only handles a larger class of queries and views than RBM, but can outperform it when restricted to conjunctive views in some cases.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3037003",
                    "name": "Yinjun Wu"
                },
                {
                    "authorId": "2517099",
                    "name": "Abdussalam Alawini"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "31f3f495282b63f75e18e409d9c183f4b86cc3dc",
            "title": "Automating Software Citation using GitCite",
            "abstract": "The ability to cite software and give credit to its authors and contributors is increasingly important. While the number of online open-source software repositories has grown rapidly over the past few years, few are being properly cited when used due to the difficulty of creating appropriate citations and the lack of automated techniques. This paper presents GitCite, a model for software citation with version control which enables citations to be inferred for any project component based on a small number of explicit citations attached to subdirectories/files, and an implementation that integrates with Git and GitHub. The implementation includes a browser extension and a local executable tool, which enable citations to be added/modified/deleted to software project repositories and managed through functions such as fork/merge/copy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21023662",
                    "name": "Leshang Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "bc70cb93a457c0b62d3ed11faef167a3b2738943",
            "title": "Data Citation: A New Provenance Challenge",
            "abstract": "In today\u2019s era of big data-driven science, an increasing amount of information is being published as curated online databases and retrieved by queries, raising the question of how query results should be cited. Because it is infeasible to associate citation information with every possible query, one approach is to specify citations for a small set of frequent queries \u2013 citation views \u2013 and then use these views to construct a citation for general queries. In this paper, we describe this model of citation views, how they are used to construct citations for general queries, and an ef\ufb01cient approach to implementing this model. We also show the connection between data citation and data provenance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2517099",
                    "name": "Abdussalam Alawini"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1688094",
                    "name": "G. Silvello"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                },
                {
                    "authorId": "3037003",
                    "name": "Yinjun Wu"
                }
            ]
        },
        {
            "paperId": "bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3",
            "title": "Realizing the potential of data science",
            "abstract": "Data science promises new insights, helping transform information into knowledge that can drive science and industry.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2781539",
                    "name": "F. Berman"
                },
                {
                    "authorId": "1722400",
                    "name": "Rob A. Rutenbar"
                },
                {
                    "authorId": "2341622",
                    "name": "B. Hailpern"
                },
                {
                    "authorId": "2105998204",
                    "name": "Henrik Christensen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "153468945",
                    "name": "D. Estrin"
                },
                {
                    "authorId": "101251726",
                    "name": "Michael J. Franklin"
                },
                {
                    "authorId": "1708269",
                    "name": "M. Martonosi"
                },
                {
                    "authorId": "2059337676",
                    "name": "Padma Raghavan"
                },
                {
                    "authorId": "2598812",
                    "name": "V. Stodden"
                },
                {
                    "authorId": "7934073",
                    "name": "A. Szalay"
                }
            ]
        },
        {
            "paperId": "f58bc09784e63091276fd809c6ab4d660e3bde85",
            "title": "Data Citation: Giving Credit Where Credit is Due",
            "abstract": "An increasing amount of information is being published in structured databases and retrieved using queries, raising the question of how query results should be cited. Since there are a large number of possible queries over a database, one strategy is to specify citations to a small set of frequent queries - citation views - and use these to construct citations to other \"general\" queries. We present three approaches to implementing citation views and describe alternative policies for the joint, alternate and aggregated use of citation views. Extensive experiments using both synthetic and realistic citation views and queries show the trade-offs between the approaches in terms of the time to generate citations, as well as the size of the resulting citation. They also show that the choice of policy has a huge effect both on performance and size, leading to useful guidelines for what policies to use and how to specify citation views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3037003",
                    "name": "Yinjun Wu"
                },
                {
                    "authorId": "2517099",
                    "name": "Abdussalam Alawini"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1688094",
                    "name": "G. Silvello"
                }
            ]
        },
        {
            "paperId": "413e9b12db181406668154b230a767a375a104ee",
            "title": "Generation CS",
            "abstract": "Across North America, universities and colleges are facing a significant increase in enrollment in both undergraduate computer science (CS) courses and programs. The current enrollment surge has exceeded previous CS booms, and there is a general sense that the current growth in enrollment is substantially different from that of the mid-1980s and late 1990s. For example, since the late 1990s, the U.S. Bureau of Labor data shows that the number of jobs where computing skills are needed is on an upward slope [1], illustrating the increased reliance our society has on computing. We also know that more disciplines are becoming increasingly reliant on large amounts of data, and that handling this data effectively depends on having good computational skills. This makes computer science courses at all levels of greater interest to students from other majors.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "144677709",
                    "name": "T. Camp"
                },
                {
                    "authorId": "144120635",
                    "name": "W. R. Adrion"
                },
                {
                    "authorId": "51240170",
                    "name": "Betsy Bizot"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "143896454",
                    "name": "Mary W. Hall"
                },
                {
                    "authorId": "1708512",
                    "name": "Susanne E. Hambrusch"
                },
                {
                    "authorId": "144032617",
                    "name": "E. Walker"
                },
                {
                    "authorId": "1835364",
                    "name": "S. Zweben"
                }
            ]
        },
        {
            "paperId": "4308967390fd768122a6904fa8987a8eb9d79183",
            "title": "A Model for Fine-Grained Data Citation",
            "abstract": "An increasing amount of information is being collected in structured, evolving, curated databases, driving the question of how information extracted from such datasets via queries should be cited. Unlike traditional research products, such books and journals, which have a \ufb01xed granularity, data citation is a challenge because the granularity varies. Di\ufb00erent portions of the database, with varying granularity, may have di\ufb00erent citations. Furthermore, there are an in\ufb01nite number of queries over a database, each accessing and generating di\ufb00erent subsets of the database, so we cannot hope to explicitly attach a citation to every possible result set and/or query. We present the novel problem of automatically generating citations for general queries over a relational database, and explore a solution based on a set of citation views , each of which attaches a citation to a view of the database. Citation views are then used to automatically construct citations for general queries. Our approach draws inspiration from re-sults in two areas, query rewriting using views and database provenance and combines them in a robust model. We then discuss open issues in developing a practical solution to this challenging problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1688094",
                    "name": "G. Silvello"
                }
            ]
        },
        {
            "paperId": "86b28febbca2925dd3e5a623d510af4d1f6e821b",
            "title": "Automating Data Citation: The eagle-i Experience",
            "abstract": "Data citation is of growing concern for owners of curated databases, who wish to give credit to the contributors and curators responsible for portions of the dataset and enable the data retrieved by a query to be later examined. While several databases specify how data should be cited, they leave it to users to manually construct the citations and do not generate them automatically. We report our experiences in automating data citation for an RDF dataset called eagle-i, and discuss how to generalize this to a citation framework that can work across a variety of different types of databases (e.g., relational or XML).",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2517099",
                    "name": "Abdussalam Alawini"
                },
                {
                    "authorId": "21023662",
                    "name": "Leshang Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "24741527",
                    "name": "Natan Portilho Da Silva"
                },
                {
                    "authorId": "1688094",
                    "name": "G. Silvello"
                }
            ]
        },
        {
            "paperId": "898e5b4ad9cbb86e5165bccc4f52deb520efc73f",
            "title": "Data Citation: A Computational Challenge",
            "abstract": "Data citation is an interesting computational challenge, whose solution draws on several well-studied problems in database theory: query answering using views, and provenance. We describe the problem, suggest an approach to its solution, and highlight several open research problems, both practical and theoretical.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1688094",
                    "name": "G. Silvello"
                }
            ]
        },
        {
            "paperId": "b0d6b461f9a84bcc5bbb878b1bfd023ed8a89176",
            "title": "Automating Data Citation in CiteDB",
            "abstract": "\n An increasing amount of information is being collected in structured, evolving, curated databases, driving the question of how information extracted from such datasets via queries should be cited. While several databases say how data should be cited for web-page views of the database, they leave it to users to manually construct the citations. Furthermore, they do not say how data extracted by queries other than web-page views --\n general queries\n -- should be cited. This demo shows how citations can be specified for a small set of views of the database, and used to\n automatically generate citations for general queries\n against the database.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2517099",
                    "name": "Abdussalam Alawini"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2146240777",
                    "name": "Wei Hu"
                },
                {
                    "authorId": "3037003",
                    "name": "Yinjun Wu"
                }
            ]
        },
        {
            "paperId": "ec5011645ffb7fcca1f08a70e75a78f5aa28a714",
            "title": "Generation CS",
            "abstract": "of ACM Inroads [1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144677709",
                    "name": "T. Camp"
                },
                {
                    "authorId": "144032617",
                    "name": "E. Walker"
                },
                {
                    "authorId": "2286777615",
                    "name": "Hiram College"
                },
                {
                    "authorId": "1835364",
                    "name": "S. Zweben"
                },
                {
                    "authorId": "2287061885",
                    "name": "W. Richards"
                },
                {
                    "authorId": "51240170",
                    "name": "Betsy Bizot"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "143896454",
                    "name": "Mary W. Hall"
                },
                {
                    "authorId": "1708512",
                    "name": "Susanne E. Hambrusch"
                }
            ]
        },
        {
            "paperId": "ee6b06077976b21e91be72318398e9585b4eea42",
            "title": "Generation CS",
            "abstract": "I the June issue of ACM Inroads [1], we explored the phenomenal growth of computer science (CS) in CS undergraduate degree programs and CS courses at both doctoral-granting and non-doctoral-granting units1. In the September issue of ACM Inroads [2], we investigated the impact of this enrollment surge on diversity. This article examines (1) the impact of increasing enrollments on computer science units, as reported by the units, and (2) the actions taken or not taken by the 134 doctoraland 93 non-doctoral-granting units that responded to the CRA Enrollment Survey.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144677709",
                    "name": "T. Camp"
                },
                {
                    "authorId": "144120635",
                    "name": "W. R. Adrion"
                },
                {
                    "authorId": "51240170",
                    "name": "Betsy Bizot"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "143896454",
                    "name": "Mary W. Hall"
                },
                {
                    "authorId": "1708512",
                    "name": "Susanne E. Hambrusch"
                },
                {
                    "authorId": "144032617",
                    "name": "E. Walker"
                },
                {
                    "authorId": "1835364",
                    "name": "S. Zweben"
                }
            ]
        },
        {
            "paperId": "53748fe355a6bc85fe5765142075d903f5c09111",
            "title": "PROX: Approximated Summarization of Data Provenance",
            "abstract": "Many modern applications involve collecting large amounts of data from multiple sources, and then aggregating and manipulating it in intricate ways. The complexity of such applications, combined with the size of the collected data, makes it difficult to understand the application logic and how information was derived. Data provenance has been proven helpful in this respect in different contexts; however, maintaining and presenting the full and exact provenance may be infeasible, due to its size and complex structure. For that reason, we introduce the notion of approximated summarized provenance, where we seek a compact representation of the provenance at the possible cost of information loss. Based on this notion, we have developed PROX, a system for the management, presentation and use of data provenance for complex applications. We propose to demonstrate PROX in the context of a movies rating crowd-sourcing system, letting participants view provenance summarization and use it to gain insights on the application and its underlying data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2627111",
                    "name": "Eleanor Ainy"
                },
                {
                    "authorId": "1807924",
                    "name": "P. Bourhis"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "5c683f89444b1732d9f82b705a21ac186b3c971c",
            "title": "Why data citation is a computational problem",
            "abstract": "Using database views to define citable units is the key to specifying and generating citations to data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144899863",
                    "name": "J. Frew"
                }
            ]
        },
        {
            "paperId": "03f50c025e16075d9d0a9e9b280b4a456bc7b62d",
            "title": "Managing General and Individual Knowledge in Crowd Mining Applications",
            "abstract": "Crowd mining frameworks combine general knowledge, which can refer to an ontology or information in a database, with individual knowledge obtained from the crowd, which captures habits and preferences. To account for such mixed knowledge, along with user interaction and optimization issues, such frameworks must employ a complex process of reasoning, automatic crowd task generation and result analysis. In this paper, we describe a generic architecture for crowd mining applications. This architecture allows us to examine and compare the components of existing crowdsourcing systems and point out extensions required by crowd mining. It also highlights new research challenges and potential reuse of existing techniques/components. We exemplify this for the OASSIS project and for other prominent crowdsourcing frameworks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2287536",
                    "name": "Anna Kukliansky"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "16708d0e9bd3da8cc20285cbe1d82a4eb6799e90",
            "title": "Approximated Summarization of Data Provenance",
            "abstract": "Many modern applications involve collecting large amounts of data from multiple sources, and then aggregating and manipulating it in intricate ways. The complexity of such applications, combined with the size of the collected data, makes it difficult to understand how the resulting information was derived. Data provenance has proven helpful in this respect, however, maintaining and presenting the full and exact provenance information may be infeasible due to its size and complexity. We therefore introduce the notion of approximated summarized provenance, which provides a compact representation of the provenance at the possible cost of information loss. Based on this notion, we present a novel provenance summarization algorithm which, based on the semantics of the underlying data and the intended use of provenance, outputs a summary of the input provenance. Experiments measure the conciseness and accuracy of the resulting provenance summaries, and improvement in provenance usage time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2627111",
                    "name": "Eleanor Ainy"
                },
                {
                    "authorId": "1807924",
                    "name": "P. Bourhis"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "1237a326007f0905e1bbbf8492bbdb6fd15c6317",
            "title": "Layer Decomposition: An Effective Structure-Based Approach for Scientific Workflow Similarity",
            "abstract": "Scientific workflows have become a valuable tool for large-scale data processing and analysis. This has led to the creation of specialized online repositories to facilitate workflow sharing and reuse. Over time, these repositories have grown to sizes that call for advanced methods to support workflow discovery, in particular for effective similarity search. Here, we present a novel and intuitive workflow similarity measure that is based on layer decomposition. Layer decomposition accounts for the directed dataflow underlying scientific workflows, a property which has not been adequately considered in previous methods. We comparatively evaluate our algorithm using a gold standard for 24 query workflows from a repository of almost 1500 scientific workflows, and show that it a) delivers the best results for similarity search, b) has a much lower runtime than other, often highly complex competitors in structure-aware workflow comparison, and c) can be stacked easily with even faster, structure-agnostic approaches to further reduce runtime while retaining result quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1954366",
                    "name": "J. Starlinger"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1693022",
                    "name": "U. Leser"
                }
            ]
        },
        {
            "paperId": "23ddaf4c6194af2bc6b53ba217bfedb0a2927381",
            "title": "Approximated Provenance for Complex Applications",
            "abstract": "Many applications now involve the collection of large amounts of data from multiple users, and then aggregating and manipulating it in intricate ways. The complexity of such applications, combined with the size of the collected data, makes it difficult to understand how information was derived, and consequently difficult to asses its credibility, to optimize and debug its derivation, etc. Provenance has been helpful in achieving such goals in different contexts, and we illustrate its potential for novel complex applications such as those performing crowd-sourcing. Maintaining (and presenting) the full and exact provenance information may be infeasible for such applications, due to the size of the provenance and its complex structure. We propose some initial directions towards addressing this challenge, through the notion of approximated provenance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2627111",
                    "name": "Eleanor Ainy"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "2635e1ff129ac64b08ec6be89c3878684e309740",
            "title": "Ontology Assisted Crowd Mining",
            "abstract": "We present OASSIS (for Ontology ASSISted crowd mining), a prototype system which allows users to declaratively specify their information needs, and mines the crowd for answers. The answers that the system computes are concise and relevant, and represent frequent, significant data patterns. The system is based on (1) a generic model that captures both ontological knowledge, as well as the individual knowledge of crowd members from which frequent patterns are mined; (2) a query language in which users can specify their information needs and types of data patterns they seek; and (3) an efficient query evaluation algorithm, for mining semantically concise answers while minimizing the number of questions posed to the crowd. We will demonstrate OASSIS using a couple of real-life scenarios, showing how users can formulate and execute queries through the OASSIS UI and how the relevant data is mined from the crowd.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "828b31e25f3077779c4f2ab115196f9b9e30bee7",
            "title": "Top-k and Clustering with Noisy Comparisons",
            "abstract": "We study the problems of max/top-k and clustering when the comparison operations may be performed by oracles whose answer may be erroneous. Comparisons may either be of type or of value: given two data elements, the answer to a type comparison is \u201cyes\u201d if the elements have the same type and therefore belong to the same group (cluster); the answer to a value comparison orders the two data elements. We give efficient algorithms that are guaranteed to achieve correct results with high probability, analyze the cost of these algorithms in terms of the total number of comparisons (i.e., using a fixed-cost model), and show that they are essentially the best possible. We also show that fewer comparisons are needed when values and types are correlated, or when the error model is one in which the error decreases as the distance between the two elements in the sorted order increases. Finally, we examine another important class of cost functions, concave functions, which balances the number of rounds of interaction with the oracle with the number of questions asked of the oracle. Results of this article form an important first step in providing a formal basis for max/top-k and clustering queries in crowdsourcing applications, that is, when the oracle is implemented using the crowd. We explain what simplifying assumptions are made in the analysis, what results carry to a generalized crowdsourcing setting, and what extensions are required to support a full-fledged model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "d4307fc98506b3fb86298380ebe1fd849e7ee792",
            "title": "OASSIS: query driven crowd mining",
            "abstract": "Crowd data sourcing is increasingly used to gather information from the crowd and to obtain recommendations. In this paper, we explore a novel approach that broadens crowd data sourcing by enabling users to pose general questions, to mine the crowd for potentially relevant data, and to receive concise, relevant answers that represent frequent, significant data patterns. Our approach is based on (1) a simple generic model that captures both ontological knowledge as well as the individual history or habits of crowd members from which frequent patterns are mined; (2) a query language in which users can declaratively specify their information needs and the data patterns of interest; (3) an efficient query evaluation algorithm, which enables mining semantically concise answers while minimizing the number of questions posed to the crowd; and (4) an implementation of these ideas that mines the crowd through an interactive user interface. Experimental results with both real-life crowd and synthetic data demonstrate the feasibility and effectiveness of the approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "15250387",
                    "name": "Slava Novgorodov"
                },
                {
                    "authorId": "2559241",
                    "name": "Amit Somech"
                }
            ]
        },
        {
            "paperId": "dcabb31822f026e294971f9051e856053cc11e22",
            "title": "Answering regular path queries on workflow provenance",
            "abstract": "This paper proposes a novel approach for efficiently evaluating regular path queries over provenance graphs of workflows that may include recursion. The approach assumes that an execution g of a workflow G is labeled with query-agnostic reachability labels using an existing technique. At query time, given g, G and a regular path query R, the approach decomposes R into a set of subqueries R1, ..., Rk that are safe for G. For each safe subquery Ri, G is rewritten so that, using the reachability labels of nodes in g, whether or not there is a path which matches Ri between two nodes can be decided in constant time. The results of each safe subquery are then composed, possibly with some small unsafe remainder, to produce an answer to R. The approach results in an algorithm that significantly reduces the number of subqueries k over existing techniques by increasing their size and complexity, and that evaluates each subquery in time bounded by its input and output size. Experimental results demonstrate the benefit of this approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107884501",
                    "name": "Xiaocheng Huang"
                },
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "26da26955707be4ed29a4e07015bc2e832913806",
            "title": "Learning to explore scientific workflow repositories",
            "abstract": "Scientific workflows are gaining popularity, and repositories of workflows are starting to emerge. In this paper we describe TopicsExplorer, a data exploration approach for myExperiment.org, a collaborative platform for the exchange of scientific workflows and experimental plans. Our approach uses a variant of topic modeling with tags as features, and generates a browsable view of the repository. TopicsExplorer has been fully integrated into the open-source platform of myExperiment.org, and is available to users at www.myexperiment.org/topics. We also present our recently developed personalization component that customizes topics based on user feedback. Finally, we discuss our ongoing performance optimization efforts that make computing and managing personalized topic views of the myExperiment.org repository feasible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1939367",
                    "name": "Paramveer S. Dhillon"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "153193299",
                    "name": "Brian Lyons"
                }
            ]
        },
        {
            "paperId": "437d40af59c8560017fb3ce372e1ea07fa3eaf1a",
            "title": "A cascading mentoring pedagogy in a CS service learning course to broaden participation and perceptions",
            "abstract": "This study reports on the design, implementation, and evaluation of a service-learning course based on a \"cascading mentoring\" model linking together the faculty, administration, and undergraduates of an urban university's computer science department with local high school students. We present findings from surveys and post-interviews that illustrate undergraduates' and high school students' experiences in the program and how their perceptions of computing and mentoring changed based upon the outreach. In our discussion, we focus on the institutional and conceptual challenges of implementing the community service course within the university's computer science department, while also highlighting the learning opportunities for streamlining such a model for future iterations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1726545",
                    "name": "Y. Kafai"
                },
                {
                    "authorId": "46954347",
                    "name": "J. Griffin"
                },
                {
                    "authorId": "2303795",
                    "name": "Q. Burke"
                },
                {
                    "authorId": "19322162",
                    "name": "M. Slattery"
                },
                {
                    "authorId": "145032340",
                    "name": "D. Fields"
                },
                {
                    "authorId": "2747309",
                    "name": "R. Powell"
                },
                {
                    "authorId": "2090232",
                    "name": "Michele Grab"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "31737664",
                    "name": "Joseph S. Sun"
                }
            ]
        },
        {
            "paperId": "7d62e3ee01ea384007f95c3e28db4f3e0dad8526",
            "title": "Understanding Local Structure in Ranked Datasets",
            "abstract": "Ranked data is ubiquitous in real-world applications. Rankings arise naturally when users express preferences about products and services, when voters cast ballots in elections, when funding pro- posals are evaluated based on their merits and university depart- ments based on their reputation, or when genes are ordered based on their expression levels under various experimental conditions. We observe that ranked data exhibits interesting local structure, representing agreement of subsets of rankers over subsets of items. Being able to model, identify and describe such structure is im- portant, because it enables novel kinds of analysis with the poten- tial of making ground-breaking impact, but is challenging to do effectively and efficiently. We argue for the use of fundamental data management principles such as declarativeness and incremen- tal evaluation, in combination with state-of-the-art machine learn- ing and data mining techniques, for addressing the effectiveness and efficiency challenges. We describe the key ingredients of a so- lution, and propose aroadmap towards aframework that will enable robust and efficient analysis of large ranked datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "40076282",
                    "name": "Marie Jacob"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "aa3ace22c3f6b895bb5dd4813fe231c61526bd1e",
            "title": "Using the crowd for top-k and group-by queries",
            "abstract": "Group-by and top-k are fundamental constructs in database queries. However, the criteria used for grouping and ordering certain types of data -- such as unlabeled photos clustered by the same person ordered by age -- are difficult to evaluate by machines. In contrast, these tasks are easy for humans to evaluate and are therefore natural candidates for being crowd-sourced.\n We study the problem of evaluating top-k and group-by queries using the crowd to answer either type or value questions. Given two data elements, the answer to a type question is \"yes\" if the elements have the same type and therefore belong to the same group or cluster; the answer to a value question orders the two data elements. The assumption here is that there is an underlying ground truth, but that the answers returned by the crowd may sometimes be erroneous. We formalize the problems of top-k and group-by in the crowd-sourced setting, and give efficient algorithms that are guaranteed to achieve good results with high probability. We analyze the crowd-sourced cost of these algorithms in terms of the total number of type and value questions, and show that they are essentially the best possible. We also show that fewer questions are needed when values and types are correlated, or when the error model is one in which the error decreases as the distance between the two elements in the sorted order increases.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "e44c26f7e925299fcc803815bb0dc285fa8c7de6",
            "title": "Search and result presentation in scientific workflow repositories",
            "abstract": "We study the problem of searching a repository of complex hierarchical workflows whose component modules, both composite and atomic, have been annotated with keywords. Since keyword search does not use the graph structure of a workflow, we develop a model of workflows using context-free bag grammars. We then give efficient polynomial-time algorithms that, given a workflow and a keyword query, determine whether some execution of the workflow matches the query. Based on these algorithms we develop a search and ranking solution that efficiently retrieves the top-k grammars from a repository. Finally, we propose a novel result presentation method for grammars matching a keyword query, based on representative parse-trees. The effectiveness of our approach is validated through an extensive experimental evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2107884501",
                    "name": "Xiaocheng Huang"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "e8229d1cf83bf8ed36509c9de74b8b08995cb0a3",
            "title": "Education and career paths for data scientists",
            "abstract": "MOTIVATION: As industry and science are increasingly data-driven, the need for skilled data scientists is exceeding what our universities are producing. According to a Mckinsey report: \"By 2018, the United States alone could face a shortage of 140,000 to 190,000 people with deep analytical skills\". Similarly, the ability to extract knowledge from scientific data is accelerating discovery and we need the next generation of domain scientists to be experts not only in their domain but also in data management. At the same time, however, researchers in academia who focus on building instruments or data management tools are often less recognized for their contributions than researchers focusing purely on the actual science.\n OVERVIEW: The goal of this panel will be to discuss all these challenges. We will discuss various aspects of how we should be educating both the emerging \"data science\" experts and the next generation of database and domain science experts. The panel will also discuss career paths for researchers who choose to specialize in developing new methods and tools for Big Data management in domain sciences, with recommendations for how we should better support these less traditional career paths.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1718134",
                    "name": "M. Balazinska"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1686294",
                    "name": "Bill Howe"
                },
                {
                    "authorId": "1716685",
                    "name": "Alexandros Labrinidis"
                }
            ]
        },
        {
            "paperId": "1c77c523c64cfcf43f6ef8bab6785700b40c913b",
            "title": "A propagation model for provenance views of public/private workflows",
            "abstract": "We study the problem of concealing functionality of a proprietary or private module when provenance information is shown over repeated executions of a workflow which contains both public and private modules. Our approach is to use provenance views to hide carefully chosen subsets of data over all executions of the workflow to ensure \u0393-privacy: for each private module and each input x, the module's output f(x) is indistinguishable from \u0393--1 other possible values given the visible data in the workflow executions. We show that \u0393-privacy cannot be achieved simply by combining solutions for individual private modules; data hiding must also be propagated through public modules. We then examine how much additional data must be hidden and when it is safe to stop propagating data hiding. The answer depends strongly on the workflow topology as well as the behavior of public modules on the visible data. In particular, for a class of workflows (which include the common tree and chain workflows), taking private solutions for each private module, augmented with a public closure that is upstream-downstream safe, ensures \u0393-privacy. We define these notions formally and show that the restrictions are necessary. We also study the related optimization problems of minimizing the amount of hidden data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "7ad018e3da84a73d476626472527b4f641f66751",
            "title": "Provenance and uncertainty",
            "abstract": "Data provenance, a record of the origin and transformation of data, explains how output data is derived from input data. This dissertation focuses on exploring the connection between provenance and uncertainty in two main directions: (1) how a succinct representation of provenance can help infer uncertainty in the input or the output, and (2) how introducing uncertainty can facilitate publishing provenance information while hiding associated private information. \nA significant fraction of the data found in practice is imprecise, unreliable, and incomplete, and therefore uncertain. The level of uncertainty in the data must be measured and recorded in order to estimate the confidence in the results and find potential sources of error. In probabilistic databases, uncertainty in the input is recorded as a probability distribution, and the goal is to efficiently compute the induced probability distribution on the outputs. In general, this problem is computationally hard, and we seek to expand the class of inputs for which efficient evaluation is possible by exploiting provenance structure. \nIn some scenarios, the output data is directly examined for errors and is labeled accordingly. We need to trace back the errors in the output to the input so that the input can be refined for future processing. Because of incomplete labeling of the output and complexity of the processes generating it, the sources of error may be uncertain. We formalize the problem of source refinement, and propose models and solutions using provenance that can handle incomplete labeling. We also evaluate our solutions empirically for an application of source refinement in information extraction . \nData provenance is extensively used to help understand and debug scientific experiments that often involve proprietary and sensitive information. In this dissertation, we consider privacy of proprietary and commercial modules when they belong to a workflow and interact with other modules. We propose a model for module privacy that makes the exact functionality of the modules uncertain by selectively hiding provenance information. We also study the optimization problem of minimizing the information hidden while guaranteeing a desired level of privacy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "82bfbd976d4170249f20f44bd240c93b1ba21a5b",
            "title": "Efficiently tracking provenance in scientific workflows",
            "abstract": "Tracking the provenance of data produced by a workflow execution involves answering reachability queries over large provenance graphs, which can be expensive. For that, we present compact labeling schemes for efficiently answering reachability queries over provenance graphs that are derived from executions of a given workflow specification. The idea is to assign each node a reachability label such that using only the labels of any two nodes, we can quickly decide if one can reach the other. Our proposed schemes build logarithmic-size labels in linear time, and answer any query in constant time. \nIn this dissertation, we consider the reachability labeling problem for a variety of workflow settings. First, we study the static labeling problem, where the entire provenance graph is given as input. For workflows with well-nested loops and forks (i.e., parallel executions), we develop a skeleton-based labeling approach which uses the labeling for the specification as an effective skeleton for designing the labeling for its executions. Next, we turn to the dynamic labeling problem, where the input provenance graph grows over time but the nodes must be labeled on-the-fly. We first show that, in general, for workflows that contain arbitrary recursion, dynamic labeling of their executions requires long (linear-size) labels. Nevertheless, we identify a natural class of workflows with linear recursion, for which dynamic, yet compact (logarithmic-size) labeling is possible. Finally, we revisit the dynamic labeling problem when fine-grained dependencies between inputs and outputs of modules are defined over multiple workflow views. It turns out that the restriction of linear recursion, which suffices to reduce the label length before, is no longer helpful. However, for a more restricted class of workflows with strictly linear recursion and safe views, we propose a novel view-adaptive dynamic labeling approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                }
            ]
        },
        {
            "paperId": "b46be9e73c67b1ab5dd333b3ac3b31351826d97d",
            "title": "Labeling Workflow Views with Fine-Grained Dependencies",
            "abstract": "This paper considers the problem of efficiently answering reachability queries over views of provenance graphs, derived from executions of workflows that may include recursion. Such views include composite modules and model fine-grained dependencies between module inputs and outputs. A novel view-adaptive dynamic labeling scheme is developed for efficient query evaluation, in which view specifications are labeled statically (i.e. as they are created) and data items are labeled dynamically as they are produced during a workflow execution. Although the combination of fine-grained dependencies and recursive workflows entail, in general, long (linear-size) data labels, we show that for a large natural class of workflows and views, labels are compact (logarithmic-size) and reachability queries can be evaluated in constant time. Experimental results demonstrate the benefit of this approach over the state-of-the-art technique when applied for labeling multiple views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "bea391c2a6b6b22acc694dcdb0dc2aaa6dad9398",
            "title": "The reflective mentor: charting undergraduates' responses to computer science service learning (abstract only)",
            "abstract": "Community service courses are often where undergraduates make connections between academic content and practical computer science applications, build bridges between the university and the community, and ultimately increase access to technology in such communities. In this poster we report on our efforts-supported by a NSF Broadening Participation in Computing grant-to design, implement, and evaluate a service-learning course based on a \"cascading mentoring\" model linking together the faculty, administration, and undergraduates of an urban university's computer science department with area high school students. The poster presents the cascading model and through a series of post-interviews offers preliminary data charting undergraduates' experiences as both mentees and mentors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2303795",
                    "name": "Q. Burke"
                },
                {
                    "authorId": "1726545",
                    "name": "Y. Kafai"
                },
                {
                    "authorId": "46954347",
                    "name": "J. Griffin"
                },
                {
                    "authorId": "2747309",
                    "name": "R. Powell"
                },
                {
                    "authorId": "2090232",
                    "name": "Michele Grab"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "31737664",
                    "name": "Joseph S. Sun"
                }
            ]
        },
        {
            "paperId": "2d33cc316f29dcfaebf334026743a5af00e46ba1",
            "title": "Labeling recursive workflow executions on-the-fly",
            "abstract": "This paper presents a compact labeling scheme for answering reachability queries over workflow executions. In contrast to previous work, our scheme allows nodes (processes and data) in the execution graph to be labeled on-the-fly, i.e., in a dynamic fashion. In this way, reachability queries can be answered as soon as the relevant data is produced. We first show that, in general, for workflows that contain recursion, dynamic labeling of executions requires long (linear-size) labels. Fortunately, most real-life scientific workflows are linear recursive, and for this natural class we show that dynamic, yet compact (logarithmic-size) labeling is possible. Moreover, our scheme labels the executions in linear time, and answers any reachability query in constant time. We also show that linear recursive workflows are, in some sense, the largest class of workflows that allow compact, dynamic labeling schemes. Interestingly, the empirical evaluation, performed over both real and synthetic workflows, shows that our proposed dynamic scheme outperforms the state-of-the-art static scheme for large executions, and creates labels that are shorter by a factor of almost 3.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "2f1ac823d3763e5fc9d9bd8926554da0372b36f3",
            "title": "Enabling Privacy in Provenance-Aware Workflow Systems",
            "abstract": "A new paradigm for creating and correcting scientific analyses is emerging, that of provenance-aware workflow systems. In such systems, repositories of workflow specifications and of provenance graphs that represent their executions will be made available as part of scientific information sharing. This will allow users to search and query both workflow specifications and their provenance graphs: Scientists who wish to perform new analyses may search workflow repositories to find specifications of interest to reuse or modify. They may also search provenance information to understand the meaning of a workflow, or to debug a specification. Finding erroneous or suspect data, a user may then ask provenance queries to determine what downstream data might have been affected, or to understand how the process failed that led to creating the data. With the increased amount of available provenance information, there is a need to efficiently search and query scientific workflows and their executions. However, workflow authors or owners may wish to keep some information in the repository confidential. For example, intermediate data within an execution may contain sensitive information, such as a social security number, a medical record, or financial information about an individual. Although users with the appropriate access level may be allowed to see such confidential data, making it available to all users, even for scientific purposes, is an unacceptable breach of privacy. Beyond data privacy, a module itself may be proprietary, and hiding its description may not be enough: users without the appropriate access level should not be able to infer its behavior if they are allowed to see the inputs and outputs of the module. Finally, details of how certain modules in the workflow are connected may be proprietary, and so showing how data is passed between modules may reveal too much of the structure of the workflow. There is thus an inherent tradeoff between the utility of the information provided in response to a search/query and the privacy guarantees that",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                },
                {
                    "authorId": "2118047969",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "7cc425ee84d48c847a121e2966b92440a86e9aca",
            "title": "A Fine-Grained Workflow Model with Provenance-Aware Security Views",
            "abstract": "In this paper we propose a fine-grained workflow model, based on context-free graph grammars, in which the dependency relation between the inputs and outputs of a module is explicitly specified as a bipartite graph. Using this model, we develop an access control mechanism that supports provenance-aware security views. Our security model not only protects sensitive data and modules from unauthorized access, but also provides the flexibility to expose correct or partially correct data dependency relationships within the provenance information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "a47123740daa6c4e5ffec54f8618c5178f5e3e0b",
            "title": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, Melbourne, Victoria, Australia, May 31 - June 4, 2015",
            "abstract": "Welcome to SIGMOD 2015 -- officially, the 2015 ACM SIGMOD International Conference on the Management of Data! This year's conference is being held in the beautiful cultural capital of Australia, Melbourne. During the Gold Rush period of the 19th Century, Melbourne was the richest city in the world, and as a result it is filled with many unique neighborhoods and distinctive buildings. In addition to wonderful neighborhoods to explore, the city has great museums and other cultural attractions, as well as a fine multi-cultural atmosphere. For those who would like to explore the outdoors, popular highlights are the Phillip Island Nature Park (90 minutes away), which features wild penguins who return in a parade each day at sunset, and the Great Ocean Road, one of the world's most scenic coastal drives, including the famous towering 12 Apostles. \n \nSIGMOD 2015's exciting technical program reflects not only traditional topics, but the database community's role in broader data science and data analytics. The keynote from Laura Haas, \"The Power Behind the Throne: Information Integration in the Age of Data-Driven Discovery\" highlights the role of database and data integration techniques in the growing field of data science. Jignesh Patel's talk, \"From Data to Insights @ Bare Metal Speed,\" explains how hardware and software need to be co-evolved to support the needs of scalable data analytics. Jennifer Widom, winner of the 2015 ACM-W Athena Lecturer Award for fundamental contributions to computer science, will give her award talk, \"Three Favorite Results,\" on Tuesday. Christopher Re will lead a panel on \"Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?,\" with participants Divyakant Agrawal, Magdalena Balazinska, Michael Cafarella, Michael Jordan, Tim Kraska, and Raghu Ramakrishnan. Of course, there are also 106 research paper presentations, 4 tutorials, 30 demonstrations, and 18 industrial papers. Papers will be presented both as talks during the research sessions, and as part of plenary Poster Sessions. \n \nSIGMOD 2015 is preceded by the PhD Workshop, as well as workshops on leading-edge topics like data analytics (DanaC), databases and the Web (WebDB), exploratory search (ExploreDB), managing and mining spatial data (GeoRich), and graph data (GRADES); the New Researcher Symposium will take place on Wednesday. The banquet will be held in a Melbourne landmark, the Town Hall. \n \nAs in recent years, we had two submission deadlines for SIGMOD this year, one in August and one in November. The review process was journal-style, with multiple rounds of reviews coordinated by the Group Leaders. We accepted 34 of 137 papers from the first deadline and 72 of 278 from the second deadline. The total acceptance rate was about 25.5%, and we believe that the revision processhas improved the quality of the technical program.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144302930",
                    "name": "T. Sellis"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1804315",
                    "name": "Z. Ives"
                }
            ]
        },
        {
            "paperId": "c0f4b19da03f39d7b088d1edfc8480f77e6a96bf",
            "title": "Generating sound workflow views for correct provenance analysis",
            "abstract": "Workflow views abstract groups of tasks in a workflow into high level composite tasks, in order to reuse subworkflows and facilitate provenance analysis. However, unless a view is carefully designed, it may not preserve the dataflow between tasks in the workflow, that is, it may not be sound. Unsound views can be misleading and cause incorrect provenance analysis.\n This article studies the problem of efficiently identifying and correcting unsound workflow views with minimal changes, and constructing minimal sound and elucidative workflow views with a set of user-specified relevant tasks. In particular, two related problems are investigated. First, given a workflow view, we wish to split each unsound composite task into the minimal number of tasks, such that the resulting view is sound. Second, given a workflow and a set of user specified relevant tasks, we generate a sound view, such that each composite task contains at most one relevant task, and the total number of tasks is minimized. We prove that both problems are NP-hard by reduction from independent set. We then propose two local optimality conditions (weak and strong) for each problem, and design polynomial time algorithms for both problems to meet these conditions. Experiments show that our proposed algorithms are reasonably effective and efficient. The proposed techniques are useful for view analysis/construction for not only workflows, but general networks as well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39789747",
                    "name": "Ziyang Liu"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2118047969",
                    "name": "Yi Chen"
                }
            ]
        },
        {
            "paperId": "d6a6a60d62ae47a44dc730ff56dd745982399be1",
            "title": "Deriving probabilistic databases with inference ensembles",
            "abstract": "Many real-world applications deal with uncertain or missing data, prompting a surge of activity in the area of probabilistic databases. A shortcoming of prior work is the assumption that an appropriate probabilistic model, along with the necessary probability distributions, is given. We address this shortcoming by presenting a framework for learning a set of inference ensembles, termed meta-rule semi-lattices, or MRSL, from the complete portion of the data. We use the MRSL to infer probability distributions for missing data, and demonstrate experimentally that high accuracy is achieved when a single attribute value is missing per tuple. We next propose an inference algorithm based on Gibbs sampling that accurately predicts the probability distribution for multiple missing values. We also develop an optimization that greatly improves performance of multi-attribute inference for collections of tuples, while maintaining high accuracy. Finally, we develop an experimental framework to evaluate the efficiency and accuracy of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "e7d04e159d5e6982423d4d8068fd56a039346b0f",
            "title": "Keyword Search in Workflow Repositories with Access Control",
            "abstract": "A number of on-line repositories of scientific workflows are emerging. These repositories enable sharing and reuse of workflows, and aid in the design of new workflows. The growing size of these repositories, the complex hierarchical structure of the workflows, and the need to incorporate access control mechanisms make information discovery in these repositories an interesting challenge. This paper formalizes keyword search in repositories of hierarchical workflows. We start by defining what it means for a single hierarchical workflow to match a keyword query while accounting for access control, and discuss options for displaying the resulting matches within the workflow. We extend this to search over workflow repositories, by proposing various ranking semantics that build on techniques from XML search and from information retrieval, and adapting them to our setting. Disciplines Computer Sciences Comments Davidson, S., Lee, S., & Stoyanovich, J., Keyword Search in Workflow Repositories with Access Control, 5th Alberto Mendelzon Workshop on Foundations of Data Management, 2011, http://ceur-ws.org/ This conference paper is available at ScholarlyCommons: http://repository.upenn.edu/cis_papers/595 Keyword Search in Workflow Repositories with Access Control Susan Davidson, Soohyun M. Lee, and Julia Stoyanovich {susan,soohyunl,jstoy}@cis.upenn.edu Computer and Information Science Department University of Pennsylvania, Philadelphia, PA, USA Abstract. A number of on-line repositories of scientific workflows are emerging. These repositories enable sharing and reuse of workflows, and aid in the design of new workflows. The growing size of these repositories, the complex hierarchical structure of the workflows, and the need to incorporate access control mechanisms make information discovery in these repositories an interesting challenge. This paper formalizes keyword search in repositories of hierarchical workflows. We start by defining what it means for a single hierarchical workflow to match a keyword query while accounting for access control, and discuss options for displaying the resulting matches within the workflow. We extend this to search over workflow repositories, by proposing various ranking semantics that build on techniques from XML search and from information retrieval, and adapting them to our setting. A number of on-line repositories of scientific workflows are emerging. These repositories enable sharing and reuse of workflows, and aid in the design of new workflows. The growing size of these repositories, the complex hierarchical structure of the workflows, and the need to incorporate access control mechanisms make information discovery in these repositories an interesting challenge. This paper formalizes keyword search in repositories of hierarchical workflows. We start by defining what it means for a single hierarchical workflow to match a keyword query while accounting for access control, and discuss options for displaying the resulting matches within the workflow. We extend this to search over workflow repositories, by proposing various ranking semantics that build on techniques from XML search and from information retrieval, and adapting them to our setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2108216030",
                    "name": "Soo-Youn Lee"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "ef75f2cd244f89812152df76b006ee72810ad7db",
            "title": "Putting Lipstick on Pig: Enabling Database-style Workflow Provenance",
            "abstract": "Workflow provenance typically assumes that each module is a \"black-box\", so that each output depends on all inputs (coarse-grained dependencies). Furthermore, it does not model the internal state of a module, which can change between repeated executions. In practice, however, an output may depend on only a small subset of the inputs (fine-grained dependencies) as well as on the internal state of the module. We present a novel provenance framework that marries database-style and workflow-style provenance, by using Pig Latin to expose the functionality of modules, thus capturing internal state and fine-grained dependencies. A critical ingredient in our solution is the use of a novel form of provenance graph that models module invocations and yields a compact representation of fine-grained workflow provenance. It also enables a number of novel graph transformation operations, allowing to choose the desired level of granularity in provenance querying (ZoomIn and ZoomOut), and supporting \"what-if\" workflow analytic queries. We implemented our approach in the Lipstick system and developed a benchmark in support of a systematic performance evaluation. Our results demonstrate the feasibility of tracking and querying fine-grained workflow provenance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "25f12750ab7bfda8200d4859aecc745afeac61e3",
            "title": "An optimal labeling scheme for workflow provenance using skeleton labels",
            "abstract": "We develop a compact and efficient reachability labeling scheme for answering provenance queries on workflow runs that conform to a given specification. Even though a workflow run can be structurally more complex and can be arbitrarily larger than the specification due to fork (parallel) and loop executions, we show that a compact reachability labeling for a run can be efficiently computed using the fact that it originates from a fixed specification. Our labeling scheme is optimal in the sense that it uses labels of logarithmic length, runs in linear time, and answers any reachability query in constant time. Our approach is based on using the reachability labeling for the specification as an effective skeleton for designing the reachability labeling for workflow runs. We also demonstrate empirically the effectiveness of our skeleton-based labeling approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "6628a1db0677f38a9413628fb2a96d2ca9fa7147",
            "title": "Privacy issues in scientific workflow provenance",
            "abstract": "A scientific workflow often deals with proprietary modules as well as private or confidential data, such as health or medical information. Hence providing exact answers to provenance queries over all executions of the workflow may reveal private information. In this paper we first study the potential privacy issues in a scientific workflow -- module privacy, data privacy, and provenance privacy, and frame several natural questions: (i) can we formally analyze module, data or provenance privacy giving provable privacy guarantees for an unlimited/bounded number of provenance queries? (ii) how can we answer provenance queries, providing as much information as possible to the user while still guaranteeing the required privacy? Then we look at module privacy in detail and propose a formal model from our recent work in [11]. Finally we point to several directions for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                }
            ]
        },
        {
            "paperId": "6e1ac72bb153f69902d0d71b79e8282aac48e75b",
            "title": "Exploring repositories of scientific workflows",
            "abstract": "Scientific workflows are gaining popularity, and repositories of workflows are starting to emerge. In this paper we present some initial experiences of information discovery in repositories of scientific workflows. In the first part of the paper we consider a collection of VisTrails workflows, and explore how this collection may be summarized when workflow modules are used as features. We present a hierarchical browsable view of the repository in which categories are derived using frequent itemset mining or latent Dirichlet allocation. We demonstrate that both approaches may be used for effective data exploration. In the second part of the paper we focus on a collection of Taverna workflows from myExperiment.org, and consider how these workflows may be browsed using modules and tags as features. Finally, we outline some interesting challenges and describe conditions under which these techniques work well for repositories of scientific workflows, and conditions under which additional work is needed for effective data exploration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1685978",
                    "name": "B. Taskar"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "af0b30fdc335b476bf6ce91525d7558626c508e6",
            "title": "Provenance views for module privacy",
            "abstract": "Scientific workflow systems increasingly store provenance information about the module executions used to produce a data item, as well as the parameter settings and intermediate data items passed between module executions. However, authors/owners of workflows may wish to keep some of this information confidential. In particular, a module may be proprietary, and users should not be able to infer its behavior by seeing mappings between all data inputs and outputs. The problem we address in this paper is the following: Given a workflow, abstractly modeled by a relation R, a privacy requirement ? and costs associated with data. The owner of the workflow decides which data (attributes) to hide, and provides the user with a view R' which is the projection of R over attributes which have not been hidden. The goal is to minimize the cost of hidden data while guaranteeing that individual modules are ?-private. We call this the Secure-View problem. We formally define the problem, study its complexity, and offer algorithmic solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "1715972",
                    "name": "Debmalya Panigrahi"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "d8769505bf2e93ae3ff58c2a1d43ee529bfb38b0",
            "title": "Preserving Module Privacy in Workflow Provenance",
            "abstract": "We study the problem of providing workflow data provenance without revealing the functionality of any module. We develop a model that formalizes the notion of privacy of modules embedded in a workflow structure as a natural extension of privacy of standalone modules. Our model shows that by hiding a small amount of carefully chosen data, one can ensure privacy of all modules over an unbounded number of executions. The problem of identifying the smallest possible amount of such data is NP-hard, and in the full generality of our model it is in fact even hard to get a good approximation. However, we are able to design good approximation algorithms for optimizing the amount of hidden data when either the privacy model is slighted restricted or there is bounded sharing of data items among various modules. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-10-22. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/931 Preserving Module Privacy in Workflow Provenance Susan B. Davidson University of Pennsylvania susan@cis.upenn.edu Sanjeev Khanna University of Pennsylvania sanjeev@cis.upenn.edu Debmalya Panigrahi CSAIL, MIT debmalya@mit.edu Sudeepa Roy University of Pennsylvania sudeepa@cis.upenn.edu Technical Report MS-CIS-10-22 Department of Computer and Information Science University of Pennsylvania",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "1715972",
                    "name": "Debmalya Panigrahi"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "fe5b499c3bebc594b5b0382b9460723a2418ade5",
            "title": "On provenance and privacy",
            "abstract": "Provenance in scientific workflows is a double-edged sword. On the one hand, recording information about the module executions used to produce a data item, as well as the parameter settings and intermediate data items passed between module executions, enables transparency and reproducibility of results. On the other hand, a scientific workflow often contains private or confidential data and uses proprietary modules. Hence, providing exact answers to provenance queries over all executions of the workflow may reveal private information. In this paper we discuss privacy concerns in scientific workflows -- data, module, and structural privacy - and frame several natural questions: (i) Can we formally analyze data, module, and structural privacy, giving provable privacy guarantees for an unlimited/bounded number of provenance queries? (ii) How can we answer search and structural queries over repositories of workflow specifications and their executions, providing as much information as possible to the user while still guaranteeing privacy? We then highlight some recent work in this area and point to several directions for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                },
                {
                    "authorId": "2118047969",
                    "name": "Yi Chen"
                }
            ]
        },
        {
            "paperId": "097716e5aa3349a57ab041ab14a4cf635d3669df",
            "title": "Optimizing user views for workflows",
            "abstract": "A technique called user views has recently been proposed to focus user attention on relevant information in response to provenance queries over workflow executions [1, 2]: Given user input on what modules in the workflow specification are relevant to the user, a user view is a concise representation that clusters together modules to create a small number of composite modules (or clusters) such that (1) each composite module in a user view contains at most one relevant (atomic) module, thus assuming the \"meaning\" of that module; and (2) no control or data dependencies (either direct or indirect) are introduced (soundness) or removed (completeness) between relevant modules. The goal is to find a user view with a smallest number of composite modules.\n We show that for workflow specifications that are general graphs, regardless of the number of distinct modules in the input workflow and the structure of interaction between them, there always exists a user view of size at most (2k--1 -- k)2 + k, where k is the number of relevant modules. Moreover, a good user view with at most (2k--1 -- k)2 + k clusters can be computed in polynomial time in the size of the graph. We also show that this upper bound is tight. Thus in general graphs, the number of composite modules can be exponentially large in k even in an optimum user view for the specification. We also give a characterization of a good user view in terms of structural properties of each cluster in the user view.\n However, for series-parallel workflow graphs, we show that there is always a user-view with at most 2k -- 3 composite modules; further, there exist series-parallel graphs where every user view requires at least 2k -- 3 composite modules. Such graphs capture the structure of many scientific and other workflows that we have encountered in practice. For this class of graphs, we give a simple, linear time algorithm for constructing an optimum user view for a given specification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2476931",
                    "name": "Olivier Biton"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                }
            ]
        },
        {
            "paperId": "3a21e44765d8d4085e35b2f703ea1caaf2ea7eaa",
            "title": "WOLVES: Achieving Correct Provenance Analysis by Detecting and Resolving Unsound Workflow Views",
            "abstract": "Workflow views abstract groups of tasks in a workflow into composite tasks, and are used for simplifying provenance analysis, workflow sharing and reuse. An unsound view does not preserve the dataflow between tasks in the workflow, and can therefore cause incorrect provenance analysis. In this demo we present WOLVES, a system that efficiently identifies and corrects unsound workflow views with minimal changes (view correction). Since the view correction problem is NP-hard, WOLVES allows the user to choose between two forms of local optimality, strong and weak. Efficient time algorithms achieving these optimalities are implemented in WOLVES.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2075416186",
                    "name": "Peng Sun"
                },
                {
                    "authorId": "39789747",
                    "name": "Ziyang Liu"
                },
                {
                    "authorId": "143927101",
                    "name": "Sivaramakrishnan R. Natarajan"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2118047969",
                    "name": "Yi Chen"
                }
            ]
        },
        {
            "paperId": "53676b318fde17d34ce84e5f2a1803e174589e16",
            "title": "On User Views in Scientific Workflow Systems",
            "abstract": "An increasing number of scientific workflow systems are providing support for the automated tracking and storage of provenance information. However, the amount of provenance information recorded can become very large, even for a single. execution of a workflow - [6] estimates a ten-fold blowup of the size of the original input data. There is therefore a need to provide ways of allowing users to focus their attention on meaningful provenance information in provenance queries. We highlight recent work in this area on user views, showing how they can be efficiently computed given user input on relevance, or and how pre-existing views can be corrected to provide accurate provenance information. We also discuss how to search a repository of workflow specifications and their views, returning workflows at an appropriate level of complexity with respect to a hierarchy of views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2118047969",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "2075416186",
                    "name": "Peng Sun"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                }
            ]
        },
        {
            "paperId": "912c8fdc1b98ce7ca19f04936ffb7f8bc54b148a",
            "title": "Differencing Provenance in Scientific Workflows",
            "abstract": "Scientific workflow management systems are increasingly providing the ability to manage and query the provenance of data products. However, the problem of differencing the provenance of two data products produced by executions of the same specification has not been adequately addressed. Although this problem is NP-hard for general workflow specifications, an analysis of real scientific (and business) workflows shows that their specifications can be captured as series-parallel graphs overlaid with well-nested forking and looping. For this natural restriction, we present efficient, polynomial-time algorithms for differencing executions of the same specification and thereby understanding the difference in the provenance of their data products. We then describe a prototype called PDiffView built around our differencing algorithm. Experimental results demonstrate the scalability of our approach using collected, real workflows and increasingly complex runs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "143658928",
                    "name": "Anat Eyal"
                },
                {
                    "authorId": "144335444",
                    "name": "S. Khanna"
                }
            ]
        },
        {
            "paperId": "aada3f0acedce05c9b22757aad1b1a17700ee80d",
            "title": "PDiffView: Viewing the Difference in Provenance of Workflow Results",
            "abstract": "Scientific workflow systems are becoming increasingly important for managing in-silico experiments. Such experiments are typically specified as directed flow graphs, in which the nodes represent modules and edges represent data flow between the modules. Each execution (a.k.a. run) of an experiment may vary the parameters and data inputs to the modules in the specification; furthermore, alternative paths of the workflow may be followed. In this process, the scientist's goal is to identify parameter settings and approaches which lead to good final results. Comparing workflow executions of the same specification and understanding the difference between them is thus of paramount importance for understanding the provenance of final results [4].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2685772",
                    "name": "Zhuowei Bao"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2055755045",
                    "name": "Pierrick Girard"
                }
            ]
        },
        {
            "paperId": "de6e1db844c95bd93df8ebbbca543fbca4783512",
            "title": "Detecting and resolving unsound workflow views for correct provenance analysis",
            "abstract": "Workflow views abstract groups of tasks in a workflow into high level composite tasks, in order to reuse sub-workflows and facilitate provenance analysis. However, unless a view is carefully designed, it may not preserve the dataflow between tasks in the workflow, i.e., it may not be sound. Unsound views can be misleading and cause incorrect provenance analysis. This paper studies the problem of efficiently identifying and correcting unsound workflow views with minimal changes. In particular, given a workflow view, we wish to split each unsound composite task into the minimal number of tasks, such that the resulting view is sound. We prove that this problem is NP-hard by reduction from independent set. We then propose two local optimality conditions (weak and strong), and design polynomial time algorithms for correcting unsound views to meet these conditions. Experiments show that our proposed algorithms are effective and efficient, and that the strong local optimality algorithm produces better solutions than the weak local optimality algorithm with little processing overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2075416186",
                    "name": "Peng Sun"
                },
                {
                    "authorId": "39789747",
                    "name": "Ziyang Liu"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2118047969",
                    "name": "Yi Chen"
                }
            ]
        },
        {
            "paperId": "2ef259d974fd375f28413da094a72c8f6526ed66",
            "title": "The First Provenance Challenge",
            "abstract": "The first Provenance Challenge was set up in order to provide a forum for the community to help understand the capabilities of different provenance systems and the expressiveness of their provenance representations. To this end, a Functional Magnetic Resonance Imaging workflow was defined, which participants had to either simulate or run in order to produce some provenance representation, from which a set of identified queries had to be implemented and executed. Sixteen teams responded to the challenge, and submitted their inputs. In this paper, we present the challenge workflow and queries, and summarise the participants contributions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1773775",
                    "name": "L. Moreau"
                },
                {
                    "authorId": "1716911",
                    "name": "Bertram Lud\u00e4scher"
                },
                {
                    "authorId": "1729053",
                    "name": "I. Altintas"
                },
                {
                    "authorId": "1692499",
                    "name": "R. Barga"
                },
                {
                    "authorId": "1900211",
                    "name": "S. Bowers"
                },
                {
                    "authorId": "8354430",
                    "name": "Steven P. Callahan"
                },
                {
                    "authorId": "144465343",
                    "name": "George Chin"
                },
                {
                    "authorId": "2720709",
                    "name": "Ben Clifford"
                },
                {
                    "authorId": "2321516",
                    "name": "Shirley Cohen"
                },
                {
                    "authorId": "2253673465",
                    "name": "Sarah Cohen-Boulakia"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1687896",
                    "name": "E. Deelman"
                },
                {
                    "authorId": "1920295",
                    "name": "L. Digiampietri"
                },
                {
                    "authorId": "1698701",
                    "name": "Ian T Foster"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "144899863",
                    "name": "J. Frew"
                },
                {
                    "authorId": "2518189",
                    "name": "J. Futrelle"
                },
                {
                    "authorId": "20938422",
                    "name": "Tara D. Gibson"
                },
                {
                    "authorId": "145526918",
                    "name": "Y. Gil"
                },
                {
                    "authorId": "46555127",
                    "name": "C. Goble"
                },
                {
                    "authorId": "1713898",
                    "name": "J. Golbeck"
                },
                {
                    "authorId": "1727784",
                    "name": "Paul T. Groth"
                },
                {
                    "authorId": "1918069",
                    "name": "David A. Holland"
                },
                {
                    "authorId": "144339919",
                    "name": "Sheng Jiang"
                },
                {
                    "authorId": "39045812",
                    "name": "J. Kim"
                },
                {
                    "authorId": "1766755",
                    "name": "D. Koop"
                },
                {
                    "authorId": "2300926",
                    "name": "A. K\u0159enek"
                },
                {
                    "authorId": "2737817",
                    "name": "T. McPhillips"
                },
                {
                    "authorId": "39173862",
                    "name": "Gaurang Mehta"
                },
                {
                    "authorId": "145116379",
                    "name": "S. Miles"
                },
                {
                    "authorId": "29923142",
                    "name": "D. Metzger"
                },
                {
                    "authorId": "1773720",
                    "name": "S. Munroe"
                },
                {
                    "authorId": "1767834",
                    "name": "J. Myers"
                },
                {
                    "authorId": "69334655",
                    "name": "Beth Plale"
                },
                {
                    "authorId": "1734819",
                    "name": "N. Podhorszki"
                },
                {
                    "authorId": "1712372",
                    "name": "V. Ratnakar"
                },
                {
                    "authorId": "143987788",
                    "name": "E. Santos"
                },
                {
                    "authorId": "1786183",
                    "name": "C. Scheidegger"
                },
                {
                    "authorId": "3170113",
                    "name": "K. Schuchardt"
                },
                {
                    "authorId": "1745942",
                    "name": "M. Seltzer"
                },
                {
                    "authorId": "1761220",
                    "name": "Yogesh L. Simmhan"
                },
                {
                    "authorId": "143803711",
                    "name": "Cl\u00e1udio T. Silva"
                },
                {
                    "authorId": "2987171",
                    "name": "P. Slaughter"
                },
                {
                    "authorId": "34889688",
                    "name": "E. Stephan"
                },
                {
                    "authorId": "144560289",
                    "name": "R. Stevens"
                },
                {
                    "authorId": "145959720",
                    "name": "D. Turi"
                },
                {
                    "authorId": "1688854",
                    "name": "H. Vo"
                },
                {
                    "authorId": "144331405",
                    "name": "M. Wilde"
                },
                {
                    "authorId": "48019160",
                    "name": "Jun Zhao"
                }
            ]
        },
        {
            "paperId": "378a17412ed176ec4e9f36b819a3af65d298ebb8",
            "title": "Scientific Data Management: An Orphan in the Database Community?",
            "abstract": "Increasingly, scientific discovery relies on querying vast amount of information for correlations and comparisons. Scientists in biology, astronomy, medicine, etc. are assembling databases that are commonly hundreds of terabytes. Petascale databases will be become the norm in the next ten years for disciplines as disparate as astronomy, biology, environmental engineering, geophysics, hydrology, oceanography, and medicine (just to name a few!). At the same time, interdisciplinary research between Computer Science and other science and engineering disciplines lies at the forefront of the National research agenda, as evidenced by the National Academies Keck Futures Initiative and by the creation of the National Science Foundation's Office of Cyberinfrastructure. Despite these factors, the database community has been slow to extend its research mission to include scientific database applications. Specifically, there is little support for interdisciplinary work in program committees, journals, and in hiring. Scientific discovery relies critically on database technologies, such as data mining, indexing and data organization, query processing, schema representation and ontology, and stream processing. Yet, scientists often find themselves building their own solutions without the involvement of DB researchers. The panel will address both the positive and negative aspects for researchers who choose to work in scientific database applications, which include: the relative ease of acquiring federal grants for interdisciplinary scientific database work; the difficulty of getting publications about scientific database applications into top-tier conferences; the barriers to inter-disciplinary research with a focus on the extra demands it places on graduate student training and on the reduction in volume of research output; and the limited availability of tenure-track faculty positions for scientific-database researchers in Computer Science departments. The panel assembles leading experts in the application of databases to scientific computing problems. They will discuss the trends and opportunities in scientific database applications and will debate what actions the database community should take in response. research interests include data organization and indexing, distributed query processing, storage security, and data protection. Randal received the NSF CAREER award in 2003 and the DoE Early Career Principal Investigator award in 2002. He is the holder of 12 US patents. Professor of Computer and Information Science. She is an ACM Fellow, a Fulbright scholar, founding co-Director of the Center for Bioinformatics at UPenn (PCBI), and recently stepped down as Deputy Dean of the School of Engineering and Applied Science (SEAS). Dr. Davidson's research interests include database systems, database modeling, distributed systems, and bioinformatics. Within bioinformatics she \u2026",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145542377",
                    "name": "R. Burns"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1684197",
                    "name": "Y. Ioannidis"
                },
                {
                    "authorId": "1684336",
                    "name": "M. Livny"
                },
                {
                    "authorId": "49111633",
                    "name": "J. Patel"
                }
            ]
        },
        {
            "paperId": "76cf9dfecfbdcff5cb20c7bc9b730b18c18d7f66",
            "title": "Provenance and scientific workflows: challenges and opportunities",
            "abstract": "Provenance in the context of workflows, both for the data they derive and for their specification, is an essential component to allow for result reproducibility, sharing, and knowledge re-use in the scientific community. Several workshops have been held on the topic, and it has been the focus of many research projects and prototype systems. This tutorial provides an overview of research issues in provenance for scientific workflows, with a focus on recent literature and technology in this area. It is aimed at a general database research audience and at people who work with scientific data and workflows. We will (1) provide a general overview of scientific workflows, (2) describe research on provenance for scientific workflows and show in detail how provenance is supported in existing systems; (3) discuss emerging applications that are enabled by provenance; and (4) outline open problems and new directions for database-related research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                }
            ]
        },
        {
            "paperId": "d76161ad7565a83258493b0e6a16af5cd494b54e",
            "title": "Querying and Managing Provenance through User Views in Scientific Workflows",
            "abstract": "Workflow systems have become increasingly popular for managing experiments where many bioinformatics tasks are chained together. Due to the large amount of data generated by these experiments and the need for reproducible results, provenance has become of paramount importance. Workflow systems are therefore starting to provide support for querying provenance. However, the amount of provenance information may be overwhelming, so there is a need for abstraction mechanisms to help users focus on the most relevant information. The technique we pursue is that of \"user views\". Since bioinformatics tasks may themselves be complex sub-workflows, a user view determines what level of sub-workflow the user can see, and thus what data and tasks are visible in provenance queries. In this paper, we formalize the notion of user views, demonstrate how they can be used in provenance queries, and give an algorithm for generating a user view based on which tasks are relevant for the user. We then describe our prototype and give performance results. Although presented in the context of scientific workflows, the technique applies to other data-oriented workflows.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2476931",
                    "name": "Olivier Biton"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1785445",
                    "name": "Carmem S. Hara"
                }
            ]
        },
        {
            "paperId": "dc79f3de5538cedbbe4a2d3bbf61b2c8030495e2",
            "title": "Special Issue: The First Provenance Challenge",
            "abstract": "The first Provenance Challenge was set up in order to provide a forum for the community to understand the capabilities of different provenance systems and the expressiveness of their provenance representations. To this end, a functional magnetic resonance imaging workflow was defined, which participants had to either simulate or run in order to produce some provenance representation, from which a set of identified queries had to be implemented and executed. Sixteen teams responded to the challenge, and submitted their inputs. In this paper, we present the challenge workflow and queries, and summarize the participants' contributions. Copyright \u00a9 2007 John Wiley & Sons, Ltd.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1773775",
                    "name": "L. Moreau"
                },
                {
                    "authorId": "1716911",
                    "name": "Bertram Lud\u00e4scher"
                },
                {
                    "authorId": "1729053",
                    "name": "I. Altintas"
                },
                {
                    "authorId": "1692499",
                    "name": "R. Barga"
                },
                {
                    "authorId": "1900211",
                    "name": "S. Bowers"
                },
                {
                    "authorId": "8354430",
                    "name": "Steven P. Callahan"
                },
                {
                    "authorId": "144465343",
                    "name": "George Chin"
                },
                {
                    "authorId": "2720709",
                    "name": "Ben Clifford"
                },
                {
                    "authorId": "2321516",
                    "name": "Shirley Cohen"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1687896",
                    "name": "E. Deelman"
                },
                {
                    "authorId": "1920295",
                    "name": "L. Digiampietri"
                },
                {
                    "authorId": "1698701",
                    "name": "Ian T Foster"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "144899863",
                    "name": "J. Frew"
                },
                {
                    "authorId": "2518189",
                    "name": "J. Futrelle"
                },
                {
                    "authorId": "20938422",
                    "name": "Tara D. Gibson"
                },
                {
                    "authorId": "145526918",
                    "name": "Y. Gil"
                },
                {
                    "authorId": "46555127",
                    "name": "C. Goble"
                },
                {
                    "authorId": "1713898",
                    "name": "J. Golbeck"
                },
                {
                    "authorId": "143772714",
                    "name": "Paul Groth"
                },
                {
                    "authorId": "1918069",
                    "name": "David A. Holland"
                },
                {
                    "authorId": "144339919",
                    "name": "Sheng Jiang"
                },
                {
                    "authorId": "39045812",
                    "name": "J. Kim"
                },
                {
                    "authorId": "1766755",
                    "name": "D. Koop"
                },
                {
                    "authorId": "2300926",
                    "name": "A. K\u0159enek"
                },
                {
                    "authorId": "2737817",
                    "name": "T. McPhillips"
                },
                {
                    "authorId": "39173862",
                    "name": "Gaurang Mehta"
                },
                {
                    "authorId": "145116379",
                    "name": "S. Miles"
                },
                {
                    "authorId": "29923142",
                    "name": "D. Metzger"
                },
                {
                    "authorId": "1773720",
                    "name": "S. Munroe"
                },
                {
                    "authorId": "1767834",
                    "name": "J. Myers"
                },
                {
                    "authorId": "1809305",
                    "name": "Beth Plale"
                },
                {
                    "authorId": "1734819",
                    "name": "N. Podhorszki"
                },
                {
                    "authorId": "1712372",
                    "name": "V. Ratnakar"
                },
                {
                    "authorId": "143987788",
                    "name": "E. Santos"
                },
                {
                    "authorId": "1786183",
                    "name": "C. Scheidegger"
                },
                {
                    "authorId": "3170113",
                    "name": "K. Schuchardt"
                },
                {
                    "authorId": "1745942",
                    "name": "M. Seltzer"
                },
                {
                    "authorId": "1761220",
                    "name": "Yogesh L. Simmhan"
                },
                {
                    "authorId": "143803711",
                    "name": "Cl\u00e1udio T. Silva"
                },
                {
                    "authorId": "2987171",
                    "name": "P. Slaughter"
                },
                {
                    "authorId": "34889688",
                    "name": "E. Stephan"
                },
                {
                    "authorId": "144560289",
                    "name": "R. Stevens"
                },
                {
                    "authorId": "145959720",
                    "name": "D. Turi"
                },
                {
                    "authorId": "1688854",
                    "name": "H. Vo"
                },
                {
                    "authorId": "144331405",
                    "name": "M. Wilde"
                },
                {
                    "authorId": "48019160",
                    "name": "Jun Zhao"
                },
                {
                    "authorId": "2151269913",
                    "name": "Yong Zhao"
                }
            ]
        },
        {
            "paperId": "47ea1f35e4ba08444628a3393bc58aafda9666df",
            "title": "Zoom*UserViews: Querying Relevant Provenance in Workflow Systems",
            "abstract": "In this demonstration, we present the ZOOM*UserView system, and focus on the module which generates a \"user view\" based on what tasks the user perceives to be relevant in the workflow specification. We will show how user views can be used to reduce the amount of information returned by provenance queries, while focusing on information the user finds relevant. User views are based on the notion of composite tasks, and induce a higher-level specification of a workflow.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2476931",
                    "name": "Olivier Biton"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "68eefabfc05692ab38c0f8d5a78bc9b3101a4d4f",
            "title": "BioGuideSRS: querying multiple sources with a user-centric perspective",
            "abstract": "UNLABELLED\nBiologists are frequently faced with the problem of integrating information from multiple heterogeneous sources with their own experimental data. Given the large number of public sources, it is difficult to choose which sources to integrate without assistance. When doing this manually, biologists differ in their preferences concerning the sources to be queried as well as the strategies, i.e. the querying process they follow for navigating through the sources. In response to these findings, we have developed BioGuide to assist scientists search for relevant data within external sources while taking their preferences and strategies into account. In this article, we present BioGuideSRS, a user-friendly system which automatically retrieves instances of data by using BioGuide on top of the sequence retrieval system (SRS). BioGuideSRS is an Applet that can be run from its web page on any system with Java 5.0.\n\n\nAVAILABILITY\nhttp://www.bioguide-project.net.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "2476931",
                    "name": "Olivier Biton"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1792777",
                    "name": "C. Froidevaux"
                }
            ]
        },
        {
            "paperId": "9e820e230fe52ac4b948d9d50f246a47a811d507",
            "title": "Provenance in Scientific Workflow Systems",
            "abstract": "The volume of information in natural languages in electronic format is increasing exponentially. The demographics of users of information management systems are becoming increasingly multilingual. Together these trends create a requirement for information management systems to support processing of information in multiple natural languages seamlessly. Database systems, the backbones of information management, should support this requirement effectively and efficiently. Earlier research in this area had proposed multilingual operators [7, 8] for relational database systems, and discussed their implementation using existing database features. In this paper, we specifically focus on the SemEQUAL operator [8], implementing a multilingual semantic matching predicate using WordNet [12]. We explore the implementation of SemEQUAL using OrdPath [10], a positional representation for nodes of a hierarchy that is used successfully for supporting XML documents in relational systems. We propose the use of OrdPath to represent position within the Wordnet hierarchy, leveraging its ability to compute transitive closures efficiently. We show theoretically that an implementation using OrdPath will outperform those implementations proposed previously. Our initial experimental results confirm this analysis, and show that the OrdPath implementation performs significantly better. Further, since our technique is not specifically rooted to linguistic hierarchies, the same approach may benefit other applications that utilize alternative hierarchical ontologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "143658928",
                    "name": "Anat Eyal"
                },
                {
                    "authorId": "1716911",
                    "name": "Bertram Lud\u00e4scher"
                },
                {
                    "authorId": "2737817",
                    "name": "T. McPhillips"
                },
                {
                    "authorId": "1900211",
                    "name": "S. Bowers"
                },
                {
                    "authorId": "2744720",
                    "name": "M. Anand"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                }
            ]
        },
        {
            "paperId": "c62a7736d843d92263ffdaf333376f0ab6160613",
            "title": "Efficient scientific data management over trees",
            "abstract": "Fueled by novel technologies capable of producing massive amounts of data, scientists have been faced with an explosion of information that must be rapidly analyzed and integrated with other data to form hypotheses and create knowledge. Success in science now hinges critically on the availability of computational and data management tools to meet these challenges. \nMichael Stonebraker recently argued that the traditional database concept of \u201cone size fits all\u201d which provides a unique strategy to manage data in all different applications, is no longer applicable in the database market. Nowhere is this truer than with scientific data. Scientific data differs significantly from business data, for which current database technology has been developed. \nMy research is focused on tree-structured scientific data management, one type of scientific data that models an inherently hierarchical process or object. Due to its hierarchical structure, XML has become a common scientific data format (http://xml.gsfc.nasa.gov). However, XML's standard query languages, XPath and XQuery, are not well suited for many scientific applications, in particular, computational linguistics and phylogenetic tree applications. I have spent a significant portion of my research efforts to efficiently support these two types of scientific applications. Specifically, I have studied and summarized commonly used operations (queries) on the data, analyzed why XML techniques cannot be easily applied, and designed and implemented data management systems for these two types of applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "0fa6895841e3258282b7ff402bd26da3605fba42",
            "title": "Crimson: a data management system to support evaluating phylogenetic tree reconstruction algorithms",
            "abstract": "Evolutionary and systems biology increasingly rely on the construction of large phylogenetic trees which represent the relationships between species of interest. As the number and size of such trees increases, so does the need for efficient data storage and query capabilities. Although much attention has been focused on XML as a tree data model, phylogenetic trees differ from document-oriented applications in their size and depth, and their need for structure-based queries rather than path-based queries.This paper focuses on Crimson, a tree storage system for phylogenetic trees used to evaluate phylogenetic tree reconstruction algorithms within the context of the NSF CIPRes project. A goal of the modeling component of the CIPRes project is to construct a huge simulation tree representing a \"gold standard\" of evolutionary history against which phylogenetic tree reconstruction algorithms can be tested.In this demonstration, we highlight our storage and indexing strategies and show how Crimson is used for benchmarking phylogenetic tree reconstruction algorithms. We also show how our design can be used to support more general queries over phylogenetic trees.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                },
                {
                    "authorId": "145775896",
                    "name": "S. Fisher"
                },
                {
                    "authorId": "2321516",
                    "name": "Shirley Cohen"
                },
                {
                    "authorId": "2119112175",
                    "name": "Sheng Guo"
                },
                {
                    "authorId": "82101484",
                    "name": "Junhyong Kim"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "26e76477b0959136a07ae6e9283fe57063c3f942",
            "title": "Designing and Evaluating an XPath Dialect for Linguistic Queries",
            "abstract": "Linguistic research and natural language processing employ large repositories of ordered trees. XML, a standard ordered tree model, and XPath, its associated language, are natural choices for linguistic data and queries. However, several important expressive features required for linguistic queries are missing or hard to express in XPath. In this paper, we motivate and illustrate these features with a variety of linguistic queries. Then we propose extensions to XPath to support linguistic queries, and design an efficient query engine based on a novel labeling scheme. Experiments demonstrate that our language is not only sufficiently expressive for linguistic trees but also efficient for practical usage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21308992",
                    "name": "Steven Bird"
                },
                {
                    "authorId": "2118047969",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2809587",
                    "name": "Haejoong Lee"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "5be98ffeb15fd2c455fe0e0366b0d2a9d361df14",
            "title": "PATAX\u00d3: A framework to allow updates through XML views",
            "abstract": "XML has become an important medium for data exchange, and is frequently used as an interface to (i.e., a view of) a relational database. Although a lot of work has been done on querying relational databases through XML views, the problem of updating relational databases through XML views has not received much attention. In this work, we map XML views expressed using a subset of XQuery to a corresponding set of relational views. Thus, we transform the problem of updating relational databases through XML views into a classical problem of updating relational databases through relational views. We then show how updates on the XML view are mapped to updates on the corresponding relational views. Existing work on updating relational views can then be leveraged to determine whether or not the relational views are updatable with respect to the relational updates, and if so, to translate the updates to the underlying relational database.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1740182",
                    "name": "V. Braganholo"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                }
            ]
        },
        {
            "paperId": "7554e5042a23368dae8179e267bed2d710f53f01",
            "title": "Path-based Systems to Guide Scientists in the Maze of Biological Data Sources",
            "abstract": "Fueled by novel technologies capable of producing massive amounts of data for a single experiment, scientists are faced with an explosion of information which must be rapidly analyzed and combined with other data to form hypotheses and create knowledge. Today, numerous biological questions can be answered without entering a wet lab. Scientific protocols designed to answer these questions can be run entirely on a computer. Biological resources are often complementary, focused on different objects and reflecting various experts' points of view. Exploiting the richness and diversity of these resources is crucial for scientists. However, with the increase of resources, scientists have to face the problem of selecting sources and tools when interpreting their data. In this paper, we analyze the way in which biologists express and implement scientific protocols, and we identify the requirements for a system which can guide scientists in constructing protocols to answer new biological questions. We present two such systems, BioNavigation and BioGuide dedicated to help scientists select resources by following suitable paths within the growing network of interconnected biological resources.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1783625",
                    "name": "Sarah Cohen Boulakia"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1792777",
                    "name": "C. Froidevaux"
                },
                {
                    "authorId": "1776051",
                    "name": "Z. Lacroix"
                },
                {
                    "authorId": "143858195",
                    "name": "Maria-Esther Vidal"
                }
            ]
        },
        {
            "paperId": "c4dad137b66edd5efe3a205e48f988956daca7e7",
            "title": "An Efficient XPath Query Processor for XML Streams",
            "abstract": "Streaming XPath evaluation algorithms must record a potentially exponential number of pattern matches when both predicates and descendant axes are present in queries, and the XML data is recursive. In this paper, we use a compact data structure to encode these pattern matches rather than storing them explicitly. We then propose a polynomial time streaming algorithm to evaluate XPath queries by probing the data structure in a lazy fashion. Extensive experiments show that our approach not only has a good theoretical complexity bound but is also efficient in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118051328",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "3771e38cf4178df5ab97852a72631bf56aef02f5",
            "title": "Efficiently Supporting Structure Queries on Phylogenetic Trees",
            "abstract": "With phylogenetics becoming increasingly important in biomedical research, the number of phylogenetic studies is increasing rapidly and huge mount of phylogenetic data has been generated and stored in databases. How to efficiently extract information from the data has become an important research problem. In this paper, we focus on a class of important queries on phylogenetic trees: structure queries which include least common ancestor, minimal spanning clade, tree pattern match and tree projection. After analyzing the characteristics of the phylogenetic tree as well as structure queries, we propose a storage system based on labeling using RDBMS and design algorithms for query evaluation. We implement these algorithms and compare them with existing techniques. Performance studies prove the efficiency of our strategy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "82101484",
                    "name": "Junhyong Kim"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "5fb86e905b7817cae1df1aebae62501198899d35",
            "title": "Efficient xpath query processing on stored and streaming xml data",
            "abstract": "With XML emerging as the de facto standard for data representation format, efficient query evaluation on XML data is a very important research challenge. Different applications require queries to be evaluated using different processing models. In some applications, XML data needs to be stored locally for query processing, for example, scientific data. In other applications, XML data arrives continuously as a stream and requires on-line processing, such as the streams for stock and traffic information. \nIn this thesis we discuss how to efficiently evaluate XPath queries on locally stored XML data and XML streams. We also exploit an encoding format to speed up query processing. \nFor locally stored XML data, a Bi-LAbeling based System (BLAS) is proposed to efficiently process XPath queries by leveraging well-developed relational technology. Compared to existing work on XPath processing using relational databases, BLAS translates an XPath query to a more efficient SQL query that contains fewer selections and joins and requires fewer disk accesses to execute. \nFor streaming XML data, an XPath query engine (TwigM) is presented. We observe that XPath evaluation algorithms on XML streams must record a potentially exponential number of pattern matches. We propose a compact data structure to encode these pattern matches rather than storing them explicitly. We then design a polynomial time algorithm to evaluate queries by probing the data structure in a lazy fashion. \nAn Encoded XML Processing system (EXPedite) is then proposed to further speed up the performance. It consists of a general and effective encoding scheme and efficient parsing and query processing algorithms on encoded XML streams. By leveraging the encoding, processing queries over the encoded data is much more efficient than processing them over the original XML stream. \nWe believe that the \u201cone size fits all\u201d concept in the traditional database model is no longer suitable for various application needs today. This thesis provides a comprehensive solution for XPath query processing and gives guidelines as how to choose the right techniques according to application requirements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118051328",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "8b8d59da21c475ab287f28a904dc639044134e33",
            "title": "Active XML and Data Activation",
            "abstract": "The field of distributed data management [13] has centered for many years around the relational model. More recently, the Web has made the world wide and intranet publication of data much simpler, by relying on HTML, Web browsers, plain-text search engines and query forms. However, the management of distributed information remained cumbersome. The situation is dramatically improving today with the introduction of XML [18] and Web services [21]. Together, these two standards provide an infrastructure for distributed computing at large, independent of any platform, system or programming language, i.e., the appropriate framework for distributed management of information. We discuss here Active XML (AXML, for short), a declarative framework that harnesses these emerging standards for the integration and management of distributed data. An AXML document is an XML document where some of the data is given explicitly, while other portions are given only intensionally by means of embedded calls to Web services. By calling the services, one can obtain up-to-date information. In particular, AXML provides control over the activation of service calls both from the client side (pull) or from the server side (push). It should be noted that the idea of mixing data and code is not new, e.g., stored procedures in relational systems [15], method calls in object-oriented databases [10], and queries in scripting languages such as PHP. The novelty is that since both XML and Web services are standards, AXML documents can be universally understood, and therefore can be universally exchanged. In the present paper, we focus on the idea of \u201cactivating\u201d portions of static data, e.g., transforming an XML document into an Active XML document. As we will see, a wide range of XML sub-documents are candidates for being activated. Furthermore, one may want to activate data for a number of different motivations. In some cases, it may be to introduce new functionalities, e.g., to provide monitoring over the data; in other cases, it may be for performance reasons, e.g., to lower the cost of refresh. Activation may be performed either manually (by the application designer) or automatically (e.g., by a Web server).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "d78ad1acd403dedbfc6ebf62f1aa2d2474a6f1a5",
            "title": "Extending XPath to Support Linguistic Queries",
            "abstract": "Linguistic research and language technology development employ large repositories of ordered trees. XML, a standard ordered tree model, and XPath, its associated language, are natural choices for linguistic data storage and queries. However, several important expressive features required for linguistic queries are missing in XPath. In this paper, we motivate and illustrate these features with a variety of linguistic queries. Then we define extensions to XPath which support linguistic tree queries, and describe an efficient query engine based on a novel labeling scheme. Experiments demonstrate that our language is not only sufficiently expressive for linguistic trees but also efficient for practical usage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21308992",
                    "name": "Steven Bird"
                },
                {
                    "authorId": "2118051328",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2809587",
                    "name": "Haejoong Lee"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "f73046a35ff51c38be98590d1f74f8b1f82c1ab5",
            "title": "ViteX: a streaming XPath processing system",
            "abstract": "We present ViteX, an XPath processing system on XML streams with polynomial time complexity. ViteX uses a polynomial-space data structure to encode an exponential number of pattern matches (in the query size) which are required to process queries correctly during a single sequential scan of XML. Then ViteX computes query solutions by probing the data structure in a lazy fashion without enumerating pattern matches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118051328",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "232c2f503a4b71e09dc8fe99fe1bdd6d51d52df0",
            "title": "BLAS: an efficient XPath processing system",
            "abstract": "We present BLAS, a Bi-LAbeling based System, for efficiently processing complex XPath queries over XML data. BLAS uses P-labeling to process queries involving consecutive child axes, and D-labeling to process queries involving descendant axes traversal. The XML data is stored in labeled form, and indexed to optimize descendent axis traversals. Three algorithms are presented for translating complex XPath queries to SQL expressions, and two alternate query engines are provided. Experimental results demonstrate that the BLAS system has a substantial performance improvement compared to traditional XPath processing using D-labeling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118051328",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "8a0c384b1dc56b2617396aa5573e7787ddaf9ada",
            "title": "Reasoning about functional and key dependencies in hierarchically structured data",
            "abstract": "This dissertation investigates how constraints can be used to check the consistency of data being exchanged between different sources. Data exchange involves transformations of data, and therefore the \u201ctransformed\u201d data can be seen as a view of its source. Thus, the problem we investigate is how constraints are propagated to views, when the data involved is not restricted to relational tables, but may be hierarchically structured in several levels of nesting. The ability to determine constraint propagation relies on the ability to determine constraint implication. This is because the validity of a constraint on the view may not result directly from constraints defined on the source data, but from their consequences. Therefore, the dissertation starts by investigating two forms of constraints: nested functional dependencies and keys for XML, and their implication problems. More specifically, we present a definition of functional dependencies for a nested relational model, and a sound and complete set of inference rules for determining logical implication for the case when no empty sets are present. Motivated by the popularity of XML as a data exchange format, we present a definition of keys for XML that are independent of any type specification. We study two notions of keys: strong keys, and weak keys, and for each of them we derive a sound and complete set of inference rules, as well as algorithms for determining their implication. Capitalizing on the results of XML key implication, we investigate the problem of propagating XML keys to relational views. That is, the problem of determining what are the functional dependencies that are guaranteed to hold in a relational representation of XML data, given that a set of XML keys hold on the XML document. We provide two algorithms: one is to check whether a functional dependency is propagated from XML keys, and the other is to compute a minimum cover for all functional dependencies on a universal relation given certain XML keys. The ability to compute XML key propagation is a first step toward establishing a connection between XML data and its relational representation at the semantic level.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785445",
                    "name": "Carmem S. Hara"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144502903",
                    "name": "W. Fan"
                }
            ]
        },
        {
            "paperId": "97199415b732b2dab552533fa1f0d70b7675655e",
            "title": "EXPedite: a system for encoded XML processing",
            "abstract": "As XML becomes an increasingly popular format for information exchange, the efficient processing of broadcast XML data on a constrained device (for example, a cell phone or a PDA) becomes a critical task. In this paper we present the EXPedite system: a new model of data processing in an information exchange environment, which \"migrates\" the power of the data-sending server to receivers for efficient processing. It consists of a simple and general encoding scheme for servers, and streaming query processing algorithms on encoded XML stream for data receivers with constrained computing abilities. Experiments show the impressive performance of EXPedite.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118051328",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "1678202",
                    "name": "G. Mihaila"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "145535161",
                    "name": "S. Padmanabhan"
                }
            ]
        },
        {
            "paperId": "b9791fe3f5e52976605b7f22e6924b8d06f1be0d",
            "title": "Propagating XML View Updates to a Relational Database",
            "abstract": "This paper addresses the question of updating relational databases through XML views. Using a notion of query trees to capture the notions of selection, projection, nesting, grouping, and heterogeneous sets found throughout most XML query languages, we show how XML views expressed using query trees can be mapped to a set of corresponding relational views. We then show how updates on the XML view are mapped to updates on the corresponding relational views. Existing work on updating relational views can then be leveraged to determine whether or not the relational views are updatable with respect to the relational updates, and if so, to translate the updates to the underlying relational database.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1740182",
                    "name": "V. Braganholo"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                }
            ]
        },
        {
            "paperId": "238692b9dc2edcbafa10e9f490c79cd9aa169fec",
            "title": "Propagating XML constraints to relations",
            "abstract": "We present a technique for refining the design of relational storage for XML data based on XML key propagation. Three algorithms are presented: one checks whether a given functional dependency is propagated from XML keys via a predefined view; the others compute a minimum cover for all functional dependencies on a universal relation given XML keys. Experimental results show that these algorithms are efficient in practice. We also investigate the complexity of propagating other XML constraints to relations, and the effect of increasing the power of the transformation language. Computing XML key propagation is a first step toward establishing a connection between XML data and its relational representation at the semantic level.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144502903",
                    "name": "W. Fan"
                },
                {
                    "authorId": "1785445",
                    "name": "Carmem S. Hara"
                }
            ]
        },
        {
            "paperId": "641885106a6f4d0506787e364ed518c79ece452c",
            "title": "Reasoning About the Updatability of XML Views Over Relational Databases",
            "abstract": "XML has become an important medium for data exchange, and is also used as an interface to \u2013 i.e. a view of \u2013 a relational database. While previous work has considered XML views for the purpose of querying relational databases (e.g. Silkroute), in this paper we consider the problem of updating a relational database through an XML view. Using the nested relational algebra as the formalism for an XML view of a relational database, we study the problem of when such views are updatable. Our results rely on the observation that in many XML views of relational databases, the nest operator occurs last and the unnest operator does not occur at all. Since in this case the nest operator is invertible, we can consider this important class of XML views as if they were flat relational views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1740182",
                    "name": "V. Braganholo"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                }
            ]
        },
        {
            "paperId": "6e94288cc99fe229a364de9367f82d6779c7d063",
            "title": "UXQuery: Building Updatable XML Views over Relational Databases",
            "abstract": "XML has become an important medium for data exchange, and is frequently used as an interface to \u2013 i.e. a view of \u2013 a relational database. Although much attention has been paid to the problem of querying relational databases through XML views, the problem of updating relational databases through XML views has not been addressed. In this paper we investigate how a subset of XQuery can be used to build updatable XML views, so that an update to the view can be unambiguously translated to a set of updates on the underlying relational database, assuming that certain key and foreign key constraints hold. In particular, we show how views defined in this subset of XQuery can be mapped to a set of relational views, thus transforming the problem of updating relational databases through XML views into a classical problem of updating relational databases through relational views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1740182",
                    "name": "V. Braganholo"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                }
            ]
        },
        {
            "paperId": "852cae3c1388974e1ea8ac8115dae5fdab56cbf8",
            "title": "Using XQuery to Build Updatable XML Views Over Relational Databases",
            "abstract": "XML has become an important medium for data exchange, and is frequently used as an interface to i.e. a view of a relational database. Although much attention has been paid to the problem of querying relational databases through XML views, the problem of updating relational databases through XML views has not been addressed. In this paper we investigate how a subset of XQuery can be used to build updatable XML views, so that an update to the view can be unambiguously translated to a set of updates on the underlying relational database, assuming that certain key and foreign key constraints hold. In particular, we show how views defined in this subset of XQuery can be mapped to a set of relational views, thus transforming the problem of updating relational databases through XML views into a classical problem of updating relational databases through relational views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1740182",
                    "name": "V. Braganholo"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                }
            ]
        },
        {
            "paperId": "c4f0cdaa18c33219e0d5a52901b463312119287a",
            "title": "On the updatability of XML views over relational databases",
            "abstract": "XML has become an important medium for data exchange, and is also used as an interface to \u2013 i.e. a view of \u2013 a relational database. While previous work has considered XML views for the purpose of querying relational databases (e.g. Silkroute), in this paper we consider the problem of updating a relational database through an XML view. Using the nested relational algebra as the formalism for an XML view of a relational database, we study the problem of when such views are updatable. Our results rely on the observation that in many XML views of relational databases, the nest operator occurs last and the unnest operator does not occur at all. Since in this case the nest operator is invertible, we can consider this important class of XML views as if they were flat relational views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1740182",
                    "name": "V. Braganholo"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                }
            ]
        },
        {
            "paperId": "fecbb76344a971c8512f46635edd512fb8668693",
            "title": "Sharing Biomedical Data with Impunity and Ease",
            "abstract": "BIOLOGICAL DATA is increasingly being shared between databases using a variety of different data formats. While some of these formats have been developed almost exclusively for communicating between viewers and databases (e.g., AGAVE and GAME) and others for sharing annotation as well as viewing (e.g., DAS), many formats are also being used to exchange data between databases (e.g., EMBL, ASN.1, and specialized XML DTDs). The appeal of a data exchange format is that it is a way of serving data in a uniform, flexible, and easily parseable form. Data exchange formats are also largely self-describing and hence easy to understand. However, agreeing to use a specific exchange format does not solve the data exchange problem by itself. Exporters must map their data into the exchange format, and importers of data must again map from the exchange format into their local format (or more generally, model). Thus data exchange is inextricably tied up with writing mappings (or transformations) between data formats. Several problems are associated with writing mappings between data formats. First, it is an inherently difficult problem. The writer of the mapping must understand both how the data is being represented in the exchange format and how they are representing it in their own model. Second, semantic information is frequently not captured in a data exchange format. For example, information about keys, foreign keys and constraints is often omitted. Clearly, constructing a mapping must be guided by an understanding of the semantics of the data since otherwise the mapping may cause run-time constraint violations. As an example of these problems, we having recently been conducting an experiment involving exchanging microarray gene expression data using a developing standard called MAGE-OM/ML (Spellman et al., 2002). The semantics of MAGE is specified using UML modeling tools (MAGE-OM), however the exchange is effected using an XML representation of the standard. Prior to this standardization effort, a relational database called RAD (On-line data) had been developed at the Penn Center for Bioinformatics to store gene expression data as well as its associated sample annotation data. When the MAGE-ML standard is finalized, data will be imported from collaborators and exported from RAD using this format. However, each of these data representations\u2014RAD and MAGE\u2014has been developed independently. In our experiment, the following problems emerged: 1. The data exported by RAD into MAGE-ML through some transformation may fail to validate against the constraints of MAGE-OM. Example: Gene expression annotation in MAGE-ML requires information about the process of sample preparation. The annotation interface in RADv2 required only information about the end result of the sample preparation, with an (optional) free-text description of the process. Since in MAGE-ML the biomaterial can only exist if the biosource is present (and so on down the process), the sample information in RADv2 was inconsistent with MAGE-ML. RAD was therefore modified so that RADv3 is consistent with MAGEML, and a new annotation interface is under development to force the process to be captured. 2. The data imported by RAD through some transformation from MAGE-ML may violate integrity constraints in RAD. If the MAGE-ML data is consistent with respect to the constraints of MAGE-OM, then there must be some inconsistency between MAGE-OM and the constraints expressed in RAD.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "e4821dd83c675d518792825bd42f741f1adf7a99",
            "title": "Constraints preserving schema mapping from XML to relations",
            "abstract": "As XML becomes a standard for data representation on the internet, there is a growing interest in storing XML using relational database technology. To date, none of these techniques have considered the semantics of XML as expressed by keys and foreign keys. In this paper, we present a storage mapping which preserves not only the content and structure of XML data, but also its semantics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118051328",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "fc4805669f98b9dd61a24e9fcd0c25bb132c41ae",
            "title": "Validating Constraints in XML",
            "abstract": "The role of XML in data exchange is evolving from one of merely conveying the structure of data to one that also conveys its semantics. In particular, several proposals for key and foreign key constraints have recently appeared, and aspects of these proposals have been adopted within XMLSchema. Although several validators for XMLSchema appear to check for keys, relatively little attention has been paid to the general problem of how to check constraints in XML. In this paper, we examine the problem of checking keys in XML documents and describe a native validator based on SAX. The algorithm relies on an indexing technique based on the paths found in key definitions, and can be used for checking the correctness of an entire document (bulk checking) as well as for checking updates as they are made to the document (incremental checking). The asymptotic performance of the algorithm is linear in the size of the document or update. We also discuss how XML keys can be checked in relational representations of XMLdocuments, and compare the performance of our native validator against hand-coded relational constraints. Extrapolating from this experience, we propose how a relational schema can be designed to check XMLSchema key constraints using efficient relational PRIMARY KEY or UNIQUE constraints. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-02-03. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/31 Validating Constraints in XML Yi Chen, Susan B. Davidson and Yifeng Zheng ! \" #$ % & ' #$ '( )* #$+% -,.#$ 0/! 12 ,./435+\" #6 yicn@saul.cis.upenn.edu 7 susan@cis.upenn.edu 7 yifeng@saul.cis.upenn.edu",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118051328",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "2458f41d6c4434a3f2d408472616682ecf81bcaa",
            "title": "Indexing Keys in Hierarchical Data",
            "abstract": "Building on a notion of keys for XML, we propose a novel indexing scheme for hierarchical data that is based not only on the structure but also the content of the data. The index can be used to check the validity of data with respect to a set of key specifications, as well as for efficiently evaluating queries and updates on key paths. We develop algorithms for the construction and incremental maintenance of the indexing structure, and study the complexity of these algorithms. Finally, we discuss how our indexing techniques can be used for more general queries involving key paths. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-01-30. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/46 Indexing Keys in Hierarchical Data Yi Chen, Susan B. Davidson and Yifeng Zheng ! #\" $% ! &$% '( !) * + , .-/ 101 + -32,)4 ! yicn@saul.cis.upenn.edu 5 susan@cis.upenn.edu 5 yifeng@seas.upenn.edu",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118051328",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2158584710",
                    "name": "Yifeng Zheng"
                }
            ]
        },
        {
            "paperId": "3a19d46351f9d5f0e04c040f504142489dd8614d",
            "title": "Keys for XML",
            "abstract": "We discuss the definition of keys for XML documents, payingparticular attention to the concept of a relative key, which is commonly used in hierarchically structured documents and scientific databases. \ufffd 2002 Published by Elsevier Science B.V.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144502903",
                    "name": "W. Fan"
                },
                {
                    "authorId": "1785445",
                    "name": "Carmem S. Hara"
                },
                {
                    "authorId": "34582619",
                    "name": "W. Tan"
                }
            ]
        },
        {
            "paperId": "4508ea250c4690a4fc200d59cae648835d49799e",
            "title": "Engineering Robust Distributed Database Software",
            "abstract": "Abstract : The research has focused on a technology for mobile information management.' Underlying this technology is a mathematical foundation enabling the use of formal methods in developing and reasoning about the construction of mobile information management components and their use in database integration and transformation. The salient features of our approach are: use of unmaterialized views; dynamic integration of data consumers and data sources, using mobile query processes; data interface specifications, based on XML schemas; specifications for transformations, based on XML query languages; the development of formal methods, focusing on query and constraint reformulation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                },
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "e3ac9146e7c1f2bb688a7d91934e7a73be893934",
            "title": "Database Transformations for Biological Applications",
            "abstract": "The goal of this project was to develop tools to facilitate data transformations between heterogeneous data sources found throughout biomedical applications. Such transformations are necessary when sharing data between different groups working on related problems as well as when querying data spread over different databases, files and software analysis packages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065407026",
                    "name": "C. Overton"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "0c6efd10c82c9f65a6e35d45efd36e42d22bfedb",
            "title": "Scaling the tower of babel: data integration and warehousing with the WWW",
            "abstract": "The Web can be thought of as one large, disorganized database. Within it there are several related files (web pages), some of which are functions that take input from the user and provide another web page as output. For example, query interfaces to databases such as GenBank and SWISS-PROT can be thought of as such functions. Some of the web pages contain explicit pointers (hot links) to other web pages; other contain implicit pointers (for example, accession numbers) to components of other web pages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "377a962aa1c3bf821a63f5cb7793186f7881c41f",
            "title": "Reasoning about nested functional dependencies",
            "abstract": "Functional dependencies add semantics to a database schema, and are useful for studying various problems, such as database design, query optimization and how dependencies are carried into a view. In the context of a nested relational model, these dependencies can be extended by using path expressions instead of attribute names, resulting in a class of dependencies that we call nested functional dependencies (NFDs). NFDs define a natural class of dependencies in complex data structures; in particular they allow the specification of many useful intraand inter-set dependencies (i.e., dependencies that are local to a set and dependencies that require consistency between sets). Such constraints cannot be captured by existing notions of functional, multi-valued, or join dependencies. This paper presents the definition of NFDs and gives their meaning by translation to logic. It then presents a sound and complete set of eight inference rules for NFDs, and discusses approaches to handling the existence of empty sets in instances. Empty sets add complexity in reasoning since formulas such as VZ E R.P(z) are trivially true when R is empty. This axiomatization represents a first step in reasoning about constraints on data warehouse applications, where both the source and target databases support complex types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785445",
                    "name": "Carmem S. Hara"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "45634caf76fc8b36a55a84c1eba4f0cc3168ff3d",
            "title": "Specifying updates in biomedical databases",
            "abstract": "Many of the publicly available biomedical data sources-such as Genbank and SwissProt-are not stored in traditional databases but in a variety of file formats (e.g. ASN.1 and EMBL). The data is complex, involving deeply nested structures. While query languages for such data have been well-studied, the issue of updating such databases has not. The need for a concise update language is critical since the changes to the data are typically very small when compared to the entire value. Starting with a query language called the Collection Programming Language (CPL), we describe an extension called CPL+ which provides an intuitive framework for updates on complex values. We illustrate the language using examples and present various optimizations that can substantially improve the performance of complex updates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1727465",
                    "name": "Hartmut Liefke"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "46f68ba2de84e7894ea0aa09fe0c1c579336bc87",
            "title": "Specifying Database Transformations in WOL",
            "abstract": "WOL is a Horn-clause language for specifying transformations involving complex types and recursive data-structures. Its declarative syntax makes programs easy to modify in response to schema evolution; the ability to specify partial clauses facilitates transformations when schemas are very large and data is drawn from multiple sources; and the inclusion of constraints enables a number of optimizations when completing and implementing transformations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2907709",
                    "name": "A. Kosky"
                }
            ]
        },
        {
            "paperId": "525daab6cb503b0a8624dfc96a7055bc28db5502",
            "title": "Extending Database Integration Technology",
            "abstract": "Abstract : Formal approaches to the semantics of databases and database languages can have immediate and practical consequences in extending database integration technologies to include a vastly greater range of data sources and data structures. We consider three broad areas; collection types, schema transformation, and partial information; that are central to obtaining interoperability of heterogeneous data sources. In each of these areas we have developed working prototypes that have been put to practical use. This proposal describes work on collection types and schema transformation, and outlines a plan for the development of both principles and implementation of practical languages and tools that will extend database integration technology well beyond its current confines to cope with legacy systems, structured files, data-intensive applications, and other non-standard data sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "80eaa8dfa3c978ec4343dda2ad73d8d3fc6e7904",
            "title": "Inference Rules for Nested Functional Dependencies",
            "abstract": "Functional dependencies add semantics to a database schema, and are useful for studying various problems, such as database design, query optimization and how dependencies are carried into a view. In the context of a nested relational model, these dependencies can be extended by using path expressions instead of attribute names, resulting in a class of dependencies that we call nested functional dependencies (NFDs). NFDs define a natural class of dependencies in complex data structures; in particular they allow the specification of many useful intraand inter-set dependencies (i.e., dependencies that are local to a set and dependencies that require consistency between sets). Such constraints cannot be captured by existing notions of functional, multi-valued, or join dependencies. This paper presents the definition of NFDs and gives their meaning by translation to logic. It then presents a sound and complete set of eight inference rules for NFDs, and discusses approaches to handling the existence of empty sets in instances. Empty sets add complexity in reasoning since formulas such as \u2200x \u2208 R.P (x) are trivially true when R is empty. This axiomatization represents a first step in reasoning about constraints on data warehouse applications, where both the source and target databases support complex types. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-98-19. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/53 Inference Rules for Nested Functional Dependencies Carmem S. Hara and Susan B. Davidson Dept. of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104-6389 Phone (215) 898-3490, Fax (215) 898-0587 Email: chara@saul.cis.upenn.edu, susan@central.cis.upenn.edu",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785445",
                    "name": "Carmem S. Hara"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "224d2ab5c62ff66483ad688a88ff4dd3c2ff29a4",
            "title": "Querying an Object-Oriented Database Using CPL",
            "abstract": "The Collection Programming Language CPL is based on a complex value model of data and has successfully been used for querying transforming and integrating data from a wide variety of structured data sources relational ACeDB and ASN among others However since there is no notion of objects and classes in CPL it cannot adequately model recursive types or inheritance and hence cannot be used to query object oriented databases OODBs By adding a reference type and four operations to CPL dereference method invocation identity test and class type cast it is possible to express a large class of interesting safe queries against OODBs As an example of how the extended CPL can be used to query an OODB we will describe how the extended language has been used as a query interface to Shore databases",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1785445",
                    "name": "Carmem S. Hara"
                },
                {
                    "authorId": "145378077",
                    "name": "Lucian Popa"
                }
            ]
        },
        {
            "paperId": "6e5fd61fe7e565816098f2d0e6f66883ed600b8f",
            "title": "WOL: a language for database transformations and constraints",
            "abstract": "The need to transform data between heterogeneous databases arises from a number of critical tasks in data management. These tasks are complicated by schema evolution in the underlying databases and by the presence of non-standard database constraints. We describe a declarative language called WOL (Well-founded Object Logic) for specifying such transformations, and its implementation in a system called Morphase (an \"enzyme\" for morphing data). WOL is designed to allow transformations between the complex data structures which arise in object-oriented databases as well as in complex relational databases, and to allow for reasoning about the interactions between database transformations and constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2907709",
                    "name": "A. Kosky"
                }
            ]
        },
        {
            "paperId": "5c851ff1718610a8c0215f1d6cc1c7d6e6ba1beb",
            "title": "The design and implementation of massively parallel knowledge representation and reasoning systems: a connectionist approach",
            "abstract": "E cient knowledge representation and reasoning is an important component of intelligent activity, and is a crucial aspect in the design of large-scale intelligent systems. This dissertation explores the design, analysis, and implementation of massively parallel knowledge representation and reasoning systems which can encode very large knowledge bases and respond to a class of queries in real-time, with reasoning episodes expected to span a fraction of a second. The dissertation attempts to design e cient, large-scale knowledge base systems by: (i) exploiting massive parallelism; and (ii) constraining representational and inferential capabilities to achieve tractability, while still retaining su cient expressive power to capture a broad class of reasoning in intelligent systems. To this end, shruti, a connectionist reasoning system which models re exive| i.e., e ortless and spontaneous|reasoning serves as the knowledge representation and reasoning framework. Shruti-based massively parallel systems that can encode very large knowledge-bases and perform a class of reasoning in real-time are developed. Shruti-cm5, the parallel re exive reasoning system on the Connection Machine CM-5, encodes large arti cial knowledge bases with over half a million (randomly generated) rules and facts, and responds to a range of queries requiring derivation depths of up to eight in well under a second. Shruti-cm5 running WordNet, a real-world lexical database, responds to queries in times ranging from a few to a few hundred milliseconds. With a view toward exploiting machine characteristics and knowledge base structure, a quantitative analysis optimizes performance with respect to the probability of nding related knowledge elements on the same processor, thereby leading to an optimal mapping of the knowledge base onto the underlying parallel machine. The analysis makes some interesting predictions|validated by experimental data|about query response time and its relation to computation and communication costs. These results help engineer performance improvements and steer the course of future research. From a practical standpoint, in addition to providing some new insights into mapping structured connectionist networks onto massively parallel machines, this work develops viable technology for supporting large-scale knowledge base systems. iv",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3220932",
                    "name": "D. Mani"
                },
                {
                    "authorId": "94844546",
                    "name": "Dissertation Supervisor"
                },
                {
                    "authorId": "48462607",
                    "name": "L. Shastri"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2057422274",
                    "name": "David Bailey"
                },
                {
                    "authorId": "1398702583",
                    "name": "J. Feldman"
                },
                {
                    "authorId": "2226978789",
                    "name": "Ben Gomes"
                },
                {
                    "authorId": "1746807",
                    "name": "Dan Jurafsky"
                },
                {
                    "authorId": "2273592305",
                    "name": "Srini Narayanan"
                },
                {
                    "authorId": "2227023895",
                    "name": "Jee Aaronson"
                },
                {
                    "authorId": "2059978274",
                    "name": "T. Fontaine"
                },
                {
                    "authorId": "2226982789",
                    "name": "Jim Gee"
                },
                {
                    "authorId": "143826664",
                    "name": "C. Geib"
                },
                {
                    "authorId": "2621890",
                    "name": "Libby Levison"
                },
                {
                    "authorId": "8046671",
                    "name": "Chuck C. Liang"
                }
            ]
        },
        {
            "paperId": "7822984afa0169e3504b376d659c3fa53a2fd633",
            "title": "Graphical communicating shared resources: a language for the specification, refinement and analysis of real-time systems",
            "abstract": "The Communicating Shared Resources (CSR) paradigm is an ongoing project at the University of Pennsylvania to build a framework for the development of real-time systems. This project has been motivated by a demand for a rigorous framework in which various design alternatives for a real-time system can be formally specified and rigorously analyzed and tested before implementation. This is an effort to reduce the potentially high cost associated with incorrect operation of real-time systems which are often embedded in safety-critical applications. The work presented in this thesis is a first step towards incorporating software engineering practices into the CSR paradigm. This is achieved, on one hand, by developing a formal, graphical CSR formalism, the Graphical Communicating Shared Resources (GCSR); the GCSR language adopts the intuitive concepts of nodes and edges in state diagrams, an informal specification language that is popular within the software engineering community. In addition, defining a refinement theory for GCSR allows the development of real-time systems within this formalism in a top-down and modular fashion, also a popular design methodology within the software engineering community. The GCSR language adopts a syntax that allows a modular and hierarchical, thus, scalable description of a real-time system. It supports notions of comunication through events, interrupt, concurrency, and time to describe the functional and temporal requirements of a real-time system. In addition, GCSR allows the explicit representation of resources and priorities to resolve resource contention, in such a way that produces easy to understand and modify specifications. The semantics of GCSR is defined operationally either through a direct translation of a GCSR description to a labeled transition system, or indirectly through a sound translation to the Algebra of Communicating Shared Resources (ACSR) [LBGG94] a timed process algebra that also has an operational semantics. The GCSR-ACSR correspondence makes GCSR benefit from process algebraic analysis techniques such as equivalence checking, state space exploration, testing as well as simulation. In addition, the tight correspondence between GCSR and ACSR makes it possible to use the graphical and textual notations interchangeably and to have a sound theory for graphical transformation operations, e.g., to minimize the number of edges and nodes in a GCSR specification without affecting the behavioral description. To support the top-down and modular development of a real-time specification in GCSR, we have augmented ACSR and thus GCSR with a refinement theory. The refinement theory allows relabeling of events, addition of implementation events, and substitution of a time and resource-consuming action with a process that may use fewer or more resources than the refined action. Consistency between an abstract specification and a refined specification is defined in terms of an ordering relation over traces that is extended to sets of traces according to the Hoare ordering or Egli-Milner ordering. The trace ordering relation relates traces that share timing properties such as equal duration and preservation of timed occurrences of communication events of the abstract specification. To facilitate the practical use of the refinement theory, we have characterized the extended trace ordering relations by a set of transformation rules that syntactically derive a refined process from an abstract one. The transformation rules define basic graphical operations that represent GCSR refinements. This thesis or dissertation is available at ScholarlyCommons: http://repository.upenn.edu/ircs_reports/98 To experiment with the GCSR language and its refinement theory, we have developed a tool set that allows the specification, refinement, and analysis of real-time systems modeled in GCSR. We report our evaluation in the case of the Production Cell case study [LL95]. Comments University of Pennsylvania Institute for Research in Cognitive Science Technical Report No. IRCS-96-18. This thesis or dissertation is available at ScholarlyCommons: http://repository.upenn.edu/ircs_reports/98 University of Pennsylvania 3401 Walnut Street, Suite 400A Philadelphia, PA 19104-6228 October 1996 Site of the NSF Science and Technology Center for Research in Cognitive Science IRCS Report 96--18 Institute for Research in Cognitive Science Graphical Communicating Shared Resources: A Language for the Specification, Refinement, and Anaylsis of Real-Time Systems Ph.D. Dissertation",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1577210966",
                    "name": "AbdallahAbderazek Ben"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "6cf33c45a0be03f4fad126f7b604cd6a2961f190",
            "title": "A Data Transformation System for Biological Data Sources",
            "abstract": "Scientific data of importance to biologists in the Humitn Genome Project resides not only in conventional da.tabases, but in structured files maintained in a number of different formats (e.g. ASN.1 a.nd ACE) as well a.s sequence analysis packages (e.g. BLAST and FASTA). These formats and packages contain a number of data types not found in conventional databases, such as lists and variants, and may be deeply nested. We present in this paper techniques for querying and transforming such data, and illustrate their use in a prototype system developed in conjunction with the Human Genome Center for Chromosome 22. We also describe optimizations performed by the system, a crucial issue for bulk data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "153503092",
                    "name": "Kyle Hart"
                },
                {
                    "authorId": "2248447709",
                    "name": "G. C. Overton"
                },
                {
                    "authorId": "2246843328",
                    "name": "Limsoon Wong"
                }
            ]
        },
        {
            "paperId": "b37cfb16b31861280ca00c4fbd34d4aa023d69aa",
            "title": "Programming Constructs for Unstructured Data",
            "abstract": "We investigate languages for querying and transforming unstructured data by which we mean languages than can be used without knowledge of the structure (schema) of the database. There are two reasons for wanting to do this. First, some data models have emerged in which the schema is either completely absent or only provides weak constraints on the data. Second, it is sometimes convenient, for the purposes of browsing, to query the database without reference to the schema. For example one may want to \\grep\" all character strings in the database, or one might want to nd the information associated with a certain eld name no matter where it occurs in the database. This paper introduces a labelled tree model of data and investigates various programming structures for querying and transforming such data. In particular, it considers various restrictions of structural recursion that give rise to well-deened queries even when the input data contains cycles. It also discusses issues of observable equivalence of such structures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                }
            ]
        },
        {
            "paperId": "31478d46ccffa7c6d5a0d253889b12cadbabd81f",
            "title": "Facilitating transformations in a human genome project database",
            "abstract": "Human Genome Project databases present a confluence of interesting database challenges: rapid schema and data evolution, complex data entry and constraint management, and the need to integrate multiple data sources and software systems which range over a wide variety of models and formats. While these challenges are not necessarily unique to biological databases, their combination, intensity and complexity are unusual and make automated solutions imperative. We illustrate these problems in the context of the Philadelphia Genome Center for Human Chromosome 22, and describe a new approach to a solution for these problems, by means of a deductive language for expressing database transformations and constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2907709",
                    "name": "A. Kosky"
                },
                {
                    "authorId": "1898192",
                    "name": "B. Eckman"
                }
            ]
        },
        {
            "paperId": "8aa0d5dc1a3c4fc4be05b784794c6fcde23a2efe",
            "title": "Deadlock prevention in the RTC programming system for distributed real-time applications",
            "abstract": "The RTC distributed real-time programming system was implemented using AND-OR locking of system resources to meet real-time and concurrency control requirements. Since RTC processes can hold locks while acquiring others, deadlock is possible and therefore a deadlock prevention technique was implemented for AND-OR locking in such systems. The authors briefly discuss the RTC programming system, illustrate the system's use in programming a timed version of the classic dining philosophers example, describe the deadlock prevention technique, and show how it is applied in the RTC dining philosophers example.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2662770",
                    "name": "V. Wolfe"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                }
            ]
        },
        {
            "paperId": "0eeba1592dbd1211991acdb1ca537d4382dc1ae5",
            "title": "A basis for interactive schema merging",
            "abstract": "The authors present a technique for merging the schemas of heterogeneous databases that generalizes to several different data models, and show how it can be used in an interactive program that merges entity-relationship diagrams. Given a collection of schemas to be merged, the user asserts the correspondence between entities and relationships in the various schemas by defining 'isa' relations between them. These assertions are then considered to be elementary schemas, and are combined with the elementary schemas in the merge. Since the method defines the merge to be the join in an information ordering on schemas, it is a commutative and associative operation, which means that the merge is defined independent of the order in which schemas are presented. They briefly describe a prototype interactive schema merging tool that has been built on these principles.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2907709",
                    "name": "A. Kosky"
                },
                {
                    "authorId": "68981749",
                    "name": "M. Vaninwegen"
                }
            ]
        },
        {
            "paperId": "f783e1b90219fa485b8fbbd6ed716bc62da40bfc",
            "title": "Proving Properties of Real-Time Distributed Systems: A Comparison of Three Approaches",
            "abstract": "Three formal methods for specifying properties of real-time systems are reviewed and used in a common example. Two of them offer a graphical representation and the third is an algebraic language. The example is that of an automatic railroad system with sensors to detect the train position and controls for the gate mechanism. Associated with each formalism is a proof methodology which is described and used to prove a safety property about the example. A comparison is made between the three formalisms according to various criteria including the expressiveness, readability, maintainability of the language, support for real-time concepts, method for expressing properties and proof mechanisms. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-92-20. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/363 Proving Properties of Real-Time Distributed Systems: A Comparison of Three Approaches MS-CIS-92-20 GRASP LAB 306 Patrice Brkmond-Grkgoire Susan Davidson Insup Lee University of Pennsylvania School of Engineering and Applied Science Computer and Information Science Department Philadelphia, PA 19104-6389",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405514465",
                    "name": "Patrice Br\u00e9mond-Gr\u00e9goire"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                }
            ]
        },
        {
            "paperId": "15776177aa00ed41a15488a32285c663effc4343",
            "title": "Timed Atomic Commitment",
            "abstract": "Timed atomic commitment is defined, protocols to implement it in a realistic operating environment are devised, and its usefulness is shown through an example. In a large class of hard-real-time control applications, components execute concurrently on distributed nodes and must coordinate, under timing constraints, to perform the control task. As such, they perform a type of atomic commitment. Traditional atomic commitment differs, however, because there are no timing constraints; agreement is eventual. The authors define timed atomic commitment (TAC), which requires the processes to be functionally consistent, but allows the outcome to include an exceptional state, indicating that timing constraints have been violated. Centralized and decentralized protocols to implement TAC are presented. Programming constructs for TAC are introduced, and their use is illustrated in a coordinating robots example. >",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                },
                {
                    "authorId": "2662770",
                    "name": "V. Wolfe"
                }
            ]
        },
        {
            "paperId": "3447d54786980096c0f458986807f2cea38114a4",
            "title": "Supporting real-time concurrency",
            "abstract": "Concurrent real-time applications are complicated since both timing and consistency constraints must be met for correct performance. Furthermore, techniques to enforce these two forms of constraints are often incompatible. For instance, priority-driven preemptive scheduling, which is optimal for meeting timing constraints in some systems, may leave a shared resource's state inconsistent. On the other hand, mutual exclusion techniques that ensure the consistency of shared resources are not well-suited to meeting timing constraints. This dissertation develops concepts and programming language constructs for facilitating the enforcement of both real-time and consistency constraints in applications with concurrency. \nOur programming paradigm combines an object-based paradigm for the specification of shared resources, and a distributed transaction-based paradigm for the specification of application processes. Resources provide abstract views of shared system entities, such as devices and data structures. Each resource has a state and defines a set of actions that can be invoked by processes to examine or change its state. A resource also specifies scheduling constraints on the execution of its actions to ensure the maintenance of its state's consistency. Processes access resources by invoking actions and express precedence, consistency and timing constraints on action invocations. The implementation of our language constructs with real-time scheduling and locking for concurrency control is also described, including a novel deadlock prevention technique. The utility of the constructs are demonstrated in two ways. First, we describe their use to solve a general concurrent real-time problem called timed atomic commitment. We then describe how they were used to program a graphic simulation of two robot arms coordinating to pick up a moving object under timing constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2662770",
                    "name": "V. Wolfe"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                }
            ]
        },
        {
            "paperId": "3aa47eb4103896b155424317110bcbdecb543736",
            "title": "Semi-materialization: a performance analysis",
            "abstract": "The performance of queries on relational views can be greatly improved by keeping a stored copy of the materialized view. However, the cost of maintaining materialized views during updates on base relations is often high. This is especially so in the case of view definitions with universal quantifiers. An alternative approach called semi-materialization has been proposed, whereby carefully chosen redundant subsets of data of individual relations are stored. These subsets represent an intermediate state of view evaluation rather than a complete evaluation. The authors compare the performance of queries over views using query modification, semi-materialization and full materialization. The view definition expressions considered are: select-project-join, and a general form of calculus expressions. The results show comparable performance of full-materialization and semi-materialization for select-project-join expressions, and superior performance of semi-materialization for general expressions.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8445292",
                    "name": "M. Kamel"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "23edf877cbd41802878d3660cfc96fe8aabc4625",
            "title": "Redundancy: an approach to the efficient implementation of semantic integrity assertions",
            "abstract": "Semantic integrity assertions accurately model the real world by defining consistent database states; however, the assertions are expensive to enforce because checking them may require numerous database accesses. The authors therefore propose a method which alleviates this expense by storing redundant data. Integrity constraints are assumed to be expressed in a relational calculus-like language with an arbitrary number of quantifiers and a general logical structure. The method chooses clustered redundant data such that the benefit of using the redundant data for testing the assertions outweighs the cost of their maintenance during updates.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8445292",
                    "name": "M. Kamel"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "5688ca204923d94e9e589a26987871009de8b21e",
            "title": "Replicated data and partition failures",
            "abstract": "In a distributed database system, data is often replicated to improve performance and availability. By storing copies of shared data on processors where it is frequently accessed, the need for expensive, remote read accesses is decreased. By storing copies of critical data on processors with independent failure modes, the probability that at least one copy of the data will be accessible increases. In theory, data replication makes it possible to provide arbitrarily high data availability. In practice, realizing the benefits of data replication is difficult since the correctness of data must be maintained. One important aspect of correctness with replicated data is mutual consistency: all copies of the same logical data-item must agree on exactly one \"current value\" for the data-item. Furthermore, this value should \"make sense\" in terms of the transactions executed on copies of the data-item. When communication fails between sites containing copies of the same logical data-item, mutual consistency between copies becomes complicated to ensure. The most disruptive of these communication failures are partition failures, which fragment the network into isolated subnetworks called partitions. Unless partition failures are detected and recognized by all affected processors, independent and uncoordinated updates may be applied to different copies of the data, thereby compromising the correctness of data. Consider, for example, an Airline Reservation System implemented by a distributed database which splits into two partitions when the communication network fails. If, at the time of the failure, all the nodes have one seat remaining for PAN AM 537, reservations could be made in both partitions. This would violate correctness: who should get the last seat? There should not be more seats reserved for a flight than physically exist on the plane. (Some airlines do not implement this constraint and allow overbookings.) The design of a replicated data management algorithm tolerating partition failures (or partition processing strategy) is a notoriously hard problem. Typically, the cause or extent of a partition failure cannot be discerned by the processors themselves. At best, a processor may be able to identify the other processors in its partition; but, for the processors outside of its partition, it will not be able to distinguish between the case where those processors are simply isolated from it and the case where those processors are down. In addition, slow responses can cause the network to appear partitioned even when it is not, further complicating the design of a fault-tolerant algorithm. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-89-02. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/753 REPLICATED DATA AND PARTITION FAILURES",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "e27e6fc30adefb111b8234f198b8543b2379e02e",
            "title": "A Performance Analysis of Times Synchronous Communication Primitives",
            "abstract": "The performance of two algorithms for timed synchronous communication between a single sender and a single receiver is analyzed. Each weakens the definition of correct timed synchronous communication in a different way, and exhibits a different undesirable behavior. Their sensitivity to various parameters is discussed. These parameters include how long the processes are willing to wait for communication to be successful, how well synchronized the processes are, the assumed upper bound on message delay, and the actual end-to-end message delay distribution. The fault tolerance of the algorithms is discussed and a mixed strategy is proposed that avoids some of the performance problems. >",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "097c0df452aed1aa1525dad857e970abb2864185",
            "title": "Maintaining Consistency Over A Network in Real-Time Applications",
            "abstract": "In a large class of hard-real-time control applications, components execute concurrently on distributed nodes and periodically coordinate, under timing constraints, to perform the control task. As such, they perform a type of atomic commitment. Traditional atomic commitment differs, however, because there is no notion of a deadline. We therefore present a model and correctness criteria for timed atomic commitment (TAC) which requires the processes to be functionally consistent, but allows the outcome to include an exceptional state, indicating that timing constraints have been violated. An extension to accomodate the periodic behavior frequently found in control applications is then discussed, and a decentralized implementation is outlined. We conclude by presenting language constructs for these features, and illustrate their use with an example.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2662770",
                    "name": "V. Wolfe"
                }
            ]
        },
        {
            "paperId": "6467cafc635f4b9ee0a5c893907efd01fb455dee",
            "title": "Language Constructs for Distributed Real-Time Consistency",
            "abstract": "In this paper, we present a model and language constructs for a distributed real-time system with the goal of allowing the structured specification of functional and timing constraints, along with explicit, early error recovery from timing faults. To do this, we draw on ideas from (non-distributed) real-time programming and distributed transaction-based systems [81]. A complete language is not specified; the constructs described are assumed to be embedded in a block-structured procedural host programming language such as C [9] or C++ [10] (our current preliminary implementation is in C). The model consists of resources, processes, and a global scheduler. Resources are abstractions that export operations to processes, and specify acceptable concurrency of operations to the scheduler. Processes manipulate resources using the exported operations, and specify synchronization and restrictions on concurrency (at the exported operation level) to the scheduler. Examples of the types of information given to the scheduler are that a set of operations should be performed \"simultaneously\", or that a sequence of operations should be performed without interference by another process. The global scheduler embodies the entity or entities that schedule the CPU, memory, devices and other resources in the system. It performs preemptive scheduling of all resources based on dynamic priorities associated with the processes, preserves restrictions on concurrency stated by resources and processes, and is capable of giving \"guarantees\" to processes that they will receive resources during a specified future time interval. The remainder of the paper is structured as follows. In the next section, we present language constructs for an expression of timing constraints called temporal scopes, and described resources and processes. Section 3 describes what is required of the global scheduler to support these constructs, and what is entailed in guaranteeing functional consistency.' We conclude in Section 4. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-89-78. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/813 Language Constructs For Distributed Real-Time Consistency MS-CIS-89-78 GRASP LAB 199 Victor Wolfe Susan Davidson Insup Lee Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2662770",
                    "name": "V. Wolfe"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                }
            ]
        },
        {
            "paperId": "73c966f41bc9030020688414518bb7ae86089b1a",
            "title": "Language constructs for timed atomic commitment",
            "abstract": "In a large class of hard-real-time control applications, components execute concurrently on distributed nodes and must coordinate, under timing constraints, to perform the control task. As such, they perform a type of atomic commitment. In traditional atomic commitment there are no timing constraints; agreement is eventual. The authors present a definition of timed atomic commitment (TAC) which requires the processes to be functionally consistent, but allows the outcome to include an exceptional state, indicating that faults have caused timing constraints to be violated. The authors also present a high-level language construct that facilitates the use of TAC in distributed real-time programming and discuss its behavior when faults occur.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                },
                {
                    "authorId": "2662770",
                    "name": "V. Wolfe"
                }
            ]
        },
        {
            "paperId": "953d9105cd694498c53a00eb8956fa06bc3f433e",
            "title": "A protocol for timed atomic commitment",
            "abstract": "A model and correctness criteria for timed atomic commitment (TAC) are presented which require the processes to be functionally consistent, but allow the outcome to include an exceptional state, indicating that timing constraints have been violated. Correct TAC behavior is defined by presenting an abstract description of the processes involved in the commitment and minimal correctness criteria for their behavior. The correctness criteria capture the intuitive notion that an exception outcome should only occur in the presence of faults, and an aborted outcome should only occur if faults occur or some process votes no. A centralized two-phase commit protocol was modified to meet the correctness criteria by introducing deadlines on the various stages the participants go through (voting and performing), and on the decision phase for the coordinator. The deadlines are derived using several system parameters: maximum message delay, clock drift, and execution time. The protocol is then shown to be correct.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                },
                {
                    "authorId": "2662770",
                    "name": "V. Wolfe"
                }
            ]
        },
        {
            "paperId": "2d45c6cbce7adce333150c392af87b889455739a",
            "title": "A semantics for complex objects and approximate queries",
            "abstract": "A new definition of complex objects is introduced which provides a denotation for incomplete tuples as well as partially described sets. Set values are \u201csandwiched\u201d between \u201ccomplete\u201d and \u201cconsistent\u201d descriptions (representing the Smyth and Hoare powerdomains respectively), allowing the maximal values to be arbitrary subsets of maximal elements in the domain of the set. We also examine the use of rules in defining queries over such objects.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "51160366",
                    "name": "A. Watters"
                }
            ]
        },
        {
            "paperId": "86f12cc08272656db01f2bfed0053c5487cf7684",
            "title": "Modeling Reliable Distributed Real-Time Programs",
            "abstract": "A model for distributed hard real-time programs should incorporate real-time characteristics and be capable of analyzing time-related reliability issues. We introduce a model called the Real-Tie Selection/ Resolution (RT-SIR) Model with these capabilities and demonstrate it by example. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-88-81. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/599 MODELING RELIABLE DISTRIBUTED REAL-TIME PROGRAMS Victor Wolfe, Susan B. Davidson and lnsup Lee MS-CIS-88-81 GRASP LAB 157 Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 191 04",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2662770",
                    "name": "V. Wolfe"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                }
            ]
        },
        {
            "paperId": "87431431ada98ed543511cc24e807990e05fb667",
            "title": "Partial Computation in Real-Time Database Systems: A Research Plan",
            "abstract": "State-of-the-art database management systems are inappropriate for real-time applications due to their lack of speed and predictability of response. To combat these problems, the scheduler needs to be able to take advantage of the vast quantity of semantic and timing information that is typically available in such systems. Furthermore, to improve predictability of response, the system should be capable of providing a partial, but correct, response in a timely manner. We therefore propose to develop a semantics for real-time database systems that incorporates temporal knowledge of data-objects, their validity, and computation using their values. This temporal knowledge should include not just historical information but future knowledge of when to expect values to appear. This semantics will be used to develop a notion of approximate or partial computation, and to develop schedulers appropriate for real-time transactions. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-88-82. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/684 PARTIAL COMPUTATION IN REAL-TIME DATABASE SYSTEMS: A RESEARCH PLAN Susan 8. Davidson and lnsup Lee MS-CIS-88-82 GRASP LAB 158 Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 191 04",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                }
            ]
        },
        {
            "paperId": "9895d9e63e87d6650c1a023aa968b0b440d072b0",
            "title": "Formally Integrating Real-Time Specification: A Research Proposal",
            "abstract": "To date, research in reasoning about timing properties of real-time programs has considered specification and implementation as separate issues. Specification uses formal methods; it abstracts out program execution, defining a specification that is independent of any machine-specific details (see [I, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] for examples). In this manner, it describes only the high-level timing requirements of processes in the system, and dependencies between them. One then typically attempts to prove the mutual consistency of these timing constraints, or to determine whether the constraints maintain a safety property critical to system correctness. However, since the model has abstracted out machine-specific details, these correctness proofs either assume very optimistic operating environment (such as a one to one assignment of processes to processors), or make very pessimistic assumptions (such as that all interleavings of process executions are possible). Since neither of these assumptions will hold in practice, these \"predictions\" about the behavior of the system may not be accurate. The implementation level captures this operating environment: a realtime system is characterized by such things as process schedulers, devices and local clocks. However, advances here have been primarily in scheduling theory (examples of which are [15, 16]) and language design (examples of which are [15, 16, 17, 18,19,20]). Unfortunately, since formal models have not been used at this level, proofs of time-related properties cannot be made. To construct these proofs, we must show that an implementation is correct with respect to a specification; timing properties that can be shown to hold about the specification will therefore be known to hold for the implementation. We therefore need to represent the implementation formally so as to prove that the implementation satisfies the specification. The proof of satisfaction requires a well-defined formal mapping between the implementation and specification models. We therefore propose to develop an integrated bi-level approach to the problem of reasoning about timing properties of real-time programs. At the specification level, we will use the Timed Acceptances model, a logically sound and complete axiom system which we have recently developed [21]. Using this model, the effect of interaction among time dependent processes can be precisely specified and then analyzed. We will then develop a formal implementation model (similar to the specification model) which captures operational behaviors: for example, the assignment of processes to processors, assumptions about scheduling and clock synchronization, and the different treatment of execution and wait times. A mapping will then be formulated between these two layers. The bulk of our proposed work will be to formulate the implementation layer and define a mapping between it and the specification layer. We also need to continue work on the Timed Acceptances model to facilitate its use as a specification model, and to provide \"hooks\" for mappings between the two layers. The rest of this proposal is organized as follows. The next section overviews related work in formal specification models. Section 3 describes our current specification model and proposed enhancements. We also detail the proposed implementation model, and required properties of the mappings between the two models. Section 4 provides a summary of the proposed research, and a yearly plan. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-88-84. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/766 FORMALLY INTEGRATING REAL-TIME SPECIFICATION AND IMPLEMENTATION: A RESEARCH PROPOSAL lnsup Lee, Susan B. Davidson and Richard Gerber MS-CIS-88-84 GRASP LAB 160 Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 191 04",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "143878156",
                    "name": "R. Gerber"
                }
            ]
        },
        {
            "paperId": "d892c1465c07cb35af4af92abbb997a194764e20",
            "title": "Partial Computation in Real-Time Database Systems",
            "abstract": "A critical component of real-time systems in the database, which is used to store external input such as environmental readings from sensors, as well as system information. Typically these databases are large, due to vast quantities of historical data, and are distributed, due to the distributed topology of the devices controlling the application. Hence, sophisticated database management systems are needed. However, most of the time database systems are hand-coded. Off-the-shelf database management systems are not used due in part to a lack of predictability of response [1, 2]. We motivate the use of partial computation of database queries as a method of improving the fault-tolerance and predictability of response in real-time database systems. Comments",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "51160366",
                    "name": "A. Watters"
                }
            ]
        },
        {
            "paperId": "049dd65779fca07f5afc0666ffccf9c16334d674",
            "title": "A Performance Comparison of Optimistic versus Conservative Strategies during Partition Failures in Distributed Databases",
            "abstract": "Abstract:Two strategies for processing transactions during partition failures in distributed databases are reviewed: the optimistic protocol and conservative class conflict graph analysis. Both use graph techniques for detecting and resolving conflicts, although one is \u201coptimistic,\u201d detecting and resolving conflict after the failure is repaired, while the other is \u201cconservative,\u201d detecting and preventing potential conflicts when the failure occurs. A simulation comparing the two approaches with respect to the cost of missed opportunity, the cost of repair, and overhead cost is presented, along with sample results. The optimistic protocol generally minimizes missed opportunity, while conservative class conflict graph analysis requires less overhead and no repair. The applicability of these approaches to fractured networks involving more than two partitions is also discussed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "2053858496",
                    "name": "Mark M. Winkler"
                }
            ]
        },
        {
            "paperId": "65c9f635d8317dc9d1e477af25491bbf8c812262",
            "title": "Adding Time to Synchronous Process Communications",
            "abstract": "In distributed real-time systems, communicating processes cannot be delayed for arbitrary amounts of time while waiting for messages. Thus, communication primitives used for real-time programming usually allow the inclusion of a deadline or timeout to limit potential delays due to synchronization. This paper interprets timed synchronous communication as having absolute deadlines. Various ways of implementing deadlines are discussed, and two useful timed synchronous communication problems are identified which differ in the number of participating senders and receivers and type of synchronous communication. For each problem, a simple algorithm is presented and shown to be correct. The algorithms are shown to guarantee maximal success and to require the smallest delay intervals during which processes wait for synchronous communication. We also evaluate the number of messages used to reach agreement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "9271f4a775b47667314f4f48aa4c411b44ede792",
            "title": "Motivating Time as a First Class Entity",
            "abstract": "In hard real-time applications, programs must not only be functionally correct but must also meet timing constraints. Unfortunately, little work has been done to allow a high-level incorporation of timing constraints into distributed real-time programs. Instead the programmer is required to ensure system timing through a complicated synchronization process or through low-level programming, making it difficult to create and modify programs. In this report, we describe six features that must be integrated into a high level language and underlying support system in order to promote time to a first class position in distributed real-time programming systems: expressibility of time, real-time communication, enforcement of timing constraints, fault tolerance to violations of constraints, ensuring distributed system state consistency in the time domain, and static timing verification. For each feature we describe what is required, what related work had been performed, and why this work does not adequately provide sufficient capabilities for distributed real-time programming. We then briefly outline an integrated approach to provide these six features using a high-level distributed programming language and system tools such as compilers, operating systems, and timing analyzers to enforce and verify timing constraints. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-87-54. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/288 MOTIVATING TIME AS A FIRST CLASS ENTITY lnsup Lee Susan Davidson Victor Wolfe . Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 191 04-6389 (revised October 1987) Acknowledgements: This research was supported in part by NSF grants DCR 8501482, DMC 851 2838, MCS 8219196-CER, U.S. Army grants DAA29-84-K-0061, DAA29-84-9-0027 and a grant from AT&T's Telecommunications Program at the University of Pennsylvania. Motivating Time As A First Class Entity Insup Lee, Susan Davidson, Victor Wolfe Department of Computer and Information Science The University of Pennsylvania Philadelphia, PA 19104 October 30, 1987 Abstract In hard real-time applications, programs must not only be functionally correct but must also meet timing constraints. Unfortunately, little work has been done to allow a high-level incorporation of timing constraints into distributed real-time programs. Instead the programmer is required to ensure system timing through a complicated synchronization process or through low-level programming, making it difficult to create and modify programs. In this report, we describe six features that must be integrated into a high level language and underlying support system in order to promote time to a first class position in distributed real-time programming systems: expressibility of time, real-time communication, enforcement of timing constraints, fault tolerance to violations of constraints, ensuring distributed system state consistency in the time domain, and static timing verification. For each feature we describe what is required, what related work has been performed, and why this work does not adequately provide sufficient capabilities for distributed real-time programming. We then briefly outline an integrated approach to provide these six features using a high-level distributed programming language and system tools such as compilers, operating systems, and timing analyzers to enforce and verify timing constraints.In hard real-time applications, programs must not only be functionally correct but must also meet timing constraints. Unfortunately, little work has been done to allow a high-level incorporation of timing constraints into distributed real-time programs. Instead the programmer is required to ensure system timing through a complicated synchronization process or through low-level programming, making it difficult to create and modify programs. In this report, we describe six features that must be integrated into a high level language and underlying support system in order to promote time to a first class position in distributed real-time programming systems: expressibility of time, real-time communication, enforcement of timing constraints, fault tolerance to violations of constraints, ensuring distributed system state consistency in the time domain, and static timing verification. For each feature we describe what is required, what related work has been performed, and why this work does not adequately provide sufficient capabilities for distributed real-time programming. We then briefly outline an integrated approach to provide these six features using a high-level distributed programming language and system tools such as compilers, operating systems, and timing analyzers to enforce and verify timing constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144637634",
                    "name": "Insup Lee"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                },
                {
                    "authorId": "1403843160",
                    "name": "V. Fay-Wolfe"
                }
            ]
        },
        {
            "paperId": "fcb8acf285e0157f0b9dfb9f457a95dc1c4da903",
            "title": "Applications of Byzantine agreement in database systems",
            "abstract": "In this paper we study when and how B Byzantine agreement protocol can he used in general-purpose database management systems. We present an overview of the failure model used for Byzantine agreement, and of the protocol itself. We then present correctness criteria for database processing in this failure environment and discuss strategies for satisfying them. In doing this, we present new failure models for input/output nodes and study ways to distribute input transactions to processing nodes under these models. Finally, we investigate applications of Byzantine agreement protocols in the more common failure environment where processors are assumed to halt after a failure.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398574232",
                    "name": "H. Garcia-Molina"
                },
                {
                    "authorId": "1840093",
                    "name": "Frank M. Pittelli"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "40294ced8d50305b53f3f4e4a3a32fa14b682a1d",
            "title": "Is byzantine agreement useful in a distributed database?",
            "abstract": "This paper is an attempt to bridge the gap that is developing between \"practitioners\" and \"theoreticians\" with respect to the use of Byzantine Agreement protocols in distributed database systems. We present an informal overview of Byzantine Agreement and study when and how this type of protocol can be used in general-purpose database management systems. We argue that the main application of this protocol is in the distribution of input transactions to a fully replicated database system. We also argue that other database uses, such as for transaction commit, message broadcast, and object location, may be limited.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398574232",
                    "name": "H. Garcia-Molina"
                },
                {
                    "authorId": "1840093",
                    "name": "Frank M. Pittelli"
                },
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "671a7b74800ad3bb8ae71e34be71cf685910e381",
            "title": "Optimism and consistency in partitioned distributed database systems",
            "abstract": "A protocol for transaction processing during partition failures is presented which guarantees mutual consistency between copies of data-items after repair is completed. The protocol is \u201coptimistic\u201d in that transactions are processed without restrictions during failure; conflicts are then detected at repair time using a precedence graph, and are resolved by backing out transactions according to some backout strategy. The resulting database state then corresponds to a serial execution of some subset of transactions run during the failure. Results from simulation and probabilistic modeling show that the optimistic protocol is a reasonable alternative in many cases. Conditions under which the protocol performs well are noted, and suggestions are made as to how performance can be improved. In particular, a backout strategy is presented which takes into account individual transaction costs and attempts to minimize total backout cost. Although the problem of choosing transactions to minimize total backout cost is, in general, NP-complete, the backout strategy is efficient and produces very good results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        },
        {
            "paperId": "26a7d1e9e5e139bf0ddf86f39db2cf0b875adf2f",
            "title": "An optimistic protocol for partitioned distributed database systems",
            "abstract": "A protocol for transaction processing during partition failures is presented which guarantees mutual consistency between copies of data-items after repair is completed. The protocol is \"optimistic\" in that transactions are processed without restrictions during the failure; conflicts are detected at repair time using a precedence graph and are resolved by backing out transactions according to some backout strategy. \nThe protocol is then evaluated using simulation and probabilistic modeling. In the simulation, several parameters are varied such as the number of transactions processed in a group, the type of transactions processed, the number of data-items present in the database, and the distribution of references to data-items. The simulation also uses different backout strategies. From these results we note conditions under which the protocol performs well, i.e. conditions under which the protocol backs out a small percentage of the transaction run. A probabilistic model is developed to estimate the expected number of transactions backed out using most of the above database and transaction parameters, and is shown to agree with simulation results. \nSuggestions are then made on how to improve the performance of the protocol. Insights gained from the simulation and probabilistic modeling are used to develop a backout strategy which takes into account individual transaction costs and attempts to minimize total backout cost. Although the problem of choosing transactions to minimize total backout cost is, in general, NP-complete, the backout strategy is efficient and produces very good results. Details of how to efficiently implement the optimistic protocol are also discussed. In particular, efficient algorithms are given for constructing the precedence graph in the context of several different concurrency control algorithms. Finally, the question of using the semantics of transactions to decrease the number of transactions backed out is considered, and guidelines for future research are given.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49702692",
                    "name": "S. Davidson"
                }
            ]
        }
    ]
}