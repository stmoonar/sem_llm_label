{
    "authorId": "2133842786",
    "papers": [
        {
            "paperId": "0f84514e42c080efbc94f4aa2b2261e81d08ca24",
            "title": "Curricular Object Manipulation in LiDAR-based Object Detection",
            "abstract": "This paper explores the potential of curriculum learning in LiDAR-based 3D object detection by proposing a curricular object manipulation (COM) framework. The framework embeds the curricular training strategy into both the loss design and the augmentation process. For the loss design, we propose the COMLoss to dynamically predict object-level difficulties and emphasize objects of different difficulties based on training stages. On top of the widely-used augmentation technique called GT-Aug in Li-DAR detection tasks, we propose a novel COMAug strategy which first clusters objects in ground-truth database based on well-designed heuristics. Group-level difficulties rather than individual ones are then predicted and updated during training for stable results. Model performance and generalization capabilities can be improved by sampling and augmenting progressively more difficult objects into the training samples. Extensive experiments and ablation studies reveal the superior and generality of the proposed framework. The code is available at https://github.com/ZZY816/COM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121394809",
                    "name": "Ziyue Zhu"
                },
                {
                    "authorId": "2112721678",
                    "name": "Q. Meng"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "2218779354",
                    "name": "Liujiang Yan"
                },
                {
                    "authorId": "2119197417",
                    "name": "Jian Yang"
                }
            ]
        },
        {
            "paperId": "d12418585c26c617db33fe3d8baed11fe8a00226",
            "title": "RS-DPSNet: Deep Plane Sweep Network for Rolling Shutter Stereo Images",
            "abstract": "Since the rolling shutter (RS) camera successively exposes each scanline, accurately reconstructing scene depth from an RS stereo image pair remains a great challenge. Directly applying the deep-learning-based depth estimation methods tailored for the global shutter (GS) stereo images leads to undesirable RS depth results due to inherent flaws in the network structure. In this letter, we fill this gap by developing an end-to-end RS-stereo-aware plane sweep network to improve the accuracy of the classic GS-based algorithm (i.e. DPSNet) in estimating the RS depth map. Specifically, we derive the RS-stereo-aware plane sweep model and further produce a more accurate and efficient cost volume through the effective incorporation of this model within DPSNet. Furthermore, to enable learning-based approaches to address the depth estimation problem in the context of RS stereo images, we contribute the first RS stereo dataset, CARLA-RSS. Experimental results demonstrate that our proposed pipeline achieves state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052131055",
                    "name": "Bin Fan"
                },
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "2116916030",
                    "name": "Yuchao Dai"
                },
                {
                    "authorId": "40214723",
                    "name": "Mingyi He"
                }
            ]
        },
        {
            "paperId": "6dc0bf7bfc8d72959245178a54cc1243b1e7d267",
            "title": "Robust Visual Object Tracking with Two-Stream Residual Convolutional Networks",
            "abstract": "The current deep learning based visual tracking approaches have been very successful by learning the target classification and/or estimation model from a large amount of supervised training data in offline mode. However, most of them can still fail in tracking objects due to some more challenging issues such as dense distractor objects, confusing background, motion blurs, and so on. Inspired by the human \u201cvisual tracking\u201d capability which leverages motion cues to distinguish the target from the background, we propose a Two-Stream Residual Convolutional Network (TS-RCN) for visual tracking, which successfully exploits both appearance and motion features for model update. Our TS-RCN can be integrated with existing deep learning based visual trackers. To further improve the tracking performance, we adopt a \u201cwider\u201d residual network ResNeXt as its feature extraction backbone. To the best of our knowledge, TS-RCN is the first end-to-end trainable two-stream visual tracking system, which makes full use of both appearance and motion features of the target. We have extensively evaluated the TS-RCN on most widely used benchmark datasets including VOT2018, VOT2019, and GOT-10K. The experiment results have successfully demonstrated that our two-stream model can greatly outperform the appearance-based tracker, and achieves state-of-the-art performance. The tracking system can run at up to 38.1 FPS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153010939",
                    "name": "Ning Zhang"
                },
                {
                    "authorId": "1800425",
                    "name": "Jingen Liu"
                },
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "50190972",
                    "name": "Dan Zeng"
                },
                {
                    "authorId": "144025741",
                    "name": "Tao Mei"
                }
            ]
        },
        {
            "paperId": "cfe5800ed99eea12f1e8265bce0d86419f696239",
            "title": "Relative Pose Estimation For Stereo Rolling Shutter Cameras",
            "abstract": "In this paper, we present a novel linear algorithm to estimate the 6 DoF relative pose from consecutive frames of stereo rolling shutter (RS) cameras. Our method is derived based on the assumption that stereo cameras undergo motion with constant velocity around the center of the baseline, which needs 9 pairs of correspondences on both left and right consecutive frames. The stereo RS images enable the recovery of depth maps from the semi-global matching (SGM) algorithm. With the estimated camera motion and depth map, we can correct the RS images to get the undistorted images without any scene structure assumption. Experiments on both simulated points and synthetic RS images demonstrate the effectiveness of our algorithm in relative pose estimation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "2052131055",
                    "name": "Bin Fan"
                },
                {
                    "authorId": "1681554",
                    "name": "Yuchao Dai"
                }
            ]
        },
        {
            "paperId": "4d07ff083412ce5e60121615909adc0c842cd06b",
            "title": "l1-Regularized Hull Representation for Visual Tracking",
            "abstract": ". Due to various factors such as partial occlusions, fast motion and illumination variations, developing an e\ufb00ective and e\ufb03cient appearance model is a challenging task. In this paper, we propose a simple and e\ufb00ective tracking algorithm with an appearance model based on (cid:96) 1 -regularized hull representation with target templates. (cid:96) 1 - regularized a\ufb03ne combinations can cover target appearances which do not appear in target templates. (cid:96) 1 constraint enables the tracking algorithm to robustly deal with partial occlusions and outliers. A novel likelihood function is introduced, which is derived from the reconstruction residual between a target candidate and the target templates and target template coe\ufb03cients. Experimental results on several challenging video sequences against state-of-the-art tracking algorithms demonstrate that the proposed tracking algorithm is robust to partial occlusions, illumination variations, background clutters, etc.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2039394292",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "9720088",
                    "name": "Yuanyun Wang"
                },
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "2414877",
                    "name": "Chengzhi Deng"
                }
            ]
        },
        {
            "paperId": "4fd30692ec72dbaa5ca451d52de5565a3e1b657d",
            "title": "Physics-Inspired Garment Recovery from a Single-View Image",
            "abstract": "Most recent garment capturing techniques rely on acquiring multiple views of clothing, which may not always be readily available, especially in the case of pre-existing photographs from the web. As an alternative, we propose a method that is able to compute a 3D model of a human body and its outfit from a single photograph with little human interaction. Our algorithm is not only able to capture the global shape and overall geometry of the clothing, it can also extract the physical properties (i.e., material parameters needed for simulation) of cloth. Unlike previous methods using full 3D information (i.e., depth, multi-view images, or sampled 3D geometry), our approach achieves garment recovery from a single-view image by using physical, statistical, and geometric priors and a combination of parameter estimation, semantic parsing, shape/pose recovery, and physics-based cloth simulation. We demonstrate the effectiveness of our algorithm by re-purposing the reconstructed garments for virtual try-on and garment transfer applications and for cloth animation on digital characters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "70599695",
                    "name": "Shan Yang"
                },
                {
                    "authorId": "1831485",
                    "name": "Zherong Pan"
                },
                {
                    "authorId": "9215506",
                    "name": "Tanya Amert"
                },
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "1714982",
                    "name": "Licheng Yu"
                },
                {
                    "authorId": "1685538",
                    "name": "Tamara L. Berg"
                },
                {
                    "authorId": "144247566",
                    "name": "Ming C Lin"
                }
            ]
        },
        {
            "paperId": "ad0be3afaeb9308f3d9065111fa016911b771a35",
            "title": "Retweet Wars: Tweet Popularity Prediction via Dynamic Multimodal Regression",
            "abstract": "If a picture is worth a thousand words, then images should be utilized together with other available data modalities when predicting the virality of online posts, such as tweets. In this paper, we re-visit the tweet popularity prediction problem by considering all data modalities: tweet language semantics, embedded images, author' social relationships, and the diffusion process of tweets. To model the content of tweets, we propose a joint-embedding neural network that combines visual, textual, and social cues together. Such content features can be either used for prediction directly, or for pre-conditioning a 'dynamics RNN', which models the message propagation process. A novel Poisson regression loss is optimized to train the network. We demonstrate that content based features can be used to improve upon social features and dynamics features via our joint-embedding regression model. Our model outperforms the state-of-the-art on multiple large-scale real-world datasets collected from Twitter.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "40454588",
                    "name": "Jan-Michael Frahm"
                }
            ]
        },
        {
            "paperId": "a8b509eca0cb741dce2a0c7254bd90c7159dd6f6",
            "title": "Fast and Accurate Satellite Multi-view Stereo Using Edge-Aware Interpolation",
            "abstract": "In this paper, we propose a fast and accurate approach for 3D reconstructions from satellite images. Compared with traditional images, satellite imagery features enormous pixel count, inaccurate camera calibration, and low ground sampling rate, all of which makes multi-view stereo for satellite images more challenging. Our approach first computes sparse but reliable 2D feature matches between image pairs. Such feature matches are used to compensate the extrinsic calibration errors. Preliminary dense correspondences are obtained via edge-aware interpolation of sparse feature matches. We rely on fast bilateral smoothing to refine such initial dense matches, which greatly improves the computational efficiency of our method. The smoothed dense correspondences and the refined camera model are then used to obtain dense 3D point clouds via triangulation. Our proposed method outperforms state-of-the-art baseline methods in both efficiency and accuracy on real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "40454588",
                    "name": "Jan-Michael Frahm"
                }
            ]
        },
        {
            "paperId": "f8f543a3af22d4ae6ca7614094018092ffae98b2",
            "title": "Single View Parametric Building Reconstruction from Satellite Imagery",
            "abstract": "Satellite images have broad coverage, thus are ideal for large-scale urban reconstruction tasks. However, their low ground sampling resolution posed great challenges in using traditional volumetric or stereo methods to perform 3D reconstructions. In this paper, we propose a novel deep learning based approach to perform single-view parametric reconstructions from satellite imagery. By parametrizing buildings as 3D cuboids, our method extends object detection systems to simultaneously localize buildings and directly fit parametric models for each identified building. We utilize geo-registered GIS vector maps and Lidar data as supervision to train the network. Especially, we deconvolve the feature maps and combine convolutional feature maps at different stages of the network to deal with the heavily cluttered but small in size building instances from satellite imagery. We further enforce physical constraints that building cannot overlap by predicting building boundaries using a separate fully convolutional network. We demonstrate the effectiveness of our proposed methods on real-world data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "40454588",
                    "name": "Jan-Michael Frahm"
                }
            ]
        },
        {
            "paperId": "d9fa14d9a914ed5f899d48dba005a82160f58f56",
            "title": "Detailed Garment Recovery from a Single-View Image",
            "abstract": "Most recent garment capturing techniques rely on acquiring multiple views of clothing, which may not always be readily available, especially in the case of pre-existing photographs from the web. As an alternative, we pro- pose a method that is able to compute a rich and realistic 3D model of a human body and its outfits from a single photograph with little human in- teraction. Our algorithm is not only able to capture the global shape and geometry of the clothing, it can also extract small but important details of cloth, such as occluded wrinkles and folds. Unlike previous methods using full 3D information (i.e. depth, multi-view images, or sampled 3D geom- etry), our approach achieves detailed garment recovery from a single-view image by using statistical, geometric, and physical priors and a combina- tion of parameter estimation, semantic parsing, shape recovery, and physics- based cloth simulation. We demonstrate the effectiveness of our algorithm by re-purposing the reconstructed garments for virtual try-on and garment transfer applications, as well as cloth animation for digital characters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "70599695",
                    "name": "Shan Yang"
                },
                {
                    "authorId": "9215506",
                    "name": "Tanya Amert"
                },
                {
                    "authorId": "1831485",
                    "name": "Zherong Pan"
                },
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "1714982",
                    "name": "Licheng Yu"
                },
                {
                    "authorId": "1685538",
                    "name": "Tamara L. Berg"
                },
                {
                    "authorId": "144247566",
                    "name": "Ming C Lin"
                }
            ]
        }
    ]
}