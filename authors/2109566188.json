{
    "authorId": "2109566188",
    "papers": [
        {
            "paperId": "0cb2ca7ab5145c078ecae88ce0dab28acb559767",
            "title": "Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2291065818",
                    "name": "Juan Manuel Zambrano Chaves"
                },
                {
                    "authorId": "2267360148",
                    "name": "Shih-Cheng Huang"
                },
                {
                    "authorId": "2279740020",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2263936549",
                    "name": "Hanwen Xu"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "2267010012",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2291048834",
                    "name": "Yujia Xie"
                },
                {
                    "authorId": "2268760479",
                    "name": "Mahmoud Khademi"
                },
                {
                    "authorId": "2291073936",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "3032929",
                    "name": "H. Awadalla"
                },
                {
                    "authorId": "2268383237",
                    "name": "Julia Gong"
                },
                {
                    "authorId": "2291084567",
                    "name": "Houdong Hu"
                },
                {
                    "authorId": "2279705714",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2109738542",
                    "name": "Chun-yue Li"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2260374573",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                },
                {
                    "authorId": "2188270295",
                    "name": "M. Lungren"
                },
                {
                    "authorId": "2275638250",
                    "name": "Serena Yeung-Levy"
                },
                {
                    "authorId": "2249123829",
                    "name": "Curtis P. Langlotz"
                },
                {
                    "authorId": "2302416483",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "ab88cdefc888fb102b91140bd6e6f8eafef3d135",
            "title": "Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation",
            "abstract": "The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world clinics. Frontier general-domain models such as GPT-4V still have significant performance gaps in multimodal biomedical applications. More importantly, less-acknowledged pragmatic issues, including accessibility, model cost, and tedious manual evaluation make it hard for clinicians to use state-of-the-art large models directly on private patient data. Here, we explore training open-source small multimodal models (SMMs) to bridge competency gaps for unmet clinical needs in radiology. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space, as exemplified by LLaVA-Med. For training, we assemble a large dataset of over 697 thousand radiology image-text pairs. For evaluation, we propose CheXprompt, a GPT-4-based metric for factuality evaluation, and demonstrate its parity with expert evaluation. For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training. The resulting LlaVA-Rad (7B) model attains state-of-the-art results on standard radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B). The inference of LlaVA-Rad is fast and can be performed on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2291065818",
                    "name": "Juan Manuel Zambrano Chaves"
                },
                {
                    "authorId": "2267360148",
                    "name": "Shih-Cheng Huang"
                },
                {
                    "authorId": "2279740020",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2263936549",
                    "name": "Hanwen Xu"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2291048834",
                    "name": "Yujia Xie"
                },
                {
                    "authorId": "2268760479",
                    "name": "Mahmoud Khademi"
                },
                {
                    "authorId": "2291073936",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "3032929",
                    "name": "H. Awadalla"
                },
                {
                    "authorId": "2268383237",
                    "name": "Julia Gong"
                },
                {
                    "authorId": "2291084567",
                    "name": "Houdong Hu"
                },
                {
                    "authorId": "2279705714",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2244470181",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2260374573",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                },
                {
                    "authorId": "2188270295",
                    "name": "M. Lungren"
                },
                {
                    "authorId": "2275638250",
                    "name": "Serena Yeung-Levy"
                },
                {
                    "authorId": "2249123829",
                    "name": "Curtis P. Langlotz"
                },
                {
                    "authorId": "2261294491",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "d8ff39b155f71a0391b5e13c7eb4a16d3e31e55f",
            "title": "Foundation Models for Biomedical Image Segmentation: A Survey",
            "abstract": "Recent advancements in biomedical image analysis have been significantly driven by the Segment Anything Model (SAM). This transformative technology, originally developed for general-purpose computer vision, has found rapid application in medical image processing. Within the last year, marked by over 100 publications, SAM has demonstrated its prowess in zero-shot learning adaptations for medical imaging. The fundamental premise of SAM lies in its capability to segment or identify objects in images without prior knowledge of the object type or imaging modality. This approach aligns well with tasks achievable by the human visual system, though its application in non-biological vision contexts remains more theoretically challenging. A notable feature of SAM is its ability to adjust segmentation according to a specified resolution scale or area of interest, akin to semantic priming. This adaptability has spurred a wave of creativity and innovation in applying SAM to medical imaging. Our review focuses on the period from April 1, 2023, to September 30, 2023, a critical first six months post-initial publication. We examine the adaptations and integrations of SAM necessary to address longstanding clinical challenges, particularly in the context of 33 open datasets covered in our analysis. While SAM approaches or achieves state-of-the-art performance in numerous applications, it falls short in certain areas, such as segmentation of the carotid artery, adrenal glands, optic nerve, and mandible bone. Our survey delves into the innovative techniques where SAM's foundational approach excels and explores the core concepts in translating and applying these models effectively in diverse medical imaging scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268317049",
                    "name": "Ho Hin Lee"
                },
                {
                    "authorId": "2260374573",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2220964895",
                    "name": "Theodore Zhao"
                },
                {
                    "authorId": "2279740020",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2279705714",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "2239208043",
                    "name": "Bennett A. Landman"
                },
                {
                    "authorId": "34430081",
                    "name": "Yuankai Huo"
                },
                {
                    "authorId": "2264626868",
                    "name": "Alberto Santamar\u00eda-Pang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "0413a92b91e2dbcbfaa042d0519f3d919bfe57e7",
            "title": "Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology",
            "abstract": "Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2112677245",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "7016395",
                    "name": "C. Moung"
                },
                {
                    "authorId": "2228760409",
                    "name": "Jacob Abel"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "6150927",
                    "name": "R. Weerasinghe"
                },
                {
                    "authorId": "35695043",
                    "name": "B. Piening"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "46392172",
                    "name": "C. Bifulco"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "38755d2468e85194b5cf9b017d035080f7a2890f",
            "title": "Enhancing Medical Text Evaluation with GPT-4",
            "abstract": "In the evaluation of medical text generation, it is essential to scrutinize each piece of information and ensure the utmost accuracy of the evaluation. Existing evaluation metrics either focus on coarse-level evaluation that assigns one score for the whole generated output or rely on evaluation models trained on general domain, resulting in inaccuracies when adapted to the medical domain. To address these issues, we propose a set of factuality-centric evaluation aspects and design corresponding GPT-4-based metrics for medical text generation. We systematically compare these metrics with existing ones on clinical note generation and medical report summarization tasks, revealing low inter-metric correlation. A comprehensive human evaluation confirms that the proposed GPT-4-based metrics exhibit substantially higher agreement with human judgments than existing evaluation metrics. Our study contributes to the understanding of medical text generation evaluation and offers a more reliable alternative to existing metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267021210",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2267010012",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2313045584",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "1395101702",
                    "name": "Zelalem Gero"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "5814bd146b37e13115af4330caf3a751159a156f",
            "title": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs",
            "abstract": "Biomedical data is inherently multimodal, comprising physical measurements and natural language narratives. A generalist biomedical AI model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image-text pairs. Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image-text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision-language processing. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering (VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of standard datasets, substantially outperforming prior approaches. Intriguingly, by large-scale pretraining on diverse biomedical image types, BiomedCLIP even outperforms state-of-the-art radiology-specific models such as BioViL in radiology-specific tasks such as RSNA pneumonia detection. In summary, BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications. We release our models at https://aka.ms/biomedclip to facilitate future research in multimodal biomedical AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2143558044",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "6157638",
                    "name": "J. Bagga"
                },
                {
                    "authorId": "1846722967",
                    "name": "Robert Tinn"
                },
                {
                    "authorId": "2159543147",
                    "name": "Sam Preston"
                },
                {
                    "authorId": "35794791",
                    "name": "Rajesh N. Rao"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "48269128",
                    "name": "Naveen Valluri"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "4204731",
                    "name": "M. Lungren"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "6c12769939dd75bd681d37ea17cce7e6a57f5c6e",
            "title": "Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events",
            "abstract": "Large language models (LLMs), such as GPT-4, have demonstrated remarkable capabilities across a wide range of tasks, including health applications. In this paper, we study how LLMs can be used to scale biomedical knowledge curation. We find that while LLMs already possess decent competency in structuring biomedical text, by distillation into a task-specific student model through self-supervised learning, substantial gains can be attained over out-of-box LLMs, with additional advantages such as cost, efficiency, and white-box model access. We conduct a case study on adverse drug event (ADE) extraction, which is an important area for improving care. On standard ADE extraction evaluation, a GPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised state-of-the-art models without using any labeled data. Despite being over 1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by over 6 absolute points in F1 and GPT-4 by over 5 absolute points. Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT) and ADE extraction architecture shed light on best practice for biomedical knowledge extraction. Similar gains were attained by distillation for other standard biomedical knowledge extraction tasks such as gene-disease associations and protected health information, further illustrating the promise of this approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112677245",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "3310870",
                    "name": "Yonas G. Woldesenbet"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2223542076",
                    "name": "Praneeth Sanapathi"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "48269128",
                    "name": "Naveen Valluri"
                },
                {
                    "authorId": "35009788",
                    "name": "Erika Strandberg"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "91664510519b562d17ad5e3cde3a282b92a77fac",
            "title": "DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation",
            "abstract": "Medical text generation aims to assist with administrative work and highlight salient information to support decision-making. To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness, conciseness, and attribution of the generated text at a fine-grained level. The metrics can be computed by various types of evaluators including instruction-following (both proprietary and open-source) and supervised entailment models. We demonstrate the effectiveness of the resulting framework, DocLens, with three evaluators on three tasks: clinical note generation, radiology report summarization, and patient question summarization. A comprehensive human study shows that DocLens exhibits substantially higher agreement with the judgments of medical experts than existing metrics. The results also highlight the need to improve open-source evaluators and suggest potential directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267021210",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "1395101702",
                    "name": "Zelalem Gero"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "d004c6312ecabf4ec621f2cda9a28d0f11692cc0",
            "title": "TRIALSCOPE: A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models",
            "abstract": "The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million cancer patients from a large US healthcare network, we show that TRIALSCOPE can produce high-quality structuring of real-world data and generates comparable results to marquee cancer trials. In addition to facilitating in-silicon clinical trial design and optimization, TRIALSCOPE may be used to empower synthetic controls, pragmatic trials, post-market surveillance, as well as support fine-grained patient-like-me reasoning in precision diagnosis and treatment.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2264999269",
                    "name": "Javier Gonz'alez"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "1395101702",
                    "name": "Zelalem Gero"
                },
                {
                    "authorId": "2264962264",
                    "name": "Jass Bagga"
                },
                {
                    "authorId": "2264166620",
                    "name": "Risa Ueno"
                },
                {
                    "authorId": "2264694280",
                    "name": "Isabel Chien"
                },
                {
                    "authorId": "2264960547",
                    "name": "Eduard Orakvin"
                },
                {
                    "authorId": "2264962872",
                    "name": "Emre Kiciman"
                },
                {
                    "authorId": "2264522015",
                    "name": "Aditya V. Nori"
                },
                {
                    "authorId": "6150927",
                    "name": "R. Weerasinghe"
                },
                {
                    "authorId": "6097546",
                    "name": "R. Leidner"
                },
                {
                    "authorId": "2264514939",
                    "name": "B. Piening"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "46392172",
                    "name": "C. Bifulco"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "f22d71c7ce9720ba1f717a4f1181488200e78198",
            "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
            "abstract": "Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109737569",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "2143856368",
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "120157163",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        }
    ]
}