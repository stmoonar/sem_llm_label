{
    "authorId": "2912732",
    "papers": [
        {
            "paperId": "74804cc714cae570673e4b15b6e5b8f2b32245ae",
            "title": "Integrity 2024: Integrity in Social Networks and Media",
            "abstract": "Integrity 2024 is the fifth edition of the Workshop on Integrity in Social Networks and Media, held in conjunction with the ACM Conference on Web Search and Data Mining (WSDM) since the 2020 edition [1-4]. The goal of the workshop is to bring together academic and industry researchers working on integrity, fairness, trust and safety in social networks to discuss the most pressing risks and cutting-edge technologies to reliably measure and mitigate them. The event consists of invited talks from academic experts and industry leaders as well as peer-reviewed papers and posters through an open call-for-papers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2209212295",
                    "name": "Llu\u00eds Garcia-Pueyo"
                },
                {
                    "authorId": "2276779989",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "2209214648",
                    "name": "Prathyusha Senthil Kumar"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1701195",
                    "name": "Panayiotis Tsaparas"
                },
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                },
                {
                    "authorId": "2313479443",
                    "name": "Giuseppe Manco"
                },
                {
                    "authorId": "2290565901",
                    "name": "Anton Andryeyev"
                },
                {
                    "authorId": "40598011",
                    "name": "S. Cresci"
                },
                {
                    "authorId": "2239640744",
                    "name": "T. Sellis"
                },
                {
                    "authorId": "144000523",
                    "name": "Anthony McCosker"
                }
            ]
        },
        {
            "paperId": "ae6b3e5fbcecd0241b9627d3a338dbe29ffc1c54",
            "title": "Detecting and Limiting Negative User Experiences in Social Media Platforms",
            "abstract": "Item ranking is important to a social media platform\u2019s success. The order in which posts, videos, messages, comments, ads, used products, notifications are presented to a user greatly affects the time spent on the platform, how often they visit it, how much they interact with each other, and the quantity and quality of the content they post. To this end, item ranking algorithms use models that predict the likelihood of different events, e.g., the user liking, sharing, commenting on a video, clicking/converting on an ad, or opening the platform\u2019s app from a notification. Unfortunately, by solely relying on such event-prediction models, social media platforms tend to over optimize for short-term objectives and ignore the long-term effects. In this paper, we propose an approach that aims at improving item ranking long-term impact. The approach primarily relies on an ML model that predicts negative user experiences. The model utilizes all available UI events: the details of an action can reveal how positive or negative the user experience has been; for example, a user writing a lengthy report asking for a given video to be taken down, likely had a very negative experience. Furthermore, the model takes into account detected integrity (e.g., hostile speech or graphic violence) and quality (e.g., click or engagement bait) issues with the content. Note that those issues can be perceived very differently from different users. Therefore, developing a personalized model, where a prediction refers to a specific user for a specific piece of content at a specific point in time, is a fundamental design choice in our approach. Besides the personalized ML model, our approach consists of two more pieces: (a) the way the personalized model is integrated with an item ranking algorithm and (b) the metrics, methodology, and success criteria for the long term impact of detecting and limiting negative user experiences. Our evaluation process uses extensive A/B testing on the Facebook platform: we compare the impact of our approach in treatment groups against production control groups. The AB test results indicate a 5% to 50% reduction in hides, reports, and submitted feedback. Furthermore, we compare against a baseline that does not include some of the crucial elements of our approach: the comparison shows our approach has a 100x to 30x lower False Positive Ratio than a baseline. Lastly, we present the results from a large scale survey, where we observe a statistically significant improvement of 3 to 6 percent in users\u2019 sentiment regarding content suffering from nudity, clickbait, false / misleading, witnessing-hate, and violence issues.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2209212295",
                    "name": "Llu\u00eds Garcia-Pueyo"
                },
                {
                    "authorId": "2215457688",
                    "name": "Vinodh Kumar Sunkara"
                },
                {
                    "authorId": "2209214648",
                    "name": "Prathyusha Senthil Kumar"
                },
                {
                    "authorId": "2215458825",
                    "name": "Mohit Diwan"
                },
                {
                    "authorId": "2055641179",
                    "name": "Qian Ge"
                },
                {
                    "authorId": "2215457669",
                    "name": "Behrang Javaherian"
                },
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                }
            ]
        },
        {
            "paperId": "3f53f9ebdc0b775a5e9868c328fe97d074c9b97e",
            "title": "Top-K Entity Resolution with Adaptive Locality-Sensitive Hashing",
            "abstract": "Given a set of records, entity resolution algorithms find all the records referring to each entity. In top-k entity resolution, the goal is to find all the records referring to the k largest (in terms of number of records) entities. Top-k entity resolution is driven by many modern applications that operate over just the few most popular entities in a dataset. In this paper we introduce the problem of top-k entity resolution and we summarize a novel approach for this problem; full details are presented in a technical report. Our approach is based on locality-sensitive hashing, and can very rapidly and accurately process massive datasets. Our key insight is to adaptively decide how much processing each record requires to ascertain if it refers to a top-k entity or not: the less likely a record is to refer to a top-k entity, the less it is processed. The heavily reduced amount of processing for the vast majority of records that do not refer to top-k entities, leads to significant speedups. Our experiments with images, web articles, and scientific publications show a 2x to 25x speedup compared to traditional approaches for high-dimensional data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                },
                {
                    "authorId": "1398574232",
                    "name": "H. Garcia-Molina"
                }
            ]
        },
        {
            "paperId": "49bd19008bc792f259e91abe1be550d976ffd95a",
            "title": "Live VM Migration Under Time-Constraints in Share-Nothing IaaS-Clouds",
            "abstract": "Live VM migration helps attain both cloud-wide load balancing and operational consolidation while the migrating VMs remain accessible to users. To avoid periods of high-load for the involved resources, IaaS-cloud operators assign specific time windows for such migrations to occur in an orderly manner. Moreover, providers typically rely on share-nothing architectures to attain scalability. In this paper, we focus on the real-time scheduling of live VM migrations in large share-nothing IaaS  clouds, such that migrations are completed on time and without adversely affecting agreed-upon SLAs. We propose a scalable, distributed network of brokers that oversees the progress of all on-going migration operations within the context of a provider. Brokers make use of an underlying special purpose file system, termed  MigrateFS, that is capable of both replicating and keeping in sync virtual disks while the hypervisor live-migrates VMs (i.e., RAM and CPU state). By limiting the resources consumed during migration, brokers implement policies to reduce SLA violations while seeking to complete all migration tasks on time. We evaluate two such policies, one based on task prioritization and a second that considers the financial implications set by migration deadline requirements. Using our MigrateFS prototype operating on a real cloud, we demonstrate the feasibility of performing migrations within time windows. By simulating large clouds, we assess the effectiveness of our proposed broker policies in a share-nothing configuration; we also demonstrate that our approach stresses 24 percent less an already saturated network if compared to an unsupervised set up.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118644",
                    "name": "Konstantinos Tsakalozos"
                },
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                },
                {
                    "authorId": "1703382",
                    "name": "M. Roussopoulos"
                },
                {
                    "authorId": "1774068",
                    "name": "A. Delis"
                }
            ]
        },
        {
            "paperId": "c72b6b9ea63cfcfbd158379e0ddc1d292a8a7864",
            "title": "Waldo: An Adaptive Human Interface for Crowd Entity Resolution",
            "abstract": "In Entity Resolution, the objective is to find which records of a dataset refer to the same real-world entity. Crowd Entity Resolution uses humans, in addition to machine algorithms, to improve the quality of the outcome. We study a hybrid approach that combines two common interfaces for human tasks in Crowd Entity Resolution, taking into account key observations about the advantages and disadvantages of the two interfaces. We give a formal definition to the problem of human task selection and we derive algorithms with strong optimality guarantees. Our experiments with four real-world datasets show that our hybrid approach gives an improvement of 50% to 300% in the crowd cost to resolve a dataset, compared to using a single interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                },
                {
                    "authorId": "1398574232",
                    "name": "H. Garcia-Molina"
                },
                {
                    "authorId": "1786049",
                    "name": "Y. Papakonstantinou"
                }
            ]
        },
        {
            "paperId": "20a35249a3c1d462c2e7121d5b566b6b7554ad19",
            "title": "Challenges in Data Crowdsourcing",
            "abstract": "Crowdsourcing refers to solving large problems by involving human workers that solve component sub-problems or tasks. In data crowdsourcing, the problem involves data acquisition, management, and analysis. In this paper, we provide an overview of data crowdsourcing, giving examples of problems that the authors have tackled, and presenting the key design steps involved in implementing a crowdsourced solution. We also discuss some of the open challenges that remain to be solved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398574232",
                    "name": "H. Garcia-Molina"
                },
                {
                    "authorId": "2185778",
                    "name": "Manas R. Joglekar"
                },
                {
                    "authorId": "145632757",
                    "name": "Adam Marcus"
                },
                {
                    "authorId": "145592539",
                    "name": "Aditya G. Parameswaran"
                },
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                }
            ]
        },
        {
            "paperId": "4b14da9a804c4fc2c13a19d131837ee3fa8d8ba6",
            "title": "tDP: An Optimal-Latency Budget Allocation Strategy for Crowdsourced MAXIMUM Operations",
            "abstract": "Latency is a critical factor when using a crowdsourcing platform to solve a problem like entity resolution or sorting. In practice, most frameworks attempt to reduce latency by heuristically splitting a budget of questions into rounds, so that after each round the answers are analyzed and new questions are selected. We focus on one of the most extensively studied crowdsourcing operations, the MAX operation (finding the best element in a collection under human criteria), and we study the problem of budget allocation into rounds for this operation. We provide a polynomial-time dynamic-programming budget allocation algorithm that minimizes the latency when questions form tournaments in each round. Furthermore, we study the general case where questions can be asked in any arbitrary way in each round. Our theoretical results for the general case indicate that our approach is also optimal under certain worst and average-case scenarios. We compare our approach to alternatives on Amazon Mechanical Turk, where many of our theory assumptions do not necessarily hold. We find that our approach is also optimal in practice and achieves a notable improvement over alternatives in most cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                },
                {
                    "authorId": "2933643",
                    "name": "P. Lofgren"
                },
                {
                    "authorId": "1398574232",
                    "name": "H. Garcia-Molina"
                }
            ]
        },
        {
            "paperId": "8131475c2e65775f106bc993930b942bcdc1de9d",
            "title": "Entity Resolution with crowd errors",
            "abstract": "Given a set of records, an Entity Resolution (ER) algorithm finds records that refer to the same real-world entity. Humans can often determine if two records refer to the same entity, and hence we study the problem of selecting questions to ask error-prone humans. We give a Maximum Likelihood formulation for the problem of finding the \u201cmost beneficial\u201d questions to ask next. Our theoretical results lead to a lightweight and practical algorithm, bDENSE, for selecting questions to ask humans. Our experimental results show that bDENSE can more quickly reach an accurate outcome, compared to two approaches proposed recently. Moreover, through our experimental evaluation, we identify the strengths and weaknesses of all three approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                },
                {
                    "authorId": "1398574232",
                    "name": "H. Garcia-Molina"
                }
            ]
        },
        {
            "paperId": "f99f42ce26578096b228e9b0f56d542589a671a7",
            "title": "Client Clustering for Hiring Modeling in Work Marketplaces",
            "abstract": "An important problem that online work marketplaces face is grouping clients into clusters, so that in each cluster clients are similar with respect to their hiring criteria. Such a separation allows the marketplace to \"learn\" more accurately the hiring criteria in each cluster and recommend the right contractor to each client, for a successful collaboration. We propose a Maximum Likelihood definition of the \"optimal\" client clustering along with an efficient Expectation-Maximization clustering algorithm that can be applied in large marketplaces. Our results on the job hirings at oDesk over a seven-month period show that our client-clustering approach yields significant gains compared to \"learning\" the same hiring criteria for all clients. In addition, we analyze the clustering results to find interesting differences between the hiring criteria in the different groups of clients.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                },
                {
                    "authorId": "1724134",
                    "name": "Panagiotis Papadimitriou"
                },
                {
                    "authorId": "1790944",
                    "name": "Ramesh Johari"
                },
                {
                    "authorId": "1398574232",
                    "name": "H. Garcia-Molina"
                }
            ]
        },
        {
            "paperId": "302bfb5c5063195e5691b16d37a2c3ffece9e532",
            "title": "Context Trees: Crowdsourcing Global Understanding from Local Views",
            "abstract": "\n \n Crowdsourcing struggles when workers must see all of the pieces of input to make an accurate judgment. For example, to find the most important scenes in a novel or movie, each worker must spend hours consuming the entire plot to acquire a global understanding and then apply that understanding to each local scene. To enable the crowdsourcing of large-scale goals with only local views, we introduce context trees, a crowdsourcing workflow for creating global summaries of a large input. Context trees recursively combine elements through written summaries to form a tree. Workers can then ground their local decisions by applying those summaries back down to the leaf nodes. In the case of scale ratings such as scene importance, we introduce a weighting process that percolates ratings downwards through the tree so that important nodes in unimportant branches are not overweighted. When using context trees to rate the importance of scenes in a 4000-word story and a 100-minute movie, workers\u2019 ratings are nearly as accurate as those who saw the entire input, and much improved over the traditional approach of splitting the input into independent segments. To explore whether context trees enable crowdsourcing to undertake new classes of goals, we also crowdsource the solution to a large hierarchical puzzle of 462,000 interlocking pieces.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2912732",
                    "name": "Vasilis Verroios"
                },
                {
                    "authorId": "145879842",
                    "name": "Michael S. Bernstein"
                }
            ]
        }
    ]
}