{
    "authorId": "145638130",
    "papers": [
        {
            "paperId": "25830ec7e05b3350a6347f8f3c5d94feb143a350",
            "title": "Continual Learning: A Review of Techniques, Challenges, and Future Directions",
            "abstract": "Continual learning (CL), or the ability to acquire, process, and learn from new information without forgetting acquired knowledge, is a fundamental quality of an intelligent agent. The human brain has evolved into gracefully dealing with ever-changing circumstances and learning from experience with the help of complex neurophysiological mechanisms. Even though artificial intelligence takes after human intelligence, traditional neural networks do not possess the ability to adapt to dynamic environments. When presented with new information, an artificial neural network (ANN) often completely forgets its prior knowledge, a phenomenon called catastrophic forgetting or catastrophic interference. Incorporating CL capabilities into ANNs is an active field of research and is integral to achieving artificial general intelligence. In this review, we revisit CL approaches and critically examine their strengths and limitations. We conclude that CL approaches should look beyond mitigating catastrophic forgetting and strive for systems that can learn, store, recall, and transfer knowledge, much like the human brain. To this end, we highlight the importance of adopting alternative brain-inspired data representations and learning algorithms and provide our perspective on promising new directions where CL could play an instrumental role.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267292460",
                    "name": "Buddhi Wickramasinghe"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2268786741",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "6bae288e831c26e0fa8724578558daa2cd0f4780",
            "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
            "abstract": "Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059786774",
                    "name": "Utkarsh Saxena"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "2268786741",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "acb34debf5a43ac47f83a2b5fbd6f9579ce2a5ac",
            "title": "Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples",
            "abstract": "Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness on both synthetic and real-world label noise. Experiments on the CIFAR dataset with 25% synthetic corruption show 7.36% generalization improvements on average. Additionally, we observe generalization improvements of up to 2.63% on naturally corrupted datasets like WebVision1.0 and Clothing1M.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "20708889",
                    "name": "Sangamesh Kodge"
                },
                {
                    "authorId": "2064867801",
                    "name": "Deepak Ravikumar"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2268786741",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "14fc27f151a12e141e8cc84da050e64b8d90eb56",
            "title": "CoDeC: Communication-Efficient Decentralized Continual Learning",
            "abstract": "Training at the edge utilizes continuously evolving data generated at different locations. Privacy concerns prohibit the co-location of this spatially as well as temporally distributed data, deeming it crucial to design training algorithms that enable efficient continual learning over decentralized private data. Decentralized learning allows serverless training with spatially distributed data. A fundamental barrier in such distributed learning is the high bandwidth cost of communicating model updates between agents. Moreover, existing works under this training paradigm are not inherently suitable for learning a temporal sequence of tasks while retaining the previously acquired knowledge. In this work, we propose CoDeC, a novel communication-efficient decentralized continual learning algorithm which addresses these challenges. We mitigate catastrophic forgetting while learning a task sequence in a decentralized learning setup by combining orthogonal gradient projection with gossip averaging across decentralized agents. Further, CoDeC includes a novel lossless communication compression scheme based on the gradient subspaces. We express layer-wise gradients as a linear combination of the basis vectors of these gradient subspaces and communicate the associated coefficients. We theoretically analyze the convergence rate for our algorithm and demonstrate through an extensive set of experiments that CoDeC successfully learns distributed continual tasks with minimal forgetting. The proposed compression scheme results in up to 4.8x reduction in communication costs with iso-performance as the full communication baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "6063e55be14840dede97a56e4f41062bf094548d",
            "title": "Deep Unlearning: Fast and Efficient Gradient-free Class Forgetting",
            "abstract": "Machine unlearning is a prominent and challenging field, driven by regulatory demands for user data deletion and heightened privacy awareness. Existing approaches involve retraining model or multiple finetuning steps for each deletion request, often constrained by computational limits and restricted data access. In this work, we introduce a novel class unlearning algorithm designed to strategically eliminate specific classes from the learned model. Our algorithm first estimates the Retain and the Forget Spaces using Singular Value Decomposition on the layerwise activations for a small subset of samples from the retain and unlearn classes, respectively. We then compute the shared information between these spaces and remove it from the forget space to isolate class-discriminatory feature space. Finally, we obtain the unlearned model by updating the weights to suppress the class discriminatory features from the activation spaces. We demonstrate our algorithm's efficacy on ImageNet using a Vision Transformer with only $\\sim 1.5\\%$ drop in retain accuracy compared to the original model while maintaining under $1\\%$ accuracy on the unlearned class samples. Furthermore, our algorithm exhibits competitive unlearning performance and resilience against Membership Inference Attacks (MIA). Compared to baselines, it achieves an average accuracy improvement of $1.38\\%$ on the ImageNet dataset while requiring up to $10 \\times$ fewer samples for unlearning. Additionally, under stronger MIA attacks on the CIFAR-100 dataset using a ResNet18 architecture, our approach outperforms the best baseline by $1.8\\%$. Our code is available at https://github.com/sangamesh-kodge/class_forgetting.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "20708889",
                    "name": "Sangamesh Kodge"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2268786741",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "8023869b534ae182e3e7d5df25690a062106912d",
            "title": "Continual Learning with Scaled Gradient Projection",
            "abstract": "In neural networks, continual learning results in gradient interference among sequential tasks, leading to catastrophic forgetting of old tasks while learning new ones. This issue is addressed in recent methods by storing the important gradient spaces for old tasks and updating the model orthogonally during new tasks. However, such restrictive orthogonal gradient updates hamper the learning capability of the new tasks resulting in sub-optimal performance. To improve new learning while minimizing forgetting, in this paper we propose a Scaled Gradient Projection (SGP) method, where we combine the orthogonal gradient projections with scaled gradient steps along the important gradient spaces for the past tasks. The degree of gradient scaling along these spaces depends on the importance of the bases spanning them. We propose an efficient method for computing and accumulating importance of these bases using the singular value decomposition of the input representations for each task. We conduct extensive experiments ranging from continual image classification to reinforcement learning tasks and report better performance with less training overhead than the state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "b5c834d985aa3193e37b5a71b4d97b6bc8ae4f99",
            "title": "Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning",
            "abstract": "Decentralized learning enables serverless training of deep neural networks (DNNs) in a distributed manner on multiple nodes. This allows for the use of large datasets, as well as the ability to train with a wide variety of data sources. However, one of the key challenges with decentralized learning is heterogeneity in the data distribution across the nodes. In this paper, we propose In-Distribution Knowledge Distillation (IDKD) to address the challenge of heterogeneous data distribution. The goal of IDKD is to homogenize the data distribution across the nodes. While such data homogenization can be achieved by exchanging data among the nodes sacrificing privacy, IDKD achieves the same objective using a common public dataset across nodes without breaking the privacy constraint. This public dataset is different from the training dataset and is used to distill the knowledge from each node and communicate it to its neighbors through the generated labels. With traditional knowledge distillation, the generalization of the distilled model is reduced because all the public dataset samples are used irrespective of their similarity to the local dataset. Thus, we introduce an Out-of-Distribution (OoD) detector at each node to label a subset of the public dataset that maps close to the local training data distribution. Finally, only labels corresponding to these subsets are exchanged among the nodes and with appropriate label averaging each node is finetuned on these data subsets along with its local data. Our experiments on multiple image classification datasets and graph topologies show that the proposed IDKD scheme is more effective than traditional knowledge distillation and achieves state-of-the-art generalization performance on heterogeneously distributed data with minimal communication overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064867801",
                    "name": "Deepak Ravikumar"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "0b6d4e57387903ae1a36319969a6615ab8959db5",
            "title": "A cross-layer approach to cognitive computing: invited",
            "abstract": "Remarkable advances in machine learning and artificial intelligence have been made in various domains, achieving near-human performance in a plethora of cognitive tasks including vision, speech and natural language processing. However, implementations of such cognitive algorithms in conventional \"von-Neumann\" architectures are orders of magnitude more area and power expensive than the biological brain. Therefore, it is imperative to search for fundamentally new approaches so that the improvement in computing performance and efficiency can keep up with the exponential growth of the AI computational demand. In this article, we present a cross-layer approach to the exploration of new paradigms in cognitive computing. This effort spans new learning algorithms inspired from biological information processing principles, network architectures best suited for such algorithms, and neuromorphic hardware substrates such as computing-in-memory fabrics in order to build intelligent machines that can achieve orders of improvement in energy efficiency at cognitive processing. We argue that such cross-layer innovations in cognitive computing are well-poised to enable a new wave of autonomous intelligence across the computing spectrum, from resource-constrained IoT devices to the cloud.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2119128751",
                    "name": "Cheng Wang"
                },
                {
                    "authorId": "145291370",
                    "name": "A. Raghunathan"
                },
                {
                    "authorId": "2257216834",
                    "name": "K. Roy"
                }
            ]
        },
        {
            "paperId": "3620a90f314b16ae7fff8f07433e8c6954544f2d",
            "title": "Synthetic Dataset Generation for Privacy-Preserving Machine Learning",
            "abstract": "Machine Learning (ML) has achieved enormous success in solving a variety of problems in computer vision, speech recognition, object detection, to name a few. The principal reason for this success is the availability of huge datasets for training deep neural networks (DNNs). However, datasets can not be publicly released if they contain sensitive information such as medical or financial records. In such cases, data privacy becomes a major concern. Encryption methods offer a possible solution to this issue, however their deployment on ML applications is non-trivial, as they seriously impact the classification accuracy and result in substantial computational overhead.Alternatively, obfuscation techniques can be used, but maintaining a good balance between visual privacy and accuracy is challenging. In this work, we propose a method to generate secure synthetic datasets from the original private datasets. In our method, given a network with Batch Normalization (BN) layers pre-trained on the original dataset, we first record the layer-wise BN statistics. Next, using the BN statistics and the pre-trained model, we generate the synthetic dataset by optimizing random noises such that the synthetic data match the layer-wise statistical distribution of the original model. We evaluate our method on image classification dataset (CIFAR10) and show that our synthetic data can be used for training networks from scratch, producing reasonable classification performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "117455765",
                    "name": "Efstathia Soufleri"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "dd41d2d8fd2d3ea2c6ec74d07b456db942bfa8de",
            "title": "Gradient Projection Memory for Continual Learning",
            "abstract": "The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "150134055",
                    "name": "Isha Garg"
                },
                {
                    "authorId": "2257216834",
                    "name": "K. Roy"
                }
            ]
        }
    ]
}