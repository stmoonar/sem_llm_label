{
    "authorId": "1452678825",
    "papers": [
        {
            "paperId": "2ba3607e71165002ce5057b2afbc25c4211f67c2",
            "title": "An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance",
            "abstract": "Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset \u2013 (i) concept : comprising 600 images that are cross-culturally coherent, focusing on a single concept per image; and (ii) application : comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "2140457932",
                    "name": "Sathyanarayanan Ramamoorthy"
                },
                {
                    "authorId": "2290031618",
                    "name": "Yueqi Song"
                },
                {
                    "authorId": "2285194103",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "4a02d2fef5b94a4da8346f1fa88ae5f9294f886b",
            "title": "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance",
            "abstract": "Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: https://github.com/simran-khanuja/image-transcreation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "2140457932",
                    "name": "Sathyanarayanan Ramamoorthy"
                },
                {
                    "authorId": "2290031618",
                    "name": "Yueqi Song"
                },
                {
                    "authorId": "2285194103",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "c909718ec002e05e4ac77da2bfbec5ac88f18c3e",
            "title": "What Is Missing in Multilingual Visual Reasoning and How to Fix It",
            "abstract": "NLP models today strive for supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, multimodal capabilities by testing on a visual reasoning task. We observe that proprietary systems like GPT-4V obtain the best performance on this task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex reasoning, and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex reasoning, and a novel method that leverages image captioning to address multimodality. Our interventions achieve the best open performance on this task in a zero-shot setting, boosting open model LLaVA by 13.4%, while also minorly improving GPT-4V's performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290031618",
                    "name": "Yueqi Song"
                },
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "2285194103",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "17605c43ca3eb982c99642052ddc21a93d116594",
            "title": "GlobalBench: A Benchmark for Global Progress in Natural Language Processing",
            "abstract": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "148310739",
                    "name": "Yueqi Song"
                },
                {
                    "authorId": "2218206121",
                    "name": "Catherine Cui"
                },
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "144118452",
                    "name": "Pengfei Liu"
                },
                {
                    "authorId": "48556979",
                    "name": "FAHIM FAISAL"
                },
                {
                    "authorId": "1475670743",
                    "name": "Alissa Ostapenko"
                },
                {
                    "authorId": "9162688",
                    "name": "Genta Indra Winata"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "49513989",
                    "name": "Antonios Anastasopoulos"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "d5dd7230cccace7e77095d3b5fd8394850f59170",
            "title": "Multi-lingual and Multi-cultural Figurative Language Understanding",
            "abstract": "Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \\datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1735001746",
                    "name": "Anubha Kabra"
                },
                {
                    "authorId": "1381444447",
                    "name": "Emmy Liu"
                },
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "9162688",
                    "name": "Genta Indra Winata"
                },
                {
                    "authorId": "2220548276",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "2056773747",
                    "name": "Anuoluwapo Aremu"
                },
                {
                    "authorId": "1988654955",
                    "name": "Perez Ogayo"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "eeb6601a3c557e970817ed056265a6c62e5d09dc",
            "title": "DeMuX: Data-efficient Multilingual Learning",
            "abstract": "Pre-trained multilingual models have enabled deployment of NLP technologies for multiple languages. However, optimally fine-tuning these models under an annotation budget, such that performance on desired target languages is jointly maximized, still remains an open question. In this paper, we introduce DeMuX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points. Our code is released here: https://github.com/simran-khanuja/demux.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "2220844123",
                    "name": "Srinivas Gowriraj"
                },
                {
                    "authorId": "32273391",
                    "name": "L. Dery"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "1b19977b65685085f7c48efc9db7c52772230509",
            "title": "XTREME-S: Evaluating Cross-lingual Speech Representations",
            "abstract": "We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual speech representations in many languages. XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering 102 languages from 10+ language families, 3 different domains and 4 task families, XTREME-S aims to simplify multilingual speech representation evaluation, as well as catalyze research in\"universal\"speech representation learning. This paper describes the new benchmark and establishes the first speech-only and speech-text baselines using XLS-R and mSLAM on all downstream tasks. We motivate the design choices and detail how to use the benchmark. Datasets and fine-tuning scripts are made easily accessible at https://hf.co/datasets/google/xtreme_s.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2480903",
                    "name": "Alexis Conneau"
                },
                {
                    "authorId": "12295226",
                    "name": "Ankur Bapna"
                },
                {
                    "authorId": "2153632494",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2088199087",
                    "name": "Min Ma"
                },
                {
                    "authorId": "138609838",
                    "name": "Patrick von Platen"
                },
                {
                    "authorId": "2159547851",
                    "name": "Anton Lozhkov"
                },
                {
                    "authorId": "144507724",
                    "name": "Colin Cherry"
                },
                {
                    "authorId": "1691944",
                    "name": "Ye Jia"
                },
                {
                    "authorId": "2059157845",
                    "name": "Clara Rivera"
                },
                {
                    "authorId": "26688118",
                    "name": "Mihir Kale"
                },
                {
                    "authorId": "8775666",
                    "name": "D. Esch"
                },
                {
                    "authorId": "82840075",
                    "name": "Vera Axelrod"
                },
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "144797264",
                    "name": "J. Clark"
                },
                {
                    "authorId": "2345617",
                    "name": "Orhan Firat"
                },
                {
                    "authorId": "2325985",
                    "name": "Michael Auli"
                },
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "2909504",
                    "name": "Jason Riesa"
                },
                {
                    "authorId": "145657834",
                    "name": "Melvin Johnson"
                }
            ]
        },
        {
            "paperId": "1ce67b5555c123ae1efb710965567f51bf764423",
            "title": "mSLAM: Massively multilingual joint pre-training for speech and text",
            "abstract": "We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12295226",
                    "name": "Ankur Bapna"
                },
                {
                    "authorId": "144507724",
                    "name": "Colin Cherry"
                },
                {
                    "authorId": "2153632494",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "1691944",
                    "name": "Ye Jia"
                },
                {
                    "authorId": "145657834",
                    "name": "Melvin Johnson"
                },
                {
                    "authorId": "2109716647",
                    "name": "Yong Cheng"
                },
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "2909504",
                    "name": "Jason Riesa"
                },
                {
                    "authorId": "2480903",
                    "name": "Alexis Conneau"
                }
            ]
        },
        {
            "paperId": "497d5e7861e5e89ab599f8936ccdae10162776d8",
            "title": "Evaluating the Diversity, Equity, and Inclusion of NLP Technology: A Case Study for Indian Languages",
            "abstract": "In order for NLP technology to be widely applicable, fair, and useful, it needs to serve a diverse set of speakers across the world\u2019s languages, be equitable, i.e., not unduly biased towards any particular language, and be inclusive of all users, particularly in low-resource settings where compute constraints are common. In this paper, we propose an evaluation paradigm that assesses NLP technologies across all three dimensions. While diversity and inclusion have received attention in recent literature, equity is currently unexplored. We propose to address this gap using the Gini coefficient, a well-established metric used for estimating societal wealth inequality. Using our paradigm, we highlight the distressed state of current technologies for Indian (IN) languages (a linguistically large and diverse set, with a varied speaker population), across all three dimensions. To improve upon these metrics, we demonstrate the importance of region-specific choices in model building and dataset creation, and more importantly, propose a novel, generalisable approach to optimal resource allocation during fine-tuning. Finally, we discuss steps to mitigate these biases and encourage the community to employ multi-faceted evaluation when building linguistically diverse and equitable technologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "2406435",
                    "name": "Partha P. Talukdar"
                }
            ]
        },
        {
            "paperId": "7c73fe9266db0b35f551645ca37c6e7ce0980c4b",
            "title": "FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech",
            "abstract": "We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Speech-Text Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like speech-only w2v-BERT [1] and speech-text multimodal mSLAM [2]. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding.1.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2480903",
                    "name": "Alexis Conneau"
                },
                {
                    "authorId": "2088199087",
                    "name": "Min Ma"
                },
                {
                    "authorId": "1452678825",
                    "name": "Simran Khanuja"
                },
                {
                    "authorId": "2153632494",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "82840075",
                    "name": "Vera Axelrod"
                },
                {
                    "authorId": "35186886",
                    "name": "Siddharth Dalmia"
                },
                {
                    "authorId": "2909504",
                    "name": "Jason Riesa"
                },
                {
                    "authorId": "2059157845",
                    "name": "Clara Rivera"
                },
                {
                    "authorId": "12295226",
                    "name": "Ankur Bapna"
                }
            ]
        }
    ]
}