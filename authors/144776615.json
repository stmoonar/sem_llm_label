{
    "authorId": "144776615",
    "papers": [
        {
            "paperId": "5476caad3df79df4dbde029c5b0a69b14c5b5ca3",
            "title": "Centralized Selection with Preferences in the Presence of Biases",
            "abstract": "This paper considers the scenario in which there are multiple institutions, each with a limited capacity for candidates, and candidates, each with preferences over the institutions. A central entity evaluates the utility of each candidate to the institutions, and the goal is to select candidates for each institution in a way that maximizes utility while also considering the candidates' preferences. The paper focuses on the setting in which candidates are divided into multiple groups and the observed utilities of candidates in some groups are biased--systematically lower than their true utilities. The first result is that, in these biased settings, prior algorithms can lead to selections with sub-optimal true utility and significant discrepancies in the fraction of candidates from each group that get their preferred choices. Subsequently, an algorithm is presented along with proof that it produces selections that achieve near-optimal group fairness with respect to preferences while also nearly maximizing the true utility under distributional assumptions. Further, extensive empirical validation of these results in real-world and synthetic settings, in which the distributional assumptions may not hold, are presented.",
            "fieldsOfStudy": [
                "Computer Science",
                "Economics",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "2261914455",
                    "name": "Amit Kumar"
                },
                {
                    "authorId": "1810064",
                    "name": "Nisheeth K. Vishnoi"
                },
                {
                    "authorId": "2319412379",
                    "name": "Andrew Xu"
                }
            ]
        },
        {
            "paperId": "a02dbf0c1d2debc1a825e55f17958853eb3d40aa",
            "title": "Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach",
            "abstract": "In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a ``desired'' classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees and encode context-specific group fairness properties. Evaluation on real-world datasets shows that this approach consistently boosts the quality of collected outcome data and improves the fraction of true positives for all groups, with only a small reduction in predictive utility.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "152276166",
                    "name": "Anay Mehrotra"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                }
            ]
        },
        {
            "paperId": "5c412966c62b46235c6535ce7c4071057a6f6da1",
            "title": "Bias in Evaluation Processes: An Optimization-Based Model",
            "abstract": "Biases with respect to socially-salient attributes of individuals have been well documented in evaluation processes used in settings such as admissions and hiring. We view such an evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution and model it as a solution to a loss minimization problem subject to an information constraint. Our model has two parameters that have been identified as factors leading to biases: the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function. We characterize the distributions that arise from our model and study the effect of the parameters on the observed distribution. The outputs of our model enrich the class of distributions that can be used to capture variation across groups in the observed evaluations. We empirically validate our model by fitting real-world datasets and use it to study the effect of interventions in a downstream selection task. These results contribute to an understanding of the emergence of bias in evaluation processes and provide tools to guide the deployment of interventions to mitigate biases.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "2261914455",
                    "name": "Amit Kumar"
                },
                {
                    "authorId": "152276166",
                    "name": "Anay Mehrotra"
                },
                {
                    "authorId": "1810064",
                    "name": "Nisheeth K. Vishnoi"
                }
            ]
        },
        {
            "paperId": "9b7cadce8fc34c0d440389f1cfa19dee4af2bace",
            "title": "Designing Closed-Loop Models for Task Allocation",
            "abstract": "Automatically assigning tasks to people is challenging because human performance can vary across tasks for many reasons. This challenge is further compounded in real-life settings in which no oracle exists to assess the quality of human decisions and task assignments made. Instead, we find ourselves in a\"closed\"decision-making loop in which the same fallible human decisions we rely on in practice must also be used to guide task allocation. How can imperfect and potentially biased human decisions train an accurate allocation model? Our key insight is to exploit weak prior information on human-task similarity to bootstrap model training. We show that the use of such a weak prior can improve task allocation accuracy, even when human decision-makers are fallible and biased. We present both theoretical analysis and empirical evaluation over synthetic data and a social media toxicity detection task. Results demonstrate the efficacy of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                }
            ]
        },
        {
            "paperId": "fe966e80c07e93dfcfe7e5a38511470df3e991a8",
            "title": "Subset Selection Based On Multiple Rankings in the Presence of Bias: Effectiveness of Fairness Constraints for Multiwinner Voting Score Functions",
            "abstract": "We consider the problem of subset selection where one is given multiple rankings of items and the goal is to select the highest ``quality'' subset. Score functions from the multiwinner voting literature have been used to aggregate rankings into quality scores for subsets. We study this setting of subset selection problems when, in addition, rankings may contain systemic or unconscious biases toward a group of items. For a general model of input rankings and biases, we show that requiring the selected subset to satisfy group fairness constraints can improve the quality of the selection with respect to unbiased rankings. Importantly, we show that for fairness constraints to be effective, different multiwinner score functions may require a drastically different number of rankings: While for some functions, fairness constraints need an exponential number of rankings to recover a close-to-optimal solution, for others, this dependency is only polynomial. This result relies on a novel notion of ``smoothness'' of submodular functions in this setting that quantifies how well a function can ``correctly'' assess the quality of items in the presence of bias. The results in this paper can be used to guide the choice of multiwinner score functions for the subset selection setting considered here; we additionally provide a tool to empirically enable this.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382811289",
                    "name": "Niclas Boehmer"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "2002075",
                    "name": "Lingxiao Huang"
                },
                {
                    "authorId": "152276166",
                    "name": "Anay Mehrotra"
                },
                {
                    "authorId": "1810064",
                    "name": "Nisheeth K. Vishnoi"
                }
            ]
        },
        {
            "paperId": "0bb976bce77bea2a0fa75488af4ea12e01117a6b",
            "title": "Addressing Strategic Manipulation Disparities in Fair Classification",
            "abstract": "In real-world classification settings, such as loan application evaluation or content moderation on online platforms, individuals respond to classifier predictions by strategically updating their features to increase their likelihood of receiving a particular (positive) decision (at a certain cost). Yet, when different demographic groups have different feature distributions or pay different update costs, prior work has shown that individuals from minority groups often pay a higher cost to update their features. Fair classification aims to address such classifier performance disparities by constraining the classifiers to satisfy statistical fairness properties. However, we show that standard fairness constraints do not guarantee that the constrained classifier reduces the disparity in strategic manipulation cost. To address such biases in strategic settings and provide equal opportunities for strategic manipulation, we propose a constrained optimization framework that constructs classifiers that lower the strategic manipulation cost for minority groups. We develop our framework by studying theoretical connections between group-specific strategic cost disparity and standard selection rate fairness metrics (e.g., statistical rate and true positive rate). Empirically, we show the efficacy of this approach over multiple real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                }
            ]
        },
        {
            "paperId": "e163a71a6def72e27cb2f51fe21b9382051cbab5",
            "title": "Revisiting Group Fairness Metrics: The Effect of Networks",
            "abstract": "An increasing amount of work studies fairness in socio-technical settings from a computational perspective. This work has introduced a variety of metrics to measure fairness in different settings. Most of these metrics, however, do not account for the interactions between individuals or evaluate any underlying network's effect on the outcomes measured. While a wide body of work studies the organization of individuals into a network structure and how individuals access resources in networks, the impact of network structure on fairness has been largely unexplored. We introduce templates for group fairness metrics that account for network structure. More specifically, we present two types of group fairness metrics that measure distinct yet complementary forms of bias in networks. The first type of metric evaluates how access to others in the network is distributed across groups. The second type of metric evaluates how groups distribute their interactions across other groups, and hence captures inter-group biases. We find that ignoring the network can lead to spurious fairness evaluations by either not capturing imbalances in influence and reach illuminated by the first type of metric, or by overlooking interaction biases as evaluated by the second type of metric. Our empirical study illustrates these pronounced differences between network and non-network evaluations of fairness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152276166",
                    "name": "Anay Mehrotra"
                },
                {
                    "authorId": "144275165",
                    "name": "Jeffery Sachs"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                }
            ]
        },
        {
            "paperId": "b01688f7586b3ab3ea864cb6698a5aa2dc9c0d67",
            "title": "Auditing for Diversity Using Representative Examples",
            "abstract": "Assessing the diversity of a dataset of information associated with people is crucial before using such data for downstream applications. For a given dataset, this often involves computing the imbalance or disparity in the empirical marginal distribution of a protected attribute (e.g. gender, dialect, etc.). However, real-world datasets, such as images from Google Search or collections of Twitter posts, often do not have protected attributes labeled. Consequently, to derive disparity measures for such datasets, the elements need to hand-labeled or crowd-annotated, which are expensive processes. We propose a cost-effective approach to approximate the disparity of a given unlabeled dataset, with respect to a protected attribute, using a control set of labeled representative examples. Our proposed algorithm uses the pairwise similarity between elements in the dataset and elements in the control set to effectively bootstrap an approximation to the disparity of the dataset. Importantly, we show that using a control set whose size is much smaller than the size of the dataset is sufficient to achieve a small approximation error. Further, based on our theoretical framework, we also provide an algorithm to construct adaptive control sets that achieve smaller approximation errors than randomly chosen control sets. Simulations on two image datasets and one Twitter dataset demonstrate the efficacy of our approach (using random and adaptive control sets) in auditing the diversity of a wide variety of datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                }
            ]
        },
        {
            "paperId": "dd2a44ced60bf871070c3381c73e8258f922bae8",
            "title": "Fair Classification with Adversarial Perturbations",
            "abstract": "We study fair classification in the presence of an omniscient adversary that, given an $\\eta$, is allowed to choose an arbitrary $\\eta$-fraction of the training samples and arbitrarily perturb their protected attributes. The motivation comes from settings in which protected attributes can be incorrect due to strategic misreporting, malicious actors, or errors in imputation; and prior approaches that make stochastic or independence assumptions on errors may not satisfy their guarantees in this adversarial setting. Our main contribution is an optimization framework to learn fair classifiers in this adversarial setting that comes with provable guarantees on accuracy and fairness. Our framework works with multiple and non-binary protected attributes, is designed for the large class of linear-fractional fairness metrics, and can also handle perturbations besides protected attributes. We prove near-tightness of our framework's guarantees for natural hypothesis classes: no algorithm can have significantly better accuracy and any algorithm with better fairness must have lower accuracy. Empirically, we evaluate the classifiers produced by our framework for statistical rate on real-world and synthetic datasets for a family of adversaries.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "152276166",
                    "name": "Anay Mehrotra"
                },
                {
                    "authorId": "1810064",
                    "name": "Nisheeth K. Vishnoi"
                }
            ]
        },
        {
            "paperId": "0c0b5ac3b73f3010cc6a1b73cf9e031cfc26339d",
            "title": "Dialect Diversity in Text Summarization on Twitter",
            "abstract": "Discussions on Twitter involve participation from different communities with different dialects and it is often necessary to summarize a large number of posts into a representative sample to provide a synopsis. Yet, any such representative sample should sufficiently portray the underlying dialect diversity to present the voices of different participating communities representing the dialects. Extractive summarization algorithms perform the task of constructing subsets that succinctly capture the topic of any given set of posts. However, we observe that there is dialect bias in the summaries generated by common summarization approaches, i.e., they often return summaries that under-represent certain dialects. The vast majority of existing \u201cfair\u201d summarization approaches require socially salient attribute labels (in this case, dialect) to ensure that the generated summary is fair with respect to the socially salient attribute. Nevertheless, in many applications, these labels do not exist. Furthermore, due to the ever-evolving nature of dialects in social media, it is unreasonable to label or accurately infer the dialect of every social media post. To correct for the dialect bias, we employ a framework that takes an existing text summarization algorithm as a blackbox and, using a small set of dialect-diverse sentences, returns a summary that is relatively more dialect-diverse. Crucially, this approach does not need the posts being summarized to have dialect labels, ensuring that the diversification process is independent of dialect classification/identification models. We show the efficacy of our approach on Twitter datasets containing posts written in dialects used by different social groups defined by race or gender; in all cases, our approach leads to improved dialect diversity compared to standard text summarization approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                }
            ]
        }
    ]
}