{
    "authorId": "3152861",
    "papers": [
        {
            "paperId": "5fd5fd8aa89a3938f089a4b42de84bf36cc7aaca",
            "title": "Lecture Notes on Monadic First- and Second-Order Logic on Strings",
            "abstract": "These notes present the essentials of first- and second-order monadic logics on strings with introductory purposes. We discuss Monadic First-Order logic and show that it is strictly less expressive than Finite-State Automata, in that it only captures a strict subset of Regular Languages -- the non-counting ones. We then introduce Monadic Second-Order logic; such a logic is, syntactically, a superset of Monadic First-Order logic and captures Regular Languages exactly. We also show how to transform an automaton into a corresponding formula and vice versa. Finally, we discuss the use of logical characterizations of classes of languages as the basis for automatic verification techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1799805",
                    "name": "D. Mandrioli"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1783938",
                    "name": "A. Morzenti"
                },
                {
                    "authorId": "1691130",
                    "name": "Matteo Pradella"
                },
                {
                    "authorId": "145549418",
                    "name": "M. Rossi"
                }
            ]
        },
        {
            "paperId": "60d1f2b1e14fee535b0a7507f571979f8cc459c6",
            "title": "PG-Triggers: Triggers for Property Graphs",
            "abstract": "Graph databases are emerging as the leading data management technology for storing large knowledge graphs; significant efforts are ongoing to produce new standards (such as the Graph Query Language, GQL), as well as enrich them with properties, types, schemas, and keys. In this article, we introduce PG-Triggers, a complete proposal for adding triggers to Property Graphs, along the direction marked by the SQL3 Standard. We define the syntax and semantics of PG-Triggers and then illustrate how they can be implemented on top of Neo4j, one of the most popular graph databases. In particular, we introduce a syntax-directed translation from PG-Triggers into Neo4j, which makes use of the so-called APOC triggers; APOC is a community-contributed library for augmenting the Cypher query language supported by Neo4j. We also cover Memgraph, and show that our approach applies to this system in a similar way. We illustrate the use of PG-Triggers through a life science application inspired by the COVID-19 pandemic. The main objective of this article is to introduce an active database standard for graph databases as a first-class citizen at a time when reactive graph management is in its infancy, so as to minimize the conversion efforts towards a full-fledged standard proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38439313",
                    "name": "Alessia Gagliardi"
                },
                {
                    "authorId": "144910102",
                    "name": "Anna Bernasconi"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                }
            ]
        },
        {
            "paperId": "773906066fe308b465e284d148181613c43a6fb7",
            "title": "Injecting Conceptual Constraints into Data Fabrics",
            "abstract": "Unlike traditional sources managed by DBMSs, data lakes do not provide any guarantee about the quality of the data they store, which can severely limit their use for analysis purposes. The recent notion of data fabric, which introduces a semantic layer allowing uniform access to underlying data sources, makes it possible to tackle this problem by specifying conceptual constraints to which data sources must adhere to be considered meaningful. Along these lines, in this discussion paper, we exploit the data fabric approach by proposing a general methodology for data curation in data fabrics based on: (i) the specification of integrity constraints over a conceptual representation of the data lake and (ii) the automatic translation and enforcement of such constraints over the actual data. We discuss the advantages of this idea and the challenges behind its implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "9ff7207af43cb1e378d441620493016e5a1c14ba",
            "title": "Reactive Company Control in Company Knowledge Graphs",
            "abstract": "The Company Control Problem consists in understanding who exerts decision power in companies. Central banks, financial intelligence units, and market regulators are all interested in this problem, which is crucial for their core goals. In the context where these actors operate, changes in company control call for immediate reactions.Yet, computing control relationships is a computationally expensive problem that involves traversing the entire shareholding structure and aggregating shares over multiple paths.In the context of the joint European banking supervision, the Bank of Italy will soon handle the shareholding graph of all European companies, which comprises hundreds of millions of entities (firms and individuals) and billions of edges and properties. This graph is highly volatile as the Bank continuously receives updates about shareholding relationships with unpredictable high frequency. This makes the straightforward bulk solution, where all the company control relationships are computed and materialized whenever a change occurs, unaffordable in practice.In this work, we present an incremental rule-based formalization of the problem, adopting the Vadalog fragment of the Datalog+/- families of languages. Our approach analyzes the specific change, singles out the portions of the graph that are affected by it, and selectively updates them. This allows one both to timely evaluate the impact of ownership variations on an extensive European-scale shareholding graph and to enable economists to perform the so-called \"what-if analysis\", i.e., simulation scenarios to proactively study the consequences of potential share acquisition operations, that currently are prohibitively time expensive. We provide an extensive experimental evaluation on very large company graphs, comparatively confirming the scalability of our technique in a real production setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1645341080",
                    "name": "Davide Magnanimi"
                },
                {
                    "authorId": "1716855",
                    "name": "Luigi Bellomarini"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "2e86a75d03ed9064f0dba5aa63480a562e723d9c",
            "title": "Conceptual Constraints for Data Quality in Data Lakes",
            "abstract": "A data lake is a loosely-structured collection of data at scale built for analysis purposes that is initially fed with almost no requirement of data quality. This approach aims at eliminating any effort before the actual exploitation of data, but the problem is only delayed since robust and defensible data analysis can only be performed after very complex data preparation activities. In this paper, we address this problem by proposing a novel and general approach to data curation in data lakes based on: (i) the specification of integrity constraints over a conceptual representation of the data lake and (ii) the automatic translation and enforcement of such constraints over the actual data. We discuss the advantages of this idea and the challenges behind its implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "b1119c6a7775c9c65bec45791e2668592039c565",
            "title": "SoCRATe: a Framework for Compensating Users Over Time with Limited Availability Recommendations",
            "abstract": "We present our preliminary ideas for developing SoCRATe, a framework and an online system dedicated to providing recommendations to users when items\u2019 availability is limited. SoCRATe is relevant to several real-world applications, among which movie and task recommendations. SoCRATe has several appealing features: it watches users as they consume recommendations and accounts for user feedback in refining recommendations in the next round, it implements loss compensation strategies to make up for sub-optimal recommendations, in terms of accuracy, when items have limited availability, and it decides when to re-generate recommendations on a need-based fashion. SoCRATe accommodates real users as well as simulated users to enable testing multiple recommendation choice models. To frame evaluation, SoCRATe introduces a new set of measures that capture recommendation accuracy over time as well as throughput and user satisfaction. All these features make SoCRATe unique and able to adapt recommendations to user preferences in a resource-limited setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "daff53367af4565f5c32e308155df57601f6eb26",
            "title": "SoCRATe: A Recommendation System with Limited-Availability Items",
            "abstract": "We demonstrate SoCRATe, an online system dedicated to providing adaptive recommendations to users when items have limited availability. SoCRATe is relevant to several real-world applications, among which movie and task recommendations. SoCRATe has several appealing features: (i) watching users as they consume recommendations and accounting for user feedback in refining recommendations in the next round; (ii) implementing loss compensation strategies to make up for sub-optimal recommendations, in terms of accuracy, when items have limited availability; (iii) deciding when to re-generate recommendations on a need-based fashion. SoCRATe accommodates real users as well as simulated users to enable testing multiple recommendation choice models. To frame evaluation, SoCRATe introduces a new set of measures that capture recommendation accuracy, user satisfaction and item consumption over time. All these features make SoCRATe unique and able to adapt recommendations to user preferences in a resource-limited setting. A video of SoCRATe is available at https://youtu.be/4wlaScc_rUo.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "e85efa7562ac9cadeae7a79ac39bbd83b5036368",
            "title": "Helping Wine Lovers With Taxonomies",
            "abstract": "We formally investigate the problem of retrieving the best results complying with multiple preferences expressed in a logic-based language when data are stored in relational tables with taxonomic domains. We introduce two operators that rewrite preferences for enforcing transitivity, which guarantees soundness of the result, and specificity, which solves conflicts among preferences. We show that these two properties cannot be achieved together and identify the only two possibilities to ensure transitivity and minimize conflicts. Our approach proves effective when tested over both synthetic and real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "f47f5a0c8062739494be5e620f6dc3d04e14e7ea",
            "title": "Ambiguity Detection and Textual Claims Generation from Relational Data",
            "abstract": "Computational fact checking, (given a textual claim and a table, verify if the claim holds w.r.t. the given data) and data-to-text generation (given a subset of cells, produce a sentence describing them) exploit the relationship between relational data and natural language text. Despite promising results in these areas, state-of-the-art solutions simply fail in managing \u201cdata-ambiguity\", i.e., the case when there are multiple interpretations of the relationship between the textual sentence and the relational data. To tackle this problem, we present Pythia, a system that, given a relational table \ud835\udc37 , generates textual sentences that contain factual ambiguities w.r.t. the data in \ud835\udc37 . Such sentences can then be used to train target applications in handling data-ambiguity. In this paper, we discuss how Pythia generates data ambiguous sentences for a given table in an unsupervised fashion using data profiling and query generation. We then show how two existing downstream applications, namely data-to-text and computational fact checking, benefit from Pythia\u2019s generated sentences, improving the state-of-the-art results without manual user effort.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "784e5f1579d62e86624a6789057e3fd45832704f",
            "title": "Preference Queries over Taxonomic Domains",
            "abstract": "When composing multiple preferences characterizing the most suitable results for a user, several issues may arise. Indeed, preferences can be partially contradictory, suffer from a mismatch with the level of detail of the actual data, and even lack natural properties such as transitivity. In this paper we formally investigate the problem of retrieving the best results complying with multiple preferences expressed in a logic-based language. Data are stored in relational tables with taxonomic domains, which allow the specification of preferences also over values that are more generic than those in the database. In this framework, we introduce two operators that rewrite preferences for enforcing the important properties of transitivity, which guarantees soundness of the result, and specificity, which solves all conflicts among preferences. Although, as we show, these two properties cannot be fully achieved together, we use our operators to identify the only two alternatives that ensure transitivity and minimize the residual conflicts. Building on this finding, we devise a technique, based on an original heuristics, for selecting the best results according to the two possible alternatives. We finally show, with a number of experiments over both synthetic and real-world datasets, the effectiveness and practical feasibility of the overall approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "0364369ef75078c74c5d2c1bb2a87d29de595d27",
            "title": "Foundations of Context-aware Preference Propagation",
            "abstract": "Preferences are a fundamental ingredient in a variety of fields, ranging from economics to computer science, for deciding the best choices among possible alternatives. Contexts provide another important aspect to be considered in the selection of the best choices, since, very often, preferences are affected by context. In particular, the problem of preference propagation from more generic to more specific contexts naturally arises. Such a problem has only been addressed in a very limited way and always resorts to practical, ad hoc approaches. To fill this gap, in this article, we analyze preference propagation in a principled way and adopt an abstract context model without making any specific assumptions on how preferences are stated. Our framework only requires that the contexts form a partially ordered set and that preferences define a strict partial order on the objects of interest. We first formalize the basic properties that any propagation process should satisfy. We then introduce an algebraic model for preference propagation that relies on two abstract operators for combining preferences, and, under mild assumptions, we prove that the only possible interpretations for such operators are the well-known Pareto and Prioritized composition. We then study several propagation methods based on such operators and precisely characterize them in terms of the stated properties. We finally identify a method meeting all the requirements, on the basis of which we provide an efficient algorithm for preference propagation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "3220c0f982f6fabd354e2a44ce45db56e0a4afbc",
            "title": "Flexible Skylines",
            "abstract": "Skyline and ranking queries are two popular, alternative ways of discovering interesting data in large datasets. Skyline queries are simple to specify, as they just return the set of all non-dominated tuples, thereby providing an overall view of potentially interesting results. However, they are not equipped with any means to accommodate user preferences or to control the cardinality of the result set. Ranking queries adopt, instead, a specific scoring function to rank tuples, and can easily control the output size. While specifying a scoring function allows one to give different importance to different attributes by means of, e.g., weight parameters, choosing the \u201cright\u201d weights to use is known to be a hard problem. In this article, we embrace the skyline approach by introducing an original framework able to capture user preferences by means of constraints on the weights used in a scoring function, which is typically much easier than specifying precise weight values. To this end, we introduce the novel concept of F-dominance, i.e., dominance with respect to a family of scoring functions F: a tuple t is said to F-dominate tuple s when t is always better than or equal to s according to all the functions in F. Based on F-dominance, we present two flexible skyline (F-skyline) operators, both returning a subset of the skyline: nd, characterizing the set of non-F-dominated tuples; po, referring to the tuples that are also potentially optimal, i.e., best according to some function in F. While nd and po coincide and reduce to the traditional skyline when F is the family of all monotone scoring functions, their behaviors differ when subsets thereof are considered. We discuss the formal properties of these new operators, show how to implement them efficiently, and evaluate them on both synthetic and real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "b154f64e2fca23d8a3166b2ff1362b7439896e50",
            "title": "Where Porceddu is better than Pasta",
            "abstract": ". Preferences and contexts are fundamental aspects for deciding the best choices among possible options. We formalize the problem of propagating preferences from more generic to more speci\ufb01c contexts and study the key properties of propagation within an algebraic framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "215eb52ed161ce3ae68f0beabf8fd0bda25586a0",
            "title": "Flexible Score Aggregation (Extended Abstract)",
            "abstract": "Ranking objects according to different criteria is a central issue in many data-intensive applications. Yet, no existing solution deals with the case of partially specified score aggregation functions (e.g., a weighted sum with no precisely known weight values). We address multi-source top-k queries with constraints (rather than precise values) on the weights. Our solution is instance optimal and provides increased flexibility with negligible overhead wrt classical top-k queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "0b99842250ed4501376919ab081b2673ddd1036b",
            "title": "FA + TA <FSA: Flexible Score Aggregation",
            "abstract": "The problem of aggregating scores, so as to provide a ranking of objects in a dataset according to different evaluation criteria, is central to many modern data-intensive applications. Although efficient (instance optimal) algorithms exist to this purpose (such as the Threshold Algorithm TA and its variants) none of them is able to deal with scenarios in which the function used to aggregate scores is only partially specified. This is the typical case when the function is a weighted sum, and the user is unable to provide precise values for the weights. In this paper, we consider the problem of processing multi-source top-k queries, when only constraints, rather than precise values, are available for the weights. After observing that the so-called Fagin's Algorithm (FA) can be adapted to solve the problem, yet only when no constraints at all are present (a case in which our queries will return the k-skyband of the dataset), we introduce the novel FSA algorithm, which we prove to be instance optimal for any set of constraints on the weights. We also propose several optimizations to the basic FSA logic so as to improve execution times. Experimental analysis on both real and synthetic datasets shows that our optimizations are indeed highly effective and that the increased flexibility provided by FSA introduces little overhead with respect to the case of classical top-k queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "0bc83a9bf8e87d32df7a2ad105e2d4bce5ca0c3d",
            "title": "Ethics-aware Data Governance (Vision Paper)",
            "abstract": "The number of datasets available to legal practitioners, policy makers, scientists, and many other categories of citizens is growing at an unprecedented rate. Ethics-aware data processing has become a pressing need, considering that data are often used within critical decision processes (e.g., staff evaluation, college admission, criminal sentencing). The goal of this paper is to propose a vision for the injection of ethical principles (fairness, non-discrimination, transparency, data protection, diversity, and human interpretability of results) into the data analysis lifecycle (source selection, data integration, and knowledge extraction) so as to make them first-class requirements. In our vision, a comprehensive checklist of ethical desiderata for data protection and processing needs to be developed, along with methods and techniques to ensure and verify that these ethically motivated requirements and related legal norms are fulfilled throughout the data selection and exploration processes. Ethical requirements can then be enforced at all the steps of knowledge extraction through a unified data modeling and analysis methodology relying on appropriate conceptual and technical tools.",
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "authors": [
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1741158",
                    "name": "P. Atzeni"
                },
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "1716642",
                    "name": "Ilaria Bartolini"
                },
                {
                    "authorId": "1766807",
                    "name": "L. Cabibbo"
                },
                {
                    "authorId": "2767120",
                    "name": "L. Calderoni"
                },
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "1791339",
                    "name": "Valter Crescenzi"
                },
                {
                    "authorId": "1807669",
                    "name": "Juan Carlos De Martin"
                },
                {
                    "authorId": "51209609",
                    "name": "Selina Fenoglietto"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "145042418",
                    "name": "S. Greco"
                },
                {
                    "authorId": "34675913",
                    "name": "F. Isgr\u00f2"
                },
                {
                    "authorId": "1747625",
                    "name": "D. Maio"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "145228093",
                    "name": "M. Matera"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "1698803",
                    "name": "Cristian Molinaro"
                },
                {
                    "authorId": "1742653",
                    "name": "M. Patella"
                },
                {
                    "authorId": "145123169",
                    "name": "R. Prevete"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1471635988",
                    "name": "A. Santangelo"
                },
                {
                    "authorId": "1688359",
                    "name": "Andrea Tagarelli"
                },
                {
                    "authorId": "2527878",
                    "name": "G. Tamburrini"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "54b6fd16fcaaad0a39fc23bd2a9cf043b802376b",
            "title": "Beyond Skyline and Ranking Queries: Restricted Skylines (Extended Abstract)",
            "abstract": "Traditionally, skyline and ranking queries have been treated separately as alternative ways of discovering interesting data in potentially large datasets. While ranking queries adopt a specific scoring function to rank tuples, skyline queries return the set of non-dominated tuples and are independent of attribute scales and scoring functions. Ranking queries are thus less general, but cheaper to compute and widely used. In this paper, we integrate these two approaches under the unifying framework of restricted skylines by applying the notion of dominance to a set of scoring functions of interest.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "058c1d53b5b6cafccec2b5b50740ac16c221a6b7",
            "title": "Googling the Deep Web (Extended abstract)",
            "abstract": "The Deep Web is constituted by data that are accessible through Web pages, but not indexable by search engines as they are returned in dynamic pages. In this paper we propose a conceptual framework for answering keyword queries on Deep Web sources represented as relational tables with so-called access limitations. We formalize the notion of optimal answer and characterize queries for which an answer can be found.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "c014d33f0de1fd0f54589f07cc4e8d4ed71f312a",
            "title": "Querying the Deep Web: Back to the Foundations",
            "abstract": "The Deep Web is the large corpus of data accessible on the Web through forms and presented in dynamically-generated pages, but not indexable as static pages, and therefore invisible to search engines. Deep Web data are usually modelled as relations with so-called access limitations, that is, they can be queried only by selecting certain attributes. In this paper we give some fundamental complexity results on the problem of processing conjunctive (select-project-join) queries on relational data with access limitations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "2791513",
                    "name": "Igor Razgon"
                },
                {
                    "authorId": "144739409",
                    "name": "M. Ugarte"
                }
            ]
        },
        {
            "paperId": "ea8e075849ab7015ac1768f47e16bfc8272b0318",
            "title": "Reconciling Skyline and Ranking Queries",
            "abstract": "Traditionally, skyline and ranking queries have been treated separately as alternative ways of discovering interesting data in potentially large datasets. While ranking queries adopt a specific scoring function to rank tuples, skyline queries return the set of non-dominated tuples and are independent of attribute scales and scoring functions. Ranking queries are thus less general, but usually cheaper to compute and widely used in data management systems. \n \nWe propose a framework to seamlessly integrate these two approaches by introducing the notion of restricted skyline queries (R-skylines). We propose R-skyline operators that generalize both skyline and ranking queries by applying the notion of dominance to a set of scoring functions of interest. Such sets can be characterized, e.g., by imposing constraints on the function's parameters, such as the weights in a linear scoring function. We discuss the formal properties of these new operators, show how to implement them efficiently, and evaluate them on both synthetic and real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "841cade87adeda923f3e909ccfa75e09f70cc336",
            "title": "Crowdsourcing for Top-K Query Processing over Uncertain Data",
            "abstract": "Querying uncertain data has become a prominent application due to the proliferation of user-generated content from social media and of data streams from sensors. When data ambiguity cannot be reduced algorithmically, crowdsourcing proves a viable approach, which consists of posting tasks to humans and harnessing their judgment for improving the confidence about data values or relationships. This paper tackles the problem of processing top-K queries over uncertain data with the help of crowdsourcing for quickly converging to the realordering of relevant results. Several offline and online approaches for addressing questions to a crowd are defined and contrasted on both synthetic and real data sets, with the aim of minimizing the crowd interactions necessary to find the realordering of the result set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38140483",
                    "name": "Eleonora Ciceri"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "d199cb68cbedc76a217c4d2a4c7c57f91908604b",
            "title": "Humans Fighting Uncertainty: Crowdsourcing for Top-K Query Processing",
            "abstract": "Querying uncertain data has become a prominent application due to the proliferation of user-generated content from social media and of data streams from sensors. When data ambiguity cannot be reduced algorithmically, crowdsourcing proves a viable approach, which consists in posting tasks to humans and harnessing their judgment for improving the confidence about data values or relationships. This paper tackles the problem of processing top-K queries over uncertain data with the help of crowdsourcing for quickly converging to the real ordering of relevant results. Several o\u270fine and online approaches for addressing questions to a crowd are defined and contrasted on both synthetic and real data sets, with the aim of minimizing the crowd interactions necessary to find the real ordering of the result set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38140483",
                    "name": "Eleonora Ciceri"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "755babb5bd72331c062bd0a2bc5f776e16d8d742",
            "title": "Keyword Search in the Deep Web",
            "abstract": "The Deep Web is constituted by data accessible through Web \npages, but not readily indexable by search engines, as they are returned \nin dynamic pages. In this paper we propose a framework for accessing \nDeep Web sources, represented as relational tables with so-called ac- \ncess limitations, with keyword-based queries. We formalize the notion \nof optimal answer and investigate methods for query processing. To our \nknowledge, this problem has never been studied in a systematic way.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "7632442171446e71183531f2bfadd5594565636b",
            "title": "When Food Matters: Identifying Food-related Events on Twitter",
            "abstract": "Food communities in Twitter are growing every year, and food-related content permeates everyday conversations. Users meet on Twitter to share recipes, give cooking advices or simply inform others about what they are eating. While some of these food-related conversations are not associated with any special occurrence, many conversations take place instead during specific events. The detection of food-related events gives interesting insights: people do not talk only about Halloween and Easter, but they also create their own food-related events, such as the promotion of products (e.g., an online petition to propose the production of bacon-flavored chips) or themed home-made recipes (e.g., a day of recipes dedicated to chocolate). In this paper, we propose an approach that accurately captures food-related content from the tweet live stream, and analyze the detected conversations to identify food-related events. The proposed technique is general as it can be applied to the identification of other thematic events in digital streams.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38140483",
                    "name": "Eleonora Ciceri"
                },
                {
                    "authorId": "2455857",
                    "name": "Ilio Catallo"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                }
            ]
        },
        {
            "paperId": "93b0c7091a78a4756d769ce7d993a1343da43eb8",
            "title": "Champagne: A Web Tool for the Execution of Crowdsourcing Campaigns",
            "abstract": "We present Champagne, a web tool for the execution of crowdsourcing campaigns. Through Champagne, task requesters can model crowdsourcing campaigns as a sequence of choices regarding different, independent crowdsourcing design decisions. Such decisions include, e.g., the possibility of qualifying some workers as expert reviewers, or of combining different quality assurance techniques to be used during campaign execution. In this regard, a walkthrough example showcasing the capabilities of the platform is reported. Moreover, we show that our modular approach in the design of campaigns overcomes many of the limitations exposed by the major platforms available in the market.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3354794",
                    "name": "Carlo Bernaschina"
                },
                {
                    "authorId": "2455857",
                    "name": "Ilio Catallo"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "e6664b56dd85b4375c00a85386729214ce2563ec",
            "title": "On the Role of Task Design in Crowdsourcing Campaigns",
            "abstract": "\n \n Despite the success of crowdsourcing marketplaces, fully harnessing their massive workforce remains challenging. In this work we study the effect on crowdsourcing campaigns of different feedback and payment strategies. Our results reveal the joint effect of feedback and payment on the quality and quantity of the outcome.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3354794",
                    "name": "Carlo Bernaschina"
                },
                {
                    "authorId": "2455857",
                    "name": "Ilio Catallo"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "bf4f76c3da8a46783dfd2b72651e2300901ced25",
            "title": "Robust aggregation of GWAP tracks for local image annotation",
            "abstract": "The possibility of assigning labels to localized regions in an image enables flexible image retrieval paradigms. However, the process of automatically segmenting and tagging images is notoriously hard, due to the presence of occlusions, noise, challenging illumination conditions, background clutter, etc. For this reason, human computation has recently emerged as a viable alternative when computer vision algorithms fail to provide a satisfactory answer. For example, Games with a purpose (GWAP) represent a powerful crowdsourcing mechanism to collect implicit annotations from human players. In this paper we consider the problem of aggregating the gaming tracks collected by a GWAP we developed to solve challenging instances of image segmentation problems. In particular we consider the existence of malicious players, who might try to fool the rules of the game to achieve higher rewards. The proposed solution can automatically estimate the reliability of human players, thus identifying cheaters. This information is exploited to aggregate the gaming tracks, thus significantly improving the image segmentation result and the quality of local image annotations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3354794",
                    "name": "Carlo Bernaschina"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "144431084",
                    "name": "Luca Galli"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "1324708ddf12d7940bc2377131e54d97111a1197",
            "title": "Fashion-focused creative commons social dataset",
            "abstract": "In this work, we present a fashion-focused Creative Commons dataset, which is designed to contain a mix of general images as well as a large component of images that are focused on fashion (i.e., relevant to particular clothing items or fashion accessories). The dataset contains 4810 images and related metadata. Furthermore, a ground truth on image's tags is presented. Ground truth generation for large-scale datasets is a necessary but expensive task. Traditional expert based approaches have become an expensive and non-scalable solution. For this reason, we turn to crowdsourcing techniques in order to collect ground truth labels; in particular we make use of the commercial crowdsourcing platform, Amazon Mechanical Turk (AMT). Two different groups of annotators (i.e., trusted annotators known to the authors and crowdsourcing workers on AMT) participated in the ground truth creation. Annotation agreement between the two groups is analyzed. Applications of the dataset in different contexts are discussed. This dataset contributes to research areas such as crowdsourcing for multimedia, multimedia content analysis, and design of systems that can elicit fashion preferences from users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689471",
                    "name": "B. Loni"
                },
                {
                    "authorId": "2057702255",
                    "name": "Mar\u00eda Men\u00e9ndez"
                },
                {
                    "authorId": "2065871876",
                    "name": "Mihai Georgescu"
                },
                {
                    "authorId": "144431084",
                    "name": "Luca Galli"
                },
                {
                    "authorId": "145921225",
                    "name": "C. Massari"
                },
                {
                    "authorId": "1757482",
                    "name": "I. S. Alting\u00f6vde"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1813567",
                    "name": "M. Melenhorst"
                },
                {
                    "authorId": "3285093",
                    "name": "Raynor Vliegendhart"
                },
                {
                    "authorId": "145980903",
                    "name": "M. Larson"
                }
            ]
        },
        {
            "paperId": "22b43adcfcd11edacc62e3e7b92fdde25a0a1555",
            "title": "On the dependency on the size of the data when chasing under conceptual dependencies",
            "abstract": "Conceptual dependencies (CDs) are particular kinds of key dependencies (KDs) and inclusion dependencies (IDs) that precisely characterize relational schemata modeled according to the main features of the Entity-Relationship (ER) model. An instance for such a schema may be inconsistent (data violate the dependencies) and incomplete (data constitute a piece of correct information, but not necessarily all the relevant information). While undecidable under general KDs and IDs, query answering under incomplete data is known to be decidable for CDs. The known techniques are based on the chase -- a special instance, organized in levels of depth, that is a representative of all the instances that satisfy the dependencies and that include the initial instance. Although the chase generally has infinite size, query answering can be addressed by posing the query (or a rewriting thereof) on a finite, initial part of the chase. Contrary to previous claims, we show that the maximum level of such an initial part cannot be bounded by a constant that does not depend on the size of the initial instance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "321dcb5360e61cc2820a012d02deaf11f7b9f7e9",
            "title": "Top-k diversity queries over bounded regions",
            "abstract": "Top-k diversity queries over objects embedded in a low-dimensional vector space aim to retrieve the best k objects that are both relevant to given user's criteria and well distributed over a designated region. An interesting case is provided by spatial Web objects, which are produced in great quantity by location-based services that let users attach content to places and are found also in domains like trip planning, news analysis, and real estate. In this article we present a technique for addressing such queries that, unlike existing methods for diversified top-k queries, does not require accessing and scanning all relevant objects in order to find the best k results. Our Space Partitioning and Probing (SPP) algorithm works by progressively exploring the vector space, while keeping track of the already seen objects and of their relevance and position. The goal is to provide a good quality result set in terms of both relevance and diversity. We assess quality by using as a baseline the result set computed by MMR, one of the most popular diversification algorithms, while minimizing the number of accessed objects. In order to do so, SPP exploits score-based and distance-based access methods, which are available, for instance, in most geo-referenced Web data sources. Experiments with both synthetic and real data show that SPP produces results that are relevant and spatially well distributed, while significantly reducing the number of accessed objects and incurring a very low computational overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2455857",
                    "name": "Ilio Catallo"
                },
                {
                    "authorId": "38140483",
                    "name": "Eleonora Ciceri"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "af21fb7327f1e718f6d4779ce7ef117060996879",
            "title": "On the difference between checking integrity constraints before or after updates",
            "abstract": "Integrity checking is a crucial issue, as databases change their instance all the time and therefore need to be checked continuously and rapidly. Decades of research have produced a plethora of methods for checking integrity constraints of a database in an incremental manner. However, not much has been said about when to check integrity. In this paper, we study the differences and similarities between checking integrity before an update (a.k.a. pre-test) or after (a.k.a. post-test) in order to assess the respective convenience and properties.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "e5d235e021568baae812cfc5fc5087a0da3dc1ac",
            "title": "Determining Relevant Relations for Datalog Queries under Access Limitations is Undecidable",
            "abstract": "Access limitations are restrictions in the way in which the tuples of a relation can be accessed. Under access limitations, query answering becomes more complex than in the traditional case, with no guarantee that the answer tuples that can be extracted (aka maximal answer) are all those that would be found without access limitations (aka complete answer). The field of query answering under access limitations has been broadly investigated in the past. Attention has been devoted to the problem of determining relations that are relevant for a query, i.e., those (possibly off-query) relations that might need to be accessed in order to find all tuples in the maximal answer. In this short paper, we show that relevance is undecidable for Datalog queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "0b6124d048a6b029947ab1228940c95ec7735bd3",
            "title": "A Framework for Crowdsourced Multimedia Processing and Querying",
            "abstract": "This paper introduces a conceptual and architectural framework for addressing the design, execution and verification of tasks by a crowd of performers. The proposed framework is substantiated by an ongoing application to a problem of trademark logo detection in video collections. Preliminary results show that the contribution of crowds can improve the recall of state-of-the-art traditional algorithms, with no loss in terms of precision. However, task-to-executor matching, as expected, has an important influence on the task performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710630",
                    "name": "A. Bozzon"
                },
                {
                    "authorId": "2455857",
                    "name": "Ilio Catallo"
                },
                {
                    "authorId": "38140483",
                    "name": "Eleonora Ciceri"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "1db03d5ebae9e5e656aa42307618f8ce6816ce5c",
            "title": "Cost-Aware Rank Join with Random and Sorted Access",
            "abstract": "In this paper, we address the problem of joining ranked results produced by two or more services on the web. We consider services endowed with two kinds of access that are often available: 1) sorted access, which returns tuples sorted by score; 2) random access, which returns tuples matching a given join attribute value. Rank join operators combine objects of two or more relations and output the k combinations with the highest aggregate score. While the past literature has studied suitable bounding schemes for this setting, in this paper we focus on the definition of a pulling strategy, which determines the order of invocation of the joined services. We propose the Cost-Aware with Random and Sorted access (CARS) pulling strategy, which is derived at compile-time and is oblivious of the query-dependent score distributions. We cast CARS as the solution of an optimization problem based on a small set of parameters characterizing the joined services. We validate the proposed strategy with experiments on both real and synthetic data sets. We show that CARS outperforms prior proposals and that its overall access cost is always within a very short margin from that of an oracle-based optimal strategy. In addition, CARS is shown to be robust w.r.t. the uncertainty that may characterize the estimated parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "28ac78add837d9be2fe84b345fedcf238bff7524",
            "title": "Top-k bounded diversification",
            "abstract": "This paper investigates diversity queries over objects embedded in a low-dimensional vector space. An interesting case is provided by spatial Web objects, which are produced in great quantity by location-based services that let users attach content to places, and arise also in trip planning, news analysis, and real estate scenarios. The targeted queries aim at retrieving the best set of objects relevant to given user criteria and well distributed over a region of interest. Such queries are a particular case of diversified top-k queries, for which existing methods are too costly, as they evaluate diversity by accessing and scanning all relevant objects, even if only a small subset is needed. We therefore introduce Space Partitioning and Probing (SPP), an algorithm that minimizes the number of accessed objects while finding exactly the same result as MMR, the most popular diversification algorithm. SPP belongs to a family of algorithms that rely only on score-based and distance-based access methods, which are available in most geo-referenced Web data sources, and do not require retrieving all the relevant objects. Experiments show that SPP significantly reduces the number of accessed objects while incurring a very low computational overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "2e02fd4d64af878b80b8895f926dd4716fcc3488",
            "title": "Efficient Diversification of Top-k Queries over Bounded Regions",
            "abstract": "This paper reports on recent findings regarding diversity queries over objects embedded in a low-dimensional vector space. Among the many contexts of interest, we mention spatial Web objects, which are abundant in location-based services that let users attach content to places. Typical queries aim at retrieving the best set of relevant objects that are well distributed over a region of interest. Existing methods for answering diversified top-k queries are too costly, as they evaluate diversity by accessing and scanning all relevant objects, even if only a small subset thereof is needed. Our proposal, named SPP, is an algorithm that, while finding exactly the same result as MMR (one of the most popular diversification algorithms), does not require retrieving all the relevant objects and, indeed, minimizes the number of accessed objects. Experiments confirm that SPP saves a significant amount of accesses while incurring a very low computational overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "355b34c23d7109b9c853b983544e6b0fa31d960a",
            "title": "The CUBRIK project: human-enhanced time-aware multimedia search",
            "abstract": "The Cubrik Project is an Integrated Project of the 7th Framework Programme that aims at contributing to the multimedia search domain by opening the architecture of multimedia search engines to the integration of open source and third party content annotation and query processing components, and by exploiting the contribution of humans and communities in all the phases of multimedia search, from content processing to query processing and relevance feedback processing. The CUBRIK presentation will showcase the architectural concept and scientific background of the project and demonstrate an initial scenario of human-enhanced content and query processing pipeline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1710630",
                    "name": "A. Bozzon"
                },
                {
                    "authorId": "2455857",
                    "name": "Ilio Catallo"
                },
                {
                    "authorId": "38140483",
                    "name": "Eleonora Ciceri"
                },
                {
                    "authorId": "34638428",
                    "name": "F. Nucci"
                },
                {
                    "authorId": "35125294",
                    "name": "Vincenzo Croce"
                },
                {
                    "authorId": "1757482",
                    "name": "I. S. Alting\u00f6vde"
                },
                {
                    "authorId": "1685548",
                    "name": "W. Siberski"
                },
                {
                    "authorId": "1720285",
                    "name": "Fausto Giunchiglia"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                },
                {
                    "authorId": "145980903",
                    "name": "M. Larson"
                },
                {
                    "authorId": "145643264",
                    "name": "E. Izquierdo"
                },
                {
                    "authorId": "1747572",
                    "name": "P. Daras"
                },
                {
                    "authorId": "2506179",
                    "name": "Otto Chrons"
                },
                {
                    "authorId": "2163444",
                    "name": "Ralph Traph\u00f6ner"
                },
                {
                    "authorId": "33444156",
                    "name": "B. Decker"
                },
                {
                    "authorId": "144536004",
                    "name": "Jack Lomas"
                },
                {
                    "authorId": "47924142",
                    "name": "P. Aichroth"
                },
                {
                    "authorId": "36721488",
                    "name": "J. Novak"
                },
                {
                    "authorId": "3354786",
                    "name": "Ghislain Sillaume"
                },
                {
                    "authorId": "1402939222",
                    "name": "Fernando S\u00e1nchez-Figueroa"
                },
                {
                    "authorId": "1403405467",
                    "name": "Carolina Salas-Parra"
                }
            ]
        },
        {
            "paperId": "6a4e9c9934e7dc1cabf452c85c235f5ea662ca8e",
            "title": "Proximity measures for rank join",
            "abstract": "We introduce the proximity rank join problem, where we are given a set of relations whose tuples are equipped with a score and a real-valued feature vector. Given a target feature vector, the goal is to return the K combinations of tuples with high scores that are as close as possible to the target and to each other, according to some notion of distance or dissimilarity. The setting closely resembles that of traditional rank join, but the geometry of the vector space plays a distinctive role in the computation of the overall score of a combination. Also, the input relations typically return their results either by distance from the target or by score. Because of these aspects, it turns out that traditional rank join algorithms, such as the well-known HRJN, have shortcomings in solving the proximity rank join problem, as they may read more input than needed. To overcome this weakness, we define a tight bound (used as a stopping criterion) that guarantees instance optimality, that is, an I/O cost is achieved that is always within a constant factor of optimal. The tight bound can also be used to drive an adaptive pulling strategy, deciding at each step which relation to access next. For practically relevant classes of problems, we show how to compute the tight bound efficiently. An extensive experimental study validates our results and demonstrates significant gains over existing solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "dad557ff928c689d42bfd9479eb6eb752ba7e9e0",
            "title": "Dealing with the Deep Web and all its Quirks",
            "abstract": "Several approaches harvest, query, or combine Deep Web sources. Yet, in addition to well-studied aspects of the problem such as query answering using views, access limitations, or top-k querying, the Deep Web exhibits a number of peculiarities that are often neglected. First, the services usually deliver not all results, but only the top-n results according to some ranking function. This function may not be compatible with the ordering specified in a user\u2019s query. Subsequent results have to be obtained by paging, or may not even be accessible. Second, the services may deliver results in a granularity that is incompatible with the query or joinable services (e.g., months vs. exact dates). Moreover, the services may perform selections or ranking over attributes that are not exposed in the results: this poses an incompleteness problem. Additional challenges come from uncertainty, recency constraints, and inter-service dependencies. In this article, we shed light on these peculiarities, and compile a list of desiderata of a query answering system for the Deep Web.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1764889",
                    "name": "Meghyn Bienvenu"
                },
                {
                    "authorId": "1682639",
                    "name": "Daniel Deutch"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "e0fa145e17fc1e530e011a9f724b4b4aa337bb4a",
            "title": "A Draw-and-Guess Game to Segment Images",
            "abstract": "Human Computation is defined as the integration of human tasks and automated algorithms to achieve superior quality in complex tasks like multimedia content analysis. This paper discusses a scenario in which human computation is used to segment time stamped fashion images for mining trends based on visual features of garments (e.g., color and texture) and attributes of portrayed subjects (e.g., gender and age). State-of-the-art algorithms for body part detection and feature extraction can produce low quality results when parts of the body are occluded and when dealing with complex human poses. In such cases, these algorithms could benefit from the assistance of human agents. In order to jointly leverage the potential of crowds and image analysis algorithms, a game with a purpose (GWAP) is proposed, whereby players can help segment images for which specialized algorithms have failed, so as to improve the extraction of color and texture features of garments and their association with the features of the subject wearing them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144431084",
                    "name": "Luca Galli"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                },
                {
                    "authorId": "36721488",
                    "name": "J. Novak"
                }
            ]
        },
        {
            "paperId": "352e2de4e0ea1cd2a44ee3ddf6efbab4ff66b88a",
            "title": "Inconsistency-Tolerant Integrity Checking",
            "abstract": "All methods for efficient integrity checking require all integrity constraints to be totally satisfied, before any update is executed. However, a certain amount of inconsistency is the rule, rather than the exception in databases. In this paper, we close the gap between theory and practice of integrity checking, i.e., between the unrealistic theoretical requirement of total integrity and the practical need for inconsistency tolerance, which we define for integrity checking methods. We show that most of them can still be used to check whether updates preserve integrity, even if the current state is inconsistent. Inconsistency-tolerant integrity checking proves beneficial both for integrity preservation and query answering. Also, we show that it is useful for view updating, repairs, schema evolution, and other applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802431",
                    "name": "H. Decker"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "b993679bdb3dc4795d2bf8371cf92c77c43281cd",
            "title": "Context through answer set programming",
            "abstract": "In a world of global networking, the variety and abundance of information generates the need for effectively and efficiently gathering, synthesizing, and querying the available data while removing information noise. The concept of context has been developed and refined since the first approaches to ubiquitous computing [3], the research area of everywhere computing systems, which has the objective to provide help and information to users in an almost imperceptible way. At first, the idea of context was limited to time and location; later it was extended also to the other external environmental factors, current trends and phenomena that may change or influence the information and services available to a user. In this work, we refer to context primarily in relation with the effects that this notion has over data.\n A system where context awareness is integrated with -- yet orthogonal to -- data management, allows the knowledge of the context in which the data are used to drive the process of focussing on currently useful information (represented as a view), keeping noise at bay: this activity is called context-aware data tailoring [1]. To do this, we model the context as a first-class citizen, by means of a tree-shaped structure called Context Dimension Tree (CDT). Formally defined in [1], the CDT is composed of two kinds of nodes: black nodes, which represent context dimensions, and white nodes, which represent context values. Context dimensions (black nodes) model the different perspectives from which the domain of interest can be seen with respect to the user, the system and their interactions. The values these dimensions can assume are represented as white nodes. A context is obtained as a set of dimension values, thus of white nodes. The hierarchical nature of the CDT grants different levels of abstraction to represent contexts.\n In this paper we propose Answer Set Programming (ASP) as a unified tool to address all the design-time and run-time tasks related to context management. Building on preliminary work presented in [2], at design time we encode contextual information via a disjunctive logic program using the approach of ASP: given a program representing a CDT, each admissible context is represented by a model (also called answer set) of the program. From a CDT many possible contexts can be built, obtained by guessing if a value is a context element or not. Structural constraints to the composition of contexts also apply, along with application-dependent constraints preventing meaningless combinations of white nodes. Candidate contexts, which are candidate models of the program, are checked against the constraints defined, and the -- possibly multiple -- admissible contexts correspond to the multiple models of the program in accordance with ASP fundamentals. Still at design time, the designer extends the program with suitable context-aware views over the extensional data, which establish the relationships between each context and the data to be focussed upon. At run time, whenever new context information is acquired from the system conditions and/or detected by sensors, we use ASP techniques to i) validate this information against the admissible contexts produced at design time from the CDT, and ii) evaluate user queries over the current context-dependent views.\n Overall, the main contribution of this work is the design of an ASP framework for supporting design-time and run-time data tailoring in context-aware systems. Notice that ASP allows us to retain the orthogonality of context modeling while adopting the same framework as for data representation. Moreover, the multiplicity of answers permits support for the imperfection of the gathered context information since, when multiple (possibly incompatible) contexts are detected, the system will be able to deliver all the answers to the user queries, each answer differently focussed according to one of the validated contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36983079",
                    "name": "Angelo Rauseo"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "d39f01b7668a5d49c756c59d79f86f3ac2b114f9",
            "title": "Ranking with uncertain scoring functions: semantics and sensitivity measures",
            "abstract": "Ranking queries report the top-K results according to a user-defined scoring function. A widely used scoring function is the weighted summation of multiple scores. Often times, users cannot precisely specify the weights in such functions in order to produce the preferred order of results. Adopting uncertain/incomplete scoring functions (e.g., using weight ranges and partially-specified weight preferences) can better capture user's preferences in this scenario.\n In this paper, we study two aspects in uncertain scoring functions. The first aspect is the semantics of ranking queries, and the second aspect is the sensitivity of computed results to refinements made by the user. We formalize and solve multiple problems under both aspects, and present novel techniques that compute query results efficiently to comply with the interactive nature of these problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144018773",
                    "name": "Mohamed A. Soliman"
                },
                {
                    "authorId": "1743316",
                    "name": "Ihab F. Ilyas"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "020fcf4b33807079aacded35df18917fc21f4bb1",
            "title": "Proximity rank join",
            "abstract": "We introduce the proximity rank join problem, where we are given a set of relations whose tuples are equipped with a score and a real-valued feature vector. Given a target feature vector, the goal is to return the K combinations of tuples with high scores that are as close as possible to the target and to each other, according to some notion of distance. The setting closely resembles that of traditional rank join, but the geometry of the vector space plays a distinctive role in the computation of the overall score of a combination. Also, the input relations typically return their results either by distance from the target or by score. Because of these aspects, it turns out that traditional rank join algorithms, such as the well-known HRJN, have shortcomings in solving the proximity rank join problem, as they may read more input than needed. To overcome this weakness, we define a tight bound (used as a stopping criterion) that guarantees instance optimality, i.e., an I/O cost is achieved that is always within a constant factor of optimal. The tight bound can also be used to drive an adaptive pulling strategy, deciding at each step which relation to access next. For practically relevant classes of problems, we show how to compute the tight bound efficiently. An extensive experimental study validates our results and demonstrates significant gains over existing solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "105039a24287051672fc4753942a01d627c6a4da",
            "title": "Search Computing: Managing Complex Search Queries",
            "abstract": "Search computing focuses on building answers to complex search queries (for example, \"Where can I attend an interesting conference in my field near a sunny beach?\") by interacting with a constellation of cooperating search services, and using result ranking and joining as the dominant factors for service composition. The service computing paradigm has so far been neutral to the specific features of search applications and services. To address this weakness, search computing advocates a new approach in which search, join, and ranking are the central aspects for service composition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "2063956974",
                    "name": "A. Abid"
                },
                {
                    "authorId": "3158752",
                    "name": "M. A. Helou"
                },
                {
                    "authorId": "38191345",
                    "name": "D. Barbieri"
                },
                {
                    "authorId": "1710630",
                    "name": "A. Bozzon"
                },
                {
                    "authorId": "144864244",
                    "name": "Daniele Braga"
                },
                {
                    "authorId": "2112146631",
                    "name": "Marco Brambilla"
                },
                {
                    "authorId": "32452484",
                    "name": "A. Campi"
                },
                {
                    "authorId": "2650912",
                    "name": "F. Corcoglioniti"
                },
                {
                    "authorId": "2539248",
                    "name": "Emanuele Della Valle"
                },
                {
                    "authorId": "1775620",
                    "name": "D. Eynard"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "1786155",
                    "name": "Michael Grossniklaus"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "2084128",
                    "name": "S. Ronchi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                },
                {
                    "authorId": "2400772",
                    "name": "S. Vadacca"
                }
            ]
        },
        {
            "paperId": "177c56010357b71a104ea091eac27241c8c46053",
            "title": "Optimizing Query Processing for the Hidden Web",
            "abstract": "The term Deep Web (sometimes also called Hidden Web) refers to the data content that is created dynamically as the result of a specific search on the Web. In this respect, such content resides outside web pages, and is only accessible through interaction with the web site - typically via HTML forms. It is believed that the size of the Deep Web is several orders of magnitude larger than that of the so-called Surface Web, i.e., the web that is accessible and indexable by search engines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "4dce675d367e614570b484411a05abe8b57c3563",
            "title": "Top-k pipe join",
            "abstract": "In the context of service composition and orchestration, service invocation is typically scheduled according to execution plans, whose topology establishes whether different services are to be invoked in parallel or in a sequence. In the latter case, we may have a configuration, called pipe join, in which the output of a service is used as input for another service. When the services involved in a pipe join output results sorted by score, the problem arises of efficiently determining the join tuples (aka combinations) with the highest combined scores. In this paper we study different execution strategies related to the pipe join configuration. First, we consider a strategy that minimizes the access costs to achieve a target number of combinations. Then, we propose a strategy that explicitly considers the scores of the output tuples in order to provide deterministic guarantees that the top-k combinations have been found. Finally, a hybrid strategy is presented.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "7ade545d0ebd4b58d9d0023938efaa68d3f0f525",
            "title": "Querying incomplete data over extended ER schemata",
            "abstract": "Abstract Since Chen's Entity-Relationship (ER) model, conceptual modeling has been playing a fundamental role in relational data design. In this paper we consider an extended ER (EER) model enriched with cardinality constraints, disjointness assertions, and is a relations among both entities and relationships. In this setting, we consider the case of incomplete data, which is likely to occur, for instance, when data from different sources are integrated. In such a context, we address the problem of providing correct answers to conjunctive queries by reasoning on the schema. Based on previous results about decidability of the problem, we provide a query answering algorithm that performs rewriting of the initial query into a recursive Datalog query encoding the information about the schema. We finally show extensions to more general settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "805529b8723d3f69874f733aeb310fd16151ec23",
            "title": "Querying the deep web",
            "abstract": "Data stored outside Web pages and accessible from the Web, typically through HTML forms, consitute the so-called Deep Web. Such data are of great value, but difficult to query and search. We survey techniques to optimize query processing on the Deep Web, in a setting where data are represented in the relational model. We illustrate optimizations both at query plan generation time and at runtime, highlighting the role of integrity constraints. We discuss several prototype systems that address the query processing problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "c10ea0aaab85a4fd453cc18c3ec85f5a7d2d4ce7",
            "title": "Search Computing Systems (Extended Abstract)",
            "abstract": "Search Computing defines a new class of applications, which enable end users to perform exploratory search processes over multi-domain data sources available on the Web. These applications exploit suitable software frameworks and models that make it possible for expert users to configure the data sources to be searched and the interfaces for query submission and result visualization. We describe some usage scenarios and the reference architecture for Search Computing systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "2063956974",
                    "name": "A. Abid"
                },
                {
                    "authorId": "3158752",
                    "name": "M. A. Helou"
                },
                {
                    "authorId": "1710630",
                    "name": "A. Bozzon"
                },
                {
                    "authorId": "144864244",
                    "name": "Daniele Braga"
                },
                {
                    "authorId": "2112146631",
                    "name": "Marco Brambilla"
                },
                {
                    "authorId": "32452484",
                    "name": "A. Campi"
                },
                {
                    "authorId": "2650912",
                    "name": "F. Corcoglioniti"
                },
                {
                    "authorId": "2539248",
                    "name": "Emanuele Della Valle"
                },
                {
                    "authorId": "1775620",
                    "name": "D. Eynard"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "1786155",
                    "name": "Michael Grossniklaus"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "2084128",
                    "name": "S. Ronchi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                },
                {
                    "authorId": "2400772",
                    "name": "S. Vadacca"
                }
            ]
        },
        {
            "paperId": "03629e41f60c60747477ed3cb1238b5dad5e69e3",
            "title": "Dynamic Query Optimization under Access Limitations and Dependencies",
            "abstract": "Unlike relational tables in a database, data sources on the Web typically can only be accessed in limited ways. In particular, some of the source fields may be required as input and thus need to be mandatorily filled in order to access the source. Answering queries over sources with access limitations is a complex task that requires a possibly recursive evaluation even when the query is non-recursive. After reviewing the main techniques for query answering in this context, in this article we consider the impact of functional and inclusion dependencies on dynamic query optimization under access limitations. In particular, we address the implication problem for functional dependencies and simple full-width inclusion dependencies, and prove that it can be decided in polynomial time. Then we provide necessary and sufficient conditions, based on the dependencies together with the data retrieved at a certain step of the query answering process, that allow avoiding unnecessary accesses to the sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "1733846",
                    "name": "Diego Calvanese"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "0b2a33315399027cdad3a3437d575835b2ff9899",
            "title": "Logic in databases: report on the LID 2008 workshop",
            "abstract": "Th e Lo gic in Da tab ase s (LI DO0 8) w ork sh op w as h eld at th eDI S d ep artm en t of OLa S ap ien za O u n ivers ity, R om e, Italy,b etw ee n M ay 19-2 0, 2008.LI DO 08 w as estab lish ed as a foru m for b rin gin g toget h er re-se arch ers an d p rac tition ers, fro m th e acad em ia an d th e in -d u stry , wh o are fo cu sin g on all logica l asp ec ts of d ata m an -agem en t.LI DO 08 w as a con s u en ce of th ree su cce ssfu l p ast ev en ts:\u00a5 LI DO 96,an in tern ation alw ork sh op on Lo gic in Da tab ases ,wh ich LI DO0 8 d eriv es its n am e fro m ;\u00a5 IIDB O06, an intern ation al w ork sh op on In con sisten cyan d In com p let en ess in Da tab ase s;\u00a5 LA A ICO 06 , an in tern ation al w orks h op on L og ical A s-p ec ts an d A p p lication s of In teg rity Co n stra ints.In ord er to gu ara n tee its con tinu ity, a S te erin g Co m m ittee,ch aired b y G eo rg G ottlob , w as fou n d ed ; its m em b ers are A n -d rea Ca l`o, Ja n Ch om ick i, H en n in g Ch ristian sen , La ks V .S .La k sh m an an , Da v id e M artin en gh i, Din o P ed res ch i, Jef Wi -jse n , an d Ca rlo Za n iolo.Th is w orks h op w as org an ized b y A n d rea C al`o, La ks V .S .La k sh m an an , an d Da v id e M artin en gh i; it attra ct ed 18 p a-p er su b m ission s ou t of wh ich 7 w ere sele ct ed for lon g p re -se n ta tion an d 6 for sh ort p re sen tation at th e w orksh op ; th ew ork sh op attra ct ed aro u n d 50 reg istered p articip an ts. De-tails an d p res en tation s are availab le at th e w orks h op W ebsite: http://conferenze.dei.polimi.it/lid2008/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "87172d4fce863958da368fc4c9bc30cd66df8099",
            "title": "Data-driven optimization of search service composition for answering multi-domain queries",
            "abstract": "multi-domain queries requires the combination of knowledge from various domains. Such queries are inadequately answered by general-purpose search engines, because domain- specific systems typically exhibit sophisticated knowledge about their own fields of expertise. Moreover, multi-domain queries typically require combining in the result domain knowledge possibly coming from multiple web resources, therefore conventional crawling and indexing techniques, based on individual pages, are not adequate. In this paper we present a conceptual framework for addressing the composition of search services for solving multi-domain queries. The approach consists in building an infrastructure for search service composition that leaves within each search system the responsibility of maintaining and improving its domain knowledge, and whose main challenge is to provide the \"glue\" between them; such glue is expressed in the format of joins upon search service results, and for this feature we regard our approach as \"data-driven\". We present an overall architecture, and the work that has been done so far in the development of some of the main modules.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38191345",
                    "name": "D. Barbieri"
                },
                {
                    "authorId": "1710630",
                    "name": "A. Bozzon"
                },
                {
                    "authorId": "144864244",
                    "name": "Daniele Braga"
                },
                {
                    "authorId": "40350773",
                    "name": "Marco Brambilla"
                },
                {
                    "authorId": "32452484",
                    "name": "A. Campi"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "2539248",
                    "name": "Emanuele Della Valle"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "1786155",
                    "name": "Michael Grossniklaus"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "2084128",
                    "name": "S. Ronchi"
                },
                {
                    "authorId": "1749128",
                    "name": "M. Tagliasacchi"
                }
            ]
        },
        {
            "paperId": "e391c558b12c2d0c45ca8107cbfaaf04a614e9b3",
            "title": "Database Integrity Checking",
            "abstract": "Integrity constraints (or simply \u201cconstraints\u201d) are formal representations of invariant conditions for the semantic cor-rectness of database records. Constraints can be expressed in declarative languages such as datalog, predicate logic, or SQL. This article highlights the historical background of integrity constraints and the essential features of their simplified incremental evaluation. It concludes with an outlook on future trends.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802431",
                    "name": "H. Decker"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "040487cb8b4027dd81eb0ca81a43f70c84201cca",
            "title": "A New Generation Search Engine Supporting Cross Domain Queries",
            "abstract": "A microwave heating apparatus has a body with a front wall, and a heating chamber within the body for holding an object to be heated. A microwave source is provided for generating microwaves, and a wave guide is connected between the microwave source and a microwave inlet for guiding the microwaves from the microwave source to the heating chamber. A radiant energy heater is disposed within the heating chamber for heating the object to be heated by radiant energy, and a power box is positioned above the chamber and housing the microwave source and the high voltage circuit components therefor. A microwave source cooling fan is provided in the power box, and thermal insulation covers the walls of the heating chamber, the upper surface of the thermal insulation on the top of the chamber and the bottom surface of the power box being spaced to provide a cooling conduit. The front wall of the body has an air suction inlet therein opening into one end of the cooling conduit, and a power box cooling fan is provided at the other end of the cooling conduit. The power box has a suction port therein opening into the cooling conduit upstream of the power box cooling fan for admitting air from the cooling conduit into the power box, the microwave source cooling fan and the power box cooling fan being in parallel flow relation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144864244",
                    "name": "Daniele Braga"
                },
                {
                    "authorId": "1733846",
                    "name": "Diego Calvanese"
                },
                {
                    "authorId": "32452484",
                    "name": "A. Campi"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "144896044",
                    "name": "F. Daniel"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "28a2cee04a45ff73881df6a39ae0c335f92ae27c",
            "title": "NGS: a framework for multi-domain query answering",
            "abstract": "If we consider a query involving multiple domains, such as \"find all database conferences held within six months in locations whose seasonal average temperature is 28degC and for which a cheap travel solution exists\", we note that (i) general- purpose search engines fail to answer multi-domain queries and (ii) specific search services may cover one of such domains, but no general integration framework is readily available. Currently, the only way to treat such cases is to separately query dedicated services and feed the result of one search as input to another, or to pairwise compare them by hand. This paper presents NGS, a framework providing fully automated support for cross-domain queries. In particular, NGS (a) integrates different kinds of services (search engines, web services, and wrapped web pages) into a global ontology, i.e., a unified view of the concepts supported by the available services, (b) covers query formulation aspects over the global ontology, and query rewriting in terms of the actual services, and (c) offers several optimization opportunities leveraging the characteristics of the different services at hand, based on several different cost metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144864244",
                    "name": "Daniele Braga"
                },
                {
                    "authorId": "1733846",
                    "name": "Diego Calvanese"
                },
                {
                    "authorId": "32452484",
                    "name": "A. Campi"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "144896044",
                    "name": "F. Daniel"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "34e2a4c6fee423493e50636955b04ff217d4a6aa",
            "title": "Mashing Up Search Services",
            "abstract": "Mashup languages offer new graphic interfaces for service composition. Normally, composition is limited to simple services, such as RSS or Atom feeds, but users can potentially use visual mashup languages for complex service compositions, with typed parameters and well-defined I/O interfaces. Composing search services introduces new issues, however, such as determining the optimal sequence of search invocations and separately composing ranked entries into a globally ranked result. Enabling end users to mash up services through suitable abstractions and tools is a viable option for improving service-based computations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144864244",
                    "name": "Daniele Braga"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "144896044",
                    "name": "F. Daniel"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "b3f98a3d4530c0acc2a34b21675daa0f2040f837",
            "title": "Classifying integrity checking methods with regard to inconsistency tolerance",
            "abstract": "We define and examine six classes of methods for integrity checking: case-based, compositional, relevance-based, simplification-based, total-integrity-dependent, and measure-based ones. Each, except the penultimate, corresponds to a particular form of inconsistency tolerance. Inconsistency measures provide a new approach to integrity checking and inconsistency tolerance. For many methods, proofs or disproofs of their inconsistency tolerance become easier and more transparent by our classification. In general, a better understanding of inconsistency-tolerant integrity checking is achieved",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802431",
                    "name": "H. Decker"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "c79cdb74eb23a521feb902f4cc4d63d662a257ff",
            "title": "Optimization of multi-domain queries on the web",
            "abstract": "Where can I attend an interesting database workshop close to a sunny beach? Who are the strongest experts on service computing based upon their recent publication record and accepted European projects? Can I spend an April weekend in a city served by a low-cost direct flight from Milano offering a Mahler's symphony? We regard the above queries as multi-domain queries, i.e., queries that can be answered by combining knowledge from two or more domains (such as: seaside locations, flights, publications, accepted projects, conference offerings, and so on). This information is available on the Web, but no general-purpose software system can accept the above queries nor compute the answer. At the most, dedicated systems support specific multi-domain compositions (e.g., Google-local locates information such as restaurants and hotels upon geographic maps). \n \nThis paper presents an overall framework for multi-domain queries on the Web. We address the following problems: (a) expressing multi-domain queries with an abstract formalism, (b) separating the treatment of \"search\" services within the model, by highlighting their differences from \"exact\" Web services, (c) explaining how the same query can be mapped to multiple \"query plans\", i.e., a well-defined scheduling of service invocations, possibly in parallel, which complies with their access limitations and preserves the ranking order in which search services return results; (d) introducing cross-domain joins as first-class operation within plans; (e) evaluating the query plans against several cost metrics so as to choose the most promising one for execution. This framework adapts to a variety of application contexts, ranging from end-user-oriented mash-up scenarios up to complex application integration scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144864244",
                    "name": "Daniele Braga"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "144896044",
                    "name": "F. Daniel"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "ef0d886a8308058e0180bc6ad00e829c96f22f1a",
            "title": "Querying Data under Access Limitations",
            "abstract": "Data sources on the web are often accessible through web interfaces that present them as relational tables, but require certain attributes to be mandatorily selected, e.g., via a web form. In a scenario where we integrate a set of such sources, and we pose queries over them, the values needed to access a source may have to be retrieved from other sources that are possibly not even mentioned in the query: answering queries at best can then be done only with a potentially recursive query plan that gets all obtainable answers to the query. Since data sources are typically distributed over a network, a major cost indicator for the execution of a query plan is the number of accesses to remote sources. In this paper we present an optimization technique for conjunctive queries that produces a query plan that: (1) minimizes the number of accesses according to a strong notion of minimality; (2) excludes all sources that are not relevant for the query. We introduce Toorjah, a prototype system that answers queries posed on sources with limitations by means of optimized query plans. Toorjah adopts a strategy that is aimed to retrieve answers as early as possible during query processing, and to present them to the user as they are computed. We provide experimental evidence of the effectiveness of our optimization, by showing the reduction of the number of accesses in a large number of cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "0c6adbe27b63b8a798f645e04499e9aff41814ce",
            "title": "Getting Rid of Straitjackets for Flexible Integrity Checking",
            "abstract": "Various requirements that usually are imposed on data, constraints, updates and methods for checking the integrity of databases can be perceived as inflexible straitjackets. We show that such restrictions can be notably relaxed or even completely abandoned without forfeiting any major advantage. On the contrary, a significant amount of flexibility is gained by relaxing or abandoning several established prerequisites for integrity checking.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802431",
                    "name": "H. Decker"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "126385aff49695f5f9a8874cee615ce16b50a0e3",
            "title": "Query Optimisation for Web Data Sources: Minimisation of the Number of Accesses",
            "abstract": "When relational data have access constraints that require certain attributes to be selected in queries, as in the case of (wrapped) Web sources accessible via forms, a recursive query plan is needed to answer queries at best. We present a query plan optimisation technique for several classes of queries that minimises the number of accesses according to a novel, strong notion of minimality. We provide experimental evidence of the effectiveness of our technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "2845927",
                    "name": "Domenico Carbotta"
                }
            ]
        },
        {
            "paperId": "eeb7633b6bfca1d28a74e8b154cd9d5479911ef2",
            "title": "Optimization of Query Plans in the presence of Access Limitations",
            "abstract": "We consider the problem of querying data sources that have limited capabilities and can thus only be accessed by complying with certain binding patterns for their attributes. This is often the case, e.g., in the context of data on the web queryable via web forms as well as in legacy data wrapped in relational tables. In such contexts, computing the answer to a user query cannot be done as in a traditional database; instead, a query plan is needed that takes the access limitations into",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "1733846",
                    "name": "Diego Calvanese"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "21c7249b15c17322d6e33de1e003144bd92e3e68",
            "title": "Can Integrity Tolerate Inconsistency?",
            "abstract": "An unconditional and hitherto unquestioned basic requirement for integrity checking is that the data need to be consistent before the update, such that the success of a simplified test can guarantee the invariance of integrity after the update. We answer the question whether this consistency requirement can be relaxed with \u201dYes, at least sometimes, and to some extent.\u201d",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802431",
                    "name": "H. Decker"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "3fb6b849c8958a931c91974221aeb7f05bfc5037",
            "title": "On Simplification of Database Integrity Constraints",
            "abstract": "Without proper simplification techniques, database integrity checking can be prohibitively time consuming. Several methods have been developed for producing simplified incremental checks for each update but none until now of sufficient quality and generality for providing a true practical impact, and the present paper is an attempt to fill this gap. On the theoretical side, a general characterization is introduced of the problem of simplification of integrity constraints and a natural definition is given of what it means for a simplification procedure to be ideal. We prove that ideality of simplification is strictly related to query containment; in fact, an ideal simplification pro-cedure can only exist in database languages for which query containment is decidable. However, simplifications that do not qualify as ideal may also be relevant for practical purposes. We present a concrete approach based on transformation operators that apply to integrity constraints written in a rich DATALOG-like language with negation. The resulting procedure produces, at design-time, simplified constraints for parametric transaction patterns, which can then be instantiated and checked for consistency at run-time. These tests take place before the execution of the update, so that only consistency-preserving updates are eventually given to the database. The extension to more expressive languages and the application of the framework to other contexts, such as data integration and concurrent database systems, are also discussed. Our experiments show that the simplifications obtained with our method may give rise to much better performance than with previous methods and that further improvements are achieved by checking consistency before executing the update.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145397935",
                    "name": "Henning Christiansen"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "98f704212ddca44ed3239fef53ec4d8492734a74",
            "title": "Avenues to Flexible Data Integrity Checking",
            "abstract": "Traditional methods for integrity checking in relational or deductive databases heavily rely on the assumption that data have integrity before the execution of updates. In this way, as has always been claimed, one can automatically derive strategies to check, in an incremental way, whether data preserve their integrity after the update. On the other hand, this consistency assumption greatly reduces applicability of such methods, since it is most often the case that small parts of a database do not comply with the integrity constraints, especially when the data are distributed or have been integrated from different sources. In this paper, we revisit integrity checking from an inconsistency-tolerant viewpoint. We show that most methods for integrity checking (though not all) are still applicable in the presence of inconsistencies and may be used to guarantee that the satisfied instances of the integrity constraints will continue to be satisfied after the update",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802431",
                    "name": "H. Decker"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "af41789cb87860ff46fea84d20a6d47aa374dbe3",
            "title": "Integrity Checking for Uncertain Data",
            "abstract": "The uncertainty associated to stored information can be put in direct correspondence to the extent to which these data violate conditions expressed as semantic integrity constraints. Thus, imposing and checking such constraints provides a better control over uncertain data. We present and discuss a condition which ensures the violation tolerance of methods for integrity checking. Usually, such methods are supposed to work correctly only if all constraints are satisfied before each update. Applied to express and check conditions about uncertain data, violation tolerance means that stored data the uncertainty of which violates integrity can be tolerated while updates can be safely checked for introducing violations of constraints about uncertainty. We also discuss the soundness and completeness of violation-tolerant integrity checking and assert it for several methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802431",
                    "name": "H. Decker"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "d6355c6b092ead351fedd047ef7af283c73db315",
            "title": "On Using Simplification and Correction Tables for Integrity Maintenance in Integrated Databases",
            "abstract": "When a database is defined as views over autonomous sources, inconsistencies with respect to global integrity constraints are to be expected. This paper investigates the possibility of using simplification techniques for integrity constraints in order to maintain, in an incremental way, a correction table of virtual updates which, if executed, would restore consistency; access can be made through auxiliary views that take the table into account. The approach employs assumptions about local source consistency as well as cross-source constraints whenever possible",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145397935",
                    "name": "Henning Christiansen"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "4c55033d6c5116247ae1a291308d2121d3481189",
            "title": "XQBE: the Swiss Army Knife for Semi-structured Data",
            "abstract": "The growing importance of XML calls for easier access to data management technologies, in order to provide domain experts who are inexperienced in database technologies with the possibility to directly query and transform domain specific data. Intuitiveness and simplicity are gained with the use of a graphical representation. The former is obtained by depicting the hierarchical XML data model as tree structures; the latter consists in considering only elements, attributes, and un-typed textual data. In this regard, the graphical framework of XQBE is a suitable tool for querying, transforming, and updating XML data as well as for specifying integrity constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144864244",
                    "name": "Daniele Braga"
                },
                {
                    "authorId": "32452484",
                    "name": "A. Campi"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "3103730",
                    "name": "Alessandro Raffio"
                },
                {
                    "authorId": "3347983",
                    "name": "Damiano Salvi"
                }
            ]
        },
        {
            "paperId": "93310006b31c76a2fe198c799e721914a7f3e723",
            "title": "Advanced Techniques for Efficient Data Integrity Checking",
            "abstract": "Integrity constraint checking, understood as the verification of data correctness and wellformedness conditions that must be satisfied in any state of a database, is not fully supported by current database technology. In a typical scenario, a database is required to comply with given semantic criteria (the integrity constraints) and to maintain the compliance each time data are updated. Since the introduction of the SQL2 standard, the SQL language started supporting assertions, which allow one to define general data consistency requirements expressing arbitrarily complex \u201cbusiness rules\u201d that may go beyond predefined constraints such as primary keys and foreign keys. General integrity constraints are, however, far from being widely available in commercial systems; in fact, their usage is commonly not encouraged, since the database management system would not be able to provide their incremental evaluation. Given the size of today\u2019s data repositories and the frequency at which updates may occur, any non-incremental approach, even for conditions whose complexity is only linear in the size of the database, may prove unfeasible in practice. Typically it is the database designer and the application programmer who take care of enforcing integrity via hand-coded pieces of programs that run either at the application level or within the DBMS (e.g., triggers). These solutions are, however, both difficult to maintain and error prone: small changes in a database schema may require subtle modifications in such programs. In this respect, database management systems need to be extended with means to verify, automatically and incrementally, that no violation of integrity is introduced by database updates. For this purpose we develop a procedure aimed at producing incremental checks whose satisfaction guarantees data integrity. A so-called simplification procedure takes in input a set of constraints and a pattern of updates to be executed on the data and outputs a set of optimized constraints which are as incremental as possible with respect to the hypothesis that the database is initially consistent. In particular, the proposed approach allows the compilation of incremental checks at database design time, thus without burdening database run time performance with expensive optimization operations. Furthermore, integrity verification may take place before the execution of the update, which means that the database will never reach illegal states and, thus, rollback as well as repair actions are virtually unneeded. The simplification process is unavoidably bound to a function that gives an approximate measure of the cost of evaluating the simplified constraints in actual database states and it is natural to characterize as optimal a simplification with a minimal cost. It is shown that, for any sensible cost function, no simplification procedure exists that returns optimal results in all cases. In spite of this negative result, that holds for the most general setting, important contexts can be found in which optimality can indeed always be guaranteed. Furthermore, non-optimal simplification may imply a slight loss of efficiency, but still is a great improvement with respect to non-incremental checking. Finally, we extend the applicability of simplification to a number of different contexts, such as recursive databases, concurrent database systems, data integration systems and XML document collections, and provide a performance evaluation of the proposed model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "603ae31cf9d14cf7f1ae83084522e4b40a72b3cc",
            "title": "Optimal Database Locks for Efficient Integrity Checking",
            "abstract": "In concurrent database systems, correctness of update transactions refers to the equivalent effects of the execution schedule and some serial schedule over the same set of transactions. Integrity constraints add further semantic requirements to the correctness of the database states reached upon the execution of update transactions. Several methods for efficient integrity checking and enforcing exist. We show in this paper how to apply one such method to automatically extend update transactions with locks and simplified consistency tests on the locked entities. All schedules produced in this way are conflict serializable and preserve consistency. For certain classes of databases we also guarantee that the amount of locked database entities is minimal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "c79bb2ad7b56bc0a8eb37efedea72223f84a6d4e",
            "title": "Symbolic constraints for meta-logic programming",
            "abstract": "Logic programming, with its declarative bias as well as unification and the direct representation of linguistic structures, is well qualified for meta-programming, i.e., programs working with representations of other programs as their data. However, constraint techniques seem necessary in order to fully exploit this paradigm. In the DEMOII system, the language of constraint handling rules (CHRs) has been used in order to provide a functionality that appears difficult to obtain without such means. For example, reversibility of a meta-interpreter, which can be obtained by means of constraints, turns it into a powerful program generator; in the same way, negation-as-failure implemented by means of constraints provides an incremental evaluation of integrity constraints. This paper focuses on the design of such constraints and their implementation by means of CHR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145397935",
                    "name": "Henning Christiansen"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        }
    ]
}