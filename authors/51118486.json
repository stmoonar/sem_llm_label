{
    "authorId": "51118486",
    "papers": [
        {
            "paperId": "770e43b95e5f294ce2593bc3c1e2f0c2baefc7a4",
            "title": "A Mathematical Framework, a Taxonomy of Modeling Paradigms, and a Suite of Learning Techniques for Neural-Symbolic Systems",
            "abstract": "The field of Neural-Symbolic (NeSy) systems is growing rapidly. Proposed approaches show great promise in achieving symbiotic unions of neural and symbolic methods. However, each NeSy system differs in fundamental ways. There is a pressing need for a unifying theory to illuminate the commonalities and differences in approaches and enable further progress. In this paper, we introduce Neural-Symbolic Energy-Based Models (NeSy-EBMs), a unifying mathematical framework for discriminative and generative modeling with probabilistic and non-probabilistic NeSy approaches. We utilize NeSy-EBMs to develop a taxonomy of modeling paradigms focusing on a system's neural-symbolic interface and reasoning capabilities. Additionally, we introduce a suite of learning techniques for NeSy-EBMs. Importantly, NeSy-EBMs allow the derivation of general expressions for gradients of prominent learning losses, and we provide four learning approaches that leverage methods from multiple domains, including bilevel and stochastic policy optimization. Finally, we present Neural Probabilistic Soft Logic (NeuPSL), an open-source NeSy-EBM library designed for scalability and expressivity, facilitating real-world application of NeSy systems. Through extensive empirical analysis across multiple datasets, we demonstrate the practical advantages of NeSy-EBMs in various tasks, including image classification, graph node labeling, autonomous vehicle situation awareness, and question answering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "2111682654",
                    "name": "Connor Pryor"
                },
                {
                    "authorId": "2279871869",
                    "name": "Changyu Gao"
                },
                {
                    "authorId": "2044198106",
                    "name": "Alon Albalak"
                },
                {
                    "authorId": "48082844",
                    "name": "Eriq Augustine"
                },
                {
                    "authorId": "2311459647",
                    "name": "William Wang"
                },
                {
                    "authorId": "2279835751",
                    "name": "Stephen Wright"
                },
                {
                    "authorId": "2279830000",
                    "name": "Lise Getoor"
                }
            ]
        },
        {
            "paperId": "8eda93fbde4b2dd941dd3c883937aa5fd338d103",
            "title": "Modeling Patterns for Neural-Symbolic Reasoning Using Energy-based Models",
            "abstract": "Neural-symbolic (NeSy) AI strives to empower machine learning and large language models with fast, reliable predictions that exhibit commonsense and trustworthy reasoning by seamlessly integrating neural and symbolic methods. With such a broad scope, several taxonomies have been proposed to categorize this integration, emphasizing knowledge representation, reasoning algorithms, and applications. We introduce a knowledge representation-agnostic taxonomy focusing on the neural-symbolic interface capturing methods that reason with probability, logic, and arithmetic constraints. Moreover, we derive expressions for gradients of a prominent class of learning losses and a formalization of reasoning and learning. Through a rigorous empirical analysis spanning three tasks, we show NeSy approaches reach up to a 37% improvement over neural baselines in a semi-supervised setting and a 19% improvement over GPT-4 on question-answering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "2111682654",
                    "name": "Connor Pryor"
                },
                {
                    "authorId": "2279830000",
                    "name": "Lise Getoor"
                }
            ]
        },
        {
            "paperId": "eaa80e21239dc7ab623e73d858a8807f8f3f0d06",
            "title": "Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning",
            "abstract": "We leverage convex and bilevel optimization techniques to develop a general gradient-based parameter learning framework for neural-symbolic (NeSy) systems. We demonstrate our framework with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across 8 datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "2279871869",
                    "name": "Changyu Gao"
                },
                {
                    "authorId": "2111682654",
                    "name": "Connor Pryor"
                },
                {
                    "authorId": "2279835751",
                    "name": "Stephen Wright"
                },
                {
                    "authorId": "2279830000",
                    "name": "Lise Getoor"
                }
            ]
        },
        {
            "paperId": "4a2008b8793d6ee8a14a95b2c21f30fe55720834",
            "title": "Simplifying Distributed Neural Network Training on Massive Graphs: Randomized Partitions Improve Model Aggregation",
            "abstract": "Distributed training of GNNs enables learning on massive graphs (e.g., social and e-commerce networks) that exceed the storage and computational capacity of a single machine. To reach performance comparable to centralized training, distributed frameworks focus on maximally recovering cross-instance node dependencies with either communication across instances or periodic fallback to centralized training, which create overhead and limit the framework scalability. In this work, we present a simplified framework for distributed GNN training that does not rely on the aforementioned costly operations, and has improved scalability, convergence speed and performance over the state-of-the-art approaches. Specifically, our framework (1) assembles independent trainers, each of which asynchronously learns a local model on locally-available parts of the training graph, and (2) only conducts periodic (time-based) model aggregation to synchronize the local models. Backed by our theoretical analysis, instead of maximizing the recovery of cross-instance node dependencies -- which has been considered the key behind closing the performance gap between model aggregation and centralized training -- , our framework leverages randomized assignment of nodes or super-nodes (i.e., collections of original nodes) to partition the training graph such that it improves data uniformity and minimizes the discrepancy of gradient and loss function across instances. In our experiments on social and e-commerce networks with up to 1.3 billion edges, our proposed RandomTMA and SuperTMA approaches -- despite using less training data -- achieve state-of-the-art performance and 2.31x speedup compared to the fastest baseline, and show better robustness to trainer failures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50077183",
                    "name": "Jiong Zhu"
                },
                {
                    "authorId": "8856206",
                    "name": "Aishwarya N. Reganti"
                },
                {
                    "authorId": "2057479333",
                    "name": "E-Wen Huang"
                },
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "145850291",
                    "name": "Nikhil S. Rao"
                },
                {
                    "authorId": "2691095",
                    "name": "Karthik Subbian"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                }
            ]
        },
        {
            "paperId": "181b954bf2d4cc327ddfd1c356aa427822bcf388",
            "title": "Order-Invariant Cardinality Estimators Are Differentially Private",
            "abstract": "We consider privacy in the context of streaming algorithms for cardinality estimation. We show that a large class of algorithms all satisfy $\\epsilon$-differential privacy, so long as (a) the algorithm is combined with a simple down-sampling procedure, and (b) the cardinality of the input stream is $\\Omega(k/\\epsilon)$. Here, $k$ is a certain parameter of the sketch that is always at most the sketch size in bits, but is typically much smaller. We also show that, even with no modification, algorithms in our class satisfy $(\\epsilon, \\delta)$-differential privacy, where $\\delta$ falls exponentially with the stream cardinality. Our analysis applies to essentially all popular cardinality estimation algorithms, and substantially generalizes and tightens privacy bounds from earlier works.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "32523323",
                    "name": "J. Thaler"
                },
                {
                    "authorId": "144722992",
                    "name": "Daniel Ting"
                }
            ]
        },
        {
            "paperId": "45645b392ce5cd914007b5ae2c572c0cd591e835",
            "title": "Emotion Recognition in Conversation using Probabilistic Soft Logic",
            "abstract": "Creating agents that can both appropriately respond to conversations and understand complex human linguistic tendencies and social cues has been a long standing challenge in the NLP community. A recent pillar of research revolves around emotion recognition in conversation (ERC); a sub-field of emotion recognition that focuses on conversations or dialogues that contain two or more utterances. In this work, we explore an approach to ERC that exploits the use of neural embeddings along with complex structures in dialogues. We implement our approach in a framework called Probabilistic Soft Logic (PSL), a declarative templating language that uses first-order like logical rules, that when combined with data, define a particular class of graphical model. Additionally, PSL provides functionality for the incorporation of results from neural models into PSL models. This allows our model to take advantage of advanced neural methods, such as sentence embeddings, and logical reasoning over the structure of a dialogue. We compare our method with state-of-the-art purely neural ERC systems, and see almost a 20% improvement. With these results, we provide an extensive qualitative and quantitative analysis over the DailyDialog conversation dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48082844",
                    "name": "Eriq Augustine"
                },
                {
                    "authorId": "3674787",
                    "name": "Pegah Jandaghi"
                },
                {
                    "authorId": "2044198106",
                    "name": "Alon Albalak"
                },
                {
                    "authorId": "2111682654",
                    "name": "Connor Pryor"
                },
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "2145535635",
                    "name": "William Wang"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                }
            ]
        },
        {
            "paperId": "4a1a0338634e1bd4c32eb289bb52fe0c7a409abc",
            "title": "NeuPSL: Neural Probabilistic Soft Logic",
            "abstract": "In this paper, we introduce Neural Probabilistic Soft Logic (NeuPSL), a novel neuro-symbolic (NeSy) framework that unites state-of-the-art symbolic reasoning with the low-level perception of deep neural networks. To model the boundary between neural and symbolic representations, we propose a family of energy-based models, NeSy Energy-Based Models, and show that they are general enough to include NeuPSL and many other NeSy approaches. Using this framework, we show how to seamlessly integrate neural and symbolic parameter learning and inference in NeuPSL. Through an extensive empirical evaluation, we demonstrate the benefits of using NeSy methods, achieving upwards of 30% improvement over independent neural network models. On a well-established NeSy task, MNIST-Addition, NeuPSL demonstrates its joint reasoning capabilities by outperforming existing NeSy approaches by up to 10% in low-data settings. Furthermore, NeuPSL achieves a 5% boost in performance over state-of-the-art NeSy methods in a canonical citation network task with up to a 40 times speed up.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111682654",
                    "name": "Connor Pryor"
                },
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "48082844",
                    "name": "Eriq Augustine"
                },
                {
                    "authorId": "2044198106",
                    "name": "Alon Albalak"
                },
                {
                    "authorId": "2145535635",
                    "name": "William Wang"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                }
            ]
        },
        {
            "paperId": "6ee39bd7166a0cdd5e2472c5d2f784bd2b3f4fe6",
            "title": "Visual Sudoku Puzzle Classification: A Suite of Collective Neuro-Symbolic Tasks",
            "abstract": "Neuro-symbolic computing (NeSy) is an emerging field that has the goal of integrating the low-level representational power of deep neural networks with high-level symbolic reasoning. Due to the youth of the field and the complexity of neuro-symbolic integration, there are few benchmarks that showcase the powers of NeSy, and even fewer built specifically with NeSy in mind. To address the lack of NeSy benchmarks, we introduce Visual Sudoku Puzzle Classification (ViSudo-PC). ViSudo-PC is a new NeSy benchmark dataset combining visual perception with relational constraints. The goal of the benchmark is to both highlight opportunities and elicit challenges. In addition to providing a new NeSy benchmark suite, we also provide an exploratory analysis that showcases ViSudo-PC\u2019s difficulty and possibilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48082844",
                    "name": "Eriq Augustine"
                },
                {
                    "authorId": "2111682654",
                    "name": "Connor Pryor"
                },
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "2634786",
                    "name": "J. Pujara"
                },
                {
                    "authorId": "1682479",
                    "name": "William Yang Wang"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                }
            ]
        },
        {
            "paperId": "8229bfb97e709abdd3c7da05e23811c4042f748e",
            "title": "Context-Aware Online Collective Inference for Templated Graphical Models",
            "abstract": "In this work, we examine online collective inference , the problem of maintaining and performing inference over a sequence of evolving graphical models. We utilize templated graphical models (TGM), a general class of graphical models expressed via templates and instantiated with data. A key challenge is minimizing the cost of instan-tiating the updated model. To address this, we de\ufb01ne a class of exact and approximate context-aware methods for updating an existing TGM. These methods avoid a full re-instantiation by us-ing the context of the updates to only add relevant components to the graphical model. Further, we provide stability bounds for the general online inference problem and regret bounds for a proposed approximation. Finally, we implement our approach in probabilistic soft logic, and test it on several online collective inference tasks. Through these experiments we verify the bounds on regret and stability, and show that our approximate on-line approach consistently runs two to \ufb01ve times faster than the of\ufb02ine alternative while, surprisingly, maintaining the quality of the predictions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "2111682654",
                    "name": "Connor Pryor"
                },
                {
                    "authorId": "48082844",
                    "name": "Eriq Augustine"
                },
                {
                    "authorId": "2109474521",
                    "name": "Alexander Miller"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                }
            ]
        },
        {
            "paperId": "0693863d771aaca4030f4b25b6e16131b99697aa",
            "title": "HyperFair: A Soft Approach to Integrating Fairness Criteria",
            "abstract": "Recommender systems are being employed across an increasingly diverse set of domains that can potentially make a significant social and individual impact. For this reason, considering fairness is a critical step in the design and evaluation of such systems. In this paper, we introduce HyperFair, a general framework for enforcing soft fairness constraints in a hybrid recommender system. HyperFair models integrate variations of fairness metrics as a regularization of a joint inference objective function. We implement our approach using probabilistic soft logic and show that it is particularly well-suited for this task as it is expressive and structural constraints can be added to the system in a concise and interpretable manner. We propose two ways to employ the methods we introduce: first as an extension of a probabilistic soft logic recommender system template; second as a fair retrofitting technique that can be used to improve the fairness of predictions from a black-box model. We empirically validate our approach by implementing multiple HyperFair hybrid recommenders and compare them to a state-of-the-art fair recommender. We also run experiments showing the effectiveness of our methods for the task of retrofitting a black-box model and the trade-off between the amount of fairness enforced and the prediction performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "51118486",
                    "name": "Charles Dickens"
                },
                {
                    "authorId": "2109455180",
                    "name": "Rishika Singh"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                }
            ]
        }
    ]
}