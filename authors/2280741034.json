{
    "authorId": "2280741034",
    "papers": [
        {
            "paperId": "0130f2fab75c30c26635be32fdafa6a03c35af6a",
            "title": "GraphStorm: all-in-one graph machine learning framework for industry applications",
            "abstract": "Graph machine learning (GML) is effective in many business applications. However, making GML easy to use and applicable to industry applications with massive datasets remain challenging. We developed GraphStorm, which provides an end-to-end solution for scalable graph construction, graph model training and inference. GraphStorm has the following desirable properties: (a) Easy to use: it can perform graph construction and model training and inference with just a single command; (b) Expert-friendly: GraphStorm contains many advanced GML modeling techniques to handle complex graph data and improve model performance; (c) Scalable: every component in GraphStorm can operate on graphs with billions of nodes and can scale model training and inference to different hardware without changing any code. GraphStorm has been used and deployed for over a dozen billion-scale industry applications after its release in May 2023. It is open-sourced in Github: https://github.com/awslabs/graphstorm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283934850",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2284037254",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2299062897",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "2284069643",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "2146354747",
                    "name": "Runjie Ma"
                },
                {
                    "authorId": "2280741034",
                    "name": "Houyu Zhang"
                },
                {
                    "authorId": "2255392614",
                    "name": "Zichen Wang"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "10429687",
                    "name": "Israt Nisa"
                },
                {
                    "authorId": "3125115",
                    "name": "Alejandro Mottini"
                },
                {
                    "authorId": "2305618086",
                    "name": "Qingjun Cui"
                },
                {
                    "authorId": "145344187",
                    "name": "H. Rangwala"
                },
                {
                    "authorId": "2305616323",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "24334d3666a7f3dfb33134ad74c931cf1a594604",
            "title": "SST: Semantic and Structural Transformers for Hierarchy-aware Language Models in E-commerce",
            "abstract": "Hierarchies are common structures used to organize data, such as e-commerce hierarchies associated with product data. With these product hierarchies, we aim to learn hierarchy-aware product text embeddings to improve fine-tuning performance on a variety of downstream e-commerce tasks. Existing methods leverage hierarchies by either aligning the text embeddings to separate hierarchical embeddings or by aligning the hierarchical information implicitly within a unified text Transformer. Although these models optimize to predict hierarchy information, performing further fine-tuning on new tasks is non-trivial. To bridge this gap, we propose a pre-training architecture to implicitly encode the hierarchy within the product text and then directly leverage a sub-set of the pre-training model during fine-tuning. Pre-training is done through Semantic and Structural Transformers (SST) where the Semantic-Transformer first encodes the product text into a contextual embedding, which is then used by the Structural-Transformer to infer the product\u2019s path in the hierarchy. Fine-tuning is done using only the initial Semantic-Transformer, now that hierarchy-aware text embeddings are learned. With this design, we eliminate the need of linking each fine-tuning dataset with corresponding hierarchies. This leads to fine-tuning performance improvements on critical e-commerce downstream tasks over the existing state-of-the-art hierarchy models, even when hierarchy data $is$ available during fine-tuning. Moreover, this improvement is consistent even after augmenting our baseline models to support fine-tuning. We conclude by discussing how such implicit structural encodings can be leveraged beyond the e-commerce domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51133706",
                    "name": "Karan Samel"
                },
                {
                    "authorId": "2280741034",
                    "name": "Houyu Zhang"
                },
                {
                    "authorId": "2280563037",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2281442930",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "2280574284",
                    "name": "Qing Ping"
                },
                {
                    "authorId": "2280550850",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "2280583879",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                }
            ]
        }
    ]
}