{
    "authorId": "2164214077",
    "papers": [
        {
            "paperId": "281327eef023d0689e2b90002c1c50902023296b",
            "title": "Blind Image Quality Index With Cross-Domain Interaction and Cross-Scale Integration",
            "abstract": "With the assistance of Convolutional Neural Networks (CNNs), Image Quality Assessment (IQA) models have made great progress in evaluating both simulated distortion and authentic distortion. However, most of the existing IQA models only learn the features of distorted images, and thus do not make full use of the available feature representation of other domains. Furthermore, the common multi-scale fusion strategies are relatively simple, such as downsampling and concatenating, which further limits the prediction performance. To this end, we propose a novel blind image quality index with cross-domain interaction and cross-scale integration, which is designed based on the combination of CNN and Transformer. First, the hierarchical spatial-domain and gradient-domain representations are obtained through a typical CNN architecture. Then, based on the proposed gradient-query cross-attention, these two types of features are fully interacted in the Cross-Domain Interaction (CDI) module. To represent the distortion information more comprehensively, the Cross-Scale Integration (CSI) module is proposed to combine the information between different scales progressively. Finally, the quality score is obtained through a simple regression module. The experimental results on five public IQA databases of both simulated and authentic scenes show that the proposed model outperforms the compared state-of-the-art metrics. In addition, cross-database experiments show that the proposed model has strong generalization performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46677837",
                    "name": "Bo Hu"
                },
                {
                    "authorId": "2232108027",
                    "name": "Guang Zhu"
                },
                {
                    "authorId": "2151529209",
                    "name": "Leida Li"
                },
                {
                    "authorId": null,
                    "name": "Ji Gan"
                },
                {
                    "authorId": "47113192",
                    "name": "Weisheng Li"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "3a3603601a28a635c4cad0a3e6bf8b2ff268fa45",
            "title": "Unconstrained Facial Expression Recognition With No-Reference De-Elements Learning",
            "abstract": "Most unconstrained facial expression recognition (FER) methods take original facial images as inputs to learn discriminative features by well-designed loss functions, which cannot reflect important visual information in faces. Although existing methods have explored the visual information of constrained facial expressions, there is no explicit modeling of what visual information is important for unconstrained FER. To find out valuable information of unconstrained facial expressions, we pose a new problem of no-reference de-elements learning: we decompose any unconstrained facial image into the facial expression element and a neutral face without the reference of corresponding neutral faces. Importantly, the element provides visualization results to understand important facial expression information and improves the discriminative power of features. Moreover, we propose a simple yet effective De-Elements Network (DENet) to learn the element and introduce appropriate constraints to overcome no ground truth of corresponding neutral faces during the de-elements learning. We extensively evaluate the proposed method on in-the-wild FER datasets including RAF-DB, AffectNet, SFEW and FERPlus. The comparable results show that our method is promising to improve classification performance and achieves equivalent performance compared with state-of-the-art methods. Also, we demonstrate the strong generalization performance on realistic occlusion and pose variation datasets and the cross-dataset evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118386010",
                    "name": "Hangyu Li"
                },
                {
                    "authorId": "144050305",
                    "name": "N. Wang"
                },
                {
                    "authorId": "1477973008",
                    "name": "Xi Yang"
                },
                {
                    "authorId": "2118777145",
                    "name": "Xiaoyu Wang"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "66feba0806db6d7045458b7aa4cdd3d1a4b65929",
            "title": "Disguised Heterogeneous Face Generation With Iterative-Adversarial Style Unification",
            "abstract": "Heterogeneous face recognition (HFR), which refers to matching face images with different modalities, is essential to public safety. Although HFR has made promising progress in recent years, disguised faces in HFR scenarios still remain a major challenge for the following reasons. First, most existing HFR methods focus on traditional scenarios without disguised accessories, and the performance degrades when dealing directly with disguised faces. Second, there is a need for disguised heterogeneous face datasets, which is essential for developing the related research community. Third, colorful accessories are distinct from heterogeneous face images in terms of their modalities, and their direct combination results in style inconsistency and poor quality. Therefore, we propose a disguised heterogeneous face generation method based on an iterative-adversarial style unification framework. Our approach aims to gradually learn frame textures to detail textures in multiple confrontation iterations, resulting in style unification for disguised accessories and heterogeneous faces. We also construct a disguised heterogeneous face dataset, which contains a disguised NIR-VIS subset and a disguised sketch-photo subset. Moreover, we provide benchmark evaluations conducted on our proposed dataset with face recognition and image quality assessment, demonstrating the superiority of our method over direct addition and two representative disguised face generation techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299758",
                    "name": "Chunlei Peng"
                },
                {
                    "authorId": "2240899903",
                    "name": "Zimo Kong"
                },
                {
                    "authorId": "4308702",
                    "name": "Decheng Liu"
                },
                {
                    "authorId": "144050305",
                    "name": "N. Wang"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "d809f5d5b881ac69d3ccb5bd6bf202f76c7120b6",
            "title": "Graph-Based Information Separator and Area Convolutional Network for EEG-Based Intention Decoding",
            "abstract": "In the current research on brain-computer interface (BCI), the electroencephalography (EEG) signal is usually only represented by a 2-D matrix, and the installation position of the EEG electrodes and the correlation between them are not considered. Actually, the cerebral cortex is a continuous potential surface and the information collected directly from each electrode is influenced by the other electrodes, so direct use of the raw data results in information redundancy. This article converts the EEG signal into a graph, and then creates an information separator (IS) based on the Laplace matrix of the graph to obtain the independent source information of electrode nodes, and propose an IS-based area convolutional network (IS-ACN). Integrating the proposed IS with some advanced methods, the experimental results show that the incorporation of IS can enhance the performance of these methods. By observing and tracking samples with abnormal noise in the BCI competition IV data set 2a, it is demonstrated that the proposed method can greatly reduce the influence of noise and effectively obtain the source features of EEG signals with low signal-to-noise ratio, and the average accuracy and kappa coefficient of the proposed IS-ACN on this data set are 80.59% and 74.1%, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2552652",
                    "name": "Xianlun Tang"
                },
                {
                    "authorId": "13802955",
                    "name": "Gengxin Liu"
                },
                {
                    "authorId": "2089750924",
                    "name": "Xin Deng"
                },
                {
                    "authorId": "2152351716",
                    "name": "Ke Liu"
                },
                {
                    "authorId": "9230586",
                    "name": "Yin Tian"
                },
                {
                    "authorId": "2156033936",
                    "name": "Huiming Wang"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "f8bc46944e4ab75baee77b57c1b6c2666b2fa129",
            "title": "MCS-GAN: A Different Understanding for Generalization of Deep Forgery Detection",
            "abstract": "After several years of development, deep synthesis technology has made significant progress in image and video synthesis. Deep forgery represented by Deepfakes has become a research hotspot, which is used as a tool for disinformation attacks. The current strongly discriminative models can have good performance on specific datasets, even close to 100% accuracy. Unfortunately, since a specific discriminative method only fits a specific data distribution, and different forgery methods or datasets have different data distributions. These methods fail to achieve high performance in cross-dataset detection. In response to this problem and focusing on the actual situation, we adjust the strong generalization detection across the dataset to the generalization detection of unseen fake video. We propose Multi-Crise-Cross Attention and StyleGANv2 Generative Adversarial Network (MCS-GAN). Firstly, we built a Generative Adversarial Network (GAN) framework to learn the distribution of real face data and generate corresponding face images. Secondly, to break the high stitch between the fake region and the background, the model needs to have strong enough feature analysis and pixel restoration capabilities. Therefore, we propose a generator consisting of a Multi-Crise-Cross-Attention (MC) encoder and a StyleGANv2 (SG2) decoder. Finally, to avoid the situation where as long as a face is normal or different faces are abnormal, we set a latent space encoding discriminator and increase the ratio of latent space vector, so as to detect anomaly generated by the forgery operation acting on latent space. We conduct some model generalization experiments on videos on the Internet and some popular deepfake databases. The results show that the accuracy of our method is better compared with the best methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112869738",
                    "name": "Shuai Xiao"
                },
                {
                    "authorId": "2124702432",
                    "name": "Guipeng Lan"
                },
                {
                    "authorId": "3021550",
                    "name": "Jiachen Yang"
                },
                {
                    "authorId": "145440137",
                    "name": "Wen Lu"
                },
                {
                    "authorId": "2167019145",
                    "name": "Qinggang Meng"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "0df05c7f0aeb10484cb678ab9b7fd9ae46af3a9f",
            "title": "MLNet: A Multi-Domain Lightweight Network for Multi-Focus Image Fusion",
            "abstract": "Existing multi-focus image fusion (MFIF) methods are difficult to achieve satisfactory results in both fusion performance and rate simultaneously. The spatial domain methods are hard to determine the focus/defocus boundary (FDB), and the transform domain methods are likely to damage the content information of the source images. Moreover, the deep learning-based MFIF methods are usually confronted with low rate due to complex models and enormous learnable parameters. To address these issues, we propose a multi-domain lightweight network (MLNet) for MFIF, which can achieve competitive results in both performance and rate. The proposed MLNet mainly includes three modules, namely focus extraction (FE), focus measure (FM) and image fusion (IF). In the interpretable FE module, the image features extracted by discrete cosine transform-based convolution (DCTConv) and local binary pattern-based convolution (LBPConv) are concatenated and fed into the FM module. DCTConv based on transform domain takes DCT coefficients to construct a fixed convolution kernel without parameter learning, which can effectively capture the high/low frequency content of the image. LBPConv based on spatial domain can achieve structure features and gradient information from source images. In the FM module, a 3-layer 1 \u00d7 1 convolution with a few learnable parameters is employed to generate the initial decision map, which has the properties of flexible input. The fused image is obtained by the IF module according to the final decision map. In terms of quantitative and qualitative evaluations, extensive experiments validate that the proposed method outperforms existing state-of-the-art methods on three public datasets. In addition, the proposed MLNet contains only 0.01 M parameters, which is 0.2% of the first CNN-based MFIF method [25].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111840862",
                    "name": "Xixi Nie"
                },
                {
                    "authorId": "1632599353",
                    "name": "Boxia Hu"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "0e0e103a2a1ed0241df544928d09d2a6b55c766e",
            "title": "Cross-Modality Person Re-identification with Memory-Based Contrastive Embedding",
            "abstract": "Visible-infrared person re-identification (VI-ReID) aims to retrieve the person images of the same identity from the RGB to infrared image space, which is very important for real-world surveillance system. In practice, VI-ReID is more challenging due to the heterogeneous modality discrepancy, which further aggravates the challenges of traditional single-modality person ReID problem, i.e., inter-class confusion and intra-class variations. In this paper, we propose an aggregated memory-based cross-modality deep metric learning framework, which benefits from the increasing number of learned modality-aware and modality-agnostic centroid proxies for cluster contrast and mutual information learning. Furthermore, to suppress the modality discrepancy, the proposed cross-modality alignment objective simultaneously utilizes both historical and up-to-date learned cluster proxies for enhanced cross-modality association. Such training mechanism helps to obtain hard positive references through increased diversity of learned cluster proxies, and finally achieves stronger ``pulling close'' effect between cross-modality image features. Extensive experiment results demonstrate the effectiveness of the proposed method, surpassing state-of-the-art works significantly by a large margin on the commonly used VI-ReID datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168159692",
                    "name": "De Cheng"
                },
                {
                    "authorId": "2145747649",
                    "name": "Xiaolong Wang"
                },
                {
                    "authorId": "144050305",
                    "name": "N. Wang"
                },
                {
                    "authorId": "2118452829",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2118777145",
                    "name": "Xiaoyu Wang"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "13e9a3e37988010f2fbdc4db72d6cbe381199c58",
            "title": "Graph Embedding Contrastive Multi-Modal Representation Learning for Clustering",
            "abstract": "Multi-modal clustering (MMC) aims to explore complementary information from diverse modalities for clustering performance facilitating. This article studies challenging problems in MMC methods based on deep neural networks. On one hand, most existing methods lack a unified objective to simultaneously learn the inter- and intra-modality consistency, resulting in a limited representation learning capacity. On the other hand, most existing processes are modeled for a finite sample set and cannot handle out-of-sample data. To handle the above two challenges, we propose a novel Graph Embedding Contrastive Multi-modal Clustering network (GECMC), which treats the representation learning and multi-modal clustering as two sides of one coin rather than two separate problems. In brief, we specifically design a contrastive loss by benefiting from pseudo-labels to explore consistency across modalities. Thus, GECMC shows an effective way to maximize the similarities of intra-cluster representations while minimizing the similarities of inter-cluster representations at both inter- and intra-modality levels. So, the clustering and representation learning interact and jointly evolve in a co-training framework. After that, we build a clustering layer parameterized with cluster centroids, showing that GECMC can learn the clustering labels with given samples and handle out-of-sample data. GECMC yields superior results than 14 competitive methods on four challenging datasets. Codes and datasets are available: https://github.com/xdweixia/GECMC.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100681690",
                    "name": "Wei Xia"
                },
                {
                    "authorId": "48469895",
                    "name": "Tianxiu Wang"
                },
                {
                    "authorId": "38469552",
                    "name": "Quanxue Gao"
                },
                {
                    "authorId": "2909406",
                    "name": "Ming Yang"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "18df6c47dd6ff7dbd36fb94553be3fe8f0905b8b",
            "title": "Dual Conditional Normalization Pyramid Network for Face Photo-Sketch Synthesis",
            "abstract": "Face photo-sketch synthesis has undergone remarkable progress with the rapid development of deep learning techniques. Cutting-edge methods directly learn the cross-domain mapping between photos and sketches, which ignores the available reference samples. We argue that the reference samples can provide adequate prior information on texture and content in this task and improve the visual performance of synthetic images. This paper proposes a Dual Conditional Normalization Pyramid (DCNP) network with a multi-scale pyramid structure. The core of the DCNP network is a Dual Conditional Normalization (DCN) based architecture, which can obtain prior information on different semantics from reference samples. Specifically, DCN contains two conditional normalization branches. The first branch allows for spatially-adaptive normalization of the reference image conditioned on the semantic mask of the input image. The second branch enables adaptive instance normalization of the input image conditioned on the reference image. DCN can emphasize the isolated importance of textural and spatial factors by disintegrating the entire cross-domain mapping into two branches. To avoid information redundancy and improve the final performance, we propose a Gated Channel Attention Fusion (GCAF) module to distill and fuse the helpful information of the two branches. Qualitative and quantitative experimental results demonstrate the superior performance of the proposed method over the state-of-the-art approaches in structural information preservation and realistic texture generation. The code is public in https://github.com/Tony0720/DCNP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152186704",
                    "name": "Mingrui Zhu"
                },
                {
                    "authorId": "2210943718",
                    "name": "Zicheng Wu"
                },
                {
                    "authorId": "144050305",
                    "name": "N. Wang"
                },
                {
                    "authorId": "2200083189",
                    "name": "Heng Yang"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "229a351592d115ae93b095e3e19afad688004c28",
            "title": "Cross-Modal Enhancement Network for Multimodal Sentiment Analysis",
            "abstract": "Multimodal sentiment analysis (MSA) plays an important role in many applications, such as intelligent question-answering, computer-assisted psychotherapy and video understanding, and has attracted considerable attention in recent years. It leverages multimodal signals including verbal language, facial gestures, and acoustic behaviors to identify sentiments in videos. Language modality typically outperforms nonverbal modalities in MSA. Therefore, strengthening the significance of language in MSA will be a vital way to promote recognition accuracy. Considering that the meaning of a sentence often varies in different nonverbal contexts, combining nonverbal information with text representations is conducive to understanding the exact emotion conveyed by an utterance. In this paper, we propose a Cross-modal Enhancement Network (CENet) model to enhance text representations by integrating visual and acoustic information into a language model. Specifically, it embeds a Cross-modal Enhancement (CE) module, which enhances each word representation according to long-range emotional cues implied in unaligned nonverbal data, into a transformer-based pre-trained language model. Moreover, a feature transformation strategy is introduced for acoustic and visual modalities to reduce the distribution differences between the initial representations of verbal and nonverbal modalities, thereby facilitating the fusion of distinct modalities. Extensive experiments on benchmark datasets demonstrate the significant gains of CENet over state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119264226",
                    "name": "Di Wang"
                },
                {
                    "authorId": "2171358684",
                    "name": "Shuai Liu"
                },
                {
                    "authorId": "2117950114",
                    "name": "Quan Wang"
                },
                {
                    "authorId": "2207890",
                    "name": "Yu-min Tian"
                },
                {
                    "authorId": "1753049",
                    "name": "Lihuo He"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        }
    ]
}