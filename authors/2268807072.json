{
    "authorId": "2268807072",
    "papers": [
        {
            "paperId": "4f015f1a144940193de5aa4687ad58e2ffcbbfb1",
            "title": "Multi-Modal Generative AI: Multi-modal LLM, Diffusion and Beyond",
            "abstract": "Multi-modal generative AI has received increasing attention in both academia and industry. Particularly, two dominant families of techniques are: i) The multi-modal large language model (MLLM) such as GPT-4V, which shows impressive ability for multi-modal understanding; ii) The diffusion model such as Sora, which exhibits remarkable multi-modal powers, especially with respect to visual generation. As such, one natural question arises: Is it possible to have a unified model for both understanding and generation? To answer this question, in this paper, we first provide a detailed review of both MLLM and diffusion models, including their probabilistic modeling procedure, multi-modal architecture design, and advanced applications to image/video large language models as well as text-to-image/video generation. Then, we discuss the two important questions on the unified model: i) whether the unified model should adopt the auto-regressive or diffusion probabilistic modeling, and ii) whether the model should utilize a dense architecture or the Mixture of Experts(MoE) architectures to better support generation and understanding, two objectives. We further provide several possible strategies for building a unified model and analyze their potential advantages and disadvantages. We also summarize existing large-scale multi-modal datasets for better model pretraining in the future. To conclude the paper, we present several challenging future directions, which we believe can contribute to the ongoing advancement of multi-modal generative AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2191043236",
                    "name": "Hong Chen"
                },
                {
                    "authorId": "2256599610",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2261888564",
                    "name": "Yuwei Zhou"
                },
                {
                    "authorId": "2268807072",
                    "name": "Bin Huang"
                },
                {
                    "authorId": "2129509567",
                    "name": "Yipeng Zhang"
                },
                {
                    "authorId": "2260613991",
                    "name": "Wei Feng"
                },
                {
                    "authorId": "2261888448",
                    "name": "Houlun Chen"
                },
                {
                    "authorId": "2118690469",
                    "name": "Zeyang Zhang"
                },
                {
                    "authorId": "1993657480",
                    "name": "Siao Tang"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "17ff6a0844afe74796022e7aaf372553e9303d72",
            "title": "VTimeLLM: Empower LLM to Grasp Video Moments",
            "abstract": "Large language models (LLMs) have shown remarkable text understanding capabilities, which have been ex-tended as Video LLMs to handle video data for compre-hending visual details. However, existing Video LLMs can only provide a coarse description of the entire video, failing to capture the precise start and end time bound-ary of specific events. In this paper, we solve this issue via proposing VTimeLLM, a novel Video LLM designed for fine-grained video moment understanding and reasoning with respect to time boundary. Specifically, our VTimeLLM adopts a boundary-aware three-stage training strategy, which respectively utilizes image-text pairs for feature alignment, multiple-event videos to increase temporal-boundary awareness, and high-quality video-instruction tuning to further improve temporal understanding ability as well as align with human intents. Extensive experiments demonstrate that in fine-grained time-related comprehension tasks for videos such as Temporal Video Grounding and Dense Video Captioning, VTimeLLM significantly outperforms existing Video LLMs. Besides, benefits from the fine-grained temporal understanding of the videos further enable VTimeLLM to beat existing Video LLMs in video di-alogue benchmark, showing its superior cross-modal understanding and reasoning abilities. 11Our project page is at https://github.com/huangb23/VTimeLLM",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268807072",
                    "name": "Bin Huang"
                },
                {
                    "authorId": "2256599610",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2191043236",
                    "name": "Hong Chen"
                },
                {
                    "authorId": "2261934586",
                    "name": "Zihan Song"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "c73389370d524190aa675601056a7705c92a859a",
            "title": "Global-Local GraphFormer: Towards Better Understanding of User Intentions in Sequential Recommendation",
            "abstract": "Transformer-based model has gained great success in the multimedia sequential recommendation task due to its strong ability to handle sequential data. However, existing Transformer-based models regard the items in the sequential data as a user-specific fully-connected graph (local graph) and only explicitly consider the temporal information in the local graph to capture the users\u2019 intentions, ignoring the fact that the user-item bipartite graph (global graph) may carry important relation patterns to the sequential items. Additionally, it is still unclear whether (and how) the information hidden in the global graphs can help the Transformer-based models better understand the users\u2019 sequential behavior according to the current literature. To investigate this important problem, we propose to utilize the global graph information to help the Transformer-based sequential recommendation, where the information from different modalities, i.e., user-item interactions in the global graph and the temporal patterns in the historical sequences, are taken into account jointly. In concrete, we propose two Global-Local (GL) GraphFormer models for utilizing both the global graph and local temporal information. One GL-GraphFormer is able to gift the Transformer-based model with both first- and second-order graph information through two specifically designed encodings. The other GL-GraphFormer transfers higher-order graph information into the local Transformer with pretrained Graph Neural Networks (GNNs). Extensive experiments on several real-world datasets demonstrate that i) our proposed GL-GraphFormers can bring substantial improvement over baseline methods, and ii) the benefits of different orders of global graph information vary with the dataset sparsity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2191043236",
                    "name": "Hong Chen"
                },
                {
                    "authorId": "2268807072",
                    "name": "Bin Huang"
                },
                {
                    "authorId": "2256599610",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2261888564",
                    "name": "Yuwei Zhou"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "79301de035c8d15f751abcd78106a4e8e2bab845",
            "title": "Acceleration Speed Optimization of Intelligent EVs in Consideration of Battery Aging",
            "abstract": "A speed optimization framework is presented to minimize both the battery aging and the energy consumption for intelligent electric vehicles (EVs) during acceleration process. To describe quantitatively the battery capacity loss and to optimize in real-life driving conditions, a control-oriented battery life model is derived. Then, the formulation of the speed optimization during an acceleration process is established and solved by sequential quadratic programming (SQP) algorithm. Several simulations are conducted in the case that an intelligent EV speeds up to the specified speed considering different road slopes. The results during an acceleration process from 0 to 100\u00a0km/h within 14 s show that the loss of battery capacity can be reduced by 9.6% while the battery energy consumption is merely increased by 1.6% compared with the strategy that only considers the optimal energy consumption.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2154868",
                    "name": "B. Gao"
                },
                {
                    "authorId": "115032802",
                    "name": "Lulu Guo"
                },
                {
                    "authorId": "2114173491",
                    "name": "Q. Zheng"
                },
                {
                    "authorId": "2268807072",
                    "name": "Bin Huang"
                },
                {
                    "authorId": "70637868",
                    "name": "Hong Chen"
                }
            ]
        }
    ]
}