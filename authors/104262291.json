{
    "authorId": "104262291",
    "papers": [
        {
            "paperId": "2d7ae8ffb61e9952ddbc6ca24cedbf9f4741fd63",
            "title": "MUSENET: Multi-Scenario Learning for Repeat-Aware Personalized Recommendation",
            "abstract": "Personalized recommendation has been instrumental in many real applications. Despite the great progress, the underlying multi-scenario characteristics (e.g., users may behave differently under different scenarios) are largely ignored by existing recommender systems. Intuitively, modeling different scenarios properly could significantly improve the recommendation accuracy, and some existing work has explored this direction. However, these work assumes the scenarios are explicitly given, and thus becomes less effective when such information is unavailable. To complicate things further, proper scenario modeling from data is challenging and the recommendation models may easily overfit to some scenarios. In this paper, we propose a multi-scenario learning framework, MUSENET, for personalized recommendation. The key idea of MUSENET is to learn multiple implicit scenarios from the user behaviors, with a careful design inspired by the causal interpretation of recommender systems to avoid the overfitting issue. Additionally, since users' repeat consumptions account for a large part of the user behavior data on many e-commerce platforms, a repeat-aware mechanism is integrated to handle users' repurchase intentions within each scenario. Comprehensive experimental results on both industrial and public datasets demonstrate the effectiveness of the proposed approach compared with the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104262291",
                    "name": "Senrong Xu"
                },
                {
                    "authorId": "2145728192",
                    "name": "Liangyue Li"
                },
                {
                    "authorId": "46461580",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "8157135",
                    "name": "Zulong Chen"
                },
                {
                    "authorId": "2180430182",
                    "name": "Han Wu"
                },
                {
                    "authorId": "2117522225",
                    "name": "Quan Lu"
                },
                {
                    "authorId": "2058143613",
                    "name": "H. Tong"
                }
            ]
        },
        {
            "paperId": "00fcfa3e36065bf0e6c11abda0553325600f0855",
            "title": "Detecting Topology Attacks against Graph Neural Networks",
            "abstract": "Graph neural networks (GNNs) have been widely used in many real applications, and recent studies have revealed their vulnerabilities against topology attacks. To address this issue, existing efforts have mainly been dedicated to improving the robustness of GNNs, while little attention has been paid to the detection of such attacks. In this work, we study the victim node detection problem under topology attacks against GNNs. Our approach is built upon the key observation rooted in the intrinsic message passing nature of GNNs. That is, the neighborhood of a victim node tends to have two competing group forces, pushing the node classification results towards the original label and the targeted label, respectively. Based on this observation, we propose to detect victim nodes by deliberately designing an effective measurement of the neighborhood variance for each node. Extensive experimental results on four real-world datasets and five existing topology attacks show the effectiveness and efficiency of the proposed detection approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104262291",
                    "name": "Senrong Xu"
                },
                {
                    "authorId": "46461580",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "2145728192",
                    "name": "Liangyue Li"
                },
                {
                    "authorId": "2150081274",
                    "name": "Wei Yang"
                },
                {
                    "authorId": "102467100",
                    "name": "F. Xu"
                },
                {
                    "authorId": "8163721",
                    "name": "Hanghang Tong"
                }
            ]
        },
        {
            "paperId": "634718948c0c212a80a0b43942225a1647b0389f",
            "title": "On the Vulnerability of Graph Learning-based Collaborative Filtering",
            "abstract": "Graph learning-based collaborative filtering (GLCF), which is built upon the message-passing mechanism of graph neural networks (GNNs), has received great recent attention and exhibited superior performance in recommender systems. However, although GNNs can be easily compromised by adversarial attacks as shown by the prior work, little attention has been paid to the vulnerability of GLCF. Questions like can GLCF models be just as easily fooled as GNNs remain largely unexplored. In this article, we propose to study the vulnerability of GLCF. Specifically, we first propose an adversarial attack against CLCF. Considering the unique challenges of attacking GLCF, we propose to adopt the greedy strategy in searching for the local optimal perturbations and design a reasonable attacking utility function to handle the non-differentiable ranking-oriented metrics. Next, we propose a defense to robustify GCLF. The defense is based on the observation that attacks usually introduce suspicious interactions into the graph to manipulate the message-passing process. We then propose to measure the suspicious score of each interaction and further reduce the message weight of suspicious interactions. We also give a theoretical guarantee of its robustness. Experimental results on three benchmark datasets show the effectiveness of both our attack and defense.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104262291",
                    "name": "Senrong Xu"
                },
                {
                    "authorId": "2145728192",
                    "name": "Liangyue Li"
                },
                {
                    "authorId": "15401196",
                    "name": "Zenan Li"
                },
                {
                    "authorId": "46461580",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "102467100",
                    "name": "F. Xu"
                },
                {
                    "authorId": "8157135",
                    "name": "Zulong Chen"
                },
                {
                    "authorId": "2117522225",
                    "name": "Quan Lu"
                },
                {
                    "authorId": "8163721",
                    "name": "Hanghang Tong"
                }
            ]
        }
    ]
}