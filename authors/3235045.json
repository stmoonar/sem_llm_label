{
    "authorId": "3235045",
    "papers": [
        {
            "paperId": "98f56460bc7e3aada663ca77a9b393176815be26",
            "title": "Roman Open Data: a data visualization & exploratory interface for the academic training of university students",
            "abstract": "- Within the framework of the Archeology degree course at the University of Barcelona, a series of practical activities are proposed to provide students with a series of techniques and knowledge that they can apply in future work and research. It was Antonio Aguilera, one of the original developers of the CEIPAC amphoric epigraphy database, who promoted its use as part of a hypothetical activity where university students were faced with the discovery of an inscription on ceramics in an archaeological excavation. This would give them experience of managing the digital corpus that is essential to read and understand the Latin epigraphic texts, as well as knowledge of where to find parallels related to the inscription. This concomitance would enable them to date the archaeological strata, to understand the type of product that reaches the excavation, as well as the distribution of the food containers. Here the practice is transferred to the Roman Open Data portal (https://romanopendata.eu), an exploratory interface developed within the framework of the European EPNet project, which allows geolocation of epigraphic searches on a map, consulting \u2013 through the use of computational tools to be able to analyze epigraphic data on amphorae \u2013 members of the academic community, while promoting the principles and practices of Open Science in the context of Digital Humanities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145081008",
                    "name": "J. Lorenzo"
                },
                {
                    "authorId": "108152555",
                    "name": "Jordi P\u00e9rez Gonz\u00e1lez"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "2165085882",
                    "name": "Antonio Aguilera Mart\u00edn"
                }
            ]
        },
        {
            "paperId": "e80aba67e92aec3ef06ed31193832c6f81e9a2ed",
            "title": "INODE",
            "abstract": "A full-fledged data exploration system must combine different access modalities with a powerful concept of guiding the user in the exploration process, by being reactive and anticipative both for data discovery and for data linking. Such systems are a real opportunity for our community to cater to users with different domain and data science expertise. We introduce INODE - an end-to-end data exploration system - that leverages, on the one hand, Machine Learning and, on the other hand, semantics for the purpose of Data Management (DM). Our vision is to develop a classic unified, comprehensive platform that provides extensive access to open datasets, and we demonstrate it in three significant use cases in the fields of Cancer Biomarker Research, Research and Innovation Policy Making, and Astrophysics. INODE offers sustainable services in (a) data modeling and linking, (b) integrated query processing using natural language, (c) guidance, and (d) data exploration through visualization, thus facilitating the user in discovering new insights. We demonstrate that our system is uniquely accessible to a wide range of users from larger scientific communities to the public. Finally, we briefly illustrate how this work paves the way for new research opportunities in DM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "2463025",
                    "name": "Fr\u00e9d\u00e9ric B. Bastian"
                },
                {
                    "authorId": "1726022751",
                    "name": "Theofilos Belmpas"
                },
                {
                    "authorId": "3075644",
                    "name": "Martin Braschler"
                },
                {
                    "authorId": "151240818",
                    "name": "Ursin Brunner"
                },
                {
                    "authorId": "2072276517",
                    "name": "D. Calvanese"
                },
                {
                    "authorId": "14469238",
                    "name": "M. Fabricius"
                },
                {
                    "authorId": "1726045725",
                    "name": "Orest Gkini"
                },
                {
                    "authorId": "2072251887",
                    "name": "Catherine Kosten"
                },
                {
                    "authorId": "1796236",
                    "name": "D. Lanti"
                },
                {
                    "authorId": "40469553",
                    "name": "Antonis Litke"
                },
                {
                    "authorId": "1403433652",
                    "name": "Hendrik L\u00fccke-Tieke"
                },
                {
                    "authorId": "1970432",
                    "name": "F. Massucci"
                },
                {
                    "authorId": "1725471",
                    "name": "T. M. Farias"
                },
                {
                    "authorId": "1806182",
                    "name": "A. Mosca"
                },
                {
                    "authorId": "2072251867",
                    "name": "Francesco Multari"
                },
                {
                    "authorId": "145679608",
                    "name": "N. Papadakis"
                },
                {
                    "authorId": "153516669",
                    "name": "D. Papadopoulos"
                },
                {
                    "authorId": "2061639772",
                    "name": "Yogendra Patil"
                },
                {
                    "authorId": "2072259106",
                    "name": "Aur\u00e9lien Personnaz"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "2723088",
                    "name": "A. Sima"
                },
                {
                    "authorId": "153437025",
                    "name": "Ellery Smith"
                },
                {
                    "authorId": "1807115",
                    "name": "Dimitrios Skoutas"
                },
                {
                    "authorId": "2075402169",
                    "name": "S. Subramanian"
                },
                {
                    "authorId": "2055961473",
                    "name": "G. Xiao"
                },
                {
                    "authorId": "2113917675",
                    "name": "Kurt Stockinger"
                }
            ]
        },
        {
            "paperId": "1fe6f51fdb1079f14609c82a503d482c00fa078a",
            "title": "UNiCS: The Open Data Platform for Research and Innovation",
            "abstract": "The paper introduces UNiCS, the open platform for Research and Innovation data management. The UNiCS platform follows the Ontology-Based Data Access (OBDA) approach, which eases the access to a vast amount of heterogeneous data and offers the final users the possibility to formulate queries using terms from the knowledge domain they are experts in. In UNiCS, each query gets transformed in a set of optimised queries to different data sources. Moreover, the OBDA approach makes the semantics of the data explicit, thus offering an intuitive way to access, explore, visualise, analyse, and post-process them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51260446",
                    "name": "Xavi Gimenez"
                },
                {
                    "authorId": "1806182",
                    "name": "A. Mosca"
                },
                {
                    "authorId": "144440593",
                    "name": "Fernando Roda"
                },
                {
                    "authorId": "6050259",
                    "name": "B. Rondelli"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                }
            ]
        },
        {
            "paperId": "7627a5509a8ee274ea799ecb7d595582ae07d4c8",
            "title": "The OBDA-Based \"Observatory of Research and Innovation\" of the Tuscany Region",
            "abstract": "The Tuscany\u2019s Observatory of Research and Innovation portal is an instrument to promote more transparent and inclusive governance in the region. We show its interactive dashboard and underlying SPARQL endpoint, powered by SIRIS Academic\u2019s UNiCS platform, which integrates Open Data on the Higher Education & Research field, following the Ontology-Based Data Access approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1806182",
                    "name": "A. Mosca"
                },
                {
                    "authorId": "6050259",
                    "name": "B. Rondelli"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                }
            ]
        },
        {
            "paperId": "28804824439a0cc04ed8f437de11e72fd7ba8029",
            "title": "GROM: a General Rewriter of Semantic Mappings",
            "abstract": "We present GROM, a tool conceived to handle high-level schema mappings between semantic descriptions of a source and a target database. GROM rewrites mappings between the virtual, view-based semantic schemas, in terms of mappings between the two physical databases, and then executes them. The system serves the purpose of teaching two main lessons. First, designing mappings among higher-level descriptions is often simpler than working with the original schemas. Second, as soon as the view-definition language becomes more expressive, to handle, for example, negation, the mapping problem becomes extremely challenging from the technical viewpoint, so that one needs to find a proper trade-off between expressiveness and scalability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785690",
                    "name": "G. Mecca"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "145530628",
                    "name": "Donatello Santoro"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                }
            ]
        },
        {
            "paperId": "70511e2dc1ce044d34132fe18d246325582c1e74",
            "title": "TINTIN: a Tool for INcremental INTegrity checking of Assertions in SQL Server",
            "abstract": "We present TINTIN, a tool to perform efficient integrity checking of SQL assertions in SQL Server. TINTIN rewrites each assertion into a set of standard SQL queries that, given a set of insertions and deletions of tuples, allow to incrementally compute whether this update violates the assertion or not. If one of such queries returns a non empty answer, then the assertion is violated. Efficiency is achieved by evaluating only those data and those assertions that can actually be violated according to the update. TINTIN is aimed at two different purposes. First, to show the feasibility of our approach by implementing it on a commercial relational DBMS. Second, to illustrate that the efficiency we achieve is good enough for making assertions to be used in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072500893",
                    "name": "Xavier Oriol"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                }
            ]
        },
        {
            "paperId": "ea14cf91c42650c152f7ec482dfdb5fb3ba55c9e",
            "title": "Rewriting Ontology-Based Mappings",
            "abstract": "Data translation consists of the task of moving data from a source database to a target database. This task is usually performed by developing mappings, i.e. executable transformations from the source to the target schema. However, a richer description of the target database semantics may be available in the form of an ontology. This is typically defined as a set of views over the base tables that provides a unified conceptual view of the underlying data. We investigate how the mapping process changes when such a rich conceptualization of the target database is available. We develop a translation algorithm that automatically rewrites a mapping from the source schema to the target ontology into an equivalent mapping from the source to the target databases. Then, we show how to handle this problem when an ontology is available also for the source. Differently from previous approaches, the language we use in view definitions has the full power of non-recursive Datalog with negation. In the paper, we study the implications of adopting such an expressive language. Experiments are conducted to illustrate the trade-off between expressibility of the view language and efficiency of the chase engine used to perform the data exchange.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785690",
                    "name": "G. Mecca"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "145530628",
                    "name": "Donatello Santoro"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                }
            ]
        },
        {
            "paperId": "f85bdca290abb5b25d047272f292417f067eb283",
            "title": "A \u2018historical case\u2019 of Ontology-Based Data Access",
            "abstract": "Historical research has steadily been adopting semantic technologies to tackle several recent problems in the field, such as making explicit the semantics contained in the historical sources, formalising them and linking them. Over the last decades, in social sciences and humanities an immense amount of new quantifiable data have been accumulated and made available in interchangeable formats, opening up new possibilities for solving old questions and posing new ones. This paper introduces a web-based platform to ease the access of scholars to historical and cultural data distributed across different data sources. The approach relies on the Ontology-Based Data Access (OBDA) paradigm, where the different datasets are virtually integrated by a conceptual layer (an ontology). This work is focused on investigating the mechanisms and characteristics of the food production and commercial trade system during the Roman Empire.",
            "fieldsOfStudy": [
                "Geography",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1733846",
                    "name": "Diego Calvanese"
                },
                {
                    "authorId": "1806182",
                    "name": "A. Mosca"
                },
                {
                    "authorId": "145726280",
                    "name": "Jos\u00e9 Remesal"
                },
                {
                    "authorId": "2511068",
                    "name": "Mart\u00edn Rezk"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                }
            ]
        },
        {
            "paperId": "0a629b573d0fbf8589942770a3929fc8e813b228",
            "title": "Query containment in entity SQL",
            "abstract": "We describe a software architecture we have developed for a constructive containment checker of Entity SQL queries defined over extended ER schemas expressed in Microsoft's Entity Data Model. Our application of interest is compilation of object-to-relational mappings for Microsoft's ADO.NET Entity Framework, which has been shipping since 2007. The supported language includes several features which have been individually addressed in the past but, to the best of our knowledge, they have not been addressed all at once before. Moreover, when embarking on an implementation, we found no guidance in the literature on how to modularize the software or apply published algorithms to a commercially-supported language. This paper reports on our experience in addressing these real-world challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "1737944",
                    "name": "P. Bernstein"
                },
                {
                    "authorId": "34701823",
                    "name": "I. Santos"
                },
                {
                    "authorId": "2208580",
                    "name": "Yannis Katsis"
                },
                {
                    "authorId": "143973671",
                    "name": "S. Melnik"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                }
            ]
        },
        {
            "paperId": "49561879e8eddce1d51700e7e38da2e90c3c7218",
            "title": "Validation of schema mappings with nested queries",
            "abstract": "With the emergence of the Web and the wide use of XML for representing data, \n the ability to map not only flat relational but also nested data has become \n crucial. The design of schema mappings is a semi-automatic process. A human \n designer is needed to guide the process, choose among mapping candidates, and \n successively refine the mapping. The designer needs a way to figure out \n whether the mapping is what was intended. Our approach to mapping validation \n allows the designer to check whether the mapping satisfies certain desirable \n properties. In this paper, we focus on the validation of mappings between \n nested relational schemas, in which the mapping assertions are either \n inclusions or equalities of nested queries. We focus on the nested relational \n setting since most XML\u2019s Document Type Definitions (DTDs) can be represented \n in this model. We perform the validation by reasoning on the schemas and \n mapping definition. We take into account the integrity constraints defined on \n both the source and target schema. We consider constraints and mapping\u2019s \n queries which may contain arithmetic comparisons and negations. This class of \n mapping scenarios is significantly more expressive than the ones addressed by \n previous work on nested relational mapping validation. We encode the given \n mapping scenario into a single flat database schema, so we can take advantage \n of our previous work on validating flat relational mappings, and reformulate \n each desirable property check as a query satisfiability problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "145120696",
                    "name": "Carles Farr\u00e9"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                },
                {
                    "authorId": "2316210",
                    "name": "Toni Urp\u00ed"
                }
            ]
        },
        {
            "paperId": "c5e8e60d20707e46f360d4ec546dd3a56c7d28f4",
            "title": "Query Containment in Entity SQL (Extended Abstract)",
            "abstract": "We describe a software architecture we have developed for a constructive containment checker of Entity SQL queries defined over extended ER schemas expressed in Microsoft\u2019s Entity Data Model. Our application of interest is compilation of object-torelational mappings for Microsoft's ADO.NET Entity Framework, which has been shipping since 2007. The supported language includes several features which have been individually addressed in the past but, to the best of our knowledge, they have not been addressed all at once before. Moreover, when embarking on an implementation, we found no guidance in the literature on how to modularize the software or apply published algorithms to a commercially-supported language. This paper reports on our experience in addressing these real-world challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "1737944",
                    "name": "P. Bernstein"
                },
                {
                    "authorId": "34701823",
                    "name": "I. Santos"
                },
                {
                    "authorId": "2208580",
                    "name": "Yannis Katsis"
                },
                {
                    "authorId": "143973671",
                    "name": "S. Melnik"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                }
            ]
        },
        {
            "paperId": "f0194949e708ca0dffd124dc5707fd18e269e7c9",
            "title": "Incremental mapping compilation in an object-to-relational mapping system",
            "abstract": "In an object-to-relational mapping system (ORM), mapping expressions explain how to expose relational data as objects and how to store objects in tables. If mappings are sufficiently expressive, then it is possible to define lossy mappings. If a user updates an object, stores it in the database based on a lossy mapping, and then retrieves the object from the database, the user might get a different result than the updated state of the object; that is, the mapping might not \"roundtrip.\" To avoid this, the ORM should validate that user-defined mappings roundtrip the data. However, this problem is NP-hard, so mapping validation can be very slow for large or complex mappings.\n We circumvent this problem by developing an incremental compiler for OR mappings. Given a validated mapping, a modification to the object schema is compiled into incremental modifications of the mapping. We define the problem formally, present algorithms to solve it for Microsoft's Entity Framework, and report on an implementation. For some mappings, incremental compilation is over 100 times faster than a full mapping compilation, in one case dropping from 8 hours to 50 seconds.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1737944",
                    "name": "P. Bernstein"
                },
                {
                    "authorId": "40076282",
                    "name": "Marie Jacob"
                },
                {
                    "authorId": "144022533",
                    "name": "Jorge P\u00e9rez"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "3055107",
                    "name": "James F. Terwilliger"
                }
            ]
        },
        {
            "paperId": "5b9113ff2f21f29ab1f613692c9de6cf86e1334b",
            "title": "MVT: a schema mapping validation tool",
            "abstract": "Schema mappings define relationships between schemas in a declarative way. We demonstrate MVT, a mapping validation tool that allows the designer to ask whether the mapping has certain desirable properties. The answers to these questions will provide information on whether the mapping adequately matches the intended needs and requirements. MVT is able to deal with a highly expressive class of mappings and database schemas, which allows the use of negations, order comparisons and null values. The tool does not only provide a Boolean answer as test result, but also a feedback for that result. Depending on the tested property and on the test result, the provided feedback can be in the form of example schema instances, or in the form of an explanation, that is, highlighting the mapping assertions and schema constraints responsible for getting such a result.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "145120696",
                    "name": "Carles Farr\u00e9"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                },
                {
                    "authorId": "2316210",
                    "name": "Toni Urp\u00ed"
                }
            ]
        },
        {
            "paperId": "9200bb98afe2b493771542eafc9f11e4293cde44",
            "title": "SVTe: a tool to validate database schemas giving explanations",
            "abstract": "We present SVTe, a tool to validate relational database schemas that allows performing several tests to determine the correctness of a schema. When a test is satisfied, our tool provides an example of a database state satisfying the test. Otherwise, it gives an explanation. We describe the main functionalities of SVTe, its architecture and provide some experimental results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145120696",
                    "name": "Carles Farr\u00e9"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                },
                {
                    "authorId": "2316210",
                    "name": "Toni Urp\u00ed"
                }
            ]
        },
        {
            "paperId": "fcb41138db021b5d549c98fc1d6788331b679200",
            "title": "Computing explanations for unlively queries in databases",
            "abstract": "A query is unlively if it always returns an empty answer. Debugging a database schema requires not only determining unlively queries, but also fixing them. To the best of our knowledge, the existing methods do not provide the designer with an explanation of why a query is not lively. In this paper, we propose a method for computing explanations that is independent of the particular method used to determine liveliness. It provides three levels of search: one explanation, a maximal set of non-overlapping explanations, and all explanations. The first two levels require only a linear number of calls to the underlying method. We also propose a filter to reduce the number of these calls, and experimentally compare our method with the best known method for finding unsatisfiable subsets of constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "145120696",
                    "name": "Carles Farr\u00e9"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                },
                {
                    "authorId": "2316210",
                    "name": "Toni Urp\u00ed"
                }
            ]
        }
    ]
}