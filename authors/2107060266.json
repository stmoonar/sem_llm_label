{
    "authorId": "2107060266",
    "papers": [
        {
            "paperId": "3ec0e2c64ecbfd895d0f580ac5b8004398b4b50f",
            "title": "Federated Ensemble Learning: Increasing the Capacity of Label Private Recommendation Systems",
            "abstract": ".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31255332",
                    "name": "Meisam Hejazinia"
                },
                {
                    "authorId": "68973739",
                    "name": "Dzmitry Huba"
                },
                {
                    "authorId": "2909360",
                    "name": "Ilias Leontiadis"
                },
                {
                    "authorId": "10995410",
                    "name": "Kiwan Maeng"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "145557680",
                    "name": "Luca Melis"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "3490923",
                    "name": "Milad Nasr"
                },
                {
                    "authorId": "2150030991",
                    "name": "Kaikai Wang"
                },
                {
                    "authorId": "2797270",
                    "name": "Carole-Jean Wu"
                }
            ]
        },
        {
            "paperId": "43602d85c69fd64c62592fc3a332a5b2eeb2c98b",
            "title": "FEL: High Capacity Learning for Recommendation and Ranking via Federated Ensemble Learning",
            "abstract": "Federated learning (FL) has emerged as an effective approach to address consumer privacy needs. FL has been successfully applied to certain machine learning tasks, such as training smart keyboard models and keyword spotting. Despite FL's initial success, many important deep learning use cases, such as ranking and recommendation tasks, have been limited from on-device learning. One of the key challenges faced by practical FL adoption for DL-based ranking and recommendation is the prohibitive resource requirements that cannot be satisfied by modern mobile systems. We propose Federated Ensemble Learning (FEL) as a solution to tackle the large memory requirement of deep learning ranking and recommendation tasks. FEL enables large-scale ranking and recommendation model training on-device by simultaneously training multiple model versions on disjoint clusters of client devices. FEL integrates the trained sub-models via an over-arch layer into an ensemble model that is hosted on the server. Our experiments demonstrate that FEL leads to 0.43-2.31% model quality improvement over traditional on-device federated learning - a significant improvement for ranking and recommendation system use cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31255332",
                    "name": "Meisam Hejazinia"
                },
                {
                    "authorId": "68973739",
                    "name": "Dzmitry Huba"
                },
                {
                    "authorId": "2909360",
                    "name": "Ilias Leontiadis"
                },
                {
                    "authorId": "10995410",
                    "name": "Kiwan Maeng"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "145557680",
                    "name": "Luca Melis"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "3490923",
                    "name": "Milad Nasr"
                },
                {
                    "authorId": "2150030991",
                    "name": "Kaikai Wang"
                },
                {
                    "authorId": "2797270",
                    "name": "Carole-Jean Wu"
                }
            ]
        },
        {
            "paperId": "38fca90fef15b226dbae8db408c5efe8066dde1c",
            "title": "Federated Learning with Buffered Asynchronous Aggregation",
            "abstract": "Scalability and privacy are two critical concerns for cross-device federated learning (FL) systems. In this work, we identify that synchronous FL - synchronized aggregation of client updates in FL - cannot scale efficiently beyond a few hundred clients training in parallel. It leads to diminishing returns in model performance and training speed, analogous to large-batch training. On the other hand, asynchronous aggregation of client updates in FL (i.e., asynchronous FL) alleviates the scalability issue. However, aggregating individual client updates is incompatible with Secure Aggregation, which could result in an undesirable level of privacy for the system. To address these concerns, we propose a novel buffered asynchronous aggregation method, FedBuff, that is agnostic to the choice of optimizer, and combines the best properties of synchronous and asynchronous FL. We empirically demonstrate that FedBuff is 3.3x more efficient than synchronous FL and up to 2.5x more efficient than asynchronous FL, while being compatible with privacy-preserving technologies such as Secure Aggregation and differential privacy. We provide theoretical convergence guarantees in a smooth non-convex setting. Finally, we show that under differentially private training, FedBuff can outperform FedAvgM at low privacy settings and achieve the same utility for higher privacy settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2131399147",
                    "name": "John Nguyen"
                },
                {
                    "authorId": "40243893",
                    "name": "Kshitiz Malik"
                },
                {
                    "authorId": "19277351",
                    "name": "Hongyuan Zhan"
                },
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2066127975",
                    "name": "Michael G. Rabbat"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "68973739",
                    "name": "Dzmitry Huba"
                }
            ]
        },
        {
            "paperId": "4ace0211031f9ef79c70fbeca9e07dc121be90ff",
            "title": "Antipodes of Label Differential Privacy: PATE and ALIBI",
            "abstract": "We consider the privacy-preserving machine learning (ML) setting where the trained model must satisfy differential privacy (DP) with respect to the labels of the training examples. We propose two novel approaches based on, respectively, the Laplace mechanism and the PATE framework, and demonstrate their effectiveness on standard benchmarks. While recent work by Ghazi et al. proposed Label DP schemes based on a randomized response mechanism, we argue that additive Laplace noise coupled with Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover, we show how to achieve very strong privacy levels in some regimes, with our adaptation of the PATE framework that builds on recent advances in semi-supervised learning. We complement theoretical analysis of our algorithms' privacy guarantees with empirical evaluation of their memorization properties. Our evaluation suggests that comparing different algorithms according to their provable DP guarantees can be misleading and favor a less private algorithm with a tighter analysis. Code for implementation of algorithms and memorization attacks is available from https://github.com/facebookresearch/label_dp_antipodes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "2107059646",
                    "name": "I. Shilov"
                },
                {
                    "authorId": "2444919",
                    "name": "Florian Tram\u00e8r"
                }
            ]
        },
        {
            "paperId": "b87b5797b661d6137301dd3167d573a94493936e",
            "title": "Papaya: Practical, Private, and Scalable Federated Learning",
            "abstract": "Cross-device Federated Learning (FL) is a distributed learning paradigm with several challenges that differentiate it from traditional distributed learning, variability in the system characteristics on each device, and millions of clients coordinating with a central server being primary ones. Most FL systems described in the literature are synchronous - they perform a synchronized aggregation of model updates from individual clients. Scaling synchronous FL is challenging since increasing the number of clients training in parallel leads to diminishing returns in training speed, analogous to large-batch training. Moreover, stragglers hinder synchronous FL training. In this work, we outline a production asynchronous FL system design. Our work tackles the aforementioned issues, sketches of some of the system design challenges and their solutions, and touches upon principles that emerged from building a production FL system for millions of clients. Empirically, we demonstrate that asynchronous FL converges faster than synchronous FL when training across nearly one hundred million devices. In particular, in high concurrency settings, asynchronous FL is 5x faster and has nearly 8x less communication overhead than synchronous FL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "68973739",
                    "name": "Dzmitry Huba"
                },
                {
                    "authorId": "2131399147",
                    "name": "John Nguyen"
                },
                {
                    "authorId": "40243893",
                    "name": "Kshitiz Malik"
                },
                {
                    "authorId": "3252651",
                    "name": "Ruiyu Zhu"
                },
                {
                    "authorId": "2066127975",
                    "name": "Michael G. Rabbat"
                },
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2797270",
                    "name": "Carole-Jean Wu"
                },
                {
                    "authorId": "19277351",
                    "name": "Hongyuan Zhan"
                },
                {
                    "authorId": "2139735162",
                    "name": "Pavel Ustinov"
                },
                {
                    "authorId": "47490833",
                    "name": "H. Srinivas"
                },
                {
                    "authorId": "2150030991",
                    "name": "Kaikai Wang"
                },
                {
                    "authorId": "2139734446",
                    "name": "Anthony Shoumikhin"
                },
                {
                    "authorId": "2146537748",
                    "name": "Jesik Min"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                }
            ]
        },
        {
            "paperId": "bea1187a1f8a68f1a93f0c2fa10d31f93a30f84e",
            "title": "Opacus: User-Friendly Differential Privacy Library in PyTorch",
            "abstract": "We introduce Opacus, a free, open-source PyTorch library for training deep learning models with differential privacy (hosted at opacus.ai). Opacus is designed for simplicity, flexibility, and speed. It provides a simple and user-friendly API, and enables machine learning practitioners to make a training pipeline private by adding as little as two lines to their code. It supports a wide variety of layers, including multi-head attention, convolution, LSTM, GRU (and generic RNN), and embedding, right out of the box and provides the means for supporting other user-defined layers. Opacus computes batched per-sample gradients, providing higher efficiency compared to the traditional\"micro batch\"approach. In this paper we present Opacus, detail the principles that drove its implementation and unique features, and benchmark it against other frameworks for training models with differential privacy as well as standard PyTorch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2107059646",
                    "name": "I. Shilov"
                },
                {
                    "authorId": "2319225824",
                    "name": "Alexandre Sablayrolles"
                },
                {
                    "authorId": "1389630028",
                    "name": "Davide Testuggine"
                },
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "2131399147",
                    "name": "John Nguyen"
                },
                {
                    "authorId": "2129469303",
                    "name": "Sayan Gosh"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "2024689450",
                    "name": "Jessica Zhao"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                }
            ]
        }
    ]
}