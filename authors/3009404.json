{
    "authorId": "3009404",
    "papers": [
        {
            "paperId": "749418ec951e87b5ef14eef0cf48da22d40b31a2",
            "title": "Sales Forecasting for Pricing Strategies Based on Time Series and Learning Techniques",
            "abstract": ": Time series exist in a wide variety of domains, such as market prices, healthcare and agriculture. Mod-elling time series data enables forecasting, anomaly detection, and data exploration. Few studies compare technologies and methodologies in the context of time series analysis, and existing tools are often limited in functionality. This paper focuses on the formulation and refinement of pricing strategies in mass retail, based on learning methods for sales forecasting and evaluation. The aim is to support BOOPER, a French startup specializing in pricing solutions for the retail sector. We focus on the strategy where each model is refined for a single product, studying both ensemble and parametric techniques as well as deep learning. To use these methods a hyperparameter setting is needed. The aim of this study is to provide an overview of the sensitivity of product sales to price fluctuations and promotions. The aim is also, to adapt existing methods using optimized machine and deep learning models, such as the Temporal Fusion Transformer (TFT) and the Temporal Convolutional Network (TCN), to capture the behaviour of each product. The idea is to improve their performance and adapt them to the specific requirements. We therefore provide an overview and experimental study of product learning models for each dataset, enabling informed decisions to be made about the most appropriate model and tool for each case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2289738499",
                    "name": "Jean-Christophe Ricklin"
                },
                {
                    "authorId": "2294885566",
                    "name": "Ines Ben Amor"
                },
                {
                    "authorId": "2294885909",
                    "name": "Ra\u00efd Mansi"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1878146",
                    "name": "H. B. Zghal"
                }
            ]
        },
        {
            "paperId": "ac01a7b2721e57b6f4469e769a38e46d00116f13",
            "title": "HybEA: Hybrid Attention Models for Entity Alignment",
            "abstract": "The proliferation of Knowledge Graphs (KGs) that support a wide variety of applications, like entity search, question answering and recommender systems, has led to the need for identifying overlapping information among different KGs. Entity Alignment (EA) is the problem of detecting such overlapping information among KGs that refer to the same real-world entities. Recent works have shown a great potential in exploiting KG embeddings for the task of EA, with most works focusing on the structural representation of entities (i.e., entity neighborhoods) in a KG and some works also exploiting the available factual information of entities (e.g., their names and associated literal values). However, real-word KGs exhibit high levels of structural and semantic heterogeneity, making EA a challenging task in which most existing methods struggle to achieve good results. In this work, we propose HybEA, an open-source EA method that focuses on both structure and facts, using two separate attention-based models. Our experimental results show that HybEA outperforms state-of-the-art methods by at least 5% and as much as 20+% (with an average difference of 11+%) Hits@1, in 5 widely used benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20596647",
                    "name": "N. Fanourakis"
                },
                {
                    "authorId": "2309477306",
                    "name": "Fatia Lekbour"
                },
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "2309478208",
                    "name": "Guillaume Renton"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "af34a9ff474d53a291c023df7e55fcfbbb08c4e7",
            "title": "AutoML for Explainable Anomaly Detection (XAD)",
            "abstract": "Numerous algorithms have been proposed for detecting anomalies (outliers, novelties) in an unsupervised manner. Unfortunately, it is not trivial, in general, to understand why a given sample (record) is labelled as an anomaly and thus diagnose its root causes. We propose the following reduced-dimensionality, surrogate model approach to explain detector decisions: approximate the detection model with another one that employs only a small subset of features. Subsequently, samples can be visualized in this low-dimensionality space for human understanding. To this end, we develop PROTEUS , an AutoML pipeline to produce the surrogate model, specifically designed for feature selection on imbalanced datasets. The PROTEUS surrogate model can not only explain the training data, but also the out-of-sample (unseen) data. In other words, PROTEUS produces predictive explanations by approximating the decision surface of an unsupervised detector. PROTEUS is designed to return an accurate estimate of out-of-sample predictive performance to serve as a metric of the quality of the approximation. Computational experiments confirm the efficacy of PROTEUS to produce predictive explanations for different families of detectors and to reliably estimate their predictive performance in unseen data. Unlike several ad-hoc feature importance methods, PROTEUS is robust to high-dimensional data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055402500",
                    "name": "Nikolaos Myrtakis"
                },
                {
                    "authorId": "2284494086",
                    "name": "Ioannis Tsamardinos"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "b44ac7d1f3ecc9e0f753625716dfa23e66e126c0",
            "title": "Do We Really Need Imputation in AutoML Predictive Modeling?",
            "abstract": "Numerous real-world data contain missing values, while in contrast, most Machine Learning (ML) algorithms assume complete datasets. For this reason, several imputation algorithms have been proposed to predict and fill in the missing values. Given the advances in predictive modeling algorithms tuned in an Automated Machine Learning context (AutoML) setting, a question that naturally arises is to what extent sophisticated imputation algorithms (e.g., Neural Network based) are really needed, or we can obtain a descent performance using simple methods like Mean/Mode (MM). In this article, we experimentally compare six state-of-the-art representatives of different imputation algorithmic families from an AutoML predictive modeling perspective, including a feature selection step and combined algorithm and hyper-parameter selection. We used a commercial AutoML tool for our experiments, in which we included the selected imputation methods. Experiments ran on 25 binary classification real-world incomplete datasets with missing values and 10 binary classification complete datasets in which synthetic missing values are introduced according to different missingness mechanisms, at varying missing frequencies. The main conclusion drawn from our experiments is that the best method on average is the Denoise AutoEncoder on real-world datasets and the MissForest in simulated datasets, followed closely by MM. In addition, binary indicator variables encoding missingness patterns actually improve predictive performance, on average. Last, although there are cases where Neural-Network-based imputation significantly improves predictive performance, this comes at a great computational cost and requires measuring all feature values to impute new samples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256804295",
                    "name": "G. Paterakis"
                },
                {
                    "authorId": "1491912491",
                    "name": "S. Fafalios"
                },
                {
                    "authorId": "3137932",
                    "name": "Paulos Charonyktakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2284494086",
                    "name": "Ioannis Tsamardinos"
                }
            ]
        },
        {
            "paperId": "b21471bf20222466daae14136cd358304a1957ba",
            "title": "Structural Bias in Knowledge Graphs for the Entity Alignment Task",
            "abstract": ". Knowledge Graphs (KGs) have recently gained attention for representing knowledge about a particular domain and play a central role in a multitude of AI tasks like recommendations and query answering. Recent works have revealed that KG embedding methods used to implement these tasks often exhibit direct forms of bias (e.g., related to gender, nationality, etc.) leading to discrimination. In this work, we are interested in the impact of indirect forms of bias related to the structural diversity of KGs in entity alignment (EA) tasks. In this respect, we propose an exploration-based sampling algorithm, SUSIE, that generates challenging benchmark data for EA methods, with respect to structural diversity. SUSIE requires setting the value of a single hyperparam-eter, which affects the connectivity of the generated KGs. The generated samples exhibit similar characteristics to some of the most challenging real-world KGs for EA tasks. Using our sampling, we demonstrate that state-of-the-art EA methods, like RREA, RDGCN, MultiKE and PARIS, exhibit different robustness to structurally diverse input KGs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20596647",
                    "name": "N. Fanourakis"
                },
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "1781993",
                    "name": "E. Pitoura"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                }
            ]
        },
        {
            "paperId": "4b9048ee0a33ffa37d73a2507a554e4e6543fe6a",
            "title": "Scheduling Continuous Operators for IoT edge Analytics with Time Constraints",
            "abstract": "Data stream processing and analytics (DSPA) engines are used to extract in (near) real-time valuable information from multiple IoT data streams. Deploying DSPA applications at the IoT network edge through Edge/Fog architectures is currently one of the core challenges for reducing both network delays and network bandwidth usage to reach the Cloud. In this paper, we address the problem of scheduling continuous DSPA operators to Fog-Cloud nodes featuring both computational and network resources. We are paying particular attention to the dynamic workload of these nodes due to variability of IoT data stream rates and the sharing of nodes' resources by multiple DSPA applications. In this respect, we propose TSOO, a resource-aware and time-efficient heuristic algorithm that takes into account the limited Fog computational resources, the real-time response constraints of DSPA applications, as well as, congestion and delay issues on Fog-to-Cloud network resources. Via extensive simulation experiments, we show that TSOO approximates an optimal operators' placement with a low execution cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5817238",
                    "name": "Patient Ntumba"
                },
                {
                    "authorId": "1708457",
                    "name": "N. Georgantas"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "7c640a85c26145218df952fd3fd14e9f269376cd",
            "title": "SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for Exposing Deepfakes",
            "abstract": "Modern deepfake detectors have achieved encouraging results, when training and test images are drawn from the same data collection. However, when these detectors are applied to images produced with unknown deepfake-generation techniques, considerable performance degradations are commonly observed. In this paper, we propose a novel deepfake detector, called SeeABLE, that formalizes the detection problem as a (one-class) out-of-distribution detection task and generalizes better to unseen deepfakes. Specifically, SeeABLE first generates local image perturbations (referred to as soft-discrepancies) and then pushes the perturbed faces towards predefined prototypes using a novel regression-based bounded contrastive loss. To strengthen the generalization performance of SeeABLE to unknown deepfake types, we generate a rich set of soft discrepancies and train the detector: (i) to localize, which part of the face was modified, and (ii) to identify the alteration type. To demonstrate the capabilities of SeeABLE, we perform rigorous experiments on several widely-used deepfake datasets and show that our model convincingly outperforms competing state-of-the-art detectors, while exhibiting highly encouraging generalization capabilities. The source code for SeeABLE is available from: https://github.com/anonymous-author-sub/seeable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69890172",
                    "name": "Nicolas Larue"
                },
                {
                    "authorId": "145448545",
                    "name": "Ngoc-Son Vu"
                },
                {
                    "authorId": "51301592",
                    "name": "Vitomir \u0160truc"
                },
                {
                    "authorId": "34862665",
                    "name": "P. Peer"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "4d727d2db556cefaabae2a402e9233d993d863e6",
            "title": "PROTEUS: Predictive Explanation of Anomalies",
            "abstract": "Numerous algorithms have been proposed for detecting anomalies (outliers, novelties) in an unsupervised manner. Unfortunately, it is not trivial, in general, to understand why a given sample (record) is labelled as an anomaly and thus diagnose its root causes. We propose the following reduced-dimensionality, surrogate model approach to explain detector decisions: approximate the detection model with another one that employs only a small subset of features. Subsequently, samples can be visualized in this low-dimensionality space for human understanding. To this end, we develop PROTEUS, an AutoML pipeline to produce the surrogate model, specifically designed for feature selection on imbalanced datasets. The PROTEUS surrogate model can not only explain the training data, but also the out-of-sample (unseen) data. In other words, PROTEUS produces predictive explanations by approximating the decision surface of an unsupervised detector. PROTEUS is designed to return an accurate estimate of out-of-sample predictive performance to serve as a metric of the quality of the approximation. Computational experiments confirm the efficacy of PROTEUS to produce predictive explanations for different families of detectors and to reliably estimate their predictive performance in unseen data. Unlike several ad-hoc feature importance methods, PROTEUS is robust to high-dimensional data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055402500",
                    "name": "Nikolaos Myrtakis"
                },
                {
                    "authorId": "3032369",
                    "name": "I. Tsamardinos"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "5e7251cb1341ffef0486ab534007b3cae0c7bc30",
            "title": "Face Liveness Detection Competition (LivDet-Face) - 2021",
            "abstract": "Liveness Detection (LivDet)-Face is an international competition series open to academia and industry. The competition\u2019s objective is to assess and report state-of-the-art in liveness / Presentation Attack Detection (PAD) for face recognition. Impersonation and presentation of false samples to the sensors can be classified as presentation attacks and the ability for the sensors to detect such attempts is known as PAD. LivDet-Face 2021 * will be the first edition of the face liveness competition. This competition serves as an important benchmark in face presentation attack detection, offering (a) an independent assessment of the current state of the art in face PAD, and (b) a common evaluation protocol, availability of Presentation Attack Instruments (PAI) and live face image dataset through the Biometric Evaluation and Testing (BEAT) platform. The competition can be easily followed by researchers after it is closed, in a platform in which participants can compare their solutions against the LivDet-Face winners.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10170418",
                    "name": "Sandip Purnapatra"
                },
                {
                    "authorId": "2120336996",
                    "name": "Nic Smalt"
                },
                {
                    "authorId": "1898436",
                    "name": "Keivan Bahmani"
                },
                {
                    "authorId": "2114941512",
                    "name": "Priyanka Das"
                },
                {
                    "authorId": "2000859",
                    "name": "David Yambay"
                },
                {
                    "authorId": "145649958",
                    "name": "A. Mohammadi"
                },
                {
                    "authorId": "3110004",
                    "name": "Anjith George"
                },
                {
                    "authorId": "1731727",
                    "name": "T. Bourlai"
                },
                {
                    "authorId": "145607451",
                    "name": "S. Marcel"
                },
                {
                    "authorId": "1782501",
                    "name": "S. Schuckers"
                },
                {
                    "authorId": "1944448620",
                    "name": "Meiling Fang"
                },
                {
                    "authorId": "2265721",
                    "name": "N. Damer"
                },
                {
                    "authorId": "52224165",
                    "name": "Fadi Boutros"
                },
                {
                    "authorId": "145307900",
                    "name": "Arjan Kuijper"
                },
                {
                    "authorId": "1410151432",
                    "name": "Alperen Kantarci"
                },
                {
                    "authorId": "65933572",
                    "name": "Basar Demir"
                },
                {
                    "authorId": "2120348129",
                    "name": "Zafer Yildiz"
                },
                {
                    "authorId": "2120326292",
                    "name": "Zabi Ghafoory"
                },
                {
                    "authorId": "137659234",
                    "name": "Hasan Dertli"
                },
                {
                    "authorId": "3025777",
                    "name": "H. K. Ekenel"
                },
                {
                    "authorId": "2120345539",
                    "name": "Son Vu"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1856739409",
                    "name": "Liang Dashuang"
                },
                {
                    "authorId": "30675531",
                    "name": "Zhang Guanghao"
                },
                {
                    "authorId": "2120348131",
                    "name": "Hao Zhanlong"
                },
                {
                    "authorId": "1664716168",
                    "name": "Liu Junfu"
                },
                {
                    "authorId": "48532290",
                    "name": "Jin Yufeng"
                },
                {
                    "authorId": "2120354474",
                    "name": "Samo Liu"
                },
                {
                    "authorId": "2148653485",
                    "name": "Samuel Huang"
                },
                {
                    "authorId": "2120326100",
                    "name": "Salieri Kuei"
                },
                {
                    "authorId": "1711646",
                    "name": "Jag Mohan Singh"
                },
                {
                    "authorId": "151434651",
                    "name": "Raghavendra Ramachandra"
                }
            ]
        },
        {
            "paperId": "688e020cbe8edbe95f26942f466e2dfd916dbb70",
            "title": "Efficient Scheduling of Streaming Operators for IoT Edge Analytics",
            "abstract": "Data stream processing and analytics (DSPA) applications are widely used to process the ever increasing amounts of data streams produced by highly geographical distributed data sources such as fixed and mobile IoT devices in order to extract valuable information in a timely manner for real-time actuation. To efficiently handle this ever increasing amount of data streams, the emerging Edge/Fog computing paradigms is used as the middle-tier between the Cloud and the IoT devices to process data streams closer to their sources and to reduce the network resource usage and network delay to reach the Cloud. In this paper, we account for the fact that both network resources and computational resources can be limited and shareable among multiple DSPA applications in the Edge-Fog-Cloud architecture, hence it is necessary to ensure their efficient usage. In this respect, we propose a resource-aware and time-efficient heuristic called SOO that identifies a good DSPA operator placement on the Edge-Fog-Cloud architecture towards optimizing the trade-off between the computational and network resource usage. Via thorough simulation experiments, we show that the solution provided by SOO is very close to the optimal one while the execution time is considerably reduced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5817238",
                    "name": "Patient Ntumba"
                },
                {
                    "authorId": "1708457",
                    "name": "N. Georgantas"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "6de47d3852a9d09fbdeb4bce6d82ec74d7f1a784",
            "title": "Scheduling Continuous Operators for IoT Edge Analytics",
            "abstract": "In this paper we are interested in exploring the Edge-Fog-Cloud architecture as an alternative approach to the Cloud-based IoT data analytics. Given the limitations of Fog in terms of limited computational resources that can also be shared among multiple analytics with continuous operators over data streams, we introduce a holistic cost model that accounts both the network and computational resources available in the Edge-Fog-Cloud architecture. Then, we propose scheduling algorithms RCS and SOO-CPLEX for placing continuous operators for data stream analytics at the network edge. The former dynamically places continuous operators between the Cloud and the Fog according to the evolution of data streams rates and uses as less as possible Fog computational resources to satisfy the constraints regarding the usage of both computational and network resources. The latter statically places continuous operators between the Cloud and the Fog to minimize the overall computational and network resource usage cost. Based on thorough experiments, we evaluate the effectiveness of SOO-CPLEX and RCS using simulation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5817238",
                    "name": "Patient Ntumba"
                },
                {
                    "authorId": "1708457",
                    "name": "N. Georgantas"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "7f2b02a79eb06a76127d6dbbc39a216a660a4e51",
            "title": "On Predictive Explanation of Data Anomalies",
            "abstract": "Numerous algorithms have been proposed for detecting anomalies (outliers, novelties) in an unsupervised manner. Unfortunately, it is not trivial, in general, to understand why a given sample (record) is labelled as an anomaly and thus diagnose its root causes. We propose the following reduced-dimensionality, surrogate model approach to explain detector decisions: approximate the detection model with another one that employs only a small subset of features. Subsequently, samples can be visualized in this low-dimensionality space for human understanding. To this end, we develop PROTEUS, an AutoML pipeline to produce the surrogate model, specifically designed for feature selection on imbalanced datasets. The PROTEUS surrogate model can not only explain the training data, but also the out-of-sample (unseen) data. In other words, PROTEUS produces predictive explanations by approximating the decision surface of an unsupervised detector. PROTEUS is designed to return an accurate estimate of out-of-sample predictive performance to serve as a metric of the quality of the approximation. Computational experiments confirm the efficacy of PROTEUS to produce predictive explanations for different families of detectors and to reliably estimate their predictive performance in unseen data. Unlike several ad-hoc feature importance methods, PROTEUS is robust to high-dimensional data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055402500",
                    "name": "Nikolaos Myrtakis"
                },
                {
                    "authorId": "3032369",
                    "name": "I. Tsamardinos"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "85ba2d3a9ee9a57fa321ec3b2dd41dbc6fb02e9e",
            "title": "FairER: Entity Resolution With Fairness Constraints",
            "abstract": "There is an urgent call to detect and prevent \"biased data\" at the earliest possible stage of the data pipelines used to build automated decision-making systems. In this paper, we are focusing on controlling the data bias in entity resolution (ER) tasks aiming to discover and unify records/descriptions from different data sources that refer to the same real-world entity. We formally define the ER problem with fairness constraints ensuring that all groups of entities have similar chances to be resolved. Then, we introduce FairER, a greedy algorithm for solving this problem for fairness criteria based on equal matching decisions. Our experiments show that FairER achieves similar or higher accuracy against two baseline methods over 7 datasets, while guaranteeing minimal bias.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "1781993",
                    "name": "E. Pitoura"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "8e2e08a11737ab55d7bd3022265ac4c308e28d0a",
            "title": "A Comparative Evaluation of Anomaly Explanation Algorithms",
            "abstract": "Detection of anomalies (i.e., outliers ) in multi-dimensional data is a well-studied subject in machine learning. Unfortunately, un-supervised detectors provide no explanation about why a data point was considered as abnormal or which of its features (i.e. subspaces) exhibit at best its outlyingness. Such outlier explanations are crucial to diagnose the root cause of data anomalies and enable corrective actions to prevent or remedy their effect in downstream data processing. In this work, we present a comprehensive framework for comparing different unsupervised outlier explanation algorithms that are domain and detector-agnostic. Using real and synthetic datasets, we assess the effectiveness and efficiency of two point explanation algorithms (Beam [28] and RefOut [18]) ranking subspaces that best explain the outlyingness of individual data points and two explanation summarization algorithms (LookOut [15] and HiCS [17]) ranking subspaces that best exhibit as many outlier points from inliers as possible. To the best of our knowledge, this is the first detailed evaluation of existing explanation algorithms aiming to uncover several missing insights from the literature such as: (a) Is it effective to combine any explanation algorithm with any off-the-shelf outlier detector? (b) How is the behavior of an outlier detection and explanation pipeline affected by the number or the correlation of features in a dataset? and (c) What is the quality of summaries in the presence of outliers explained by subspaces of different dimensionality?",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055402500",
                    "name": "Nikolaos Myrtakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2060863572",
                    "name": "Eric Simon"
                }
            ]
        },
        {
            "paperId": "46d1abf5f0f4693727ac5884c9cc66c956fd6c53",
            "title": "Implications of the Multi-Modality of User Perceived Page Load Time",
            "abstract": "Web browsing is one of the most popular applications for both desktop and mobile users. A lot of effort has been devoted to speedup the Web, as well as in designing metrics that can accurately tell whether a webpage loaded fast or not. An often implicit assumption made by industrial and academic research communities is that a single metric is sufficient to assess whether a webpage loaded fast. In this paper we collect and make publicly available a unique dataset which contains webpage features (e.g., number and type of embedded objects) along with both objective and subjective Web quality metrics. This dataset was collected by crawling over 100 websites\u2014representative of the top 1 M websites in the Web\u2014while crowdsourcing 6,000 user opinions on user perceived page load time (uPLT). We show that the uPLT distribution is often multi-modal and that, in practice, no more than three modes are present. The main conclusion drawn from our analysis is that, for complex webpages, each of the different objective QoE metrics proposed in the literature (such as AFT, TTI, PLT, etc.) is suited to approximate one of the different uPLT modes. Index Terms\u2014Web Performance, Quality Of Experience, Measurements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35897619",
                    "name": "Flavia Salutari"
                },
                {
                    "authorId": "2650248",
                    "name": "Diego N. da Hora"
                },
                {
                    "authorId": "1769836",
                    "name": "Matteo Varvello"
                },
                {
                    "authorId": "145850321",
                    "name": "R. Teixeira"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1878342442",
                    "name": "Dario Rossi"
                }
            ]
        },
        {
            "paperId": "7293b51020d422ff14515abc7c91962713ea8391",
            "title": "Putting the Human Back in the AutoML Loop",
            "abstract": "Automated Machine Learning (AutoML) is a rapidly rising sub-field of Machine Learning. AutoML aims to fully automate the machine learning process end-to-end, democratizing Machine Learning to non-experts and drastically increasing the productivity of expert analysts. So far, most comparisons of AutoML systems focus on quantitative criteria such as predictive performance and execution time. In this paper, we examine AutoML services for predictive modeling tasks from a user\u2019s perspective, going beyond predictive performance. We present a wide palette of criteria and dimensions on which to evaluate and compare these services as a user. This qualitative comparative methodology is applied on seven AutoML systems, namely Auger.AI, BigML, H2O\u2019s Driverless AI, Darwin, Just Add Data Bio, Rapid-Miner, and Watson. The comparison indicates the strengths and weaknesses of each service, the needs that it covers, the segment of users that is most appropriate for, and the possibilities for improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9034179",
                    "name": "Iordanis Xanthopoulos"
                },
                {
                    "authorId": "3032369",
                    "name": "I. Tsamardinos"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2060863572",
                    "name": "Eric Simon"
                },
                {
                    "authorId": "2765127",
                    "name": "Alejandro Salinger"
                }
            ]
        },
        {
            "paperId": "9feea09cd9cc196b3eeec4e6551ed2f4d7cd9a57",
            "title": "Benchmarking Blocking Algorithms for Web Entities",
            "abstract": "An increasing number of entities are described by interlinked data rather than documents on the Web. Entity Resolution (ER) aims to identify descriptions of the same real-world entity within one or across knowledge bases in the Web of data. To reduce the required number of pairwise comparisons among descriptions, ER methods typically perform a pre-processing step, called blocking, which places similar entity descriptions into blocks and thus only compare descriptions within the same block. We experimentally evaluate several blocking methods proposed for the Web of data using real datasets, whose characteristics significantly impact their effectiveness and efficiency. The proposed experimental evaluation framework allows us to better understand the characteristics of the missed matching entity descriptions and contrast them with ground truth obtained from different kinds of relatedness links.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "f11cba099b8cf14815f7b3d85f55ecfddbf9f04d",
            "title": "An Overview of End-to-End Entity Resolution for Big Data",
            "abstract": "One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows for Big Data, critically review the pros and cons of existing methods, and conclude with the main open research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                }
            ]
        },
        {
            "paperId": "49beb14873347bad73d96a870a59d07ae6cbb727",
            "title": "Detecting Mobile Crowdsensing Context in the Wild",
            "abstract": "Understanding the sensing context of raw data is crucial for assessing the quality of large crowdsourced spatio-temporal datasets. Detecting sensing contexts in the wild is a challenging task and requires features from smartphone sensors that are not always available. In this paper, we propose three heuristic algorithms for detecting sensing contexts such as in/out-pocket, under/over-ground, and in/out-door for crowdsourced datasets that are destined for human mobility mining. These are unsupervised binary classifiers with a small memory footprint and execution time. Using a segment of the Ambiciti real dataset \u2013 a feature-limited crowdsourced dataset \u2013 we report that our algorithms perform equally well in terms of balanced accuracy (within 4.3%) when compared to machine learning (ML) models reported by an AutoML tool.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34292316",
                    "name": "R. Agarwal"
                },
                {
                    "authorId": "35639986",
                    "name": "Shaan Chopra"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1708457",
                    "name": "N. Georgantas"
                },
                {
                    "authorId": "1688880",
                    "name": "V. Issarny"
                }
            ]
        },
        {
            "paperId": "7395570cbed63777cce6bdc2b2e2a07811d5e04b",
            "title": "End-to-End Entity Resolution for Big Data: A Survey",
            "abstract": "One of the most important tasks for improving data quality and the reliability of data analytics results is Entity Resolution (ER). ER aims to identify different descriptions that refer to the same real-world entity, and remains a challenging problem. While previous works have studied specific aspects of ER (and mostly in traditional settings), in this survey, we provide for the first time an end-to-end view of modern ER workflows, and of the novel aspects of entity indexing and matching methods in order to cope with more than one of the Big Data characteristics simultaneously. We present the basic concepts, processing steps and execution strategies that have been proposed by different communities, i.e., database, semantic Web and machine learning, in order to cope with the loose structuredness, extreme diversity, high speed and large scale of entity descriptions used by real-world applications. Finally, we provide a synthetic discussion of the existing approaches, and conclude with a detailed presentation of open research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                }
            ]
        },
        {
            "paperId": "7a693aa4e14b74535d1904cdd5724ab17df214c8",
            "title": "Inferring Context of Mobile Data Crowdsensed in the Wild",
            "abstract": "Understanding the sensing context of raw data is crucial for assessing the quality of large crowdsourced spatio-temporal datasets. Accelerometer's precision can vary considerably depending on whether the phone is in-pocket or out-pocket, i.e., held in hand. GPS accuracy can be very low in places like underground metro stations. Further, jump-lengths are shorter and have higher frequency when a person is indoor. Hence, we focus on contexts such as in/out-pocket, under/over-ground, and in/out-door that can be essential for reliably inferring human mobility attributes and properties (e.g., location, jump-length, and mobility activity like walking or driving) from crowdsensed data. Our work is motivated by the fact that most of the publicly available crowdsensing datasets (e.g. PRIVA'MOV and Beijing taxi dataset) do not include data from specialized sensors such as light, barometer, etc. considered by state-of-the-art algorithms for detecting the above mentioned contexts. Therefore, we focus on mining context from the limited features available in the publicly available mobility related crowdsensing datasets. Moreover, as ground truth is typically not available in these datasets, we pay special attention to minimizing the training or tuning efforts of the introduced algorithms. Our algorithms are unsupervised binary classifiers with a small memory footprint and execution time. As the lack of certain features prohibits us to consider state-of-the-art algorithms as baselines, we compare the performance of our heuristic algorithms against Machine Learning (ML) models built by an AutoML tool using the same set of features. Our experimental evaluation with a segment of the Ambiciti dataset demonstrates that when compared to the best baseline ML model w.r.t. balanced accuracy (see Table I), our algorithm for in/out-pocket performs equally well, while for under/over-ground and in/out-door contexts, for a specific hyper-parameter, our corresponding algorithms are within 4.3% and 1%, respectively. Concerning memory, our algorithms require 0kB, 4kB, and 0kB, respectively, while they take 0.08sec, 0.17sec and 0.003sec, respectively, for execution. Our algorithms are lightweight enough to be integrated into smartphone applications. Context information mined onboard thus remains private and can be used to annotate users' personal trajectories and incentivize them to participate in crowd-measurement campaigns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34292316",
                    "name": "R. Agarwal"
                },
                {
                    "authorId": "35639986",
                    "name": "Shaan Chopra"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1708457",
                    "name": "N. Georgantas"
                },
                {
                    "authorId": "1688880",
                    "name": "V. Issarny"
                }
            ]
        },
        {
            "paperId": "cd4d3ab157224d1754176efac5e736e8c2678a04",
            "title": "MinoanER: Schema-Agnostic, Non-Iterative, Massively Parallel Resolution of Web Entities",
            "abstract": "Entity Resolution (ER) aims to identify different descriptions in various Knowledge Bases (KBs) that refer to the same entity. ER is challenged by the Variety, Volume and Veracity of entity descriptions published in the Web of Data. To address them, we propose the MinoanER framework that simultaneously fulfills full automation, support of highly heterogeneous entities, and massive parallelization of the ER process. MinoanER leverages a token-based similarity of entities to define a new metric that derives the similarity of neighboring entities from the most important relations, as they are indicated only by statistics. A composite blocking method is employed to capture different sources of matching evidence from the content, neighbors, or names of entities. The search space of candidate pairs for comparison is compactly abstracted by a novel disjunctive blocking graph and processed by a non-iterative, massively parallel matching algorithm that consists of four generic, schema-agnostic matching rules that are quite robust with respect to their internal configuration. We demonstrate that the effectiveness of MinoanER is comparable to existing ER tools over real KBs exhibiting low Variety, but it outperforms them significantly when matching KBs with high Variety.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "4b0f9e180665e2609aa80af906e4bd2a6fd5839b",
            "title": "Simplifying Entity Resolution on Web Data with Schema-Agnostic, Non-Iterative Matching",
            "abstract": "Entity Resolution (ER) aims to identify different descriptions in various Knowledge Bases (KBs) that refer to the same entity. ER is challenged by the Variety, Volume and Veracity of descriptions published in the Web of Data. To address them, we propose the MinoanER framework that fulfills full automation and support of highly heterogeneous entities. MinoanER leverages a token-based similarity of entities to define a new metric that derives the similarity of neighboring entities from the most important relations, indicated only by statistics. For high efficiency, similarities are computed from a set of schema-agnostic blocks and processed in a non-iterative way that involves four threshold-free heuristics. We demonstrate that the effectiveness of MinoanER is comparable to existing ER tools over real KBs exhibiting low heterogeneity in terms of entity types and content. Yet, MinoanER outperforms state-of-the-art ER tools when matching highly heterogeneous KBs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "5c66214c49493e9d84ec68401e039dadc5341f86",
            "title": "A practical method for measuring Web above-the-fold time",
            "abstract": "Page load time (PLT) is still the most common application Quality of Service (QoS) metric to estimate the Quality of Experience (QoE) of Web users. Yet, recent literature abounds with interesting proposals for alternative metrics (e.g., Above The Fold, SpeedIndex and variants) that aim at closely capturing how users perceive the Webpage rendering process. However, these novel metrics are typically computationally expensive, as they require to monitor and post-process videos of the rendering process, and have failed to be widely deployed. In this demo, we show our implementation of an opensource Chrome extension that implements a practical and lightweight method to measure the approximated Above-the-Fold (AATF) time, as well as others Web performance metrics. The idea is, instead of accurately monitoring the rendering output, to track the download time of the last visible object on screen (i.e., \"above the fold\"). Our plugin also has options to save detailed reports for later analysis, a functionality ideally suited for researchers wanting to gather data from Web experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2650248",
                    "name": "Diego N. da Hora"
                },
                {
                    "authorId": "1878342442",
                    "name": "Dario Rossi"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "145850321",
                    "name": "R. Teixeira"
                }
            ]
        },
        {
            "paperId": "873a4032a47fc03168afc0abc0b27ed9fde0198d",
            "title": "SIOTOME: An Edge-ISP Collaborative Architecture for IoT Security",
            "abstract": "Modern households are deploying Internet of Things (IoT) devices at a fast pace. The heterogeneity of these devices, which range from low-end sensors to smart TVs, make securing home IoT particularly challenging. To make matters worse, many consumer-IoT devices are hard or impossible to secure because device manufacturers fail to adopt security best practices (e.g., regular software patches). In this paper we propose a novel, cooperative system between the home gateway and the Internet Service Provider (ISP) to provide data driven security solutions for detecting and isolating IoT security attacks. Our approach is based on a combination of a large-scale view from the ISP (using powerful machine learning techniques on traffic traces), and the fine-grained view of the per-device activity from the home (using edge processing techniques) to provide efficient, yet privacy-aware IoT security services.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258818632",
                    "name": "Hamed Haddadi"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2258808425",
                    "name": "Renata Teixeira"
                },
                {
                    "authorId": "2258936800",
                    "name": "Kenjiro Cho"
                },
                {
                    "authorId": "2259950390",
                    "name": "Shigeya Suzuki"
                },
                {
                    "authorId": "1688974",
                    "name": "A. Perrig"
                }
            ]
        },
        {
            "paperId": "169c30fbbf17e0d512116040cf3ae0fff3b41436",
            "title": "Massively-Parallel Feature Selection for Big Data",
            "abstract": "We present the Parallel, Forward-Backward with Pruning (PFBP) algorithm for feature selection (FS) in Big Data settings (high dimensionality and/or sample size). To tackle the challenges of Big Data FS PFBP partitions the data matrix both in terms of rows (samples, training examples) as well as columns (features). By employing the concepts of p-values of conditional independence tests and meta-analysis techniques PFBP manages to rely only on computations local to a partition while minimizing communication costs. Then, it employs powerful and safe (asymptotically sound) heuristics to make early, approximate decisions, such as Early Dropping of features from consideration in subsequent iterations, Early Stopping of consideration of features within the same iteration, or Early Return of the winner in each iteration. PFBP provides asymptotic guarantees of optimal-ity for data distributions faithfully representable by a causal network (Bayesian network or maximal ancestral graph). Our empirical analysis confirms a super-linear speedup of the algorithm with increasing sample size, linear scalability with respect to the number of features and processing cores, while dominating other competitive algorithms in its class.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3032369",
                    "name": "I. Tsamardinos"
                },
                {
                    "authorId": "2484331",
                    "name": "Giorgos Borboudakis"
                },
                {
                    "authorId": "2615278",
                    "name": "Pavlos Katsogridakis"
                },
                {
                    "authorId": "2385564",
                    "name": "Polyvios Pratikakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "392e6cdb9c79e038e864c69731e9cadfeec95895",
            "title": "Web-Scale Blocking, Iterative and Progressive Entity Resolution",
            "abstract": "Entity resolution aims to identify descriptions of the same entity within or across knowledge bases. In this work, we provide a comprehensive and cohesive overview of the key research results in the area of entity resolution. We are interested in frameworks addressing the new challenges in entity resolution posed by the Web of data in which real world entities are described by interlinked data rather than documents. Since such descriptions are usually partial, overlapping and sometimes evolving, entity resolution emerges as a central problem both to increase dataset linking, but also to search the Web of data for entities and their relations. We focus on Web-scale blocking, iterative and progressive solutions for entity resolution. Specifically, to reduce the required number of comparisons, blocking is performed to place similar descriptions into blocks and executes comparisons to identify matches only between descriptions within the same block. To minimize the number of missed matches, an iterative entity resolution process can exploit any intermediate results of blocking and matching, discovering new candidate description pairs for resolution. Finally, we overview works on progressive entity resolution, which attempt to discover as many matches as possible given limited computing budget, by estimating the matching likelihood of yet unresolved descriptions, based on the matches found so far.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                }
            ]
        },
        {
            "paperId": "424684291454ffcb29fd2b910fda122cbecb83ec",
            "title": "Extracting usage patterns of home IoT devices",
            "abstract": "Ubiquitous connectivity and smart technologies gradually transform homes into Intranet of Things, where a multitude of connected, intelligent devices allow for novel home automation services. Providing new services for home users (e.g., energy saving automations) and Internet Service Providers (e.g., network management and troubleshooting) requires an in-depth analysis of various kinds of data (connectivity, performance, usage) collected from home networks. In this paper, we explore new Machine-to-Machine data analysis techniques that go beyond binary association rule mining for traditional market basket analysis considered by previous studies, to analyze individual device logs of home gateways. We introduce a multidimensional patterns mining framework, to extract complex device co-usage patterns of 201 residential broadband users of an ISP, subscribed to a triple-play service. Our results show that our analytics engine provides valuable insights for emerging use cases such as monitoring for energy efficiency, and \u201cthings\u201d recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2835209",
                    "name": "G. Poghosyan"
                },
                {
                    "authorId": "2285687",
                    "name": "Ioannis Pefkianakis"
                },
                {
                    "authorId": "3283901",
                    "name": "P. L. Guyadec"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "570a49754b44fa76e9fc40e0e081199d9b2234a4",
            "title": "Characterizing Home Device Usage From Wireless Traffic Time Series",
            "abstract": "The analysis of temporal behavioral patterns of home network users can reveal important information to Internet Service Providers (ISPs) and help them to optimize their networks and offer new services (e.g., remote software upgrades, troubleshooting, energy savings). This study uses time series analysis of continuous traffic data from wireless home networks , to extract traffic patterns recurring within, or across homes, and assess the impact of different device types (fixed or portable) on home traffic. Traditional techniques for time series analysis are not suited in this respect, due to the limited stationary and evolving distribution properties of wireless home traffic data. We propose a novel framework that relies on a correlation-based similarity measure of time series , as well as a notion of strong stationarity to define motifs and dominant devices. Using this framework, we analyze the wireless traffic collected from 196 home gateways over two months. The proposed approach goes beyond existing application-specific analysis techniques, such as analysis of wireless traffic, which mainly rely on data aggregated across hundreds, or thousands of users. Our framework, enables the extraction of recurring patterns from traffic time series of individual homes, leading to a much more fine-grained analysis of the behavior patterns of the users. We also determine the best time aggregation policy w.r.t. to the number and statistical importance of the extracted motifs, as well as the device types dominating these motifs and the overall gateway traffic. Our results show that ISPs can exceed the simple observation of the aggregated gateway traffic and better understand their networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1701357",
                    "name": "Katsiaryna Mirylenka"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2285687",
                    "name": "Ioannis Pefkianakis"
                },
                {
                    "authorId": "46518232",
                    "name": "M. May"
                }
            ]
        },
        {
            "paperId": "cecf1f16132aecf0ecf256d4b7728cd3e6754dd2",
            "title": "Algebraic Structures for Capturing the Provenance of SPARQL Queries",
            "abstract": "The evaluation of SPARQL algebra queries on various kinds of annotated RDF graphs can be seen as a particular case of the evaluation of these queries on RDF graphs annotated with elements of so-called spm-semirings. Spm-semirings extend semirings, used for representing the provenance of positive relational algebra queries on annotated relational data, with a new operator to capture the semantics of the non-monotone SPARQL operators. Furthermore, spm-semiring-based annotations ensure that desired SPARQL query equivalences hold when querying annotated RDF. In this work, in addition to introducing spm-semirings, we study their properties and provide an alternative characterization of these structures in terms of semirings with an embedded boolean algebra (or seba-structure for short). This characterization allows us to construct spm-semirings and identify a universal object in the class of spm-semirings. Finally, we show that this universal object provides a provenance representation of poly-sized overhead and can be used to evaluate SPARQL queries on arbitrary spm-semiring-annotated RDF graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94161058",
                    "name": "F. Geerts"
                },
                {
                    "authorId": "1742512387",
                    "name": "T. Unger"
                },
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "1791376",
                    "name": "I. Fundulaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "f0b65cca9cb4a1115ae4c93234755d3db290cb41",
            "title": "Minoan ER: Progressive Entity Resolution in the Web of Data",
            "abstract": "Entity resolution aims to identify descriptions of the same entity within or across knowledge bases. In this work, we present the Minoan ER platform for resolving entities described by linked data in the Web (e.g., in RDF). To reduce the required number of comparisons, Minoan ER performs blocking to place similar descriptions into blocks and executes comparisons to identify matches only between descriptions within the same block. Moreover, it explores in a pay-as-you-go fashion any intermediate results of matching to obtain similarity evidence of entity neighbors and discover new candidate description pairs for resolution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "4be7bddf56bd067f01631df91023c4d3632dcfa5",
            "title": "Report on the First International Workshop on Personal Data Analytics in the Internet of Things (PDA@IOT 2014)",
            "abstract": "The 1st International Workshop on Personal Data Analytics in the Internet of Things (PDA@IOT), held in conjunction with VLDB 2014, aims at sparking research on data analytics, shifting the focus from business to consumers services. While much of the public and academic discourse about personal data has been dominated by a focus on the privacy concerns and the risks they raise to the individual, especially when they are seen as the new oil of the global economy. PDA@IOT focus on how persons could effectively exploit the data they massively create in CyberPhysicalworlds. We believe that the full potential of the IoT goes far beyond connecting \u201cthings\u201d to the Internet: it is about using data to create new value for people. In a People-centric computing paradigm, both small scale \npersonal data and large scale aggregated data should be exploited to identify unmet needs and proactively offer \nthem to users. PDA@IOT seeks to address current technology barriers that impede existing personal data \nprocessing and analytics solutions to empower people in personal decision making. \nThe PDA@IOT ambition is to provide a unique forum for researchers and practitioners that approach personal data from different angles, ranging from data management and processing, to data mining and human-data interaction, as well as to nourish the interdisciplinary synergies required to tackle the challenges and problems emerging in People-centric Computing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "6793724501c1e11662c2eabcfc75c408f5dbf097",
            "title": "A Services Framework for Using Social Web as Social Sensor Web",
            "abstract": "Online social networking sites carry a lot of information organized around the people who actually submit it. They rely a lot on the interconnections between the actors of the network and they relate the generated information in terms of connections or ties among those actors. On the other hand, social networks carry a lot of information in real time or almost real time since people \"report\" information as they see it unfolding before their eyes. We are targeting this information and we try to understand when this information can be related to refer to a single event, which has both a spatial and a temporal dimension. These social sensors can alert us on what is happening in the society when it happens and by aggregating the different reports from people who are on the scene. In order to achieve that we suggest and describe a set of services that can be used for collection of the information, identifying the discussion topics and provide at the end an alert if the discussion qualifies for and contains enough information to define an event. The goal is for the system to work with no prior knowledge of the events we seek.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "70098450",
                    "name": "G. Kariotis"
                },
                {
                    "authorId": "112953835",
                    "name": "Lila Theodoridou"
                },
                {
                    "authorId": "2475928",
                    "name": "N. Petalidis"
                },
                {
                    "authorId": "115747901",
                    "name": "S. Kleisarxaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "ef8229e87310efc05f20a12d7e8a2fb4efb6ccd3",
            "title": "Big data entity resolution: From highly to somehow similar entity descriptions in the Web",
            "abstract": "In the Web of data, entities are described by interlinked data rather than documents on the Web. In this work, we focus on entity resolution in the Web of data, i.e., identifying descriptions that refer to the same real-world entity. To reduce the required number of pairwise comparisons, methods for entity resolution perform blocking as a pre-processing step. A blocking technique places similar entity descriptions into blocks and executes comparisons only between descriptions within the same block. We experimentally evaluate blocking techniques proposed for the Web of data and present dataset characteristics that determine the effectiveness and efficiency of such methods. Furthermore, we analyze the characteristics of the missed matching entity descriptions and examine different types of links that blocking techniques can potentially identify.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "f3646931ef0fef7eb3543147d635e82a25535187",
            "title": "Querying Temporal Drifts at Multiple Granularities",
            "abstract": "There exists a large body of work on online drift detection with the goal of dynamically finding and maintaining changes in data streams. In this paper, we adopt a query-based approach to drift detection. Our approach relies on a drift index, a structure that captures drift at different time granularities and enables flexible drift queries. We formalize different drift queries that represent real-world scenarios and develop query evaluation algorithms that use different materializations of the drift index as well as strategies for online index maintenance. We describe a thorough study of the performance of our algorithms on real-world and synthetic datasets with varying change rates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3036804",
                    "name": "Sofia Kleisarchaki"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1765141",
                    "name": "A. Chouakria"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "1608ae7922cce0d9273a03d1cc72dad261b38e8a",
            "title": "RSS feeds behavior analysis, structure and vocabulary",
            "abstract": "Purpose \u2013 The purpose of this paper is to present a thorough analysis of three complementary features of real-scale really simple syndication (RSS)/Atom feeds, namely, publication activity, items characteristics and their textual vocabulary, that the authors believe are crucial for emerging Web 2.0 applications. Previous works on RSS/Atom statistical characteristics do not provide a precise and updated characterization of feeds\u2019 behavior and content, characterization that can be used to successfully benchmark the effectiveness and efficiency of various Web syndication processing/analysis techniques. Design/methodology/approach \u2013 The authors empirical study relies on a large-scale testbed acquired over an eight-month campaign from 2010. They collected a total number of 10,794,285 items originating from 8,155 productive feeds. The authors deeply analyze feeds productivity (types and bandwidth), content (XML, text and duplicates) and textual content (vocabulary and buzz-words). Findings \u2013 The findings of the...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1763974",
                    "name": "Nicolas Travers"
                },
                {
                    "authorId": "3278251",
                    "name": "Zeinab Hmedeh"
                },
                {
                    "authorId": "2512311",
                    "name": "Nelly Vouzoukidou"
                },
                {
                    "authorId": "1807596",
                    "name": "C\u00e9dric du Mouza"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "144911319",
                    "name": "M. Scholl"
                }
            ]
        },
        {
            "paperId": "7800a1c22416af9be5e9a7206e964dfb382f9921",
            "title": "Online Detection of Topic Change in Social Posts",
            "abstract": "Gaining deep insights of the social Web content is a challenging Big Data analytics problem, especially when dealing with social posts of high volume and arrival rate consisting of high variable topics. Detecting and tracking the topics that the users discuss in popular microblogging applications like Twitter and studying the evolution of each topic reveals crowd interests and intelligence. The evolution summarizes the changes that are occurring on the topics over a given time horizon inside the evolving data stream. For instance, some topics may disappear at some point in time due to lack of users' interest, while others are retained over time adopting either a stable or an evolving behaviour. The analysis and storage of such dynamic and massive content with spatio-thematic properties poses new challenges for research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3036804",
                    "name": "Sofia Kleisarchaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1410931382",
                    "name": "Ahlame Douzal-Chouakria"
                }
            ]
        },
        {
            "paperId": "7c786e159cf20f821f039efb99167a6d538cd147",
            "title": "MeowsReader: Real-Time Ranking and Filtering of News with Generalized Continuous Top-k Queries",
            "abstract": "This demonstration presents MeowsReader, a real-time news ranking and filtering prototype. MeowsReader illustrates how a general class of continuous top-k queries offers a suitable abstraction for modeling and implementing real-time search services over highly dynamic information streams combining keyword search and realtime web signals about information items. Users express their interest by simple text queries and continuously receive the best matching results in an alert-like environment. The main innovative feature are dynamic item scores which take account of information decay, real-time web attention and other online user feedback. Additionally, a trends detection mechanism automatically generates trending entities from the input streams, which can smoothly be added to user profiles in form of keyword queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2512311",
                    "name": "Nelly Vouzoukidou"
                },
                {
                    "authorId": "1747227",
                    "name": "B. Amann"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "c1fab31a821ef8a382f37ed5a0fb860f0f9048a7",
            "title": "Uncertainty-Aware Sensor Data Management and Early Warning for Monitoring Industrial Infrastructures",
            "abstract": "In several industrial applications, monitoring large-scale infrastructures in order to provide notifications for abnormal behavior is of high significance. For this purpose, the deployment of large-scale sensor networks is the current trend. However, this results in handling vast amounts of low-level, and often unreliable, data, while an efficient and real-time data manipulation is a strong demand. In this paper, the authors propose an uncertainty-aware data management system capable of monitoring interrelations between large and heterogeneous sensor data streams in real-time. To this end, an efficient similarity function is employed instead of the typical correlation coefficient to monitor dynamic phenomena for timely alerting notifications, and to guarantee the validity of detected extreme events. Experimental evaluation with a set of real data recorded by distinct sensors in an industrial water desalination plant reveals a superior performance of our proposed approach in terms of achieving significantly reduced execution times, along with increased accuracy in detecting extreme events and highly correlated pairs of sensor data streams, when compared with state-of-the-art data stream processing techniques.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2250866",
                    "name": "G. Tzagkarakis"
                },
                {
                    "authorId": "3235794",
                    "name": "Aleka Seliniotaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1694755",
                    "name": "P. Tsakalides"
                }
            ]
        },
        {
            "paperId": "e27e897e0bab3aa67810f4c688b38f2a4f076d13",
            "title": "Stream correlation monitoring for uncertainty-aware data processing systems",
            "abstract": "In several industrial applications, monitoring large-scale infrastructures in order to provide notifications for abnormal behavior is of high significance. For this purpose, the deployment of large-scale sensor networks is the current trend. However, this results in handling vast amounts of low-level, and often unreliable, data, while an efficient and real-time data manipulation is a strong demand. In this paper, we propose an uncertainty-aware data management system capable of monitoring pairwise correlations of large sensor data streams in real-time. An efficient similarity function based on the truncated DFT is employed instead of the typical correlation coefficient to monitor dynamic phenomena for timely alerting notifications, and to guarantee the validity of detected extreme events. Experimental evaluation with a set of real data recorded by distinct sensors in an industrial water desalination plant reveals a high performance of our proposed approach in terms of achieving significantly reduced execution times, along with increased accuracy in detecting highly correlated pairs of sensor data streams.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3235794",
                    "name": "Aleka Seliniotaki"
                },
                {
                    "authorId": "2250866",
                    "name": "G. Tzagkarakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1694755",
                    "name": "P. Tsakalides"
                }
            ]
        },
        {
            "paperId": "530ebd34a30b5750517d827ea8ba00bc2e3adbd6",
            "title": "Entity resolution in the web of data",
            "abstract": "This tutorial provides an overview of the key research results in the area of entity resolution that are relevant to addressing the new challenges in entity resolution posed by the Web of data, in which real world entities are described by interlinked data rather than documents. Since such descriptions are usually partial, overlapping and sometimes evolving, entity resolution emerges as a central problem both to increase dataset linking but also to search the Web of data for entities and their relations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                }
            ]
        },
        {
            "paperId": "d6d58b822b327a9322b05f32f3f530b7d8c18a58",
            "title": "Algebraic structures for capturing the provenance of SPARQL queries",
            "abstract": "We show that the evaluation of SPARQL algebra queries on various notions of annotated RDF graphs can be seen as particular cases of the evaluation of these queries on RDF graphs annotated with elements of so-called spm-semirings. Spm-semirings extend semirings, used for positive relational algebra queries on annotated relational data, with a new operator to capture the semantics of the non-monotone SPARQL operator OPTIONAL. Furthermore, spm-semiring-based annotations ensure that desired SPARQL query equivalences hold when querying annotated RDF. In addition to introducing spm-semirings, we study their properties and provide an alternative characterization of these structures in terms of semirings with an embedded boolean algebra (or seba-structure for short). This characterization allows to construct spm-semirings and to identify a universal object in the class of spm-semirings. Finally, we show that this universal object provides a concise provenance representation and can be used to evaluate SPARQL queries on arbitrary spm-semiring-annotated RDF graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1729031",
                    "name": "Floris Geerts"
                },
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1791376",
                    "name": "I. Fundulaki"
                }
            ]
        },
        {
            "paperId": "e9c9f47a5b22c0f3cf155db03cce5c39f494d290",
            "title": "High-level change detection in RDF(S) KBs",
            "abstract": "With the increasing use of Web 2.0 to create, disseminate, and consume large volumes of data, more and more information is published and becomes available for potential data consumers, that is, applications/services, individual users and communities, outside their production site. The most representative example of this trend is Linked Open Data (LOD), a set of interlinked data and knowledge bases. The main challenge in this context is data governance within loosely coordinated organizations that are publishing added-value interlinked data on the Web, bringing together issues related to data management and data quality, in order to support the full lifecycle of data production, consumption, and management. In this article, we are interested in curation issues for RDF(S) data, which is the default data model for LOD. In particular, we are addressing change management for RDF(S) data maintained by large communities (scientists, librarians, etc.) which act as curators to ensure high quality of data. Such curated Knowledge Bases (KBs) are constantly evolving for various reasons, such as the inclusion of new experimental evidence or observations, or the correction of erroneous conceptualizations. Managing such changes poses several research problems, including the problem of detecting the changes (delta) between versions of the same KB developed and maintained by different groups of curators, a crucial task for assisting them in understanding the involved changes. This becomes all the more important as curated KBs are interconnected (through copying or referencing) and thus changes need to be propagated from one KB to another either within or across communities. This article addresses this problem by proposing a change language which allows the formulation of concise and intuitive deltas. The language is expressive enough to describe unambiguously any possible change encountered in curated KBs expressed in RDF(S), and can be efficiently and deterministically detected in an automated way. Moreover, we devise a change detection algorithm which is sound and complete with respect to the aforementioned language, and study appropriate semantics for executing the deltas expressed in our language in order to move backwards and forwards in a multiversion repository, using only the corresponding deltas. Finally, we evaluate through experiments the effectiveness and efficiency of our algorithms using real ontologies from the cultural, bioinformatics, and entertainment domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2835399",
                    "name": "Vicky Papavasileiou"
                },
                {
                    "authorId": "1684066",
                    "name": "G. Flouris"
                },
                {
                    "authorId": "1791376",
                    "name": "I. Fundulaki"
                },
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "185d0a08ee1bfe009c6a2161e7a7eb664e07972d",
            "title": "Diachronic linked data: towards long-term preservation of structured interrelated information",
            "abstract": "The Linked Data Paradigm is a promising technology for publishing, sharing, and connecting data on the Web, which provides new perspectives for data integration and interoperability. However, the proliferation of distributed, interconnected linked data sources on the Web poses significant new challenges for consistently managing the vast number of potentially large datasets and their interdependencies. In this article we focus on the key problem of preserving evolving structured interlinked data. We argue that a number of issues, which hinder applications and users, are related to the temporal aspect that is intrinsic in Linked Data. We present three use cases to motivate our approach, we discuss problems that occur, and propose a direction for a solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145044578",
                    "name": "S. Auer"
                },
                {
                    "authorId": "7974521",
                    "name": "Theodore Dalamagas"
                },
                {
                    "authorId": "145458253",
                    "name": "H. Parkinson"
                },
                {
                    "authorId": "1750270",
                    "name": "F. Bancilhon"
                },
                {
                    "authorId": "1684066",
                    "name": "G. Flouris"
                },
                {
                    "authorId": "1760642",
                    "name": "Dimitris Sacharidis"
                },
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                },
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "1794648",
                    "name": "Y. Stavrakas"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2325154",
                    "name": "George Papastefanatos"
                },
                {
                    "authorId": "147260820",
                    "name": "Kostas Thiveos"
                }
            ]
        },
        {
            "paperId": "4fdc0ccb2afb2a47135f4852f21f750ad50d6b10",
            "title": "Indexes Analysis for Matching Subscriptions in RSS feeds",
            "abstract": "The explosion of published information on the Web leads to the emergence of a Web syndication paradigm, which transforms the passive reader into an active information collector. Information consumers subscribe to RSS/Atom feeds and are notified whenever a piece of news (item) is published. The success of this Web syndication now offered on Web sites, blogs, and social media, however raises scalability issues. There is a vital need for efficient real-time filtering methods across feeds, to allow users to follow effectively personally interesting information. We investigate in this paper three indexing techniques for users' subscriptions based on inverted lists or on an ordered trie. We present analytical models for memory requirements and matching time and we conduct a thorough experimental evaluation to exhibit the impact of critical workload parameters on these structures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3278251",
                    "name": "Zeinab Hmedeh"
                },
                {
                    "authorId": "2722270",
                    "name": "Harris Kourdounakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1807596",
                    "name": "C\u00e9dric du Mouza"
                },
                {
                    "authorId": "144911319",
                    "name": "M. Scholl"
                },
                {
                    "authorId": "1763974",
                    "name": "Nicolas Travers"
                }
            ]
        },
        {
            "paperId": "cccaae9a3a230af57a1cd709b40ad82918c9b69c",
            "title": "Processing continuous text queries featuring non-homogeneous scoring functions",
            "abstract": "In this work we are interested in the scalable processing of content filtering queries over text item streams. In particular, we are aiming to generalize state of the art solutions with non-homogeneous scoring functions combining query-independent item importance with query-dependent content relevance. While such complex ranking functions are widely used in web search engines this is to our knowledge the first scientific work studying their usage in a continuous query scenario. Our main contribution consists in the definition and the evaluation of new efficient in-memory data structures for indexing continuous top-k queries based on an original two-dimensional representation of text queries. We are exploring locally-optimal score bounds and heuristics that efficiently prune the search space of candidate top-k query results which have to be updated at the arrival of new stream items. Finally, we experimentally evaluate memory/matching time trade-offs of these index structures. In particular we experimentally illustrate their linear scaling behavior with respect to the number of indexed queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2512311",
                    "name": "Nelly Vouzoukidou"
                },
                {
                    "authorId": "1747227",
                    "name": "B. Amann"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "e39db8356665097d899a86541f2ef6c4b4ccc77b",
            "title": "Heuristics-based query optimisation for SPARQL",
            "abstract": "Query optimization in RDF Stores is a challenging problem as SPARQL queries typically contain many more joins than equivalent relational plans, and hence lead to a large join order search space. In such cases, cost-based query optimization often is not possible. One practical reason for this is that statistics typically are missing in web scale setting such as the Linked Open Datasets (LOD). The more profound reason is that due to the absence of schematic structure in RDF, join-hit ratio estimation requires complicated forms of correlated join statistics; and currently there are no methods to identify the relevant correlations beforehand. For this reason, the use of good heuristics is essential in SPARQL query optimization, even in the case that are partially used with cost-based statistics (i.e., hybrid query optimization). In this paper we describe a set of useful heuristics for SPARQL query optimizers. We present these in the context of a new Heuristic SPARQL Planner (HSP) that is capable of exploiting the syntactic and the structural variations of the triple patterns in a SPARQL query in order to choose an execution plan without the need of any cost model. For this, we define the variable graph and we show a reduction of the SPARQL query optimization problem to the maximum weight independent set problem. We implemented our planner on top of the MonetDB open source column-store and evaluated its effectiveness against the state-of-the-art RDF-3X engine as well as comparing the plan quality with a relational (SQL) equivalent of the benchmarks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "3113009",
                    "name": "Petros Tsialiamanis"
                },
                {
                    "authorId": "2469220",
                    "name": "Lefteris Sidirourgos"
                },
                {
                    "authorId": "1791376",
                    "name": "I. Fundulaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1687211",
                    "name": "P. Boncz"
                }
            ]
        },
        {
            "paperId": "e72f99f04c9ffde2ae5b5ffded6fb82d7c3a7a7a",
            "title": "Subscription indexes for web syndication systems",
            "abstract": "The explosion of published information on the Web leads to the emergence of a Web syndication paradigm, which transforms the passive reader into an active information collector. Information consumers subscribe to RSS/Atom feeds and are notified whenever a piece of news (item) is published. The success of this Web syndication now offered on Web sites, blogs, and social media, however raises scalability issues. There is a vital need for efficient real-time filtering methods across feeds, to allow users to follow effectively personally interesting information. We investigate in this paper three indexing techniques for users' subscriptions based on inverted lists or on an ordered trie. We present analytical models for memory requirements and matching time and we conduct a thorough experimental evaluation to exhibit the impact of critical workload parameters on these structures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3278251",
                    "name": "Zeinab Hmedeh"
                },
                {
                    "authorId": "2722270",
                    "name": "Harris Kourdounakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1807596",
                    "name": "C\u00e9dric du Mouza"
                },
                {
                    "authorId": "144911319",
                    "name": "M. Scholl"
                },
                {
                    "authorId": "1763974",
                    "name": "Nicolas Travers"
                }
            ]
        },
        {
            "paperId": "71430c804e5a681267f06d295c394e059aabf963",
            "title": "Optimizing large collections of continuous content-based RSS aggregation queries",
            "abstract": "In this article we present RoSeS (Really Open Simple and Efficient Syndication), a generic framework for content-based RSS feed querying and aggregation. RoSeS is based on a data-centric approach, using a combination of standard database concepts like declarative query languages, views and multi-query optimization. Users create personalized feeds by defining and composing content-based filtering and aggregation queries on collections of RSS feeds. Publishing these queries corresponds to defining views which can then be used for building new queries / feeds. This naturally reflects the publish-subscribe nature of RSS applications. The contributions presented in this article are a declarative RSS feed aggregation language, an extensible stream algebra for building efficient continuous multiquery execution plans for RSS aggregation views, a multi-query optimization strategy for these plans and a running prototype based on a multi-threaded asynchronous execution",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10660657",
                    "name": "J. Creus"
                },
                {
                    "authorId": "1747227",
                    "name": "B. Amann"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1763974",
                    "name": "Nicolas Travers"
                },
                {
                    "authorId": "1757944",
                    "name": "D. Vodislav"
                }
            ]
        },
        {
            "paperId": "85650705b3ea9db125b9d67f97f53dd36bc30708",
            "title": "On Computing Deltas of RDF/S Knowledge Bases",
            "abstract": "The ability to compute the differences that exist between two RDF/S Knowledge Bases (KB) is an important step to cope with the evolving nature of the Semantic Web (SW). In particular, RDF/S deltas can be employed to reduce the amount of data that need to be exchanged and managed over the network in order to build SW synchronization and versioning services. By considering deltas as sets of change operations, in this article we introduce various RDF/S differential functions which take into account inferred knowledge from an RDF/S knowledge base. We first study their correctness in transforming a source to a target RDF/S knowledge base in conjunction with the semantics of the employed change operations (i.e., with or without side-effects on inferred knowledge). Then we formally analyze desired properties of RDF/S deltas such as size minimality, semantic identity, redundancy elimination, reversibility, and composability, as well as identify those RDF/S differential functions that satisfy them. Subsequently, we experimentally evaluate the computing time and size of the produced deltas over real and synthetic RDF/S knowledge bases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3150649",
                    "name": "Dimitris Zeginis"
                },
                {
                    "authorId": "1801959",
                    "name": "Yannis Tzitzikas"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "85f5be59ef9fae9fc343d5652b61162dbf499563",
            "title": "On Provenance of Queries on Semantic Web Data",
            "abstract": "Capturing trustworthiness, reputation, and reliability of Semantic Web data manipulated by SPARQL requires researchers to represent adequate provenance information, usually modeled as source data annotations and propagated to query results along with a query evaluation. Alternatively, abstract provenance models can capture the relationship between query results and source data by taking into account the employed query operators. The authors argue the benefits of the latter for settings in which query results are materialized in several repositories and analyzed by multiple users. They also investigate how relational provenance models can be leveraged for SPARQL queries, and advocate for new provenance models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2228792",
                    "name": "Yannis Theoharis"
                },
                {
                    "authorId": "1791376",
                    "name": "I. Fundulaki"
                },
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "07f3d312aa5dda18f9baf65192cd999c7c3f9cb1",
            "title": "KP-LAB Knowledge Practices Laboratory -- Specifications for the Knowledge Matchmaker (V.2.0), the Knowledge Synthesizer (V.1.0) and the Analytical and Knowledge Mining Services (V.1.0)",
            "abstract": "This deliverable presents specifications of three components responsible for advanced manipulation with the knowledge stored in the KP-Lab Semantic Web Knowledge Middleware (SWKM). It starts with motivating scenarios defined within various Working Knots (WKs), extracting relevant functional requirements and mapping them on the high-level requirements, of particular driving objectives and user tasks (described in deliverable [D2.4]). The first component is Knowledge Matchmaker (V2.0), which utilizes various text mining, information extraction, and heuristic methods for advanced access to and manipulation with shared knowledge artefacts according to the explicit meaning of artefacts expressed by their textual content, as well as metadata, including semantic tagging. This second version presents a set of completely new services supporting miscellaneous functionalities such as support for semantic tagging process, search for similar artefacts, information extraction capabilities, as well as recommendation services. Next two components are completely new. The Knowledge Synthesizer (V1.0) can be used to combine information found in multiple sources; this feature is necessary to allow automated merging of the conceptualizations modeled in independently edited conceptualizations. The Analytical and Knowledge Mining Services (V1.0) provide means for analyzing participation and activities within past or running knowledge creation processes, as well as for support of knowledge evolution analysis (e.g. via identification of critical patterns in selected knowledge creation processes).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8156143",
                    "name": "J\u00e1n Parali\u010d"
                },
                {
                    "authorId": "3186048",
                    "name": "K. Furd\u00edk"
                },
                {
                    "authorId": "5444046",
                    "name": "P. Bednar"
                },
                {
                    "authorId": "2261066",
                    "name": "F. Babi\u010d"
                },
                {
                    "authorId": "144951382",
                    "name": "J. Wagner"
                },
                {
                    "authorId": "2115764749",
                    "name": "Marek Schmidt"
                },
                {
                    "authorId": "143917717",
                    "name": "P. Smrz"
                },
                {
                    "authorId": "2908101",
                    "name": "N. Spyratos"
                },
                {
                    "authorId": "144831359",
                    "name": "E. Simonenko"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1684066",
                    "name": "G. Flouris"
                },
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "2424598",
                    "name": "Yannis Rousakis"
                }
            ]
        },
        {
            "paperId": "0dbee3dab1bf086a5ed21e528aee7b0e3fba6513",
            "title": "Specifications and Prototype of the Knowledge Repository (V.3.0) and the Knowledge Mediator (V.3.0).",
            "abstract": "This deliverable reports the technical and research development performed until M36 (January 2009) within tasks T5.2 and T5.4 of WP5 in the KP-Lab project, per the latest Description of Work (DoW) 3.2 [DoW3.2]. The described components are included in the KP-Lab Semantic Web Knowledge Middleware (SWKM) Prototype Release 3.0 software that takes place in M36. This release builds on the Prototype Release 2.0 that was presented in [D5.4]. The present deliverable includes both the specification, as well as the implementation details for the described components. The description of the features of the new functionalities is provided based on the motivating scenarios and the subsequent functional requirements. The focus and the high-level objective of the new services is the provision of improved scalability and modularity properties on the existing services, as well as improved management abilities upon conceptualizations. The implementation of the services is described by providing the related services\u0092 signatures, their proper way of use, the accepted input parameters, as well as their preconditions and effects. Initially, we describe the Delete Service, which is a Knowledge Repository service allowing the removal of existing namespaces from the repository; such removal includes the deletion of the contents of said namespaces, as well as the deletion of any reference to the namespaces themselves that exists in the repository. This new service enhances SWKM management capabilities upon conceptualizations. Then, the Named Graphs functionality is described, which is a new feature that allows a very flexible modularization of the information found in RDF KBs. We describe in detail the semantics of this feature, as well as the offered capabilities for querying and updating RDF KBs that include modularization information (i.e., information on named graphs) and the implications from their use. Finally, in the context of the Knowledge Mediator, we present the Persistent Comparison Service, which is a variation of the existing (Main Memory) Comparison Service (see M24 release, [D5.3], [D5.4]); unlike the original version, the new service works exclusively on the persistent storage, guaranteeing improved scalability features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292917418",
                    "name": "Dimitris Andreou"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1684066",
                    "name": "G. Flouris"
                },
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "2982938",
                    "name": "P. Pediaditis"
                },
                {
                    "authorId": "3113009",
                    "name": "Petros Tsialiamanis"
                }
            ]
        },
        {
            "paperId": "b8216d58def71eb66edd75e96f27058f88d90598",
            "title": "On Explicit Provenance Management in RDF/S Graphs",
            "abstract": "The notion of RDF Named Graphs has been proposed in order to assign provenance information to data described using RDF triples. In this paper, we argue that named graphs alone cannot capture provenance information in the presence of RDFS reasoning and updates. In order to address this problem, we introduce the notion of RDF/S Graphsets: a graphset is associated with a set of RDF named graphs and contain the triples that are jointly owned by the named graphs that constitute the graphset. We formalize the notions of RDF named graphs and RDF/S graphsets and propose query and update languages that can be used to handle provenance information for RDF/S graphs taking into account RDFS semantics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2982938",
                    "name": "P. Pediaditis"
                },
                {
                    "authorId": "1684066",
                    "name": "G. Flouris"
                },
                {
                    "authorId": "1791376",
                    "name": "I. Fundulaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "004e50963540a9d7f1aa20fa9961c112604c319a",
            "title": "A Formal Approach for RDF/S Ontology Evolution",
            "abstract": "In this paper, we consider the problem of ontology evolution in the face of a change operation. We devise a general-purpose algorithm for determining the effects and side-effects of a requested elementary or complex change operation. Our work is inspired by belief revision principles (i.e., validity, success and minimal change) and allows us to handle any change operation in a provably rational and consistent manner. To the best of our knowledge, this is the first approach overcoming the limitations of existing solutions, which deal with each change operation on a per-case basis. Additionally, we rely on our general change handling algorithm to implement specialized versions of it, one per desired change operation, in order to compute the equivalent set of effects and side-effects. This work was partially supported by the EU projects CASPAR (FP6-2005-IST-033572) and KP-LAB (FP6-2004-IST-27490).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152576198",
                    "name": "G. Konstantinidis"
                },
                {
                    "authorId": "1684066",
                    "name": "G. Flouris"
                },
                {
                    "authorId": "1746617",
                    "name": "G. Antoniou"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "8858684a46d30d93eb14715c4bde1f8c02a2aa9d",
            "title": "Efficient Rewriting Algorithms for Preference Queries",
            "abstract": "Preference queries are crucial for various applications (e.g. digital libraries) as they allow users to discover and order data of interest in a personalized way. In this paper, we define preferences as preorders over relational attributes and their respective domains. Then, we rely on appropriate linearizations to provide a natural semantics for the block sequence answering a preference query. Moreover, we introduce two novel rewriting algorithms (called LBA and TBA) which exploit the semantics of preference expressions for constructing progressively each block of the answer. We demonstrate experimentally the scalability and performance gains of our algorithms (up to 3 orders of magnitude) for variable database and result sizes, as well as for preference expressions of variable size and structure. To the best of our knowledge, LBA and TBA are the first algorithms for evaluating efficiently arbitrary preference queries over voluminous databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153051953",
                    "name": "Periklis Georgiadis"
                },
                {
                    "authorId": "2138559",
                    "name": "Ioannis Kapantaidakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1698173",
                    "name": "Elhadji Mamadou Nguer"
                },
                {
                    "authorId": "2908101",
                    "name": "N. Spyratos"
                }
            ]
        },
        {
            "paperId": "07c6d7d1d6018fc5292a024003372f7dc2b8f435",
            "title": "Designing personalized curricula based on student preferences",
            "abstract": "We address the problem of generating entire course sequences, given a set of target skills along with possibly prioritized student preferences over course descriptions. Compared to logic frameworks formulating course sequencing as a planning problem, our work relies on a set-theoretic framework for generating course sequences using preference-based queries. We introduce the concept of ordered partition for sequencing, and the ordered product of partitions, when it is necessary to combine more than one preference orderings. In our context, ordered partitions originate from preferences expressed over general relations, rather than on functional attributes of traditional database tuples (or objects) addressed by other approaches. We believe that the proposed framework is expressive enough to produce course sequences from descriptions expressed in diverse data models (e.g., XML, RDF/S) with respect to a variety of user preferences, also including priorities over the preferences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153051953",
                    "name": "Periklis Georgiadis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2908101",
                    "name": "N. Spyratos"
                }
            ]
        },
        {
            "paperId": "8099b52cc46b3266c08d0e0d5dc4486435cfe1f5",
            "title": "Report on the First International Workshop on Database Preservation (PresDB'07)",
            "abstract": "The need to preserve scientific, scholarly and cultural data has long been recognized. These data sets are valuable and many of them are either impossible to reproduce (e.g. climate and demographic data) or can only be recovered at enormous costs (e.g. data from high energy physics experiments). While substantial investment has been made in archiving and preserving conventional forms of these objects, such as documents, images and numerical data in some file format, the need to preserve entire databases has only recently emerged. Databases differ from fixed digital objects studied in the past, in that they change over time, they have internal structure, and they include schemas and integrity constraints, which are basic for the current and future interpretation of the data. Increasingly, database technology is being used in the storage of large numerical scientific data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "5963711",
                    "name": "P. Buneman"
                }
            ]
        },
        {
            "paperId": "8307a25bd5b63ac698eb5630b015c6584862ee0c",
            "title": "KP-LAB Knowledge Practices Laboratory -- Specification of the SWKM Knowledge Evolution, Recommendation, and Mining services",
            "abstract": "This deliverable presents the deep-level specification for the second release (M24) of the components responsible for advanced manipulation with the knowledge stored in the KP-Lab Semantic Web Knowledge Middleware (SWKM). The two components were defined in [D5.1] as Knowledge Mediator and Knowledge Matchmaker. The Knowledge Mediator services (change, comparison, versioning and registry) aim at providing functionalities to support evolving ontologies and RDF Knowledge Bases (KBs). Upon a change request, the change service will automatically determine the effects and side-effects of the request and present it to the caller for validation. A comparison service is necessary to allow one to compare two versions of an ontology or RDF KB and identify their differences. The above functionalities are coupled with a versioning system, which is used to make different versions of the same ontology (or RDF KB) persistent, and with the registry service, which allows the user to classify the stored ontologies, using some related metadata for easy access and manipulation. The Knowledge Matchmaker supports advanced mining and notification services for knowledge artefacts. It essentially enables to cluster/classify available information resources with respect to the employed ontologies, as well as, to notify about changes to content items produced/consumed within a group of learners according to explicitly subscribed preferences [DoWB]. The Notification service supports access to the knowledge repository for KP-Lab users (i.e. individual human users as well as various tools or software components) by keeping them aware of changes. Users will be able to subscribe their preferences to the KP-Lab system in order to be notified about the changes in the knowledge repository. Events (modifications) in the repository are matched with the subscriptions and notifications are propagated automatically to the users. Text Mining services are used to assist users when creating or updating the semantic descriptions of KP-Lab knowledge artefacts. The Classification Service, after a software-training period, will classify the artefacts under some pre-defined set of categories (e.g., ontology concepts), resulting in a semi-automatic generation of semantic descriptions. The Clustering Service will look for clusters of similar artefacts and automatically acquire conceptual maps from knowledge artefacts. This can lead to the update or even the creation of (new) KP-Lab ontologies managed in the sequel by the Knowledge Mediator. The services are described along with the proposed functionality for each one, based upon the motivating scenarios and the subsequent functional requirements. The functionality of the services is presented from the end-user perspective and divided into parts that form the major components of the SWKM knowledge evolution, recommendation and mining services architecture.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143917717",
                    "name": "P. Smrz"
                },
                {
                    "authorId": "50975551",
                    "name": "Vil\u00e9m Sklen\u00e1k"
                },
                {
                    "authorId": "1740821",
                    "name": "V. Sv\u00e1tek"
                },
                {
                    "authorId": "2191108",
                    "name": "Martin Kavalec"
                },
                {
                    "authorId": "2176813",
                    "name": "Martin Svihla"
                },
                {
                    "authorId": "8156143",
                    "name": "J\u00e1n Parali\u010d"
                },
                {
                    "authorId": "3186048",
                    "name": "K. Furd\u00edk"
                },
                {
                    "authorId": "5444046",
                    "name": "P. Bednar"
                },
                {
                    "authorId": "3271692",
                    "name": "Peter Smatana"
                },
                {
                    "authorId": "2908101",
                    "name": "N. Spyratos"
                },
                {
                    "authorId": "65922388",
                    "name": "Hanen Belhajfrej"
                },
                {
                    "authorId": "14539326",
                    "name": "M. Nguer"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "1801959",
                    "name": "Yannis Tzitzikas"
                },
                {
                    "authorId": "1684066",
                    "name": "G. Flouris"
                },
                {
                    "authorId": "66992002",
                    "name": "G. Markakis"
                }
            ]
        },
        {
            "paperId": "f0276e28858cc07a9ec97f0970cab9d2f8f1c35d",
            "title": "Emergent Knowledge Artifacts for Supporting Trialogical E-Learning",
            "abstract": "This article elaborates on scenarios for collaborative knowledge creation in the spirit of the trialogical learning paradigm. According to these scenarios, the group knowledge base is formed by combining the knowledge bases of the participants, according to various methods. The provision of flexible methods for defining various aspects of the group knowledge is expected to enhance synergy in the knowledge creation process and could lead to the development of tools that overcome the inelasticities of the current knowledge creation practices. Subsequently, these scenarios are projected to various knowledge representation frameworks and for each one of them, we analyze and discuss related techniques and identify issues that are worth further research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1801959",
                    "name": "Yannis Tzitzikas"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1684066",
                    "name": "G. Flouris"
                },
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "30453385",
                    "name": "H. Markkanen"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                },
                {
                    "authorId": "2908101",
                    "name": "N. Spyratos"
                }
            ]
        },
        {
            "paperId": "74c6609391ea6d9a260361b9621af5c165566c70",
            "title": "Mediating RDF/S Queries to Relational and XML Sources",
            "abstract": "Semantic Web (SW) technology aims to facilitate the integration of legacy data sources spread worldwide. Despite the plethora of SW languages (e.g., RDF/S, OWL) recently proposed for supporting large-scale information interoperation, the vast majority of legacy sources still rely on relational databases (RDB) published on the Web or corporate intranets as virtual XML. In this article, we advocate a first-order logic framework for mediating high-level queries to relational and/or XML sources using community ontologies expressed in a SW language such as RDF/S. We describe the architecture and reasoning services of our SW integration middleware, termed SWIM, and we present the main design choices and techniques for supporting powerful mappings between different data models, as well as reformulation and optimization of queries expressed against mediator ontologies and views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2736084",
                    "name": "I. Koffina"
                },
                {
                    "authorId": "2849093",
                    "name": "G. Serfiotis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "294491b5cb87526bb5c4c216f1c8222d9ea77ded",
            "title": "KP-LAB Knowledge Practices Laboratory -- Specification of the SWKM Architecture (V1.0) and Core Services",
            "abstract": "The objective of WP5 is to develop a generic middleware supporting knowledge management services for \u0091trialogical\u0092 learning applications. More precisely, the KPLab Semantic Web Knowledge Middleware (SWKM) aims to facilitate knowledge creation processes by supporting advanced interactions of collaborating learners (or workers) with knowledge artefacts (i.e. discovery, access, evolution, recommendation and mining). In this deliverable we present the high-level functionality of SWKM along with the Service-Oriented Software Architecture of the prototype system that will be developed by M12 (V 1.0) and the subsequent phases of the project. The proposed architecture broadly distinguishes three generic modules of the SWKM, i.e. the Knowledge Repository \u0096 responsible for the provision of scalable persistence services for large volumes of knowledge artefacts\u0092 descriptions and ontologies; the Knowledge Mediator \u0096 responsible for the provision of services handling the main registry, discovery and evolution for KP-Lab knowledge artefacts; and the Knowledge Matchmaker \u0096 responsible for the provision of services that support interactions of KP-Lab users with knowledge artefacts employing their semantic descriptions. The services corresponding to each one of these modules are described along with the proposed functionality for each one, based upon the motivating scenarios and the subsequent definition of the key concepts of Trialogical Learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "1801959",
                    "name": "Yannis Tzitzikas"
                },
                {
                    "authorId": "2908101",
                    "name": "N. Spyratos"
                },
                {
                    "authorId": "65922388",
                    "name": "Hanen Belhajfrej"
                },
                {
                    "authorId": "14539326",
                    "name": "M. Nguer"
                },
                {
                    "authorId": "8156143",
                    "name": "J\u00e1n Parali\u010d"
                },
                {
                    "authorId": "3186048",
                    "name": "K. Furd\u00edk"
                },
                {
                    "authorId": "3271692",
                    "name": "Peter Smatana"
                },
                {
                    "authorId": "1832373",
                    "name": "M. Sarnovsk\u00fd"
                },
                {
                    "authorId": "5444046",
                    "name": "P. Bednar"
                },
                {
                    "authorId": "143917717",
                    "name": "P. Smrz"
                },
                {
                    "authorId": "50975551",
                    "name": "Vil\u00e9m Sklen\u00e1k"
                },
                {
                    "authorId": "1740821",
                    "name": "V. Sv\u00e1tek"
                },
                {
                    "authorId": "2191108",
                    "name": "Martin Kavalec"
                },
                {
                    "authorId": "2176813",
                    "name": "Martin Svihla"
                }
            ]
        },
        {
            "paperId": "51f2b2e9bcfd5e759e306881216b5e50171101b6",
            "title": "Online curriculum on the semantic Web: the CSD-UoC portal for peer-to-peer e-learning",
            "abstract": "Online Curriculum Portals aim to support networks of instructors and learners by providing a space of convergence for enhancing peer-to-peer learning interactions among individuals of an educational institution. To this end, effective, open and scalable e-learning systems are required to acquire, store, and share knowledge under the form of learning objects (LO). In this paper, we are interested in exploiting the semantic relationships that characterize these LOs (e.g., prerequisite, part-of or see-also) in order to capture and access individual and group knowledge in conjunction with the learning processes supported by educational institutions. To achieve this functionality, Semantic Web (e.g., RDF/s) and declarative query languages (e.g., RQL) are employed to represent LOs and their relationships (e.g., LOM), as well as, to support navigation at the conceptual e-learning Portal space. In this way, different LOs could be presented to the same learners, according to the traversed schema navigation paths (i.e., learning paths). Using the Apache Jetspeed framework we are able to generate and assemble at run-time portlets (i.e., pluggable web components) for visualizing personalized views as dynamic web pages. Last but not least, both learners and instructors can employ the same Portal GUI for updating semantically described LOs and thus support an open-ended continuum of learning. To the best of our knowledge, the work presented in this paper is the first Online Curriculum Portal platform supporting the aforementioned functionality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2078908",
                    "name": "D. Kotzinos"
                },
                {
                    "authorId": "1984066",
                    "name": "Sofia Pediaditaki"
                },
                {
                    "authorId": "3308344",
                    "name": "A. Apostolidis"
                },
                {
                    "authorId": "3215289",
                    "name": "Nikolaos Athanasis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "b5c120ca06cc39fff376089352ad48779f669c7f",
            "title": "Adaptive personalisation in self e-learning networks",
            "abstract": "This paper presents some of the personalisation services designed for self e-learning networks in the SeLeNe project. A self e-learning network consists of web-based learning objects that have been made available to the network by its users, along with metadata descriptions of these learning objects and of the network's users. The architecture of the network is distributed and service-oriented. The personalisation facilities include: querying learning object descriptions to return results tailored towards users' individual goals and preferences; the ability to define views over the learning object metadata; facilities for defining new composite learning objects; and facilities for subscribing to personalised event and change notification services.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2702383",
                    "name": "Kevin Keenoy"
                },
                {
                    "authorId": "1747099",
                    "name": "A. Poulovassilis"
                },
                {
                    "authorId": "2099375",
                    "name": "G. Papamarkos"
                },
                {
                    "authorId": "20590156",
                    "name": "P. Wood"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "2944836",
                    "name": "Aimilia Magkanaraki"
                },
                {
                    "authorId": "35146422",
                    "name": "M. Stratakis"
                },
                {
                    "authorId": "1744491",
                    "name": "P. Rigaux"
                },
                {
                    "authorId": "2908101",
                    "name": "N. Spyratos"
                }
            ]
        },
        {
            "paperId": "a954ee1734d7c96c8ed7b498b34dce806594d6f4",
            "title": "Report on the workshop on metadata management in grid and peer-to-peer systems, London, December 16 2003",
            "abstract": "A workshop on <i>Metadata Management in Grid and Peer-to-Peer Systems</i> was held in the Senate House of the University of London on December 16 2003.<sup>1</sup> The workshop was organised by the <i>SeLeNe</i> (Self e-Learning Networks) IST project as part of its dissemination activities.<sup>2</sup> The goal of the workshop was to identify recent technological achievements and open challenges regarding metadata management in novel applications requiring peer-to-peer information management in a distributed or Grid setting. The target audience for this event were researchers from the Grid, peer-to-peer and e-learning communities, as well as other application areas requiring Grid and/or peer-to-peer support. The event attracted 43 participants from 8 different European countries, and we believe that it was an important step in coordinating research activities in these inter-related areas. The presentations at the workshop fell into one of four sessions, each of which we report on below.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2702383",
                    "name": "Kevin Keenoy"
                },
                {
                    "authorId": "1747099",
                    "name": "A. Poulovassilis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "144629886",
                    "name": "G. Loizou"
                },
                {
                    "authorId": "2200401",
                    "name": "G. Kokkinidis"
                },
                {
                    "authorId": "1750869",
                    "name": "G. Samaras"
                },
                {
                    "authorId": "2908101",
                    "name": "N. Spyratos"
                }
            ]
        },
        {
            "paperId": "0ebe7b4a6d858b42c8f5f4604b34dbc059091d78",
            "title": "E-services: a look behind the curtain",
            "abstract": "The emerging paradigm of electronic services promises to bring to distributed computation and services the flexibility that the web has brought to the sharing of documents. An understanding of fundamental properties of e-service composition is required in order to take full advantage of the paradigm. This paper examines proposals and standards for e-services from the perspectives of XML, data management, workflow, and process models. Key areas for study are identified, including behavioral service signatures, verification and synthesis techniques for composite services, analysis of service data manipulation commands, and XML analysis applied to service specifications. We give a sample of the relevant results and techniques in each of these areas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144837668",
                    "name": "R. Hull"
                },
                {
                    "authorId": "1750856",
                    "name": "Michael Benedikt"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "11254357",
                    "name": "Jianwen Su"
                }
            ]
        },
        {
            "paperId": "31eb8c088b74f352dbbf48ca4cf8fd77e9728e22",
            "title": "The ICS-FORTH SWIM: A Powerful Semantic Web Integration Middleware",
            "abstract": "Semantic Web (SW) technology aims to facilitate the integration of legacy data sources spread worldwide. Despite the plethora of SW languages (e.g., RDF/S, DAML+OIL, OWL) recently proposed for supporting large scale information interoperation, the vast majority of legacy sources still rely on relational databases (RDB) published on the Web or corporate intranets as virtual XML. In this paper, we advocate a Datalog framework for mediating high-level queries to relational and/or XML sources using community ontologies expressed in a SW language such as RDF/S. We describe the architecture and the reasoning services of our SW integration middleware, called SWIM, and we present the main design choices and techniques for supporting powerful mappings between different data models, as well as, reformulation and optimization of queries expressed against mediation schemas and views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "2736084",
                    "name": "I. Koffina"
                },
                {
                    "authorId": "2200401",
                    "name": "G. Kokkinidis"
                },
                {
                    "authorId": "2944836",
                    "name": "Aimilia Magkanaraki"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                },
                {
                    "authorId": "2849093",
                    "name": "G. Serfiotis"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "3ad8bc411b40480ec7996505156ff570f82a2669",
            "title": "The ICS-FORTH Semantic Web Integration Middleware (SWIM)",
            "abstract": "A cornerstone issue in the realization of the Semantic Web (SW) vision is the achievement of semantic interoperability among legacy data sources spread worldwide. In order to capture information semantics in a machine processable way, various ontology-based formalisms have been recently proposed (e.g., RDF/S [20, 5], DAML+OIL [27], OWL [10]). However, the vast majority of existing legacy data is not yet in RDF/S or any other SW language [23, 25]. As a matter of fact, most of the data is physically stored in relational database (RDB) systems and actually published on the Web or corporate intranets as virtual XML. SW applications, however, require to view data as virtual RDF, valid instance of a domain or application specific RDF/S schema, and to be able to manipulate them with high-level query languages, such as RQL [18] or RVL [24]. Therefore, we need middleware systems that can either republish XML as RDF, or publish RDB data directly as RDF, or even better be capable of doing both. Sometimes the practical solution will be to rely just on the virtual XML schema and XML query interface of an existing XML publishing system. At other times, the SW publishing middleware will be built as an alternative to the XML publishing system, taking advantage of direct access to the underlying RDB management system (RDBMS). It is also possible that the SW middleware will have to integrate data in some RDBMS with data in native XML storage. We need to deal flexibly with all these situations in a uniform framework. A decade of experience with information integration architectures based on mediators [8, 29, 26, 21] suggests that it is highly beneficial to (semi)automatically generate such systems from succinct formal specifications, rather than programming their semantics into low-level code. This greatly enhances the maintainability and reliability of the systems in an environment of often revised and shifting requirements. This paper presents the fundamental ideas for devising a comprehensive framework that allows user communities to 1. specify XML \u2192 RDF and RDB \u2192 RDF mappings; 2. verify that these mappings conform to the semantics of the employed SW ontologies; 3. compose RQL queries with these mappings and produce XML or RDB queries; 4. specify further levels of abstraction as RDF \u2192 RDF views; 5. compose RQL queries with such views; 6. perform query optimizations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "2944836",
                    "name": "Aimilia Magkanaraki"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                },
                {
                    "authorId": "144879888",
                    "name": "V. Tannen"
                }
            ]
        },
        {
            "paperId": "ce9b85c6e6a959426076d9d7e0ed8718161745f2",
            "title": "On labeling schemes for the semantic web",
            "abstract": "This paper focuses on the optimization of the navigation through voluminous subsumption hierarchies of topics employed by Portal Catalogs like Netscape Open Directory (ODP). We advocate for the use of labeling schemes for modeling these hierarchies in order to efficiently answer queries such as subsumption check, descendants, ancestors or nearest common ancestor, which usually require costly transitive closure computations. We first give a qualitative comparison of three main families of schemes, namely bit vector, prefix and interval based schemes. We then show that two labeling schemes are good candidates for an efficient implementation of label querying using standard relational DBMS, namely, the Dewey Prefix scheme [6] and an Interval scheme by Agrawal, Borgida and Jagadish [1]. We compare their storage and query evaluation performance for the 16 ODP hierarchies using the PostgreSQL engine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                },
                {
                    "authorId": "144911319",
                    "name": "M. Scholl"
                },
                {
                    "authorId": "2884211",
                    "name": "Sotirios Tourtounis"
                }
            ]
        },
        {
            "paperId": "e7c633749950dd7d6f105a5129f35d5a207fce21",
            "title": "A Data, Computation and Knowledge Grid the Case of the Arion System",
            "abstract": "The ARION system provides basic e-services of search and retrieval of objects in scientific collections, such as, datasets, simulation models and tools necessary for statistical and/or visualization processing. These collections may represent application software of scientific areas, they reside in geographically disperse organizations and constitute the system content. The user may invoke on-line computations of scientific datasets when the latter are not found into the system. Thus, ARION provides the basic infrastructure for accessing and deriving scientific information in an open, distributed and federated system. 1 ARION (still in progress) is supported by the European Commission under the 5 Framework Programme, IST-200025289, Key Action 3: Digital Heritage and Cultural Content. Web site: http://www.arion-dl.org",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1805579",
                    "name": "C. Houstis"
                },
                {
                    "authorId": "1711456",
                    "name": "S. Lalis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                },
                {
                    "authorId": "1805602",
                    "name": "M. Vavalis"
                },
                {
                    "authorId": "1784394",
                    "name": "M. Pitikakis"
                },
                {
                    "authorId": "1718383",
                    "name": "K. Kritikos"
                },
                {
                    "authorId": "2111997",
                    "name": "Antonis Smardas"
                }
            ]
        },
        {
            "paperId": "07d5ba041140d700d5bbac9ec24719e2d9a08dd9",
            "title": "On Personalizing the Catalogs of Web Portals",
            "abstract": "In this paper we propose a method for personalizing the catalogs of Web Portals. We propose ,a declarative language for defining personal semantic channels over Web Portal catalogs. A semantic channel is actually a view of one or more Portal catalogs. offers powerful primitives for filtering and restructuring available thematic topics and classified resources. A user can connect to a Portal infomediary in order to register, browse, or query his/her semantic channel. We describe the architecture and the functional modules of our infomediary acting as personalization server and we focus on a set of queries which can be considered as an API for browsing semantic channels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2908101",
                    "name": "N. Spyratos"
                },
                {
                    "authorId": "1801959",
                    "name": "Yannis Tzitzikas"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "3d7a9edf9ef874cde4754f35d5e88c29ef1163eb",
            "title": "RQL: a declarative query language for RDF",
            "abstract": "Real-scale Semantic Web applications, such as Knowledge Portals and E-Marketplaces, require the management of large volumes of metadata, i.e., information describing the available Web content and services. Better knowledge about their meaning, usage, accessibility or quality will considerably facilitate an automated processing of Web resources. The Resource Description Framework (RDF) enables the creation and exchange of metadata as normal Web data. Although voluminous RDF descriptions are already appearing, sufficiently expressive declarative languages for querying both RDF descriptions and schemas are still missing. In this paper, we propose a new RDF query language called RQL. It is a typed functional language (a la OQL) and relies on a formal model for directed labeled graphs permitting the interpretation of superimposed resource descriptions by means of one or more RDF schemas. RQL adapts the functionality of semistructured/XML query languages to the peculiarities of RDF but, foremost, it enables to uniformly query both resource descriptions and schemas. We illustrate the RQL syntax, semantics and typing system by means of a set of example queries and report on the performance of our persistent RDF Store employed by the RQL interpreter.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "2189454",
                    "name": "S. Alexaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                },
                {
                    "authorId": "144911319",
                    "name": "M. Scholl"
                }
            ]
        },
        {
            "paperId": "1df111f90e2e09ee9e531a55112025e51334a056",
            "title": "Querying Community Web Portals",
            "abstract": "A new generation of information systems such as organizational memories, vertical aggregators, infomediaries, etc. is emerging nowadays. Such systems, termed Community Web Portals, intend to support speci c communities of interest (e.g., enterprise, professional, trading) on corporate intranets or the Web. More precisely, Portal Catalogs, organize and describe various information resources (e.g., sites, documents, data) for diverse target audiences (corporate, inter-enterprise, e-marketplace, etc.), in a multitude of ways, which are far more exible and complex than those provided by standard (relational or object) databases. Yet, in commercial software for deploying Community Portals, querying is still limited to full-text (or attribute-value) retrieval and more advanced information-seeking needs implies navigational access. Furthermore, recent Web standards for describing resources are completely ignored. In this paper, we propose a declarative language suitable for querying Portal Catalogs created according to the Resource Description Framework (RDF) W3C standard. Our language, called RQL, relies on a formal graph model, that captures the RDF modeling primitives and permits the interpretation of heterogeneous descriptions by means of one or more schemas. In this context, RQL adapts the functionality of semistructured query languages to the peculiarities of RDF but also extends this functionality in order to uniformly query both resource descriptions and related schemas. Then, RQL enables to query Portal Catalogs holding multipurpose descriptions of community resources while preserving a conceptually uni ed view of the Catalog for (sub-)communities employing di erent RDF schemas. RQL is actually used by several applications aiming at building, accessing and personalizing Community Web Portals. This work was partially supported by the European project C-Web (IST-1999-13479).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                }
            ]
        },
        {
            "paperId": "251194d0550cf12fcbb9099d1c11719759ff8016",
            "title": "On Storing Voluminous RDF Descriptions: The Case of Web Portal Catalogs",
            "abstract": "The academic, corporate and industrial interest in information systems such as corporate memories, vertical aggregators, infomediaries, etc. enabling to select, classify and access, in a useful and meaningful way, various information resources (e.g., sites, documents, data) for diverse target audiences has been increasing over the last few years. These systems, termed Community Web Portals, rely on a Catalog holding descriptions, i.e., metadata, about the available resources on corporate intranets or the Web [6]. In order to e ectively disseminate community knowledge, Portal Catalogs assimilate and organize information in a multitude of ways, which are far more exible and powerful than those provided by standard (object, relational) or XML databases [3, 1]. Recent Web standards such as the W3C Resource Description Framework (RDF) [8, 5] are proved to be more suitable for creating and exchanging resource descriptions between Community Webs. In this paper, we address storage issues of voluminous RDF metadata using an object-relational DBMS (ORDBMS) and the catalog of the Open Directory Portal exported in RDF as test metadata. RDF [8] provides i) a Standard Representation Language for metadata based on directed labeled graphs in which nodes are called resources (or literals) and edges are called properties, and ii) an XML syntax for expressing metadata in a form that is both humanly readable and machine understandable. The most distinctive RDF feature is its ability to support superimposed descriptions for the same Web resources, enabling content syndication and hence, automated processing in a variety of application areas (e.g., administration, recommendation, content rating, intellectual property rights, site maps, push channels, etc.). To interpret these descriptions within or across communities, RDF allows for the de nition of schemas [5] i.e., vocabularies of labels for graph nodes (i.e., classes) and edges (i.e., properties) that can be used to describe and query RDF description bases. Many content providers (e.g., ABCNews, CNN, Time Inc.), Web Portals (e.g., Open Directory, CNET, XMLTree), browsers (e.g., Netscape 6.0, W3C Amaya), as well as, emerging application standards for Web data and services syndication (e.g., the RDF Site Summary [4], Dublin Core [15], or the Web Service Description Language [16]) have already adopted RDF. In a nutshell, the growing number of Web resources and the proliferation of description services lead nowadays to large volumes of RDF metadata (e.g., the Open Directory Portal of Netscape comprises over 170M of Subject Topics and 700M of indexed URIs). Storing Web data, such as RDF schema and resource descriptions, in an ORDBMS is not a novel issue. However, existing database representations and benchmarks [7, 13, 12, 14] need to be reassessed in our setting, since: a) RDF has a di erent data model featuring labels on both nodes and edges, as well as, taxonomies of labels (see Section 2), and, hence, demands a di erent querying functionality than in semistructured or XML databases [1]; b) our test metadata contain a signi cantly larger volume of schema information than that used in previous testbeds (i.e., XML elements, attributes). In this paper, we use PostgreSQL to compare the performance of two well-known database representations (generic versus speci c), adapted to the peculiarities of RDF. More precisely, the schema-speci c representation outperforms the generic representation both in the required storage volume and in query execution time. The performance of the two representations has been evaluated with a series of tests involving queries on schema, data or a combination thereof.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189454",
                    "name": "S. Alexaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                }
            ]
        },
        {
            "paperId": "2e0fba2ac049e77e7dd04f3844918e93485dda22",
            "title": "The ICS-FORTH RDFSuite: Managing Voluminous RDF Description Bases",
            "abstract": "Metadata are widely used in order to fully exploit information resources available on corporate intranets or the Internet. The Resource Description Framework (RDF) aims at facilitating the creation and exchange of metadata as any other Web data. The growing number of available information resources and the proliferation of description services in various user communities, lead nowadays to large volumes of RDF metadata. Managing such RDF resource descriptions and schemas with existing low-level APIs and file-based implementations does not ensure fast deployment and easy maintenance of real-scale RDF applications. In this paper, we advocate the use of database technology to support declarative access, as well as, logical and physical independence for voluminous RDF description bases. \n \nWe present RDFSuite, a suite of tools for RDF validation, storage and querying. Specifically, we introduce a formal data model for RDF description bases created using multiple schemas. Next, we present the design of a persistent RDF Store (RSSDB) for loading resource descriptions in an ORDBMS by exploring the available RDF schema knowledge. Our approach preserves the flexibility of RDF in refining schemas and/or enriching descriptions at any time, whilst it outperforms, both in storage volumes and query execution time, other approaches using a monolithic table to represent resource descriptions and schemas under the form of triples. Last, we briefly present RQL, a declarative language for querying both RDF descriptions and schemas, and sketch query evaluation on top of RSSDB.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189454",
                    "name": "S. Alexaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                },
                {
                    "authorId": "1790316",
                    "name": "Karsten Tolle"
                }
            ]
        },
        {
            "paperId": "379f542136245f0050b0dc2d0e2cbd20a56019b3",
            "title": "Conference review: Semantic Web Workshop:: models, architectures and management",
            "abstract": "i n t e l l i g e n c e \u2022 S u m m e r 2 0 0 1 39 Introduction In 1998 the World-Wide Web Consortium (W3C) inaugurated a research initiative centered on the idea of providing semantics for and facilitating the extraction of knowledge from the World-Wide Web. The Semantic Web is a vision of the creator of the WWW, Tim Berners-Lee, who describes it as \u201ca Web of data, documents, or portions of documents, that can be processed directly or indirectly by machines\u201d not just for display purposes, but for automation, integration and reuse across various applications. The primary goal of the Semantic Web is to define infrastructure, standards, and policies facilitating an explicit description of the meaning of Web resources that can be processed both by automated tools and people. This effort towards the next evolution step of the Web has given rise to a large number of research problems that relate to models, architectures, applications, and services for the Semantic Web. The following is a list of research issues that were put forth as themes for soliciting workshop submissions. \u2726 Formal Foundations of Web Metadata Standards \u2726 Semantic Interoperability Frameworks \u2726 Information and Services Brokering Architectures \u2726 Metadata Creation, Extraction, and Storage \u2726 Query Languages for the Semantic Web \u2726 Distributed Inference Services \u2726 Digital Signatures and Web of Trust \u2726 Advanced Resource Discovery Interfaces \u2726 Automated Classification of Web Resources \u2726 Superimposed Web Resource Annotation & RecommendationTools \u2726 Personalization and Intellectual Property Rights \u2726 Semantic Web Applications: Knowledge Portals, Electronic Commerce The objective of the workshop was the creation of a forum for presenting research results in developing infrastructure for the Semantic Web and for enabling and fostering interaction among international researchers. The collocation of the workshop with the European Conference on Digital Libraries broadened the intended scope of the workshop and attracted participation and interaction from industry in addition to the academic and research communities. The workshop\u2019s audience comprised researchers and practitioners in the areas of databases, intelligent information integration, knowledge representation, knowledge management, information retrieval, metadata, Web standards, digital libraries, and others. The workshop was organized as a post-conference one-day workshop at ECDL 2000 in Lisbon, Portugal. Panos Constantopoulos chaired the workshop committee with Vassilis Christophides and Dimitris Plexousakis as Program Committee Co-Chairs. A total of 29 papers were submitted to the workshop. Each paper was peer-reviewed by at least two referees. Despite the overall high quality of the submissions, only nine papers were accepted by the program committee for presentation at the single-day event. Overall, the workshop drew considerable attention at ECDL 2000: 63 registered participants from 22 countries. The workshop was sponsored by ERCIM (the European Research Consortium on Informatics and Mathematics) and the Panos Constantopoulos, Vassilis Christophides, Dimitris Plexousakis University of Crete, Heraklion, Crete, Greece and Institute of Computer Science, Foundation for Research and Technology\u2013Hellas {panos, christop, dp } @ics.forth.gr Semantic Web Workshop: Models, Architectures and Management",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "3000550",
                    "name": "P. Constantopoulos"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                }
            ]
        },
        {
            "paperId": "42bcf201adf0e00d7ca93cb1911956ee9b3d5de3",
            "title": "Querying RDF Descriptions for Community Web Portals",
            "abstract": "Community Web Portals (e.g., digital libraries, vertical aggregators, infomediaries) have become quite popular nowadays in supporting specific communities of interest on corporate intranets or the Web. Portal Catalogs, organize and describe various information resources (e.g., sites, documents, data) for diverse target audiences (corporate, interenterprise, e-marketplace, etc.), in a multitude of ways, which are far more flexible than those provided by standard databases. In this paper, we propose a declarative language suitable for querying Portal Catalogs created according to the Resource Description Framework (RDF) W3C standard. Our language, called RQL, relies on a formal graph model, that captures the RDF modeling primitives and permits the interpretation of superimposed descriptions by means of one or more schemas. In this context, RQL adapts the functionality of semistructured query languages to the peculiarities of RDF but also extends this functionality in order to uniformly query both resource descriptions and related schemas. RQL is used in several projects aiming at building, accessing and personalizing Community Web Portals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                },
                {
                    "authorId": "2189454",
                    "name": "S. Alexaki"
                }
            ]
        },
        {
            "paperId": "5c096f2c42014b4934d7fc68131ab3ed4e687236",
            "title": "A dynamic warehouse for XML Data of the Web.",
            "abstract": "The growth of the Internet and the Web is revolutionizing the way companies interact with their suppliers, partners, and clients, by enabling a substantial automation of the full spectrum of their business activities. As we move into the 21st century economy, the primary form of automation will be B2B e-commerce, in which enterprises interact with each other through entirely automated means. As an example, consider an electronic market place in a vertical industry segment, in which suppliers and buyers tie into a common IT infrastructure to exchange goods and services. This forms a supply chain in which buyers need (a) to investigate possible suppliers, (b) to check the terms and conditions under which suppliers can do business, (c) to interoperate with the suppliers enterprise support systems, i.e., workflows, and (d) to monitor ordering/purchasing for possible delays, unexpected events, react to such events, etc. This paper presents an original framework for specifying, enacting and supervising e-services on the Web. This framework is based on XML and rules-based support for products/services description and workflow mediation across organizations. Traditionally, WorkFlow Management Systems (WFMS) have focused on homogeneous and centrally controlled environments for binding people and processes within the boundary of a single organization. In the context of B2B e-commerce, WFMSs need to support collaboration between various autonomous parties, some of which may even have conflicting business goals. More precisely, they must cope with heterogeneous enterprise support environments (e.g., through different WF systems), to model the interaction of independent partners by abstracting the internal details of their activities (e.g., through different WF schemas), and finally to facilitate flexible linking and monitoring of inter-enterprise processes (e.g., through different WF enactments). To address these challenges we are currently developing a workflow mediation middleware which relies on three basic technologies: (a) the XRL workflow specification language [13, 18] for representing in XML heterogeneous workflow schemas and enactments, (b) an XML query language [4, 5] for manipulating both complex product and service descriptions, and (c) the Vortex rule-based language [12, 6] that supports heuristic reasoning in order to take on-line business decisions during the workflow execution. During recent years, workflow interoperation has received considerable attention. Numerous research projects and prototypes have been proposed [16, 2] while basic interoperability between various vendor WFMSs has been a subject of standardization efforts by the Object Management Group (see Workflow",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "144837668",
                    "name": "R. Hull"
                },
                {
                    "authorId": "2116451209",
                    "name": "Akhil Kumar"
                },
                {
                    "authorId": "1753075",
                    "name": "J\u00e9r\u00f4me Sim\u00e9on"
                }
            ]
        },
        {
            "paperId": "151ca8d2a320eefdbeffc69bbe5fe0d7b5ca323c",
            "title": "Querying Semistructured (Meta)Data and Schemas on the Web: The case of RDF RDFS",
            "abstract": "The need for descriptive information, i.e., metadata, about Web resources has been recognized in several application contexts (e.g., digital libraries, infomediaries, portals, etc.). The Resource Description Framework (RDF) aims at facilitating the creation and exchange of metadata, as any other Web data. In particular, the de nition of schema vocabularies enables the interpretation of RDF descriptions across several communities. Unfortunately, existing RDF-enabled systems (e.g., browsers, search engines) totally ignore RDF schemas. In this paper, we propose RQL, a declarative language suitable for querying both RDF descriptions and schemas. RQL relies on a graph data model capturing the RDF modeling primitives in terms of classes and relationship types, organized into appropriate subsumption taxonomies. Furthermore, a graph instantiation mechanism is introduced to interpret semistructured RDF data using schema information. Taxonomies of labels are then exploited transparently in order to facilitate querying of complex semistructured data as well as of the schema vocabularies themselves. To the best of our knowledge RQL is the rst language o ering this functionality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                }
            ]
        },
        {
            "paperId": "6ac53f84e4da85d9b7f30d6581c65ec261cd63e8",
            "title": "The RDFSuite: Managing Voluminous RDF Description Bases",
            "abstract": "Metadata are widely used in order to fully exploit information resources available on corporate intranets or the Internet. The Resource Description Framework (RDF) aims at facilitating the creation and exchange of metadata as any other Web data. The growing number of available information resources and the proliferation of description services in various user communities, lead nowadays to large volumes of RDF metadata. Managing such RDF resource descriptions and schemas with existing low-level APIs and le-based implementations does not ensure fast deployment and easy maintenance of realscale RDF applications. In this paper, we advocate the use of database technology to support declarative access, as well as, logical and physical independence for voluminous RDF description bases. We present RDFSuite, a suite of tools for RDF validation, storage and querying. Speci cally, we introduce a formal data model for RDF description bases created using multiple schemas. Compared to the current status of the W3C standard, our model relies on a complete set of validation constraints for the core RDF/S (without rei cation) and provides a richer type system including several basic types as well as union types. Next, we present the design of a persistent RDF Store (RSSDB) for loading resource descriptions in an object-relational DBMS by exploring the available RDF schema knowledge. Our approach preserves the exibility of RDF in re ning schemas and/or enriching descriptions at any time, whilst it ensures good performance for storing and querying voluminous RDF descriptions. Last, we brie y present RQL, a declarative language for querying both RDF descriptions and schemas, and sketch query evaluation on top of RSSDB.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189454",
                    "name": "S. Alexaki"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                }
            ]
        },
        {
            "paperId": "9954f676e55e8c5001cf51bc8fa056c4fcfcd176",
            "title": "Declarative Languages for Querying Portal Catalogs",
            "abstract": "As data is increasingly captured, aggregated, and digitized worldwide, new types of information systems, such as digital libraries and information (subject) gateways, emerge as core technologies of the 21st-century economy. After a rst generation of systems focusing on the accessibility of available information resources, nowadays, high quality information collections are smoothly transformed into Community Web Portals. These Portals provide the means to select, classify and access, in a semantically meaningful and ubiquitous way, diverse information resources in order to develop and maintain speci c communities of interests (e.g., professional, trading, etc.) on corporate intranets or the Web. A key Portal component is the Knowledge Catalog holding descriptive information, i.e., metadata, about the community resources (e.g., sites, documents, data, etc.). Despite the current developments in standards for describing the content and meaning of information resources (see the W3C Metadata Activity), declarative languages suitable for querying both their semantic descriptions and the employed schemas are still missing. In this paper we present such a high-level query language for Portal Catalogs (e.g., as Open Directory, CNET, XMLNews) created according to the Resource Description Framework (RDF) standard [15, 4]. RDF [15] aims at facilitating the creation and exchange of metadata as any other Web data. RDF resource descriptions are represented as directed labeled graphs (where nodes are called resources or literals and edges are called properties) which can be serialized in XML. Furthermore, RDF schema [4] vocabularies are used to de ne the labels of nodes (called classes) and edges that can be used to describe and query resources in speci c communities. These labels can be organized into appropriate taxonomies, carrying the inclusion semantics of subjects/topics in a Portal Catalog. In this context, our query language, called RQL, relies on a graph data model allowing us to interpret semistructured RDF descriptions by means of one or more RDF schemas. Note that RDF schemas (a) do not impose a strict typing on the data (by e.g., permitting multiple classi cation, optional and repeated properties); (b) can be easily extended (e.g., through specialization of both classes and property types); (c) may provide only a partial or overlapped interpretation of the underlying data (e.g., by having several, eventually incomplete schemas for the same resource descriptions); and (d) are not entirely separated from the resource descriptions (i.e., they can be queried like normal data). Thus, RQL shares the exibility and utility of the recent proposals for semistructured or XML query languages, while, at the same time, extending their functionality to the RDF schema level by exploring in a transparent way the de ned taxonomies of classes and properties, as well as, the multiple classi cation of resources. To the best of our knowledge, RQL is the rst language to smoothly combine features from thesauribased information retrieval systems (i.e., term expansion mechanisms [12]) with semistructured or XML query languages featuring variables on both property and class names (i.e., generalized path expressions [1]). Our work is motivated by the fact that existing semistructured models (e.g., OEM [18], YAT [8]) cannot capture the semantics of node and edge labels provided by RDF schemas (i.e., taxonomies of classes and property types), while semistructured or XML query languages (e.g., LOREL [2], UnQL [5], StruQL [11], XML-QL [10], XML-GL [7]) are not suited to exploit RDF schema information (i.e., pattern vs. semantic matching of labels). On the other hand, database (relational or object) schema query languages as SchemaSQL [14], XSQL [13] or Noodle [17] fail to fully accommodate RDFS",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1705358",
                    "name": "D. Plexousakis"
                },
                {
                    "authorId": "1829246",
                    "name": "G. Karvounarakis"
                },
                {
                    "authorId": "2189454",
                    "name": "S. Alexaki"
                }
            ]
        },
        {
            "paperId": "9b20fd940c9a55955ca6df0f3b358273d1b1eb92",
            "title": "On wrapping query languages and efficient XML integration",
            "abstract": "Modern applications (Web portals, digital libraries, etc.) require integrated access to various information sources (from traditional DBMS to semistructured Web repositories), fast deployment and low maintenance cost in a rapidly evolving environment. Because of its flexibility, there is an increasing interest in using XML as a middleware model for such applications. XML enables fast wrapping and declarative integration. However, query processing in XML-based integration systems is still penalized by the lack of an algebra with adequate optimization properties and the difficulty to understand source query capabilities. In this paper, we propose an algebraic approach to support efficient XML query evaluation. We define a general purpose algebra suitable for semistructured on XML query languages. We show how this algebra can be used, with appropriate type information, to also wrap more structured query languages such as OQL or SQL. Finally, we develop new optimization techniques for XML-based integration systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "1753075",
                    "name": "J\u00e9r\u00f4me Sim\u00e9on"
                }
            ]
        },
        {
            "paperId": "de590e6a3db72dd49da875b8e0730aea1846d9cf",
            "title": "Original Articles On Z39.50 wrapping and description logics",
            "abstract": "Z39.50 is a client/server protocol widely used in digital libraries and museums for searching and retriev- ing information spread over a number of heterogeneous sources. To overcome semantic and schematic discrepan- cies among the various data sources the protocol relies on a world view of information as a flat list of fields, called Access Points (AP). One of the major issues for building Z39.50 wrappers is to map this unstructured list of APs to the underlying source data structure and semantics. For highly structured sources (e.g., database manage- ment systems, knowledge base systems) this mapping is quite complex and considerably affects the quality of the retrieved data. Unfortunately, existing Z39.50 wrappers have been developed from scratch and they do not pro- vide high-level mapping languages with verifiable prop- erties. In this paper, we propose a description logic (DL) based toolkit for the declarative specification of Z39.50 wrappers. We claim that the conceptualization of AP mappings enables a formal validation of the query trans- lation quality (e.g., ill-defined mappings, inappropriate APs, etc.) and allows one to tackle a number of Z39.50 pending issues (e.g., metadata retrieval, query failures due to unsupported APs, etc.). Furthermore, our DL- based approach allows the development of Z39.50 wrap- pers enriched with a number of added-value services such as conceptual structuring of flat Z39.50 vocabularies and intelligent Z39.50 query assists. These services are quite useful for profile developers, Z39.50 wrappers administra- tors, and end-users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "3000550",
                    "name": "P. Constantopoulos"
                },
                {
                    "authorId": "52458911",
                    "name": "Vassilika Vouton"
                }
            ]
        },
        {
            "paperId": "ea61af36c168dbce7bf524db7444c945d9ee5f47",
            "title": "Towards a next generation of open scientific data repositories and services",
            "abstract": "Scientic repositories found in institutions and organizations consist of data and programs. Data consists principally of numeric data, images, and text documents. Programs consist principally of software methods for visualizing and processing data and simulators of natural processes. Data represents both measured physical behavior and the results of simulations. The integration and visualization of scientic repositories into an easily accessed interoperable networked environment is needed in many disciplines for both scientic and management purposes. To satisfy these needs we present an open hybrid architecture, which combines digital library technology, information integration mechanisms and workflow-based systems. Our experience is based on the THETIS 1 [15] project, a distributed collection of scientic repositories focused on supporting Coastal Zone Management of the Mediterranean Region in Europe. It will demonstrate its ability to respond to users such as scientists and public administration authorities that use scientic information for decision making.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1805579",
                    "name": "C. Houstis"
                },
                {
                    "authorId": "1747110",
                    "name": "C. Nikolaou"
                },
                {
                    "authorId": "1711456",
                    "name": "S. Lalis"
                },
                {
                    "authorId": "2030044",
                    "name": "S. Kapidakis"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "144463763",
                    "name": "E. Simon"
                },
                {
                    "authorId": "1693125",
                    "name": "A. Tomasic"
                }
            ]
        },
        {
            "paperId": "b79aec2f1ddf423f8a5f9b060a221f454a9811aa",
            "title": "Optimizing Generalized Path Expressions Using Full Text Indexes",
            "abstract": "Etendre les langages de requetes des SGBD objet avec des expressions de chemin generalisees permet d'interroger les donnees sans une connaissance exacte de leur structure. Cependant, l'evaluation efficace des requetes contenant des expressions de chemin generalisees reste un probleme ouvert et les techniques d'indexation classiques ne conviennent pas a ce nouveau contexte. Nous proposons d'utiliser conjointement index classiques et index plein-texte pour obtenir une evaluation efficace des requetes avec expressions de chemin generalisees. Nous definissons un cadre algebrique permettant de combiner etroitement l'utilisation des index plein-texte et les techniques standard d'optimisation. Enfin, nous donnons quelques resultats sur les performances d'un premier prototype.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "1809585",
                    "name": "G. Moerkotte"
                },
                {
                    "authorId": "1753075",
                    "name": "J\u00e9r\u00f4me Sim\u00e9on"
                }
            ]
        },
        {
            "paperId": "880ccc6b1172be694d280a28b87a5a87081d0720",
            "title": "Evaluating queries with generalized path expressions",
            "abstract": "In the past few years, query languages featuring generalized path expressions have been proposed. These languages allow the interrogation of both data and structure. They are powerful and essential for a number of applications. However, until now, their evaluation has relied on a rather naive and inefficient algorithm.In this paper, we extend an object algebra with two new operators and present some interesting rewriting techniques for queries featuring generalized path expressions. We also show how a query optimizer can integrate the new techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "1809585",
                    "name": "G. Moerkotte"
                }
            ]
        },
        {
            "paperId": "293ef93a843d68c308daf68188a6d154048c7b5b",
            "title": "Querying structured documents with hypertext links using OODBMS",
            "abstract": "Hierarchical logical structure and hypertext links are complementary and can be combined to build more powerful document management systems. Previous work exploits this complementarity for building better document processors, browsers and editing tools, but not for building sophisticated querying mechanisms. Querying in hypertext has been a requirement since [19] and has already been elaborated in many hypertext systems, but has not yet been used for hypertext systems superimposed on an underlying hierarchical logical structure.\nIn this paper we use the model and the SQL-like query language of [10] in order to manage structured documents with hypertext links. The model represents a structured documents with hypertext links. The model represents a structured document with typed links as a complex object, and uses paths through the document structure, as first class citizens in formulating queries. Several examples of queries illustrate, from a practical point of view, the expressive power of the language to retrieve documents, even without exact knowledge of their structure in a simple and homogeneous fashion. It must be stressed that the proposed model and language implement the equivalent HyTime Location Address Module. In fact, the language is more powerful than the corresponding HyQ query facilities. The implementation and the description throughout the paper use the SGML standard to represent the document structure and the object-oriented DBMS O2 to implement the query language and the storage module.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "145437631",
                    "name": "A. Rizk"
                }
            ]
        },
        {
            "paperId": "2d21640f965fac15e9745c6ae8c2896c9a2ebb12",
            "title": "From structured documents to novel query facilities",
            "abstract": "Structured documents (e.g., SGML) can benefit a lot from database support and more specifically from object-oriented database (OODB) management systems. This paper describes a natural mapping from SGML documents into OODB's and a formal extension of two OODB query languages (one SQL-like and the other calculus) in order to deal with SGML document retrieval.\nAlthough motivated by structured documents, the extensions of query languages that we present are general and useful for a variety of other OODB applications. A key element is the introduction of paths as first class citizens. The new features allow to query data (and to some extent schema) without exact knowledge of the schema in a simple and homogeneous fashion.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "144911319",
                    "name": "M. Scholl"
                }
            ]
        }
    ]
}