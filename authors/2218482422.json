{
    "authorId": "2218482422",
    "papers": [
        {
            "paperId": "7ae8594773a4d89a2f048a6730a3d4ed4976f346",
            "title": "Pre-training on High Definition X-ray Images: An Experimental Study",
            "abstract": "Existing X-ray based pre-trained vision models are usually conducted on a relatively small-scale dataset (less than 500k samples) with limited resolution (e.g., 224 $\\times$ 224). However, the key to the success of self-supervised pre-training large models lies in massive training data, and maintaining high resolution in the field of X-ray images is the guarantee of effective solutions to difficult miscellaneous diseases. In this paper, we address these issues by proposing the first high-definition (1280 $\\times$ 1280) X-ray based pre-trained foundation vision model on our newly collected large-scale dataset which contains more than 1 million X-ray images. Our model follows the masked auto-encoder framework which takes the tokens after mask processing (with a high rate) is used as input, and the masked image patches are reconstructed by the Transformer encoder-decoder network. More importantly, we introduce a novel context-aware masking strategy that utilizes the chest contour as a boundary for adaptive masking operations. We validate the effectiveness of our model on two downstream tasks, including X-ray report generation and disease recognition. Extensive experiments demonstrate that our pre-trained medical foundation vision model achieves comparable or even new state-of-the-art performance on downstream benchmark datasets. The source code and pre-trained models of this paper will be released on https://github.com/Event-AHU/Medical_Image_Analysis.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298967784",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2296752163",
                    "name": "Yuehang Li"
                },
                {
                    "authorId": "2275783706",
                    "name": "Wentao Wu"
                },
                {
                    "authorId": "2268982162",
                    "name": "Jiandong Jin"
                },
                {
                    "authorId": "2227981104",
                    "name": "Yao Rong"
                },
                {
                    "authorId": "2114189483",
                    "name": "Bowei Jiang"
                },
                {
                    "authorId": "2218482422",
                    "name": "Chuanfu Li"
                },
                {
                    "authorId": "2229726456",
                    "name": "Jin Tang"
                }
            ]
        },
        {
            "paperId": "b83a697b9a1238abe1295c58d769a04ae52be0b4",
            "title": "R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation",
            "abstract": "Inspired by the tremendous success of Large Language Models (LLMs), existing X-ray medical report generation methods attempt to leverage large models to achieve better performance. They usually adopt a Transformer to extract the visual features of a given X-ray image, and then, feed them into the LLM for text generation. How to extract more effective information for the LLMs to help them improve final results is an urgent problem that needs to be solved. Additionally, the use of visual Transformer models also brings high computational complexity. To address these issues, this paper proposes a novel context-guided efficient X-ray medical report generation framework. Specifically, we introduce the Mamba as the vision backbone with linear complexity, and the performance obtained is comparable to that of the strong Transformer model. More importantly, we perform context retrieval from the training set for samples within each mini-batch during the training phase, utilizing both positively and negatively related samples to enhance feature representation and discriminative learning. Subsequently, we feed the vision tokens, context information, and prompt statements to invoke the LLM for generating high-quality medical reports. Extensive experiments on three X-ray report generation datasets (i.e., IU-Xray, MIMIC-CXR, CheXpert Plus) fully validated the effectiveness of our proposed model. The source code of this work will be released on \\url{https://github.com/Event-AHU/Medical_Image_Analysis}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316775890",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2296752163",
                    "name": "Yuehang Li"
                },
                {
                    "authorId": "2316478844",
                    "name": "Fuling Wang"
                },
                {
                    "authorId": "2248204640",
                    "name": "Shiao Wang"
                },
                {
                    "authorId": "2218482422",
                    "name": "Chuanfu Li"
                },
                {
                    "authorId": "2114189483",
                    "name": "Bowei Jiang"
                }
            ]
        },
        {
            "paperId": "d207c3902daf3df63281c9522c5ecc09fed16aae",
            "title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset",
            "abstract": "X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence which can significantly reduce diagnostic burdens and patient wait times. Despite significant progress, we believe that the task has reached a bottleneck due to the limited benchmark datasets and the existing large models' insufficient capability enhancements in this specialized domain. Specifically, the recently released CheXpert Plus dataset lacks comparative evaluation algorithms and their results, providing only the dataset itself. This situation makes the training, evaluation, and comparison of subsequent algorithms challenging. Thus, we conduct a comprehensive benchmarking of existing mainstream X-ray report generation models and large language models (LLMs), on the CheXpert Plus dataset. We believe that the proposed benchmark can provide a solid comparative basis for subsequent algorithms and serve as a guide for researchers to quickly grasp the state-of-the-art models in this field. More importantly, we propose a large model for the X-ray image report generation using a multi-stage pre-training strategy, including self-supervised autoregressive generation and Xray-report contrastive learning, and supervised fine-tuning. Extensive experimental results indicate that the autoregressive pre-training based on Mamba effectively encodes X-ray images, and the image-text contrastive pre-training further aligns the feature spaces, achieving better experimental results. Source code can be found on \\url{https://github.com/Event-AHU/Medical_Image_Analysis}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316775890",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2316478844",
                    "name": "Fuling Wang"
                },
                {
                    "authorId": "2296752163",
                    "name": "Yuehang Li"
                },
                {
                    "authorId": null,
                    "name": "Qingchuan Ma"
                },
                {
                    "authorId": "2248204640",
                    "name": "Shiao Wang"
                },
                {
                    "authorId": null,
                    "name": "Bo Jiang"
                },
                {
                    "authorId": "2218482422",
                    "name": "Chuanfu Li"
                },
                {
                    "authorId": "2229726456",
                    "name": "Jin Tang"
                }
            ]
        },
        {
            "paperId": "1e7e2eeac837dcc7f1109215cda9064efb3b1a3b",
            "title": "AMatFormer: Efficient Feature Matching via Anchor Matching Transformer",
            "abstract": "Learning based feature matching methods have been commonly studied in recent years. The core issue for learning feature matching is to how to learn (1) discriminative representations for feature points (or regions) within each intra-image and (2) consensus representations for feature points across inter-images. Recently, self- and cross-attention models have been exploited to address this issue. However, in many scenes, features are coming with large-scale, redundant and outliers contaminated. Previous self-/cross-attention models generally conduct message passing on all primal features which thus lead to redundant learning and high computational cost. To mitigate limitations, inspired by recent seed matching methods, in this article, we propose a novel efficient Anchor Matching Transformer (AMatFormer) for the feature matching problem. AMatFormer has two main aspects: First, it mainly conducts self-/cross-attention on some anchor features and leverages these anchor features as message bottleneck to learn the representations for all primal features. Thus, it can be implemented efficiently and compactly. Second, AMatFormer adopts a shared FFN module to further embed the features of two images into the common domain and thus learn the consensus feature representations for the matching problem. Experiments on several benchmarks demonstrate the effectiveness and efficiency of the proposed AMatFormer matching approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152390773",
                    "name": "Bo Jiang"
                },
                {
                    "authorId": "1646713373",
                    "name": "S. Luo"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2218482422",
                    "name": "Chuanfu Li"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                }
            ]
        },
        {
            "paperId": "53c79d46cfc30c51be39da28135bc04322a3c582",
            "title": "Chest X-ray Image Quality Control via Transformer and Label Correlation Regularization Loss",
            "abstract": "Quality control of chest X-rays is especially important for radiologists to make accurate diagnoses. Presently, the quality control of chest X-ray image is done manually, which is time-consuming and labor-intensive. Therefore, it is very necessary to research on automatic and intelligent image quality control. However, to the best of our knowledge, this task has not been studied before. The main challenges lie in how to obtain effective feature representation and to make full use of prior medical knowledge. In this work, we propose a quality control model for chest X-ray image via Transformer and label correlation regularization loss. Specifically, we utilize a multi-scale fusion strategy to discriminate unclear and subtle image features for the X-ray image quality control task. In addition, we incorporate prior medical knowledge into our network via label correlation regularization loss to understand of the inherent correlations of different quality labels which further enhance the learning accuracy. We also constructed a novel dataset for chest x-ray quality control task. Experiments on the dataset demonstrate the superiority of our proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2295381924",
                    "name": "Ming Liu"
                },
                {
                    "authorId": "2269674068",
                    "name": "Lili Huang"
                },
                {
                    "authorId": "2199993185",
                    "name": "Bo Jiang"
                },
                {
                    "authorId": "2218482422",
                    "name": "Chuanfu Li"
                },
                {
                    "authorId": "2260595835",
                    "name": "Jin Tang"
                }
            ]
        }
    ]
}