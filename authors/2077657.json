{
    "authorId": "2077657",
    "papers": [
        {
            "paperId": "3efa931b7e245eafe7c2ac20a905d0052f656048",
            "title": "BOLD: Knowledge Graph Exploration and Analysis Platform",
            "abstract": "The linked open data (LOD) cloud maintains several interlinked knowledge graphs. These graphs span various domains such as government, media, life sciences, etc. The graphs are often manually curated or automatically extracted (e.g. YAGO\u2014Yet Another Great Ontology) using information extraction techniques. They are used in various applications such as data governance, fraud detection, fact checking, etc. Although the graphs in LOD are widely used, they do not contain metadata about their represen-tativeness (distribution of key features). Since most of the graphs are automatically curated, bias can manifest due to sensitive features and their causal influences, or through under (over)- representation of certain entities (e.g. people) and relations (e.g. president-of, works-for). The aim of this work is to develop a system to automatically generate bias profiles (metadata about the representativeness of data) for knowledge graphs. As a result, the metadata can be used as a guide for users to choose bias free (balanced) datasets for their studies. Moreover, it enables researchers to quickly gauge the relevance of a graph for a problem at hand (e.g. classification task).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292416748",
                    "name": "Egor Dmitriev"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "2292410791",
                    "name": "Mirko Tobias Sch\u00e4fer"
                }
            ]
        },
        {
            "paperId": "0e0897cecb970f13647c8aac878262fb50f0b8eb",
            "title": "Leveraging Graph Embeddings for Opinion Leader Detection",
            "abstract": "Nowadays, social media plays an important role in many fields, such as the promotion of measures against major infectious diseases, merchandising, etc. In social media, some people are known as opinion leaders due to their strong ability to influence the opinions of others. The detection of opinion leaders has become an important task in social network analysis. Social networks are often represented in the form of graphs which allows a large number of graph analysis methods to be used for opinion leader detection. Some studies have attempted to apply graph representation learning for opinion leader detection and achieved good results. In this paper, we propose a model-agnostic framework that formulate the opinion leader detection problem as a ranking task of node embeddings. A variety of methods and datasets are chosen to analyze the performance of our framework both qualitatively and quantitatively. Based on the analysis results, we propose a strategy that combines opinion leaders detected by two different ranking algorithms to obtain a more comprehensive set of opinion leaders. And we analyze the temporal changes of the opinion leaders in one of the dynamic social networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056017114",
                    "name": "Yu Hui"
                },
                {
                    "authorId": "2217252340",
                    "name": "Luuk Buijsman"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "49184018",
                    "name": "Shihan Wang"
                }
            ]
        },
        {
            "paperId": "93a3085a10d5d697e212128fd9f9cd0779631540",
            "title": "MGTCOM: Community Detection in Multimodal Graphs",
            "abstract": "Community detection is the task of discovering groups of nodes sharing similar patterns within a network. With recent advancements in deep learning, methods utilizing graph representation learning and deep clustering have shown great results in community detection. However, these methods often rely on the topology of networks (i) ignoring important features such as network heterogeneity, temporality, multimodality, and other possibly relevant features. Besides, (ii) the number of communities is not known a priori and is often left to model selection. In addition, (iii) in multimodal networks all nodes are assumed to be symmetrical in their features; while true for homogeneous networks, most of the real-world networks are heterogeneous where feature availability often varies. In this paper, we propose a novel framework (named MGTCOM) that overcomes the above challenges (i)--(iii). MGTCOM identifies communities through multimodal feature learning by leveraging a new sampling technique for unsupervised learning of temporal embeddings. Importantly, MGTCOM is an end-to-end framework optimizing network embeddings, communities, and the number of communities in tandem. In order to assess its performance, we carried out an extensive evaluation on a number of multimodal networks. We found out that our method is competitive against state-of-the-art and performs well in inductive inference.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2141698881",
                    "name": "E. Dmitriev"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "2190748781",
                    "name": "S. Wang"
                }
            ]
        },
        {
            "paperId": "4f6116cd58453f65feec0d1b7122c3a8c96d9bc9",
            "title": "Leveraging Static Models for Link Prediction in Temporal Knowledge Graphs",
            "abstract": "Including temporal scopes of facts in knowledge graph embedding (KGE) presents significant opportunities for improving the resulting embeddings, and consequently for increased performance in downstream applications. Yet, little research effort has focussed on this area and much of the carried out research reports only marginally improved results compared to models trained without temporal scopes (static models). Furthermore, rather than leveraging existing work on static models, they introduce new models specific to temporal knowledge graphs. We propose a novel perspective that takes advantage of the power of existing static embedding models by focussing effort on manipulating the data instead. Our method, SPLIME, draws inspiration from the field of signal processing and early work in graph embedding. We show that SPLIME competes with or outperforms the current state of the art in temporal KGE. Additionally, we uncover issues with the procedure currently used to assess the performance of static models on temporal graphs and introduce two ways to counteract them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115952424",
                    "name": "Wessel Radstok"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                }
            ]
        },
        {
            "paperId": "75f7e3459e53fa0775c941cb703f049797851ef0",
            "title": "Are Knowledge Graph Embedding Models Biased, or Is it the Data That They Are Trained on?",
            "abstract": ". Recent studies on bias analysis of knowledge graph (KG) embedding models focus primarily on altering the models such that sensitive features are dealt with di\ufb00erently from other features. The underlying implication is that the models cause bias, or that it is their task to solve it. In this paper we argue that the problem is not caused by the models but by the data, and that it is the responsibility of the expert to ensure that the data is representative for the intended goal. To support this claim, we experiment with two di\ufb00erent knowledge graphs and show that the bias is not only present in the models, but also in the data. Next, we show that by adding new samples to balance the distribution of facts with regards to speci\ufb01c sensitive features, we can reduce the bias in the models. 3",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115952424",
                    "name": "Wessel Radstok"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "84022689",
                    "name": "M. Sch\u00e4fer"
                }
            ]
        },
        {
            "paperId": "d4a1b1c590471efdfc4727e82f0a1459ffe2b874",
            "title": "Tensor Decomposition for Link Prediction in Temporal Knowledge Graphs",
            "abstract": "We study temporal knowledge graph completion by using tensor decomposition. In particular, we use Candecomp/Parafac decomposition to factorize a given four dimensional sparse representation of a temporal knowledge graph into rank-one tensors that correspond to entities (subject and object), relations and timestamps. Using the factorized tensors, we can perform link and timestamp prediction. We compared our approach against the state of the art and found out that we are highly competitive. We report our preliminary experimental results on 5 different datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                }
            ]
        },
        {
            "paperId": "30446d5852fffa618c600837552d1e5ddb236b2e",
            "title": "Reinforced Anytime Bottom Up Rule Learning for Knowledge Graph Completion",
            "abstract": "Most of todays work on knowledge graph completion is concerned with sub-symbolic approaches that focus on the concept of embedding a given graph in a low dimensional vector space. Against this trend, we propose an approach called AnyBURL that is rooted in the symbolic space. Its core algorithm is based on sampling paths, which are generalized into Horn rules. Previously published results show that the prediction quality of AnyBURL is on the same level as current state of the art with the additional benefit of offering an explanation for the predicted fact. In this paper, we are concerned with two extensions of AnyBURL. Firstly, we change AnyBURLs interpretation of rules from $\\Theta$-subsumption into $\\Theta$-subsumption under Object Identity. Secondly, we introduce reinforcement learning to better guide the sampling process. We found out that reinforcement learning helps finding more valuable rules earlier in the search process. We measure the impact of both extensions and compare the resulting approach with current state of the art approaches. Our results show that AnyBURL outperforms most sub-symbolic methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1795542",
                    "name": "Christian Meilicke"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "37685043",
                    "name": "Manuel Fink"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "8a833036e153f6e6da829f0028c84be53368f868",
            "title": "Towards Temporal Knowledge Graph Embeddings with Arbitrary Time Precision",
            "abstract": "Acknowledging the dynamic nature of knowledge graphs, the problem of learning temporal knowledge graph embeddings has recently gained attention. Essentially, the goal is to learn vector representation for the nodes and edges of a knowledge graph taking time into account. These representations must preserve certain properties of the original graph, so as to allow not only classification or clustering tasks, as for classical graph embeddings, but also approximate time-dependent query answering or link predictions over knowledge graphs. For instance, \"who was the leader of Germany in 1994?'' or \"when was Bonn the capital of Germany?'' Several existing work in the area adapt existing knowledge graph embedding models, adding a time dimension, usually restricting to one time granularity, like years or days, or treating time as fixed labels. However, this is not adequate for many facts of life, for instance historical and sensory data. In this work, we introduce and evaluate an approach that gracefully adjusts to time validity of virtually any granularity. Our model is robust to non-contiguous validity periods. It is generic enough to adapt to many existing non-temporal models and its size (number of parameters) does not depend on the size of the graph (number of entities and relations).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3070790",
                    "name": "J. Leblay"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "89121677",
                    "name": "Xin Liu"
                }
            ]
        },
        {
            "paperId": "64362f33fe0954a97336fcb1b6dfc806d4d3775c",
            "title": "Anytime Bottom-Up Rule Learning for Knowledge Graph Completion",
            "abstract": "We propose an anytime bottom-up technique for learning logical rules from large knowledge graphs. We apply the learned rules to predict candidates in the context of knowledge graph completion. Our approach outperforms other rule-based approaches and it is competitive with current state of the art, which is based on latent representations. Besides, our approach is significantly faster, requires less computational resources, and yields an explanation in terms of the rules that propose a candidate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1795542",
                    "name": "Christian Meilicke"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "8792454",
                    "name": "Daniel Ruffinelli"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "be683bc3ef805d6a0a2c491128e3db8150f33954",
            "title": "Fast Interval Joins for Temporal SPARQL Queries",
            "abstract": "Knowledge graphs enriched with temporal information are becoming more and more common. As an example, the Wikidata KG contains millions of temporal facts associated with validity intervals (i.e., start and end time) covering a variety of domains. While these facts are interesting, computing temporal relations between their intervals allows to discover temporal relations holding between facts (e.g., \u201cfootball players that get divorced after moving from a team to another\u201d). In this paper we study the problem of computing different kinds of interval joins in temporal KGs. In principle, interval joins can be computed by resorting to query languages like SPARQL. However, this language is not optimized for such a task, which makes it hard to answer real-world queries. For instance, the query \u201cfind players that were married while being member of a team\u201d times out on Wikidata. We present efficient algorithms to compute interval joins for the main Allen\u2019s relations (e.g., before, after, during, meets). We also address the problem of interval coalescing, which is used for merging contiguous or overlapping intervals of temporal facts, and propose an efficient algorithm. We integrate our interval joins and coalescing algorithms into a light SPARQL extension called iSPARQL. We evaluated the performance of our algorithms on real-world temporal kgs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "145750056",
                    "name": "G. Pirr\u00f2"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "e605d3517c3c1894740e812378f66a51f2101a00",
            "title": "Time-Aware Probabilistic Knowledge Graphs",
            "abstract": "The emergence of open information extraction as a tool for constructing and expanding knowledge graphs has aided the growth of temporal data, for instance, YAGO, NELL and Wikidata. While YAGO and Wikidata maintain the valid time of facts, NELL records the time point at which a fact is retrieved from some Web corpora. Collectively, these knowledge graphs (KG) store facts extracted from Wikipedia and other sources. Due to the imprecise nature of the extraction tools that are used to build and expand KG, such as NELL, the facts in the KG are weighted (a confidence value representing the correctness of a fact). Additionally, NELL can be considered as a transaction time KG because every fact is associated with extraction date. On the other hand, YAGO and Wikidata use the valid time model because they maintain facts together with their validity time (temporal scope). In this paper, we propose a bitemporal model (that combines transaction and valid time models) for maintaining and querying bitemporal probabilistic knowledge graphs. We study coalescing and scalability of marginal and MAP inference. Moreover, we show that complexity of reasoning tasks in atemporal probabilistic KG carry over to the bitemporal setting. Finally, we report our evaluation results of the proposed model. 2012 ACM Subject Classification Information systems \u2192 Web Ontology Language (OWL); Computing methodologies \u2192 Probabilistic reasoning; Computing methodologies \u2192 Temporal reasoning",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "e8a085f138383bd7847eadcd2d0bbb8ed751af88",
            "title": "Leveraging Graph Neighborhoods for Efficient Inference",
            "abstract": "Several probabilistic extensions of description logic languages have been proposed and thoroughly studied. However, their practical use has been hampered by intractability of various reasoning tasks. While present-day knowledge bases (KBs) contain millions of instances and thousands of axioms, most state-of-the-art reasoners are capable of handling small scale KBs with thousands of instances. Thus, recent research has focused on leveraging the structure of KBs and queries in order to speed up inference runtime. However, these efforts have not been satisfactory in providing reasoners that are suitable for practical use in large scale KBs. In this study, we aim to tackle this challenging problem. In doing so, we use a probabilistic extension of OWL RL (called PRORL) as a modeling language and exploit graph neighborhoods (of undirected graphical models) for efficient approximate probabilistic inference. We show that subgraph extraction based inference is much faster and has comparable accuracy to full graph inference. We perform several experiments, in order to support our claim, over a NELL KB containing millions of instances and thousands of axioms. Furthermore, we propose a novel graph-based algorithm to automatically partition inferences rules based on their structure for efficient parallel inference.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "027a3f21c55bc3aeed0eb499cb0d62ed5215684c",
            "title": "Deriving Validity Time in Knowledge Graph",
            "abstract": "Knowledge Graphs (KGs) are a popular means to represent knowledge on the Web, typically in the form of node/edge labelled directed graphs. We consider temporal KGs, in which edges are further annotated with time intervals, reflecting when the relationship between entities held in time. In this paper, we focus on the task of predicting time validity for unannotated edges. We introduce the problem as a variation of relational embedding. We adapt existing approaches, and explore the importance example selection and the incorporation of side information in the learning process. We present our experimental evaluation in details.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3070790",
                    "name": "J. Leblay"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                }
            ]
        },
        {
            "paperId": "19c9471462637b870e191ac7578cd7972a4533fc",
            "title": "Towards Partition-Aware Lifted Inference",
            "abstract": "There is an ever increasing number of rule learning algorithms and tools for automatic knowledge base (KB) construction. These tools often produce weighted rules and facts that make up a probabilistic KB (PKB). In such a PKB, probabilistic inference is used in order to perform marginal inference, consistency checking and other tasks. However, in general, inference is known to be intractable. Hence, recently, there are a number of studies aimed at lifting (making tractable or approximating) inference by exploiting symmetries in the structure of a PKB. These studies alleviate grounding entirely a given PKB which can generate a sizable factor graph for inference (e.g. to compute the probability of a query). In line with this, we propose a novel technique to automatically partition rules based on their structure for efficient parallel grounding. In addition, we perform query expansion so as to generate a factor graph small enough to be used for efficient probability computation. We present a novel approximate marginal inference algorithm that uses N-hop subgraph extraction and query expansion. Moreover, we show that our system is much faster than state-of-the-art systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "83ae6c66fc7902ff5d386adb7ea179ba0b69a2ed",
            "title": "Tractable reasoning in probabilistic OWL profiles",
            "abstract": "Through the advance of information extraction and data mining, a number of knowledge bases (KBs) have been created, for instance, NELL and Google knowledge Vault. In line with this, probabilistic extensions of various description logics have been proposed for reasoning in probabilistic KBs. However, most of these languages are not tractable impeding their practical use. Since present-day KBs can be very large, tractable reasoning is essential. In this work, we propose probabilistic extensions of OWL 2 RL and OWL 2 EL by using probabilistic soft logic for which inference is known to be tractable. We show that inference in probabilistic extensions of OWL 2 RL and OWL 2 EL remains tractable. We present experimental results over a YAGO KB that contains hundreds of schema axioms and thousands of instances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "cc39ee4399238a6a43a477147baf8a04f2f3dbe7",
            "title": "Towards Probabilistic Bitemporal Knowledge Graphs",
            "abstract": "The emergence of open information extraction as a tool for constructing and expanding knowledge graphs has aided the growth of temporal data, for instance, YAGO, NELL and Wikidata. While YAGO and Wikidata maintain the valid time of facts, NELL records the time point at which a fact is retrieved from some Web corpora. Collectively, these knowledge graphs (KGs) store facts extracted from Wikipedia and other sources. Due to the imprecise nature of the extraction tools that are used to build and expand KGs, such as NELL, the facts in the KGs are weighted (a confidence value representing the correctness of a fact). Additionally, NELL can be considered as a transaction time KG because every fact is associated with extraction date. On the other hand, YAGO and Wikidata use the valid time model because they only maintain facts together with their validity time (temporal scope). In this paper, we propose a bitemporal model (that combines transaction and valid time models) for maintaining and querying probabilistic temporal knowledge graphs. We report our evaluation results of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "0374a75bb69cf7f2d44e5427b6471244ac9b2c56",
            "title": "Scaling Probabilistic Temporal Query Evaluation",
            "abstract": "Open information extraction has driven automatic construction of (temporal) knowledge graphs (e.g. YAGO) that maintain probabilistic (temporal) facts and inference rules. One of the most important tasks in these knowledge graphs is query evaluation. This task is well known to be #P-hard. One of the bottlenecks of probabilistic (temporal) query evaluation is finding efficient ways of grounding the query and inference rules, to generate a factor graph that can be used for approximate query evaluation or to retrieve lineages of queries for exact evaluation. In this work, we propose the PRATiQUE (PRobAbilistic Temporal QUery Evaluation) framework for scalable temporal query evaluation. It harnesses the structure of temporal inference rules for efficient in-database grounding, i.e., it uses partitions to store structurally equivalent rules. Besides,PRATiQUE leverages a state-of-the-art Gibbs sampler to compute marginal probabilities of query answers. We report on an extensive experimental evaluation, which confirms the efficiency of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                }
            ]
        },
        {
            "paperId": "4dc4a76b0204d0bb111b120d6ace8e769660183a",
            "title": "TeCoRe: Temporal Conflict Resolution in Knowledge Graphs",
            "abstract": "The management of uncertainty is crucial when harvesting structured content from unstructured and noisy sources. Knowledge Graphs (kgs), maintaining both numerical and non-numerical facts supported by an underlying schema, are a prominent example. Knowledge Graph management is challenging because: (i) most of existing kgs focus on static data, thus impeding the availability of timewise knowledge; (ii) facts in kgs are usually accompanied by a confidence score, which witnesses how likely it is for them to hold.We demonstrate TeCoRe, a system for temporal inference and conflict resolution in uncertain temporal knowledge graphs (utkgs). At the heart of TeCoRe are two state-of-the-art probabilistic reasoners that are able to deal with temporal constraints efficiently. While one is scalable, the other can cope with more expressive constraints. The demonstration will focus on enabling users and applications to find inconsistencies in utkgs. TeCoRe provides an interface allowing to select utkgs and editing constraints; shows the maximal consistent subset of the utkg, and displays statistics (e.g., number of noisy facts removed) about the debugging process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "145750056",
                    "name": "G. Pirr\u00f2"
                },
                {
                    "authorId": "2264589",
                    "name": "J. Schoenfisch"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "660e01a53591e994f6249df612768472373edb75",
            "title": "Rule Based Temporal Inference",
            "abstract": "Time-wise knowledge is relevant in knowledge graphs as the majority facts are true in some time period, for instance, (Barack Obama, president of, USA, 2009, 2017). Consequently, temporal information extraction and temporal scoping of facts in knowledge graphs have been a focus of recent research. Due to this, a number of temporal knowledge graphs have become available such as YAGO and Wikidata. In addition, since the temporal facts are obtained from open text, they can be weighted, i.e., the extraction tools assign each fact with a confidence score indicating how likely that fact is to be true. Temporal facts coupled with confidence scores result in a probabilistic temporal knowledge graph. In such a graph, probabilistic query evaluation (marginal inference) and computing most probable explanations (MPE inference) are fundamental problems. In addition, in these problems temporal coalescing, an important research in temporal databases, is very challenging. In this work, we study these problems by using probabilistic programming. We report experimental results comparing the efficiency of several state of the art systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "7c2551f72eaed69ff0dd3d721436fd7740dad88b",
            "title": "Time Travel Queries in RDF Archives",
            "abstract": "We research the problem of querying RDF archives. In this setting novel data management challenges emerge in terms of support for time-traversing structured queries. We formalize an extension of SPARQL, called SPARQ\u2013LTL, which incorporates primitives inspired by linear temporal logic. We give formal semantics for SPARQ\u2013LTL and devise query rewriting strategies from SPARQ\u2013 LTL into SPARQL. The usage of SPARQ\u2013LTL allows to gain conciseness and readability when expressing complex temporal queries. We implemented our approach and evaluated query running time and query succinctness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1705291",
                    "name": "Valeria Fionda"
                },
                {
                    "authorId": "145750056",
                    "name": "G. Pirr\u00f2"
                }
            ]
        },
        {
            "paperId": "83523b75a1c69f2e8fadbd3b6d54078344063c84",
            "title": "Marrying Uncertainty and Time in Knowledge Graphs",
            "abstract": "\n \n The management of uncertainty is crucial when harvesting structured content from unstructured and noisy sources. Knowledge Graphs ( KGs ) are a prominent example. KGs maintain both numerical and non-numerical facts, with the support of an underlying schema. These facts are usually accompanied by a confidence score that witnesses how likely is for them to hold. Despite their popularity, most of existing KGs focus on static data thus impeding the availabilityof timewise knowledge. What is missing is a comprehensive solution for the management of uncertain and temporal data in KGs . The goal of this paper is to fill this gap. We rely on two main ingredients. The first is a numerical extension of Markov Logic Networks (MLNs) that provide the necessary underpinning to formalize the syntax and semantics of uncertain temporal KGs . The second is a set of Datalog constraints with inequalities that extend the underlying schema of the KGs and help to detect inconsistencies. From a theoretical point of view, we discuss the complexity of two important classes of queries for uncertain temporal KGs: maximuma-posteriori and conditional probability inference. Due to the hardness of these problems and the fact that MLN solvers do not scale well, we also explore the usage of Probabilistic Soft Logics (PSL) as a practical tool to support our reasoning tasks. We report on an experimental evaluation comparing the MLN and PSL approaches.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "145750056",
                    "name": "G. Pirr\u00f2"
                },
                {
                    "authorId": "2264589",
                    "name": "J. Schoenfisch"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "221802c451eb00a337e39b131ab3a7dcb0744e31",
            "title": "Gize: A Time Warp in the Web of Data",
            "abstract": "We introduce the Gize framework for querying historical RDF data. Gize builds upon two main pillars: a lightweight approach to keep historical data, and an extension of SPARQL called SPARQ\u2013LTL, which incorporates temporal logic primitives to enable a rich class of queries. One striking point of Gize is that its features can be readily made available in existing query processors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705291",
                    "name": "Valeria Fionda"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "145750056",
                    "name": "G. Pirr\u00f2"
                }
            ]
        },
        {
            "paperId": "842ebc5d3fea91a0cb73f735d9a8f6201e9189ba",
            "title": "On the Containment of SPARQL Queries under Entailment Regimes",
            "abstract": "\n \n Most description logics (DL) query languages allow instance retrieval from an ABox. However, SPARQL is a schema query language allowing access to the TBox (in addition to the ABox). Moreover, its entailment regimes enable to take into account knowledge inferred from knowledge bases in the query answering process. This provides a new perspective for the containment problem. In this paper, we study the containment of SPARQL queries over OWL EL axioms under entailment. OWL EL is the language used by many large scale ontologies and is based on EL++. The main contribution is a novel approach to rewriting queries using SPARQL property paths and the \u03bc-calculus in order to reduce containment test under entailment into validity check in the \u03bc-calculus.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                }
            ]
        },
        {
            "paperId": "9559f930b542c879cb8eca6d47679afd758e211b",
            "title": "Leveraging Structural Information in Ontology Matching",
            "abstract": "Ontology matching is an important part of enabling the semantic web to reach its full potential. Most existing ontology matching methods are mainly based on linguistic information (label, name, title and comment) but from the results achieved it is realized that this information is not sufficient. The latest ontology matching research works are trying to deeply dig into the structural information of ontologies by using \"similarity-flooding\" method. However, there are several innate issues in similarity-flooding methods that lead to wrong matching results. In this paper, we report the problems of similarity-flooding in ontology matching and propose a novel method to effectively leverage the structural information of the ontology. The evaluation is conducted on OAEI ontology matching benchmarks from 2011 to 2015. The result shows that the proposed approach performs comparatively well with other state of the art matching systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144801997",
                    "name": "Cheng Xie"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1721898",
                    "name": "Blerina Spahiu"
                },
                {
                    "authorId": "1720869",
                    "name": "Hongming Cai"
                }
            ]
        },
        {
            "paperId": "badfdeba7420a7fae248ceee132d8aa7de812405",
            "title": "Markov Logic Networks with Numerical Constraints",
            "abstract": "Markov logic networks (MLNs) have proven to be useful tools for reasoning about uncertainty in complex knowledge bases. In this paper, we extend MLNs with numerical constraints and present an efficient implementation in terms of a cutting plane method. This extension is useful for reasoning over uncertain temporal data. To show the applicability of this extension, we enrich log-linear description logics (DLs) with concrete domains (datatypes). Thereby, allowing to reason over weighted DLs with datatypes. Moreover, we use the resulting formalism to reason about temporal assertions in DB-pedia, thus illustrating its practical use.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "2106098608",
                    "name": "Jakob Huber"
                },
                {
                    "authorId": "1795542",
                    "name": "Christian Meilicke"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "fb04ec343a729f6cbbf5e4da758647173430e33a",
            "title": "Uncertain Temporal Knowledge Graphs",
            "abstract": "Temporal data can be found in various sources from patient histories, purchase histories, employee histories, to web logs. Recent advances in open information extraction have paved the way for automatic construction of knowledge graphs (kgs) from such sources. Often the extraction tools used to construct kgs produce facts and rules along with their confidence scores, leading to the notion of uncertain temporal kgs. The facts and rules contained in these graphs tend to be noisy and erroneous due to either the accuracy of the extraction tools or uncertainty in the source data. In this work, we use a numerical extension of Markov logic networks to provide formal syntax and semantics for uncertain temporal kgs. Moreover, we propose a set of datalog constraints with inequalities, that extend the underlying schema of the kgs and help in resolving conflicting facts. Finally, we characterize the complexity of two important queries, maximum a-posteriori and conditional probability inference, for uncertain temporal kgs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "941d55ae887a4139ddada715ee82ae630b99746a",
            "title": "Probabilistic EL++ with nominals and concrete domains",
            "abstract": "We present MEL (M denotes Markov logic networks) an extension of the log-linear description logics EL-LL with concrete domains, nominals, and instances. We use Markov logic networks (MLNs) in order to find the most probable, classified and coherent EL ontology from anMEL knowledge base. In particular, we develop a novel way to deal with concrete domains by extending MLN\u2019s cutting plane inference (CPI) algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "2106098608",
                    "name": "Jakob Huber"
                },
                {
                    "authorId": "1795542",
                    "name": "Christian Meilicke"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "bfa03cacb3643923f497c67275f97ee513241406",
            "title": "Towards Log-Linear Logics with Concrete Domains",
            "abstract": "We present \nMEL \n++ \n(M denotes Markov logic net- \nworks) an extension of the log-linear description log- \nics \nEL \n++ \n-LL with concrete domains, nominals, and in- \nstances. We use Markov logic networks (MLNs) in or- \nder to find the most probable, classified and coherent \nEL \n++ \nontology from an \nMEL \n++ \nknowledge base. In \nparticular, we develop a novel way to deal with concrete \ndomains (also known as datatypes) by extending MLN\u2019s \ncutting plane inference (CPI) algorithm.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "2106098608",
                    "name": "Jakob Huber"
                },
                {
                    "authorId": "1698459",
                    "name": "H. Stuckenschmidt"
                }
            ]
        },
        {
            "paperId": "937216813f08aade8d16cc7a55a3d6268d99a219",
            "title": "Schema Query Containment",
            "abstract": "SPARQL is a schema query language allowing access to the TBox part of a knowledge base. Moreover its entailment regimes enable to take into account knowledge inferred from persistently stored knowledge bases in the query answering process. Thus, the emergence of SPARQL entailment regimes provide a new perspective for the containment problem. As one has to deal with axiomatic triples, datatype reasoning, and blank nodes that result in infinite answers. Of particular interest for us is the union of conjunctive queries that are a core fragment of SPARQL. In this paper, we study the containment of such queries based on the OWL-ALCH Direct and RDF-Based Semantics entailment regimes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                }
            ]
        },
        {
            "paperId": "04460e93fe3823934821ae0f2c5bf70e383bd963",
            "title": "An FCA Framework for Knowledge Discovery in SPARQL Query Answers",
            "abstract": "Formal concept analysis (FCA) is used for knowledge discovery within data. In FCA, concept lattices are very good tools for classification and organization of data. Hence, they can also be used to visualize the answers of a SPARQL query instead of the usual answer formats such as: RDF/XML, JSON, CSV, and HTML. Consequently, in this work, we apply FCA to reveal and visualize hidden relations within SPARQL query answers by means of concept lattices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1699343",
                    "name": "A. Napoli"
                }
            ]
        },
        {
            "paperId": "07902ecb4c4a977d84e840fdae71ec66d63f1ce4",
            "title": "Lattice Based Data Access (LBDA): An Approach for Organizing and Accessing Linked Open Data in Biology",
            "abstract": "In the recent years, web has turned into a \"Web of Data\" with a signi_cant increase in the number of users looking for direct access to the data embedded in web pages. At the same time, a large amount of Linked Open Data (LOD) is available on line which allows e_ective exploration and navigation. However, there are still needs to bridge the gap between di_erent data sources and formats, for improving data analysis, data integration and information retrieval. This paper focuses on Lattice Based Data Access (LBDA), a framework following the lines of Ontology Based Data Access (OBDA) and which is based on Formal Concept Analysis (FCA) and Relational Concept Analysis (RCA). In this way, operations such as query answering on data are carried out on concept lattices which are acting a representation and an indexing of data. The LBDA framework provides a view over LOD with reference to data and constraints provided by a user, and it helps in information retrieval and knowledge discovery thanks to FCA and RCA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33973438",
                    "name": "Mehwish Alam"
                },
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1797384",
                    "name": "Adrien Coulet"
                },
                {
                    "authorId": "1699343",
                    "name": "A. Napoli"
                },
                {
                    "authorId": "1399515325",
                    "name": "Malika Sma\u00efl-Tabbone"
                }
            ]
        },
        {
            "paperId": "e7849f8bfb6019621d35708ce7b42259dfac6ca5",
            "title": "A Study on the Correspondence between FCA and ELI Ontologies",
            "abstract": "The description logic EL has been used to support ontology design in various domains, and especially in biology and medecine. EL is known for its efficient reasoning and query answering capabilities. By contrast, ontology design and query answering can be supported and guided within an FCA framework. Accordingly, in this paper, we propose a formal transformation of ELI (an extension of EL with inverse roles) ontologies into an FCA framework, i.e. KELI, and we provide a formal characterization of this transformation. Then we show that SPARQL query answering over ELI ontologies can be reduced to lattice query answering over KELI concept lattices. This simplifies the query answering task and shows that some basic semantic web tasks can be improved when considered from an FCA perspective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1699343",
                    "name": "A. Napoli"
                }
            ]
        },
        {
            "paperId": "0f8ee4878d13014e528d079d573e716880881fc1",
            "title": "A Benchmark for Semantic Web Query Containment, Equivalence and Satisfiability",
            "abstract": "The problem of SPARQL query containment has recently attracted a lot of attention due to its fundamental purpose in query optimization and information integration. New approaches to this problem, have been put forth, that can be implemented in practice. However, these approaches suffer from various limitations: coverage (size and type of queries), response time (how long it takes to determine containment), and the technique applied to encode the problem. In order to experimentally assess implementation limitations, we designed a benchmark suite offering different experimental settings depending on the type of queries, projection and reasoning (RDFS). We have applied this benchmark to three available systems using different techniques highlighting the strengths and weaknesses of such systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1800811",
                    "name": "J. Euzenat"
                },
                {
                    "authorId": "1768207",
                    "name": "P. Genev\u00e8s"
                },
                {
                    "authorId": "1685111",
                    "name": "Nabil Laya\u00efda"
                }
            ]
        },
        {
            "paperId": "44fa688beb7b0a2a58540eebbab49f5b87ca3cc4",
            "title": "SPARQL Query Containment Under SHI Axioms",
            "abstract": "\n \n SPARQL query containment under schema axioms is the problem of determining whether, for any RDF graph satisfying a given set of schema axioms, the answers to a query are contained in the answers of another query. This problem has major applications for verification and optimization of queries. In order to solve it, we rely on the mu-calculus. Firstly, we provide a mapping from RDF graphs into transition systems. Secondly, SPARQL queries and RDFS and SHI axioms are encoded into mu-calculus formulas. This allows us to reduce query containment and equivalence to satisfiability in the mu-calculus. Finally, we prove a double exponential upper bound for containment under SHI schema axioms.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "1800811",
                    "name": "J. Euzenat"
                },
                {
                    "authorId": "1768207",
                    "name": "P. Genev\u00e8s"
                },
                {
                    "authorId": "1685111",
                    "name": "Nabil Laya\u00efda"
                }
            ]
        },
        {
            "paperId": "34e7c470eaa11961584644599ca0b5cd3c1ac97a",
            "title": "PSPARQL Query Containment",
            "abstract": "Querying the semantic web is mainly done through SPARQL. This language has been studied from different perspectives such as optimization and extension. One of its extensions, PSPARQL (Path SPARQL) provides queries with paths of arbitrary length. We study the static analysis of queries written in this language, in particular, containment of queries: determining whether, for any graph, the answers to a query are contained in those of another query. Our approach consists in encoding RDF graphs as transition systems and queries as \\mu-calculus formulas and then reducing the containment problem to testing satisfiability in the logic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2077657",
                    "name": "M. Chekol"
                },
                {
                    "authorId": "2281455131",
                    "name": "J\u00e9r\u00f4me Euzenat"
                },
                {
                    "authorId": "1768207",
                    "name": "P. Genev\u00e8s"
                },
                {
                    "authorId": "1685111",
                    "name": "Nabil Laya\u00efda"
                }
            ]
        }
    ]
}