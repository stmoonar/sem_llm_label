{
    "authorId": "7661726",
    "papers": [
        {
            "paperId": "72cce47fd053bf916314d89a8174726c58c05e02",
            "title": "Towards Open-Domain Twitter User Profile Inference",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4428136",
                    "name": "Haoyang Wen"
                },
                {
                    "authorId": "123034558",
                    "name": "Zhenxin Xiao"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                }
            ]
        },
        {
            "paperId": "3d5d32986c66a192fff1c6cc269161fc7539599a",
            "title": "TRM:Temporal Relocation Module for Video Recognition",
            "abstract": "One of the key differences between video and image understanding lies in how to model the temporal information. Due to the limit of convolution kernel size, most previous methods try to model long-term temporal information via sequentially stacked convolution layers. Such conventional manner doesn\u2019t explicitly differentiate regions/pixels with various temporal receptive requirements and may suffer from temporal information distortion. In this paper, we propose a novel Temporal Relocation Module (TRM), which can capture the long-term temporal dependence in a spatial-aware manner adaptively. Specifically, it relocates the spatial features along the temporal dimension, through which an adaptive temporal receptive field is aligned to within the global temporal interval of input video, TRM can potentially model the long-term temporal information with an equivalent receptive field of the entire video. Experiment results on three representative video recognition benchmarks demonstrate TRM outperforms previous state-of-the-arts noticeably and verifies the effectiveness of our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "89892460",
                    "name": "Yijun Qian"
                },
                {
                    "authorId": "3374337",
                    "name": "Guoliang Kang"
                },
                {
                    "authorId": "8547960",
                    "name": "Lijun Yu"
                },
                {
                    "authorId": "3446043",
                    "name": "Wenhe Liu"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                }
            ]
        },
        {
            "paperId": "6ac02dee3372535d9f3fbb3f67b8ef881b50e2aa",
            "title": "Training Vision-Language Transformers from Captions Alone",
            "abstract": "Vision-Language Transformers can be learned without low-level human labels (e.g. class labels, bounding boxes, etc). Existing work, whether explicitly utilizing bounding boxes or patches, assumes that the visual backbone must first be trained on ImageNet class prediction before being integrated into a multimodal linguistic pipeline. We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders that does not require this supervision. In fact, in a head-to-head comparison between ViLT, the current state-of-the-art patch-based vision-language transformer which is pretrained with supervised object classification, and our model, VLC, we find that our approach 1. outperforms ViLT on standard benchmarks, 2. provides more interpretable and intuitive patch visualizations, and 3. is competitive with many larger models that utilize ROIs trained on annotated bounding-boxes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1970583",
                    "name": "Liangke Gui"
                },
                {
                    "authorId": "2110991956",
                    "name": "Qiuyuan Huang"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "3312309",
                    "name": "Yonatan Bisk"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "048726930aa5062a55bc02119237b4a6df97e082",
            "title": "MuCAI'21: 2nd ACM Multimedia Workshop on Multimodal Conversational AI",
            "abstract": "The second edition of the International Workshop on Multimodal Conversational AI puts forward a diverse set of contributions that aim to brainstorm this new field. Conversational agents are now becoming a commodity as this technology is being applied to a wide range of domains. Healthcare, assisting technologies, e-commerce, information seeking, are some of the domains where multimodal conversational AI is being explored. The wide use of multimodal conversational agents exposes the many challenges in achieving more natural, human-like, and engaging conversational agents. The research contributions of the Workshop actively address several of relevant challenges: How to include assistive-technologies in dialog systems? How can agents engage in negotiation in dialogs? How to handle the embodiment of conversational agents? Keynote speakers, both with real-world experience in conversational AI, will share their most recent and exciting work. The panel will address technological, ethical, legal and social aspects of conversational search. Finally, invited contributions from research projects will showcase how the different domains can benefit from conversational technology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144431718",
                    "name": "Jo\u00e3o Magalh\u00e3es"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "138754258",
                    "name": "R. Sousa"
                },
                {
                    "authorId": "143960221",
                    "name": "Carlos Santiago"
                }
            ]
        },
        {
            "paperId": "6e0a4138a39ce8259c153b917c2c41dbd62f1c69",
            "title": "Importance of Parasagittal Sensor Information in Tongue Motion Capture Through a Diphonic Analysis",
            "abstract": "Our study examines the information obtained by adding two parasagittal sensors to the standard midsagittal configuration of an Electromagnetic Articulography (EMA) observation of lingual articulation. In this work, we present a large and phonetically balanced corpus obtained from an EMA recording session of a single English native speaker reading 1899 sentences from the Harvard and TIMIT corpora. According to a statistical analysis of the diphones produced during the recording session, the motion captured by the parasagittal sensors has a low correlation to the midsagittal sensors in the mediolateral direction. We perform a geometric analysis of the lateral tongue by the measure of its width and using a proxy of the tongue\u2019s curvature that is computed using the Menger curvature. To provide a better understanding of the tongue sensor motion we present dynamic visualizations of all diphones. Finally, we present a summary of the velocity information computed from the tongue sensor information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "145324126",
                    "name": "Sarah L. Taylor"
                },
                {
                    "authorId": "1739506",
                    "name": "M. Tiede"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "2057077632",
                    "name": "Iain A. Matthews"
                }
            ]
        },
        {
            "paperId": "91ad6333dd6475b027c7313ec082fd4128981258",
            "title": "Statistical Distance Metric Learning for Image Set Retrieval",
            "abstract": "Measuring similarity between two image sets is instrumental in many computer vision tasks, such as video face recognition, multi-shot person re-identification and gait recognition. In most of the recent works, it is done by aggregating the embedding features of images as a fixed size vector, and calculating a metric in vector space (i.e. Euclidean distance). The embedding feature function can be learned by deep metric learning (DML) technique. However, methods relying on feature aggregation fail to capture the diversity and uncertainty within image sets. In this paper, we obviate the need of feature aggregation and propose a novel Statistical Distance Metric Learning (SDML) framework, which represents each image set as a probability distribution in embedding feature space and compares two image sets by statistical distance between their distributions. Among all types of statistical distance, we choose Jeffrey\u2019s divergence (JD), which can be obtained from two embedding feature sets by kNN based density estimator. We also design a statistical centroid loss function to enhance the discriminative power of training process. Our SDML framework naturally preserves the diversity within an image set, and the relation between two sets. We evaluate our proposed approach on gait recognition and multi-shot person re-id. The experiment results show that SDML outperforms conventional DML, and also receives competitive/superior performance comparing to the previous state-of-the-arts on the aforementioned tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2998709",
                    "name": "Ting-yao Hu"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                }
            ]
        },
        {
            "paperId": "d80278ee0ad1f025eaea16ff6cad755c6a0c92ce",
            "title": "MMPT'21: International Joint Workshop on Multi-Modal Pre-Training for Multimedia Understanding",
            "abstract": "Pre-training has been an emerging topic that provides a way to learn strong representation in many fields (e.g., natural language processing, computing vision). In the last few years, we have witnessed many research works on multi-modal pre-training which have achieved state-of-the-art performances on many multimedia tasks (e.g., image-text retrieval, video localization, speech recognition). In this workshop, we aim to gather peer researchers on related topics for more insightful discussion. We also intend to attract more researchers to explore and investigate more opportunities of designing and using innovative pre-training models for multimedia tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2127734772",
                    "name": "Bei Liu"
                },
                {
                    "authorId": "3247966",
                    "name": "Jianlong Fu"
                },
                {
                    "authorId": "3009919",
                    "name": "Shizhe Chen"
                },
                {
                    "authorId": "1721329",
                    "name": "Qin Jin"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "2105924099",
                    "name": "Yong Rui"
                }
            ]
        },
        {
            "paperId": "f17845a9bc2e3173b6af5020a1e54b8d639cf4df",
            "title": "iFetch: Multimodal Conversational Agents for the Online Fashion Marketplace",
            "abstract": "Most of the interaction between large organizations and their users will be mediated by AI agents in the near future. This perception is becoming undisputed as online shopping dominates entire market segments, and the new \"digitally-native\" generations become consumers. iFetch is a new generation of task-oriented conversational agents that interact with users seamlessly using verbal and visual information. Through the conversation, iFetch provides targeted advice and a \"physical store-like\" experience while maintaining user engagement. This context entails the following vital components: 1) highly complex memory models that keep track of the conversation, 2) extraction of key semantic features from language and images that reveal user intent, 3) generation of multimodal responses that will keep users engaged in the conversation and 4) an interrelated knowledge base of products from which to extract relevant product lists.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "138754258",
                    "name": "R. Sousa"
                },
                {
                    "authorId": "1398405127",
                    "name": "Pedro Ferreira"
                },
                {
                    "authorId": "2069913901",
                    "name": "Pedro Costa"
                },
                {
                    "authorId": "145154317",
                    "name": "Pedro Azevedo"
                },
                {
                    "authorId": "145848221",
                    "name": "J. Costeira"
                },
                {
                    "authorId": "2141079399",
                    "name": "Carlos Santiago"
                },
                {
                    "authorId": "144431718",
                    "name": "Jo\u00e3o Magalh\u00e3es"
                },
                {
                    "authorId": "2141110199",
                    "name": "David Semedo"
                },
                {
                    "authorId": "134637131",
                    "name": "Rafael Ferreira"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                }
            ]
        },
        {
            "paperId": "f3200d17ff73bfb38180dccd796cc616f995a52f",
            "title": "Learning to Hallucinate Examples from Extrinsic and Intrinsic Supervision",
            "abstract": "Learning to hallucinate additional examples has recently been shown as a promising direction to address few-shot learning tasks. This work investigates two important yet overlooked natural supervision signals for guiding the hallucination process \u2013 (i) extrinsic: classifiers trained on hallucinated examples should be close to strong classifiers that would be learned from a large amount of real examples; and (ii) intrinsic: clusters of hallucinated and real examples belonging to the same class should be pulled together, while simultaneously pushing apart clusters of hallucinated and real examples from different classes. We achieve (i) by introducing an additional mentor model on data-abundant base classes for directing the hallucinator, and achieve (ii) by performing contrastive learning between hallucinated and real examples. As a general, model-agnostic framework, our dual mentor-and self-directed (DMAS) hallucinator significantly improves few-shot learning performance on widely-used benchmarks in various scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1970583",
                    "name": "Liangke Gui"
                },
                {
                    "authorId": "1453740540",
                    "name": "Adrien Bardes"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "145670946",
                    "name": "M. Hebert"
                },
                {
                    "authorId": "2302062",
                    "name": "Yu-Xiong Wang"
                }
            ]
        },
        {
            "paperId": "235437e7dbe2aa20bc912697e969689ce8d54a1e",
            "title": "Gun Source and Muzzle Head Detection",
            "abstract": "\n There is a surging need across the world for protection against gun violence. There are three main areas that we have identified as challenging in research that tries to curb gun violence: temporal location of gunshots, gun type prediction and gun source (shooter) detection. Our\n task is gun source detection and muzzle head detection, where the muzzle head is the round opening of the firing end of the gun. We would like to locate the muzzle head of the gun in the video visually, and identify who has fired the shot. In our formulation, we turn the problem of muzzle\n head detection into two sub-problems of human object detection and gun smoke detection. Our assumption is that the muzzle head typically lies between the gun smoke caused by the shot and the shooter. We have interesting results both in bounding the shooter as well as detecting the gun smoke.\n In our experiments, we are successful in detecting the muzzle head by detecting the gun smoke and the shooter.\n",
            "fieldsOfStudy": [
                "Computer Science",
                "Geology"
            ],
            "authors": [
                {
                    "authorId": "144812501",
                    "name": "Zhong Zhou"
                },
                {
                    "authorId": "51239364",
                    "name": "I. C. Etinger"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "2064429921",
                    "name": "A. Waibel"
                }
            ]
        }
    ]
}