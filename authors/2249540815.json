{
    "authorId": "2249540815",
    "papers": [
        {
            "paperId": "289533e16e509d0ba8f499a371b4e470f3e492de",
            "title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation",
            "abstract": "Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a \u201cbigram paraphrase\u201d attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257002716",
                    "name": "Abe Bohan Hou"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2125062703",
                    "name": "Yichen Wang"
                },
                {
                    "authorId": "2257002712",
                    "name": "Yung-Sung Chuang"
                },
                {
                    "authorId": "2257103558",
                    "name": "Hongwei Wang"
                },
                {
                    "authorId": "2121272448",
                    "name": "Lingfeng Shen"
                },
                {
                    "authorId": "7536576",
                    "name": "Benjamin Van Durme"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "5a8a6b61033ba2355f7c149cec596c88a1d61954",
            "title": "Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks",
            "abstract": "The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2125062703",
                    "name": "Yichen Wang"
                },
                {
                    "authorId": "2284701198",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2257002716",
                    "name": "Abe Bohan Hou"
                },
                {
                    "authorId": "2284689671",
                    "name": "Xiao Pu"
                },
                {
                    "authorId": "2154740036",
                    "name": "Chao Shen"
                },
                {
                    "authorId": "2265689457",
                    "name": "Xiaoming Liu"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                }
            ]
        },
        {
            "paperId": "752f684371c9901791259dc4afd04b9754e803d1",
            "title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?",
            "abstract": "Large language models (LLMs) demonstrate great potential for problems with implicit graphical structures, while recent works seek to enhance the graph reasoning capabilities of LLMs through specialized instruction tuning. The resulting 'graph LLMs' are evaluated with in-distribution settings only, thus it remains underexplored whether LLMs are learning generalizable graph reasoning skills or merely memorizing patterns in the synthetic training data. To this end, we propose the NLGift benchmark, an evaluation suite of LLM graph reasoning generalization: whether LLMs could go beyond semantic, numeric, structural, reasoning patterns in the synthetic training data and improve utility on real-world graph-based tasks. Extensive experiments with two LLMs across four graph reasoning tasks demonstrate that while generalization on simple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to generalize across reasoning and real-world patterns, casting doubt on the benefit of synthetic graph tuning for real-world tasks with underlying network structures. We explore three strategies to improve LLM graph reasoning generalization, and we find that while post-training alignment is most promising for real-world tasks, empowering LLM graph reasoning to go beyond pattern memorization remains an open research question.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308074430",
                    "name": "Yizhuo Zhang"
                },
                {
                    "authorId": "2256778370",
                    "name": "Heng Wang"
                },
                {
                    "authorId": "2284701198",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "2257023881",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "94c28609614719c68469081ed99315f54cb1fb6a",
            "title": "k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text",
            "abstract": "Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257002716",
                    "name": "Abe Bohan Hou"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2125062703",
                    "name": "Yichen Wang"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                }
            ]
        },
        {
            "paperId": "9b9e44d43eb6bc0be7a07a92b2100dd6a7f3b158",
            "title": "Learning Syntax Without Planting Trees: Understanding When and Why Transformers Generalize Hierarchically",
            "abstract": "Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge. We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically. We then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure. When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order). Finally, we take a Bayesian perspective to further uncover transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2030973547",
                    "name": "Madhur Panwar"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2298441996",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2286065792",
                    "name": "Navin Goyal"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "000cc7bff1a286193286f095f1668eacedbefc1a",
            "title": "LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud",
            "abstract": "hy-pothetically",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2276030791",
                    "name": "Meng Zhang"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2155412834",
                    "name": "Tianle Wang"
                },
                {
                    "authorId": "2115471757",
                    "name": "Fatemehsadat Mireshghallah"
                },
                {
                    "authorId": "2250011506",
                    "name": "Binyi Chen"
                },
                {
                    "authorId": "2256769500",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "33d944de189d6edf3a510ea195803a381c5a3bab",
            "title": "Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models",
            "abstract": "Large language models (LLMs) are widely adopted in knowledge-intensive tasks and have achieved impressive performance thanks to their knowledge abilities. While LLMs have demonstrated outstanding performance on atomic or linear (multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with interweaving constraints remains an underexplored problem. In this work, we propose geometric reasoning over structured knowledge, where pieces of knowledge are connected in a graph structure and models need to fill in the missing information of this graph. Such geometric knowledge reasoning would require the ability to handle structured knowledge, reason with uncertainty, verify facts, and backtrack when an error occurs. We specifically propose KNOWLEDGE CROSSWORDS, a multi-blank QA dataset where each problem consists of a natural language question representing the geometric constraints of an incomplete entity network, where LLMs are tasked with working out the missing entities while meeting all factual constraints. KNOWLEDGE CROSSWORDS contains 2,101 individual problems, covering a wide array of knowledge domains and further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLM prompting approaches on the KNOWLEDGE CROSSWORDS benchmark. We additionally propose two new approaches, STAGED PROMPTING and VERIFY-ALL, to augment LLMs\u2019 ability to backtrack and verify structured constraints. Our results demonstrate that while baseline approaches perform well on easier problems but struggle with questions on the hard side, our proposed VERIFY-ALL outperforms other methods by a large margin and is more robust with hard problems. Further analysis reveals that LLMs\u2019 ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more. Code and data are publicly available at https://github.com/Wenwen-D/KnowledgeCrosswords.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2037491",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2169159066",
                    "name": "Yuhan Liu"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "3f4ccf64ffe23b5dc095ae0401eecf9445deb024",
            "title": "Resolving Knowledge Conflicts in Large Language Models",
            "abstract": "Large language models (LLMs) often encounter knowledge conflicts, scenarios where discrepancy arises between the internal parametric knowledge of LLMs and non-parametric information provided in the prompt context. In this work we ask what are the desiderata for LLMs when a knowledge conflict arises and whether existing LLMs fulfill them. We posit that LLMs should 1) identify knowledge conflicts, 2) pinpoint conflicting information segments, and 3) provide distinct answers or viewpoints in conflicting scenarios. To this end, we introduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual knowledge conflicts and quantitatively evaluating to what extent LLMs achieve these goals. KNOWLEDGE CONFLICT includes diverse and complex situations of knowledge conflict, knowledge from diverse entities and domains, two synthetic conflict creation methods, and settings with progressively increasing difficulty to reflect realistic knowledge conflicts. Extensive experiments with the KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting knowledge and produce a response with distinct answers amidst conflicting information. To address these challenges, we propose new instruction-based approaches that augment LLMs to better achieve the three goals. Further analysis shows that abilities to tackle knowledge conflicts are greatly impacted by factors such as knowledge domain and prompt text, while generating robust responses to knowledge conflict scenarios remains an open research question.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108853330",
                    "name": "Yike Wang"
                },
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2256778370",
                    "name": "Heng Wang"
                },
                {
                    "authorId": "2254168375",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "513fdb089e079f1aa640b76da2427eee64a86439",
            "title": "Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language Models",
            "abstract": "We propose Knowledge Crosswords, a geometric knowledge reasoning benchmark consisting of incomplete knowledge networks bounded by structured factual constraints, where LLMs are tasked with inferring the missing facts to meet all constraints. The novel setting of geometric knowledge reasoning necessitates new LM abilities beyond existing atomic/linear multi-hop QA, such as backtracking, verifying facts and constraints, reasoning with uncertainty, and more. Knowledge Crosswords contains 2,101 individual problems, covering diverse knowledge domains, and is further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLMs and approaches on Knowledge Crosswords. Results demonstrate that baseline approaches struggle with larger knowledge networks and semantically-equivalent entity distractors. In light of their limitations, we propose two new approaches, Staged Prompting and Verify-All, to augment LLMs' abilities for error-aware backtracking and constraint verification. Our Verify-All significantly outperforms prior methods and is more robust towards problems in the hard subset. Further analysis shows that geometric knowledge reasoning poses new challenges to LLMs' knowledge abilities, particularly in robustness towards varying option orders, complex structural constraints in knowledge networks,\"none of the above\"scenarios, and more.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282214127",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "2284701198",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2169159066",
                    "name": "Yuhan Liu"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "94be9ee72de86a909fd29a739f203e1aa6bc165e",
            "title": "On the Zero-Shot Generalization of Machine-Generated Text Detectors",
            "abstract": "The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2243189215",
                    "name": "Xiao Pu"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2257023881",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                }
            ]
        }
    ]
}