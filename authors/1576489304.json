{
    "authorId": "1576489304",
    "papers": [
        {
            "paperId": "0113019c10511ae55feab9f7a12cdeadee72164c",
            "title": "TiV-ODE: A Neural ODE-based Approach for Controllable Video Generation From Text-Image Pairs",
            "abstract": "Videos capture the evolution of continuous dynamical systems over time in the form of discrete image sequences. Recently, video generation models have been widely used in robotic research. However, generating controllable videos from image-text pairs is an important yet underexplored research topic in both robotic and computer vision communities. This paper introduces an innovative and elegant framework named TiV-ODE, formulating this task as modeling the dynamical system in a continuous space. Specifically, our framework leverages the ability of Neural Ordinary Differential Equations (Neural ODEs) to model the complex dynamical system depicted by videos as a nonlinear ordinary differential equation. The resulting framework offers control over the generated videos\u2019 dynamics, content, and frame rate, a feature not provided by previous methods. Experiments demonstrate the ability of the proposed method to generate highly controllable and visually consistent videos and its capability of modeling dynamical systems. Overall, this work is a significant step towards developing advanced controllable video generation models that can handle complex and dynamic scenes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130973203",
                    "name": "Yucheng Xu"
                },
                {
                    "authorId": "2315860661",
                    "name": "Nanbo Li"
                },
                {
                    "authorId": "2282529324",
                    "name": "Arushi Goel"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "2315635684",
                    "name": "Zijian Guo"
                },
                {
                    "authorId": "69533920",
                    "name": "H. Kasaei"
                },
                {
                    "authorId": "3160474",
                    "name": "M. Kasaei"
                },
                {
                    "authorId": "2266266761",
                    "name": "Zhibin Li"
                }
            ]
        },
        {
            "paperId": "0c57cb87fcfb345ddc13bab3f20e9c203bac52e9",
            "title": "JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability",
            "abstract": "Large Language Models (LLMs) have demonstrated a remarkable potential in medical knowledge acquisition and question-answering. However, LLMs can potentially hallucinate and yield factually incorrect outcomes, even with domain-specific pretraining. Previously, retrieval augmented generation (RAG) has limited success in addressing hallucinations. Unlike previous methods in RAG where the retrieval model was trained separately from the LLM, we introduce JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning phase. The synchronized training mechanism enhances JMLR's ability to retrieve clinical guidelines and leverage medical knowledge to reason and answer questions and reduces the demand for computational resources. We evaluated JMLR on the important medical question-answering application. Our experimental results demonstrate that JMLR-13B (70.5%) outperforms a previous state-of-the-art open-source model using conventional pre-training and fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances reasoning quality and reduces hallucinations better than Claude3-Opus. Additionally, JMLR-13B (148 GPU hours) also trains much faster than Meditron-70B (42630 GPU hours). Through this work, we provide a new and efficient knowledge enhancement method for healthcare, demonstrating the potential of integrating retrieval and LLM training for medical question-answering systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261394407",
                    "name": "Junda Wang"
                },
                {
                    "authorId": "2261462978",
                    "name": "Zhichao Yang"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "2261455807",
                    "name": "Hong Yu"
                }
            ]
        },
        {
            "paperId": "106c322e90bf7b62e65a14132c6b700e76695d94",
            "title": "UMass-BioNLP at MEDIQA-M3G 2024: DermPrompt - A Systematic Exploration of Prompt Engineering with GPT-4V for Dermatological Diagnosis",
            "abstract": "This paper presents our team\u2019s participation in the MEDIQA-ClinicalNLP 2024 shared task B. We present a novel approach to diagnosing clinical dermatology cases by integrating large multimodal models, specifically leveraging the capabilities of GPT-4V under a retriever and a re-ranker framework. Our investigation reveals that GPT-4V, when used as a retrieval agent, can accurately retrieve the correct skin condition 85% of the time using dermatological images and brief patient histories. Additionally, we empirically show that Naive Chain-of-Thought (CoT) works well for retrieval while Medical Guidelines Grounded CoT is required for accurate dermatological diagnosis. Further, we introduce a Multi-Agent Conversation (MAC) framework and show it\u2019s superior performance and potential over the best CoT strategy. The experiments suggest that using naive CoT for retrieval and multi-agent conversation for critique-based diagnosis, GPT-4V can lead to an early and accurate diagnosis of dermatological conditions. The implications of this work extend to improving diagnostic workflows, supporting dermatological education, and enhancing patient care by providing a scalable, accessible, and accurate diagnostic tool.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284871029",
                    "name": "Parth Vashisht"
                },
                {
                    "authorId": "2261672647",
                    "name": "Abhilasha Lodha"
                },
                {
                    "authorId": "2298898229",
                    "name": "Mukta Maddipatla"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "35598776",
                    "name": "Avijit Mitra"
                },
                {
                    "authorId": "2261462978",
                    "name": "Zhichao Yang"
                },
                {
                    "authorId": "2261394407",
                    "name": "Junda Wang"
                },
                {
                    "authorId": "12693064",
                    "name": "Sunjae Kwon"
                },
                {
                    "authorId": "2261455807",
                    "name": "Hong Yu"
                }
            ]
        },
        {
            "paperId": "59978c2aa114627706c1db90770c3f3682bf436a",
            "title": "NoteChat: A Dataset of Synthetic Patient-Physician Conversations Conditioned on Clinical Notes",
            "abstract": "We introduce NoteChat, a novel cooperative 001 multi-agent framework leveraging Large Lan-002 guage Models (LLMs) to generate patient-003 physician dialogues. NoteChat embodies the 004 principle that an ensemble of role-specific 005 LLMs, through structured role-play and strate-006 gic prompting, can perform their assigned 007 roles more effectively. The synergy among 008 these role-playing LLMs results in a cohe-009 sive and efficient dialogue generation. Eval-010 uation on MTS-dialogue (Abacha et al., 2023; 011 Ben Abacha et al., 2023), a benchmark 012 dataset for patient-physician dialogues-note 013 pairs, shows that models trained with the aug-014 mented synthetic patient-physician dialogues 015 by NoteChat 1 outperforms other state-of-the-016 art models for generating clinical notes. Our 017 comprehensive automatic and human evalua-018 tion demonstrates that NoteChat substantially 019 surpasses state-of-the-art models like ChatGPT 020 and GPT-4 up to 22.78% by domain experts in 021 generating superior synthetic patient-physician 022 dialogues based on clinical notes. NoteChat 023 has the potential to engage patients directly and 024 help clinical documentation, a leading cause of 025 physician burnout (Budd, 2023). 026",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261394407",
                    "name": "Junda Wang"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "2261462978",
                    "name": "Zhichao Yang"
                },
                {
                    "authorId": "2261556443",
                    "name": "Huixue Zhou"
                },
                {
                    "authorId": "2237106751",
                    "name": "Rumeng Li"
                },
                {
                    "authorId": "2221230794",
                    "name": "Xun Wang"
                },
                {
                    "authorId": "2261449250",
                    "name": "Yucheng Xu"
                },
                {
                    "authorId": "2261455807",
                    "name": "Hong Yu"
                }
            ]
        },
        {
            "paperId": "6b15183fcf3516fb03cb40f4d29b9ed202b129c3",
            "title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization",
            "abstract": "Large Language Models (LLMs) such as GPT&Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes>100B parameter GPT variants like GPT-3.5&GPT-4 to act as synthetic experts to generate high-quality synthetics feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback generated by these synthetic feedback experts without additional human annotations, mirroring and optimizing the practical scenario in which medical professionals refine AI system outputs. Although such 100B+ parameter GPT variants have proven to demonstrate expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on their capacity to act as synthetic feedback experts and deliver expert-level edit feedback for improving the generation quality of weaker (<10B parameter) LLMs like GPT-2 (1.5B)&Llama 2 (7B) in clinical domain. So in this work, we leverage 100B+ GPT variants to act as synthetic feedback experts offering expert-level edit feedback, that is used to reduce hallucinations and align weaker (<10B parameter) LLMs with medical facts using two distinct alignment algorithms (DPO&SALT), endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of LLM-based synthetic edits in enhancing the alignment of clinical factuality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1381942456",
                    "name": "Prakamya Mishra"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "2284871029",
                    "name": "Parth Vashisht"
                },
                {
                    "authorId": "2263773255",
                    "name": "Feiyun Ouyang"
                },
                {
                    "authorId": "2263729379",
                    "name": "Beining Wang"
                },
                {
                    "authorId": "2091299128",
                    "name": "Vidhi Mody"
                },
                {
                    "authorId": "2261455807",
                    "name": "Hong Yu"
                }
            ]
        },
        {
            "paperId": "6ed465f0bb1005bb2071e1d2e31359bfc5abfeef",
            "title": "Large Language Model-based Role-Playing for Personalized Medical Jargon Extraction",
            "abstract": "Previous studies reveal that Electronic Health Records (EHR), which have been widely adopted in the U.S. to allow patients to access their personal medical information, do not have high readability to patients due to the prevalence of medical jargon. Tailoring medical notes to individual comprehension by identifying jargon that is difficult for each person will enhance the utility of generative models. We present the first quantitative analysis to measure the impact of role-playing in LLM in medical term extraction. By comparing the results of Mechanical Turk workers over 20 sentences, our study demonstrates that LLM role-playing improves F1 scores in 95% of cases across 14 different socio-demographic backgrounds. Furthermore, applying role-playing with in-context learning outperformed the previous state-of-the-art models. Our research showed that ChatGPT can improve traditional medical term extraction systems by utilizing role-play to deliver personalized patient education, a potential that previous models had not achieved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2315948742",
                    "name": "Jung Hoon Lim"
                },
                {
                    "authorId": "12693064",
                    "name": "Sunjae Kwon"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "2315934088",
                    "name": "John P.Lalor"
                },
                {
                    "authorId": "2316049813",
                    "name": "Hong Yu"
                }
            ]
        },
        {
            "paperId": "b8678f74d659610e8774aff81a5d13f975d7cade",
            "title": "ReadCtrl: Personalizing text generation with readability-controlled instruction learning",
            "abstract": "Content generation conditioning on users's readability is an important application for personalization. In an era of large language models (LLMs), readability-controlled text generation based on LLMs has become increasingly important. This paper introduces a novel methodology called\"Readability-Controlled Instruction Learning (ReadCtrl),\"which aims to instruction-tune LLMs to tailor users' readability levels. Unlike the traditional methods, which primarily focused on categorical readability adjustments typically classified as high, medium, and low or expert and layperson levels with limited success, ReadCtrl introduces a dynamic framework that enables LLMs to generate content at various (near continuous level) complexity levels, thereby enhancing their versatility across different applications. Our results show that the ReadCtrl-Mistral-7B models significantly outperformed strong baseline models such as GPT-4 and Claude-3, with a win rate of 52.1%:35.7% against GPT-4 in human evaluations. Furthermore, Read-Ctrl has shown significant improvements in automatic evaluations, as evidenced by better readability metrics (e.g., FOG, FKGL) and generation quality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and Coherence). These results underscore Read-Ctrl's effectiveness and tenacity in producing high-quality, contextually appropriate outputs that closely align with targeted readability levels, marking a significant advancement in personalized content generation using LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2263121822",
                    "name": "Hieu Tran"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "2232824313",
                    "name": "Lingxi Li"
                },
                {
                    "authorId": "2261455807",
                    "name": "Hong Yu"
                }
            ]
        },
        {
            "paperId": "d4cc89b1c38e014b8889ec5eb44734728ffc1784",
            "title": "LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data",
            "abstract": "Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78%, respectively, a 59% improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize LocalHealth to extrapolate CDC\u2019s estimates to proxy unreported neighborhoods, achieving an F1-score of 0.7291. Our work suggests that Twitter data can be effectively leveraged to simulate neighborhood-level MH outcomes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212876381",
                    "name": "Vijeta Deshpande"
                },
                {
                    "authorId": "2187932371",
                    "name": "Minhwa Lee"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "2263500068",
                    "name": "Zihao Zhang"
                },
                {
                    "authorId": "2284863797",
                    "name": "Jason Brian Gibbons"
                },
                {
                    "authorId": "2261455807",
                    "name": "Hong Yu"
                }
            ]
        },
        {
            "paperId": "2d5a22d55c409762de36cab680e686a2c91b73a2",
            "title": "EHR Interaction Between Patients and AI: NoteAid EHR Interaction",
            "abstract": "With the rapid advancement of Large Language Models (LLMs) and their outstanding performance in semantic and contextual comprehension, the potential of LLMs in specialized domains warrants exploration. This paper introduces the NoteAid EHR Interaction Pipeline, an innovative approach developed using generative LLMs to assist in patient education, a task stemming from the need to aid patients in understanding Electronic Health Records (EHRs). Building upon the NoteAid work, we designed two novel tasks from the patient's perspective: providing explanations for EHR content that patients may not understand and answering questions posed by patients after reading their EHRs. We extracted datasets containing 10,000 instances from MIMIC Discharge Summaries and 876 instances from the MADE medical notes collection, respectively, executing the two tasks through the NoteAid EHR Interaction Pipeline with these data. Performance data of LLMs on these tasks were collected and constructed as the corresponding NoteAid EHR Interaction Dataset. Through a comprehensive evaluation of the entire dataset using LLM assessment and a rigorous manual evaluation of 64 instances, we showcase the potential of LLMs in patient education. Besides, the results provide valuable data support for future exploration and applications in this domain while also supplying high-quality synthetic datasets for in-house system training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2277238665",
                    "name": "Xiaocheng Zhang"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "2261455807",
                    "name": "Hong Yu"
                }
            ]
        },
        {
            "paperId": "2e783a3fe650c8eeff81cc867fa9921729422c1d",
            "title": "PaniniQA: Enhancing Patient Education Through Interactive Question Answering",
            "abstract": "Abstract A patient portal allows discharged patients to access their personalized discharge instructions in electronic health records (EHRs). However, many patients have difficulty understanding or memorizing their discharge instructions (Zhao et al., 2017). In this paper, we present PaniniQA, a patient-centric interactive question answering system designed to help patients understand their discharge instructions. PaniniQA first identifies important clinical content from patients\u2019 discharge instructions and then formulates patient-specific educational questions. In addition, PaniniQA is also equipped with answer verification functionality to provide timely feedback to correct patients\u2019 misunderstandings. Our comprehensive automatic & human evaluation results demonstrate our PaniniQA is capable of improving patients\u2019 mastery of their medical instructions through effective interactions.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153770645",
                    "name": "Pengshan Cai"
                },
                {
                    "authorId": "1576489304",
                    "name": "Zonghai Yao"
                },
                {
                    "authorId": "39853709",
                    "name": "Fei Liu"
                },
                {
                    "authorId": "2156149847",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "2232755947",
                    "name": "Meghan Reilly"
                },
                {
                    "authorId": "2184546868",
                    "name": "Huixue Zhou"
                },
                {
                    "authorId": "2232824313",
                    "name": "Lingxi Li"
                },
                {
                    "authorId": null,
                    "name": "Yi Cao"
                },
                {
                    "authorId": "38605539",
                    "name": "Alok Kapoor"
                },
                {
                    "authorId": "2546532",
                    "name": "Adarsha S. Bajracharya"
                },
                {
                    "authorId": "5450953",
                    "name": "D. Berlowitz"
                },
                {
                    "authorId": "4590999",
                    "name": "Hongfeng Yu"
                }
            ]
        }
    ]
}