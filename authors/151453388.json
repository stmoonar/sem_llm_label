{
    "authorId": "151453388",
    "papers": [
        {
            "paperId": "faffa6c51abb671f3117d94cedeb28f3631d2f6a",
            "title": "FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory",
            "abstract": "Multi-turn textual feedback-based fashion image retrieval focuses on a real-world setting, where users can iteratively provide information to refine retrieval results until they find an item that fits all their requirements. In this work, we present a novel memory-based method, called FashionNTM, for such a multi-turn system. Our framework incorporates a new Cascaded Memory Neural Turing Machine (CM-NTM) approach for implicit state management, thereby learning to integrate information across all past turns to retrieve new images, for a given turn. Unlike vanilla Neural Turing Machine (NTM), our CM-NTM operates on multiple inputs, which interact with their respective memories via individual read and write heads, to learn complex relationships. Extensive evaluation results show that our proposed method outperforms the previous state-of-the-art algorithm by 50.5%, on Multi-turn FashionIQ [60] \u2013 the only existing multi-turn fashion dataset currently, in addition to having a relative improvement of 12.6% on Multi-turn Shoes \u2013 an extension of the singleturn Shoes dataset [5] that we created in this work. Further analysis of the model in a real-world interactive setting demonstrates two important capabilities of our model \u2013 memory retention across turns, and agnosticity to turn order for non-contradictory feedback. Finally, user study results show that images retrieved by FashionNTM were favored by 83.1% over other multi-turn models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "5002401",
                    "name": "Sahil Wadhwa"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "2213162331",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "147887099",
                    "name": "Rakesh Chada"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "fbfdc403d6a9422eeec926aa4fbf16f6113efc3e",
            "title": "Household Navigation and Manipulation for Everyday Object Rearrangement Tasks",
            "abstract": "We consider the problem of building an assistive robotic system that can help humans in daily household cleanup tasks. Creating such an autonomous system in real-world environments is inherently quite challenging, as a general solution may not suit the preferences of a particular customer. Moreover, such a system consists of multi-objective tasks comprising \u2013 (i) Detection of misplaced objects and prediction of their potentially correct placements, (ii) Fine-grained manipulation for stable object grasping, and (iii) Room-to-room navigation for transferring objects in unseen environments. This work systematically tackles each component and integrates them into a complete object rearrangement pipeline. To validate our proposed system, we conduct multiple experiments on a real robotic platform involving multi-room object transfer, user preferencebased placement, and complex pick-and-place tasks. Additional details including video demonstrations of our work are available at https://sites.google.com/eng.ucsd.edu/home-robot.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151209283",
                    "name": "S. R. Iyer"
                },
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "1443760310",
                    "name": "Jiaming Hu"
                },
                {
                    "authorId": "1677260127",
                    "name": "Akanimoh Adeleye"
                },
                {
                    "authorId": "2273549601",
                    "name": "Aditya Aggarwal"
                },
                {
                    "authorId": "2247837636",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "a2f49aef59937cea5d12c2f74d7080bbb2400197",
            "title": "Role of reward shaping in object-goal navigation",
            "abstract": "Deep reinforcement learning approaches have been a popular method for visual navigation tasks in the computer vision and robotics community of late. In most cases, the reward function has a binary structure, i.e., a large positive reward is provided when the agent reaches goal state, and a negative step penalty is assigned for every other state in the environment. A sparse signal like this makes the learning process challenging, specially in big environments, where a large number of sequential actions need to be taken to reach the target. We introduce a reward shaping mechanism which gradually adjusts the reward signal based on distance to the goal. Detailed experiments conducted using the AI2-THOR simulation environment demonstrate the efficacy of the proposed approach for object-goal navigation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500525865",
                    "name": "Srirangan Madhavan"
                },
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "122da9003959d5fde1879eb55de65a6fb4da11b9",
            "title": "Learning hierarchical relationships for object-goal navigation",
            "abstract": "Direct search for objects as part of navigation poses a challenge for small items. Utilizing context in the form of object-object relationships enable hierarchical search for targets efficiently. Most of the current approaches tend to directly incorporate sensory input into a reward-based learning approach, without learning about object relationships in the natural environment, and thus generalize poorly across domains. We present Memory-utilized Joint hierarchical Object Learning for Navigation in Indoor Rooms (MJOLNIR), a target-driven navigation algorithm, which considers the inherent relationship between target objects, and the more salient contextual objects occurring in its surrounding. Extensive experiments conducted across multiple environment settings show an $82.9\\%$ and $93.5\\%$ gain over existing state-of-the-art navigation methods in terms of the success rate (SR), and success weighted by path length (SPL), respectively. We also show that our model learns to converge much faster than other algorithms, without suffering from the well-known overfitting problem. Additional details regarding the supplementary material and code are available at https://sites.google.com/eng.ucsd.edu/mjolnir.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "1563918035",
                    "name": "Yiding Qiu"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "b0afb6d66760b40054c05399e292843dc8d4ef0d",
            "title": "Learning hierarchical relationships for object-goal navigation",
            "abstract": "Direct search for objects as part of navigation poses a challenge for small items. Utilizing context in the form of object-object relationships enable hierarchical search for targets efficiently. Most of the current approaches tend to directly incorporate sensory input into a reward-based learning approach, without learning about object relationships in the natural environment, and thus generalize poorly across domains. We present Memory-utilized Joint hierarchical Object Learning for Navigation in Indoor Rooms (MJOLNIR), a target-driven navigation algorithm, which considers the inherent relationship between target objects, and the more salient contextual objects occurring in its surrounding. Extensive experiments conducted across multiple environment settings show an 82.9% and 93.5% gain over existing state-of-the-art navigation methods in terms of the success rate (SR), and success weighted by path length (SPL), respectively. We also show that our model learns to converge much faster than other algorithms, without suffering from the well-known overfitting problem. Additional details regarding the supplementary material and code are available at https://sites.google.com/eng.ucsd.edu/mjolnir",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1563918035",
                    "name": "Yiding Qiu"
                },
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "1723059",
                    "name": "H. Christensen"
                }
            ]
        },
        {
            "paperId": "03f57e371589711f87fde60f085e544894eaf135",
            "title": "\u201cLooking at the Right Stuff\u201d \u2013 Guided Semantic-Gaze for Autonomous Driving",
            "abstract": "In recent years, predicting driver's focus of attention has been a very active area of research in the autonomous driving community. Unfortunately, existing state-of-the-art techniques achieve this by relying only on human gaze information, thereby ignoring scene semantics. We propose a novel Semantics Augmented GazE (SAGE) detection approach that captures driving specific contextual information, in addition to the raw gaze. Such a combined attention mechanism serves as a powerful tool to focus on the relevant regions in an image frame in order to make driving both safe and efficient. Using this, we design a complete saliency prediction framework - SAGE-Net, which modifies the initial prediction from SAGE by taking into account vital aspects such as distance to objects (depth), ego vehicle speed, and pedestrian crossing intent. Exhaustive experiments conducted through four popular saliency algorithms show that on 49/56 (87.5%) cases - considering both the overall dataset and crucial driving scenarios, SAGE outperforms existing techniques without any additional computational overhead during the training process. The augmented dataset along with the relevant code are available as part of the supplementary material.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "2057125723",
                    "name": "S. Mondal"
                },
                {
                    "authorId": "1723059",
                    "name": "H. Christensen"
                }
            ]
        },
        {
            "paperId": "621842b8736c34c54e9e17c0fb3f6a39bd467cf3",
            "title": "DEDUCE: Diverse scEne Detection methods in Unseen Challenging Environments",
            "abstract": "In recent years, there has been a rapid increase in the number of service robots deployed for aiding people in their daily activities. Unfortunately, most of these robots require human input for training in order to do tasks in indoor environments. Successful domestic navigation often requires access to semantic information about the environment, which can be learned without human guidance. In this paper, we propose a set of DEDUCE1 -Diverse scEne Detection methods in Unseen Challenging Environments algorithms which incorporate deep fusion models derived from scene recognition systems and object detectors. The five methods described here have been evaluated on several popular recent image datasets, as well as real-world videos acquired through multiple mobile platforms. The final results show an improvement over the existing state-of-the-art visual place recognition systems.1Supplementary material including code and the videos of the different experiments are available at https://sites.google.com/eng.ucsd.edu/deduce.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "1403028285",
                    "name": "Carlos Nieto-Granda"
                },
                {
                    "authorId": "1723059",
                    "name": "H. Christensen"
                }
            ]
        },
        {
            "paperId": "64833865f17500c8d087238e78b5dbc3cfa57a8a",
            "title": "Learning Application-Oriented Classifiers for Multi-frame Visual Recognition",
            "abstract": "Author(s): Pal, Anwesan | Advisor(s): Christensen, Henrik Iskov | Abstract: Classification, a \\textit{supervised learning} problem, is a technique to categorize a given set of data points into two (binary classification) or more (multi-class classification) targets or labels, based on their characteristics. It has wide applications in the domain of Computer Vision and Robotics. Despite the tremendous success of classification in recent times, a fundamental question remains as to whether a classifier truly learns the correct representation (\\ie the \\textit{ground-truth representation}) needed to solve a problem. The uncertainty in learning often limits generalization of such methods across different domains. This thesis presents an in-depth study of how different classifiers leverage the intrinsic properties of a dataset in order to robustly model the data distribution. The study further extends to the application of classifiers in two challenging domains - Visual Semantic Place Categorization and Human Activity Recognition.The first part of the thesis focuses on the concept of bias in action recognition videos. This is illustrated by the fact that given just a single frame of a video, some actions can be easily identified, due to the presence of certain objects and the background. The second part generalizes classification into the domain of place categorization, where the focus is to learn semantic information about a robot's environment without any human guidance. This is done through a deep fusion of the scene and object attributes. Finally, an algorithm is proposed for human action recognition which incorporates the information learnt from optical flow and visual attention mechanism in order to perform classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                }
            ]
        }
    ]
}