{
    "authorId": "1719250",
    "papers": [
        {
            "paperId": "010fff1f8a3cc3fdab0bfd68bf4d84a18892c888",
            "title": "Under-Display Camera Image Enhancement via Cascaded Curve Estimation",
            "abstract": "The new trend of full-screen devices encourages manufacturers to position a camera behind a screen, i.e., the newly-defined Under-Display Camera (UDC). Therefore, UDC image restoration has been a new realistic single image enhancement problem. In this work, we propose a curve estimation network operating on the hue (H) and saturation (S) channels to perform adaptive enhancement for degraded images captured by UDCs. The proposed network aims to match the complicated relationship between the images captured by under-display and display-free cameras. To extract effective features, we cascade the proposed curve estimation network with sharing weights, and we introduce a spatial and channel attention module in each curve estimation network to exploit attention-aware features. In addition, we learn the curve estimation network in a semi-supervised manner to alleviate the restriction of the requirement for amounts of labeled images and improve the generalization ability for unseen degraded images in various realistic scenes. The semi-supervised network consists of a supervised branch trained on labeled data and an unsupervised branch trained on unlabeled data. To train the proposed model, we build a new dataset comprised of real-world labeled and unlabeled images. Extensive experiments demonstrate that our proposed algorithm performs favorably against state-of-the-art image enhancement methods for UDC images in terms of accuracy and speed, especially on ultra-high-definition (UHD) images.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2141104652",
                    "name": "Jun Luo"
                },
                {
                    "authorId": "144850642",
                    "name": "Wenqi Ren"
                },
                {
                    "authorId": "41154933",
                    "name": "Tao Wang"
                },
                {
                    "authorId": "2698424",
                    "name": "Chongyi Li"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "05fb52fff55b6fa4b7994979ab86c4f5511012e9",
            "title": "Accurate Scene Text Detection Via Scale-Aware Data Augmentation and Shape Similarity Constraint",
            "abstract": "Scene text detection has attracted increasing concerns with the rapid development of deep neural networks in recent years. However, existing scene text detectors may overfit on the public datasets due to the limited training data, or generate inaccurate localization for arbitrary-shape scene texts. This paper presents an arbitrary-shape scene text detection method that can achieve better generalization ability and more accurate localization. We first propose a Scale-Aware Data Augmentation (SADA) technique to increase the diversity of training samples. SADA considers the scale variations and local visual variations of scene texts, which can effectively relieve the dilemma of limited training data. At the same time, SADA can enrich the training minibatch, which contributes to accelerating the training process. Furthermore, a Shape Similarity Constraint (SSC) technique is exploited to model the global shape structure of arbitrary-shape scene texts and backgrounds from the perspective of the loss function. SSC encourages the segmentation of text or non-text in the candidate boxes to be similar to the corresponding ground truth, which is helpful to localize more accurate boundaries for arbitrary-shape scene texts. Extensive experiments have demonstrated the effectiveness of the proposed techniques, and state-of-the-art performances are achieved over public arbitrary-shape scene text benchmarks (e.g., CTW1500, Total-Text and ArT).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35495931",
                    "name": "Pengwen Dai"
                },
                {
                    "authorId": "2283489024",
                    "name": "Yang Li"
                },
                {
                    "authorId": "2108906565",
                    "name": "Hua Zhang"
                },
                {
                    "authorId": "2109624802",
                    "name": "Jingzhi Li"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "0925976ac12284380d6242ea38fe8eef11272a4a",
            "title": "Multiple Adverse Weather Conditions Adaptation for Object Detection via Causal Intervention",
            "abstract": "Most state-of-the-art object detection methods have achieved impressive perfomrace on several public benchmarks, which are trained with high definition images. However, existing detectors are often sensitive to the visual variations and out-of-distribution data due to the domain gap caused by various confounders, e.g. the adverse weathre conditions. To bridge the gap, previous methods have been mainly exploring domain alignment, which requires to collect an amount of domain-specific training samples. In this paper, we introduce a novel domain adaptation model to discover a weather condition invariant feature representation. Specifically, we first employ a memory network to develop a confounder dictionary, which stores prototypes of object features under various scenarios. To guarantee the representativeness of each prototype in the dictionary, a dynamic item extraction strategy is used to update the memory dictionary. After that, we introduce a causal intervention reasoning module to explore the invariant representation of a specific object under different weather conditions. Finally, a categorical consistency regularization is used to constrain the similarities between categories in order to automatically search for the aligned instances among distinct domains. Experiments are conducted on several public benchmarks (RTTS, Foggy-Cityscapes, RID, and BDD 100K) with state-of-the-art performance achieved under multiple weather conditions.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108906565",
                    "name": "Hua Zhang"
                },
                {
                    "authorId": "22262032",
                    "name": "Liqiang Xiao"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "1691260",
                    "name": "H. Foroosh"
                }
            ]
        },
        {
            "paperId": "0ecc853059efce862451175e99915600bfc15791",
            "title": "Video Super-Resolution via a Spatio-Temporal Alignment Network",
            "abstract": "Deep convolutional neural network based video super-resolution (SR) models have achieved significant progress in recent years. Existing deep video SR methods usually impose optical flow to wrap the neighboring frames for temporal alignment. However, accurate estimation of optical flow is quite difficult, which tends to produce artifacts in the super-resolved results. To address this problem, we propose a novel end-to-end deep convolutional network that dynamically generates the spatially adaptive filters for the alignment, which are constituted by the local spatio-temporal channels of each pixel. Our method avoids generating explicit motion compensation and utilizes spatio-temporal adaptive filters to achieve the operation of alignment, which effectively fuses the multi-frame information and improves the temporal consistency of the video. Capitalizing on the proposed adaptive filter, we develop a reconstruction network and take the aligned frames as input to restore the high-resolution frames. In addition, we employ residual modules embedded with channel attention as the basic unit to extract more informative features for video SR. Both quantitative and qualitative evaluation results on three public video datasets demonstrate that the proposed method performs favorably against state-of-the-art super-resolution methods in terms of clearness and texture details.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1896324994",
                    "name": "Weilei Wen"
                },
                {
                    "authorId": "144850642",
                    "name": "Wenqi Ren"
                },
                {
                    "authorId": "2475959",
                    "name": "Yinghuan Shi"
                },
                {
                    "authorId": "36234328",
                    "name": "Yunfeng Nie"
                },
                {
                    "authorId": "47538816",
                    "name": "Jingang Zhang"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "1163c29ae6a26b1cd8d85280740410e99001a959",
            "title": "Self-Supervised Graph Neural Networks via Diverse and Interactive Message Passing",
            "abstract": "By interpreting Graph Neural Networks (GNNs) as the message passing from the spatial perspective, their success is attributed to Laplacian smoothing. However, it also leads to serious over-smoothing issue by stacking many layers. Recently, many efforts have been paid to overcome this issue in semi-supervised learning. Unfortunately, it is more serious in unsupervised node representation learning task due to the lack of supervision information. Thus, most of the unsupervised or self-supervised GNNs often employ \\textit{one-layer GCN} as the encoder. Essentially, the over-smoothing issue is caused by the over-simplification of the existing message passing, which possesses two intrinsic limits: blind message and uniform passing. In this paper, a novel Diverse and Interactive Message Passing (DIMP) is proposed for self-supervised learning by overcoming these limits. Firstly, to prevent the message from blindness and make it interactive between two connected nodes, the message is determined by both the two connected nodes instead of the attributes of one node. Secondly, to prevent the passing from uniformness and make it diverse over different attribute channels, different propagation weights are assigned to different elements in the message. To this end, a natural implementation of the message in DIMP is the element-wise product of the representations of two connected nodes. From the perspective of numerical optimization, the proposed DIMP is equivalent to performing an overlapping community detection via expectation-maximization (EM). Both the objective function of the community detection and the convergence of EM algorithm guarantee that DMIP can prevent from over-smoothing issue. Extensive evaluations on node-level and graph-level tasks demonstrate the superiority of DIMP on improving performance and overcoming over-smoothing issue.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2145775206",
                    "name": "Cheng Chen"
                },
                {
                    "authorId": "1409858224",
                    "name": "Weixun Li"
                },
                {
                    "authorId": "2661834",
                    "name": "Bingxin Niu"
                },
                {
                    "authorId": "6413344",
                    "name": "Junhua Gu"
                },
                {
                    "authorId": "2109150586",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "20ef611864dc8f7f51e089105b4cb7c5de2947e0",
            "title": "Adversarial Multiview Clustering Networks With Adaptive Fusion",
            "abstract": "The existing deep multiview clustering (MVC) methods are mainly based on autoencoder networks, which seek common latent variables to reconstruct the original input of each view individually. However, due to the view-specific reconstruction loss, it is challenging to extract consistent latent representations over multiple views for clustering. To address this challenge, we propose adversarial MVC (AMvC) networks in this article. The proposed AMvC generates each view\u2019s samples conditioning on the fused latent representations among different views to encourage a more consistent clustering structure. Specifically, multiview encoders are used to extract latent descriptions from all the views, and the corresponding generators are used to generate the reconstructed samples. The discriminative networks and the mean squared loss are jointly utilized for training the multiview encoders and generators to balance the distinctness and consistency of each view\u2019s latent representation. Moreover, an adaptive fusion layer is developed to obtain a shared latent representation, on which a clustering loss and the ${\\ell _ {1,2}}$ -norm constraint are further imposed to improve clustering performance and distinguish the latent space. Experimental results on video, image, and text datasets demonstrate that the effectiveness of our AMvC is over several state-of-the-art deep MVC methods.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109116613",
                    "name": "Qianqian Wang"
                },
                {
                    "authorId": "6018169",
                    "name": "Zhiqiang Tao"
                },
                {
                    "authorId": "100681690",
                    "name": "Wei Xia"
                },
                {
                    "authorId": "38469552",
                    "name": "Quanxue Gao"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "2143819911",
                    "name": "Licheng Jiao"
                }
            ]
        },
        {
            "paperId": "67e9f2b770794105383a4c09b507eb0b9bd9618e",
            "title": "A Survey of Deep Face Restoration: Denoise, Super-Resolution, Deblur, Artifact Removal",
            "abstract": "Face Restoration (FR) aims to restore High-Quality (HQ) faces from Low-Quality (LQ) input images, which is a domain-specific image restoration problem in the low-level computer vision area. The early face restoration methods mainly use statistic priors and degradation models, which are difficult to meet the requirements of real-world applications in practice. In recent years, face restoration has witnessed great progress after stepping into the deep learning era. However, there are few works to study deep learning-based face restoration methods systematically. Thus, this paper comprehensively surveys recent advances in deep learning techniques for face restoration. Specifically, we first summarize different problem formulations and analyze the characteristic of the face image. Second, we discuss the challenges of face restoration. Concerning these challenges, we present a comprehensive review of existing FR methods, including prior based methods and deep learning-based methods. Then, we explore developed techniques in the task of FR covering network architectures, loss functions, and benchmark datasets. We also conduct a systematic benchmark evaluation on representative methods. Finally, we discuss future directions, including network designs, metrics, benchmark datasets, applications,etc. We also provide an open-source repository for all the discussed methods, which is available at https://github.com/TaoWangzj/Awesome-Face-Restoration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156632570",
                    "name": "Tao Wang"
                },
                {
                    "authorId": "3397429",
                    "name": "Kaihao Zhang"
                },
                {
                    "authorId": "2108968061",
                    "name": "Xuanxi Chen"
                },
                {
                    "authorId": "145909988",
                    "name": "Wenhan Luo"
                },
                {
                    "authorId": "3234063",
                    "name": "Jiankang Deng"
                },
                {
                    "authorId": "2115137018",
                    "name": "Tong Lu"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "49663287",
                    "name": "Wei Liu"
                },
                {
                    "authorId": "2144400863",
                    "name": "Hongdong Li"
                },
                {
                    "authorId": "1776444",
                    "name": "S. Zafeiriou"
                }
            ]
        },
        {
            "paperId": "6ae8faa6a5a1bce6d4e986405825d605eb6a6db7",
            "title": "A Principled Design of Image Representation: Towards Forensic Tasks",
            "abstract": "Image forensics is a rising topic as the trustworthy multimedia content is critical for modern society. Like other vision-related applications, forensic analysis relies heavily on the proper image representation. Despite the importance, current theoretical understanding for such representation remains limited, with varying degrees of neglect for its key role. For this gap, we attempt to investigate the forensic-oriented image representation as a distinct problem, from the perspectives of theory, implementation, and application. Our work starts from the abstraction of basic principles that the representation for forensics should satisfy, especially revealing the criticality of robustness, interpretability, and coverage. At the theoretical level, we propose a new representation framework for forensics, called dense invariant representation (DIR), which is characterized by stable description with mathematical guarantees. At the implementation level, the discrete calculation problems of DIR are discussed, and the corresponding accurate and fast solutions are designed with generic nature and constant complexity. We demonstrate the above arguments on the dense-domain pattern detection and matching experiments, providing comparison results with state-of-the-art descriptors. Also, at the application level, the proposed DIR is initially explored in passive and active forensics, namely copy-move forgery detection and perceptual hashing, exhibiting the benefits in fulfilling the requirements of such forensic tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "153068904",
                    "name": "Shuren Qi"
                },
                {
                    "authorId": "2108051942",
                    "name": "Yushu Zhang"
                },
                {
                    "authorId": "2144449265",
                    "name": "Chao Wang"
                },
                {
                    "authorId": "1735685",
                    "name": "Jiantao Zhou"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "79b29bc682c2a2f39079dc8f9d90875eb3f56234",
            "title": "ADT-SSL: Adaptive Dual-Threshold for Semi-Supervised Learning",
            "abstract": "Semi-Supervised Learning (SSL) has advanced classi\ufb01cation tasks by inputting both labeled and unlabeled data to train a model jointly. However, existing SSL methods only consider the unlabeled data whose predictions are beyond a \ufb01xed threshold (e.g., 0.95), ignoring the valuable information from those less than 0.95. We argue that these discarded data have a large proportion and are usually of hard samples, thereby bene\ufb01ting the model training. This paper proposes an Adaptive Dual-Threshold method for Semi-Supervised Learning (ADT-SSL). Except for the \ufb01xed threshold, ADT extracts another class-adaptive threshold from the labeled data to take full advantage of the unlabeled data whose predictions are less than 0.95 but more than the extracted one. Accordingly, we engage CE and L 2 loss functions to learn from these two types of unlabeled data, respectively. For highly similar unlabeled data, we further design a novel similar loss to make the prediction of the model consistency. Extensive experiments are conducted on benchmark datasets, including CIFAR-10, CIFAR-100, and SVHN. Experimental results show that the proposed ADT-SSL achieves state-of-the-art classi\ufb01cation accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1400186047",
                    "name": "Zechen Liang"
                },
                {
                    "authorId": "2164352535",
                    "name": "Yuansheng Wang"
                },
                {
                    "authorId": "143844110",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "7ddadfd99d76d225acc0f813d8de0b3c5e7d38d6",
            "title": "Optimizing Two-Way Partial AUC With an End-to-End Framework",
            "abstract": "The Area Under the ROC Curve (AUC) is a crucial metric for machine learning, which evaluates the average performance over all possible True Positive Rates (TPRs) and False Positive Rates (FPRs). Based on the knowledge that a skillful classifier should simultaneously embrace a high TPR and a low FPR, we turn to study a more general variant called Two-way Partial AUC (TPAUC), where only the region with <inline-formula><tex-math notation=\"LaTeX\">$\\mathsf {TPR} \\geq \\alpha, \\mathsf {FPR} \\leq \\beta$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"sans-serif\">TPR</mml:mi><mml:mo>\u2265</mml:mo><mml:mi>\u03b1</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"sans-serif\">FPR</mml:mi><mml:mo>\u2264</mml:mo><mml:mi>\u03b2</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"yang-ieq1-3185311.gif\"/></alternatives></inline-formula> is included in the area. Moreover, a recent work shows that the TPAUC is essentially inconsistent with the existing Partial AUC metrics where only the FPR range is restricted, opening a new problem to seek solutions to leverage high TPAUC. Motivated by this, we present the first trial in this article to optimize this new metric. The critical challenge along this course lies in the difficulty of performing gradient-based optimization with end-to-end stochastic training, even with a proper choice of surrogate loss. To address this issue, we propose a generic framework to construct surrogate optimization problems, which supports efficient end-to-end training with deep learning. Moreover, our theoretical analyses show that: 1) the objective function of the surrogate problems will achieve an upper bound of the original problem under mild conditions, and 2) optimizing the surrogate problems leads to good generalization performance in terms of TPAUC with a high probability. Finally, empirical studies over several benchmark datasets speak to the efficacy of our framework.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "16546250",
                    "name": "Zhiyong Yang"
                },
                {
                    "authorId": "34679664",
                    "name": "Qianqian Xu"
                },
                {
                    "authorId": "3172846",
                    "name": "Shilong Bao"
                },
                {
                    "authorId": "143605211",
                    "name": "Yuan He"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "1689702",
                    "name": "Qingming Huang"
                }
            ]
        }
    ]
}