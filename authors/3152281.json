{
    "authorId": "3152281",
    "papers": [
        {
            "paperId": "3328f39a27ab130647e26dc243dab29a13d434e6",
            "title": "OVR: A Dataset for Open Vocabulary Temporal Repetition Counting in Videos",
            "abstract": "We introduce a dataset of annotations of temporal repetitions in videos. The dataset, OVR (pronounced as over), contains annotations for over 72K videos, with each annotation specifying the number of repetitions, the start and end time of the repetitions, and also a free-form description of what is repeating. The annotations are provided for videos sourced from Kinetics and Ego4D, and consequently cover both Exo and Ego viewing conditions, with a huge variety of actions and activities. Moreover, OVR is almost an order of magnitude larger than previous datasets for video repetition. We also propose a baseline transformer-based counting model, OVRCounter, that can localise and count repetitions in videos that are up to 320 frames long. The model is trained and evaluated on the OVR dataset, and its performance assessed with and without using text to specify the target class to count. The performance is also compared to a prior repetition counting model. The dataset is available for download at: https://sites.google.com/view/openvocabreps/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2420123",
                    "name": "Debidatta Dwibedi"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "2704494",
                    "name": "Jonathan Tompson"
                },
                {
                    "authorId": "2252432062",
                    "name": "A. Zisserman"
                }
            ]
        },
        {
            "paperId": "9b9a0e6b341ffc2516a41bbeafc11270ca18d092",
            "title": "FlexCap: Generating Rich, Localized, and Flexible Captions in Images",
            "abstract": "We introduce a versatile $\\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications. First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQA datasets. We also demonstrate a $\\textit{localize-then-describe}$ approach with FlexCap can be better at open-ended object detection than a $\\textit{describe-then-localize}$ approach with other VLMs. We highlight a novel characteristic of FlexCap, which is its ability to extract diverse visual information through prefix conditioning. Finally, we qualitatively demonstrate FlexCap's broad applicability in tasks such as image labeling, object attribute recognition, and visual dialog. Project webpage: https://flex-cap.github.io .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2420123",
                    "name": "Debidatta Dwibedi"
                },
                {
                    "authorId": "2253472236",
                    "name": "Vidhi Jain"
                },
                {
                    "authorId": "2704494",
                    "name": "Jonathan Tompson"
                },
                {
                    "authorId": "2252432062",
                    "name": "A. Zisserman"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                }
            ]
        },
        {
            "paperId": "c064de2c71ebc5cf05493f49dc312b033c36b3b9",
            "title": "Genie: Generative Interactive Environments",
            "abstract": "We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2286344472",
                    "name": "Jake Bruce"
                },
                {
                    "authorId": "2286305249",
                    "name": "Michael D. Dennis"
                },
                {
                    "authorId": "2268732203",
                    "name": "Ashley Edwards"
                },
                {
                    "authorId": "1410302742",
                    "name": "Jack Parker-Holder"
                },
                {
                    "authorId": "2286668292",
                    "name": "Yuge Shi"
                },
                {
                    "authorId": "2286317624",
                    "name": "Edward Hughes"
                },
                {
                    "authorId": "2286308060",
                    "name": "Matthew Lai"
                },
                {
                    "authorId": "38792754",
                    "name": "Aditi Mavalankar"
                },
                {
                    "authorId": "2188779988",
                    "name": "Richie Steigerwald"
                },
                {
                    "authorId": "2293394525",
                    "name": "Chris Apps"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "33931249",
                    "name": "Sarah Bechtle"
                },
                {
                    "authorId": "145124447",
                    "name": "Feryal M. P. Behbahani"
                },
                {
                    "authorId": "2297673118",
                    "name": "Stephanie Chan"
                },
                {
                    "authorId": "2801204",
                    "name": "N. Heess"
                },
                {
                    "authorId": "2287795688",
                    "name": "Lucy Gonzalez"
                },
                {
                    "authorId": "2217144",
                    "name": "Simon Osindero"
                },
                {
                    "authorId": "1955694",
                    "name": "Sherjil Ozair"
                },
                {
                    "authorId": "2286310846",
                    "name": "Scott Reed"
                },
                {
                    "authorId": "2240716003",
                    "name": "Jingwei Zhang"
                },
                {
                    "authorId": "7912420",
                    "name": "Konrad Zolna"
                },
                {
                    "authorId": "2263705770",
                    "name": "Jeff Clune"
                },
                {
                    "authorId": "1737568",
                    "name": "Nando de Freitas"
                },
                {
                    "authorId": "2286734040",
                    "name": "Satinder Singh"
                },
                {
                    "authorId": "1389854357",
                    "name": "Tim Rocktaschel"
                }
            ]
        },
        {
            "paperId": "016315a3df05cb07a6f67fa5f6a3265b55909644",
            "title": "Lossless Adaptation of Pretrained Vision Models For Robotic Manipulation",
            "abstract": "Recent works have shown that large models pretrained on common visual learning tasks can provide useful representations for a wide range of specialized perception problems, as well as a variety of robotic manipulation tasks. While prior work on robotic manipulation has predominantly used frozen pretrained features, we demonstrate that in robotics this approach can fail to reach optimal performance, and that fine-tuning of the full model can lead to significantly better results. Unfortunately, fine-tuning disrupts the pretrained visual representation, and causes representational drift towards the fine-tuned task thus leading to a loss of the versatility of the original model. We introduce\"lossless adaptation\"to address this shortcoming of classical fine-tuning. We demonstrate that appropriate placement of our parameter efficient adapters can significantly reduce the performance gap between frozen pretrained representations and full end-to-end fine-tuning without changes to the original representation and thus preserving original capabilities of the pretrained model. We perform a comprehensive investigation across three major model architectures (ViTs, NFNets, and ResNets), supervised (ImageNet-1K classification) and self-supervised pretrained weights (CLIP, BYOL, Visual MAE) in 3 task domains and 35 individual tasks, and demonstrate that our claims are strongly validated in various settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145467103",
                    "name": "Mohit Sharma"
                },
                {
                    "authorId": "34632129",
                    "name": "C. Fantacci"
                },
                {
                    "authorId": "2145925767",
                    "name": "Yuxiang Zhou"
                },
                {
                    "authorId": "35217389",
                    "name": "Skanda Koppula"
                },
                {
                    "authorId": "2801204",
                    "name": "N. Heess"
                },
                {
                    "authorId": "36881095",
                    "name": "Jonathan Scholz"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                }
            ]
        },
        {
            "paperId": "2562fe379554d201aad312f786903f4c60b68acf",
            "title": "RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation",
            "abstract": "The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100-1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2732737",
                    "name": "Konstantinos Bousmalis"
                },
                {
                    "authorId": "3433312",
                    "name": "G. Vezzani"
                },
                {
                    "authorId": "143668237",
                    "name": "Dushyant Rao"
                },
                {
                    "authorId": "144373380",
                    "name": "Coline Devin"
                },
                {
                    "authorId": "49250083",
                    "name": "Alex X. Lee"
                },
                {
                    "authorId": "93223375",
                    "name": "Maria Bauz\u00e1"
                },
                {
                    "authorId": "121676884",
                    "name": "Todor Davchev"
                },
                {
                    "authorId": "2145925767",
                    "name": "Yuxiang Zhou"
                },
                {
                    "authorId": "25445698",
                    "name": "Agrim Gupta"
                },
                {
                    "authorId": "41211511",
                    "name": "A. Raju"
                },
                {
                    "authorId": "66850418",
                    "name": "Antoine Laurens"
                },
                {
                    "authorId": "34632129",
                    "name": "C. Fantacci"
                },
                {
                    "authorId": "2795508",
                    "name": "Valentin Dalibard"
                },
                {
                    "authorId": "7455600",
                    "name": "Martina Zambelli"
                },
                {
                    "authorId": "145279513",
                    "name": "M. Martins"
                },
                {
                    "authorId": "1976157345",
                    "name": "Rugile Pevceviciute"
                },
                {
                    "authorId": "14564467",
                    "name": "M. Blokzijl"
                },
                {
                    "authorId": "1715051",
                    "name": "Misha Denil"
                },
                {
                    "authorId": "2150504360",
                    "name": "Nathan Batchelor"
                },
                {
                    "authorId": "2066153554",
                    "name": "Thomas Lampe"
                },
                {
                    "authorId": "3166516",
                    "name": "Emilio Parisotto"
                },
                {
                    "authorId": "7912420",
                    "name": "Konrad Zolna"
                },
                {
                    "authorId": "144828948",
                    "name": "Scott E. Reed"
                },
                {
                    "authorId": "2016840",
                    "name": "Sergio Gomez Colmenarejo"
                },
                {
                    "authorId": "36881095",
                    "name": "Jonathan Scholz"
                },
                {
                    "authorId": "2799799",
                    "name": "A. Abdolmaleki"
                },
                {
                    "authorId": "50499889",
                    "name": "O. Groth"
                },
                {
                    "authorId": "46219906",
                    "name": "Jean-Baptiste Regli"
                },
                {
                    "authorId": "2545660",
                    "name": "Oleg O. Sushkov"
                },
                {
                    "authorId": "2220302388",
                    "name": "Tom Rothorl"
                },
                {
                    "authorId": "2135278508",
                    "name": "Jos\u00e9 Enrique Chen"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "2057181585",
                    "name": "David Barker"
                },
                {
                    "authorId": "2220302984",
                    "name": "Joy Ortiz"
                },
                {
                    "authorId": "3137672",
                    "name": "Martin A. Riedmiller"
                },
                {
                    "authorId": "2060551",
                    "name": "Jost Tobias Springenberg"
                },
                {
                    "authorId": "2315504",
                    "name": "R. Hadsell"
                },
                {
                    "authorId": "1692768",
                    "name": "F. Nori"
                },
                {
                    "authorId": "2801204",
                    "name": "N. Heess"
                }
            ]
        },
        {
            "paperId": "2837f5ef4b3bc5f790fd2a9f43895f6000870cce",
            "title": "RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation",
            "abstract": "For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46196063",
                    "name": "Mel Vecer\u00edk"
                },
                {
                    "authorId": "2786693",
                    "name": "Carl Doersch"
                },
                {
                    "authorId": "2143686965",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "121676884",
                    "name": "Todor Davchev"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "50282575",
                    "name": "Guangyao Zhou"
                },
                {
                    "authorId": "2315504",
                    "name": "R. Hadsell"
                },
                {
                    "authorId": "3377447",
                    "name": "L. Agapito"
                },
                {
                    "authorId": "36881095",
                    "name": "Jonathan Scholz"
                }
            ]
        },
        {
            "paperId": "42585728685d7b2567e62b03463d1520f1bbe47e",
            "title": "Perception Test: A Diagnostic Benchmark for Multimodal Video Models",
            "abstract": "We propose a novel multimodal video benchmark - the Perception Test - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, SeViLA, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object and point tracks, temporal action and sound segments), enabling both language and non-language evaluations. The fine-tuning and validation splits of the benchmark are publicly available (CC-BY license), in addition to a challenge server with a held-out test split. Human baseline results compared to state-of-the-art video QA models show a substantial gap in performance (91.4% vs 46.2%), suggesting that there is significant room for improvement in multimodal video understanding. Dataset, baseline code, and challenge server are available at https://github.com/deepmind/perception_test",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218430942",
                    "name": "Viorica Puatruaucean"
                },
                {
                    "authorId": "1466466597",
                    "name": "Lucas Smaira"
                },
                {
                    "authorId": "2110759501",
                    "name": "Ankush Gupta"
                },
                {
                    "authorId": "2078462206",
                    "name": "Adri\u00e0 Recasens Continente"
                },
                {
                    "authorId": "72361999",
                    "name": "L. Markeeva"
                },
                {
                    "authorId": "32689129",
                    "name": "Dylan Banarse"
                },
                {
                    "authorId": "35217389",
                    "name": "Skanda Koppula"
                },
                {
                    "authorId": "69425681",
                    "name": "Joseph Heyward"
                },
                {
                    "authorId": "145478807",
                    "name": "Mateusz Malinowski"
                },
                {
                    "authorId": "7607499",
                    "name": "Yezhou Yang"
                },
                {
                    "authorId": "2786693",
                    "name": "Carl Doersch"
                },
                {
                    "authorId": "2166868706",
                    "name": "Tatiana Matejovicova"
                },
                {
                    "authorId": "1390139201",
                    "name": "Yury Sulsky"
                },
                {
                    "authorId": "19200186",
                    "name": "Antoine Miech"
                },
                {
                    "authorId": "80501158",
                    "name": "A. Fr\u00e9chette"
                },
                {
                    "authorId": "2051744127",
                    "name": "H. Klimczak"
                },
                {
                    "authorId": "50812848",
                    "name": "R. Koster"
                },
                {
                    "authorId": "2048571",
                    "name": "Junlin Zhang"
                },
                {
                    "authorId": "2218062983",
                    "name": "Stephanie Winkler"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "2217144",
                    "name": "Simon Osindero"
                },
                {
                    "authorId": "145089978",
                    "name": "D. Damen"
                },
                {
                    "authorId": "1688869",
                    "name": "Andrew Zisserman"
                },
                {
                    "authorId": "35681810",
                    "name": "Jo\u00e3o Carreira"
                }
            ]
        },
        {
            "paperId": "8a5cb7c72f27c97d9dd7b79a0efb0261bf12f7fa",
            "title": "Learning from One Continuous Video Stream",
            "abstract": "We introduce a framework for online learning from a single continuous video stream-the way people and animals learn, without mini-batches, data augmentation or shuffling. This poses great challenges given the high correlation between consecutive video frames and there is very little prior work on it. Our framework allows us to do a first deep dive into the topic and includes a collection of streams and tasks composed from two existing video datasets, plus methodology for performance evaluation that considers both adaptation and generalization. We employ pixel-to-pixel modelling as a practical and flexible way to switch between pre-training and single-stream evaluation as well as between arbitrary tasks, without ever requiring changes to models and always using the same pixel loss. Equipped with this framework we obtained large single-stream learning gains from pre-training with a novel family of future prediction tasks, found that momentum hurts, and that the pace of weight updates matters. The combination of these insights leads to matching the performance of IID learning with batch size 1, when using the same architecture and without costly replay buffers. An overview of the pa-per is available online at https://sites.google.com/view/one-stream-video.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257349317",
                    "name": "Jo\u00e3o Carreira"
                },
                {
                    "authorId": "2269226002",
                    "name": "Michael King"
                },
                {
                    "authorId": "1756112",
                    "name": "Viorica Patraucean"
                },
                {
                    "authorId": "81367191",
                    "name": "Dilara Gokay"
                },
                {
                    "authorId": "2273228",
                    "name": "Catalin Ionescu"
                },
                {
                    "authorId": "2269177160",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "2269145047",
                    "name": "Daniel Zoran"
                },
                {
                    "authorId": "69425681",
                    "name": "Joseph Heyward"
                },
                {
                    "authorId": "2786693",
                    "name": "Carl Doersch"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "145089978",
                    "name": "D. Damen"
                },
                {
                    "authorId": "2252432062",
                    "name": "A. Zisserman"
                }
            ]
        },
        {
            "paperId": "fc98d0e981f193b159d8fab5343f094cf06dcf20",
            "title": "TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement",
            "abstract": "We present a novel model for Tracking Any Point (TAP) that effectively tracks any queried point on any physical surface throughout a video sequence. Our approach employs two stages: (1) a matching stage, which independently locates a suitable candidate point match for the query point on every other frame, and (2) a refinement stage, which updates both the trajectory and query features based on local correlations. The resulting model surpasses all baseline methods by a significant margin on the TAP-Vid benchmark, as demonstrated by an approximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our model facilitates fast inference on long and high-resolution video sequences. On a modern GPU, our implementation has the capacity to track points faster than real-time. Given the high-quality trajectories extracted from a large dataset, we demonstrate a proof-of-concept diffusion model which generates trajectories from static images, enabling plausible animations. Visualizations, source code, and pretrained models can be found at https://deepmind-tapir.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2786693",
                    "name": "Carl Doersch"
                },
                {
                    "authorId": "2143686965",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "46196063",
                    "name": "Mel Vecer\u00edk"
                },
                {
                    "authorId": "81367191",
                    "name": "Dilara Gokay"
                },
                {
                    "authorId": "2110759501",
                    "name": "Ankush Gupta"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "35681810",
                    "name": "Jo\u00e3o Carreira"
                },
                {
                    "authorId": "1688869",
                    "name": "Andrew Zisserman"
                }
            ]
        },
        {
            "paperId": "d81c7c08765add0fd478aa3d6b3a9f4b7c1003a8",
            "title": "TAP-Vid: A Benchmark for Tracking Any Point in a Video",
            "abstract": "Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move. This information is useful to make inferences about 3D shape, physical properties and object interactions. While the problem of tracking arbitrary physical points on surfaces over longer video clips has received some attention, no dataset or benchmark for evaluation existed, until now. In this paper, we first formalize the problem, naming it tracking any point (TAP). We introduce a companion benchmark, TAP-Vid, which is composed of both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. Central to the construction of our benchmark is a novel semi-automatic crowdsourced pipeline which uses optical flow estimates to compensate for easier, short-term motion like camera shake, allowing annotators to focus on harder sections of video. We validate our pipeline on synthetic data and propose a simple end-to-end point tracking model TAP-Net, showing that it outperforms all prior methods on our benchmark when trained on synthetic data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2786693",
                    "name": "Carl Doersch"
                },
                {
                    "authorId": "2110759501",
                    "name": "Ankush Gupta"
                },
                {
                    "authorId": "72361999",
                    "name": "L. Markeeva"
                },
                {
                    "authorId": "39257069",
                    "name": "Adri\u00e0 Recasens"
                },
                {
                    "authorId": "1466466597",
                    "name": "Lucas Smaira"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "35681810",
                    "name": "Jo\u00e3o Carreira"
                },
                {
                    "authorId": "1688869",
                    "name": "Andrew Zisserman"
                },
                {
                    "authorId": "7607499",
                    "name": "Yezhou Yang"
                }
            ]
        }
    ]
}