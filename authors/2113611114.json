{
    "authorId": "2113611114",
    "papers": [
        {
            "paperId": "30934c501058ffa434aa2da39b6681725305101b",
            "title": "MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1490933248",
                    "name": "Priyanka Kargupta"
                },
                {
                    "authorId": "2113611114",
                    "name": "Tanay Komarlu"
                },
                {
                    "authorId": "3396235",
                    "name": "Susik Yoon"
                },
                {
                    "authorId": "2265623885",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "2265725240",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "91e1311c3b127f6793c85639333fe21854041cbb",
            "title": "MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities",
            "abstract": "Text classification is essential for organizing unstructured text. Traditional methods rely on human annotations or, more recently, a set of class seed words for supervision, which can be costly, particularly for specialized or emerging domains. To address this, using class surface names alone as extremely weak supervision has been proposed. However, existing approaches treat different levels of text granularity (documents, sentences, or words) independently, disregarding inter-granularity class disagreements and the context identifiable exclusively through joint extraction. In order to tackle these issues, we introduce MEGClass, an extremely weakly-supervised text classification method that leverages Mutually-Enhancing Text Granularities. MEGClass utilizes coarse- and fine-grained context signals obtained by jointly considering a document's most class-indicative words and sentences. This approach enables the learning of a contextualized document representation that captures the most discriminative class indicators. By preserving the heterogeneity of potential classes, MEGClass can select the most informative class-indicative documents as iterative feedback to enhance the initial word-based class representations and ultimately fine-tune a pre-trained text classifier. Extensive experiments on seven benchmark datasets demonstrate that MEGClass outperforms other weakly and extremely weakly supervised methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1490933248",
                    "name": "Priyanka Kargupta"
                },
                {
                    "authorId": "2113611114",
                    "name": "Tanay Komarlu"
                },
                {
                    "authorId": "3396235",
                    "name": "Susik Yoon"
                },
                {
                    "authorId": "2154990549",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "9dd8d14842256e16dcc2ababcd5dddbeb8bee726",
            "title": "OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing",
            "abstract": "Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, is a basic but important task for knowledge extraction from unstructured text. FET has been studied extensively in natural language processing and typically relies on human-annotated corpora for training, which is costly and difficult to scale. Recent studies explore the utilization of pre-trained language models (PLMs) as a knowledge base to generate rich and context-aware weak supervision for FET. However, a PLM still requires direction and guidance to serve as a knowledge base as they often generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel annotation-free, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods as well as a typical LLM method, ChatGPT. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113611114",
                    "name": "Tanay Komarlu"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "2154990549",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "a1c297c2b0d54817670532edfa937444e595249f",
            "title": "OntoType: Ontology-Guided Zero-Shot Fine-Grained Entity Typing with Weak Supervision from Pre-Trained Language Models",
            "abstract": "Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, \ufb01ne-grained semantic types, will play an important role in natural language understanding. A supervised FET method, which typically relies on human-annotated corpora for training, is costly and dif\ufb01cult to scale. Recent studies leverage pre-trained language models (PLMs) to generate rich and context-aware weak supervision for FET. However, a PLM may still generate a mixture of rough and \ufb01ne-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel zero-shot, ontology-guided FET method, O NTO T YPE , which follows a type ontological structure, from coarse to \ufb01ne, ensembles multiple PLM prompting results to generate a set of type candidates, and re\ufb01nes its type resolution, under the lo-cal context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot \ufb01ne-grained entity typing methods. Our error analysis shows that re\ufb01nement of the existing ontology structures will further improve \ufb01ne-grained entity typing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113611114",
                    "name": "Tanay Komarlu"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "2265623885",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "2265725240",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "f6aff03ccf4e97abfcc6a1a51980efa4eecd915b",
            "title": "The Future of AI-Assisted Writing",
            "abstract": "The development of Natural Language Generation models has led to the creation of powerful Artificial Intelligence-assisted writing tools. These tools are capable of predicting users' needs and actively providing suggestions as they write. In this work, we conduct a comparative user-study between such tools from an information retrieval lens: pull and push. Specifically, we investigate the user demand of AI-assisted writing, the impact of the two paradigms on quality, ownership of the writing product, and efficiency and enjoyment of the writing process. We also seek to understand the impact of bias of AI-assisted writing. Our findings show that users welcome seamless assistance of AI in their writing. Furthermore, AI helped users to diversify the ideas in their writing while keeping it clear and concise more quickly. Users also enjoyed the collaboration with AI-assisted writing tools and did not feel a lack of ownership. Finally, although participants did not experience bias in our experiments, they still expressed explicit and clear concerns that should be addressed in future AI-assisted writing tools.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220846096",
                    "name": "Carlos Alves Pereira"
                },
                {
                    "authorId": "2113611114",
                    "name": "Tanay Komarlu"
                },
                {
                    "authorId": "2220834691",
                    "name": "Wael Mobeirek"
                }
            ]
        },
        {
            "paperId": "2d289cce9f5a375d2978da4ffb1f7fd6eb8cafdc",
            "title": "Teaching Testing with Modern Technology Stacks in Undergraduate Software Engineering Courses",
            "abstract": "Students' experience with software testing in undergraduate computing courses is often relatively shallow, as compared to the importance of the topic. This experience report describes introducing industrial-strength testing into CMPSC 156, an upper division course in software engineering at UC Santa Barbara. We describe our efforts to modify our software engineering course to introduce rigorous test-coverage requirements into full-stack web development projects, requirements similar to those the authors had experienced in a professional software development setting. We present student feedback on the course and coverage metrics for the projects. We reflect on what about these changes worked (or didn't), and provide suggestions for other instructors that would like to give their students a deeper experience with software testing in their software engineering courses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113618519",
                    "name": "Scott P. Chow"
                },
                {
                    "authorId": "2113611114",
                    "name": "Tanay Komarlu"
                },
                {
                    "authorId": "152826787",
                    "name": "Phill Conrad"
                }
            ]
        }
    ]
}