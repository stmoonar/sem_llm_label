{
    "authorId": "2218869839",
    "papers": [
        {
            "paperId": "4b4ee637ef5107299212479c37a6594db5a72227",
            "title": "Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset",
            "abstract": "Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could generate relevant reasoning and demonstrate improved performance after finetuning, they fall short of a desired standard, indicating ample room for improvement. To the best of our knowledge, CMExam is the first Chinese medical exam dataset to provide comprehensive medical annotations. The experiments and findings of LLM evaluation also provide valuable insights into the challenges and potential solutions in developing Chinese medical QA systems and LLM evaluation pipelines. The dataset and relevant code are available at https://github.com/williamliujl/CMExam.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "52290752",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "2069521803",
                    "name": "Zhongyu Tian"
                },
                {
                    "authorId": "2170752745",
                    "name": "Andrew Liu"
                },
                {
                    "authorId": "3408469",
                    "name": "Helin Wang"
                },
                {
                    "authorId": "2061592207",
                    "name": "Chenyu You"
                },
                {
                    "authorId": "2107781537",
                    "name": "Zhenhua Guo"
                },
                {
                    "authorId": "145081293",
                    "name": "Lei Zhu"
                },
                {
                    "authorId": "2154744792",
                    "name": "Michael Lingzhi Li"
                }
            ]
        },
        {
            "paperId": "85722b13631d9846866d45ff2bfc2a2fe1026ac8",
            "title": "LLMRec: Benchmarking Large Language Models on Recommendation Task",
            "abstract": "Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "3741691",
                    "name": "Chao-Hong Liu"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2190432576",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "52290752",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "2165702320",
                    "name": "Kangan Zhou"
                },
                {
                    "authorId": "2154871075",
                    "name": "Yueqi Xie"
                },
                {
                    "authorId": "150346771",
                    "name": "Yuwei Cao"
                },
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2061592207",
                    "name": "Chenyu You"
                },
                {
                    "authorId": "2233087809",
                    "name": "Philip S.Yu"
                }
            ]
        },
        {
            "paperId": "8e6c4425e48b09d64827c64d8de0008f41f9be54",
            "title": "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model",
            "abstract": "Integrating large language models (LLMs) into healthcare holds great potential but faces challenges. Pre-training LLMs from scratch for domains like medicine is resource-heavy and often unfeasible. On the other hand, sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions and may not tap into domain-specific insights. In response, we present a multi-stage training method combining Domain-specific Continued Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). In addition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages. The medical LLM trained with our pipeline, Qilin-Med, shows substantial performance improvement. In the CPT and SFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set, respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by 7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on the Huatuo-26M test set, bringing further improvement to the SFT phase (12.69 in BLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced the model's performance through the Retrieval Augmented Generation (RAG) approach. Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8% on CMExam. These results highlight the contribution of our novel training approach in building LLMs for medical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258550058",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "52290752",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "2170752745",
                    "name": "Andrew Liu"
                }
            ]
        },
        {
            "paperId": "afd609e63462aff0e4d2ccd4051a4ce7041511d8",
            "title": "Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study",
            "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive performance across various vision and language tasks, yet their potential applications in recommendation tasks with visual assistance remain unexplored. To bridge this gap, we present a preliminary case study investigating the recommendation capabilities of GPT-4V(ison), a recently released LMM by OpenAI. We construct a series of qualitative test samples spanning multiple domains and employ these samples to assess the quality of GPT-4V's responses within recommendation scenarios. Evaluation results on these test samples prove that GPT-4V has remarkable zero-shot recommendation abilities across diverse domains, thanks to its robust visual-text comprehension capabilities and extensive general knowledge. However, we have also identified some limitations in using GPT-4V for recommendations, including a tendency to provide similar responses when given similar inputs. This report concludes with an in-depth discussion of the challenges and research opportunities associated with utilizing GPT-4V in recommendation scenarios. Our objective is to explore the potential of extending LMMs from vision and language tasks to recommendation tasks. We hope to inspire further research into next-generation multimodal generative recommendation models, which can enhance user experiences by offering greater diversity and interactivity. All images and prompts used in this report will be accessible at https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2265583496",
                    "name": "Meng Cao"
                },
                {
                    "authorId": "2265951415",
                    "name": "You-Liang Huang"
                },
                {
                    "authorId": "2258550058",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "2265621321",
                    "name": "Peiyan Zhang"
                },
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "2154871075",
                    "name": "Yueqi Xie"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "2173708050",
                    "name": "Jaeboum Kim"
                }
            ]
        },
        {
            "paperId": "bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3",
            "title": "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge",
            "abstract": "Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tasks (e.g., enhancing clinical diagnostics and providing medical education), a review of these efforts, particularly their development, practical applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face. In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioners in developing medical LLMs tailored to their specific needs. In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide an understanding of the advantages and limitations of LLMs in medicine. Overall, in this review, we address the following questions: 1) What are the practices for developing medical LLMs 2) How to measure the medical task performance of LLMs in a medical setting? 3) How have medical LLMs been employed in real-world practice? 4) What challenges arise from the use of medical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insights into the opportunities for LLMs in medicine and serve as a practical resource. We also maintain a regularly updated list of practical guides on medical LLMs at https://github.com/AI-in-Health/MedLLMsPracticalGuide",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265945051",
                    "name": "Hongjian Zhou"
                },
                {
                    "authorId": "2265754434",
                    "name": "Boyang Gu"
                },
                {
                    "authorId": "2265783554",
                    "name": "Xinyu Zou"
                },
                {
                    "authorId": "2265924823",
                    "name": "Yiru Li"
                },
                {
                    "authorId": "2265803524",
                    "name": "Sam S. Chen"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                },
                {
                    "authorId": "2265754569",
                    "name": "Chengfeng Mao"
                },
                {
                    "authorId": "2269413539",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2266043640",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2218961178",
                    "name": "Fenglin Liu"
                }
            ]
        },
        {
            "paperId": "c67a58bb5eb9cb6557a6032bb058a5cab978907f",
            "title": "Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare",
            "abstract": "Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218869839",
                    "name": "Junling Liu"
                },
                {
                    "authorId": "2262615869",
                    "name": "Ziming Wang"
                },
                {
                    "authorId": "2258550058",
                    "name": "Qichen Ye"
                },
                {
                    "authorId": "52290752",
                    "name": "Dading Chong"
                },
                {
                    "authorId": "1800462890",
                    "name": "Peilin Zhou"
                },
                {
                    "authorId": "2147311343",
                    "name": "Y. Hua"
                }
            ]
        }
    ]
}