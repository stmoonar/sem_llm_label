{
    "authorId": "2129782",
    "papers": [
        {
            "paperId": "1bd2d785aaa687a43569311e32578e4b8e6fe4c1",
            "title": "Automatic Combination of Sample Selection Strategies for Few-Shot Learning",
            "abstract": "In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a strong modality, dataset and approach dependence for the majority of strategies as well as their dependence on the number of shots - demonstrating that the sample selection strategies play a significant role for lower number of shots, but regresses to random selection at higher number of shots.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                },
                {
                    "authorId": "1717534",
                    "name": "J. Vanschoren"
                }
            ]
        },
        {
            "paperId": "23dd61ae65124c232d3efa78899fe0636644bda9",
            "title": "Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer",
            "abstract": "Prompt tuning is a modular and efficient solution for training large language models (LLMs). One of its main advantages is task modularity, making it suitable for multi-task problems. However, current soft-prompt-based methods often sacrifice multi-task modularity, requiring the training process to be fully or partially repeated for each newly added task. While recent work on task vectors applied arithmetic operations on full model weights to achieve the desired multi-task performance, a similar approach for soft-prompts is still missing. To this end, we introduce Task Prompt Vectors, created by element-wise difference between weights of tuned soft-prompts and their random initialization. Experimental results on 12 NLU datasets show that task prompt vectors can be used in low-resource settings to effectively initialize prompt tuning on similar tasks. In addition, we show that task prompt vectors are independent of the random initialization of prompt tuning. This allows prompt arithmetics with the pre-trained vectors from different tasks. In this way, by arithmetic addition of task prompt vectors from multiple tasks, we are able to outperform a state-of-the-art baseline in some cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266340559",
                    "name": "R\u00f3bert Belanec"
                },
                {
                    "authorId": "2309245086",
                    "name": "Simon Ostermann"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "30bfb0a8801c8e8ba65a6009027839ac34cd374e",
            "title": "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices",
            "abstract": "While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of in-context learning to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of prompt format.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "62ac45c894b38a9ec6ca089d1bea292281089d04",
            "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection",
            "abstract": "High-quality text generation capability of recent Large Language Models (LLMs) causes concerns about their misuse (e.g., in massive generation/spread of disinformation). Machine-generated text (MGT) detection is important to cope with such threats. However, it is susceptible to authorship obfuscation (AO) methods, such as paraphrasing, which can cause MGTs to evade detection. So far, this was evaluated only in monolingual settings. Thus, the susceptibility of recently proposed multilingual detectors is still unknown. We fill this gap by comprehensively benchmarking the performance of 10 well-known AO methods, attacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10 $\\times$ 37 $\\times$ 11 = 4,070 combinations). We also evaluate the effect of data augmentation on adversarial robustness using obfuscated texts. The results indicate that all tested AO methods can cause evasion of automated detection in all tested languages, where homoglyph attacks are especially successful. However, some of the AO methods severely damaged the text, making it no longer readable or easily recognizable by humans (e.g., changed language, weird characters).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260109257",
                    "name": "Dominik Macko"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "2260072705",
                    "name": "Jason Samuel Lucas"
                },
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "66674465",
                    "name": "Nafis Irtiza Tripto"
                },
                {
                    "authorId": "2279666194",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "657efb4b79c2c1761d6ca99b961f7bedd66cd955",
            "title": "MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts",
            "abstract": "Recent LLMs are able to generate high-quality multilingual texts, indistinguishable for humans from authentic human-written ones. Research in machine-generated text detection is however mostly focused on the English language and longer texts, such as news articles, scientific papers or student essays. Social-media texts are usually much shorter and often feature informal language, grammatical errors, or distinct linguistic items (e.g., emoticons, hashtags). There is a gap in studying the ability of existing methods in detection of such texts, reflected also in the lack of existing multilingual benchmark datasets. To fill this gap we propose the first multilingual (22 languages) and multi-platform (5 social media platforms) dataset for benchmarking machine-generated text detection in the social-media domain, called MultiSocial. It contains 472,097 texts, of which about 58k are human-written and approximately the same amount is generated by each of 7 multilingual LLMs. We use this benchmark to compare existing detection methods in zero-shot as well as fine-tuned form. Our results indicate that the fine-tuned detectors have no problem to be trained on social-media texts and that the platform selection for training matters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260109257",
                    "name": "Dominik Macko"
                },
                {
                    "authorId": "2307079780",
                    "name": "Jakub Kopal"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                }
            ]
        },
        {
            "paperId": "7f42b3a79ad1e176f5c881f1764b0ae87f65c252",
            "title": "Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance",
            "abstract": "When solving NLP tasks with limited labelled data, researchers can either use a general large language model without further update, or use a small number of labelled examples to tune a specialised smaller model. In this work, we address the research gap of how many labelled samples are required for the specialised small models to outperform general large models, while taking the performance variance into consideration. By observing the behaviour of fine-tuning, instruction-tuning, prompting and in-context learning on 7 language models, we identify such performance break-even points across 8 representative text classification tasks of varying characteristics. We show that the specialised models often need only few samples (on average $10 - 1000$) to be on par or better than the general ones. At the same time, the number of required labels strongly depends on the dataset or task characteristics, with this number being significantly lower on multi-class datasets (up to $100$) than on binary datasets (up to $5000$). When performance variance is taken into consideration, the number of required labels increases on average by $100 - 200\\%$ and even up to $1500\\%$ in specific cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "abd552a721666deb24938ba8f77731790a847ea0",
            "title": "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation",
            "abstract": "While fine-tuning of pre-trained language models generally helps to overcome the lack of labelled training samples, it also displays model performance instability. This instability mainly originates from randomness in initialisation or data shuffling. To address this, researchers either modify the training process or augment the available samples, which typically results in increased computational costs. We propose a new mitigation strategy, called Delayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths of ensembling, noise regularisation and model interpolation, while retaining computational efficiency. We compare DENI with 9 representative mitigation strategies across 3 models, 4 tuning strategies and 7 text classification datasets. We show that: 1) DENI outperforms the best performing mitigation strategy (Ensemble), while using only a fraction of its cost; 2) the mitigation strategies are beneficial for parameter-efficient fine-tuning (PEFT) methods, outperforming full fine-tuning in specific cases; and 3) combining DENI with data augmentation often leads to even more effective instability mitigation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "1865742502",
                    "name": "J\u00e1n Cegin"
                },
                {
                    "authorId": "2266340559",
                    "name": "R\u00f3bert Belanec"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "c6db2f2e73f75c69639e566f6a65a14a19c48bba",
            "title": "Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation",
            "abstract": "The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune downstream models. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts lexical diversity and downstream model performance. We compare the effects over 5 different LLMs, 6 datasets and 2 downstream models. We show that diversity is most increased by taboo words, but downstream model performance is highest with hints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1865742502",
                    "name": "J\u00e1n Cegin"
                },
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "3165716",
                    "name": "Jakub Simko"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                },
                {
                    "authorId": "2069676493",
                    "name": "Peter Brusilovsky"
                }
            ]
        },
        {
            "paperId": "12868220782ded0a5eeb31b78ce527dce2d37332",
            "title": "Disinformation Capabilities of Large Language Models",
            "abstract": "Automated disinformation generation is often listed as an important risk associated with large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in the English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how good they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2217274494",
                    "name": "Ivan Vykopal"
                },
                {
                    "authorId": "2217264741",
                    "name": "Mat'uvs Pikuliak"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "144535025",
                    "name": "R\u00f3bert M\u00f3ro"
                },
                {
                    "authorId": "2260109257",
                    "name": "Dominik Macko"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        },
        {
            "paperId": "20eab2c39326db3dd6469538249f847fd4b56e62",
            "title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
            "abstract": "Learning with limited labelled data, such as prompting, in-context learning, fine-tuning, meta-learning or few-shot learning, aims to effectively train a model using only a small amount of labelled samples. However, these approaches have been observed to be excessively sensitive to the effects of uncontrolled randomness caused by non-determinism in the training process. The randomness negatively affects the stability of the models, leading to large variances in results across training runs. When such sensitivity is disregarded, it can unintentionally, but unfortunately also intentionally, create an imaginary perception of research progress. Recently, this area started to attract research attention and the number of relevant studies is continuously growing. In this survey, we provide a comprehensive overview of 415 papers addressing the effects of randomness on the stability of learning with limited labelled data. We distinguish between four main tasks addressed in the papers (investigate/evaluate; determine; mitigate; benchmark/compare/report randomness effects), providing findings for each one. Furthermore, we identify and discuss seven challenges and open problems together with possible directions to facilitate further research. The ultimate goal of this survey is to emphasise the importance of this growing research area, which so far has not received an appropriate level of attention, and reveal impactful directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051747952",
                    "name": "Branislav Pecher"
                },
                {
                    "authorId": "2129782",
                    "name": "Ivan Srba"
                },
                {
                    "authorId": "1726847",
                    "name": "M. Bielikov\u00e1"
                }
            ]
        }
    ]
}