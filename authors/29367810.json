{
    "authorId": "29367810",
    "papers": [
        {
            "paperId": "ddd64e0b3111a26e91cf7cde8e567cc27da311c8",
            "title": "Relational Distant Supervision for Image Captioning without Image-Text Pairs",
            "abstract": "Unsupervised image captioning aims to generate descriptions of images without relying on any image-sentence pairs for training. Most existing works use detected visual objects or concepts as bridge to connect images and texts. Considering that the relationship between objects carries more information, we use the object relationship as a more accurate connection between images and texts. In this paper, we adapt the idea of distant supervision that extracts the knowledge about object relationships from an external corpus and imparts them to images to facilitate inferring visual object relationships, without introducing any extra pre-trained relationship detectors. Based on these learned informative relationships, we construct pseudo image-sentence pairs for captioning model training. Specifically, our method consists of three modules: (1) a relationship learning module that learns to infer relationships from images under the distant supervision; (2) a relationship-to-sentence module that transforms the inferred relationships into sentences to generate pseudo image-sentence pairs; (3) an image captioning module that is trained by using the generated image-sentence pairs. Promising results on three datasets show that our method outperforms the state-of-the-art methods of unsupervised image captioning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114756085",
                    "name": "Yayun Qi"
                },
                {
                    "authorId": "29367810",
                    "name": "Wentian Zhao"
                },
                {
                    "authorId": "2293650452",
                    "name": "Xinxiao Wu"
                }
            ]
        },
        {
            "paperId": "128f1136077469f2929a1b09dcc8d8bf5d5508e8",
            "title": "Text2Layer: Layered Image Generation using Latent Diffusion Model",
            "abstract": "Layer compositing is one of the most popular image editing workflows among both amateurs and professionals. Motivated by the success of diffusion models, we explore layer compositing from a layered image generation perspective. Instead of generating an image, we propose to generate background, foreground, layer mask, and the composed image simultaneously. To achieve layered image generation, we train an autoencoder that is able to reconstruct layered images and train diffusion models on the latent representation. One benefit of the proposed problem is to enable better compositing workflows in addition to the high-quality image output. Another benefit is producing higher-quality layer masks compared to masks produced by a separate step of image segmentation. Experimental results show that the proposed method is able to generate high-quality layered images and initiates a benchmark for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108262811",
                    "name": "Xinyang Zhang"
                },
                {
                    "authorId": "29367810",
                    "name": "Wentian Zhao"
                },
                {
                    "authorId": "2124828071",
                    "name": "Xin Lu"
                },
                {
                    "authorId": "2065529439",
                    "name": "J. Chien"
                }
            ]
        },
        {
            "paperId": "4910e373592683eab1f7f4cd0dbf1a4effe35696",
            "title": "Multi-modal Dependency Tree for Video Captioning",
            "abstract": "Generating \ufb02uent and relevant language to describe visual content is critical for the video captioning task. Many existing methods generate captions using sequence models that predict words in a left-to-right order. In this paper, we investigate a graph structured model by explicitly modeling the hierarchical structure in the sentences to further improve the \ufb02uency and relevance of the generated captions. To this end, we propose a novel video captioning method that generates a sentence by \ufb01rst constructing a multi-modal dependency tree and then traversing the constructed tree, where the syntactic structure and semantic relationship in the sentence are represented by the tree topology. To take full advantage of the information from both vision and language, both the visual and textual representation features are encoded into each tree node. Different from existing dependency parsing methods that generate uni-modal dependency trees for language understanding, our method constructs multi-modal dependency trees for language generation of videos. We also propose a tree-structured reinforcement learning algorithm to effectively optimize the captioning model, where a novel reward is designed by evaluating the semantic consistency between the generated sub-trees and the ground-truth tree. Extensive experiments on several video captioning datasets demonstrate the effectiveness of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29367810",
                    "name": "Wentian Zhao"
                },
                {
                    "authorId": "2125709",
                    "name": "Xinxiao Wu"
                }
            ]
        },
        {
            "paperId": "7c5e6720fa4c3cd73fd915bc71dea5e78184b262",
            "title": "Boosting Entity-Aware Image Captioning With Multi-Modal Knowledge Graph",
            "abstract": "Entity-aware image captioning aims to describe named entities and events related to the image by utilizing the background knowledge in the associated article. This task remains challenging as it is difficult to learn the association between named entities and visual cues due to the long-tail distribution of named entities. Furthermore, the complexity of the article brings difficulty in extracting fine-grained relationships between entities to generate informative event descriptions about the image. To tackle these challenges, we propose a novel approach that constructs a multi-modal knowledge graph (MMKG) to associate the visual objects with named entities and capture the relationship between entities simultaneously with the help of external knowledge collected from the web. Specifically, we build a text sub-graph by extracting named entities and their relationships from the article, and build an image sub-graph by detecting the objects in the image. To connect these two sub-graphs, we propose a cross-modal entity matching module trained using a knowledge base that contains Wikipedia entries and the corresponding images. Finally, the MMKG is integrated into the captioning model via a graph attention mechanism. Extensive experiments on both GoodNews and NYTimes800 k datasets demonstrate the effectiveness of our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29367810",
                    "name": "Wentian Zhao"
                },
                {
                    "authorId": "2113667024",
                    "name": "Yao Hu"
                },
                {
                    "authorId": "2149694729",
                    "name": "Heda Wang"
                },
                {
                    "authorId": "2125709",
                    "name": "Xinxiao Wu"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                }
            ]
        },
        {
            "paperId": "9421fa26257e6a8d59bb874cf3b376c6d4c4118b",
            "title": "MemCap: Memorizing Style Knowledge for Image Captioning",
            "abstract": "Generating stylized captions for images is a challenging task since it requires not only describing the content of the image accurately but also expressing the desired linguistic style appropriately. In this paper, we propose MemCap, a novel stylized image captioning method that explicitly encodes the knowledge about linguistic styles with memory mechanism. Rather than relying heavily on a language model to capture style factors in existing methods, our method resorts to memorizing stylized elements learned from training corpus. Particularly, we design a memory module that comprises a set of embedding vectors for encoding style-related phrases in training corpus. To acquire the style-related phrases, we develop a sentence decomposing algorithm that splits a stylized sentence into a style-related part that reflects the linguistic style and a content-related part that contains the visual content. When generating captions, our MemCap first extracts content-relevant style knowledge from the memory module via an attention mechanism and then incorporates the extracted knowledge into a language model. Extensive experiments on two stylized image captioning datasets (SentiCap and FlickrStyle10K) demonstrate the effectiveness of our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29367810",
                    "name": "Wentian Zhao"
                },
                {
                    "authorId": "2125709",
                    "name": "Xinxiao Wu"
                },
                {
                    "authorId": "2674678",
                    "name": "Xiaoxun Zhang"
                }
            ]
        },
        {
            "paperId": "bf2f223b2f4c425275f6b31f9e0111af32b41882",
            "title": "Cross-Domain Image Captioning via Cross-Modal Retrieval and Model Adaptation",
            "abstract": "In recent years, large scale datasets of paired images and sentences have enabled the remarkable success in automatically generating descriptions for images, namely image captioning. However, it is labour-intensive and time-consuming to collect a sufficient number of paired images and sentences in each domain. It may be beneficial to transfer the image captioning model trained in an existing domain with pairs of images and sentences (i.e., source domain) to a new domain with only unpaired data (i.e., target domain). In this paper, we propose a cross-modal retrieval aided approach to cross-domain image captioning that leverages a cross-modal retrieval model to generate pseudo pairs of images and sentences in the target domain to facilitate the adaptation of the captioning model. To learn the correlation between images and sentences in the target domain, we propose an iterative cross-modal retrieval process where a cross-modal retrieval model is first pre-trained using the source domain data and then applied to the target domain data to acquire an initial set of pseudo image-sentence pairs. The pseudo image-sentence pairs are further refined by iteratively fine-tuning the retrieval model with the pseudo image-sentence pairs and updating the pseudo image-sentence pairs using the retrieval model. To make the linguistic patterns of the sentences learned in the source domain adapt well to the target domain, we propose an adaptive image captioning model with a self-attention mechanism fine-tuned using the refined pseudo image-sentence pairs. Experimental results on several settings where MSCOCO is used as the source domain and five different datasets (Flickr30k, TGIF, CUB-200, Oxford-102 and Conceptual) are used as the target domains demonstrate that our method achieves mostly better or comparable performance against the state-of-the-art methods. We also extend our method to cross-domain video captioning where MSR-VTT is used as the source domain and two other datasets (MSVD and Charades Captions) are used as the target domains to further demonstrate the effectiveness of our method.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "29367810",
                    "name": "Wentian Zhao"
                },
                {
                    "authorId": "2125709",
                    "name": "Xinxiao Wu"
                },
                {
                    "authorId": "33642939",
                    "name": "Jiebo Luo"
                }
            ]
        },
        {
            "paperId": "097981245eb3c66cc10a3164275d0bd52f5ae22a",
            "title": "Relational Reasoning using Prior Knowledge for Visual Captioning",
            "abstract": "Exploiting relationships among objects has achieved remarkable progress in interpreting images or videos by natural language. Most existing methods resort to first detecting objects and their relationships, and then generating textual descriptions, which heavily depends on pre-trained detectors and leads to performance drop when facing problems of heavy occlusion, tiny-size objects and long-tail in object detection. In addition, the separate procedure of detecting and captioning results in semantic inconsistency between the pre-defined object/relation categories and the target lexical words. We exploit prior human commonsense knowledge for reasoning relationships between objects without any pre-trained detectors and reaching semantic coherency within one image or video in captioning. The prior knowledge (e.g., in the form of knowledge graph) provides commonsense semantic correlation and constraint between objects that are not explicit in the image and video, serving as useful guidance to build semantic graph for sentence generation. Particularly, we present a joint reasoning method that incorporates 1) commonsense reasoning for embedding image or video regions into semantic space to build semantic graph and 2) relational reasoning for encoding semantic graph to generate sentences. Extensive experiments on the MS-COCO image captioning benchmark and the MSVD video captioning benchmark validate the superiority of our method on leveraging prior commonsense knowledge to enhance relational reasoning for visual captioning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47286885",
                    "name": "Jingyi Hou"
                },
                {
                    "authorId": "2125709",
                    "name": "Xinxiao Wu"
                },
                {
                    "authorId": "2114756085",
                    "name": "Yayun Qi"
                },
                {
                    "authorId": "29367810",
                    "name": "Wentian Zhao"
                },
                {
                    "authorId": "33642939",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "7415267",
                    "name": "Yunde Jia"
                }
            ]
        },
        {
            "paperId": "ce40cd5214d556e9b8ca8ca401597321cb29b8d6",
            "title": "Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning",
            "abstract": "Video captioning is a challenging task that involves not only visual perception but also syntax representation learning. Recent progress in video captioning has been achieved through visual perception, but syntax representation learning is still under-explored. We propose a novel video captioning approach that takes into account both visual perception and syntax representation learning to generate accurate descriptions of videos. Specifically, we use sentence templates composed of Part-of-Speech (POS) tags to represent the syntax structure of captions, and accordingly, syntax representation learning is performed by directly inferring POS tags from videos. The visual perception is implemented by a mixture model which translates visual cues into lexical words that are conditional on the learned syntactic structure of sentences. Thus, a video captioning task consists of two sub-tasks: video POS tagging and visual cue translation, which are jointly modeled and trained in an end-to-end fashion. Evaluations on three public benchmark datasets demonstrate that our proposed method achieves substantially better performance than the state-of-the-art methods, which validates the superiority of joint modeling of syntax representation learning and visual perception for video captioning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47286885",
                    "name": "Jingyi Hou"
                },
                {
                    "authorId": "2125709",
                    "name": "Xinxiao Wu"
                },
                {
                    "authorId": "29367810",
                    "name": "Wentian Zhao"
                },
                {
                    "authorId": "33642939",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "7415267",
                    "name": "Yunde Jia"
                }
            ]
        }
    ]
}