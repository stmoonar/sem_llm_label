{
    "authorId": "1491627343",
    "papers": [
        {
            "paperId": "3b23659111ac4e2f7c54874b74a1a3df8eea5a2a",
            "title": "MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing",
            "abstract": "Memes have evolved as a prevalent medium for diverse communication, ranging from humour to propaganda. With the rising popularity of image-focused content, there is a growing need to explore its potential harm from different aspects. Previous studies have analyzed memes in closed settings - detecting harm, applying semantic labels, and offering natural language explanations. To extend this research, we introduce MemeMQA, a multimodal question-answering framework aiming to solicit accurate responses to structured questions while providing coherent explanations. We curate MemeMQACorpus, a new dataset featuring 1,880 questions related to 1,122 memes with corresponding answer-explanation pairs. We further propose ARSENAL, a novel two-stage multimodal framework that leverages the reasoning capabilities of LLMs to address MemeMQA. We benchmark MemeMQA using competitive baselines and demonstrate its superiority - ~18% enhanced answer prediction accuracy and distinct text generation lead across various metrics measuring lexical and semantic alignment over the best baseline. We analyze ARSENAL's robustness through diversification of question-set, confounder-based evaluation regarding MemeMQA's generalizability, and modality-specific assessment, enhancing our understanding of meme interpretation in the multimodal communication landscape.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256986135",
                    "name": "Siddhant Agarwal"
                },
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2256999352",
                    "name": "Tanmoy Chakraborty"
                }
            ]
        },
        {
            "paperId": "412ee19ec745042bd60cecee647283ecb4409f57",
            "title": "Emotion-Aware Multimodal Fusion for Meme Emotion Detection",
            "abstract": "The ever-evolving social media discourse has witnessed an overwhelming use of memes to express opinions or dissent. Besides being misused for spreading malcontent, they are mined by corporations and political parties to glean the public's opinion. Therefore, memes predominantly offer affect-enriched insights towards ascertaining the societal psyche. However, the current approaches are yet to model the affective dimensions expressed in memes effectively. They rely extensively on large multimodal datasets for pre-training and do not generalize well due to constrained visual-linguistic grounding. In this paper, we introduce <monospace>MOOD</monospace> (Meme emOtiOns Dataset), which embodies six basic emotions. We then present <monospace>ALFRED</monospace> (emotion-Aware muLtimodal Fusion foR Emotion Detection), a novel multimodal neural framework that (i) explicitly models emotion-enriched visual cues, and (ii) employs an efficient cross-modal fusion via a gating mechanism. Our investigation establishes <monospace>ALFRED</monospace>'s superiority over existing baselines by 4.94% F1. Additionally, <monospace>ALFRED</monospace> competes strongly with previous best approaches on the challenging <monospace>Memotion</monospace> task. We then discuss <monospace>ALFRED</monospace>'s domain-agnostic generalizability by demonstrating its dominance on two recently-released datasets \u2013 <monospace>HarMeme</monospace> and <monospace>Dank Memes</monospace>, over other baselines. Further, we analyze <monospace>ALFRED</monospace>'s interpretability using attention maps. Finally, we highlight the inherent challenges posed by the complex interplay of disparate modality-specific cues toward meme analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "2292433951",
                    "name": "Ramaneswaran S"
                },
                {
                    "authorId": "2249917924",
                    "name": "Md. Shad Akhtar"
                },
                {
                    "authorId": "2249914540",
                    "name": "Tanmoy Chakraborty"
                }
            ]
        },
        {
            "paperId": "83b95829433b68e1d1cc2afb3b05343ebd0e6ebb",
            "title": "Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models",
            "abstract": "In the evolving landscape of online communication, moderating hate speech (HS) presents an intricate challenge, compounded by the multimodal nature of digital content. This comprehensive survey delves into the recent strides in HS moderation, spotlighting the burgeoning role of large language models (LLMs) and large multimodal models (LMMs). Our exploration begins with a thorough analysis of current literature, revealing the nuanced interplay between textual, visual, and auditory elements in propagating HS. We uncover a notable trend towards integrating these modalities, primarily due to the complexity and subtlety with which HS is disseminated. A significant emphasis is placed on the advances facilitated by LLMs and LMMs, which have begun to redefine the boundaries of detection and moderation capabilities. We identify existing gaps in research, particularly in the context of underrepresented languages and cultures, and the need for solutions to handle low-resource settings. The survey concludes with a forward-looking perspective, outlining potential avenues for future research, including the exploration of novel AI methodologies, the ethical governance of AI in moderation, and the development of more nuanced, context-aware systems. This comprehensive overview aims to catalyze further research and foster a collaborative effort towards more sophisticated, responsible, and human-centric approaches to HS moderation in the digital era. WARNING: This paper contains offensive examples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72043108",
                    "name": "Ming Shan Hee"
                },
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "2261821068",
                    "name": "Rui Cao"
                },
                {
                    "authorId": "2281824700",
                    "name": "Palash Nandi"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2256999352",
                    "name": "Tanmoy Chakraborty"
                },
                {
                    "authorId": "2282413929",
                    "name": "Roy Ka-Wei Lee"
                }
            ]
        },
        {
            "paperId": "b91bcadc6227f7d61e406bb6957850231733442e",
            "title": "Factuality Challenges in the Era of Large Language Models",
            "abstract": "The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as\"hallucinations.\"Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to shed light on navigating various aspects of veracity in the era of generative AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256988818",
                    "name": "Isabelle Augenstein"
                },
                {
                    "authorId": "2256987318",
                    "name": "Timothy Baldwin"
                },
                {
                    "authorId": "2284591948",
                    "name": "Meeyoung Cha"
                },
                {
                    "authorId": "2256999352",
                    "name": "Tanmoy Chakraborty"
                },
                {
                    "authorId": "1683012",
                    "name": "Giovanni Luca Ciampaglia"
                },
                {
                    "authorId": "2256993500",
                    "name": "David Corney"
                },
                {
                    "authorId": "2256999256",
                    "name": "Renee DiResta"
                },
                {
                    "authorId": "48898287",
                    "name": "Emilio Ferrara"
                },
                {
                    "authorId": "2256997151",
                    "name": "Scott Hale"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "2256998951",
                    "name": "Eduard H. Hovy"
                },
                {
                    "authorId": "2257003221",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2268194233",
                    "name": "Filippo Menczer"
                },
                {
                    "authorId": "31329752",
                    "name": "Rub\u00e9n M\u00edguez"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2994143",
                    "name": "Dietram A. Scheufele"
                },
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "2256998588",
                    "name": "Giovanni Zagni"
                }
            ]
        },
        {
            "paperId": "ce228f30ac7cd8cc6976ec2af8965888cb1a33bf",
            "title": "MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization",
            "abstract": "Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena renders them an ideal vehicle for communication. To comprehend the subtle message conveyed within a meme, one must understand the relevant background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like knowyourmeme.com, currently, there is no efficient way to deduce a meme\u2019s context dynamically. In this work, we propose a novel task, MEMEX - given a meme and a related document, the aim is to mine the context that succinctly explains the background of the meme. At first, we develop MCC (Meme Context Corpus), a novel dataset for MEMEX. Further, to benchmark MCC, we propose MIME (MultImodal Meme Explainer), a multimodal neural framework that uses external knowledge-enriched meme representation and a multi-level approach to capture the cross-modal semantic dependencies between the meme and the context. MIME surpasses several unimodal and multimodal systems and yields an absolute improvement of 4% F1-score over the best baseline. Lastly, we conduct detailed analyses of MIME\u2019s performance, highlighting the aspects that could lead to optimal modeling of cross-modal contextual associations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "2218454024",
                    "name": "S Ramaneswaran"
                },
                {
                    "authorId": "46368735",
                    "name": "Udit Arora"
                },
                {
                    "authorId": "46815454",
                    "name": "Md. Shad Akhtar"
                },
                {
                    "authorId": "144054829",
                    "name": "Tanmoy Chakraborty"
                }
            ]
        },
        {
            "paperId": "d03fc0e109be9a321f78f17b4346a00e0e8e080b",
            "title": "Characterizing the Entities in Harmful Memes: Who is the Hero, the Villain, the Victim?",
            "abstract": "Memes can sway people\u2019s opinions over social media as they combine visual and textual information in an easy-to-consume manner. Since memes instantly turn viral, it becomes crucial to infer their intent and potentially associated harmfulness to take timely measures as needed. A common problem associated with meme comprehension lies in detecting the entities referenced and characterizing the role of each of these entities. Here, we aim to understand whether the meme glorifies, vilifies, or victimizes each entity it refers to. To this end, we address the task of role identification of entities in harmful memes, i.e., detecting who is the \u2018hero\u2019, the \u2018villain\u2019, and the \u2018victim\u2019 in the meme, if any. We utilize HVVMemes \u2013 a memes dataset on US Politics and Covid-19 memes, released recently as part of the CONSTRAINT@ACL-2022 shared-task. It contains memes, entities referenced, and their associated roles: hero, villain, victim, and other. We further design VECTOR (Visual-semantic role dEteCToR), a robust multi-modal framework for the task, which integrates entity-based contextual information in the multi-modal representation and compare it to several standard unimodal (text-only or image-only) or multi-modal (image+text) models. Our experimental results show that our proposed model achieves an improvement of 4% over the best baseline and 1% over the best competing stand-alone submission from the shared-task. Besides divulging an extensive experimental setup with comparative analyses, we finally highlight the challenges encountered in addressing the complex task of semantic role labeling within memes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "1992797499",
                    "name": "Atharva Kulkarni"
                },
                {
                    "authorId": "2151858197",
                    "name": "Tharun Suresh"
                },
                {
                    "authorId": "2125404287",
                    "name": "Himanshi Mathur"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "46815454",
                    "name": "Md. Shad Akhtar"
                },
                {
                    "authorId": "144054829",
                    "name": "Tanmoy Chakraborty"
                }
            ]
        },
        {
            "paperId": "0724d235bde1bef4071961dbcfa9e63cacf42dea",
            "title": "EXPERT: Public Benchmarks for Dynamic Heterogeneous Academic Graphs",
            "abstract": "Machine learning models that learn from dynamic graphs face nontrivial challenges in learning and inference as both nodes and edges change over time. The existing large-scale graph benchmark datasets that are widely used by the community primarily focus on homogeneous node and edge attributes and are static. In this work, we present a variety of large scale, dynamic heterogeneous academic graphs to test the effectiveness of models developed for multi-step graph forecasting tasks. Our novel datasets cover both context and content information extracted from scientific publications across two communities: Artificial Intelligence (AI) and Nuclear Nonproliferation (NN). In addition, we propose a systematic approach to improve the existing evaluation procedures used in the graph forecasting models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24029613",
                    "name": "Sameera Horawalavithana"
                },
                {
                    "authorId": "27558137",
                    "name": "Ellyn Ayton"
                },
                {
                    "authorId": "2079696364",
                    "name": "A. Usenko"
                },
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "2162736840",
                    "name": "Jasmine Eshun"
                },
                {
                    "authorId": "71060479",
                    "name": "Robin Cosbey"
                },
                {
                    "authorId": "2046282",
                    "name": "M. Glenski"
                },
                {
                    "authorId": "143683394",
                    "name": "Svitlana Volkova"
                }
            ]
        },
        {
            "paperId": "1dd25d1bc1266f9f1e088c99f8b7f22d038f4d19",
            "title": "Detecting and Understanding Harmful Memes: A Survey",
            "abstract": "The automatic identification of harmful content online is of major concern for social media platforms, policymakers, and society. Researchers have studied textual, visual, and audio content, but typically in isolation. Yet, harmful content often combines multiple modalities, as in the case of memes. With this in mind, here we offer a comprehensive survey with a focus on harmful memes. Based on a systematic analysis of recent literature, we first propose a new typology of harmful memes, and then we highlight and summarize the relevant state of the art. One interesting finding is that many types of harmful memes are not really studied, e.g., such featuring self-harm and extremism, partly due to the lack of suitable datasets. We further find that existing datasets mostly capture multi-class scenarios, which are not inclusive of the affective spectrum that memes can represent. Another observation is that memes can propagate globally through repackaging in different languages and that they can also be multilingual, blending different cultures. We conclude by highlighting several challenges related to multimodal semiotics, technological constraints, and non-trivial social engagement, and we present several open-ended aspects such as delineating online harm and empirically examining related frameworks and assistive interventions, which we believe will motivate and drive future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "46815454",
                    "name": "Md. Shad Akhtar"
                },
                {
                    "authorId": "2105813793",
                    "name": "Dimitar I. Dimitrov"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "22593971",
                    "name": "Hamed Firooz"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "144925193",
                    "name": "F. Silvestri"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "144054829",
                    "name": "Tanmoy Chakraborty"
                }
            ]
        },
        {
            "paperId": "6a56d951127e0fda2993ff9982455686570a2715",
            "title": "Domain-aware Self-supervised Pre-training for Label-Efficient Meme Analysis",
            "abstract": "Existing self-supervised learning strategies are constrained to either a limited set of objectives or generic downstream tasks that predominantly target uni-modal applications. This has isolated progress for imperative multi-modal applications that are diverse in terms of complexity and domain-affinity, such as meme analysis. Here, we introduce two self-supervised pre-training methods, namely Ext-PIE-Net and MM-SimCLR that (i) employ off-the-shelf multi-modal hate-speech data during pre-training and (ii) perform self-supervised learning by incorporating multiple specialized pretext tasks, effectively catering to the required complex multi-modal representation learning for meme analysis. We experiment with different self-supervision strategies, including potential variants that could help learn rich cross-modality representations and evaluate using popular linear probing on the Hateful Memes task. The proposed solutions strongly compete with the fully supervised baseline via label-efficient training while distinctly outperforming them on all three tasks of the Memotion challenge with 0.18%, 23.64%, and 0.93% performance gain, respectively. Further, we demonstrate the generalizability of the proposed solutions by reporting competitive performance on the HarMeme task. Finally, we empirically establish the quality of the learned representations by analyzing task-specific learning, using fewer labeled training samples, and arguing that the complexity of the self-supervision strategy and downstream task at hand are correlated. Our efforts highlight the requirement of better multi-modal self-supervision methods involving specialized pretext tasks for efficient fine-tuning and generalizable performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "2106504788",
                    "name": "Mohd Khizir Siddiqui"
                },
                {
                    "authorId": "46815454",
                    "name": "Md. Shad Akhtar"
                },
                {
                    "authorId": "144054829",
                    "name": "Tanmoy Chakraborty"
                }
            ]
        },
        {
            "paperId": "7bd7435a51a43481df4b1f08ec2d0c40449e24a1",
            "title": "DISARM: Detecting the Victims Targeted by Harmful Memes",
            "abstract": "Internet memes have emerged as an increasingly popular means of communication on the Web. Although typically intended to elicit humour, they have been increasingly used to spread hatred, trolling, and cyberbullying, as well as to target specific individuals, communities, or society on political, socio-cultural, and psychological grounds. While previous work has focused on detecting harmful, hateful, and offensive memes, identifying whom they attack remains a challenging and underexplored area. Here we aim to bridge this gap. In particular, we create a dataset where we annotate each meme with its victim(s) such as the name of the targeted person(s), organization(s), and community(ies). We then propose DISARM (Detecting vIctimS targeted by hARmful Memes), a framework that uses named entity recognition and person identification to detect all entities a meme is referring to, and then, incorporates a novel contextualized multimodal deep neural network to classify whether the meme intends to harm these entities. We perform several systematic experiments on three test setups, corresponding to entities that are (a) all seen while training, (b) not seen as a harmful target on training, and (c) not seen at all on training. The evaluation results show that DISARM significantly outperforms ten unimodal and multimodal systems. Finally, we show that DISARM is interpretable and comparatively more generalizable and that it can reduce the relative error rate for harmful target identification by up to 9 points absolute over several strong multimodal rivals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491627343",
                    "name": "Shivam Sharma"
                },
                {
                    "authorId": "46815454",
                    "name": "Md. Shad Akhtar"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "144054829",
                    "name": "Tanmoy Chakraborty"
                }
            ]
        }
    ]
}