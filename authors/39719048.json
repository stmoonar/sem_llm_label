{
    "authorId": "39719048",
    "papers": [
        {
            "paperId": "264719226f68d220e1b2079dad8e0ddb3138f193",
            "title": "A Conceptual Model for Data Storytelling Highlights in Business Intelligence Environments",
            "abstract": "We introduce a conceptual model for highlights to support data analysis and storytelling in the domain of Business Intelligence, via the automated extraction, representation, and exploitation of highlights revealing key facts that are hidden in the data with which a data analyst works. The model builds on the concepts of Holistic and Elementary Highlights, along with their context, constituents and interrelationships, whose synergy can identify internal properties, patterns and key facts in a dataset being analyzed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2252596607",
                    "name": "Panos Vassiliadis"
                },
                {
                    "authorId": "2267390474",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "2007368576",
                    "name": "Faten El Outa"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2193798492",
                    "name": "Dimos Gkitsakis"
                }
            ]
        },
        {
            "paperId": "248512f2c283ec21e7018439c9a52f2fbfc527d9",
            "title": "Assessment Methods for the Interestingness of Cube Queries",
            "abstract": "In this paper, we discuss methods to assess the interestingness of a query in an environment of data cubes. We assume a hierarchical multidimensional database, storing data cubes and level hierarchies. We focus our approach on a taxonomy of the dimensions of interestingness, and specifically, relevance, surprise, novelty",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2193798492",
                    "name": "Dimos Gkitsakis"
                },
                {
                    "authorId": "2193803645",
                    "name": "Spyridon Kaloudis"
                },
                {
                    "authorId": "2193798982",
                    "name": "Eirini Mouselli"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "144996383",
                    "name": "Panos Vassiliadis"
                }
            ]
        },
        {
            "paperId": "817db70aa4fa977f3765e966fcdb525a2f0f1d2d",
            "title": "A declarative approach to data narration",
            "abstract": "This vision paper lays the preliminary foundations for Data Narrative Management Systems (DNMS), systems that enable the storage, sharing, and manipulation of data narratives. We motivate the need for such formal foundations and introduce a simple logical framework inspired by the relational model. The core of this framework is a Data Narrative Manipulation Language inspired by the extended relational algebra. We illustrate its use via examples and discuss the main challenges for the implementation of this vision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2007368576",
                    "name": "Faten El Outa"
                },
                {
                    "authorId": "144996383",
                    "name": "Panos Vassiliadis"
                }
            ]
        },
        {
            "paperId": "9c7f2de8c88925d764a133013d59e0c95b4520de",
            "title": "Data Narration for the People: Challenges and Opportunities",
            "abstract": "Data narration is the process of telling stories with insights extracted from data. It is an instance of data science [4] where the pipeline focuses on data collection and exploration, answering questions, structuring answers, and finally presenting them to stakeholders [16, 17]. This tutorial reviews the challenges and opportunities of the full and semi-automation of these steps. In doing so, it draws from the extensive literature in data narration, data exploration and data visualization. In particular, we point out key theoretical and practical contributions in each domain such as next-step recommendation and policy learning for data exploration, insight interestingness and evaluation frameworks, and the crafting of data stories for the people who will exploit them. We also identify topics that are still worth investigating, such as the inclusion of different stakeholders\u2019 profiles in designing data pipelines with the goal of providing data narration for all.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "196ae03382897f696dfd50c2550081d25de5e8a3",
            "title": "A data narrative about tuberculosis pandemic in Gabon",
            "abstract": "Data narration is the activity of crafting narratives supported by facts extracted from data analysis, using interactive visualizations. It allows the transmission of findings in the data, by visual means, in order to facilitate their reception by a target audience. Despite its recognized utility in public health, data narratives are typically limited to the transmission of treatment recommendations to educate the general public. This paper describes the crafting of a data narrative about tuberculosis pandemic in Gabon, intended to an audience of health professionals and authorities. Specifically, we describe and illustrate all phases of the crafting process, combining best practices in data and epidemic intelligence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2165689305",
                    "name": "Raymond Ondzigue Mbenga"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2085528",
                    "name": "T. Devogele"
                },
                {
                    "authorId": "2007368576",
                    "name": "Faten El Outa"
                },
                {
                    "authorId": "4039291",
                    "name": "Sydney Maghendji Nzondo"
                },
                {
                    "authorId": "3685723",
                    "name": "E. Ngoungou"
                }
            ]
        },
        {
            "paperId": "30f331273338bcf2d85387ee817321e278348a49",
            "title": "Use of Context in Data Quality Management: a Systematic Literature Review",
            "abstract": "The importance of context in data quality (DQ) was shown many years ago and nowadays is widely accepted. Early approaches and surveys defined DQ as fitness for use and showed the influence of context on DQ. This paper presents a Systematic Literature Review (SLR) for investigating how context is taken into account in recent proposals for DQ management (DQM). We specifically present the planning and execution of the SLR, the analysis criteria and our results reflecting the relationship between context and DQ in the state of the art and, particularly, how this context is defined and used for DQM. The SLR is instrumental to the identification of context components and the design of a context formal model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145020960",
                    "name": "Flavia Serra"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "48916890",
                    "name": "Adriana Marotta"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                }
            ]
        },
        {
            "paperId": "381e87eb80c99bc558e0184bf05748ec077bebea",
            "title": "Describing Multidimensional Data Through Highlights",
            "abstract": "The Intentional Analytics Model (IAM) is a new paradigm to couple OLAP and analytics. It relies on two ideas: (i) letting the user explore data by expressing his/her analysis intentions rather than the data (s)he needs, and (ii) returning enhanced cubes, i.e., multidimensional data annotated with knowledge insights in the form of model components (e",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36486438",
                    "name": "Matteo Francia"
                },
                {
                    "authorId": "1824724",
                    "name": "E. Gallinucci"
                },
                {
                    "authorId": "1711320",
                    "name": "M. Golfarelli"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2652412",
                    "name": "S. Rizzi"
                }
            ]
        },
        {
            "paperId": "81ac50f85b6cb9e434144e3f81ac003847e8283d",
            "title": "Generating Personalized Data Narrations from EDA Notebooks",
            "abstract": "In this short paper, we present our preliminary results for generating personalized data narrations by extracting messages from a collection of Exploratory Data Analysis (EDA) notebooks over a given dataset. The approach consists of extracting features from notebooks to learn what interesting messages they expose. Based on those interesting messages, we formalize the problem of producing a user-tailored data narration, i.e., a coherent sequence of messages matching a given user profile. We developed a proof of concept and experimented with Kaggle.com notebooks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35156108",
                    "name": "Alexandre Chanson"
                },
                {
                    "authorId": "2007368576",
                    "name": "Faten El Outa"
                },
                {
                    "authorId": "1714938",
                    "name": "Nicolas Labroche"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "73774797",
                    "name": "Willeme Verdeaux"
                },
                {
                    "authorId": "2163555723",
                    "name": "Lucile Jacquemart"
                }
            ]
        },
        {
            "paperId": "a45445b9841caa47fae214a4367c38fc5624edea",
            "title": "Cube Interestingness: Novelty, Relevance, Peculiarity and Surprise",
            "abstract": "In this paper, we discuss methods to assess the interestingness of a query in an environment of data cubes. We assume a hierarchical multidimensional database, storing data cubes and level hierarchies. We provide a systematic taxonomy of the dimensions of interestingness, and specifically, relevance, surprise, novelty, and peculiarity. We propose specific measures and algorithms for assessing the different dimensions of cube query interestingness in a quantitative fashion. \u2217Work done with Univ. Ioannina. 1 ar X iv :2 21 2. 03 29 4v 1 [ cs .D B ] 6 D ec 2 02 2",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2193798492",
                    "name": "Dimos Gkitsakis"
                },
                {
                    "authorId": "2193803645",
                    "name": "Spyridon Kaloudis"
                },
                {
                    "authorId": "2193798982",
                    "name": "Eirini Mouselli"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2267390474",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "2252596607",
                    "name": "Panos Vassiliadis"
                }
            ]
        },
        {
            "paperId": "4d1a1e065923aad6bf20d9ef8c9cb52706a315b5",
            "title": "Learning Analysis Behavior in SQL Workloads",
            "abstract": "This paper presents a set of analyses aiming at better understanding the SQLShare workload [13] and learning users\u2019 analysis behavior. SQLShare is a database-as-a-service platform targeting scientists and data scientists with minimal database experience, whose workload was made available to the research community. According to the authors of [13], this workload is the only one containing primarily ad-hoc hand-written queries over user-uploaded datasets. In this paper we analyze this workload, by comparing users\u2019 explorations (sequences of queries), looking for common SQL operations performed by the users during data analysis. We use a clustering algorithm to retrieve groups of similar explorations and we analyze the obtained clusters through many statistical and visual indicators for explaining analysis patterns inside clusters. To our knowledge, this is the first attempt to characterize human analysis behavior in SQL workloads.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059499321",
                    "name": "Cl\u00e9ment Moreau"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "54f4f069dc64d6d74509ebb4485b5257031e47ff",
            "title": "Clustering sequences of multi-dimensional sets of semantic elements",
            "abstract": "The study of semantic aspects of human behavior is an hot topic. Most of the time, semantic sequences describe these complex behaviors. Indeed, sequences include several information as type of human activities or places. To study these complex data, we need to define new similarity measures and select appropriate clustering processes. This article proposes a semantic similarity measure, based on ontologies, which manages complex semantic elements with different levels of detail and incertitude. An application of this approach from the domain of touristic mobility shows the interest of this process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059499321",
                    "name": "Cl\u00e9ment Moreau"
                },
                {
                    "authorId": "35156108",
                    "name": "Alexandre Chanson"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2085528",
                    "name": "T. Devogele"
                },
                {
                    "authorId": "3398299",
                    "name": "Cyril de Runz"
                }
            ]
        },
        {
            "paperId": "fa486838b09cddc906306755382adb73246c5938",
            "title": "A Fuzzy Generalisation of the Hamming Distance for Temporal Sequences",
            "abstract": "The study of temporal sequences is a main topic in different domains, especially for human mobility mining. This article defines the Fuzzy Temporal Hamming (FTH) distance between temporal sequences. This new measure generalises the Hamming distance and improves it by introducing a fuzzy time-window. This fuzzy approach tolerates temporal distortions as shifting and permutations. Moreover, the time computation of FTH is competitive with other Optimal Matching methods used for temporal sequences comparison. To validate this approach, we cluster data from a real Time-Use Survey and we compare the results obtained with other methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059499321",
                    "name": "Cl\u00e9ment Moreau"
                },
                {
                    "authorId": "2085528",
                    "name": "T. Devogele"
                },
                {
                    "authorId": "3398299",
                    "name": "Cyril de Runz"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2122860753",
                    "name": "Evelyne Moreau"
                },
                {
                    "authorId": "32864653",
                    "name": "Laurent \u00c9tienne"
                }
            ]
        },
        {
            "paperId": "5ebd9424889d8ffdbec9193bde53f98fd3f8ed8b",
            "title": "Learning Analysis Patterns using a Contextual Edit Distance",
            "abstract": "This paper presents a proposal for learning users\u2019 behavior patterns when they interactively analyse data. Users\u2019 explorations (sequences of queries) are compared looking for subsequences of common actions or operations performed by the users during data analysis. We use a hierarchical clustering algorithm to retrieve groups of similar explorations. The main difficulty is to devise a similarity measure suitable to measure similarities between sequences of human actions. We propose to use a Contextual Edit Distance (CED), a generalization of Edit Distance that manages context-dependent edition costs. CED compares two users\u2019 explorations, making special emphasis in the similarity of queries with nearby queries in the exploration, which determines a local context. We test our approach on three workloads of real users\u2019 explorations, extracting common analysis patterns, both in explorations devised by students and expert analysts. We also experiment on an artificial workload, generated with CubeLoad [19], showing that our approach is able to identify the patterns imposed by the generator. To the best of our knowledge, this is the first attempt to characterize human analysis behavior in workloads of data explorations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059499321",
                    "name": "Cl\u00e9ment Moreau"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "35156108",
                    "name": "Alexandre Chanson"
                },
                {
                    "authorId": "2085528",
                    "name": "T. Devogele"
                }
            ]
        },
        {
            "paperId": "8e8165d10f7cf17ec87e81228d96369fea87e032",
            "title": "A contextual edit distance for semantic trajectories",
            "abstract": "The understanding of daily human activity is an active research topic. Thanks to GPS and smartphones, human movements can be monitored and analyzed. In addition, by exploiting Linked Open Data and user personal data, semantic labels and annotations can be added to movements. Thus, semantic trajectories can be considered as sequences of timestamped activities where each activity is described by a semantic label. In this context, a major challenge is the comparison of such semantic trajectories, looking to extract and learning similar human mobility behaviors. We propose CED (Contextual Edit Distance), a generic similarity measure for semantic sequences comparison which improve the Edit Distance to take into account the context similarity between elements in the sequence. CED is configurable to any sequence data and business needs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059499321",
                    "name": "Cl\u00e9ment Moreau"
                },
                {
                    "authorId": "2085528",
                    "name": "T. Devogele"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "32864653",
                    "name": "Laurent \u00c9tienne"
                }
            ]
        },
        {
            "paperId": "92d632b7c6c1f4710df4b275c3808a272e3616fc",
            "title": "Methodology for Mining, Discovering and Analyzing Semantic Human Mobility Behaviors",
            "abstract": "Several institutes produce large semantic data sets about daily activities and human mobility. The analysis and the understanding of these data are crucial for urban planning, socio-psychology and political sciences or epidemiology. Currently, none of usual data mining process is customized for a complete analysis of semantic mobility sequences from data to understandable behaviors. Based on an extended review of the literature, we define in this article a new methodological pipeline, SIMBA (Semantic Indicators for Mobility and Behavior Analysis), for mine and analyze semantic mobility sequences in order to discover coherent information and human behaviors. A framework for semantic sequence mobility analysis and clustering explicability integrating different complementary statistical indicators and visual tools is proposed. To validate this methodology, we use a large set of real daily mobility sequences obtained from one household-travel survey. Complementary knowledge are automatically discovered help to this method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059499321",
                    "name": "Cl\u00e9ment Moreau"
                },
                {
                    "authorId": "2085528",
                    "name": "T. Devogele"
                },
                {
                    "authorId": "32864653",
                    "name": "Laurent \u00c9tienne"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "3398299",
                    "name": "Cyril de Runz"
                }
            ]
        },
        {
            "paperId": "aeea79eb52b17fea5c6a68bb57030d155d1fe4bd",
            "title": "The Traveling Analyst Problem: Definition and Preliminary Study",
            "abstract": "This paper introduces the Traveling Analyst Problem (TAP), an original strongly NP-hard problem where an automated algo-rithm assists an analyst to explore a dataset, by suggesting the most interesting and coherent set of queries that are estimated to be completed under a time constraint. We motivate the problem, study its complexity, propose a simple heuristic under simplifying assumptions for approximating it, and run preliminary tests to observe the behavior of this heuristic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35156108",
                    "name": "Alexandre Chanson"
                },
                {
                    "authorId": "73769911",
                    "name": "Ben Crulis"
                },
                {
                    "authorId": "1714938",
                    "name": "Nicolas Labroche"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2652412",
                    "name": "S. Rizzi"
                },
                {
                    "authorId": "144996383",
                    "name": "Panos Vassiliadis"
                }
            ]
        },
        {
            "paperId": "f8cf9d56787c4e115008ae7462e4c54da0a3f883",
            "title": "Supporting the Generation of Data Narratives",
            "abstract": ". Data narration has received increasing interest in several communities while lacking models and tools for handling, building and structuring data narratives. We present a simple prototype for supporting data narrative, based on a conceptual model de\ufb01ned in [4]. It guides a data narrator from scratch: fetch and explore data, abstract important messages based on an intentional goal, structure the contents of the data story, and render it in a visual manner. This prototype is implemented in Java as a web application using Spring, d3.js, JFreeChart and Apache PDFBox.",
            "fieldsOfStudy": [
                "History",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2007368576",
                    "name": "Faten El Outa"
                },
                {
                    "authorId": "36486438",
                    "name": "Matteo Francia"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "144996383",
                    "name": "Panos Vassiliadis"
                }
            ]
        },
        {
            "paperId": "325db60b2c592e0a06dadadc81bd933e52c18211",
            "title": "Qualitative Analysis of the SQLShareWorkload for Session Segmentation",
            "abstract": "This paper presents an ongoing work aiming at better understanding the workload of SQLShare [9]. SQLShare is database-asa-service platform targeting scientists and data scientists with minimal database experience, whose workload was made available to the research community. According to the authors of [9], this workload is the only one containing primarily ad-hoc handwritten queries over user-uploaded datasets. We analyzed this workload by extracting features that characterize SQL queries and we show how to use these features to separate sequences of SQL queries into meaningful sessions. We ran a few test over various query workloads to validate empirically our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "73774797",
                    "name": "Willeme Verdeaux"
                },
                {
                    "authorId": "73768955",
                    "name": "Yann Raimont"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                }
            ]
        },
        {
            "paperId": "7342d134974fcdf29a29ceb1aea3a1ee7a85e46d",
            "title": "Qualitative Analysis of the SQLShare Workload for Session Segmentation",
            "abstract": "This paper presents an ongoing work aiming at better understanding the workload of SQLShare [9]. SQLShare is database-as-a-service platform targeting scientists and data scientists with minimal database experience, whose workload was made available to the research community. According to the authors of [9], this workload is the only one containing primarily ad-hoc handwritten queries over user-uploaded datasets. We analyzed this workload by extracting features that characterize SQL queries and we show how to use these features to separate sequences of SQL queries into meaningful sessions. We ran a few test over various query workloads to validate empirically our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "73774797",
                    "name": "Willeme Verdeaux"
                },
                {
                    "authorId": "73768955",
                    "name": "Yann Raimont"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                }
            ]
        },
        {
            "paperId": "2b402de8530413c40374b1beab5b31066bdea69e",
            "title": "Can Models Learned from a Dataset Reflect Acquisition of Procedural Knowledge? An Experiment with Automatic Measurement of Online Review Quality",
            "abstract": "Can models learned from a dataset reflect how good are humans at mastering a particular skill? This paper studies this question in the context of online reviews writing, where the skill corresponds to the procedural knowledge needed to write helpful reviews. To this end, we model the quality of a review by a combination of various metrics stemming from text analysis (like readability, polarity, spelling errors or length) and we use customer declared helpfulness as a ground truth for constructing the model. We use Knowledge Tracing, a popular model of skill acquisition, to measure the evolution of the ability to write reviews of good quality over a period of time. While recent studies have tried to measure the quality of a review and correlate it to helpfulness, to the best of our knowledge, our work is the first to address this question as the exercise of a reviewer\u2019s skill over a sequence of reviews. Our experiments on a set of 41,681 Amazon book reviews show that it is possible to accurately assess the individual skill acquisition of writing a helpful review, based on a statistical model of the procedural knowledge at hand rather than human evaluations prone to subjectivity and variations over time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2096575189",
                    "name": "Martina Megasari"
                },
                {
                    "authorId": "144160561",
                    "name": "Pandu Wicaksono"
                },
                {
                    "authorId": "15060396",
                    "name": "Chiao-Yun Li"
                },
                {
                    "authorId": "40385867",
                    "name": "Cl\u00e9ment Chaussade"
                },
                {
                    "authorId": "2048659368",
                    "name": "Shibo Cheng"
                },
                {
                    "authorId": "1714938",
                    "name": "Nicolas Labroche"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "84178435b5c1bce0849fa43e6addbece579d0417",
            "title": "A benchmark for assessing OLAP exploration assistants",
            "abstract": ". In this demonstration paper, we present InDExBench , a benchmark designed and developed for evaluating and comparing Interactive Database Ex-ploration (IDE) assistant systems in the context of OLAP. We brie\ufb02y recall how InDExBench works behind the scenes. Then, we explain how it can be used in practice by considering the case of Sam , an OLAP IDE assistant author who wants to evaluate how her system performs, and how it compares to competitors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299529",
                    "name": "Mahfoud Djedaini"
                },
                {
                    "authorId": "1714938",
                    "name": "Nicolas Labroche"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "92603fae86949970151c5302526ed918e6cea6d7",
            "title": "An Approach for Alert Raising in Real-Time Data Warehouses",
            "abstract": "This work proposes an approach for alert raising within a real-time data warehouse environment. It is based on the calculation of confidence intervals for measures from historical facts. As new facts arrive to the data warehouse on a real-time basis, they are systematically compared with their appropriate confidence intervals and alerts are raised when anomalies are detected. The interest of this approach is illustrated using the particular real world use case of technical analysis of stock data.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1938339",
                    "name": "M. L\u00f3pez"
                },
                {
                    "authorId": "36761081",
                    "name": "S. Nadal"
                },
                {
                    "authorId": "2299529",
                    "name": "Mahfoud Djedaini"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "143796776",
                    "name": "P. Furtado"
                }
            ]
        },
        {
            "paperId": "4b72a16205a84a83297762e9e37fe86d41e3c155",
            "title": "Preference-Driven Refinement of Service Compositions",
            "abstract": "The Service Oriented Computing Paradigm proposes the construction of applications by integrating pre-existent services. Since a large number of services may be available in the Cloud, the selection of services is a crucial task in the definition of a composition. The selected services should attend the requirements of the compound application, by considering both functional and non-functional requirements (including quality and preference constraints). As the number of available services increases, the automation of the selection task becomes desirable. We propose a method for the refinement of service compositions that takes the abstract specification of a composition, the definition of concrete services and user preferences. Our algorithm produces a list of refinements in preference order. Experiments show that our method can be used in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145447618",
                    "name": "Cheikh Ba"
                },
                {
                    "authorId": "34815790",
                    "name": "U. S. Costa"
                },
                {
                    "authorId": "2114240",
                    "name": "Mirian Halfeld-Ferrari"
                },
                {
                    "authorId": "11163066",
                    "name": "R\u00e9my Ferr\u00e9"
                },
                {
                    "authorId": "1977200",
                    "name": "M. Musicante"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2066497987",
                    "name": "Sophie Robert"
                }
            ]
        },
        {
            "paperId": "ae5773342b1237452c4d1e57fa5abfd149061e0b",
            "title": "Preference-driven Re\ufb01nement of Service Compositions",
            "abstract": "The Service Oriented Computing Paradigm proposes the construction of applications by integrating pre-existent services. Since a large number of services may be available in the Cloud, the selection of services is a crucial task in the definition of a composition. The selected services should attend the requirements of the compound application, by considering both functional and non-functional requirements (including quality and preference constraints). As the number of available services increases, the automation of the selection task becomes desirable. We propose a method for the refinement of service compositions that takes the abstract specification of a composition, the definition of concrete services and user preferences. Our algorithm produces a list of refinements in preference order. Experiments show that our method can be used in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145447618",
                    "name": "Cheikh Ba"
                },
                {
                    "authorId": "34815790",
                    "name": "U. S. Costa"
                },
                {
                    "authorId": "2114240",
                    "name": "Mirian Halfeld-Ferrari"
                },
                {
                    "authorId": "11163066",
                    "name": "R\u00e9my Ferr\u00e9"
                },
                {
                    "authorId": "1977200",
                    "name": "M. Musicante"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2066497987",
                    "name": "Sophie Robert"
                }
            ]
        },
        {
            "paperId": "e86c4adcf1a132269a740f16169c849aaa1f7292",
            "title": "A Holistic Approach to OLAP Sessions Composition: The Falseto Experience",
            "abstract": "OLAP is the main paradigm for flexible and effective exploration of multidimensional cubes in data warehouses. During an OLAP session the user analyzes the results of a query and determines a new query that will give her a better understanding of information. Given the huge size of the data space, this exploration process is often tedious and may leave the user disoriented and frustrated. This paper presents an OLAP tool named Falseto (Former AnalyticaL Sessions for lEss Tedious Olap), that is meant to assist query and session composition, by letting the user summarize, browse, query, and reuse former analytical sessions. Falseto's implementation on top of a formal framework is detailed. We also report the experiments we run to obtain and analyze real OLAP sessions and assess Falseto with them. Finally, we discuss how Falseto can be seen as a starting point for bridging OLAP with exploratory search, a search paradigm centered on the user and the evolution of her knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3342952",
                    "name": "Julien Aligon"
                },
                {
                    "authorId": "2937418",
                    "name": "Kamal Boulil"
                },
                {
                    "authorId": "143909445",
                    "name": "Patrick Marcel"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "3e26ec8e78ebd4b02382dd924349608b729bbd19",
            "title": "Assessment and analysis of information quality: a multidimensional model and case studies",
            "abstract": "Information quality is a complex and multidimensional notion. In the context of information system engineering, it is also a transversal notion and to be fully understood, it needs to be evaluated jointly considering the quality of data, the quality of the underlying conceptual data model and the quality of the software system that manages these data. This paper presents a multidimensional model for exploring information in a multidimensional way, which aids in the navigation, filtering, and interpretation of quality measures, and thus in the identification of the most appropriate actions to improve information quality. Two application scenarios are presented to illustrate and validate the multidimensional approach: the first one concerns the quality of customer information at Electricite de France, a French Electricity Company, and the second concerns the quality of patient records at Curie Institute, a well-known medical institute in France. The instantiation of our multidimensional model in these contexts shows first illustrations of its applicability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1398238537",
                    "name": "I. Comyn-Wattiau"
                },
                {
                    "authorId": "2090997283",
                    "name": "Mireille Cosquer"
                },
                {
                    "authorId": "2340543",
                    "name": "Zoubida Kedad"
                },
                {
                    "authorId": "2754805",
                    "name": "Sylvaine Nugier"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2609425",
                    "name": "S. Cherfi"
                },
                {
                    "authorId": "2673479",
                    "name": "Virginie Thion"
                }
            ]
        },
        {
            "paperId": "0985f1a3c3366937c7ecc4671120b172837331fb",
            "title": "APMD-Workbench: A Benchmark for Query Personalization",
            "abstract": "Query personalization algorithms intend to deliver the most relevant data to each user according to their profiles. Validating efficiency and relevancy of such algorithms still remains a very difficult task as it requires a scalable dataset, a bunch of user profiles and queries and possibly user feedbacks. At the best of our knowledge, there is not a reference benchmark devoted to the validation of such algorithms. In most of the published papers, validation of personalized queries is done through ad hoc benchmaks whose features are not given and which are generally not provided to other authors to perform similar evaluations. In this paper, we present a benchmark for query personalization which aims to be a reference to validate query personalization systems. The benchmark is based on a large test bed derived from MovieLens and IMDb datasets, and provides, besides the data set itself, a large sample of queries and users as well as their corresponding good results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "35226856",
                    "name": "Dimitre Kostadinov"
                },
                {
                    "authorId": "1750088",
                    "name": "M. Bouzeghoub"
                }
            ]
        },
        {
            "paperId": "2863c1ecc5ea22da3ad8c3d18bc4b7e35dd67a43",
            "title": "Multidimensional Management and Analysis of Quality Measures for CRM Applications in an Electricity Company",
            "abstract": "This paper presents an approach integrating data quality into the business intelligence chain in the context of CRM applications at EDF (Electricite de France), the major electricity company in France. The main contribution of this paper is the definition and instantiation of a generic multi-dimensional star-like model for storing, analyzing and capitalizing data quality indicators, measurements and metadata. This approach is illustrated in one of EDF's CRM applications implementing the data quality-driven information supply chain for business intelligence where the role of the data quality expert is highly emphasized.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2673479",
                    "name": "Virginie Thion"
                },
                {
                    "authorId": "2340543",
                    "name": "Zoubida Kedad"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1398238537",
                    "name": "I. Comyn-Wattiau"
                },
                {
                    "authorId": "2754805",
                    "name": "Sylvaine Nugier"
                },
                {
                    "authorId": "2609425",
                    "name": "S. Cherfi"
                }
            ]
        },
        {
            "paperId": "2b74f7804eab6c6b01e598514dd008f49c5e953c",
            "title": "Multidimensional Management and Analysis of Quality Measures for CRM Applications at EDF",
            "abstract": "This paper presents an approach integrating data quality into the business intelligence chain in the context of customer-relationship management (CRM) applications at EDF (Electricite de France), the major electricity company in France. The main contribution of this paper is the definition and instantiation of a generic multi-dimensional star-like model for storing, analyzing and capitalizing data quality indicators, measurements and metadata. This approach is illustrated through one of EDF's CRM applications, implementing domain-specific quality indicators and providing quality-driven information management as a business intelligence chain. The role of the data quality expert is highly emphasized.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398238537",
                    "name": "I. Comyn-Wattiau"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2098102392",
                    "name": "Samira Sisa\u00efd-Cherfi"
                },
                {
                    "authorId": "2754805",
                    "name": "Sylvaine Nugier"
                },
                {
                    "authorId": "2340543",
                    "name": "Zoubida Kedad"
                },
                {
                    "authorId": "2096471628",
                    "name": "V. Thion-Goasdou\u00e9"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "2ae9415bca76ba0484fb84f56057a114fab8b9ae",
            "title": "Generation of a Reference Data Set for Query Personalization",
            "abstract": "This report describes the procedure followed for bu ilding a reference data set for measuring the pertinence of personalization algorit hms. The reference data set consists of a set of queries, a set of user profiles and the query resul ts that are pertinent for the user. In this way, th e results obtained with a personalization algorithm c an be compared to reference results in order to evaluate pertinence of the algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "676daa174bf5009c3ab2ba3541ac78f50f8c7814",
            "title": "A Framework for Quality Evaluation in Data Integration Systems",
            "abstract": "Ensuring and maximizing the quality and integrity of information is a crucial process for today enterprise information systems (EIS). It requires a clear understanding of the interdependencies between the dimensions characterizing quality of data (QoD), quality of conceptual data model (QoM) of the database, keystone of the EIS, and quality of data management and integration processes (QoP). The improvement of one quality dimension (such as data accuracy or model expressiveness) may have negative consequences on other quality dimensions (e.g., freshness or completeness of data). In this paper we briefly present a framework, called QUADRIS, relevant for adopting a quality improvement strategy on one or many dimensions of QoD or QoM with considering the collateral effects on the other interdependent quality dimensions. We also present the scenarios of our ongoing validations on a CRM EIS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285406",
                    "name": "J. Akoka"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1763550",
                    "name": "Omar Boucelma"
                },
                {
                    "authorId": "1750088",
                    "name": "M. Bouzeghoub"
                },
                {
                    "authorId": "1398238537",
                    "name": "I. Comyn-Wattiau"
                },
                {
                    "authorId": "2090997283",
                    "name": "Mireille Cosquer"
                },
                {
                    "authorId": "2673479",
                    "name": "Virginie Thion"
                },
                {
                    "authorId": "2340543",
                    "name": "Zoubida Kedad"
                },
                {
                    "authorId": "2754805",
                    "name": "Sylvaine Nugier"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2609425",
                    "name": "S. Cherfi"
                }
            ]
        },
        {
            "paperId": "0b761ffa9918ad9cc4b5015b7ced74a8eae2abf0",
            "title": "Data Quality Evaluation in Data Integration Systems",
            "abstract": "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L\u2019archive ouverte pluridisciplinaire HAL, est destin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents scientifiques de niveau recherche, publi\u00e9s ou non, \u00e9manant des \u00e9tablissements d\u2019enseignement et de recherche fran\u00e7ais ou \u00e9trangers, des laboratoires publics ou priv\u00e9s. Data Quality Evaluation in Data Integration Systems Veronika Peralta",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "66d6f69b642dc62e503dd0aa01f6511fe68bef99",
            "title": "Data Freshness Evaluation in Different Application Scenarios",
            "abstract": ". Data freshness has been identified as one of the most important data quality attributes in information systems. This importance increases especially in the context of systems that integrate a large set of autonomous data sources. In this paper we describe a quality evaluation framework which allows evaluation of data freshness in different architectural contexts. We also show how this quality factor may impact the reconfiguration of a data integration system to fulfill user expectations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1750088",
                    "name": "M. Bouzeghoub"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "b73cc3611769eb37d54609272f193897abf71f26",
            "title": "Analyzing and Evaluating Data Freshness in Data Integration Systems",
            "abstract": "Data freshness has been identified as one of the most important data quality attributes in information systems. This importance increases particularly in the context of systems composed of a large set of autonomous data sources where integrating data having different freshness may lead to semantic problems. This paper addresses the problem of evaluating data freshness in a data integration system and presents a taxonomy to classify different scenarios where data freshness can be evaluated. We propose a framework for modeling the data integration system and performing freshness evaluation and we illustrate the approach for a particular scenario.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "38883337",
                    "name": "R. Ruggia"
                },
                {
                    "authorId": "1750088",
                    "name": "M. Bouzeghoub"
                }
            ]
        },
        {
            "paperId": "c549096669b2e66c944bb629e6f2ae9b63de4724",
            "title": "A Framework for Data Quality Evaluation in a Data Integration System",
            "abstract": "To solve complex user requirements the information systems need to integrate data from several, possibly autonomous data sources. One challenge in such environment is to provide the user with data meeting his requirements in terms of quality. These requirements are difficult to satisfy because of the strong heterogeneity of the sources. In this paper we address the problem of data quality evaluation in data integration systems. We present a framework which is a first attempt to formalize the evaluation of data quality. It is based on a graph model of the data integration system which allows us to define evaluation methods and demonstrate propositions in terms of graph properties. To illustrate our approach, we also present a first experiment with the data freshness quality factor and we show how the framework is used to evaluate this factor according to different scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "38883337",
                    "name": "R. Ruggia"
                },
                {
                    "authorId": "2340543",
                    "name": "Zoubida Kedad"
                },
                {
                    "authorId": "1750088",
                    "name": "M. Bouzeghoub"
                }
            ]
        },
        {
            "paperId": "ef4e5eca6de3ef8b3cd3f8cf7ad6089ab5d75f95",
            "title": "A framework for analysis of data freshness",
            "abstract": "Data freshness has been identified as one of the most important data quality attributes in information systems. This importance increases particularly in the context of distributed systems, composed of a large set of autonomous data sources, where integrating data having different freshness may lead to semantic problems. There are various definitions of data freshness in the literature, depending on the applications where they are used, as well as different metrics to measure them. This paper presents an analysis of these definitions and metrics and proposes a taxonomy based upon the nature of the data, the type of application and the synchronization policies underlying the multi-source information system. We analyze, in terms of the taxonomy, the way freshness is defined and used in several types of systems and we present some open research problems in the field of data freshness evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1750088",
                    "name": "M. Bouzeghoub"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "fafd0a02ff7f93b3324197c9c11466b146d1ac34",
            "title": "On the evaluation of data freshness in data integration systems",
            "abstract": "Data freshness has been identified as one of the most important data quality factors in information systems. This importance inc reases particularly in the context of systems composed of a large set of autonomous data sources, where integrating data having different freshness may lead to semantic problems. There are various definitions of data freshness in the literature as well as different me trics to measure them, depending on the applications where they are used. This paper presen ts an analysis of these definitions and metrics and proposes a taxonomy to compare them. Th is taxonomy is useful for (i) analyzing the way data freshness is defined and used in sever al types of systems and (ii) studying the properties of the system that influence data freshn ess. We model the system and its properties as a workflow and we present a mechanism for evalua ting data freshness based on the workflow representation. RESUME . La fraicheur des donnees est l'un des facteurs de qualite les plus importants dans les systemes d'information. Cette importance est accrue dans le contexte des systemes composes d'un grand nombre de sources de donnees autonomes, ou l'integration des donnees caracterisees par des fraicheurs differentes peut m ener a des problemes semantiques. Dans la litterature, il existe differentes definitions de la fraicheur des donnees ainsi que des metriques permettant de la mesurer, et ce en foncti on des applications ou elles sont employees. Cet article presente une analyse de ces definitions et ces metriques et propose une taxonomie permettant de les comparer. Cette taxonom ie servira a (1) analyser la maniere dont la fraicheur des donnees est definie et utilis ee dans differents types de systemes, et (2) etudier les proprietes du systeme qui influencent l a fraicheur des donnees. Nous representons le systeme et ses proprietes comme un workflow et n ous presentons notre approche pour l'evaluation de la fraicheur basee dans cette repre sentation.",
            "fieldsOfStudy": [
                "Geography",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "1750088",
                    "name": "M. Bouzeghoub"
                }
            ]
        },
        {
            "paperId": "11fec0a7fba4226a4c1e1f9c32ef07ae58d3fa6e",
            "title": "On the Applicability of Rules to Automate Data Warehouse Logical Design",
            "abstract": "Data Warehouse logical design involves the definition of structures that enable an efficient access to information. The designer builds relational or multidimensional structures taking into account a conceptual schema representing the information requirements, the source databases, and non functional (mainly performance) requirements. Existing work in this area has mainly focused into aspects like data models, data structures specifically designed for DWs, and criteria for defining table partitions and indexes. This paper proposes a step forward on the automation of DW relational design through a rule-based mechanism, which automatically generates the DW schema by applying existing DW design knowledge. The proposed rules embed design strategies which are triggered by conditions on requirements and source databases, and perform the schema generation through the application of predefined DW design oriented transformations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2291903",
                    "name": "Alvaro Illarze"
                },
                {
                    "authorId": "38883337",
                    "name": "R. Ruggia"
                }
            ]
        },
        {
            "paperId": "b79a2a71f94409bd6c5ff3445dd08caf89c995ff",
            "title": "Using Design Guidelines to Improve Data Warehouse Logical Design",
            "abstract": "Data Warehouse-(DW) logical design often start with a conceptual schema and then generates relational structures. Applying this approach implies to cope with two main aspects: (i) -mapping the conceptual model structures to the logical model ones, and (ii) -taking into account implementation issues, which are not considered in the conceptual schema. This paper addresses this second aspect and presents a formalism that allows the DW designer to specify design guidelines which express design strategies related with implementation requirements. Through these guidelines the designer states high level manners to cope with different design problems, for example: managing complex and big dimensions, dimension versioning, different user profiles accessing to different attributes, high summarized data, horizontal partitions of historical data, generic dimensionality and non-additive measures. This work is part of a DW logical design environment, where the design guidelines are specified through a graphical editor and then automatically processed in order to build the logical schema.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "38883337",
                    "name": "R. Ruggia"
                }
            ]
        },
        {
            "paperId": "c446901ea9c13857dc8fa516324b9c6374661ad1",
            "title": "Towards the Automation of Data Warehouse Logical Design: a Rule-Based Approach",
            "abstract": "Data Warehouse logical design involves the definition of structures that enable an efficient access to information. The designer builds relational or multidimensional structures taking into account a conceptual schema representing the information requirements, the source databases, and non-functional requirements. Existing work in this area has mainly focused on aspects like data models, data structures specifically designed for DWs, and criteria for defining table partitions and indexes. This paper proposes a step forward to the automation of DW relational design through a rule-based mechanism, which automatically generates the DW schema by applying existing DW design knowledge. The proposed rules embed design strategies which are triggered by conditions on requirements and source databases, and perform the schema generation through the application of predefined DW design oriented transformations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2291903",
                    "name": "Alvaro Illarze"
                },
                {
                    "authorId": "38883337",
                    "name": "R. Ruggia"
                }
            ]
        }
    ]
}