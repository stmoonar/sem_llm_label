{
    "authorId": "2166476321",
    "papers": [
        {
            "paperId": "15162e1285a7681d1ea109c2a4a2cfd69e1cfb0f",
            "title": "Ranking with Long-Term Constraints",
            "abstract": "The feedback that users provide through their choices (e.g., clicks, purchases) is one of the most common types of data readily available for training search and recommendation algorithms. However, myopically training systems based on choice data may only improve short-term engagement, but not the long-term sustainability of the platform and the long-term benefits to its users, content providers, and other stakeholders. In this paper, we thus develop a new framework in which decision makers (e.g., platform operators, regulators, users) can express long-term goals for the behavior of the platform (e.g., fairness, revenue distribution, legal requirements). These goals take the form of exposure or impact targets that go well beyond individual sessions, and we provide new control-based algorithms to achieve these goals. In particular, the controllers are designed to achieve the stated long-term goals with minimum impact on short-term engagement. Beyond the principled theoretical derivation of the controllers, we evaluate the algorithms on both synthetic and real-world data. While all controllers perform well, we find that they provide interesting trade-offs in efficiency, robustness, and the ability to plan ahead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "11963742",
                    "name": "Kiant\u00e9 Brantley"
                },
                {
                    "authorId": "40960149",
                    "name": "Zhichong Fang"
                },
                {
                    "authorId": "2166476321",
                    "name": "Sarah Dean"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        },
        {
            "paperId": "c856d1a955e185bfa5711bf2b0fe1aa64f3e9abc",
            "title": "Foreword for Workshop on Decision Making for Information Retrieval and Recommender Systems",
            "abstract": "1 FOREWORD Most of the recent progress in information retrieval (IR) and recommender systems has been fueled by deep learning: the success of neural networks has brought tremendous opportunities to model highly complex patterns for prediction. However, algorithmic advances on accurate predictions and improved user modeling are just a small part of designing considerations of a much larger system. IR and recommender systems difer from other machine learning domains because they are inherently part of an ecosystem \u2013 in the simplest case, a world of items and users. In these ecosystems, system designers face a broad range of decisions \u2013 e.g., how to balance popularity, which incentives should be given to which users, or what safeguards to put in place to ensure the platform thrives in the long-run. Some of the decision making problems are statistical and algorithmic in nature, such as coping with the uncertainty of data and models, while others can involve building systems for the satisfactory of multiple parties. In general, there are many complex decision-making challenges faced by real-world IR and recommender systems, but existing approaches often make oversimplifed assumptions about the environment, data, and human behavior. Ignoring those challenges or treating them simply as pattern recognition problems can cost the engagement, accessibility, fairness, inclusiveness, and ultimately the vitality of IR and recommendation systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118285164",
                    "name": "Da Xu"
                },
                {
                    "authorId": "48839531",
                    "name": "Tobias Schnabel"
                },
                {
                    "authorId": "2087078033",
                    "name": "Xiquan Cui"
                },
                {
                    "authorId": "2166476321",
                    "name": "Sarah Dean"
                },
                {
                    "authorId": "143736660",
                    "name": "Aniket Deshmukh"
                },
                {
                    "authorId": "2156653838",
                    "name": "Bo Yang"
                },
                {
                    "authorId": "2112273246",
                    "name": "Shipeng Yu"
                }
            ]
        },
        {
            "paperId": "0357499656bea3c723b73a6ae9ad1d42589e2dde",
            "title": "Online Convex Optimization with Unbounded Memory",
            "abstract": "Online convex optimization (OCO) is a widely used framework in online learning. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. However, in many applications the learner's loss depends not only on the current decision but on the entire history of decisions until that point. The OCO framework and its existing generalizations do not capture this, and they can only be applied to many settings of interest after a long series of approximation arguments. They also leave open the question of whether the dependence on memory is tight because there are no non-trivial lower bounds. In this work we introduce a generalization of the OCO framework,\"Online Convex Optimization with Unbounded Memory\", that captures long-term dependence on past decisions. We introduce the notion of $p$-effective memory capacity, $H_p$, that quantifies the maximum influence of past decisions on present losses. We prove an $O(\\sqrt{H_p T})$ upper bound on the policy regret and a matching (worst-case) lower bound. As a special case, we prove the first non-trivial lower bound for OCO with finite memory \\citep{anavaHM2015online}, which could be of independent interest, and also improve existing upper bounds. We demonstrate the broad applicability of our framework by using it to derive regret bounds, and to improve and simplify existing regret bound derivations, for a variety of online learning problems including online linear control and an online variant of performative prediction.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2108651907",
                    "name": "Raunak Kumar"
                },
                {
                    "authorId": "2166476321",
                    "name": "Sarah Dean"
                },
                {
                    "authorId": "2633757",
                    "name": "Robert D. Kleinberg"
                }
            ]
        },
        {
            "paperId": "7150542359e88e2cc561e09bfad07a35d0514f44",
            "title": "Cross-Dataset Propensity Estimation for Debiasing Recommender Systems",
            "abstract": "Datasets for training recommender systems are often subject to distribution shift induced by users\u2019 and recommenders\u2019 selection biases. In this paper, we study the impact of selection bias on datasets with different quantization. We then leverage two differently quantized datasets from different source distributions to mitigate distribution shift by applying the inverse probability scoring method from causal inference. Empirically, our approach gains signi\ufb01cant performance improvement over single-dataset methods and alternative ways of combining two datasets",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2136358260",
                    "name": "Fengyu Li"
                },
                {
                    "authorId": "2166476321",
                    "name": "Sarah Dean"
                }
            ]
        },
        {
            "paperId": "740dbd216cb102c8e2816bdbb08365ecc3340525",
            "title": "Modeling Content Creator Incentives on Algorithm-Curated Platforms",
            "abstract": "Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by algorithms, including modern factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices, e.g., non-negative vs. unconstrained factorization, significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models, like exposure games, for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools for numerically finding equilibria in exposure games, and illustrate results of an audit on the MovieLens and LastFM datasets. Among else, we find that the strategically produced content exhibits strong dependence between algorithmic exploration and content diversity, and between model expressivity and bias towards gender-based user and creator groups.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "29785199",
                    "name": "Jiri Hron"
                },
                {
                    "authorId": "48778049",
                    "name": "K. Krauth"
                },
                {
                    "authorId": "1694621",
                    "name": "Michael I. Jordan"
                },
                {
                    "authorId": "19238593",
                    "name": "Niki Kilbertus"
                },
                {
                    "authorId": "2166476321",
                    "name": "Sarah Dean"
                }
            ]
        },
        {
            "paperId": "8786b784adb60f807db903afa81349e077c4cff5",
            "title": "Preference Dynamics Under Personalized Recommendations",
            "abstract": "The design of content recommendation systems underpins many online platforms: social media feeds, online news aggregators, and audio/video hosting websites all choose how best to organize an enormous amount of content for users to consume. Many projects (both practical and academic) have designed algorithms to match users to content they will enjoy under the assumption that user's preferences and opinions do not change with the content they see. However, increasing amounts of evidence suggest that individuals' preferences are directly shaped by what content they see---radicalization, rabbit holes, polarization, and boredom are all example phenomena of preferences affected by content. Polarization in particular can occur even in ecosystems with \"mass media,\" where no personalization takes place, as recently explored in a natural model of preference dynamics by [14] and [13]. If all users' preferences are drawn towards content they already like, or are repelled from content they already dislike, uniform consumption of media leads to a population of heterogeneous preferences converging towards only two poles. In this work, we explore whether some phenomenon akin to polarization occurs when users receive personalized content recommendations. We use a similar model of preference dynamics, where an individual's preferences move towards content the consume and enjoy, and away from content they consume and dislike. We show that standard user reward maximization is an almost trivial goal in such an environment (a large class of simple algorithms will achieve only constant regret). A more interesting objective, then, is to understand under what conditions a recommendation algorithm can ensure stationarity of user's preferences. We show how to design a content recommendations which can achieve approximate stationarity, under mild conditions on the set of available content, when a user's preferences are known, and how one can learn enough about a user's preferences to implement such a strategy even when user preferences are initially unknown.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2166476321",
                    "name": "Sarah Dean"
                },
                {
                    "authorId": "144848816",
                    "name": "Jamie Morgenstern"
                }
            ]
        },
        {
            "paperId": "e42a3e626322f3fb46f31d40dacfc5142cdca044",
            "title": "Emergent specialization from participation dynamics and multi-learner retraining",
            "abstract": "Numerous online services are data-driven: the behavior of users affects the system's parameters, and the system's parameters affect the users' experience of the service, which in turn affects the way users may interact with the system. For example, people may choose to use a service only for tasks that already works well, or they may choose to switch to a different service. These adaptations influence the ability of a system to learn about a population of users and tasks in order to improve its performance broadly. In this work, we analyze a class of such dynamics -- where users allocate their participation amongst services to reduce the individual risk they experience, and services update their model parameters to reduce the service's risk on their current user population. We refer to these dynamics as \\emph{risk-reducing}, which cover a broad class of common model updates including gradient descent and multiplicative weights. For this general class of dynamics, we show that asymptotically stable equilibria are always segmented, with sub-populations allocated to a single learner. Under mild assumptions, the utilitarian social optimum is a stable equilibrium. In contrast to previous work, which shows that repeated risk minimization can result in (Hashimoto et al., 2018; Miller et al., 2021), we find that repeated myopic updates with multiple learners lead to better outcomes. We illustrate the phenomena via a simulated example initialized from real data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2166476321",
                    "name": "Sarah Dean"
                },
                {
                    "authorId": "22174631",
                    "name": "Mihaela Curmei"
                },
                {
                    "authorId": "1785356",
                    "name": "L. Ratliff"
                },
                {
                    "authorId": "144848816",
                    "name": "Jamie Morgenstern"
                },
                {
                    "authorId": "144651919",
                    "name": "Maryam Fazel"
                }
            ]
        }
    ]
}