{
    "authorId": "40372969",
    "papers": [
        {
            "paperId": "47f8e4b701fea6054236c00508e84b73ae900862",
            "title": "Data Dependencies Extended for Variety and Veracity: A Family Tree (Extended abstract)",
            "abstract": "To address the variety and veracity issues of big data, data dependencies have been extended as data quality rules to adapt to various data types, ranging from (1) categorical data with equality relationships to (2) heterogeneous data with similarity relationships, and (3) numerical data with order relationships. In this survey, we briefly review the recent proposals on data dependencies categorized into the aforesaid types of data. In addition to (a) the concepts of these data dependency notations, we investigate (b) the extension relationships between data dependencies. It forms a family tree of extensions, mostly rooted in FDs. Moreover, we summarize (c) the discovery of dependencies from data, and (d) the applications of the extended data dependencies. Finally, we conclude with several directions of future studies on the emerging data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "2041297209",
                    "name": "Fei Gao"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                },
                {
                    "authorId": "121900345",
                    "name": "Chaokun Wang"
                }
            ]
        },
        {
            "paperId": "9c44cd875e8a75845eca9494a5fcb5a8dfc66632",
            "title": "Regression-Oriented Knowledge Distillation for Lightweight Ship Orientation Angle Prediction with Optical Remote Sensing Images",
            "abstract": "Ship orientation angle prediction (SOAP) with optical remote sensing images is an important image processing task, which often relies on deep convolutional neural networks (CNNs) to make accurate predictions. This paper proposes a novel framework to reduce the model sizes and computational costs of SOAP models without harming prediction accuracy. First, a new SOAP model called Mobile-SOAP is designed based on MobileNetV2, achieving state-of-the-art prediction accuracy. Four tiny SOAP models are also created by replacing the convolutional blocks in Mobile-SOAP with four small-scale networks, respectively. Then, to transfer knowledge from Mobile-SOAP to four lightweight models, we propose a novel knowledge distillation (KD) framework termed SOAP-KD consisting of a novel feature-based guidance loss and an optimized synthetic samples-based knowledge transfer mechanism. Lastly, extensive experiments on the FGSC-23 dataset confirm the superiority of Mobile-SOAP over existing models and also demonstrate the effectiveness of SOAP-KD in improving the prediction performance of four specially designed tiny models. Notably, by using SOAP-KD, the test mean absolute error of the ShuffleNetV2x1.0-based model is only 8% higher than that of Mobile-SOAP, but its number of parameters and multiply-accumulate operations (MACs) are respectively 61.6% and 60.8% less.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110398900",
                    "name": "Zhan Shi"
                },
                {
                    "authorId": "2223465867",
                    "name": "Xin Ding"
                },
                {
                    "authorId": "2223209763",
                    "name": "Peng Ding"
                },
                {
                    "authorId": "2154927728",
                    "name": "Chun Yang"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                },
                {
                    "authorId": "2199015700",
                    "name": "Xiao-dong Song"
                }
            ]
        },
        {
            "paperId": "a7e58dc03d029100fd437e229f7ee80e976fc842",
            "title": "HYTREL: Hypergraph-enhanced Tabular Data Representation Learning",
            "abstract": "Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks. However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HYTREL, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs - where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show that HYTREL is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HYTREL iff the two tables are identical up to permutations. Our empirical results demonstrate that HYTREL consistently outperforms other competitive baselines on four downstream tasks with minimal pretraining, illustrating the advantages of incorporating the inductive biases associated with tabular data into the representations. Finally, our qualitative analyses showcase that HYTREL can assimilate the table structures to generate robust representations for the cells, rows, columns, and the entire table.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2901524",
                    "name": "Pei Chen"
                },
                {
                    "authorId": "3081847",
                    "name": "Soumajyoti Sarkar"
                },
                {
                    "authorId": "8789103",
                    "name": "Leonard Lausen"
                },
                {
                    "authorId": "2057595515",
                    "name": "Balasubramaniam Srinivasan"
                },
                {
                    "authorId": "40881843",
                    "name": "Sheng Zha"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "b83b0bce880acde2ae6b81e6cccbb470e5f37a2a",
            "title": "RST-style Discourse Parsing Guided by Document-level Content Structures",
            "abstract": "Rhetorical Structure Theory based Discourse Parsing (RST-DP) explores how clauses, sentences, and large text spans compose a whole discourse and presents the rhetorical structure as a hierarchical tree. Existing RST parsing pipelines construct rhetorical structures without the knowledge of document-level content structures, which causes relatively low performance when predicting the discourse relations for large text spans. Recognizing the value of high-level content-related information in facilitating discourse relation recognition, we propose a novel pipeline for RST-DP that incorporates structure-aware news content sentence representations derived from the task of News Discourse Profiling. By incorporating only a few additional layers, this enhanced pipeline exhibits promising performance across various RST parsing metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150655891",
                    "name": "Ming Li"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                }
            ]
        },
        {
            "paperId": "c64aca1e6595f32c0c6c2f753a006e4141caa0b3",
            "title": "Semi-supervised News Discourse Profiling with Contrastive Learning",
            "abstract": "News Discourse Profiling seeks to scrutinize the event-related role of each sentence in a news article and has been proven useful across various downstream applications. Specifically, within the context of a given news discourse, each sentence is assigned to a pre-defined category contingent upon its depiction of the news event structure. However, existing approaches suffer from an inadequacy of available human-annotated data, due to the laborious and time-intensive nature of generating discourse-level annotations. In this paper, we present a novel approach, denoted as Intra-document Contrastive Learning with Distillation (ICLD), for addressing the news discourse profiling task, capitalizing on its unique structural characteristics. Notably, we are the first to apply a semi-supervised methodology within this task paradigm, and evaluation demonstrates the effectiveness of the presented approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150655891",
                    "name": "Ming Li"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                }
            ]
        },
        {
            "paperId": "cea60b913d647e108f530cf8b608ce167a7dae33",
            "title": "Composition-contrastive Learning for Sentence Embeddings",
            "abstract": "Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66299364",
                    "name": "Sachin Chanchani"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                }
            ]
        },
        {
            "paperId": "f408ece61ff06460a0ddbb6c2278744f67ecb7cd",
            "title": "An Efficient Transfer Learning Method with Auxiliary Information",
            "abstract": "Transfer learning (TL) is an information reuse learning tool, which can help us learn better classification effect than traditional single task learning, because transfer learning can share information within the task-to-task model. Most TL algorithms are studied in the field of data improvement, doing some data extraction and transformation. However, it ignores that existing the additional information to improve the model\u2019s accuracy, like Universum samples in the training data with privileged information. In this article, we focus on considering prior data to improve the TL algorithm, and the additional features also called privileged information are incorporated into the learning to improve the learning paradigm. In addition, we also carry out the Universum samples which do not belong to any indicated categories into the transfer learning paradigm to improve the utilization of prior knowledge. We propose a new TL Model (PU-TLSVM), in which each task with corresponding privileged features and Universum data is considered in the proposed model, so as to apply tasks with a priori data to the training stage. Then, we use Lagrange duality theorem to optimize our model to obtain the optimal discriminant for target task classification. Finally, we make a lot of predictions and tests to compare the actual effectiveness of the proposed method with the previous methods. The experiment results indicate that the proposed method is more effective and robust than other baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144330453",
                    "name": "Bo Liu"
                },
                {
                    "authorId": "2179441160",
                    "name": "Liangjiao Li"
                },
                {
                    "authorId": "1720010",
                    "name": "Yanshan Xiao"
                },
                {
                    "authorId": "2148897802",
                    "name": "Kai Wang"
                },
                {
                    "authorId": "2230666594",
                    "name": "Jian Hu"
                },
                {
                    "authorId": "2155882061",
                    "name": "Junrui Liu"
                },
                {
                    "authorId": "2109279821",
                    "name": "Qihang Chen"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                }
            ]
        },
        {
            "paperId": "0097e87c101b257c05bfaf2236170748fe09bd43",
            "title": "Less is More: Simplifying Feature Extractors Prevents Overfitting for Neural Discourse Parsing Models",
            "abstract": "Complex feature extractors are widely employed for text representation building. However, these complex feature extractors can lead to severe overfitting problems especially when the training datasets are small, which is especially the case for several discourse parsing tasks. Thus, we propose to remove additional feature extractors and only utilize self-attention mechanism to exploit pretrained neural language models in order to mitigate the overfitting problem. Experiments on three common discourse parsing tasks (News Discourse Profiling, Rhetorical Structure Theory based Discourse Parsing and Penn Discourse Treebank based Discourse Parsing) show that powered by recent pretrained language models, our simplied feature extractors obtain better generalizabilities and meanwhile achieve comparable or even better system performance. The simplified feature extractors have fewer learnable parameters and less processing time. Codes will be released and this simple yet effective model can serve as a better baseline for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150655891",
                    "name": "Ming Li"
                },
                {
                    "authorId": "2188133952",
                    "name": "Sijing Yu"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                }
            ]
        },
        {
            "paperId": "3718864bbd0657b09561db62b4898db51a7bc402",
            "title": "Modeling Document-level Temporal Structures for Building Temporal Dependency Graphs",
            "abstract": "We propose to leverage news discourse profiling to model document-level temporal structures for building temporal dependency graphs. Our key observation is that the functional roles of sentences used for profiling news discourse signify different time frames relevant to a news story and can, therefore, help to recover the global temporal structure of a document. Our analyses and experiments with the widely used knowledge distillation technique show that discourse profiling effectively identifies distant inter-sentence event and (or) time expression pairs that are temporally related and otherwise difficult to locate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3466801",
                    "name": "Prafulla Kumar Choubey"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                }
            ]
        },
        {
            "paperId": "50580a4e4bcf565c5e63bbddb11db79aa5cd30d4",
            "title": "Crossroads, Buildings and Neighborhoods: A Dataset for Fine-grained Location Recognition",
            "abstract": "General domain Named Entity Recognition (NER) datasets like CoNLL-2003 mostly annotate coarse-grained location entities such as a country or a city. But many applications require identifying fine-grained locations from texts and mapping them precisely to geographic sites, e.g., a crossroad, an apartment building, or a grocery store. In this paper, we introduce a new dataset HarveyNER with fine-grained locations annotated in tweets. This dataset presents unique challenges and characterizes many complex and long location mentions in informal descriptions. We built strong baseline models using Curriculum Learning and experimented with different heuristic curricula to better recognize difficult location mentions. Experimental results show that the simple curricula can improve the system\u2019s performance on hard cases and its overall performance, and outperform several other baseline systems. The dataset and the baseline models can be found at https://github.com/brickee/HarveyNER.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2901524",
                    "name": "Pei Chen"
                },
                {
                    "authorId": "2141375220",
                    "name": "Haotian Xu"
                },
                {
                    "authorId": "2175569031",
                    "name": "Cheng Zhang"
                },
                {
                    "authorId": "40372969",
                    "name": "Ruihong Huang"
                }
            ]
        }
    ]
}