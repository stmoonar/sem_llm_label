{
    "authorId": "1766853",
    "papers": [
        {
            "paperId": "6049f92e687e5db4ea509a83df4372099c516fd8",
            "title": "Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty",
            "abstract": "Open Information Extraction (OIE) task aims at extracting structured facts from unstructured text, typically in the form of (subject, relation, object) triples. Despite the potential of large language models (LLMs) like ChatGPT as a general task solver, they lag behind state-of-the-art (supervised) methods in OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant context from relevant relations and generate structured output due to the restrictions on fine-tuning the model. Second, LLMs generates responses autoregressively based on probability, which makes the predicted relations lack confidence. In this paper, we assess the capabilities of LLMs in improving the OIE task. Particularly, we propose various in-context learning strategies to enhance LLM's instruction-following ability and a demonstration uncertainty quantification module to enhance the confidence of the generated relations. Our experiments on three OIE benchmark datasets show that our approach holds its own against established supervised methods, both quantitatively and qualitatively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238210413",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2238385975",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "2256769706",
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2238209691",
                    "name": "Takao Osaki"
                },
                {
                    "authorId": "2057662658",
                    "name": "Katsushi Matsuda"
                },
                {
                    "authorId": "2204622281",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "6847b9658f287f430098199cd81bf26308da13f9",
            "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
            "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988575",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2182238045",
                    "name": "Can Zheng"
                },
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2123930262",
                    "name": "Tanmoy Chowdhury"
                },
                {
                    "authorId": "2110425042",
                    "name": "Yun-Qing Li"
                },
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2211987764",
                    "name": "Tian-yu Zhao"
                },
                {
                    "authorId": "2218486790",
                    "name": "Amit Panalkar"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": null,
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2204622281",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2218495127",
                    "name": "Chris White"
                },
                {
                    "authorId": "144966687",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "2188744953",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "72c4978bd4d369491040b0015e8d4f7cb7f30592",
            "title": "Hierarchical Graph Neural Networks for Causal Discovery and Root Cause Localization",
            "abstract": "In this paper, we propose REASON, a novel framework that enables the automatic discovery of both intra-level (i.e., within-network) and inter-level (i.e., across-network) causal relationships for root cause localization. REASON consists of Topological Causal Discovery and Individual Causal Discovery. The Topological Causal Discovery component aims to model the fault propagation in order to trace back to the root causes. To achieve this, we propose novel hierarchical graph neural networks to construct interdependent causal networks by modeling both intra-level and inter-level non-linear causal relations. Based on the learned interdependent causal networks, we then leverage random walks with restarts to model the network propagation of a system fault. The Individual Causal Discovery component focuses on capturing abrupt change patterns of a single system entity. This component examines the temporal patterns of each entity's metric data (i.e., time series), and estimates its likelihood of being a root cause based on the Extreme Value theory. Combining the topological and individual causal scores, the top K system entities are identified as root causes. Extensive experiments on three real-world datasets with case studies demonstrate the effectiveness and superiority of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1669829502",
                    "name": "Dongjie Wang"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2090567",
                    "name": "Jingchao Ni"
                },
                {
                    "authorId": "2069440591",
                    "name": "Liang Tong"
                },
                {
                    "authorId": "40072305",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "2274395",
                    "name": "Yanjie Fu"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                }
            ]
        },
        {
            "paperId": "75c08892179fc478f87d7020b5daff9fca4f3389",
            "title": "Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models",
            "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a \u201cchatbot\u201d, and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988349",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2182238045",
                    "name": "Can Zheng"
                },
                {
                    "authorId": "4142921",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2123930262",
                    "name": "Tanmoy Chowdhury"
                },
                {
                    "authorId": "2110425042",
                    "name": "Yun-Qing Li"
                },
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "2211987764",
                    "name": "Tian-yu Zhao"
                },
                {
                    "authorId": "2218486790",
                    "name": "Amit Panalkar"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": null,
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2204622281",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2218495127",
                    "name": "Chris White"
                },
                {
                    "authorId": "9937103",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "885792c0fe147bed2395cd4788e7007833a3e763",
            "title": "Disentangled Causal Graph Learning for Online Unsupervised Root Cause Analysis",
            "abstract": "The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault. In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on multivariate singular spectrum analysis and cumulative sum statistics. To efficiently update the RCA model, we propose an incremental disentangled causal graph learning approach to decouple the state-invariant and state-dependent information. After that, CORAL applies a random walk with restarts to the updated causal graph to accurately identify root causes. The online RCA process terminates when the causal graph and the generated root cause list converge. Extensive experiments on three real-world datasets with case studies demonstrate the effectiveness and superiority of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1669829502",
                    "name": "Dongjie Wang"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2274395",
                    "name": "Yanjie Fu"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                }
            ]
        },
        {
            "paperId": "9c499647c3bec5ab4e2a9702309c44c92326000e",
            "title": "Interdependent Causal Networks for Root Cause Localization",
            "abstract": "The goal of root cause analysis is to identify the underlying causes of system problems by discovering and analyzing the causal structure from system monitoring data. It is indispensable for maintaining the stability and robustness of large-scale complex systems. Existing methods mainly focus on the construction of a single effective isolated causal network, whereas many real-world systems are complex and exhibit interdependent structures (i.e., multiple networks of a system are interconnected by cross-network links). In interdependent networks, the malfunctioning effects of problematic system entities can propagate to other networks or different levels of system entities. Consequently, ignoring the interdependency results in suboptimal root cause analysis outcomes. In this paper, we propose REASON, a novel framework that enables the automatic discovery of both intra-level (i.e., within-network) and inter-level (i.e., across-network) causal relationships for root cause localization. REASON consists of Topological Causal Discovery (TCD) and Individual Causal Discovery (ICD). The TCD component aims to model the fault propagation in order to trace back to the root causes. To achieve this, we propose novel hierarchical graph neural networks to construct interdependent causal networks by modeling both intra-level and inter-level non-linear causal relations. Based on the learned interdependent causal networks, we then leverage random walk with restarts to model the network propagation of a system fault. The ICD component focuses on capturing abrupt change patterns of a single system entity. This component examines the temporal patterns of each entity's metric data (i.e., time series), and estimates its likelihood of being a root cause based on the Extreme Value theory. Combining the topological and individual causal scores, the top K system entities are identified as root causes. Extensive experiments on three real-world datasets validate the effectiveness of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1669829502",
                    "name": "Dongjie Wang"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2090567",
                    "name": "Jingchao Ni"
                },
                {
                    "authorId": "2227555279",
                    "name": "Liang Tong"
                },
                {
                    "authorId": "40072305",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "2274395",
                    "name": "Yanjie Fu"
                },
                {
                    "authorId": "2145225646",
                    "name": "Haifeng Chen"
                }
            ]
        },
        {
            "paperId": "c04aed08eca7ac15629213862f1aa48535a66cae",
            "title": "GLAD: Content-Aware Dynamic Graphs For Log Anomaly Detection",
            "abstract": "Logs play a crucial role in system monitoring and debugging by recording valuable system information, including events and states. Although various methods have been proposed to detect anomalies in log sequences, they often overlook the significance of considering relations among system components, such as services and users, which can be identified from log contents. Understanding these relations is vital for detecting anomalies and their underlying causes. To address this issue, we introduce GLAD, a Graph-based Log Anomaly Detection framework designed to detect relational anomalies in system logs. GLAD incorporates log semantics, relational patterns, and sequential patterns into a unified framework for anomaly detection. Specifically, GLAD first introduces a field extraction module that utilizes prompt-based few-shot learning to identify essential fields from log contents. Then GLAD constructs dynamic log graphs for sliding windows by interconnecting extracted fields and log events parsed from the log parser. These graphs represent events and fields as nodes and their relations as edges. Subsequently, GLAD utilizes a temporal-attentive graph edge anomaly detection model for identifying anomalous relations in these dynamic log graphs. This model employs a Graph Neural Network (GNN)-based encoder enhanced with transformers to capture content, structural and temporal features. We evaluate our proposed method11Our code is available at https://github.com/yul091/GraphLogAD on three datasets, and the results demonstrate the effectiveness of GLAD in detecting anomalies indicated by varying relational patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120501497",
                    "name": "Yufei Li"
                },
                {
                    "authorId": "2238385975",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "2256769706",
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "47557891",
                    "name": "Yuncong Chen"
                },
                {
                    "authorId": "3007026",
                    "name": "Wenchao Yu"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2239399824",
                    "name": "Cong Liu"
                }
            ]
        },
        {
            "paperId": "f8de328be46416974b028284b77c1b318f35ca81",
            "title": "Incremental Causal Graph Learning for Online Root Cause Analysis",
            "abstract": "The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault. In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on multivariate singular spectrum analysis and cumulative sum statistics. To efficiently update the RCA model, we propose an incremental disentangled causal graph learning approach to decouple the state-invariant and state-dependent information. After that, CORAL applies a random walk with restarts to the updated causal graph to accurately identify root causes. The online RCA process terminates when the causal graph and the generated root cause list converge. Extensive experiments on three real-world datasets demonstrate the effectiveness and superiority of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1669829502",
                    "name": "Dongjie Wang"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2274395",
                    "name": "Yanjie Fu"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "2145225646",
                    "name": "Haifeng Chen"
                }
            ]
        },
        {
            "paperId": "1ae600e7a24da0e6914efaaa75ff12e9d6e21208",
            "title": "Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings",
            "abstract": "Learning fine-grained embeddings is essential for extending the generalizability of models pre-trained on \u201ccoarse\u201d labels (e.g., animals). It is crucial to fields for which fine-grained labeling (e.g., breeds of animals) is expensive, but fine-grained prediction is desirable, such as medicine. The dilemma necessitates adaptation of a \u201ccoarsely\u201d pre-trained model to new tasks with a few \u201cfiner-grained\u201d training labels. However, coarsely supervised pre-training tends to suppress intra-class variation, which is vital for cross-granularity adaptation. In this paper, we develop a training framework underlain by a novel superclass-conditional Gaussian mixture model (SCGM). SCGM imitates the generative process of samples from hierarchies of classes through latent variable modeling of the fine-grained subclasses. The framework is agnostic to the encoders and only adds a few distribution related parameters, thus is efficient, and flexible to different domains. The model parameters are learned end-to-end by maximum-likelihood estimation via a principled Expectation-Maximization algorithm. Extensive experiments on benchmark datasets and a real-life medical dataset indicate the effectiveness of our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2090567",
                    "name": "Jingchao Ni"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2808421",
                    "name": "T. Asakura"
                },
                {
                    "authorId": "2059027847",
                    "name": "T. Soma"
                },
                {
                    "authorId": "2181925397",
                    "name": "Sho Kato"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                }
            ]
        },
        {
            "paperId": "57b5a5c8890c5cdd8ed7351c2247eea649c38d84",
            "title": "Towards Learning Disentangled Representations for Time Series",
            "abstract": "Promising progress has been made toward learning efficient time series representations in recent years, but the learned representations often lack interpretability and do not encode semantic meanings by the complex interactions of many latent factors. Learning representations that disentangle these latent factors can bring semantic-rich representations of time series and further enhance interpretability. However, directly adopting the sequential models, such as Long Short-Term Memory Variational AutoEncoder (LSTM-VAE), would encounter a Kullback?Leibler (KL) vanishing problem: the LSTM decoder often generates sequential data without efficiently using latent representations, and the latent spaces sometimes could even be independent of the observation space. And traditional disentanglement methods may intensify the trend of KL vanishing along with the disentanglement process, because they tend to penalize the mutual information between the latent space and the observations. In this paper, we propose Disentangle Time-Series, a novel disentanglement enhancement framework for time series data. Our framework achieves multi-level disentanglement by covering both individual latent factors and group semantic segments. We propose augmenting the original VAE objective by decomposing the evidence lower-bound and extracting evidence linking factorial representations to disentanglement. Additionally, we introduce a mutual information maximization term between the observation space to the latent space to alleviate the KL vanishing problem while preserving the disentanglement property. Experimental results on five real-world IoT datasets demonstrate that the representations learned by DTS achieve superior performance in various tasks with better interpretability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144462862",
                    "name": "Yuening Li"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "1759658",
                    "name": "D. Zha"
                },
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2090567",
                    "name": "Jingchao Ni"
                },
                {
                    "authorId": "2109953207",
                    "name": "Denghui Zhang"
                },
                {
                    "authorId": "2145225543",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2109724398",
                    "name": "Xia Hu"
                }
            ]
        }
    ]
}