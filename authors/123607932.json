{
    "authorId": "123607932",
    "papers": [
        {
            "paperId": "1196cd4aa938a34105755feb47ce1610b58ea5de",
            "title": "BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning",
            "abstract": "Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment. In this paper, we present a novel generative model, BindGPT which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site. Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step. We pretrain BindGPT on a large-scale dataset and fine-tune it with reinforcement learning using scores from external simulation software. We demonstrate how a single pretrained language model can serve at the same time as a 3D molecular generative model, conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator. Notably, the model does not make any representational equivariance assumptions about the domain of generation. We show how such simple conceptual approach combined with pretraining and scaling can perform on par or better than the current best specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1416761815",
                    "name": "Artem Zholus"
                },
                {
                    "authorId": "2065397580",
                    "name": "Maksim Kuznetsov"
                },
                {
                    "authorId": "2298441287",
                    "name": "Roman Schutski"
                },
                {
                    "authorId": "2021173899",
                    "name": "Shayakhmetov Rim"
                },
                {
                    "authorId": "36061519",
                    "name": "Daniil Polykovskiy"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                },
                {
                    "authorId": "2264824500",
                    "name": "Alex Zhavoronkov"
                }
            ]
        },
        {
            "paperId": "1463eb69e412ee1e44bfcb75e666cfc1dc0ba3fc",
            "title": "Intelligent Switching for Reset-Free RL",
            "abstract": "In the real world, the strong episode resetting mechanisms that are needed to train agents in simulation are unavailable. The \\textit{resetting} assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional handcrafted mechanisms or human interventions. Recent work aims to train agents (\\textit{forward}) with learned resets by constructing a second (\\textit{backward}) agent that returns the forward agent to the initial state. We find that the termination and timing of the transitions between these two agents are crucial for algorithm success. With this in mind, we create a new algorithm, Reset Free RL with Intelligently Switching Controller (RISC) which intelligently switches between the two agents based on the agent's confidence in achieving its current goal. Our new method achieves state-of-the-art performance on several challenging environments for reset-free RL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111849117",
                    "name": "Darshan Patil"
                },
                {
                    "authorId": "10197529",
                    "name": "Janarthanan Rajendran"
                },
                {
                    "authorId": "2994035",
                    "name": "G. Berseth"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ]
        },
        {
            "paperId": "1fb76e69ee4180204f9480853abc8c7bc5d4ddcf",
            "title": "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models",
            "abstract": "Despite their widespread adoption, large language models (LLMs) remain prohibitive to use under resource constraints, with their ever growing sizes only increasing the barrier for use. One noted issue is the high latency associated with auto-regressive generation, rendering large LLMs use dependent on advanced computing infrastructure. Assisted decoding, where a smaller draft model guides a larger target model's generation, has helped alleviate this, but remains dependent on alignment between the two models. Thus if the draft model is insufficiently capable on some domain relative to the target model, performance can degrade. Alternatively, one can leverage multiple draft models to better cover the expertise of the target, but when multiple black-box draft models are available, selecting an assistant without details about its construction can be difficult. To better understand this decision making problem, we observe it as a contextual bandit, where a policy must choose a draft model based on a context. We show that even without prior knowledge of the draft models, creating an offline dataset from only outputs of independent draft/target models and training a policy over the alignment of these outputs can accelerate performance on multiple domains provided the candidates are effective. Further results show this to hold on various settings with multiple assisted decoding candidates, highlighting its flexibility and the advantageous role that such decision making can play.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223971135",
                    "name": "Jerry Huang"
                },
                {
                    "authorId": "32899078",
                    "name": "Prasanna Parthasarathi"
                },
                {
                    "authorId": "2066076226",
                    "name": "Mehdi Rezagholizadeh"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ]
        },
        {
            "paperId": "2ad2bded5db34ab49c7d2a84e8d162b9707b56e8",
            "title": "Exploring Quantization for Efficient Pre-Training of Transformer Language Models",
            "abstract": "The increasing scale of Transformer models has led to an increase in their pre-training computational requirements. While quantization has proven to be effective after pre-training and during fine-tuning, applying quantization in Transformers during pre-training has remained largely unexplored at scale for language modeling. This study aims to explore the impact of quantization for efficient pre-training of Transformers, with a focus on linear layer components. By systematically applying straightforward linear quantization to weights, activations, gradients, and optimizer states, we assess its effects on model efficiency, stability, and performance during training. By offering a comprehensive recipe of effective quantization strategies to be applied during the pre-training of Transformers, we promote high training efficiency from scratch while retaining language modeling ability. Code is available at https://github.com/chandar-lab/EfficientLLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1598457720",
                    "name": "Kamran Chitsaz"
                },
                {
                    "authorId": "2303408438",
                    "name": "Quentin Fournier"
                },
                {
                    "authorId": "2311505548",
                    "name": "Gonccalo Mordido"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ]
        },
        {
            "paperId": "3937b11717c22f62ab0b48dfd89e5dab75cedf40",
            "title": "Are self-explanations from Large Language Models faithful?",
            "abstract": "Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152446182",
                    "name": "Andreas Madsen"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                },
                {
                    "authorId": "145732771",
                    "name": "Siva Reddy"
                }
            ]
        },
        {
            "paperId": "4bb13c7bd9ea892f09f1c891faa93fc8d0dbf026",
            "title": "Predicting the Impact of Model Expansion through the Minima Manifold: A Loss Landscape Perspective",
            "abstract": "The optimal model for a given task is often challenging to determine, requiring training multiple models from scratch which becomes prohibitive as dataset and model sizes grow. A more efficient alternative is to reuse smaller pre-trained models by expanding them, however, this is not widely adopted as how this impacts training dynamics remains poorly understood. While prior works have introduced statistics to measure these effects, they remain flawed. To rectify this, we offer a new approach for understanding and quantifying the impact of expansion through the lens of the loss landscape, which has been shown to contain a manifold of linearly connected minima. Building on this new perspective, we propose a metric to study the impact of expansion by estimating the size of the manifold. Experimental results show a clear relationship between gains in performance and manifold size, enabling the comparison of candidate models and presenting a first step towards expanding models more reliably based on geometric properties of the loss landscape.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51437465",
                    "name": "Pranshu Malviya"
                },
                {
                    "authorId": "2223971135",
                    "name": "Jerry Huang"
                },
                {
                    "authorId": "2303408438",
                    "name": "Quentin Fournier"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ]
        },
        {
            "paperId": "570929aad9390837a8d0664cc274e818c6204655",
            "title": "Sub-goal Distillation: A Method to Improve Small Language Agents",
            "abstract": "While Large Language Models (LLMs) have demonstrated significant promise as agents in interactive tasks, their substantial computational requirements and restricted number of calls constrain their practical utility, especially in long-horizon interactive tasks such as decision-making or in scenarios involving continuous ongoing tasks. To address these constraints, we propose a method for transferring the performance of an LLM with billions of parameters to a much smaller language model (770M parameters). Our approach involves constructing a hierarchical agent comprising a planning module, which learns through Knowledge Distillation from an LLM to generate sub-goals, and an execution module, which learns to accomplish these sub-goals using elementary actions. In detail, we leverage an LLM to annotate an oracle path with a sequence of sub-goals towards completing a goal. Subsequently, we utilize this annotated data to fine-tune both the planning and execution modules. Importantly, neither module relies on real-time access to an LLM during inference, significantly reducing the overall cost associated with LLM interactions to a fixed cost. In ScienceWorld, a challenging and multi-task interactive text environment, our method surpasses standard imitation learning based solely on elementary actions by 16.7% (absolute). Our analysis highlights the efficiency of our approach compared to other LLM-based methods. Our code and annotated data for distillation can be found on GitHub.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2300090442",
                    "name": "Maryam Hashemzadeh"
                },
                {
                    "authorId": "2281825070",
                    "name": "Elias Stengel-Eskin"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                },
                {
                    "authorId": "2269049987",
                    "name": "Marc-Alexandre C\u00f4t\u00e9"
                }
            ]
        },
        {
            "paperId": "5a93e89d01dc3ba61a3bd2a3ac64aa895859f471",
            "title": "Interpretability Needs a New Paradigm",
            "abstract": "Interpretability is the study of explaining models in understandable terms to humans. At present, interpretability is divided into two paradigms: the intrinsic paradigm, which believes that only models designed to be explained can be explained, and the post-hoc paradigm, which believes that black-box models can be explained. At the core of this debate is how each paradigm ensures its explanations are faithful, i.e., true to the model's behavior. This is important, as false but convincing explanations lead to unsupported confidence in artificial intelligence (AI), which can be dangerous. This paper's position is that we should think about new paradigms while staying vigilant regarding faithfulness. First, by examining the history of paradigms in science, we see that paradigms are constantly evolving. Then, by examining the current paradigms, we can understand their underlying beliefs, the value they bring, and their limitations. Finally, this paper presents 3 emerging paradigms for interpretability. The first paradigm designs models such that faithfulness can be easily measured. Another optimizes models such that explanations become faithful. The last paradigm proposes to develop models that produce both a prediction and an explanation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "152446182",
                    "name": "Andreas Madsen"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                },
                {
                    "authorId": "145732771",
                    "name": "Siva Reddy"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ]
        },
        {
            "paperId": "6b4d67a3ef12ffda9b1f98b0f059c39b35ab9086",
            "title": "MVP: Minimal Viable Phrase for Long Text Understanding",
            "abstract": "A recent renewal in interest in long text understanding has sparked the emergence of high-quality long text benchmarks, as well as new models demonstrating significant performance improvements on these benchmarks. However, gauging the implication of these advancements based solely on the length of the input text offers limited insight. Such benchmarks may require models to parse long-range dependencies or merely to locate and comprehend the relevant paragraph within a longer text. This work introduces the Minimal Viable Phrase (MVP), a novel metric that determines, through perturbations to the input text, the shortest average text length that needs to be preserved to execute the task with limited performance degradation. Our evaluation of the popular SCROLLS benchmark reveals that only one of its seven tasks necessitates an MVP of over 512 tokens\u2013the maximum text length manageable by the previous generation of pre-trained models. We highlight the limited need for understanding long-range dependencies in resolving these tasks, discuss the specific design decisions that seem to have led to the QuALITY task requiring reliance on long-range dependencies to be solved, and point out specific modeling choices that seem to outperform on the QuALITY task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66684367",
                    "name": "Louis Clou\u00e2tre"
                },
                {
                    "authorId": "2301579793",
                    "name": "Amal Zouaq"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ]
        },
        {
            "paperId": "6ba6da5c9c2929f0912728b27dea0853f0c402ce",
            "title": "Toward Debugging Deep Reinforcement Learning Programs with RLExplorer",
            "abstract": "Deep reinforcement learning (DRL) has shown success in diverse domains such as robotics, computer games, and recommendation systems. However, like any other software system, DRL-based software systems are susceptible to faults that pose unique challenges for debugging and diagnosing. These faults often result in unexpected behavior without explicit failures and error messages, making debugging difficult and time-consuming. Therefore, automating the monitoring and diagnosis of DRL systems is crucial to alleviate the burden on developers. In this paper, we propose RLExplorer, the first fault diagnosis approach for DRL-based software systems. RLExplorer automatically monitors training traces and runs diagnosis routines based on properties of the DRL learning dynamics to detect the occurrence of DRL-specific faults. It then logs the results of these diagnoses as warnings that cover theoretical concepts, recommended practices, and potential solutions to the identified faults. We conducted two sets of evaluations to assess RLExplorer. Our first evaluation of faulty DRL samples from Stack Overflow revealed that our approach can effectively diagnose real faults in 83% of the cases. Our second evaluation of RLExplorer with 15 DRL experts/developers showed that (1) RLExplorer could identify 3.6 times more defects than manual debugging and (2) RLExplorer is easily integrated into DRL applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164417937",
                    "name": "Rached Bouchoucha"
                },
                {
                    "authorId": "2174027925",
                    "name": "Ahmed Haj Yahmed"
                },
                {
                    "authorId": "2111849117",
                    "name": "Darshan Patil"
                },
                {
                    "authorId": "10197529",
                    "name": "Janarthanan Rajendran"
                },
                {
                    "authorId": "2076564",
                    "name": "Amin Nikanjam"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                },
                {
                    "authorId": "1703493",
                    "name": "Foutse Khomh"
                }
            ]
        }
    ]
}