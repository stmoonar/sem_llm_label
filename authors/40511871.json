{
    "authorId": "40511871",
    "papers": [
        {
            "paperId": "01911d5badfd2397fba9f0b71c7873071c392b33",
            "title": "HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent",
            "abstract": "Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains. Our work has the following contributions:(1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a detailed recipe for the data generation procedure along with data analysis and human evaluations. The data generation pipeline is transferrable and can be easily adapted for labeled conversation data generation in other domains. (3) The proposed data-collection pipeline is mostly based on LLMs with minimal human involvement for annotation, which is time and cost-efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2282498869",
                    "name": "Zicheng Huang"
                },
                {
                    "authorId": "2261645232",
                    "name": "Wenxiang Hu"
                },
                {
                    "authorId": "2288381545",
                    "name": "Xi Fang"
                },
                {
                    "authorId": "2282467353",
                    "name": "Rajesh Kumar Cherukuri"
                },
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "2282467375",
                    "name": "Lorenzo Malandri"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                }
            ]
        },
        {
            "paperId": "1e73f6f927997b7f86606a77ff9795d77948a53d",
            "title": "On Regret-Optimal Learning in Decentralized Multiplayer Multiarmed Bandits",
            "abstract": "We consider the problem of learning in single-player and multiplayer multiarmed bandit models. Bandit problems are classes of online learning problems that capture exploration versus exploitation tradeoffs. In a multiarmed bandit model, players can pick among many arms, and each play of an arm generates an i.i.d. reward from an unknown distribution. The objective is to design a policy that maximizes the expected reward over a time horizon for a single-player setting and the sum of expected rewards for the multiplayer setting. In the multiplayer setting, arms may give different rewards to different players. There is no separate channel for coordination among the players. Any attempt at communication is costly and adds to regret. We propose two decentralizable policies, <inline-formula> <tex-math notation=\"LaTeX\">$\\tt E^3$</tex-math></inline-formula> (<inline-formula><tex-math notation=\"LaTeX\">$\\tt E$ </tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\">$\\tt cubed$</tex-math></inline-formula>) and <inline-formula><tex-math notation=\"LaTeX\">$\\tt E^3$</tex-math></inline-formula>-<inline-formula> <tex-math notation=\"LaTeX\">$\\tt TS$</tex-math></inline-formula>, that can be used in both single-player and multiplayer settings. These policies are shown to yield expected regret that grows, at most, as <inline-formula> <tex-math notation=\"LaTeX\">$O(\\log ^{1+\\delta } T)$</tex-math></inline-formula> (and <inline-formula> <tex-math notation=\"LaTeX\">$O(\\log T)$</tex-math></inline-formula> under some assumption). It is well known that <inline-formula><tex-math notation=\"LaTeX\">$O(\\log T)$</tex-math></inline-formula> is the lower bound on the rate of growth of regret even in a centralized case. The proposed algorithms improve on prior work where regret grew at <inline-formula><tex-math notation=\"LaTeX\">$O(\\log ^2 T)$</tex-math></inline-formula>. More fundamentally, these policies address the question of additional cost incurred in decentralized online learning, suggesting that there is, at most, an <inline-formula><tex-math notation=\"LaTeX\">$\\delta$</tex-math></inline-formula>-factor cost in terms of the order of regret. This solves a problem of relevance in many domains and had been open for a while.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "1956081",
                    "name": "D. Kalathil"
                },
                {
                    "authorId": "33495368",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "95eb582a0bb28bcd489a711042e9334b6a545c16",
            "title": "Decentralized Learning for Multiplayer Multiarmed Bandits",
            "abstract": "We consider the problem of distributed online learning with multiple players in multiarmed bandit (MAB) models. Each player can pick among multiple arms. When a player picks an arm, it gets a reward. We consider both independent identically distributed (i.i.d.) reward model and Markovian reward model. In the i.i.d. model, each arm is modeled as an i.i.d. process with an unknown distribution with an unknown mean. In the Markovian model, each arm is modeled as a finite, irreducible, aperiodic and reversible Markov chain with an unknown probability transition matrix and stationary distribution. The arms give different rewards to different players. If two players pick the same arm, there is a collision, and neither of them get any reward. There is no dedicated control channel for coordination or communication among the players. Any other communication between the users is costly and will add to the regret. We propose an online index-based distributed learning policy called dUCB4 algorithm that trades off exploration versus exploitation in the right way, and achieves expected regret that grows at most as near- O(log2T). The motivation comes from opportunistic spectrum access by multiple secondary users in cognitive radio networks wherein they must pick among various wireless channels that look different to different users. This is the first distributed learning algorithm for multiplayer MABs with heterogeneous players (that have player-dependent rewards) to the best of our knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1956081",
                    "name": "D. Kalathil"
                },
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "33495368",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "149dfd75984d01e2d68fc44d0f728c087b6eaea0",
            "title": "Optimal decentralized control in unidirectional one-step delayed sharing pattern with partial output feedback",
            "abstract": "We consider optimal decentralized LQG control for a plant with nested structure controlled by two players receiving partial output observations from the plant. A unidirectional one-step delayed error-free communication channel is assumed to exist between the two players, i.e., there is a unit delay in information transmission in one direction and no transmission in the other direction. One of the players has access to a subset of state variables while the other can only observe a noisy output. We show that the problem has a partially nested information structure, and thus a linear optimal control law exists. Using summary statistics, the computation of the optimal law is made tractable and a deterministic convex optimization problem is formulated whose solution yields the optimal control law.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "1956081",
                    "name": "D. Kalathil"
                },
                {
                    "authorId": "33495368",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "b1004cb10e92a073d24f8fa81dcb6c3353014ee2",
            "title": "Optimal Decentralized Control With Asymmetric One-Step Delayed Information Sharing",
            "abstract": "We consider optimal control of decentralized LQG problems for plants having nested subsystems controlled by two players with asymmetric information sharing patterns between them. In the main scenario, the players are assumed to have a unidirectional error-free, unlimited-rate communication channel with a unit delay in one direction and no communication in the other. A second model, presented for completeness, considers a channel with no delay in one direction and a unit delay in the other. Delayed information-sharing patterns do not, in general, admit linear optimal control laws and are thus difficult to control optimally. However, in these scenarios, we show that the problems have a partially nested information structure and, thus, linear optimal control laws exist. Summary statistics are identified and analytical solutions to the optimal control laws are derived. State and output feedback cases are solved for both scenarios.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "1956081",
                    "name": "D. Kalathil"
                },
                {
                    "authorId": "33495368",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "74dd67021bbec8dcdf979cba356b9d2f8235fc89",
            "title": "Decentralized learning for multi-player multi-armed bandits",
            "abstract": "We consider the problem of distributed online learning with multiple players in multi-armed bandit models. Each player can pick among multiple arms. As a player picks an arm, it gets a reward from an unknown distribution with an unknown mean. The arms give different rewards to different players. If two players pick the same arm, there is a \u201ccollision\u201d, and neither of them get any reward. There is no dedicated control channel for coordination or communication among the players. Any other communication between the users is costly and will add to the regret. We propose an online index-based learning policy called dUCB4 algorithm that trades off exploration v. exploitation in the right way, and achieves expected regret that grows at most near-O(log2 T). The motivation comes from opportunistic spectrum access by multiple secondary users in cognitive radio networks wherein they must pick among various wireless channels that look different to different users.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1956081",
                    "name": "D. Kalathil"
                },
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "33495368",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "d44c32e60009b6cc1a79ed89fe0bf95196e00da7",
            "title": "Multi-player multi-armed bandits: Decentralized learning with IID rewards",
            "abstract": "We consider the decentralized multi-armed bandit problem with distinct arms for each players. Each player can pick one arm at each time instant and can get a random reward from an unknown distribution with an unknown mean. The arms give different rewards to different players. If more than one player select the same arm, everyone gets a zero reward. There is no dedicated control channel for communication or coordination among the user. We propose an online learning algorithm called dUCB4 which achieves a near-O(log2 T). The motivation comes from opportunistic spectrum access by multiple secondary users in cognitive radio networks wherein they must pick among various wireless channels that look different to different users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1956081",
                    "name": "D. Kalathil"
                },
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "33495368",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "ac4561c92f0f724bbae278e82c14b4d862b1e4f0",
            "title": "On a restless multi-armed bandit problem with non-identical arms",
            "abstract": "We consider the following learning problem motivated by opportunistic spectrum access in cognitive radio networks. There are N independent Gilbert-Elliott channels with possibly non-identical transition matrices. It is desired to have an online policy to maximize the long-term expected discounted reward from accessing one channel at each time dynamically. While there is a stream of recent results on this problem when the channels are identical, much less is known for the harder case of non-identical channels. We provide the first characterization of the structure of the optimal policy for this problem when the channels can be non-identical, in the Bayesian case (when the transition matrices are known). We also provide the first provably efficient learning algorithm for a non-Bayesian version of this problem (when the transition matrices are unknown). Specifically, for the special case of two positively correlated channels, we use the structure we identify to develop a novel mapping to a different multi-armed bandit with countably-infinite arms, in which each arm corresponds to a threshold-based policy. Using this mapping, we propose a policy that achieves near-logarithmic regret for this problem with respect to an \u220a-optimal solution.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "3171751",
                    "name": "Yi Gai"
                },
                {
                    "authorId": "1701475",
                    "name": "B. Krishnamachari"
                }
            ]
        }
    ]
}