{
    "authorId": "145948198",
    "papers": [
        {
            "paperId": "0c735ab7312686cdf2090eb55cc7e5c2d82ffb37",
            "title": "Information Operations in Turkey: Manufacturing Resilience with Free Twitter Accounts",
            "abstract": "Following the 2016 US elections Twitter launched their Information Operations (IO) hub where they archive account activity connected to state linked information operations. In June 2020, Twitter took down and released a set of accounts linked to Turkey's ruling political party (AKP). We investigate these accounts in the aftermath of the takedown to explore whether AKP-linked operations are ongoing and to understand the strategies they use to remain resilient to disruption. We collect live accounts that appear to be part of the same network, ~30% of which have been suspended by Twitter since our collection. We create a BERT-based classifier that shows similarity between these two networks, develop a taxonomy to categorize these accounts, find direct sequel accounts between the Turkish takedown and the live accounts, and find evidence that Turkish IO actors deliberately construct their network to withstand large-scale shutdown by utilizing explicit and implicit signals of coordination. We compare our findings from the Turkish operation to Russian and Chinese IO on Twitter and find that Turkey's IO utilizes a unique group structure to remain resilient. Our work highlights the fundamental imbalance between IO actors quickly and easily creating free accounts and the social media platforms spending significant resources on detection and removal, and contributes novel findings about Turkish IO on Twitter.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1610968726",
                    "name": "Maya Merhi"
                },
                {
                    "authorId": "2528276",
                    "name": "S. Rajtmajer"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "1718384deb5a0298c31b902fff4e0caba6aaf298",
            "title": "Adversarial Socialbot Learning via Multi-Agent Deep Hierarchical Reinforcement Learning",
            "abstract": "Socialbots are software-driven user accounts on social platforms, acting autonomously (mimicking human behavior), with the aims to influence the opinions of other users or spread targeted misinfor-mation for particular goals. As socialbots undermine the ecosystem of social platforms, they are often considered harmful. As such, there have been several computational efforts to auto-detect the socialbots. However, to our best knowledge, the adversarial nature of these socialbots has not yet been studied. This begs a question \u201ccan adversaries, controlling socialbots, exploit AI techniques to their advantage?\" To this question, we successfully demonstrate that indeed it is possible for adversaries to exploit computational learning mechanism such as reinforcement learning (RL) to maximize the influence of socialbots while avoiding being detected. We first formulate the adversarial socialbot learning as a cooperative game between two functional hierarchical RL agents. While one agent curates a sequence of activities that can avoid the detection, the other agent aims to maximize network influence by selectively connecting with right users. Our proposed policy networks train with a vast amount of synthetic graphs and generalize better than baselines on unseen real-life graphs both in terms of maximizing network influence (up to +18%) and sustainable stealthiness (up to +40% undetectability) under a strong bot detector (with 90% detection accuracy). During inference, the complexity of our approach scales linearly, independent of a network\u2019s structure and the virality of news. This makes our approach a practical adversarial attack when deployed in a real-life setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "1404639619",
                    "name": "Long Tran-Thanh"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "1f4488575651ede537c5d15edc184dbcdc06a551",
            "title": "MASCOT: A Quantization Framework for Efficient Matrix Factorization in Recommender Systems",
            "abstract": "In recent years, quantization methods have successfully accelerated the training of large deep neural network (DNN) models by reducing the level of precision in computing operations (e.g., forward/backward passes) without sacrificing its accuracy. In this work, therefore, we attempt to apply such a quantization idea to the popular Matrix factorization (MF) methods to deal with the growing scale of models and datasets in recommender systems. However, to our dismay, we observe that the state-of-the-art quantization methods are not effective in the training of MF models, unlike their successes in the training of DNN models. To this phenomenon, we posit that two distinctive features in training MF models could explain the difference: (i) the training of MF models is much more memory-intensive than that of DNN models, and (ii) the quantization errors across users and items in recommendation are not uniform. From these observations, we develop a quantization framework for MF models, named MASCOT, employing novel strategies (i.e., m-quantization and g-switching) to successfully address the aforementioned limitations of quantization in the training of MF models. The comprehensive evaluation using four real-world datasets demonstrates that MASCOT improves the training performance of MF models by about 45%, compared to the training without quantization, while maintaining low model errors, and the strategies and implementation optimizations of MASCOT are quite effective in the training of MF models. For the detailed information about MASCOT, we release the code of MASCOT and the datasets at: https://github.com/Yujaeseo/lCDM-2021_MASCOT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3373303",
                    "name": "Yunyong Ko"
                },
                {
                    "authorId": "2135620775",
                    "name": "Jae-Seo Yu"
                },
                {
                    "authorId": "123063050",
                    "name": "Hong-Kyun Bae"
                },
                {
                    "authorId": "2145791785",
                    "name": "Y. Park"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2155626647",
                    "name": "Sang-Wook Kim"
                }
            ]
        },
        {
            "paperId": "3152af9509b4dfe865acbda19ec74bd0b97686b8",
            "title": "WILSON: A Divide and Conquer Approach for Fast and Effective News Timeline Summarization",
            "abstract": "Major news media frequently uses the method of news timeline summarization to summarize important daily news over major events across the timeline. While various sophisticated meth-ods have been proposed to generate both concise and complete news timelines, in practice, generating timelines from a large number of news articles not only faces quality issues but also encounters the challenge of generation speed, which all existing methods have neglected. To mitigate these issues, in this work, we propose to speed up timeline generation by dividing the whole summarization task into sub-summarization tasks, adopting the \u201cdivide and conquer\" philosophy: (1) date selection and (2) text summarization. Furthermore, since existing methods in news timeline summarization pay less attention to the date selection than text summarization, in this paper, we re-examine the role of date selection in news timeline summarization and demonstrate that accurate date selection \u201calone\" can significantly contribute to the task of news timeline summarization. Leveraging on the explicit date selection, then, we propose a simple yet fast and effective news time-line summarization method, named WILSON (neWs tImeLine SummarizatiON). Experimented on two widely used timeline summarization benchmark datasets, timeline17 and crisis , empirical evaluation shows that WILSON outperforms state-of-the-art approaches in both speed and ROUGE scores, significantly improving ROUGE-2 F1 scores by 9 . 5% \u223c 17 . 7% and reducing generation time by two orders of magnitude. A further user study with professional journalists also validates the superiority of WILSON. Finally, we build a real-time news timeline summarization system and achieve encouraging results on an industrial-level corpus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50325598",
                    "name": "Yiming Liao"
                },
                {
                    "authorId": "50695187",
                    "name": "Shuguang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "3c8b76b8d28c4787222dfbb916df88339ab46bf6",
            "title": "Look Before You Leap: Confirming Edge Signs in Random Walk with Restart for Personalized Node Ranking in Signed Networks",
            "abstract": "In this paper, we address the personalized node ranking (PNR) problem for signed networks, which aims to rank nodes in an order most relevant to a given seed node in a signed network. The recently-proposed PNR methods introduce the concept of the signed random surfer, denoted as SRSurfer, that performs the score propagation between nodes using the balance theory. However, in real settings of signed networks, edge relationships often do not strictly follow the rules of the balance theory. Therefore, SRSurfer-based PNR methods frequently perform incorrect score propagation to nodes, thereby degrading the accuracy of PNR. To address this limitation, we propose a novel random-walk based PNR approach with sign verification, named as OBOE (lOok Before yOu lEap). Specifically, OBOE carefully verifies the score propagation of SRSurfer by using the topological features of nodes. Then, OBOE corrects all incorrect score propagation cases by exploiting the statistics of a given network. The experiments on 3 real-world signed networks show that OBOE consistently and significantly outperforms 5 competing methods with improvement up to 13%, 95%, and 249% in top-k PNR, bottom-k PNR, and troll identification tasks, respectively. All OBOE codes and datasets are available at: http://github.com/wonchang24/OBOE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153487132",
                    "name": "Wonchang Lee"
                },
                {
                    "authorId": "2553299",
                    "name": "Yeon-Chang Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "51967113",
                    "name": "Sang-Wook Kim"
                }
            ]
        },
        {
            "paperId": "7845bfb55f5ce573b87d77bb76d4d38829b37620",
            "title": "TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation",
            "abstract": "Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called\"Turing Test\"problem for neural text generation methods. In this work, we present the TuringBench benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels {Human, GPT-1, GPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3, GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large, FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two benchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and (3) a website with leaderboards. Our preliminary experimental results using TuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art TT detection models. The TuringBench is available at: https://turingbench.ist.psu.edu/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "2115851910",
                    "name": "Zeyu Ma"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "144142354",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "7ca96c9988a5bc199074272402d834959f3db588",
            "title": "Large-Scale Data-Driven Airline Market Influence Maximization",
            "abstract": "We present a prediction-driven optimization framework to maximize the market influence in the US domestic air passenger transportation market by adjusting flight frequencies. At the lower level, our neural networks consider a wide variety of features, such as classical air carrier performance features and transportation network features, to predict the market influence. On top of the prediction models, we define a budget-constrained flight frequency optimization problem to maximize the market influence over 2,262 routes. This problem falls into the category of the non-linear optimization problem, which cannot be solved exactly by conventional methods. To this end, we present a novel adaptive gradient ascent (AGA) method. Our prediction models show two to eleven times better accuracy in terms of the median root-mean-square error (RMSE) over baselines. In addition, our AGA optimization method runs 690 times faster with a better optimization result (in one of our largest scale experiments) than a greedy algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51239375",
                    "name": "Duanshun Li"
                },
                {
                    "authorId": "144979786",
                    "name": "Jing Liu"
                },
                {
                    "authorId": "2106414808",
                    "name": "Jinsung Jeon"
                },
                {
                    "authorId": "2111503084",
                    "name": "Seoyoung Hong"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "5166698",
                    "name": "Noseong Park"
                }
            ]
        },
        {
            "paperId": "80c9802fce15df2c1c80185377b9de16ff0130e1",
            "title": "A Unified Framework of Homomorphic Encryption for Multiple Parties with Non-Interactive Setup",
            "abstract": ". Homomorphic Encryption (HE), \ufb01rst constructed in 2009, is a class of encryption schemes that enables computation over encrypted data. Variants of HE in the context of multiple parties have led to the development of two di\ufb00erent lines of HE schemes \u2013 Multi-Party Homomorphic Encryption (MPHE) and Multi-Key Homomorphic Encryption (MKHE). These primitives cater to di\ufb00erent applications and each approach has its own pros and cons. At a high level, MPHE schemes tend to be much more e\ufb03cient but require the set of computing parties to be \ufb01xed throughout the entire operation, frequently a limiting assumption. On the other hand, MKHE schemes tend to have poor scaling (quadratic) with the number of parties but allow us to add new parties to the joint computation anytime since they support computation between ciphertexts under di\ufb00erent keys. In this work, we formalize a new variant of HE called Multi-Group Homomorphic Encryption (MGHE). Stated informally, an MGHE scheme provides a seamless integration between MPHE and MKHE, and combines the best of both these primitives. In an MGHE scheme, a group of parties generates a public key jointly which results in the compact ciphertexts and e\ufb03cient homomorphic operations, similar to MPHE. However, unlike MPHE, it also supports computations on encrypted data under di\ufb00erent keys, a property enjoyed by MKHE schemes. We provide a concrete construction of such an MGHE scheme from the BFV scheme. The public key generation procedure of our scheme is fully non-interactive so that the set of computing parties does not have to be determined and no information about other parties is needed in advance of individual key generation. At the heart of our construction is a novel refactoring of the relinearization key to avoid interaction as typically needed. We also implement our scheme and demonstrate that the this generalization does not incur any additional overhead and in fact, can be more performant than existing MPHE and MKHE schemes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087057270",
                    "name": "Hyesun Kwak"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "11977411",
                    "name": "Yongsoo Song"
                },
                {
                    "authorId": "21287668",
                    "name": "Sameer Wagh"
                }
            ]
        },
        {
            "paperId": "84b724d182d0452d92681f21857048bc0547326e",
            "title": "Does Clickbait Actually Attract More Clicks? Three Clickbait Studies You Must Read",
            "abstract": "Studies show that users do not reliably click more often on headlines classified as clickbait by automated classifiers. Is this because the linguistic criteria (e.g., use of lists or questions) emphasized by the classifiers are not psychologically relevant in attracting interest, or because their classifications are confounded by other unknown factors associated with assumptions of the classifiers? We address these possibilities with three studies\u2014a quasi-experiment using headlines classified as clickbait by three machine-learning models (Study 1), a controlled experiment varying the headline of an identical news story to contain only one clickbait characteristic (Study 2), and a computational analysis of four classifiers using real-world sharing data (Study 3). Studies 1 and 2 revealed that clickbait did not generate more curiosity than non-clickbait. Study 3 revealed that while some headlines generate more engagement, the detectors agreed on a classification only 47% of the time, raising fundamental questions about their validity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2089408593",
                    "name": "Maria D. Molina"
                },
                {
                    "authorId": "47493614",
                    "name": "S. Sundar"
                },
                {
                    "authorId": "9949334",
                    "name": "Md Main Uddin Rony"
                },
                {
                    "authorId": "2789540",
                    "name": "Naeemul Hassan"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "a21a6a16da84af05738fcabb6b3c994ff6371d89",
            "title": "ALADDIN: Asymmetric Centralized Training for Distributed Deep Learning",
            "abstract": "To speed up the training of massive deep neural network (DNN) models, distributed training has been widely studied. In general, a centralized training, a type of distributed training, suffers from the communication bottleneck between a parameter server (PS) and workers. On the other hand, a decentralized training suffers from increased parameter variance among workers that causes slower model convergence. Addressing this dilemma, in this work, we propose a novel centralized training algorithm, ALADDIN, employing \"asymmetric\" communication between PS and workers for the PS bottleneck problem and novel updating strategies for both local and global parameters to mitigate the increased variance problem. Through a convergence analysis, we show that the convergence rate of ALADDIN is O(1 \u00f8nk ) on the non-convex problem, where n is the number of workers and k is the number of training iterations. The empirical evaluation using ResNet-50 and VGG-16 models demonstrates that (1) ALADDIN shows significantly better training throughput with up to 191% and 34% improvement compared to a synchronous algorithm and the state-of-the-art decentralized algorithm, respectively, (2) models trained by ALADDIN converge to the accuracies, comparable to those of the synchronous algorithm, within the shortest time, and (3) the convergence of ALADDIN is robust under various heterogeneous environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3373303",
                    "name": "Yunyong Ko"
                },
                {
                    "authorId": "2110474772",
                    "name": "Kibong Choi"
                },
                {
                    "authorId": "65887745",
                    "name": "Hyunseung Jei"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2155626647",
                    "name": "Sang-Wook Kim"
                }
            ]
        },
        {
            "paperId": "e03d9684a19c8f8e29ee97b347d4f1e280a88e44",
            "title": "Crossmodal Clustered Contrastive Learning: Grounding of Spoken Language to Gesture",
            "abstract": "Crossmodal grounding is a key technical challenge when generating relevant and well-timed gestures from spoken language. Often, the same gesture can accompany semantically different spoken language phrases which makes crossmodal grounding especially challenging. For example, a gesture (semi-circular with both hands) could co-occur with semantically different phrases \u201dentire bottom row\u201d (referring to a physical point) and \u201dmolecules expand and decay\u201d (referring to a scientific phenomena). In this paper, we introduce a self-supervised approach to learn representations better suited to such many-to-one grounding relationships between spoken language and gestures. As part of this approach, we propose a new contrastive loss function, Crossmodal Cluster NCE, that guides the model to learn spoken language representations which are consistent with the similarities in the gesture space. This gesture-aware space can help us generate more relevant gestures given language as input. We demonstrate the effectiveness of our approach on a publicly available dataset through quantitative and qualitative evaluations. Our proposed methodology significantly outperforms prior approaches for gestures-language grounding. Link to code: https://github.com/dondongwon/CC_NCE_GENEA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "118242121",
                    "name": "Chaitanya Ahuja"
                }
            ]
        },
        {
            "paperId": "e1946801a9f3109a8c5ffed3ba07af501d714f4f",
            "title": "The Impact of Mobile Ordering Service on Offline Store Diversity and Product Diversity",
            "abstract": "Offline retail stores have adopted mobile ordering technology to enhance their customer experience. A mobile ordering channel allows customers to find a nearby store and choose a product by lowering the search costs. However, the impact of mobile ordering services on the diversity of customer experiences has not been examined. In this study, the effects of mobile ordering technology on store and product diversity are measured. We analyzed the transaction data of 170,635 users over 16 weeks of store visits. The effect of mobile ordering technology on store and product diversity was estimated using the difference-in-differences method. We find that mobile ordering services can positively affect store and product diversity. The results are consistent after analyzing the data resampled with propensity score matching. This study provides the managerial implication that mobile ordering technology is valuable for offline retail stores that aim to extend their customers\u2019 shopping and product experiences. ,",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48253744",
                    "name": "Yujin Hwang"
                },
                {
                    "authorId": "49121729",
                    "name": "Nakyung Kyung"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "38793527",
                    "name": "Jaemin Jung"
                },
                {
                    "authorId": "2341187",
                    "name": "Sung-Hyuk Park"
                }
            ]
        },
        {
            "paperId": "f829674dceb91b76c9350b90d9432530eb1f7ca1",
            "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education",
            "abstract": "Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q&A, and knowledge tracing, to demonstrate the superiority of MathBERT over BASE BERT. Our experiments show that MathBERT outperforms prior best methods by 1.2-22% and BASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific vocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT pre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT vocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the participated leaning platforms: Stride, Inc, a commercial educational resource provider, and ASSISTments.org, a free online educational platform. We release MathBERT for public usage at: https://github.com/tbs17/MathBERT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115733400",
                    "name": "J. Shen"
                },
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "35596962",
                    "name": "Ethan Prihar"
                },
                {
                    "authorId": "1686529",
                    "name": "N. Heffernan"
                },
                {
                    "authorId": "7916525",
                    "name": "Xintao Wu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "ff264e7831aca07b69bc02fd667d03bac1751b39",
            "title": "MelBERT: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories",
            "abstract": "Automated metaphor detection is a challenging task to identify the metaphorical expression of words in a sentence. To tackle this problem, we adopt pre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we propose a novel metaphor detection model, namely metaphor-aware late interaction over BERT (MelBERT). Our model not only leverages contextualized word representation but also benefits from linguistic metaphor identification theories to detect whether the target word is metaphorical. Our empirical results demonstrate that MelBERT outperforms several strong baselines on four benchmark datasets, i.e., VUA-18, VUA-20, MOH-X, and TroFi.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111881386",
                    "name": "Minjin Choi"
                },
                {
                    "authorId": "2108302363",
                    "name": "Sunkyung Lee"
                },
                {
                    "authorId": "115153451",
                    "name": "Eunseong Choi"
                },
                {
                    "authorId": "2117003387",
                    "name": "Heesoo Park"
                },
                {
                    "authorId": "2205783649",
                    "name": "Junhyuk Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1865093",
                    "name": "Jongwuk Lee"
                }
            ]
        },
        {
            "paperId": "1841cca5b53385f6fe20293a8111c6a8269832c5",
            "title": "Gatekeeper: Analyzing G-Indexes and Improving Service Quantification",
            "abstract": "While it has been extensively studied on how to model and measure a scholar's research impact (e.g., citation analysis), there have been very few studies that systematically collect and quantify a scholar's service impact to scientific communities. To address this lack of studies, we have developed a prototype digital library, named as g, that crawls, extracts, and quantifies scholars' service impacts based on their roles as \"gatekeepers\" in Computer Science conferences. Continuing this effort, in this work, we further theoretically analyze and improve the understanding on the expected behavior of three quantification measures (i.e., G-indexes) being used in g. In addition, we demonstrate that the stretched-exponential model fits significantly better than three other heavy-tail models (i.e., power-law, log-normal, and parabolic-fractal) in capturing scholars' service impacts via three quantification measures. Finally, using the analyzed quantification measures, we present leading scholars and conferences with respect to their service impacts. Our prototype is available at: https://gatekeeper.ist.psu.edu.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146364717",
                    "name": "Jingtao Han"
                },
                {
                    "authorId": "1394822117",
                    "name": "Spyke Krepshaw"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "394a1554995797c7d17292ffa1dea2a8d28a2ed5",
            "title": "CoAID: COVID-19 Healthcare Misinformation Dataset",
            "abstract": "As the COVID-19 virus quickly spreads around the world, unfortunately, misinformation related to COVID-19 also gets created and spreads like wild fire. Such misinformation has caused confusion among people, disruptions in society, and even deadly consequences in health problems. To be able to understand, detect, and mitigate such COVID-19 misinformation, therefore, has not only deep intellectual values but also huge societal impacts. To help researchers combat COVID-19 health misinformation, therefore, we present CoAID (Covid-19 heAlthcare mIsinformation Dataset), with diverse COVID-19 healthcare misinformation, including fake news on websites and social platforms, along with users' social engagement about such news. CoAID includes 4,251 news, 296,000 related user engagements, 926 social platform posts about COVID-19, and ground truth labels. The dataset is available at: this https URL.",
            "fieldsOfStudy": [
                "Computer Science",
                "Political Science"
            ],
            "authors": [
                {
                    "authorId": "3122003",
                    "name": "Limeng Cui"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "4b78a2c797488e6de9bf7c81f9938aef9b54df28",
            "title": "The Secret to Finding a Match: A Field Experiment on Choice Capacity Design in an Online Dating Platform",
            "abstract": "Online matching platforms require new approaches to market design because firms can now control many aspects of the search and interaction process through various IT-enabled features. Although choice capacity\u2014the number of candidates a user can view and select\u2014is a key design feature of online matching platforms, its effect on engagement and matching outcomes remains unclear. We examine the effect of different choice capacities on market performance by conducting a randomized field experiment in collaboration with an online dating platform. Specifically, we design four treatment groups with different choice capacities in which users can only interact with other users in the same group and randomly assign the users to the treatment groups. We find that providing more choice capacity to male and female users has different effects on choice behaviors and matching outcomes. Although increasing the choice capacity of male users yields the highest engagement, increasing the choice capacity of female users is the most effective method to increase matching outcomes. We empirically demonstrate four mechanisms underlying the effectiveness of different choice capacity designs and generalize our findings by discussing how choice capacity can be designed to increase engagement and matching outcomes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1881127",
                    "name": "JaeHwuen Jung"
                },
                {
                    "authorId": "92307751",
                    "name": "H. Lim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2118196358",
                    "name": "Chul Kim"
                }
            ]
        },
        {
            "paperId": "56c2dedef3b2f745f726583e71966520efe98795",
            "title": "Beyond Cognitive Ability: Susceptibility to Fake News Is Also Explained by Associative Inference",
            "abstract": "We conducted a preliminary online study (N=261) investigating whether people's susceptibility to fake news on social media depends on how fake news are associated with real news that they viewed previously, as well as individuals' cognitive ability. Across two phases, we varied the association in three between-subjects conditions, i.e., associative inference, repetition, and irrelevant (control). Our study results showed limited impact of association type on participants of low cognitive ability. In contrast, for participants of high cognitive ability, their discrimination of fake news from real news tended to be worse for the associative inference condition than for the other two conditions. Thus, our findings suggest that individuals of high cognitive ability are likely to be susceptible to form the belief of fake news, but differently from those of low cognitive ability.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "83320378",
                    "name": "Sian Lee"
                },
                {
                    "authorId": "1666191052",
                    "name": "Joshua P. Forrest"
                },
                {
                    "authorId": "1665429361",
                    "name": "J. Strait"
                },
                {
                    "authorId": "150127148",
                    "name": "Haeseung Seo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2156155",
                    "name": "Aiping Xiong"
                }
            ]
        },
        {
            "paperId": "5755e608f9d45520d994e94bae47cea1dd72f41a",
            "title": "DETERRENT: Knowledge Guided Graph Attention Network for Detecting Healthcare Misinformation",
            "abstract": "To provide accurate and explainable misinformation detection, it is often useful to take an auxiliary source (e.g., social context and knowledge base) into consideration. Existing methods use social contexts such as users' engagements as complementary information to improve detection performance and derive explanations. However, due to the lack of sufficient professional knowledge, users seldom respond to healthcare information, which makes these methods less applicable. In this work, to address these shortcomings, we propose a novel knowledge guided graph attention network for detecting health misinformation better. Our proposal, named as DETERRENT, leverages on the additional information from medical knowledge graph by propagating information along with the network, incorporates a Medical Knowledge Graph and an Article-Entity Bipartite Graph, and propagates the node embeddings through Knowledge Paths. In addition, an attention mechanism is applied to calculate the importance of entities to each article, and the knowledge guided article embeddings are used for misinformation detection. DETERRENT addresses the limitation on social contexts in the healthcare domain and is capable of providing useful explanations for the results of detection. Empirical validation using two real-world datasets demonstrated the effectiveness of DETERRENT. Comparing with the best results of eight competing methods, in terms of F1 Score, DETERRENT outperforms all methods by at least 4.78% on the diabetes dataset and 12.79% on cancer dataset. We release the source code of DETERRENT at: https://github.com/cuilimeng/DETERRENT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3122003",
                    "name": "Limeng Cui"
                },
                {
                    "authorId": "150127148",
                    "name": "Haeseung Seo"
                },
                {
                    "authorId": "147612193",
                    "name": "Maryam Tabar"
                },
                {
                    "authorId": "2988239",
                    "name": "Fenglong Ma"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "6f7f087d2c27d22597e6f584bebae67dc7a8ba6c",
            "title": "SIENA: Stochastic Multi-Expert Neural Patcher",
            "abstract": "Neural network (NN) models that are solely trained to maximize the likelihood of an observed dataset are often vulnerable to adversarial attacks. Even though several methods have been proposed to enhance NN models' adversarial robustness, they often require re-training from scratch. This leads to redundant computation, especially in the NLP domain where current state-of-the-art models, such as BERT and ROBERTA, require great time and space resources. By borrowing ideas from Software Engineering, we, therefore, first introduce the Neural Patching mechanism to improve adversarial robustness by \"patching\" only parts of a NN model. Then, we propose a novel neural patching algorithm, SIENA, that transforms a textual NN model into a stochastic ensemble of multi-expert predictors by upgrading and re-training its last layer only. SIENA forces adversaries to attack not only one but multiple models that are specialized in diverse sub-sets of features, labels, and instances so that the ensemble model becomes more robust to adversarial attacks. By conducting comprehensive experiments, we demonstrate that all of CNN, RNN, BERT, and ROBERTA-based textual models, once patched by SIENA, witness an absolute increase of as much as 20% in accuracy on average under 5 different white and black-box attacks, outperforming 6 defensive baselines across 4 public NLP datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "5166698",
                    "name": "Noseong Park"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "8c6d89731e874bc7cd8cb87614c53ebae37e598f",
            "title": "Detecting Universal Trigger's Adversarial Attack with Honeypot",
            "abstract": "The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger can generate a fixed phrase that when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this new attack method that may cause significant harm, we borrow the \"honeypot\" concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework. DARCY adaptively searches and injects multiple trapdoors into an NN model to \"bait and catch\" potential attacks. Through comprehensive experiments across five public datasets, we demonstrate that DARCY detects UniTrigger's adversarial attacks with up to 99% TPR and less than 1% FPR in most cases, while showing a difference of only around 2% of F1 score on average in predicting for clean inputs. We also show that DARCY with multiple trapdoors is robust under different assumptions with respect to attackers' knowledge and skills.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "5166698",
                    "name": "Noseong Park"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "9da9cab545bd1534a8c953bb66e611243dcee24d",
            "title": "Authorship Attribution for Neural Text Generation",
            "abstract": "In recent years, the task of generating realistic short and long texts have made tremendous advancements. In particular, several recently proposed neural network-based language models have demonstrated their astonishing capabilities to generate texts that are challenging to distinguish from human-written texts with the naked eye. Despite many benefits and utilities of such neural methods, in some applications, being able to tell the \u201cauthor\u201d of a text in question becomes critically important. In this work, in the context of this Turing Test, we investigate the so-called authorship attribution problem in three versions: (1) given two texts T1 and T2, are both generated by the same method or not? (2) is the given text T written by a human or machine? (3) given a text T and k candidate neural methods, can we single out the method (among k alternatives) that generated T? Against one humanwritten and eight machine-generated texts (i.e., CTRL, GPT, GPT2, GROVER, XLM, XLNET, PPLM, FAIR), we empirically experiment with the performance of various models in three problems. By and large, we find that most generators still generate texts significantly different from human-written ones, thereby making three problems easier to solve. However, the qualities of texts generated by GPT2, GROVER, and FAIR are better, often confusing machine classifiers in solving three problems. All codes and datasets of our experiments are available at: https://bit.ly/ 302zWdz",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "145800151",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "bbf6b512207776ce9ba222746120dab403fdef74",
            "title": "TOMATO: A Topic-Wise Multi-Task Sparsity Model",
            "abstract": "The Multi-Task Learning (MTL) leverages the inter-relationship across tasks and is useful for applications with limited data. Existing works articulate different task relationship assumptions, whose validity is vital to successful multi-task training. We observe that, in many scenarios, the inter-relationship across tasks varies across different groups of data (i.e., topic), which we call within-topic task relationship hypothesis. In this case, current MTL models with homogeneous task relationship assumption cannot fully exploit different task relationships among different groups of data. Based on this observation, in this paper, we propose a generalized topic-wise multi-task architecture, to capture the within-topic task relationship, which can be combined with any existing MTL designs. Further, we propose a new specialized MTL design, topic-task-sparsity, along with two different types of sparsity constraints. The architecture, combined with the topic-task-sparsity design, constructs our proposed TOMATO model. The experiments on both synthetic and 4 real-world datasets show that our proposed models consistently outperform 6 state-of-the-art models and 2 baselines with improvement from $5%$ to $46%$ in terms of task-wise comparison, demonstrating the validity of the proposed within-topic task relationship hypothesis. We release the source codes and datasets of TOMATO at: https://github.com/JasonLC506/MTSEM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49050667",
                    "name": "Jason Zhang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "c94529aff09763b607b7594197f1bbf01c006759",
            "title": "A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger\u2019s Adversarial Attacks",
            "abstract": "The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the \u201choneypot\u201d concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger. DARCY greedily searches and injects multiple trapdoors into an NN model to \u201cbait and catch\u201d potential attacks. Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger\u2019s adversarial attacks with up to 99% TPR and less than 2% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1% margin. We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers\u2019 varying levels of knowledge and skills. We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "5166698",
                    "name": "Noseong Park"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "d4834dc9cc8a61eff5f99bd6e46b3ff5c266fabb",
            "title": "MALCOM: Generating Malicious Comments to Attack Neural Fake News Detection Models",
            "abstract": "In recent years, the proliferation of so-called \u201cfake news\u201d has caused much disruptions in society and weakened the news ecosystem. Therefore, to mitigate such problems, researchers have developed state-of-the-art (SOTA) models to autodetect fake news on social media using sophisticated data science and machine learning techniques. In this work, then, we ask \u201cwhat if adversaries attempt to attack such detection models?\u201d and investigate related issues by (i) proposing a novel attack scenario against fake news detectors, in which adversaries can post malicious comments toward news articles to mislead SOTA fake news detectors, and (ii) developing Malcom, an end-to-end adversarial comment generation framework to achieve such an attack. Through a comprehensive evaluation, we demonstrate that about 94% and 93.5% of the time on average Malcom can successfully mislead five of the latest neural detection models to always output targeted real and fake news labels. Furthermore, Malcom can also fool black box fake news detectors to always output real news labels 90% of the time on average. We also compare our attack model with four baselines across two real-world datasets, not only on attack performance but also on generated quality, coherency, transferability, and robustness. We release the source code of Malcom at https://github.com/lethaiq/MALCOM1.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "f5cd3f88e27cb63e579e59df1add1aa953ff302f",
            "title": "Understanding emotions in SNS images from posters' perspectives",
            "abstract": "As the popularity of media-based social networking services (SNS), such as Instagram and Snapchat, has increased significantly, a growing body of research has analyzed SNS images in relation to emotional analysis and classification model development. However, these prior studies were based on relatively small amounts of data, where the emotions of images were labeled from viewers' perspectives, not posters' perspectives. Consequently, we analyze 120K images that reflect poster's emotion. We develop color- and content-based classification models by considering: (1) the dynamics of SNS, in terms of the volume and variety of images shared, and (2) the fact that people express their emotions through colors and objects. We demonstrate the comparable performance of our model with models proposed in prior studies and discuss the applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115752535",
                    "name": "Junho Song"
                },
                {
                    "authorId": "35655049",
                    "name": "Kyungsik Han"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "51967113",
                    "name": "Sang-Wook Kim"
                }
            ]
        },
        {
            "paperId": "fc5f962d8278695e4dd28c288180e360964b4e46",
            "title": "Different but Equal? A Field Experiment on the Impact of Recommendation Systems on Mobile and Personal Computer Channels in Retail",
            "abstract": "The benefits of recommendation systems in online retail contexts have received much attention in prior work. Much of this work has been conducted in personal computer (PC)\u2013based settings, although mobile devices are becoming increasingly central to the online shopping experience. It remains to be examined if the effects of recommendation systems in retail differ across these two channels, in terms of customer-level decision outcomes. In this paper, we examine these differences in some detail, studying how product views and sales attributed to a recommendation system are different across mobile and PC-based channels. Further, we examine how the effect of a recommendation system across channels influences sales diversity, an important outcome in the retail industry. We conduct our analysis using a randomized field experiment, conducted in partnership with an online retailing firm in South Korea, where the experimental treatment is access to a recommendation system. Our results show that the use of recommendation systems enhances customer-level outcomes, such as views and sales of recommended products, clickthrough rate, and conversion. More importantly, the marginal impacts of the recommendation system are significantly higher for mobile users, indicating that the higher search costs imposed through mobile devices are more effectively reduced through recommendation systems. With respect to sales diversity, we observe that although the mobile channel leads to more diverse sales, we see no interaction effects of the recommendation system and mobile use on sales diversity. These results provide boundary conditions for the efficacy of recommendation systems in retail contexts where online sales occur across both PC-based and mobile channels. We discuss the managerial implications of these results for online retailers and conclude with opportunities for further research.",
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "34803014",
                    "name": "A. Gopal"
                },
                {
                    "authorId": "2341187",
                    "name": "Sung-Hyuk Park"
                }
            ]
        },
        {
            "paperId": "07c161c33b0f7663b6458d52b1a2a061c00c8e87",
            "title": "GRACE: Generating Concise and Informative Contrastive Sample to Explain Neural Network Model's Prediction",
            "abstract": "Despite the recent development in the topic of explainable AI/ML for image and text data, the majority of current solutions are not suitable to explain the prediction of neural network models when the datasets are tabular and their features are in high-dimensional vectorized formats. To mitigate this limitation, therefore, we borrow two notable ideas (i.e., \"explanation by intervention\" from causality and \"explanation are contrastive\" from philosophy) and propose a novel solution, named as GRACE, that better explains neural network models' predictions for tabular datasets. In particular, given a model's prediction as label X, GRACE intervenes and generates a minimally-modified contrastive sample to be classified as Y, with an intuitive textual explanation, answering the question of \"Why X rather than Y?\" We carry out comprehensive experiments using eleven public datasets of different scales and domains (e.g., # of features ranges from 5 to 216) and compare GRACE with competing baselines on different measures: fidelity, conciseness, info-gain, and influence. The user-studies show that our generated explanation is not only more intuitive and easy-to-understand but also facilitates end-users to make as much as 60% more accurate post-explanation decisions than that of Lime.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "0b11343d4f247826e725a458465174aa2578e52a",
            "title": "dEFEND: Explainable Fake News Detection",
            "abstract": "In recent years, to mitigate the problem of fake news, computational detection of fake news has been studied, producing some promising early results. While important, however, we argue that a critical missing piece of the study be the explainability of such detection, i.e., why a particular piece of news is detected as fake. In this paper, therefore, we study the explainable detection of fake news. We develop a sentence-comment co-attention sub-network to exploit both news contents and user comments to jointly capture explainable top-k check-worthy sentences and user comments for fake news detection. We conduct extensive experiments on real-world datasets and demonstrate that the proposed method not only significantly outperforms 7 state-of-the-art fake news detection methods by at least 5.33% in F1-score, but also (concurrently) identifies top-k user comments that explain why a news piece is fake, better than baselines by 28.2% in NDCG and 30.7% in Precision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145800151",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "3122003",
                    "name": "Limeng Cui"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "38746648",
                    "name": "Huan Liu"
                }
            ]
        },
        {
            "paperId": "224beb854e540a325946219f433604feddb84473",
            "title": "Characterizing Man-made vs. Machine-made Chatbot Dialogs",
            "abstract": "The increasing usage of machine-made artifacts in news and social media can severely exacerbate the problem of false news. While knowing the parts of news content, or embedded images therein, are machine-generated or not helps determine the veracity of news, due to the recent improvement in AI techniques, it has become more dif\ufb01cult to accurately distinguish machine-made artifacts from man-made ones. In this work, therefore, we attempt to better understand and characterize distinguishing features between man-made and machine-made artifacts, especially chatbot dialog texts, which tend to be short and erroneous. Some of the characteristics that we found include: machine-made texts tend to use more words per message, interjections (e.g., hey, hi), use more \ufb01ller words (e.g., blah, you, and know) and appear to be less con\ufb01dent than man-made texts in their speech. However, we noted that privacy or entropy related features between two types of texts do not appear to be signi\ufb01-cantly different.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "14273816",
                    "name": "Jeffery Cao"
                },
                {
                    "authorId": "2116722593",
                    "name": "Qiaozhi Wang"
                },
                {
                    "authorId": "2075399930",
                    "name": "Bo Luo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "369a28f15bb770c947f0adcd048b383f61c5a48b",
            "title": "#DontTweetThis: Scoring Private Information in Social Networks",
            "abstract": "Abstract With the growing popularity of online social networks, a large amount of private or sensitive information has been posted online. In particular, studies show that users sometimes reveal too much information or unintentionally release regretful messages, especially when they are careless, emotional, or unaware of privacy risks. As such, there exist great needs to be able to identify potentially-sensitive online contents, so that users could be alerted with such findings. In this paper, we propose a context-aware, text-based quantitative model for private information assessment, namely PrivScore, which is expected to serve as the foundation of a privacy leakage alerting mechanism. We first solicit diverse opinions on the sensitiveness of private information from crowdsourcing workers, and examine the responses to discover a perceptual model behind the consensuses and disagreements. We then develop a computational scheme using deep neural networks to compute a context-free PrivScore (i.e., the \u201cconsensus\u201d privacy score among average users). Finally, we integrate tweet histories, topic preferences and social contexts to generate a personalized context-aware PrivScore. This privacy scoring mechanism could be employed to identify potentially-private messages and alert users to think again before posting them to OSNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116722593",
                    "name": "Qiaozhi Wang"
                },
                {
                    "authorId": "2053313759",
                    "name": "Hao Xue"
                },
                {
                    "authorId": "1680720",
                    "name": "Fengjun Li"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "145607083",
                    "name": "Bo Luo"
                }
            ]
        },
        {
            "paperId": "3feb54e4b555e6533b7de4c1fd7b4de74f0202f7",
            "title": "How Gullible Are You?: Predicting Susceptibility to Fake News",
            "abstract": "In this research, we hypothesize that some social users are more gullible to fake news than others, and accordingly investigate on the susceptibility of users to fake news--i.e., how to identify susceptible users, what are their characteristics, and if one can build a prediction model.Building on the crowdsourced annotations of 5 types of susceptible users in Twitter, we found out that: (1) susceptible users are correlated with a combination of user, network, and content features; (2) one can build a reasonably accurate prediction model with 0.82 in AUC-ROC for the multinomial classification task; and (3) there exists a correlation between the dominant susceptibility level of center nodes and that of the entire network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057974027",
                    "name": "T. Shen"
                },
                {
                    "authorId": "34631309",
                    "name": "R. Cowell"
                },
                {
                    "authorId": "2109995009",
                    "name": "Aditi Gupta"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "34903979",
                    "name": "A. Yadav"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "51c55a62a0ae8582c1c9aff561058f23369cc8bc",
            "title": "SAME: Sentiment-Aware Multi-Modal Embedding for Detecting Fake News",
            "abstract": "How to effectively detect fake news and prevent its diffusion on social media has gained much attention in recent years. However, relatively little focus has been given on exploiting user comments left for posts and latent sentiments therein in detecting fake news. Inspired by the rich information available in user comments on social media, therefore, we investigate whether the latent sentiments hidden in user comments can potentially help distinguish fake news from reliable content. We incorporate users' latent sentiments into an end-to-end deep embedding framework for detecting fake news, named as SAME. First, we use multi-modal networks to deal with heterogeneous data modalities. Second, to learn semantically meaningful spaces per data source, we adopt an adversarial mechanism. Third, we define a novel regularization loss to bring embeddings of relevant pairs closer. Our comprehensive validation using two real-world datasets, PolitiFact and GossipCop, demonstrates the effectiveness of SAME in detecting fake news, significantly outperforming state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3122003",
                    "name": "Limeng Cui"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "54a6b328777ed0f43f4f9d13e45fa30999f34593",
            "title": "<inline-formula><tex-math notation=\"LaTeX\">$l$</tex-math><alternatives> <inline-graphic xlink:href=\"kim-ieq1-2698461.gif\"/></alternatives></inline-formula>-Injection: Toward Effective Collaborative Filtering Using Uninteresting Items",
            "abstract": "We develop a novel framework, named as <inline-formula><tex-math notation=\"LaTeX\">$l$</tex-math><alternatives> <inline-graphic xlink:href=\"kim-ieq2-2698461.gif\"/></alternatives></inline-formula>-injection, to address the sparsity problem of recommender systems. By carefully injecting low values to a selected set of unrated user-item pairs in a user-item matrix, we demonstrate that top-<italic>N</italic> recommendation accuracies of various collaborative filtering (CF) techniques can be significantly and consistently improved. We first adopt the notion of <italic>pre-use preferences</italic> of users toward a vast amount of <italic>unrated</italic> items. Using this notion, we identify <italic>uninteresting</italic> items that have not been rated yet but are likely to receive low ratings from users, and selectively impute them as low values. As our proposed approach is method-agnostic, it can be easily applied to a variety of CF algorithms. Through comprehensive experiments with three real-life datasets (e.g., Movielens, Ciao, and Watcha), we demonstrate that our solution consistently and universally enhances the accuracies of existing CF algorithms (e.g., item-based CF, SVD-based CF, and SVD++) by 2.5 to 5 times on average. Furthermore, our solution improves the running time of those CF methods by 1.2 to 2.3 times when its setting produces the best accuracy. The datasets and codes that we used in the experiments are available at: <uri>https://goo.gl/KUrmip</uri>.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1865093",
                    "name": "Jongwuk Lee"
                },
                {
                    "authorId": "7905032",
                    "name": "Won-Seok Hwang"
                },
                {
                    "authorId": "37773165",
                    "name": "J. Parc"
                },
                {
                    "authorId": "144409057",
                    "name": "Youngnam Lee"
                },
                {
                    "authorId": "51967113",
                    "name": "Sang-Wook Kim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "5e168a8575717b92c31c36f304432f8d980622e7",
            "title": "Trust It or Not: Effects of Machine-Learning Warnings in Helping Individuals Mitigate Misinformation",
            "abstract": "Despite increased interests in the study of fake news, how to aid users' decision in handling suspicious or false information has not been well understood. To obtain a better understanding on the impact of warnings on individuals' fake news decisions, we conducted two online experiments, evaluating the effect of three warnings (i.e., one Fact-Checking and two Machine-Learning based) against a control condition, respectively. Each experiment consisted of three phases examining participants' recognition, detection, and sharing of fake news, respectively. In Experiment 1, relative to the control condition, participants' detection of both fake and real news was better when the Fact-Checking warning but not the two Machine-Learning warnings were presented with fake news. Post-session questionnaire results revealed that participants showed more trust for the Fact-Checking warning. In Experiment 2, we proposed a Machine-Learning-Graph warning that contains the detailed results of machine-learning based detection and removed the source within each news headline to test its impact on individuals' fake news detection with warnings. We did not replicate the effect of the Fact-Checking warning obtained in Experiment 1, but the Machine-Learning-Graph warning increased participants' sensitivity in differentiating fake news from real ones. Although the best performance was obtained with the Machine-Learning- Graph warning, participants trusted it less than the Fact-Checking warning. Therefore, our study results indicate that a transparent machine learning warning is critical to improving individuals' fake news detection but not necessarily increase their trust on the model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "150127148",
                    "name": "Haeseung Seo"
                },
                {
                    "authorId": "2156155",
                    "name": "Aiping Xiong"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "72b6fc39c1d44f4555a725d21f0789db6850021c",
            "title": "Constrained Local Graph Clustering by Colored Random Walk",
            "abstract": "Detecting local graph clusters is an important problem in big graph analysis. Given seed nodes in a graph, local clustering aims at finding subgraphs around the seed nodes, which consist of nodes highly relevant to the seed nodes. However, existing local clustering methods either allow only a single seed node, or assume all seed nodes are from the same cluster, which is not true in many real applications. Moreover, the assumption that all seed nodes are in a single cluster fails to use the crucial information of relations between seed nodes. In this paper, we propose a method to take advantage of such relationship. With prior knowledge of the community membership of the seed nodes, the method labels seed nodes in the same (different) community by the same (different) color. To further use this information, we introduce a color-based random walk mechanism, where colors are propagated from the seed nodes to every node in the graph. By the interaction of identical and distinct colors, we can enclose the supervision of seed nodes into the random walk process. We also propose a heuristic strategy to speed up the algorithm by more than 2 orders of magnitude. Experimental evaluations reveal that our clustering method outperforms state-of-the-art approaches by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2358187",
                    "name": "Yaowei Yan"
                },
                {
                    "authorId": "3478513",
                    "name": "Yuchen Bian"
                },
                {
                    "authorId": "153640788",
                    "name": "Dongsheng Luo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": null,
                    "name": "Xiang Zhang"
                }
            ]
        },
        {
            "paperId": "77977b9fdf012c3262755f058df43a3634eed472",
            "title": "\u201cFake News\u201d Is Not Simply False Information: A Concept Explication and Taxonomy of Online Content",
            "abstract": "As the scourge of \u201cfake news\u201d continues to plague our information environment, attention has turned toward devising automated solutions for detecting problematic online content. But, in order to build reliable algorithms for flagging \u201cfake news,\u201d we will need to go beyond broad definitions of the concept and identify distinguishing features that are specific enough for machine learning. With this objective in mind, we conducted an explication of \u201cfake news\u201d that, as a concept, has ballooned to include more than simply false information, with partisans weaponizing it to cast aspersions on the veracity of claims made by those who are politically opposed to them. We identify seven different types of online content under the label of \u201cfake news\u201d (false news, polarized content, satire, misreporting, commentary, persuasive information, and citizen journalism) and contrast them with \u201creal news\u201d by introducing a taxonomy of operational indicators in four domains\u2014message, source, structure, and network\u2014that together can help disambiguate the nature of online news content.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8771881",
                    "name": "Maria D. Molina"
                },
                {
                    "authorId": "47493614",
                    "name": "S. Sundar"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "c3e8ff240dd26f9241fb2e8023557ec62b6b9ae8",
            "title": "Why X rather than Y? Explaining Neural Model' Predictions by Generating Intervention Counterfactual Samples",
            "abstract": "Even though the topic of explainable AI/ML is very popular in text and computer vision domain, most of the previous literatures are not suitable for explaining black-box models' predictions on general data mining datasets. This is because these datasets are usually in high-dimensional vectored features format that are not as friendly and comprehensible as texts and images to the end users. In this paper, we combine the best of both worlds: \"explanations by intervention\" from causality and \"explanations are contrastive\" from philosophy and social science domain to explain neural models' predictions for tabular datasets. Specifically, given a model's prediction as label X, we propose a novel idea to intervene and generate minimally modified contrastive sample to be classified as Y, that then results in a simple natural text giving answer to the question \"Why X rather than Y?\". We carry out experiments with several datasets of different scales and compare our approach with other baselines on three different areas: fidelity, reasonableness and explainability.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2067953842",
                    "name": "Thai V. Le"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "cfbe9689bd094b4a183e2e60a09f03d03b45965e",
            "title": "Deep Reinforcement Learning for Personalized Search Story Recommendation",
            "abstract": "In recent years, \\emph{search story}, a combined display with other organic channels, has become a major source of user traffic on platforms such as e-commerce search platforms, news feed platforms and web and image search platforms. The recommended search story guides a user to identify her own preference and personal intent, which subsequently influences the user's real-time and long-term search behavior. %With such an increased importance of search stories, As search stories become increasingly important, in this work, we study the problem of personalized search story recommendation within a search engine, which aims to suggest a search story relevant to both a search keyword and an individual user's interest. To address the challenge of modeling both immediate and future values of recommended search stories (i.e., cross-channel effect), for which conventional supervised learning framework is not applicable, we resort to a Markov decision process and propose a deep reinforcement learning architecture trained by both imitation learning and reinforcement learning. We empirically demonstrate the effectiveness of our proposed approach through extensive experiments on real-world data sets from this http URL.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "49050667",
                    "name": "Jason Zhang"
                },
                {
                    "authorId": "1917527",
                    "name": "Junming Yin"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2663662",
                    "name": "Linhong Zhu"
                }
            ]
        },
        {
            "paperId": "d2b0e9a8e915822e613eea672be6847fdb450e6b",
            "title": "PATHFINDER: Graph-Based Itemset Embedding for Learning Course Recommendation and Beyond",
            "abstract": "We demonstrate a tool, named as PATHFINDER, that captures and visualizes rich latent relationships among courses as a graph, mines students' past course performance data, and recommends pathways or top-k courses most helpful to a given student, using an itemset embedding based learning model. With dedicated design for the asymmetric, non-additive and non-negative challenges specific to the problem, our model for helpfulness achieves the best performance among competing models. We demonstrate the visualization of four course relationships (e.g., mandatory, prerequisite, helpful, and top-k) in a graph. The PATHFINDER demo is publicly available at: http://140.82.60.177:8000",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1519088854",
                    "name": "Jiasheng Zhang"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "50325598",
                    "name": "Yiming Liao"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "e049ab96a034edb1b8a47ebf57fa5eb603cb656b",
            "title": "Predicting Influence Probabilities using Graph Convolutional Networks",
            "abstract": "As one of the fundamental tasks in data analytics, Influence Maximization methods have been widely used in many real-world applications. For instance, in social network analysis, after building a directed graph, where edges are weighted with influence probabilities, influence maximization methods can be used to find a set of users who can maximize the spread of information under certain cascade models. Despite their successes, however, one critical weakness of existing influence maximization methods lies in the fact that edges are weighted with historical probabilities. As such, influence maximization methods perform sub-optimal if there occur non-trivial changes in future. In response to this challenge, in this work, we propose a novel prediction-driven influence maximization method that accurately predicts future influence probabilities using graph convolutional networks and find seed users based on the predicted probabilities. The experiments with five real-world datasets show that our prediction accuracy is accurate (e.g., mean absolute percentage error less than 0.1) in many cases, and our prediction-driven influence maximization is very close to the optimal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144979786",
                    "name": "Jing Liu"
                },
                {
                    "authorId": "2004603900",
                    "name": "Yudi Chen"
                },
                {
                    "authorId": "51239375",
                    "name": "Duanshun Li"
                },
                {
                    "authorId": "5166698",
                    "name": "Noseong Park"
                },
                {
                    "authorId": "1899648",
                    "name": "Kisung Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "ecc560b2299b0167b1b843a416094b70151d187f",
            "title": "A Reverse Turing Test for Detecting Machine-Made Texts",
            "abstract": "As AI technologies rapidly advance, the artifacts created by machines will become prevalent. As recent incidents by the Deepfake illustrate, then, being able to differentiate man-made vs. machine-made artifacts, especially in social media space, becomes more important. In this preliminary work, in this regard, we formulate such a classification task as the Reverse Turing Test (RTT) and investigate on the contemporary status to be able to classify man-made vs. machine-made texts. Studying real-life machine-made texts in three domains of financial earning reports, research articles, and chatbot dialogues, we found that the classification of man-made vs. machine-made texts can be done at least as accurate as 0.84 in F1 score. We also found some differences between man-made and machine-made in sentiment, readability, and textual features, which can help differentiate them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150162416",
                    "name": "Jialin Shao"
                },
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "f24ffc50c66eb9cd6d8147bcf41697a4e9cc07d5",
            "title": "5 Sources of Clickbaits You Should Know! Using Synthetic Clickbaits to Improve Prediction and Distinguish between Bot-Generated and Human-Written Headlines",
            "abstract": "Clickbait is an attractive yet misleading headline that lures readers to commit click-conversion. Development of robust clickbait detection models has been, however, hampered due to the shortage of high-quality labeled training samples. To overcome this challenge, we investigate how to exploit human-written and machine-generated synthetic clickbaits. We first ask crowdworkers and journalism students to generate clickbaity news headlines. Second, we utilize deep generative models to generate clickbaity headlines. Through empirical evaluations, we demonstrate that synthetic clickbaits by human entities and deep generative models are consistently useful in improving the accuracy of various prediction models, by as much as 14.5% in AUC, across two real datasets and different types of algorithms. Especially, we observe an improvement in accuracy, up to 8.5% in AUC, even for top-ranked clickbait detectors from Clickbait Challenge 2017. Our study proposes a novel direction to address the shortage of labeled training data, one of fundamental bottlenecks in supervised learning, by means of synthetic training data with reinforced domain knowledge. It also provides a solution for distinguishing between bot-generated and human-written clickbaits, thus aiding the work of moderators and better alerting news consumers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "145800151",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "8771881",
                    "name": "Maria D. Molina"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "47493614",
                    "name": "S. Sundar"
                },
                {
                    "authorId": "145896397",
                    "name": "Huan Liu"
                }
            ]
        },
        {
            "paperId": "fbe8edbd0b40d5887e258ea709be979904c03ad9",
            "title": "dEFEND: A System for Explainable Fake News Detection",
            "abstract": "Despite recent advancements in computationally detecting fake news, we argue that a critical missing piece be the explainability of such detection--i.e., why a particular piece of news is detected as fake--and propose to exploit rich information in users' comments on social media to infer the authenticity of news. In this demo paper, we present our system for an explainable fake news detection called dEFEND, which can detect the authenticity of a piece of news while identifying user comments that can explain why the news is fake or real. Our solution develops a sentence-comment co-attention sub-network to exploit both news contents and user comments to jointly capture explainable top-k check-worthy sentences and user comments for fake news detection. The system is publicly accessible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3122003",
                    "name": "Limeng Cui"
                },
                {
                    "authorId": "145800151",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "145896397",
                    "name": "Huan Liu"
                }
            ]
        },
        {
            "paperId": "053c90c46bf5f29d833fb7278915f5070734e7da",
            "title": "Deep Headline Generation for Clickbait Detection",
            "abstract": "Clickbaits are catchy social posts or sensational headlines that attempt to lure readers to click. Clickbaits are pervasive on social media and can have significant negative impacts on both users and media ecosystems. For example, users may be misled to receive inaccurate information or fall into click-jacking attacks. Similarly, media platforms could lose readers' trust and revenues due to the prevalence of clickbaits. To computationally detect such clickbaits on social media using a supervised learning framework, one of the major obstacles is the lack of large-scale labeled training data, due to the high cost of labeling. With the recent advancements of deep generative models, to address this challenge, we propose to generate synthetic headlines with specific styles and explore their utilities to help improve clickbait detection. In particular, we propose to generate stylized headlines from original documents with style transfer. Furthermore, as it is non-trivial to generate stylized headlines due to several challenges such as the discrete nature of texts and the requirements of preserving semantic meaning of document while achieving style transfer, we propose a novel solution, named as Stylized Headline Generation (SHG), that can not only generate readable and realistic headlines to enlarge original training data, but also help improve the classification capacity of supervised learning. The experimental results on real-world datasets demonstrate the effectiveness of SHG in generating high-quality and high-utility headlines for clickbait detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145800151",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "38746648",
                    "name": "Huan Liu"
                }
            ]
        },
        {
            "paperId": "0cdbfef68d5fcab166e3b51d31a4789349e96683",
            "title": "My Friend Leaks My Privacy: Modeling and Analyzing Privacy in Social Networks",
            "abstract": "With the dramatically increasing participation in online social networks (OSNs), huge amount of private information becomes available on such sites. It is critical to preserve users' privacy without preventing them from socialization and sharing. Unfortunately, existing solutions fall short meeting such requirements. We argue that the key component of OSN privacy protection is protecting (sensitive) content -- privacy as having the ability to control information dissemination. We follow the concepts of private information boundaries and restricted access and limited control to introduce a social circle model. We articulate the formal constructs of this model and the desired properties for privacy protection in the model. We show that the social circle model is efficient yet practical, which provides certain level of privacy protection capabilities to users, while still facilitates socialization. We then utilize this model to analyze the most popular social network platforms on the Internet (Facebook, Google+, WeChat, etc), and demonstrate the potential privacy vulnerabilities in some social networks. Finally, we discuss the implications of the analysis, and possible future directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46172687",
                    "name": "Lingjing Yu"
                },
                {
                    "authorId": "46248185",
                    "name": "Sri Mounica Motipalli"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "145779142",
                    "name": "Peng Liu"
                },
                {
                    "authorId": "46485370",
                    "name": "Heng Xu"
                },
                {
                    "authorId": "48873848",
                    "name": "Qingyun Liu"
                },
                {
                    "authorId": "40062477",
                    "name": "Jianlong Tan"
                },
                {
                    "authorId": "145607083",
                    "name": "Bo Luo"
                }
            ]
        },
        {
            "paperId": "2fb264aec7831c52008f552e17da9511e7212956",
            "title": "Understanding Users\u2019 Privacy Attitudes through Subjective and Objective Assessments: An Instagram Case Study",
            "abstract": "Although previous studies have investigated social media users\u2019 privacy attitudes, little focus has been placed on understanding the degree of users\u2019 concern about different types of private information or the changes in users\u2019 privacy attitudes. This article presents novel insights on user attitudes toward 18 privacy items\u2014identified through a review of the literature\u2014and attitudinal changes through a comparative analysis. The authors also discuss the implications of the results that could better support users\u2019 privacy management on social media.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35655049",
                    "name": "Kyungsik Han"
                },
                {
                    "authorId": "2364106",
                    "name": "Hyung-Shik Jung"
                },
                {
                    "authorId": "1835709",
                    "name": "J. Jang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "37a28685714881a46551b1cd2f2ef2cde4c61fca",
            "title": "gOCCF: Graph-Theoretic One-Class Collaborative Filtering Based on Uninteresting Items",
            "abstract": "\n \n We investigate how to address the shortcomings of the popular One-Class Collaborative Filtering (OCCF) methods in handling challenging \u201csparse\u201d dataset in one-class setting (e.g., clicked or bookmarked), and propose a novel graph-theoretic OCCF approach, named as gOCCF, by exploiting both positive preferences (derived from rated items) as well as negative preferences (derived from unrated items). In capturing both positive and negative preferences as a bipartite graph, further, we apply the graph shattering theory to determine the right amount of negative preferences to use. Then, we develop a suite of novel graph-based OCCF methods based on the random walk with restart and belief propagation methods. Through extensive experiments using 3 real-life datasets, we show that our gOCCF effectively addresses the sparsity challenge and significantly outperforms all of 8 competing methods in accuracy on very sparse datasets while providing comparable accuracy to the best performing OCCF methods on less sparse datasets. The datasets and implementations used in the empirical validation are available for access: https://goo.gl/sfiawn.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2553299",
                    "name": "Yeon-Chang Lee"
                },
                {
                    "authorId": "51967113",
                    "name": "Sang-Wook Kim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "50a75970ba569b6341df027b2000b461feb4ea12",
            "title": "Task Relevance and Diversity as Worker Motivation in Crowdsourcing",
            "abstract": "Task assignment is a central component in crowdsourcing. Organizational studies have shown that worker motivation in completing tasks has a direct impact on the quality of individual contributions. In this work, we examine motivation-aware task assignment in the presence of a set of workers. We propose to model motivation as a balance between task relevance and task diversity and argue that an adaptive approach to task assignment can best capture the evolving nature of motivation. Worker motivation is observed and task assignment is revisited appropriately across iterations. We prove the problem to be NP-hard as well as MaxSNP-Hard and develop efficient approximation algorithms with provable guarantees. Our experiments with synthetic data examine the scalability of our algorithms, and our live real data experiments show that capturing motivation using relevance and diversity leads to high crowdwork quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3416184",
                    "name": "Julien Pilourdault"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "51000a85344122c082e41d8f3d031f07308c4c7a",
            "title": "ROAR: Robust Label Ranking for Social Emotion Mining",
            "abstract": "\n \n Understanding and predicting latent emotions of users toward online contents, known as social emotion mining, has become increasingly important to both social platforms and businesses alike. Despite recent developments, however, very little attention has been made to the issues of nuance, subjectivity, and bias of social emotions. In this paper, we fill this gap by formulating social emotion mining as a robust label ranking problem, and propose: (1) a robust measure, named as G-mean-rank (GMR), which sets a formal criterion consistent with practical intuition; and (2) a simple yet effective label ranking model, named as ROAR, that is more robust toward unbalanced datasets (which are common). Through comprehensive empirical validation using 4 real datasets and 16 benchmark semi-synthetic label ranking datasets, and a case study, we demonstrate the superiorities of our proposals over 2 popular label ranking measures and 6 competing label ranking algorithms. The datasets and implementations used in the empirical validation are available for access.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49050667",
                    "name": "Jason Zhang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "51fac29891f28d0b8500f194a35e080026f193e5",
            "title": "Seizing the Commuting Moment: Contextual Targeting Based on Mobile Transportation Apps",
            "abstract": "Despite the average daily commuting time of commuters increasing by the day, the way marketers can benefit from our commuting behaviors has not yet been thoroughly examined. Commuting can serve as ...",
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143888439",
                    "name": "A. Ghose"
                },
                {
                    "authorId": "2615891",
                    "name": "Hyeokkoo Eric Kwon"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "36472413",
                    "name": "Wonseok Oh"
                }
            ]
        },
        {
            "paperId": "ad6f321c4bf36ddfff1b18b0e1d31aa8c836217b",
            "title": "FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for Studying Fake News on Social Media",
            "abstract": "Social media has become a popular means for people to consume news. Meanwhile, it also enables the wide dissemination of fake news, i.e., news with intentionally false information, which brings significant negative effects to the society. Thus, fake news detection is attracting increasing attention. However, fake news detection is a non-trivial task, which requires multi-source information such as news content, social context, and dynamic information. First, fake news is written to fool people, which makes it difficult to detect fake news simply based on news contents. In addition to news contents, we need to explore social contexts such as user engagements and social behaviors. For example, a credible user\u2019s comment that \u201cthis is a fake news\u201d is a strong signal for detecting fake news. Second, dynamic information such as how fake news and true news propagate and how users\u2019 opinions toward news pieces are very important for extracting useful patterns for (early) fake news detection and intervention. Thus, comprehensive datasets which contain news content, social context, and dynamic information could facilitate fake news propagation, detection, and mitigation; while to the best of our knowledge, existing datasets only contains one or two aspects. Therefore, in this paper, to facilitate fake news related researches, we provide a fake news data repository FakeNewsNet, which contains two comprehensive datasets that includes news content, social context, and dynamic information. We present a comprehensive description of datasets collection, demonstrate an exploratory analysis of this dataset from different perspectives, and discuss the benefits of FakeNewsNet for potential applications on fake news study on",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145800151",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "82723305",
                    "name": "Deepak Mahudeswaran"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "38746648",
                    "name": "Huan Liu"
                }
            ]
        },
        {
            "paperId": "b851049478c4b674ad06a6aef16127bb75e301dd",
            "title": "Influential Users in Social Network Services: The Contingent Value of Connecting User Status and Brokerage",
            "abstract": "Social network services (SNSs) are fundamentally transforming the way businesses communicate with customers. Key issues in understanding social media and such services include how to generate more information traffic and how to increase information exposure. Unlike previous studies in the research stream studying patterns of information diffusion, we investigate the role of influential Twitter users, focusing on their positions in social networks, in redistributing contents generated by other users. To do this, we employ sociological theories in determining which characteristics of connecting users provide more influence. Specifically, a connecting user's position in networks observable to audience affects the user's influence in a way that connecting users with relatively high status and those embedded in a closely connected community exert the greatest impact on the network. Moreover, the value of a connecting user's network position can be contingent on the source user's status and network position: a high-status connecting user contributes more to information diffusion when tweets are generated by a low-status source user, and a connecting user who is embedded in a well-connected network contributes more to information diffusion when tweets are generated by a source user in a brokerage position. After exploring data on tweets about Apple products and services, we find evidence that support this set of theoretical claims. Such findings would also provide a useful guidance for practitioners to enhance information diffusion by effectively designing the sequence of content recipients based on their network positions.",
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108090997",
                    "name": "Young-Kyu Kim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1817200",
                    "name": "Janghyuk Lee"
                },
                {
                    "authorId": "2108586266",
                    "name": "Ji\u2010Hwan Lee"
                },
                {
                    "authorId": "1778660",
                    "name": "D. Straub"
                }
            ]
        },
        {
            "paperId": "cf933ca85f4505cebe874be2b9256748c75697be",
            "title": "Regularizing Matrix Factorization with User and Item Embeddings for Recommendation",
            "abstract": "Following recent successes in exploiting both latent factor and word embedding models in recommendation, we propose a novel Regularized Multi-Embedding (RME) based recommendation model that simultaneously encapsulates the following ideas via decomposition: (1) which items a user likes, (2) which two users co-like the same items, (3) which two items users often co-liked, and (4) which two items users often co-disliked. In experimental validation, the RME outperforms competing state-of-the-art models in both explicit and implicit feedback datasets, significantly improving Recall@5 by 5.9~7.0%, NDCG@20 by 4.3~5.6%, and MAP@10 by 7.9~8.9%. In addition, under the cold-start scenario for users with the lowest number of interactions, against the competing models, the RME outperforms NDCG@5 by 20.2% and 29.4% in MovieLens-10M and MovieLens-20M datasets, respectively. Our datasets and source code are available at: https://github.com/thanhdtran/RME.git.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120520792",
                    "name": "Thanh Tran"
                },
                {
                    "authorId": "2848353",
                    "name": "Kyumin Lee"
                },
                {
                    "authorId": "50325598",
                    "name": "Yiming Liao"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "e0c8f4689470f327405a7f6094580a843bfffa27",
            "title": "Micro-Giving: On the Use of Mobile Devices and Monetary Subsidies in Charitable Giving",
            "abstract": "Mobile devices are increasingly being used by non-profits and charitable organizations as an alternative channel for philanthropy. Specifically, organizations can construct fund-raising campaigns t...",
            "fieldsOfStudy": [
                "Business",
                "Computer Science",
                "Economics"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "34803014",
                    "name": "A. Gopal"
                },
                {
                    "authorId": "2675022",
                    "name": "Dokyun Lee"
                }
            ]
        },
        {
            "paperId": "eed1a4b3ec3b6de0fd1f0b8b2ec969b540fe41a0",
            "title": "FakeNewsNet: A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social Media",
            "abstract": "Social media has become a popular means for people to consume and share the news. At the same time, however, it has also enabled the wide dissemination of fake news, that is, news with intentionally false information, causing significant negative effects on society. To mitigate this problem, the research of fake news detection has recently received a lot of attention. Despite several existing computational solutions on the detection of fake news, the lack of comprehensive and community-driven fake news data sets has become one of major roadblocks. Not only existing data sets are scarce, they do not contain a myriad of features often required in the study such as news content, social context, and spatiotemporal information. Therefore, in this article, to facilitate fake news-related research, we present a fake news data repository FakeNewsNet, which contains two comprehensive data sets with diverse features in news content, social context, and spatiotemporal information. We present a comprehensive description of the FakeNewsNet, demonstrate an exploratory analysis of two data sets from different perspectives, and discuss the benefits of the FakeNewsNet for potential applications on fake news study on social media.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Sociology"
            ],
            "authors": [
                {
                    "authorId": "145800151",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "82723305",
                    "name": "Deepak Mahudeswaran"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "38746648",
                    "name": "Huan Liu"
                }
            ]
        },
        {
            "paperId": "05396efe70cc3e1dc0dfee157998fdbd6a651a1c",
            "title": "Understanding Temporal Backing Patterns in Online Crowdfunding Communities",
            "abstract": "Online crowdfunding platforms such as Kickstarter and Indiegogo make it possible for users to pledge funds to help creators bring their favorite projects into life. With an increasing number of users participating in crowdfunding, researchers are progressively motivated to investigate on improving user experiences by recommending projects and predicting project outcomes. To prompt the sustainable development of these platforms, understanding backers' behaviors becomes also important, as it helps platforms provide better services and improve backer retention. In particular, studying backers' temporal behaviors allows them to monitor the dynamics of backers' actions and develop appropriate strategies in time. Therefore, in this paper, we analyze a large amount of backer data from Kickstarter and Indiegogo, and do a comprehensive quantitative analysis on users' temporal backing patterns. Employing time series clustering methods, we discover four distinct temporal backing patterns on both platforms. In addition, we explore various characteristics of these backing patterns and possible factors affecting backers' behaviors. Finally, we leverage these insights to build a prediction model and show promising results to identify users' backing patterns at a very early stage. The datasets used in this paper are available at: https://goo.gl/ozgLvP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50325598",
                    "name": "Yiming Liao"
                },
                {
                    "authorId": "120520792",
                    "name": "Thanh Tran"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2848353",
                    "name": "Kyumin Lee"
                }
            ]
        },
        {
            "paperId": "5674a49501fa1e976cfdcf98e3b729999a512684",
            "title": "A Regression-Model-based Method for Combining Interestingness Measures of Association Rule Mining",
            "abstract": "Abstract",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "7ce1d51699314fd083d3e71295c80148946b7ffc",
            "title": "Stateless Puzzles for Real Time Online Fraud Preemption",
            "abstract": "The profitability of fraud in online systems such as app markets and social networks marks the failure of existing defense mechanisms. In this paper, we propose FraudSys, a real-time fraud preemption approach that imposes Bitcoin-inspired computational puzzles on the devices that post online system activities, such as reviews and likes. We introduce and leverage several novel concepts that include (i) stateless, verifiable computational puzzles, that impose minimal performance overhead, but enable the efficient verification of their authenticity, (ii) a real-time, graph based solution to assign fraud scores to user activities, and (iii) mechanisms to dynamically adjust puzzle difficulty levels based on fraud scores and the computational capabilities of devices. FraudSys does not alter the experience of users in online systems, but delays fraudulent actions and consumes significant computational resources of the fraudsters. Using real datasets from Google Play and Facebook, we demonstrate the feasibility of FraudSys by showing that the devices of honest users are minimally impacted, while fraudster controlled devices receive daily computational penalties of up to 3,079 hours. In addition, we show that with FraudSys, fraud does not pay off, as a user equipped with mining hardware (e.g., AntMiner S7) will earn less than half through fraud than from honest Bitcoin mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110109424",
                    "name": "Mizanur Rahman"
                },
                {
                    "authorId": "5659829",
                    "name": "Ruben Recabarren"
                },
                {
                    "authorId": "1798668",
                    "name": "Bogdan Carbunar"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "8f109843ea4e6091c4d64058e8b2b71e903552d6",
            "title": "Welcome message from conference co-chairs",
            "abstract": "This year, Huvitz wants to make sure to provide not only our usual great presentations, but also a new product in the main pavilion. Huvitz Ophthalmic Equipment delivers you the best quality and the most technologically advanced ophthalmic products for your practice to maximize efficiency and patient through put while minimizing down time to deliver peace of mind to the practitioner. HRK-8000A (Auto REF/KERatometer) Core Product Optical System Wavefront Technology measures the wavefront of light reflected from the retina and the refractive power with various sensors divided by sectors and analyzes them with extreme precision. Microscope System As one of the world leading ophthalmology equipment manufacturer, the professionalism of Huvitz engineering guarantees up scaled satisfaction for any R&D and production environment providing a more precise and optimized optical solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47666658",
                    "name": "Hsinchun Chen"
                },
                {
                    "authorId": "153325404",
                    "name": "D. Zeng"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "945165ac02035b2cf7ab32775f5910e159ef6431",
            "title": "Motivation-Aware Task Assignment in Crowdsourcing",
            "abstract": "We investigate how to leverage the notion of motivation in assigning tasks to workers and improving the performance of a crowdsourcing system. In particular, we propose to model motivation as the balance between task diversity\u2013i.e., the difference in skills among the tasks to complete, and task payment\u2013i.e., the difference between how much a chosen task offers to pay and how much other available tasks pay. We propose to test different task assignment strategies: (1) relevance, a strategy that assigns matching tasks, i.e., those that fit a worker's profile, (2) diversity, a strategy that chooses matching and diverse tasks, and (3) div-pay, a strategy that selects matching tasks that offer the best compromise between diversity and payment. For each strategy , we study multiple iterations where tasks are reassigned to workers as their motivation evolves. At each iteration, relevance and diversity assign tasks to a worker from an available pool of filtered tasks. div-pay, on the other hand, estimates each worker's motivation on-the-fly at each iteration, and uses it to assign tasks to the worker. Our empirical experiments study the impact of each strategy on overall performance. We examine both requester-centric and worker-centric performance dimensions and find that different strategies prevail for different dimensions. In particular, relevance offers the best task throughput while div-pay achieves the best outcome quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3416184",
                    "name": "Julien Pilourdault"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                }
            ]
        },
        {
            "paperId": "9873220aa92df2908830bf18681ebf98b410b560",
            "title": "Wearing Many (Social) Hats: How Different Are Your Different Social Network Personae?",
            "abstract": "\n \n This paper investigates when users create profiles in different social networks, whether they are redundant expressions of the same persona, or they are adapted to each platform. Using the personal webpages of 116,998 users on About.me, we identify and extract matched user profiles on several major social networks including Facebook, Twitter, LinkedIn, and Instagram. We find evidence for distinct site-specific norms, such as differences in the language used in the text of the profile self-description, and the kind of picture used as profile image. By learning a model that robustly identifies the platform given a user\u2019s profile image (0.657\u20130.829 AUC) or self-description (0.608\u20130.847 AUC), we confirm that users do adapt their behaviour to individual platforms in an identifiable and learnable manner. However, different genders and age groups adapt their behaviour differently from each other, and these differences are, in general, consistent across different platforms. We show that differences in social profile construction correspond to differences in how formal or informal the platform is.\n \n",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "36957299",
                    "name": "Changtao Zhong"
                },
                {
                    "authorId": "1818420",
                    "name": "Hau-Wen Chang"
                },
                {
                    "authorId": "2840779",
                    "name": "Dmytro Karamshuk"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2440079",
                    "name": "Nishanth R. Sastry"
                }
            ]
        },
        {
            "paperId": "df9b8323182fa869cc90426b249e9e2c6bc8a7a7",
            "title": "Inequalities, Preferences and Rankings in US Sports Coach Hiring Networks",
            "abstract": "Hiring a head coach of a college sports team is vital which will definitely have a great influence on the later development of the team. However, a lot of attention has been focused on each coach's individual features. A systematic and quantitative analysis of the whole coach hiring market is lacking. In a coach hiring network, the coaches are actually voting with their feet. It is interesting to analyze what factors are affecting the \"footprint\" left by those head coaches. In this paper, we collect more than 12,000 head coach hiring records in two different popular sports from the NCAA. Using network-based methods, we build the coach hiring network in the NCAA men's basketball and football. We find that: (1).the coach hiring network is of great inequality in coach production with a Gini coefficient close to 0.60. (2).coaches prefer to work within the same geographical region and the same division to their alma maters'. (3).the coach production rankings we calculated using network-based methods are generally correlated to the authoritative rankings, but also show disaccord in specific time period. The results provide us a novel view and better understanding of the coach hiring market in the NCAA and shed new light on the coach hiring system.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "31480727",
                    "name": "Huanshen Wei"
                },
                {
                    "authorId": "49050667",
                    "name": "Jason Zhang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "0bf61b148d927b63ffc653ff75400c16cee23636",
            "title": "Uncovering Fake Likers in Online Social Networks",
            "abstract": "As the commercial implications of Likes in online social networks multiply, the number of fake Likes also increase rapidly. To maintain a healthy ecosystem, however, it is critically important to prevent and detect such fake Likes. Toward this goal, in this paper, we investigate the problem of detecting the so-called \"fake likers\" who frequently make fake Likes for illegitimate reasons. To uncover fake Likes in online social networks, we: (1) first collect a substantial number of profiles of both fake and legitimate Likers using linkage and honeypot approaches, (2) analyze the characteristics of both types of Likers, (3) identify effective features exploiting the learned characteristics and apply them in supervised learning models, and (4) thoroughly evaluate their performances against three baseline methods and under two attack models. Our experimental results show that our proposed methods with effective features significantly outperformed baseline methods, with accuracy = 0.871, false positive rate = 0.1, and false negative rate = 0.14.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39309053",
                    "name": "Prudhvi Ratna Badri Satya"
                },
                {
                    "authorId": "2848353",
                    "name": "Kyumin Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "120520792",
                    "name": "Thanh Tran"
                },
                {
                    "authorId": "49050667",
                    "name": "Jason Zhang"
                }
            ]
        },
        {
            "paperId": "17fc098056847450482e1085520cf45fd4618275",
            "title": "\u201cTold you i didn't like it\u201d: Exploiting uninteresting items for effective collaborative filtering",
            "abstract": "We study how to improve the accuracy and running time of top-N recommendation with collaborative filtering (CF). Unlike existing works that use mostly rated items (which is only a small fraction in a rating matrix), we propose the notion of pre-use preferences of users toward a vast amount of unrated items. Using this novel notion, we effectively identify uninteresting items that were not rated yet but are likely to receive very low ratings from users, and impute them as zero. This simple-yet-novel zero-injection method applied to a set of carefully-chosen uninteresting items not only addresses the sparsity problem by enriching a rating matrix but also completely prevents uninteresting items from being recommended as top-N items, thereby improving accuracy greatly. As our proposed idea is method-agnostic, it can be easily applied to a wide variety of popular CF methods. Through comprehensive experiments using the Movielens dataset and MyMediaLite implementation, we successfully demonstrate that our solution consistently and universally improves the accuracies of popular CF methods (e.g., item-based CF, SVD-based CF, and SVD++) by two to five orders of magnitude on average. Furthermore, our approach reduces the running time of those CF methods by 1.2 to 2.3 times when its setting produces the best accuracy. The datasets and codes that we used in experiments are available at: https://goo.gl/KUrmip.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7905032",
                    "name": "Won-Seok Hwang"
                },
                {
                    "authorId": "37773165",
                    "name": "J. Parc"
                },
                {
                    "authorId": "51967113",
                    "name": "Sang-Wook Kim"
                },
                {
                    "authorId": "1865093",
                    "name": "Jongwuk Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "b8a09cfb46a6e8bf97f602e741dc7d9f5cce536f",
            "title": "Teens Engage More with Fewer Photos: Temporal and Comparative Analysis on Behaviors in Instagram",
            "abstract": "Research has suggested that teens are more active and engaged than adults on social media. Most of such observations, however, have been made through the analysis of limited ethnographic or cross-sectional data. Using a temporally extended, large-scale dataset and comparative analyses to remedy this shortcoming, we examined how and why the age difference in the behaviors of users in Instagram might have occurred through the lenses of social cognition, developmental psychology, and human-computer interaction. We proposed two hypotheses -- teens as digital natives and the need for social interactions -- as the theoretical framework for understanding the factors that help explain the behavioral differences. Our computational analysis identified the following novel findings: (1) teens post fewer photos than adults; (2) teens remove more photos based on the number of Likes the photos received; and (3) teens have less diverse photo content. Our analysis was also able to confirm prior ethnographic accounts that teens are more engaged in Liking and commenting, and express their emotions and social interests more than adults. We discussed theoretical and practical interpretations and implications as well as future research directions from the results. Our datasets are available at: https://goo.gl/LqTYNv",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "1835709",
                    "name": "J. Jang"
                },
                {
                    "authorId": "35655049",
                    "name": "Kyungsik Han"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "46449923",
                    "name": "Haiyan Jia"
                },
                {
                    "authorId": "2150671",
                    "name": "Patrick C. Shih"
                }
            ]
        },
        {
            "paperId": "c4c31f6b3e74e034dcf6bac0c3880f5189fac747",
            "title": "CrowdSky: Skyline Computation with Crowdsourcing",
            "abstract": "In this paper, we propose a crowdsourcing-based approach to solving skyline queries with incomplete data. Our main idea is to leverage crowds to infer the pair-wise preferences between tuples when the values of tuples in some attributes are unknown. Specifically, our proposed solution considers three key factors used in existing crowd-enabled algorithms: (1) minimizing a monetary cost in identifying a crowdsourced skyline by using a dominating set, (2) reducing the number of rounds for latency by parallelizing the questions asked to crowds, and (3) improving the accuracy of a crowdsourced skyline by dynamically assigning the number of crowd workers per question. We evaluate our solution over both simulated and real crowdsourcing using the Amazon Mechanical Turk. Compared to a sort-based baseline method, our solution significantly minimizes the monetary cost, and reduces the number of rounds up to two orders of magnitude. In addition, our dynamic majority voting method shows higher accuracy than both static majority voting method and the existing solution using unary questions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1865093",
                    "name": "Jongwuk Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "51967113",
                    "name": "Sang-Wook Kim"
                }
            ]
        },
        {
            "paperId": "d6877412578d31904bd2064f72c04679788d2efb",
            "title": "Likeology: modeling, predicting, and aggregating likes in social media",
            "abstract": "The recent dramatic increase in the usage and prevalence of social media has led to the creation and sharing of a significant amount of user-generated contents (UGCs) in various formats (e.g., photos, videos, blogs). Users not only generate and access UGCs in social media, but also actively evaluate and interact with them by adding comments or expressing their preferences toward the UGCs. In particular, recently, user preferences by means of a \"Like\" button have prevailed. Such a Like button appears in different names too (e.g., Like in Facebook, +1 in Google+, re-pin in Pinterest, and favorite in Flickr). Despite such massive social media data with rich Like-like relationships therein, however, there has not been a dedicated tutorial that covered the diverse aspects of Likes in a comprehensive and cohesive manner. As understanding user preferences (via Likes) and providing further personalized services such as recommendation thereof in social media has keen implications in businesses, the topic of Likes has become increasingly important in recent years. In this tutorial, as such, to address this important and timely topic, we aim to provide a 3-hour tutorial, named as \"Likeology\" that presents a comprehensive overview of Likes in social media and covers mainly 3 topics: (1) how to model Likes, (2) how to predict the evolution of Likes, and (3) how to aggregate Likes. This tutorial is partially based on our earlier version [9].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "fcdaa10b7c869e7881ef283e49cee38967355c30",
            "title": "Virtual World-Based Information Security Learning: Design and Evaluation",
            "abstract": "ABSTRACT There has been a growing interest and enthusiasm for the application of virtual worlds in learning and training. This research proposes a design framework of a virtual world-based learning environment that integrates two unique features of the virtual world technology, immersion and interactivity, with an instructional strategy that promotes self-regulatory learning. We demonstrate the usefulness and assess the e?ectiveness of our design in the context of information security learning. In particular, the information security learning module implemented in Second Life was incorporated into an Introduction to Information Security course. Data from pre- and post- learn-ing surveys were used to evaluate the e?ectiveness of the learning module. Overall, the results strongly suggest that the virtual world-based learning environment enhances information security learning, thus supporting the e?ectiveness of the proposed design framework. Additional results suggest that learner traits have an important in?uence on learning outcomes through perceived enjoyment. The study o?ers useful design and implementation guidelines for organizations and universities to develop a virtual world-based learning environment. It also rep-resents an initial step towards the design and explanation theories of virtual world-based learning environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1806976",
                    "name": "J. Ryoo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1786423",
                    "name": "A. Techatassanasoontorn"
                }
            ]
        },
        {
            "paperId": "ffe7d74f990825c15982b4a29fcf4760cc39914c",
            "title": "Teens are from mars, adults are from venus: analyzing and predicting age groups with behavioral characteristics in instagram",
            "abstract": "We present behavioral characteristics of teens and adults in Instagram and prediction of them from their behaviors. Based on two independently created datasets from user profiles and tags, we identify teens and adults, and carry out comparative analyses on their online behaviors. Our study reveals: (1) significant behavioral differences between two age groups; (2) the empirical evidence of classifying teens and adults with up to 82% accuracy, using traditional predictive models, while two baseline methods achieve 68% at best; and (3) the robustness of our models by achieving 76%---81% when tested against an independent dataset obtained without using user profiles or tags. Our datasets are available at: https://goo.gl/LqTYNv",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35655049",
                    "name": "Kyungsik Han"
                },
                {
                    "authorId": "2144568286",
                    "name": "Sanghack Lee"
                },
                {
                    "authorId": "1835709",
                    "name": "J. Jang"
                },
                {
                    "authorId": "1787283",
                    "name": "Yong Ju Jung"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "69a80667199860c28ac218e6105466de9407d223",
            "title": "Data-driven crowdsourcing: Management, mining, and applications",
            "abstract": "In this 3-hour tutorial, we present the landscape of recent developments in data management and mining research, and survey a selected set of state-of-the-art works that significantly extended existing database reserach in order to incorporate and exploit the novel notion of \u201ccrowdsourcing\u201d in a creative fashion. In particular, three speakers take turns to present the topics of human-powered database operations, crowdsourced data mining, and the application of crowdsourcing in social media, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "7d627eed1388fe2844e17f8017d26eccc5f84263",
            "title": "Exploring Tag-based Like Networks",
            "abstract": "The emergence of social media has had a significant impact on how people communicate, interact, and socialize. People engage in social media in different ways by not only adding content such as photos, texts, and videos, but also adding tags, Likes, comments, and following others. Through these activities, people form and develop social connections and networks. In this paper, we present a two-dimensional Like network formed and developed by people who have a same tag in their photos. Based on the dataset consisting of 51K photos posted by 36K users in Instagram, we present the structural and relational aspects of tag-based Like networks. Our study results highlight that Like networks have different sizes and degrees of network components depending on a tag type. We also found that a large portion of Likes came from random users for all networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35655049",
                    "name": "Kyungsik Han"
                },
                {
                    "authorId": "1835709",
                    "name": "J. Jang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "9dcbc0995422e017cec91f9c4302485fe88b0f3b",
            "title": "No Reciprocity in \"Liking\" Photos: Analyzing Like Activities in Instagram",
            "abstract": "In social media, people often press a \"Like\" button to indicate their shared interest in a particular content or to acknowledge the user who posted the content. Such activities form relationships and networks among people, raising interesting questions about their unique characteristics and implications. However, little research has investigated such Likes as a main study focus. To address this lack of understanding, based on a theoretical framework, we present an analysis of the structural, influential, and contextual aspects of Like activities from the test datasets of 20 million users and their 2 billion Like activities in Instagram. Our study results first highlight that Like activities and networks increase exponentially, and are formed and developed by one's friends and many random users. Second, we observe that five other essential Instagram elements influence the number of Likes to different extents, but following others will not necessarily increase the number of Likes that one receives. Third, we explore the relationship between LDA-based topics and Likes, characterize two user groups-specialists and generalists-and show that specialists tend to receive more Likes and promote themselves more than generalists. We finally discuss theoretical and practical implications and future research directions.",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1835709",
                    "name": "J. Jang"
                },
                {
                    "authorId": "35655049",
                    "name": "Kyungsik Han"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "c50235ee45af2574444dbe3f2b938ab17ff39340",
            "title": "LIKE and Recommendation in Social Media",
            "abstract": "This tutorial covers the state-of-the-art developments in LIKE and recommendation in social media. It is designed for graduate students, practitioners, or IT managers with general understanding on WWW and social media. No prerequisite is expected.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "38746648",
                    "name": "Huan Liu"
                }
            ]
        },
        {
            "paperId": "e46848bbfeef0ecfc5b8d0ed89db250fe22ac6b0",
            "title": "Generation Like: Comparative Characteristics in Instagram",
            "abstract": "The emergence of social media has had a significant impact on how people communicate and socialize. Teens use social media to make and maintain social connections with friends and build their reputation. However, the way of analyzing the characteristics of teens in social media has mostly relied on ethnographic accounts or quantitative analyses with small datasets. This paper shows the possibility of detecting age information in user profiles by using a combination of textual and facial recognition methods and presents a comparative study of 27K teens and adults in Instagram. Our analysis highlights that (1) teens tend to post fewer photos but highly engage in adding more tags to their own photos and receiving more Likes and comments about their photos from others, and (2) to post more selfies and express themselves more than adults, showing a higher sense of self-representation. We demonstrate the application of our novel method that shows clear trends of age differences as well as substantiates previous insights in social media.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "1835709",
                    "name": "J. Jang"
                },
                {
                    "authorId": "35655049",
                    "name": "Kyungsik Han"
                },
                {
                    "authorId": "2150671",
                    "name": "Patrick C. Shih"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "41bed95ed7beca1efb69049245ee987e12e41596",
            "title": "Human-Powered Blocking in Entity Resolution : A Feasibility Study",
            "abstract": "Entity Resolution (ER) is the problem of matching the records that refer to the same entity within or across two or more data sources. In recent years, human-powered ER solutions have been proposed so that challenging ER tasks, that machines cannot do well, can be helped by human workers. While successful in achieving high matching accuracy, existing human-powered ER methods did not incorporate a core technique, i.e., blocking, for improving the scalability of the ER process. To address this issue, this paper carries out the feasibility study to validate whether the blocking technique can be integrated into the human-powered ER. Specifically, we first propose two variations of human-powered blocking. We then validate their effectiveness in improving the scalability of the ER process through simulated crowdsourcing and AMT-based experiments in synthetic and real-life datasets, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145388895",
                    "name": "Weiqiang Li"
                },
                {
                    "authorId": "1865093",
                    "name": "Jongwuk Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "77eaabb87b516d1eb6268e46ca88e2aa34349bc2",
            "title": "IT Architecture and Organizational Learning: The Effect of Modularity and Virtualization",
            "abstract": "We study the relationship between the IT architecture and organizational learning. Specifically, we explore the impact of the modularity and virtualization of IT architecture on a firm's organizati...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3236121",
                    "name": "Insoo Son"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2229396",
                    "name": "Gwanhoo Lee"
                },
                {
                    "authorId": "2304998977",
                    "name": "Youngjin Yoo"
                }
            ]
        },
        {
            "paperId": "ecf156f2e0fcd6c349928b51701acff680744ef8",
            "title": "Recommendation of newly published research papers using belief propagation",
            "abstract": "The problem to retrieve most relevant research papers for a given academic is studied. Existing solutions cannot adequately address the recommendation of new papers due to their lack of history information, the so-called cold start problem. Using the graphical model built from citation information between a new paper pi and published papers, toward this challenge, we propose a novel approach based on a probabilistic inference algorithm, the Belief Propagation (BP), to predict the likelihood of pi's relevance to a target academic. Compared to item-based collaborative filtering method using a DBLP data set, the empirical validation shows an improvement in accuracy up to 26% in F1 score.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2746635",
                    "name": "Jiwoon Ha"
                },
                {
                    "authorId": "2449263",
                    "name": "Soon-Hyoung Kwon"
                },
                {
                    "authorId": "51967113",
                    "name": "Sang-Wook Kim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "22c51d6ad54ac32d5e3ab93119c7223944e6449e",
            "title": "On handling textual errors in latent document modeling",
            "abstract": "As large-scale text data become available on the Web, textual errors in a corpus are often inevitable (e.g., digitizing historic documents). Due to the calculation of frequencies of words, however, such textual errors can significantly impact the accuracy of statistical models such as the popular Latent Dirichlet Allocation (LDA) model. To address such an issue, in this paper, we propose two novel extensions to LDA (i.e., TE-LDA and TDE-LDA): (1) The TE-LDA model incorporates textual errors into term generation process; and (2) The TDE-LDA model extends TE-LDA further by taking into account topic dependency to leverage on semantic connections among consecutive words even if parts are typos. Using both real and synthetic data sets with varying degrees of \"errors\", our TDE-LDA model outperforms: (1) the traditional LDA model by 16%-39% (real) and 20%-63% (synthetic); and (2) the state-of-the-art N-Grams model by 11%-27% (real) and 16%-54% (synthetic).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122901239",
                    "name": "Tao Yang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "62e0cc767de7535b384d6ef4901fe7a766f73546",
            "title": "Enforcing Secure and Privacy-Preserving Information Brokering in Distributed Information Sharing",
            "abstract": "Today's organizations raise an increasing need for information sharing via on-demand access. Information brokering systems (IBSs) have been proposed to connect large-scale loosely federated data sources via a brokering overlay, in which the brokers make routing decisions to direct client queries to the requested data servers. Many existing IBSs assume that brokers are trusted and thus only adopt server-side access control for data confidentiality. However, privacy of data location and data consumer can still be inferred from metadata (such as query and access control rules) exchanged within the IBS, but little attention has been put on its protection. In this paper, we propose a novel approach to preserve privacy of multiple stakeholders involved in the information brokering process. We are among the first to formally define two privacy attacks, namely attribute-correlation attack and inference attack, and propose two countermeasure schemes automaton segmentation and query segment encryption to securely share the routing decision-making responsibility among a selected set of brokering servers. With comprehensive security analysis and experimental results, we show that our approach seamlessly integrates security enforcement with query routing to provide system-wide security with insignificant overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680720",
                    "name": "Fengjun Li"
                },
                {
                    "authorId": "145607083",
                    "name": "Bo Luo"
                },
                {
                    "authorId": "39336958",
                    "name": "Peng Liu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "8351687",
                    "name": "C. Chu"
                }
            ]
        },
        {
            "paperId": "71ebb9fc611d282c6f611baf8b17458d3133ae47",
            "title": "Understanding the Effect of Message Content and User Identity on Information Diffusion in Online Social Networks",
            "abstract": "This study attempts to understand information diffusion in online social networks by investigating the factors that affect message dissemination. Particularly, we identify contextual factors from both message and user characteristics on Twitter and examine their impact on diffusion volume and speed. We collected 11,346 tweets, which have been retweeted at least once, regarding the three major mobile carriers in Korea, between September and December 2011. These tweets have generated 59,111 retweets. Our analysis indicates that information diffusion on Twitter in terms of volume and speed is mainly affected by message characteristics such as the inclusion of corporate social responsibility activities. However, the effect of message characteristics on information diffusion is heterogeneous by user type. Our study broadens the knowledge on information diffusion mechanism in online social networks and provides managerial implications on the strategic utilization of online social networks for marketing communications with customers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3236121",
                    "name": "Insoo Son"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "47659447",
                    "name": "Youngkyu Kim"
                }
            ]
        },
        {
            "paperId": "9616f2d895183b9adcbd47100cde08edf021c484",
            "title": "Measuring the Optimality of Hadoop Optimization",
            "abstract": "In recent years, much research has focused on how to optimize Hadoop jobs. Their approaches are diverse, ranging from improving HDFS and Hadoop job scheduler to optimizing parameters in Hadoop configurations. Despite their success in improving the performance of Hadoop jobs, however, very little is known about the limit of their optimization performance. That is, how optimal is a given Hadoop optimization? When a Hadoop optimization method X improves the performance of a job by Y %, how do we know if this improvement is as good as it can be? To answer this question, in this paper, we first examine the ideal best case, the lower bound, of running time for Hadoop jobs and develop a measure to accurately estimate how optimal a given Hadoop optimization is with respect to the lower bound. Then, we demonstrate how one may exploit the proposed measure to improve the optimization of Hadoop jobs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7891272",
                    "name": "Woo-Cheol Kim"
                },
                {
                    "authorId": "3037833",
                    "name": "Changryong Baek"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "a2b2369b78420dbe0fa9cc48ad7998f863d7be47",
            "title": "Sentiment and topic analysis on social media: a multi-task multi-label classification approach",
            "abstract": "Both sentiment analysis and topic classification are frequently used in customer care and marketing. They can help people understand the brand perception and customer opinions from social media, such as online posts, tweets, forums, and blogs. As such, in recent years, many solutions have been proposed for both tasks. However, we believe that the following two problems have not been addressed adequately: (1) Conventional solutions usually treat the two tasks in isolation. When the two tasks are closely related (e.g., posts about \"customer care\" often have a \"negative\" tone), exploring their correlation may yield a better accuracy; (2) Each post is usually assigned with only one sentiment label and one topic label. Since social media is, compared to traditional document corpus, more noisy, ambiguous, and sparser, single label classification may not be able to capture the post classes accurately. To address these two problems, in this paper, we propose a multi-task multi-label (MTML) classification model that performs classification of both sentiments and topics concurrently. It incorporates results of each task from prior steps to promote and reinforce the other iteratively. For each task, the model is trained with multiple labels so that they can help address class ambiguity. In the empirical validation, we compare the accuracy of MTML model against four competing methods in two different settings. Results show that MTML produces a much higher accuracy of both sentiment and topic classifications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30698726",
                    "name": "Shu Huang"
                },
                {
                    "authorId": "145439284",
                    "name": "Wei Peng"
                },
                {
                    "authorId": "2108990296",
                    "name": "Jingxuan Li"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "f41f62f7cebca019e90d088c5fd4fde5fbe6740c",
            "title": "Behavioural description based web service composition using abstraction and refinement",
            "abstract": "The web service composition problem with respect to behavioural descriptions deals with the automatic synthesis of a coordinator web service that controls a set of web services to reach a goal state. Despite its importance, however, solving the problem for a general case when the coordinator has only partial observations remains doubly exponential in the number of variables in web service descriptions. Toward this challenge, we propose two novel signature preserving and subsuming approximation-based approaches using abstraction and refinement. Given a set of web service behavioural descriptions and a reachability goal, we automatically construct abstract web services which have less variables using over-approximation. If our method identifies a coordinator web service, the coordinator is guaranteed to control the given web services to reach the goal state no matter how they behave. Otherwise, our method refines the current abstraction by adding some variables that have strong dependency on the goal variables.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1928204",
                    "name": "Hyunyoung Kil"
                },
                {
                    "authorId": "39631150",
                    "name": "Wonhong Nam"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "fc4d02b5feb5fb01d88f00cf92cca20c531c1d94",
            "title": "Steeler nation, 12th man, and boo birds: Classifying Twitter user interests using time series",
            "abstract": "The problem of Twitter user classification using the contents of tweets is studied. We generate time series from tweets by exploiting the latent temporal information and solve the classification problem in time series domain. Our approach is inspired by the fact that Twitter users sometimes exhibit the periodicity pattern when they share their activities or express their opinions. We apply our proposed methods to both binary and multi-class classification of sports and political interests of Twitter users and compare the performance against eight conventional classification methods using textual features. Experimental results using 2.56 million tweets show that our best binary and multi-class approaches improve the classification accuracy over the best baseline binary and multi-class approaches by 15% and 142%, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Tao Yang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "40295902",
                    "name": "Su Yan"
                }
            ]
        },
        {
            "paperId": "097634a3c3c91570580ac29ac5f23a8e493c88dc",
            "title": "@Phillies Tweeting from Philly? Predicting Twitter User Locations with Spatial Word Usage",
            "abstract": "We study the problem of predicting home locations of Twitter users using contents of their tweet messages. Using three probability models for locations, we compare both the Gaussian Mixture Model (GMM) and the Maximum Likelihood Estimation (MLE). In addition, we propose two novel unsupervised methods based on the notions of Non-Localness and Geometric-Localness to prune noisy data from tweet messages. In the experiments, our unsupervised approach improves the baselines significantly and shows comparable results with the supervised state-of-the-art method. For 5,113 Twitter users in the test set, on average, our approach with only 250 selected local words or less is able to predict their home locations (within 100 miles) with the accuracy of 0.499, or has 509.3 miles of average error distance at best.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1818420",
                    "name": "Hau-Wen Chang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "3351046",
                    "name": "Mohammed Eltaher"
                },
                {
                    "authorId": "2108349726",
                    "name": "JeongKyu Lee"
                }
            ]
        },
        {
            "paperId": "0b6ddd139a34393c7d66a186187099dec9d0d3dd",
            "title": "Location Type Classification Using Tweet Content",
            "abstract": "Location context in social media plays an important role in many applications. In addition to explicit location sharing via popular \"check in\" service, user-posted content could also implicitly reveals users' location context. Identifying such a location context based on content is an interesting problem because it is not only important in inferring social ties between people, but also vital for applications such as user profiling and targeted advertising. In this paper, we study the problem of location type classification using tweet content. We extend probabilistic text classification models to incorporate temporal features and user history information in terms of probabilistic priors. Experimental results show that our extensions can boost classification accuracy effectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145575469",
                    "name": "Haibin Liu"
                },
                {
                    "authorId": "145607083",
                    "name": "Bo Luo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "1f97fcacded4e1c273558db8272f1f4ad7b9f2a3",
            "title": "Large-Scale Longitudinal Analysis of SOAP-Based and RESTful Web Services",
            "abstract": "While the usage of web services has increased explosively in recent years, very few studies examined the characteristics of web services using large-scale real data for a long period of time. In this paper, we present one such a large scale longitudinal analysis of publicly available web services of SOAP-based and RESTful types. For the period of roughly one year and from five different world-wide locations, we closely monitor the ups and downs of various basic properties of web services and their QoS values using a total of 825,132 real web services.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30401303",
                    "name": "Wei Jiang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2108842919",
                    "name": "Song Hu"
                }
            ]
        },
        {
            "paperId": "2214aafebee99461cb7101ed2750202142940e86",
            "title": "Continuous Query for QoS-Aware Automatic Service Composition",
            "abstract": "Current QoS-aware automatic service composition queries over a network of Web services are often one-time innature. After a network of Web services is built, such queries are issued once, and answers are found from the scratch. The underlying assumption is that the participating Web services are rather static so that their functional and non-functional parameters seldom change. However, such an assumption is often baseless. New services come and go, service APIs change gradually, and QoS values fluctuate. Therefore, a support for efficiently handling \"continuous\" service composition queries is desired. In this paper, we propose an event driven continuous query algorithm for QoS-aware automatic service composition problem to cope with different types of dynamic services. Moreover, we integrated this algorithm in our service composition system, QSynth. Finally, we evaluate our proposal using both real QoS data and synthetic Web service data and show the superior performance of ours, compared to the state-of-the art solution which won the performance championship of Web Service Challenge in 2009 and 2010.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30401303",
                    "name": "Wei Jiang"
                },
                {
                    "authorId": "40845069",
                    "name": "Songlin Hu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2058166740",
                    "name": "Shuai Gong"
                },
                {
                    "authorId": "145778340",
                    "name": "Zhiyong Liu"
                }
            ]
        },
        {
            "paperId": "3e7ac6505123d79c4581c22507c0d5901209f50c",
            "title": "The Viral Effect of Online Social Network on New Products Promotion: Investigating Information Diffusion on Twitter",
            "abstract": "Abstract \uc2e0\uc81c\ud488 \ud504\ub85c\ubaa8\uc158\uc5d0 \ub300\ud55c \uc628\ub77c\uc778 \uc18c\uc15c\ub124\ud2b8\uc6cc\ud06c\uc758\uad6c\uc804\ud6a8\uacfc \ubd84\uc11d : \ud2b8\uc704\ud130\uc758 \uc815\ubcf4\uc804\ub2ec\uacfc\uc815\uc744 \uc911\uc2ec\uc73c\ub85c 1) \uae40\ud615\uc9c4 * \u2024\uc190\uc778\uc218 ** \u2024\uc774\ub3d9\uc6d0 *** \uc778\ud130\ub137\uc73c\ub85c \ub300\ud45c\ub418\ub294 \uc815\ubcf4\uae30\uc220\uc758 \ubc1c\uc804\uc740 \uc6b0\ub9ac\uc758 \uc77c\uc0c1\uc0dd\ud65c\uacfc \uae30\uc5c5\ud65c\ub3d9\uc5d0 \ub9ce\uc740 \uc601\ud5a5\uc744 \ubbf8\uccd0 \uc654\ub2e4. \uc544\uc6b8\ub7ec \ucd5c\uadfc\uc5d0\ub294 \uc628\ub77c\uc778 \uc18c\uc15c\ub124\ud2b8\uc6cc\ud06c\ub77c\ub294 \uc0c8\ub85c\uc6b4 \uc778\ud130\ub137 \ucee4\ubba4\ub2c8\ucf00\uc774\uc158 \ucc44\ub110\uc758 \ub4f1\uc7a5\uacfc \ud655\uc0b0\uc73c\ub85c \uc778\ud574 \uc774\uc6a9\uc790\ub4e4\uc740 \uc2dc\uac04\uacfc \uacf5\uac04\uc801\uc778 \uc81c\uc57d \uc5c6\uc774 \uc804\uc138\uacc4\uc801\uc778 \uc758\uc0ac\uc18c\ud1b5\uc744 \ud560 \uc218 \uc788\uac8c \ub418\uc5c8\uace0 \uc6b0\ub9ac\uc0ac\ud68c\ub294 \ub610 \ub2e4\ub978 \ud328\ub7ec\ub2e4\uc784\uc758 \ubcc0\ud654\ub97c \ub9de\uc774\ud558\uace0 \uc788\ub2e4. \uc774\ub4e4 \uc18c\uc15c\ub124\ud2b8\uc6cc\ud06c \uc911 \ud2b8\uc704\ud130\ub294 \uac00\uc7a5 \ube60\ub978 \uc131\uc7a5\uc138\ub97c \ubcf4\uc774\ub294 \uc628\ub77c\uc778 \ub9e4\uccb4 \uc911 \ud558\ub098\ub85c \uc0ac\uc6a9\uc790\ub294 140 \ub2e8\uc5b4 \uc774\ub0b4\uc758 \ube44\uad50\uc801 \uac04\ub2e8\ud55c \ubb38\uc7a5\uc744 \uc628\ub77c\uc778 \uc0c1\uc5d0 \uac8c\uc2dc\ud558\uace0 (Tweet) \ub2e4\ub978 \uc0ac\uc6a9\uc790\ub4e4\uc774 \uac8c\uc2dc\ud55c \uba54\uc2dc\uc9c0\ub97c \ub2e4\uc2dc \uac8c\uc2dc\ud560 (Retweet) \uc218\ub3c4 \uc788\ub2e4. \ud2b8\uc704\ud130\uc758 \uc774\ub7ec\ud55c Tweet/Retweet \uae30\ub2a5\uc740 \uc0c8\ub85c\uc6b4 \uc628\ub77c\uc778 \uc815\ubcf4\ud655\uc0b0 \ub9e4\uce74\ub2c8\uc998\uc758 \uc608\ub97c \ubcf4\uc5ec\uc8fc\uba70 \uc0c1\ud638 \uc758\uc0ac\uc18c\ud1b5\uc758 \uc18d\ub3c4\uc640 \ubc94\uc704\uc5d0 \uc601\ud5a5\uc744 \uc8fc\uace0 \uc788\ub2e4. \ube44\uc988\ub2c8\uc2a4 \uad00\uc810\uc5d0\uc11c\uc758 \ud2b8\uc704\ud130\uc758 \ud655\uc0b0\uc740 \uc628\ub77c\uc778 \uc18c\uc15c\ub124\ud2b8\uc6cc\ud06c\ub97c \uc81c\ud488 \ud504\ub85c\ubaa8\uc158\uc744 \uc704\ud55c \uc0c8\ub85c\uc6b4 \ub9c8\ucf00\ud305 \ucee4\ubba4\ub2c8\ucf00\uc774\uc158\uc758 \ub3c4\uad6c\ub85c \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \uae30\ud68c\ub97c \uc81c\uacf5\ud558\uace0 \uc788\ub2e4. \ubcf8 \uc5f0\uad6c\ub294 \ub9c8\ucf00\ud305 \uc804\ub7b5\uc801 \uad00\uc810\uc5d0\uc11c \ud2b8\uc704\ud130\uc758 \uc7a0\uc7ac\uc131\uc744 \uc774\ud574\ud558\ub294 \ubaa9\uc801\uc5d0\uc11c \ud2b8\uc704\ud130 \uc0c1\uc758 \uc815\ubcf4\uc804\ub2ec\uccb4\uacc4 \uc911\uc2ec\uc73c\ub85c \uc2e0\uc81c\ud488 \ud504\ub85c\ubaa8\uc158\uc5d0 \ub300\ud55c \uc628\ub77c\uc778 \uc18c\uc15c\ub124\ud2b8\uc6cc\ud06c\uc758 \uad6c\uc804\ud6a8\uacfc\ub97c \ubd84\uc11d \ud558\uc600\ub2e4. \uc774\ub97c \uc704\ud574 \ud2b9\uc815 \uc2e0\uc81c\ud488\uacfc \uad00\ub828\ud558\uc5ec 2011\ub144 6\uc6d4\ubd80\ud130 9\uc6d4 \uc0ac\uc774\uc5d0 \uac8c\uc7ac\ub41c \ud2b8\uc704\ud130 \uba54\uc2dc\uc9c0\ub97c \uc218\uc9d1\ud558\uc5ec \ud2b8\uc704\ud130 \uba54\uc2dc\uc9c0\uac00 \uc81c\uacf5\ud558\ub294 \uc815\ubcf4\uc758 \ucda9\uc2e4\ub3c4, \uba54\uc2dc\uc9c0 \ud2b9\uc131, \uba54\uc2dc\uc9c0 \uac8c\uc7ac\uc790\uc758 \uc131\uaca9\ub4f1\uacfc \uac19\uc740 \ubcc0\uc218\uc640 \uc628\ub77c\uc778 \uc18c\uc15c\ub124\ud2b8\uc6cc\ud06c \uc0c1\uc758 \uad6c\uc804\ud6a8\uacfc (\ub9ac\ud2b8\uc717 \ud69f\uc218)\uc640\uc758 \uad00\uacc4\ub97c \ubd84\uc11d\ud558\uc600\ub2e4. \ubd84\uc11d\uacb0\uacfc \uc2e0\uc81c\ud488 \ucd9c\uc2dc\uc77c\uc790 \uc815\ubcf4, \uc81c\ud488\uc0ac\uc591, \ub9ac\ud2b8\uc717 \uc694\uccad, \ud2b8\uc717 \uac8c\uc2dc\uc790 \ud2b9\uc131 \ub4f1\uc774 \uad6c\uc804\ud6a8\uacfc\uc5d0 \uc720\uc758\ud55c \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uac83\uc73c\ub85c \ud30c\uc545\ub418\uc5c8\ub2e4 . \uc544\uc6b8\ub7ec \uc81c\ud488\uc758 \ud2b9\uc131\uacfc \ud2b8\uc717 \uba54\uc2dc\uc9c0 \uac8c\uc2dc\uc77c\uc5d0 \ub530\ub77c \uad6c\uc804\ud6a8\uacfc\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc774 \ub2ec\ub77c\uc9d0\uc744 \ubc1c\uacac\ud558\uc600\ub2e4. \ubcf8 \uc5f0\uad6c\uc758 \uacb0\uacfc\ub294 \uc2e0\uc81c\ud488 \ud504\ub85c\ubaa8\uc158\uacfc \uad00\ub828\ud55c \uad6c\uc804\ud6a8\uacfc \ubc1c\uc0dd\uc5d0 \uc788\uc5b4 \uc628\ub77c\uc778 \uc18c\uc15c\ub124\ud2b8\uc6cc\ud06c\uc758 \uc601\ud5a5\uc5d0 \uad00\ud55c \uc5f0\uad6c\uae30\ubc18\uc744 \uc81c\uacf5\ud558\uba70 \uae30\uc5c5\uc774 \uc0c8\ub85c\uc6b4 \uc81c\ud488\uc5d0 \ub300\ud55c \uad11\uace0 \ubc0f \ub9c8\ucf00\ud305 \ucee4\ubba4\ub2c8\ucf00\uc774\uc158\uc744 \uc218\ud589 \ud568\uc5d0 \uc788\uc5b4 \uc628\ub77c\uc778 \uc18c\uc15c\ub124\ud2b8\uc6cc\ud06c\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \ud65c\uc6a9\ud558\ub294\ub370 \ud544\uc694\ud55c \uc720\uc6a9\ud55c \uac00\uc774\ub4dc\ub97c \uc81c\uc2dc \ud560 \uac83\uc73c\ub85c \uae30\ub300\ub41c\ub2e4.Keywords : \uc2e0\uc81c\ud488 \ud504\ub85c\ubaa8\uc158, \uc628\ub77c\uc778 \uc18c\uc15c\ub124\ud2b8\uc6cc\ud06c, \ub9ac\ud2b8\uc717, \ud2b8\uc704\ud130, \uad6c\uc804\ud6a8\uacfc",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144362767",
                    "name": "Hyung-Jin Kim"
                },
                {
                    "authorId": "3236121",
                    "name": "Insoo Son"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "43e9631055ddbde48fc8bcb09a3b75154e2b6a28",
            "title": "Information Diffusion in Online Social Media",
            "abstract": "Micro-blogging has been rapidly growing and gaining explosive popularity worldwide. Among the multiple micro-blogging services available in the market, Twitter is the most popular one so far. In particular, the service is characterized with \u201cRetweet\u201d feature, which represents how a user\u2019s or a groups\u2019 ideas are disseminated throughout an online communication environment. In this study, we explore the information diffusion mechanism under online social network environments by investigating the effect of message and user characteristics on multiple dimensions of Retweet phenomenon including volume, speed, and duration. We collected about 11k tweet messages related to the subjects of major telecom service providers in Korea (i.e., SK, KT, and LGU Plus) between September 2011 and December 2011. These original tweet messages have been retweeted at least once and generated more than 59k retweets. For the empirical test, we identified contextual factors derived from message and user characteristics and conducted multiple regression analysis. We found that information features inherent in tweet messages, such as URL, Hashtag and message length, have significant effect on retweet volume and duration. Our results also show that the retweet volume is affected by message content. The findings in the study suggest that some user characteristics, such as user identity, social relation with other users, and the level of tweet participation, have strong relationship with the speed and duration of retweet. From academic perspective, our study broadens theoretical knowledge of information diffusion mechanism over online social media. For practitioners, the study also provides managerial implications regarding how to strategically utilize online social media for marketing communications with customers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3236121",
                    "name": "Insoo Son"
                },
                {
                    "authorId": "2110022407",
                    "name": "Esther Kim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "f174ca122ac7e43f8f4d024d546c7597de129d84",
            "title": "Complex Network Theory Based Web Services Composition Benchmark Toolkit",
            "abstract": "In recent years, while many research proposals have been made toward novel algorithmic solutions of a myriad of web services composition problems, their validation has been less than satisfactory. One of the reasons for this problem is the lack of real benchmark web services data with which researchers can test and verify their proposals. In this chapter, to remedy this challenge, we present a novel benchmark toolkit, WSBen, which is capable of generating synthetic web services data with diverse scenarios and configurations using complex network theory. Web services researchers therefore can evaluate their web services discovery and composition algorithms in a more systematic fashion. The development of WSBen is inspired by our preliminary study on real-world web services crawled from the Web. The proposed WSBen can: (1) generate a collection of synthetic web services files in the WSDL format conforming to diverse complex network characteristics; (2) generate queries and ground truth sets for testing discovery and composition algorithms; (3) prepare auxiliary files to help further statistical analysis; (4) convert WSDL test sets to the formats that conventional AI planners can read; and (5) provide a graphical interface to control all these functions. To illustrate the application of the WSBen, in addition, we present case studies selected from three domains: (1) web services composition; (2) AI planning; and (3) the laws of networks in Physics community. The WSBen toolkit is available at: http://pike.psu.edu/sw/wsben/. This chapter is an invited extension of authors\u2019 previous publication (Oh & Lee, 2009).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "fb9045625b1e849aee4791c534bdb8e932a99af9",
            "title": "Predicting aggregate social activities using continuous-time stochastic process",
            "abstract": "How to accurately model and predict the future status of social networks has become an important problem in recent years. Conventional solutions to such a problem often employ topological structure of the sociogram, i.e., friendship links. However, they often disregard different levels of activeness of social actors and become insufficient to deal with complex dynamics of user behaviors. In this paper, to address this issue, we first refine the notion of social activity to better describe dynamic user behaviors in social networks. We then propose a Parameterized Social Activity Model (PSAM) using continuous-time stochastic process for predicting aggregate social activities. With social activities evolving over time, PSAM itself also evolves and therefore dynamically captures the real-time characteristics of the current active population. Our experiments using two real social networks (Facebook and CiteSeer) reveal that the proposed PSAM model is effective in simulating social activity evolution and predicting aggregate social activities accurately at different time scales.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30698726",
                    "name": "Shu Huang"
                },
                {
                    "authorId": "2108556857",
                    "name": "Min Chen"
                },
                {
                    "authorId": "145607083",
                    "name": "Bo Luo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "fc7d782f9223f45c5d65ad4af5285a81b38409ae",
            "title": "Characterizing Landing Pages in Sponsored Search",
            "abstract": "Using a total of 60,419 ad links collected from three search engines (i.e., Bing, Google, and Yahoo), we characterize the ``mobile-friendliness'' of landing pages in sponsored search. In particular, we analyze the common and different characteristics between landing pages made for desktop vs. mobile device users, measure/validate the quantitative scores for their mobile-friendliness, and classify the results with respect to types of queries and landing pages. Based on our findings, we articulate that: (1) current landing pages (regardless of search engines or platforms) are \\emph{not} mobile-friendly enough, and (2) better data-driven methods (as opposed to current static methods) to help advertisers build mobile-friendly landing pages are needed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145575469",
                    "name": "Haibin Liu"
                },
                {
                    "authorId": "7891272",
                    "name": "Woo-Cheol Kim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "0bbbc3f29af6033f2a400054ca220ab9f5502a76",
            "title": "The Impact of twitter Message Characteristics on Online Viral Diffusion in Micro-blogging services",
            "abstract": "In this paper, we explore retweet mechanism by investigating the effect of message characteristics on information diffusion in terms of volume and speed. To this end, we select four main keywords (e.g., \u2018????????\u2019, \u2018??????\u2019, \u2018??????\u2019, and \u2018iPhone\u2019) that have recently been popular on online social media, embrace various social aspects, and represent distinctive message characteristics. Analyzing the frequency and velocity of retweeting for each keyword, we find that more than half of posted messages on Twitter contain personal opinions for the certain keyword, but we also find that when being retweeted, the group of messages related to the certain keyword present distinctive diffusion patters and speed according to message characteristics. The findings in the study not only broaden our theoretical knowledge of information diffusion mechanism over online social media but also provide managerial implications regarding how to strategically utilize online social media for marketing communication with customers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144942328",
                    "name": "Young-Woo Nam"
                },
                {
                    "authorId": "3236121",
                    "name": "Insoo Son"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "7210b8f2aba1a4fd4048abe86ecb0090510c75a1",
            "title": "Exploring Structural Features in Predicting Social Network Evolution",
            "abstract": "In this paper, we present a novel approach to incorporate the activity features in measuring the influence of member activities on the social network evolution. Conventional methods analyze social networks and make predictions based on all cumulative members and activities. However, since inactive members do not contribute to the network growth, including them in analysis can lead to less accurate results. Based on this observation, we propose to focus on the active population and explore the influence of member activities. We present a model that can incorporate various activity features and predict the evolution of the social activity. At the same time, an algorithm is adopted to select the most influential activity features. The experiments on two different types of social network show that the activity features can predict the evolution of the social activity accurately and our algorithm is effective to select the most influential features. Additionally, we find that the most significant activity features to determine the network evolution vary among different types of social network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30698726",
                    "name": "Shu Huang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "7324b039be4cbcc8a4bc2e0f25e7e1da721b8786",
            "title": "Game-based InfoSec Education Using OpenSim",
            "abstract": "ISBN 1-933510-96-X/$15.00 \uf0d3 2011 CISSE Current information security education approaches tend to focus on theories and concepts. Although these conventional education strategies have their own advantages, students can also benefit from pedagogical strategies that are more interactive and scenario-driven. In particular, the current netgeneration of students are often more likely to prefer learning in a feedback-rich and contextualized environment. Therefore, an environment in which learning occurs in a game-like context can be highly effective in teaching students information security topics, especially in introductory courses. Based on these observations, this paper proposes a novel security educational environment that aims to improve student engagement and learning effectiveness by using the recent developments in 3D, Web-based virtual world technologies in a game-like setting. \uf020",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1806976",
                    "name": "J. Ryoo"
                },
                {
                    "authorId": "1786423",
                    "name": "A. Techatassanasoontorn"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2232100",
                    "name": "Jeremy Lothian"
                }
            ]
        },
        {
            "paperId": "97c466fde87764a4ad78defd76e1604bb0653c03",
            "title": "Understanding the Adoption of Convergent Services: The Case of IPTV",
            "abstract": "Today, many of IT innovations that affect people's daily lives originate from digital convergence. This study examines the factors influencing consumers' adoption of IPTV, a representative convergent service combining communication and media technologies. Applying innovation diffusion theory, we developed an adoption model reflecting the unique characteristics and usage contexts of IPTV. The results indicate that relative advantage, compatibility, trialability, content diversity, monetary value, personal innovativeness, and social influence have significant direct effects on the consumer's adoption of IPTV. In addition, the specific attributes of IPTV-interactivity, content diversity, and monetary value-have significant mediating effects on the consumer's adoption via relative advantage. These results not only provide practical insights into the consumer's acceptance of new convergent services but also help practitioners to plan their marketing strategies more effectively.",
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "3236121",
                    "name": "Insoo Son"
                },
                {
                    "authorId": "11443219",
                    "name": "M. Yoo"
                },
                {
                    "authorId": "2108603687",
                    "name": "Jong-Ho Lee"
                }
            ]
        },
        {
            "paperId": "a786ce6b28b28fe5f299c0bb67f99143397e8bc0",
            "title": "Understanding Music Sharing Behavior on Social Network Services",
            "abstract": "Purpose \u2013 The purpose of this paper is to understand music sharing behaviour on social networking services (SNS). This study suggests and examines a research model which focuses on the influences of user motivations, such as self\u2010expression, ingratiation, altruism, and interactivity, on music sharing behaviour in SNS through social motivation factors.Design/methodology/approach \u2013 Data were collected from 153 Korean SNS (i.e. Cyworld, Naver Blog, Daum Blog, and Tistory) users, who have experience in purchasing music and legally sharing it on SNS. The partial least squares method was used to analyse the measurement and structural models.Findings \u2013 The study shows that interactivity, perceived ease of use, self\u2010expression, social presence, and social identity are significant positive predictors of music sharing intention on SNS.Research limitations/implications \u2013 This research is significant in light of recent interest in user activities in SNS. Better understanding of the music sharing behaviour on SNS can ...",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2110274186",
                    "name": "Yejean Park"
                },
                {
                    "authorId": "2577662",
                    "name": "Junha Kim"
                },
                {
                    "authorId": "1852376",
                    "name": "Jaejeung Kim"
                },
                {
                    "authorId": "2093208583",
                    "name": "Jung-Nam Moon"
                }
            ]
        },
        {
            "paperId": "b7ead982e364994308af48f9992b298ab35e1b6f",
            "title": "Assessing A New IT Service Model, Cloud Computing",
            "abstract": "Recently we have witnessed a new kind of IT advancement. It is a phenomenon where various types of IT capabilities are centralized into data centers to ensure that such resources are available wherever and whenever they are needed. Cloud computing is the latest version of IT service practice that reflects such new trends in IT. Although cloud computing is considered a paradigm shift of IT service design and delivery in IS communities and generates large amounts of interest from business entities, few studies have examined this new IT and business environment. To better understand the organizational application of cloud computing, this study aims both to investigate economic payoffs from cloud computing investment and explore cloud computing adoption within the firm. This study is one of the first of its kinds to assess this new IT service model (i.e., cloud computing) using empirical validation. The proposed study can make contributions to the IS literature by (1) extending the boundary of the IS literature by reflecting the new trend of the IT industry; (2) continuing the debate about business value of IT; and (3) establishing a theoretical framework for cloud computing adoption, which can be applied to further studies on cloud computing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3236121",
                    "name": "Insoo Son"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "da84ed834229b2b5e21c1f095ec7a01e68aa868d",
            "title": "Scenario Analysis of Web Service Composition based on Multi-Criteria Mathematical Goal Programming",
            "abstract": "This paper addresses the web service composition problem considering multi-criteria regarding quality of services (QoS). Three different scenarios of multi-criteria mathematical programming models are explored under the framework of network based analysis in web service composition. This work takes care of the issues pertaining to inputs and outputs matching of web services and Quality-of-Service (QoS) at the same time. The multi-criteria programming models are explored to select the desirable service composition in a variety of categories in accordance with customers' preferences in three different scenarios: (1) Optimal, (2) Compromised optimal, and (3) Acceptable. This set of multi-criteria models have both advantages and disadvantages comparing with each other, and can be used as different solvers in the network based service composition framework. The proposed regular multi-criteria programming (MCP) models are used in Scenario (1): Optimal. The proposed multi-criteria goal programming for optimal composition (MCGPO) and multi-criteria goal programming for non-optimal solution (MCGPN) models are designed for Scenarios (2): Compromised optimal and (3) Acceptable respectively. And they can find a compromised composition based on the trade-off of customer's preference on the QoS goals in case that the optimal composition satisfying both functional and QoS constraints does not exist in the network. [ Service Science , ISSN 2164-3962 (print), ISSN 2164-3970 (online), was published by Services Science Global (SSG) from 2009 to 2011 as issues under ISBN 978-1-4276-2090-3.]",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50407957",
                    "name": "Liying Cui"
                },
                {
                    "authorId": "1749112",
                    "name": "S. Kumara"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "dbd9de712399d35515fbf1ec0e19b22aca0cac06",
            "title": "High-Utility Rule Mining for Cross-Selling",
            "abstract": "In association rule mining, utility has recently been regarded as a practical measure for a rule's usefulness in that it can reflect the actual amount of output achieved by applying each rule. Even the same rule may have different utilities depending on how well the rule fits a specific business purpose. However, most recent studies have tried to apply a uniform standard to assessing rules disregarding this. This paper introduces high-utility rule mining (HURM) as an alternative approach. HURM proposes rule utility as a new measure for rules' usefulness. Rule utility, expressed in the form of a rule utility function (RUF), can be developed from three elements (opportunity, effectiveness, and probability) that are designed by considering a rule's fitness to business purposes. HURM algorithms were developed to meet each specific purpose by replacing RUFs. A cross-selling case was chosen to show how HURM can be applied to a particular business.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2341187",
                    "name": "Sung-Hyuk Park"
                },
                {
                    "authorId": "2395590",
                    "name": "S. Moon"
                }
            ]
        },
        {
            "paperId": "e9f0fdeebea50dc7309abe0d8ed02e12ffa00adc",
            "title": "The Impact of Message Characteristics on Online Viral Diffusion in Online Social Media Services : The Case of Twitter",
            "abstract": "In this paper, we explore the information diffusion mechanism under social network environments by investigating the effect of message characteristics on the volume and speed of retweeting in Twitter, a popular online social media service. To this end, we select eight main keywords (i.e., '\ubb34\uc0c1\uae09\uc2dd', '\ubc18\uac12\ub4f1\ub85d\uae08', '\ub098\uac00\uc218', '\ud3c9\ucc3d', '\uae40\uc5f0\uc544', '\ubc15\ud0dc\ud658', '\uc544\uc774\ud3f0', '\uac24\ub7ed\uc2dc') that have been popular on online social media in recent days. Each keyword represents various social aspects of Korea that recently grab people's attention such as political issues, entertainment, sports celebrities, and the latest digital products, and eventually holds distinctive message characteristics. Analyzing the frequency and velocity of retweeting for each keyword, we find that more than half of the sample messages posted on Twitter contain personal opinions for the certain keyword, but we also find that the tweets which include objective messages with hyperlink are the fastest ones when being retweeted by other followers. In overall, when being retweeted, the group of messages related to the certain keyword present distinctive diffusion patterns and speed according to message characteristics. From academic perspective, the findings in the study broaden our theoretical knowledge of information diffusion mechanism over online social media. For practitioners, the results also provide managerial implications regarding how to strategically utilize online social media for marketing communications with customers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144942328",
                    "name": "Young-Woo Nam"
                },
                {
                    "authorId": "3236121",
                    "name": "Insoo Son"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "eb1958a3f577db3e6c7f2c23c0abf91d12f3e82e",
            "title": "Towards noise-resilient document modeling",
            "abstract": "We introduce a generative probabilistic document model based on latent Dirichlet allocation (LDA), to deal with textual errors in the document collection. Our model is inspired by the fact that most large-scale text data are machine-generated and thus inevitably contain many types of noise. The new model, termed as TE-LDA, is developed from the traditional LDA by adding a switch variable into the term generation process in order to tackle the issue of noisy text data. Through extensive experiments, the efficacy of our proposed model is validated using both real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122901239",
                    "name": "Tao Yang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "03c8c7b7bb09405443588e3bd6df73c6d3300ed3",
            "title": "HARRA: fast iterative hashed record linkage for large-scale data collections",
            "abstract": "We study the performance issue of the \"iterative\" record linkage (RL) problem, where match and merge operations may occur together in iterations until convergence emerges. We first propose the Iterative Locality-Sensitive Hashing (ILSH) that dynamically merges LSH-based has tables for quick and accurate blocking. Then, by exploiting inherent characteristics within/across data sets, we develop a suite of I-LSH-based RL algorithms, named as HARRA (<u>HA</u>shed <u>R</u>eco<u>R</u>d link<u>A</u>ge). The superiority of HARRA in speed over competing RL solutions is thoroughly validated using various real data sets. While maintaining equivalent or comparable accuracy levels, for instance, HARRA runs: (1) 4.5 and 10.5 times faster than StringMap and R-Swoosh in iteratively linking 4,000 x 4,000 short records (i.e., one of the small test cases), and (2) 5.6 and 3.4 times faster than basic LSH and Multi-Probe LSH algorithms in iteratively linking 400,000 x 400,000 long records (i.e., the largest test case).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1755753",
                    "name": "Hung-sik Kim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "3880cd3dbe43f1963d1d63731b2c3e02840e9949",
            "title": "Comparative Study on Subject Classification of Academic Videos Using Noisy Transcripts",
            "abstract": "With the advance of Web technologies, the number of \"academic\" videos available on the Web (e.g., online lectures, web seminars, conference presentations, or tutorial videos) has increased explosively. A fundamental task of managing such videos is to classify them into relevant subjects. For this task, most of current content providers rely on keywords to perform the classification, while active techniques for automatic video classification focus on utilizing multi-modal features. However, in our settings, we argue that both approaches are not sufficient to solve the problem effectively. Keywords based method is very limited in terms of accuracy, while features based one lacks semantics to represent academic subjects. Toward this problem, in this paper, we propose to transform the video subject classification problem into the text categorization problem by exploiting the extracted transcripts of videos. Using both real and synthesized data, (1) we extensively study the validity of the proposed idea, (2) we analyze the performance of different text categorization methods, and (3) we study the impact of various factors of transcripts such as quality and length towards academic video classification problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1818420",
                    "name": "Hau-Wen Chang"
                },
                {
                    "authorId": "1755753",
                    "name": "Hung-sik Kim"
                },
                {
                    "authorId": "2117951350",
                    "name": "Shuyang Li"
                },
                {
                    "authorId": "2108349726",
                    "name": "JeongKyu Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "89bc5fb8b030602c980aae9a3269ea6a5690fa6f",
            "title": "High performance record linkage",
            "abstract": "In current world, the immense size of a data set makes problems in finding similar/identical data. In addition, the dirtiness of data, i.e. typos, missing/tilting information, and additional noises usually occurred by careless editing or entry mistakes, makes further difficulty to identify entity-belongs. Therefore, we focus on the faster detection of data referring the same real-world entity from a large size data set under the error prone environments, while the high accuracy of detection is maintained. In this thesis, we study high-performance linkage algorithms using four different applications. First, we introduce the image linkage algorithm to find near-duplicate images with similar characteristics by bridging two seemingly unrelated fields\u2014Multimedia Information Retrieval and Biology. Under this idea, we study how various image features and gene sequence generation methods affect the accuracy and performance of detecting near-duplicate images. Second, we develop the video linkage algorithm using record linkage methods to detect copied videos from a large multi-media database or sites such as YouTube and Yahoo Videos. The utilization of video characteristics is reflected to the hierarchical structure of the proposed algorithms. In addition, the uses of pipe-lined linkage structures accelerate the speed further. Third, the parallel linkage algorithm, the parallelization of the data linkage frame, is introduced, when slow but optimal sequential linkage frames occur where iterative matching operations apply to clean and merge dirty sets. Any data matching functions can be adapted to the proposed parallel framework because a data linkage function is considered as a black box in the parallel scheme. Finally, we introduce a hashed linkage structure based on the locality sensitive hashing (LSH) algorithm. By remedying the poverty of a basic LSH structure to suit linkage problems, the proposed hashing structure reduces the precessing time tremendously comparing to the conventional LSH structures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1755753",
                    "name": "Hung-sik Kim"
                }
            ]
        },
        {
            "paperId": "a9c1f29e639be0231800cd3e7302e54fd3ec8468",
            "title": "Efficient web service composition: from signature-level to behavioral description-level",
            "abstract": "Web services are software systems designed to support machine-to-machine inter-operation over the Web. Many researches have been carried out for the web service standards, and these efforts have significantly improved flexible and dynamic functionality of Service Oriented Architecture (SOA) in the current web services. However, there still remain a number of research challenges one of which is the web service composition (WSC) problem\u2014when a single web service does not satisfy a given requirement, one wants to automatically combine web services to satisfy the requirement entirely. In this dissertation, we tackle this WSC problem in three levels, i.e., a signature level, a behavior description level and a QoS description level based on the web service descriptions. \nFor a signature-level approach where each web service is described with its signature in WSDL, we first analyze the topological landscape of the web service networks formed by real-world web services based on graph theory. We then propose a SAT-based algorithm based on the analysis. \nSince web service composition based on signatures has a limitation due to a lack of information provided by a WSDL signature, we need additional information, e.g., a semantic annotation of data and/or a behavioral description of a web service function. Focusing on the latter one, we first define a realistic model for the WSC problem on behavioral descriptions, and investigate the computational complexities for the composition of web services on restricted (i.e., with full observation) and general cases (i.e., with partial observation). We then prove that the WSC problem with full observation is EXP-hard and the WSC problem with partial observation is 2-EXP-hard. To solve these high complexities, we propose approximation-based algorithms using abstraction and refinement. \nThe previous two approaches consider only functional requirements specified WSDL or BPEL. However, non-functional ones, such as Quality of Services (QoS) constraints help clients to select a service provider with good quality. In this case, the main aim of the WSC problem is to find a composite web service which satisfies a given complicated task with the optimal accumulated QoS value, which is called QoS-aware WSC problem. We first propose applying anytime algorithm based on beam stack search to the QoS-aware WSC problem. Moreover, to improve the basic anytime algorithm, we propose dynamic beam width with more heuristics, i.e., short backtracking and upper bound propagation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1928204",
                    "name": "Hyunyoung Kil"
                }
            ]
        },
        {
            "paperId": "ae4e8f10bd2b74094c2eb0fdeb78244127bf13da",
            "title": "DSP: Robust Semi-supervised Dimensionality Reduction Using Dual Subspace Projections",
            "abstract": "High-dimensional data usually incur learning deficiencies and computational difficulties. We present a novel semi-supervised dimensionality reduction technique that embeds high-dimensional data in an optimal low-dimensional subspace, which is learned with a few user supplied constraints as well as the structure of input data. We study two types of constraints that indicate whether or not pairs of data points originate from the same class. Data partitions that satisfy both types of constraints may be conflicting. To solve this problem, our method projects data into two different subspaces, one in the kernel space and one in the original input space, each is designed for enforcing one type of constraints. Projections in the two spaces interact and data are embedded in an optimal low-dimensional subspace where constraints are maximally satisfied. Besides constraints, our method also preserves the intrinsic data structure, such that nearby/far away data points in the original space are still near to/far from each other in the embedded space. Compared to existing techniques, our method has the following advantages: 1) It can benefit from constraints even when only a few are available. 2) It is robust and does not suffer from over fitting. 3) It handles nonlinearly separable data, but learns a linear data transformation. Thus the method can be easily generalized to new data points and is efficient in dealing with large data sets. Experiments on real data from multiple domains clearly demonstrate that significant improvements in learning accuracy can be achieved after dimensionality reduction by employing only a few constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40295902",
                    "name": "Su Yan"
                },
                {
                    "authorId": "35119991",
                    "name": "Sofien Bouaziz"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "b6694ed4465d66e1e856c8dc8d0dc56d1b122892",
            "title": "What Is an Opinion About? Exploring Political Standpoints Using Opinion Scoring Model",
            "abstract": "\n \n In this paper, we propose a generative model to automatically discover the hidden associations between topics words and opinion words. By applying those discovered hidden associations, we construct the opinion scoring models to extract statements which best express opinionists\u2019 standpoints on certain topics. For experiments, we apply our model to the political area. First, we visualize the similarities and dissimilarities between Republican and Democratic senators with respect to various topics. Second, we compare the performance of the opinion scoring models with 14 kinds of methods to find the best ones. We find that sentences extracted by our opinion scoring models can effectively express opinionists\u2019 standpoints.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47440067",
                    "name": "Bi Chen"
                },
                {
                    "authorId": "2112583186",
                    "name": "Leilei Zhu"
                },
                {
                    "authorId": "1852261",
                    "name": "Daniel Kifer"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "02f56e81e0113726e20afd9784fda9c058387928",
            "title": "BIM: Image matching using biological gene sequence alignment",
            "abstract": "Matching two images with similar contents is one of the most fundamental tasks in image processing. Due to its importance, in recent years, many novel techniques have been proposed with great successes. Toward this effort, in this paper, we propose a radically different idea by bridging two seemingly unrelated fields - Image Processing and Biology - i.e., we propose to use the popular gene sequence alignment algorithm in Biology, BLAST, in determining the similarity between images. In this proposal, we map image features to a sequence of gene alphabets (e.g., A, C, G, and T in DNA, or 23 letters in protein) to utilize a wealth of advanced algorithms and tools in BLAST. Under the new idea, in particular, we study various image features and gene sequence generation methods that impact the accuracy and performance in matching similar images. Our proposal, termed as BLASTed Image Matching (BIM), is empirically validated using real data sets. Our work can be viewed as the \u201cfirst\u201d step toward bridging Image Processing and Biology fields in the application of the well-studied image matching problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1755753",
                    "name": "Hung-sik Kim"
                },
                {
                    "authorId": "1818420",
                    "name": "Hau-Wen Chang"
                },
                {
                    "authorId": "2145575469",
                    "name": "Haibin Liu"
                },
                {
                    "authorId": "2108349726",
                    "name": "JeongKyu Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "1e679c9123b09f2248614cc2473b50f6e667ca91",
            "title": "The design and evaluation of a virtual world-based learning environment: information security learning using Second Life",
            "abstract": "This research proposes a design framework of a virtual world-based learning environment that integrates unique features of the virtual world technology with an instructional strategy. We then demonstrate the environment's usefulness and assess the effectiveness of our design framework in the context of information security learning using Second Life. Overall, the results strongly suggest that the virtual world-based learning environment enhances information security learning. In addition, our research shows that Learner traits have an important influence on learning outcomes through perceived enjoyment.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1786423",
                    "name": "A. Techatassanasoontorn"
                },
                {
                    "authorId": "1806976",
                    "name": "J. Ryoo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2057915789",
                    "name": "Taylor Davenport"
                }
            ]
        },
        {
            "paperId": "4c1d1cab4377e7eabc3007f849a08cb37733e6bc",
            "title": "Communication process and collaborative work in Web 2.0 environment",
            "abstract": "Because the higher level of media richness improves the performance of collaborative works such as knowledge sharing, efforts to raise media richness are encouraged. The Channel Expansion Theory argues that individuals' perceptions of media richness vary according to each individual's knowledge base built from prior experiences related to the communication situation. This study explored the channel expansion effects in the new CMC environment, Web 2.0. In particular, we considered communication process modes (i.e., conveyance and convergence) as a factor moderating the effects. The research model was verified by an experiment with student subjects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117051682",
                    "name": "Eunjin Kim"
                },
                {
                    "authorId": "33809893",
                    "name": "Joongho Ahn"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "732938af3293b9c91f4966f5b14e259877e9dcf8",
            "title": "On Protecting Private Information in Social Networks: A Proposal",
            "abstract": "As online social networks get more popular, it becomes increasingly critical to preserve user privacy in such networks. In this paper, we propose our preliminary results on defining and tackling information aggregation attacks over online social networks. We first introduce three major threats towards private information in online social networks. We conceptually model private information into multilevel and discretionary models. Then, we articulate information aggregation attacks under discretionary model. Finally, we present our preliminary design of \"privacy monitor,\" a framework that allows users to define their own privacy scheme, and track their actual privacy disclosure to check for any unwanted leakage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145607083",
                    "name": "Bo Luo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "80889b8d358c44d71911ccc886815630aa8394c2",
            "title": "Automatic web service composition with abstraction and refinement",
            "abstract": "The behavioral description based Web Service Composition (WSC) problem aims at the automatic construction of a coordinator web service that controls a set of web services to reach a goal state. However, solving the WSC problem exactly with a realistic model is doubly-exponential in the number of variables in web service descriptions. In this paper, we propose a novel efficient approximation-based algorithm using automatic abstraction and refinement to dramatically reduce the number of variables needed to solve the problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1928204",
                    "name": "Hyunyoung Kil"
                },
                {
                    "authorId": "39631150",
                    "name": "Wonhong Nam"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "830e09d95311ac4a2e5af42aef8ee277b71c8c7e",
            "title": "WSBen: A Web Services Discovery and Composition Benchmark Toolkit1",
            "abstract": "In this article, a novel benchmark toolkit, WSBen, for testing web services discovery and composition algorithms is presented. The WSBen includes: (1) a collection of synthetically generated web services files in WSDL format with diverse data and model characteristics; (2) queries for testing discovery and composition algorithms; (3) auxiliary files to do statistical analysis on the WSDL test sets; (4) converted WSDL test sets that conventional AI planners can read; and (5) a graphical interface to control all these behaviors. Users can fine-tune the generated WSDL test files by varying underlying network models. To illustrate the application of the WSBen, in addition, we present case studies from three domains: (1) web service composition; (2) AI planning; and (3) the laws of networks in Physics community. It is our hope that WSBen will provide useful insights in evaluating the performance of web services discovery and composition algorithms. The WSBen toolkit is available at: http://pike.psu.edu/sw/wsben/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "89321e768b6e305118ca87cbff2f8fea2256f8a1",
            "title": "Security Education Using Second Life",
            "abstract": "Institutions of higher education, government agencies, and private organizations have been making sustained efforts to teach some information security skills more efficiently. In these efforts to improve security education, the dominant pedagogical approach has been to use security exercises in a lab setting.",
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology"
            ],
            "authors": [
                {
                    "authorId": "1806976",
                    "name": "J. Ryoo"
                },
                {
                    "authorId": "1786423",
                    "name": "A. Techatassanasoontorn"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "db501579618de9c6cd1a3288f2046b60b97a1a07",
            "title": "Oracle, where shall I submit my papers?",
            "abstract": "Introduction In April 2005, a group of MIT students pulled prank [1] on the conference \u2013 \u201cWorld MultiConference on Systemics, Cybernetics and Informatics (WMSCI)\u201d [10] \u2013 known for sending unsolicited invitation emails to people in academia. The MIT students used software to generate bogus research papers, complete with context-free grammar, and submitted two of them to the conference. To their surprise, one of the gibberish papers was accepted without any reviews. The event received much attention, being covered in various media (e.g., [6]), and has became an amusing topic for debate among scientists.",
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology"
            ],
            "authors": [
                {
                    "authorId": "3179332",
                    "name": "Ergin Elmacioglu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "dfc405495fc62a45f193f922a6834e506c5c130c",
            "title": "T3: On Mapping Text To Time Series",
            "abstract": "We investigate if the mapping between text and time series data is feasible such that relevant data mining problems in text can find their counterparts in time series (and vice versa). As a preliminary work, we present the T (Text To T ime series) framework that utilizes different combinations of granularity (e.g., character or word level) and n-grams (e.g., unigram or bigram). To assign appropriate numeric values to each character, T adopts different space-filling curves (e.g., linear, Hilbert, Z orders) based on the keyboard layout. When we applied T approach to the \u201crecord linkage\u201d problem, despite the lossy transformation, T achieved comparable accuracy with considerable speed-up.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Tao Yang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "00376eed5a2171704e1f9581d9286dc0ec5e51c6",
            "title": "Effective Web Service Composition in Diverse and Large-Scale Service Networks",
            "abstract": "The main research focus of Web services is to achieve the interoperability between distributed and heterogeneous applications. Therefore, flexible composition of Web services to fulfill the given challenging requirements is one of the most important objectives in this research field. However, until now, service composition has been largely an error-prone and tedious process. Furthermore, as the number of available web services increases, finding the right Web services to satisfy the given goal becomes intractable. In this paper, toward these issues, we propose an AI planning-based framework that enables the automatic composition of Web services, and explore the following issues. First, we formulate the Web-service composition problem in terms of AI planning and network optimization problems to investigate its complexity in detail. Second, we analyze publicly available Web service sets using network analysis techniques. Third, we develop a novel Web-service benchmark tool called WSBen. Fourth, we develop a novel AI planning-based heuristic Web-service composition algorithm named WSPR. Finally, we conduct extensive experiments to verify WSPR against state-of-the-art AI planners. It is our hope that both WSPR and WSBen will provide useful insights for researchers to develop Web-service discovery and composition algorithms, and software.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1749112",
                    "name": "S. Kumara"
                }
            ]
        },
        {
            "paperId": "01bccad7079408193858f922295aaf3ddb539ba1",
            "title": "Video linkage: group based copied video detection",
            "abstract": "Sites to share user-created video clips such as YouTube and Yahoo Video have become greatly popular in recent years. One of the challenges of such sites is, however, to prevent video clips that violate copyrights by illegally copying and editing scenes from other videos. Due to the sheer number of clips uploaded every day, automatic methods to detect (illegally) copied video clips in a large collection are desirable. Toward this problem, in this paper, we present a novel framework, termed as Video Linkage, that is based on the record linkage techniques. Our proposal is based on the observations that: (1) a video clip can be represented as a \"group\" of key frames, (2) two video clips are deemed to be similar if two groups of key frames are similar as a whole - i.e., the similarity of two video clips can be measured by means of graph-based similarity measures such as maximal cardinality bipartite matching, and (3) if a video clip va is copied to vb, then va and vb must be somehow similar, but not all similar video clips are illegally copied ones - i.e., similar videos can be used as a filter for fast detection of copied videos. The validity of our observations and Video Linkage technique is thoroughly evaluated using both real and synthetic data sets - i.e., on average, our proposals achieved 0.94 as precision and 0.93 as recall across 10 genres and 6 editing patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1755753",
                    "name": "Hung-sik Kim"
                },
                {
                    "authorId": "2108349726",
                    "name": "JeongKyu Lee"
                },
                {
                    "authorId": "2145575469",
                    "name": "Haibin Liu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "1029f9f67ec36b74e6a8469a65a381e9727d43c9",
            "title": "Record Linkage as DNA Sequence Alignment Problem",
            "abstract": "Since modern database applications increasingly need to deal with dirty data due to a variety of reasons (e.g., data entry errors, heterogeneous formats, and ambiguous terms), considerable recent efforts have focused on the (record) linkage problem to determine if two entities represented as relational records are approximately the same or not. In this paper, we propose a novel idea of using the popular gene sequence alignment algorithm in Biology \u2013 BLAST. Our proposal, termed as the BLASTed linkage, is based on the observations that: (1) both problems are variants of approximate pattern matching, (2) BLAST provides the statistical guarantee of search results in a scalable manner \u2013 a greatly lacking feature in many linkage solutions, and (3) by transforming the record linkage problem into the gene sequence alignment problem, one can leverage on a wealth of advanced algorithms, implementations, and tools that have been actively developed for BLAST during the last decade. In translating English alphabets to DNA sequences of A, C, G, and T, we study four variations: (1) default \u2013 each alphabet is mapped to nucleotides under 1, 2, and 4-bit coding schemes, (2) weighted \u2013 tokens are elongated or shortened proportional to their importance, making important tokens longer in the resultant DNA sequences, (3) hybrid \u2013 each token\u2019s lexical meaning as well as its importance are considered at the same time during translation, and (4) multi-bit \u2013 tokens are selected for any of 1, 2, and 4-bit coding schemes based on the cumulative distribution functions of their importance. The efficacy of our proposed BLASTed linkage schemes are experimentally validated using both real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080375",
                    "name": "Yoojin Hong"
                },
                {
                    "authorId": "2122901239",
                    "name": "Tao Yang"
                },
                {
                    "authorId": "144323862",
                    "name": "Jaewoo Kang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "20bac1b055990bfa16f7250e03deea4cbdf4c4b1",
            "title": "LeeDeo: Web-Crawled Academic Video Search Engine",
            "abstract": "We present our vision and preliminary design toward Web-crawled academic video search engine, named as LeeDeo, that can search, crawl, archive, index, and browse ldquoacademicrdquo videos from the Web. Our proposal differs from existing general-purpose search engines such as Google or MSN whose focus is on the search of textual HTML documents or metadata of multimedia objects. Similarly, our proposal also differs from existing academic bibliographic search engines such as CiteSeer, arXiv, or Google Scholar whose focus is on the search and analysis of PDF or PS documents of academic papers. As desiderata of such an academic video search engine, we discuss various issues as follows: (1) Crawling: how to crawl, identify, and download academic videos from the Web? (2) Classification: how to determine the so-called academic videos from the rest? (3) Extraction: how to extract metadata and transcripts from the classified videos? (4) Indexing: how to build indexes for search engines? and (5) Interface: how to provide interface for efficient browse and search of academic videos?",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1755753",
                    "name": "Hung-sik Kim"
                },
                {
                    "authorId": "2110115640",
                    "name": "Eun Kyung Kim"
                },
                {
                    "authorId": "2111470542",
                    "name": "Su Yan"
                },
                {
                    "authorId": "2108266742",
                    "name": "Johnny Chen"
                },
                {
                    "authorId": "2108349726",
                    "name": "JeongKyu Lee"
                }
            ]
        },
        {
            "paperId": "443e0d21ce7b506edf3fc0c158387179a520a571",
            "title": "Computational Complexity of Web Service Composition Based on Behavioral Descriptions",
            "abstract": "The Web service composition (WSC) problem on behavioral descriptions deals with the automatic construction of a coordinator web service to control a set of web services to reach the goal states. As such, WSC is one of the fundamental techniques to enable the Service Oriented Architecture on the Web. Despite its importance and implications, however, very few studies exist on the computational complexities of the WSC problem. In this paper, we present two novel theoretical findings on WSC problems: (1) Solving the WSC problem with \"complete\" information is EXP-hard, and (2) Solving the WSC problem with \"incomplete\" information is 2-EXP-hard. These findings imply that more efforts to devise efficient approximate solutions to the WSC problem be needed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1928204",
                    "name": "Hyunyoung Kil"
                },
                {
                    "authorId": "39631150",
                    "name": "Wonhong Nam"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "737e7b7b695755bb86f9c89a215b162ec836cff9",
            "title": "The ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics",
            "abstract": "The ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics. Its primary purpose is to serve as a reference repository of research results, but we believe that it can also be an object of study and a platform for research in its own right. We describe an enriched and standardized reference corpus derived from the ACL Anthology that can be used for research in scholarly document processing. This corpus, which we call the ACL Anthology Reference Corpus (ACL ARC), brings together the recent activities of a number of research groups around the world. Our goal is to make the corpus widely available, and to encourage other researchers to use it as a standard testbed for experiments in both bibliographic and bibliometric research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21308992",
                    "name": "Steven Bird"
                },
                {
                    "authorId": "144301565",
                    "name": "R. Dale"
                },
                {
                    "authorId": "1752326",
                    "name": "B. Dorr"
                },
                {
                    "authorId": "2640566",
                    "name": "Bryan R. Gibson"
                },
                {
                    "authorId": "2002551",
                    "name": "M. T. Joseph"
                },
                {
                    "authorId": "37596605",
                    "name": "Min-Yen Kan"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2715185",
                    "name": "B. Powley"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                },
                {
                    "authorId": "2889313",
                    "name": "Yee Fan Tan"
                }
            ]
        },
        {
            "paperId": "8d84c198406e0e8317e3039dd88a9f30adb992a1",
            "title": "A Web Service Composition Framework Using Integer Programming with Non-functional Objectives and Constraints",
            "abstract": "In this paper, we propose a Web service composition framework that uses Integer Linear Programming with non-functional objectives and constraints, in addition to the syntactic matching of Web services features. We envision that when Web services are fully deployed and commercialized in the near future, the criteria of Web service composition to achieve objectives will vary depending on users' needs or preferences from the number of Web services to non-functional objectives, such as costs, time, and/or reputation. Such non-functional attributes cannot be readily considered in planning-graph, constraint satisfaction, or propositional satisfiability techniques, which are predominantly logic-based. This paper shows how the proposed Integer Linear Programming framework can be utilized to compose Web services with non-functional attributes. This framework enables our composition software agent to identify the best composition result that satisfies both non-functional requirements as well as functional ones, namely, parameter matching. A preliminary implementation of the proposed idea and further research directions are also discussed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40382609",
                    "name": "John Jung-Woon Yoo"
                },
                {
                    "authorId": "1749112",
                    "name": "S. Kumara"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                }
            ]
        },
        {
            "paperId": "d6a75d98506813d4991b04626cf04faaa4dd2d19",
            "title": "Type-Aware Web Service Composition Using Boolean Satisfiability Solver",
            "abstract": "The goal of the Web Service Composition (WSC) problem is to find an optimal \"composition\" of web services to satisfy a given request using their syntactic and/or semantic features, when no single service satisfies it. In this paper, in particular, we study the WSC problem from semantic aspects, exploiting the supertype-subtype relationship among parameters, and propose a novel solutionbased on techniques for the boolean satisfiability problem (SAT). Given a set of web service descriptions and a requirement web service, we reduce the WSC problem into a reachability problem on a state-transition system, and then we find the shortest path for the reachability problem, which is amount to the optimal composition. A preliminary experiment using 7 examples reveals that our proposal can find optimal compositions of web services efficiently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39631150",
                    "name": "Wonhong Nam"
                },
                {
                    "authorId": "1928204",
                    "name": "Hyunyoung Kil"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "0d8ce5113bbd70ec9292bf0648864eb82140de98",
            "title": "Semantic Web-Service Discovery and Composition Using Flexible Parameter Matching",
            "abstract": "When there are a large number of web services available and no single service satisfies the given request, one has to compose multiple web services to fulfill the goal, considering syntactic and semantic aspects. To address the web service composition issue, in this paper, we present a new composition algorithm by extending our previous work, WSRP (web-service planner) so as to determine relations between different parameter types during the process of service composition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                },
                {
                    "authorId": "40382609",
                    "name": "John Jung-Woon Yoo"
                },
                {
                    "authorId": "1928204",
                    "name": "Hyunyoung Kil"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1749112",
                    "name": "S. Kumara"
                }
            ]
        },
        {
            "paperId": "1e39769211fe01f42833db522ab89f522a4b1095",
            "title": "HICCUP: Hierarchical Clustering Based Value Imputation using Heterogeneous Gene Expression Microarray Datasets",
            "abstract": "A novel microarray value imputation method, HICCUP1, is presented. HICCUP improves upon existing value imputation methods in the several ways. (1) By judiciously integrating heterogeneous microarray datasets using hierarchical clustering, HICCUP overcomes the limitation of using only single dataset with limited number of samples; (2) Unlike local or global value imputation methods, by mining association rules, HICCUP selects appropriate subsets of the most relevant samples for better value imputation; and (3) by exploiting relationship among the sample space (e.g., cancer vs. non-cancer samples), HICCUP improves the accuracy of value imputation. Experiments with a real prostate cancer microarray dataset verify that HICCUP outperforms existing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2178224",
                    "name": "Qiankun Zhao"
                },
                {
                    "authorId": "143930195",
                    "name": "P. Mitra"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144323862",
                    "name": "Jaewoo Kang"
                }
            ]
        },
        {
            "paperId": "2a285cd56334e0930710fca87bb60998de9092de",
            "title": "Measuring conference quality by mining program committee characteristics",
            "abstract": "Bibliometrics are important measures for venue quality in digital libraries. Impacts of venues are usually the major consideration for subscription decision-making, and for ranking and recommending high-quality venues and documents. For digital libraries in the Computer Science literature domain, conferences play a major role as an important publication and dissemination outlet. However, with a recent profusion of conferences and rapidly expanding fields, it is increasingly challenging for researchers and librarians to assess the quality of conferences. We propose a set of novel heuristics to automatically discover prestigious (and low-quality) conferences by mining the characteristics of Program Committee members. We examine the proposed cues both in isolation and combination under a classification scheme. Evaluation on a collection of 2,979 conferences and 16,147 PC members shows that our heuristics, when combined, correctly classify about 92% of the conferences, with a low false positive rate of 0.035 and a recall of more than 73% for identifying reputable conferences. Furthermore, we demonstrate empirically that our heuristics can also effectively detect a set of low-quality conferences, with a false positive rate of merely 0.002. We also report our experience of detecting two previously unknown low-quality conferences. Finally, we apply the proposed techniques to the entire quality spectrum by ranking conferences in the collection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3017391",
                    "name": "Ziming Zhuang"
                },
                {
                    "authorId": "3179332",
                    "name": "Ergin Elmacioglu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "145157784",
                    "name": "C. Lee Giles"
                }
            ]
        },
        {
            "paperId": "30c0d6b845dec5000ac3a2d2c6bfac8150dee7b7",
            "title": "Group Linkage",
            "abstract": "Poor quality data is prevalent in databases due to a variety of reasons, including transcription errors, lack of standards for recording database fields, etc. To be able to query and integrate such data, considerable recent work has focused on the record linkage problem, i.e., determine if two entities represented as relational records are approximately the same. Often entities are represented as groups of relational records, rather than individual relational records, e.g., households in a census survey consist of a group of persons. We refer to the problem of determining if two entities represented as groups are approximately the same as group linkage. Intuitively, two groups can be linked to each other if (i) there is high enough similarity between \"matching\" pairs of individual records that constitute the two groups, and (ii) there is a large fraction of such matching record pairs. In this paper, we formalize this intuition and propose a group linkage measure based on bipartite graph matching. Given a data set consisting of a large number of groups, efficiently finding groups with a high group linkage similarity to an input query group requires quickly eliminating the many groups that are unlikely to be desired matches. To enable this task, we present simpler group similarity measures that can be used either during fast pre-processing steps or as approximations to our proposed group linkage measure. These measures can be easily instantiated using SQL, permitting our techniques to be implemented inside the database system itself. We experimentally validate the utility of our measures and techniques using a variety of real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3e821453ffcd47c80fe7201b6615cfd563a85948",
            "title": "Adaptive sorted neighborhood methods for efficient record linkage",
            "abstract": "Traditionally, record linkage algorithms have played an important role in maintaining digital libraries - i.e., identifying matching citations or authors for consolidation in updating or integrating digital libraries. As such, a variety of record linkage algorithms have been developed and deployed successfully. Often, however, existing solutions have a set of parameters whose values are set by human experts off-lineand are fixed during the execution. Since finding the ideal values of such parameters is not straightforward, or no such single ideal value even exists, the applicability of existing solutions to new scenarios or domains is greatly hampered. To remedy this problem, we argue that one can achieve significant improvement by adaptively and dynamically changing such parameters of record linkage algorithms. To validate our hypothesis, we take a classical record linkage algorithm, the sorted neighborhood method (SNM), and demonstrate how we can achieve improved accuracy and performance by adaptively changing its fixed sliding window size. Our claim is analytically and empirically validated using both real and synthetic data sets of digital libraries and other domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111470542",
                    "name": "Su Yan"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "37596605",
                    "name": "Min-Yen Kan"
                },
                {
                    "authorId": "145157784",
                    "name": "C. Lee Giles"
                }
            ]
        },
        {
            "paperId": "4a377cf8b46f9b9138c051bc974ed3c8169d4aa6",
            "title": "Parallel linkage",
            "abstract": "We study the parallelization of the (record) linkage problem - i.e., to identify matching records between two collections of records, A and B. One of main idiosyncrasies of the linkage problem, compared to Database join, is the fact that once two records a in A and b in B are matched and merged to c, c needs to be compared to the rest of records in A and B again since it may incur new matching. This re-feeding stage of the linkage problem requires its solution to be iterative, and complicates the problem significantly. Toward this problem, we first discuss three plausible scenarios of inputs - when both collections are clean, only one is clean, and both are dirty. Then, we show that the intricate interplay between match and merge can exploit the characteristics of each scenario to achieve good parallelization. Our parallel algorithms achieve 6.55-7.49 times faster in speedup compared to sequential ones with 8 processors, and 11.15-18.56% improvement in efficiency compared to P-Swoosh.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1755753",
                    "name": "Hung-sik Kim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "51afa0801308c05baca7669589c9135a4b69c2a6",
            "title": "Report on the First International VLDB Workshop on Clean Databases (CleanDB 2006)",
            "abstract": "In this report, we provide a summary of the First Int'l VLDB Workshop on Clean Databases (CleanDB 2006), which took place at Seoul, Korea, on September 11, 2006, in conjunction with the 32nd Int'l Conference on Very Large Data Bases (VLDB).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": null,
                    "name": "Chen Li"
                }
            ]
        },
        {
            "paperId": "852c1bc0fe5f2a8dada78b1f867375ce7da38bb8",
            "title": "Automaton segmentation: a new approach to preserve privacy in xml information brokering",
            "abstract": "A Distributed Information Brokering System (DIBS) is a peer-to-peer overlay network that comprises diverse data servers and brokering components helping client queries locate the data server(s). Many existing information brokering systems adopt server side access control deployment and honest assumptions on brokers. However, little attention has been drawn on privacy of data and metadata stored and exchanged within DIBS. In this paper, we address privacy-preserving information sharing via on-demand information access. We propose a flexible and scalable system using a broker-coordinator overlay network. Through an innovative automaton segmentation scheme, distributed access control enforcement, and query segment encryption, our system integrates security enforcement and query forwarding while preserving system-wide privacy. We present the automaton segmentation approach, analyze privacy preservation in details, and finally examine the end-to-end performance and scalability through experiments and analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680720",
                    "name": "Fengjun Li"
                },
                {
                    "authorId": "145607083",
                    "name": "Bo Luo"
                },
                {
                    "authorId": "39336958",
                    "name": "Peng Liu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "8351687",
                    "name": "C. Chu"
                }
            ]
        },
        {
            "paperId": "93c9dafe85e6b40b5944566f2b3db7e439ddac30",
            "title": "Incremental adaptation of XPath access control views",
            "abstract": "Materialized XPath access-control views are commonly used for enforcing access control. When access control rules defining a materialized XML access-control view change, the view must be adapted to reflect these changes. The process of updating a materialized view after its definition changes is referred to as view adaptation. While XPath security views have been widely reported in literature, the problem of view adaptation for XPath security views has not been addressed. View adaptation results in view downtime during which users are denied access to security views to prevent unauthorized access. Thus, efficient view adaptation is important for making XPath security views pragmatic. In this work, we show how to adapt an XPath access-control view incrementally by re-using the existing view, which reduces computation and communication costs significantly, and results in less downtime for the end-user. Empirical evaluations confirm that the incremental view adaptation algorithms presented in this paper are efficient and scalable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39390227",
                    "name": "Padmapriya Ayyagari"
                },
                {
                    "authorId": "143930195",
                    "name": "P. Mitra"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "39336958",
                    "name": "Peng Liu"
                },
                {
                    "authorId": "1686360",
                    "name": "Wang-Chien Lee"
                }
            ]
        },
        {
            "paperId": "93cb0983d65f08337e12cbf283d975c9ad492d4e",
            "title": "Toward alternative measures for ranking venues: a case of database research community",
            "abstract": "Ranking of publication venues is often closely related with important issues such as evaluating the contributions of individual scholars/research groups, or subscription decision making. The development of large-scale digital libraries and the availability of various meta data provide the possibility of building new measures more efficiently and accurately. In this work, we propose two novel measures for ranking the impacts of academic venues an easy-to-implement seed-based measure that does not use citation analysis, and a realistic browsing-based measure that takes an article reader's behavior into account. Both measures are computationally efficient yet mimic the results of the widely accepted Impact Factor. In particular, our proposal exploits the fact that: (1)in most disciplines, there are \"top\" venues that most people agree on; and (2) articles that appeared in good venues are more likely to be viewed by readers. Our proposed measures are extensively evaluated on a test case of the Database research community using two real bibliography data sets - ACM and DBLP. Finally, ranks of venues by our proposed measures are compared against the Impact Factor using the Spearman's rank correlation coefficient, and their positive rank order relationship is proved with a statistical significance test.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111470542",
                    "name": "Su Yan"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "97328bbbe015373dcbfbcb783a71e2ffb1835faa",
            "title": "Web based linkage",
            "abstract": "When a variety of names are used for the same real-world entity, the problem of detecting all such variants has been known as the (record) linkage or entity resolution problem. In this paper, toward this problem, we propose a novel approach that uses the Web as the collective knowledge source in addition to contents of entities. Our hypothesis is that if an entity e1 is a duplicate of another entity e2, and if e1 frequently appears together with information I on the Web, then e2 may appear frequently with I on the Web. By using search engines, we analyze the frequency, URLs, or contents of the returned web pages to capture the information I of an entity. Extensive experiments verify that our hypothesis holds in many real settings, and the idea of using the Web as the additional source for the linkage problem is promising. Our proposal shows 51% (on average) and 193% (at best) improvement in precision/recall compared to a baseline approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3179332",
                    "name": "Ergin Elmacioglu"
                },
                {
                    "authorId": "37596605",
                    "name": "Min-Yen Kan"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                }
            ]
        },
        {
            "paperId": "9de831deca9d5b43e459b28f2e20d778cf811314",
            "title": "The Effect of Simplicity and Perceived Control on Perceived Ease of Use",
            "abstract": "As the complexity dramatically increases with technological development, the ease of use of technology has been regarded as a critical factor to adpot. This study extends existing Technology Acceptance Model (TAM) proposed by Moon and Kim (2001) by introducing simplicity and perceived control. Simplicity has recently been suggested as an important concept for a successful user interface design by John Maeda (2004; 2006) of the MIT Media Lab. The concept of simplicity not only refers to a simple layout, as stated in previous studies, but also embraces interface organization, functionality, structure, work flow and framework. From the literature review, we define simplicity as the antecedent to perceived ease of use and classify it into four sub-dimensions: Reduction, Organization, Integration, and Prioritizing. In addition to simplicity, we also bring in control as an important antecedent to perceived ease of use. Control theory explains that control is conceptually related to the direct manipulation of an object and also related to users\u2019 psychological status when they use the object. In this study we demonstrate the role of simplicity and perceived control in the existing TAM. We then apply the extended TAM to the blog service. A survey was conducted for a pilot test (n=59). The results indicate that both perceived control and simplicity influence ease of use in the TAM. This finding asserts the importance of the simplicity and Corresponding Author Lee et al. / The Effect of Simplicity and Perceived Control on Perceived Ease of Use -2perceived control for the web interface design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2093208583",
                    "name": "Jung-Nam Moon"
                },
                {
                    "authorId": "2157167024",
                    "name": "Yongjin Kim"
                }
            ]
        },
        {
            "paperId": "aa0cb3e10bb617920a3c4afbbfecfa3c201c8258",
            "title": "Open Innovation in the High-Tech Firms: Evidence from the Biotechnology Industry",
            "abstract": "Open innovation is based on a different knowledge landscape, with a different logic about the sources and uses of ideas. It implies that firms increasingly rely on external sources of innovation by emphasizing these ideas and resources. Recently, really good ideas are coming from outside the organization, especially in the high-tech industry. Using datasets from the UK biotechnology industry, this paper explores a firm\u2019s willingness to participate in open innovation. The results indicate that trust and IT infrastructure lead firms to participate in open innovation, while commitment and switching costs hinder participation. Although the level of knowledge for firms to share increases with the level of trust between the firms, high switching costs protect the knowledge exposure in open innovation. This research broadens the study of open innovation by applying the concept as a medium of knowledge transfer in the biotech industry.",
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108603687",
                    "name": "Jong-Ho Lee"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "b598219671579c7e819d719da4f5725ba5abf753",
            "title": "Are your citations clean?",
            "abstract": "If they are, only one can refer to a distinct document; if not, many can refer to the same document.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144323862",
                    "name": "Jaewoo Kang"
                },
                {
                    "authorId": "143930195",
                    "name": "P. Mitra"
                },
                {
                    "authorId": "145157784",
                    "name": "C. Lee Giles"
                },
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                }
            ]
        },
        {
            "paperId": "d838cf840e1c77597a31416187f060c1a77ed2f0",
            "title": "Behavior-Inductive Data Modeling for Enterprise Information Systems",
            "abstract": "Traditional database design tools commonly have a restriction that their users are assumed to have expertise in entity-relationship (ER) modeling. What we have found is that once an enterprise-wide business description is prepared, even a novice field worker is able to obtain an ER model with the assistance of a design tool which automatically extracts data objects from the description and which semiautomatically classifies them into entities or attributes. Traditional entity-oriented automated database design tools have another limitation that a bunch of attribute redundancies can be induced by concealing or omitting some meaningful relationships. To avoid the major negative habits of traditional approaches, our design tool treats relationships rather than entities as the focal point in database design. Our results with an option trading application have shown that, with just a few interactions, field workers can use our tool to generate an appropriate ER model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3563437",
                    "name": "Namgyu Kim"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2395590",
                    "name": "S. Moon"
                }
            ]
        },
        {
            "paperId": "dd1bd99e9415cadcda7ccd05a0017ecccb3cda70",
            "title": "PSNUS: Web People Name Disambiguation by Simple Clustering with Rich Features",
            "abstract": "We describe about the system description of the PSNUS team for the SemEval-2007 Web People Search Task. The system is based on the clustering of the web pages by using a variety of features extracted and generated from the data provided. This system achieves F\u03b1=0.5 = 0.75 and F\u03b1=0.2 = 0.78 for the final test data set of the task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3179332",
                    "name": "Ergin Elmacioglu"
                },
                {
                    "authorId": "2889313",
                    "name": "Yee Fan Tan"
                },
                {
                    "authorId": "2111470542",
                    "name": "Su Yan"
                },
                {
                    "authorId": "37596605",
                    "name": "Min-Yen Kan"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "e93a4726d06183122d5a0625ed35a0687b5e6fcf",
            "title": "Scalable Name Disambiguation using Multi-level Graph Partition",
            "abstract": "When non-unique values are used as the identifier of entities, due to their homonym, confusion can occur. In particular, when (part of) \u201cnames\u201d of entities are used as their identifier, the problem is often referred to as the name disambiguation problem, where goal is to sort out the erroneous entities due to name homonyms (e.g., if only last name is used as the identifier, one cannot distinguish \u201cVannevar Bush\u201d from \u201cGeorge Bush\u201d). In this paper, in particular, we study the scalability issue of the name disambiguation problem \u2013 when (1) a small number of entities with large contents or (2) a large number of entities get un-distinguishable due to homonyms, how to resolve it? We first carefully examine two of the state-of-the-art solutions to the name disambiguation problem, and point out their limitations with respect to scalability. Then, we adapt the multi-level graph partition technique to solve the large-scale name disambiguation problem. Our claim is empirically validated via experimentation \u2013 our proposal shows orders of magnitude improvement in terms of performance while maintaining equivalent or reasonable accuracy compared to competing solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "067d8445e870f4da1be6aa3d495dcb4099c80a07",
            "title": "Algorithms for Web Services Discovery and Composition Based on Syntactic and Semantic Service Descriptions",
            "abstract": "As the number of available Web services is rapidly increasing, the effective and efficient service discovery and composition becomes a pressing problem for value-added distributed applications. To address this problem, in this paper, we present efficient service discovery and composition algorithms that exploit both syntactic and semantic service descriptions of Web services",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                },
                {
                    "authorId": "1928204",
                    "name": "Hyunyoung Kil"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1749112",
                    "name": "S. Kumara"
                }
            ]
        },
        {
            "paperId": "123c3eaf723ec9145023afe118cd070f1dc993cf",
            "title": "OpenArXiv = arXiv + RDBMS + web services",
            "abstract": "The OpenArXiv project aims to significantly improve the arXiv digital library in two ways: (1) by managing digital documents with an RDBMS and exploiting state-of-the-art database techniques, we add more sophisticated and flexible services, e.g., contents-based search, advanced query processing and triggers technology; and (2) by utilizing the standard XML-based Web services framework, we build a programmable interface to arXiv so that not only human users but also software agents can freely access the contents of arXiv in many applications",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072759072",
                    "name": "Justin Fisher"
                },
                {
                    "authorId": "1928204",
                    "name": "Hyunyoung Kil"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "4327cc6a50eba222b227109c1f0f99f7eea03fe0",
            "title": "In-broker access control: towards efficient end-to-end performance of information brokerage systems",
            "abstract": "An XML brokerage system is a distributed XML database system that comprises data sources and brokers which, respectively, hold XML documents and document distribution information. However, all existing information brokerage systems view or handle query brokering and access control as two orthogonal issues: query brokering is a system issue that concerns costs and performance, while access control is a security issue that concerns information confidentiality. As a result, access control deployment strategies (in terms of where and when to do access control) and the impact of such strategies on end-to-end system performance are neglected by existing information brokerage systems. In addition, data source side access control deployment is taken-for-granted as the \"right\" thing to do. In this paper, we challenge this traditional, taken-for-granted access control deployment methodology, and argue that query brokering and access control are not two orthogonal issues because access control deployment strategies can have a significant impact on the \"whole\" system's end-to-end performance. We propose the first in-broker access control deployment strategy where access control is \"pushed\" from the boundary into the \"heart\" of the information brokerage system",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680720",
                    "name": "Fengjun Li"
                },
                {
                    "authorId": "2075399930",
                    "name": "Bo Luo"
                },
                {
                    "authorId": "39336958",
                    "name": "Peng Liu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "143930195",
                    "name": "P. Mitra"
                },
                {
                    "authorId": "1686360",
                    "name": "Wang-Chien Lee"
                },
                {
                    "authorId": "8351687",
                    "name": "C. Chu"
                }
            ]
        },
        {
            "paperId": "610f59fc93e8e7c608750ff9a51a927ffbc4d639",
            "title": "Report on the 7th ACM International Workshop on Web Information and Data Management (WIDM 2005)",
            "abstract": "The 7th ACM International Workshop on Web Information and Data Management (WIDM 2005) was held at the Hilton Bremen Hotel in Bremen (Germany), on November 5, 2005, in conjunction with the 14th ACM International Conference on Information and Knowledge Management (CIKM 2005). The main objective of the workshop was to bring together researchers, industrial practitioners, and developers to discuss how Web information can be extracted, stored, analyzed, and processed to provide useful knowledge to the end users for various advanced database and Web applications. WIDM 2005 was sponsored by ACM SIGIR and by GI (Gesellshaft f\u00fcr Informatik) and was in cooperation with ACM SIGMOD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "a039e00398485c3b495829d91b9612acb9a534a5",
            "title": "Report on the 7th ACM International Workshop on Web Information and Data Management: (WIDM 2005)",
            "abstract": "In this report, to our best recollection, we provide a summary of the 7th ACM International Workshop on Web Information and Data Management (WIDM 2005), which took place at the Hilton Bremen Hotel, in Bremen, on November 5, 2005, in conjunction with the 14th ACM Int'l Conf. on Information and Knowledge Management (CIKM).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "b151fedd767cb67970dbde0a07ad1848ef88ebe6",
            "title": "Improving Grouped-Entity Resolution Using Quasi-Cliques",
            "abstract": "The entity resolution (ER) problem, which identifies duplicate entities that refer to the same real world entity, is essential in many applications. In this paper, in particular, we focus on resolving entities that contain a group of related elements in them (e.g., an author entity with a list of citations, a singer entity with song list, or an intermediate result by GROUP BY SQL query). Such entities, named as grouped-entities, frequently occur in many applications. The previous approaches toward grouped-entity resolution often rely on textual similarity, and produce a large number of false positives. As a complementing technique, in this paper, we present our experience of applying a recently proposed graph mining technique, Quasi-Clique, atop conventional ER solutions. Our approach exploits contextual information mined from the group of elements per entity in addition to syntactic similarity. Extensive experiments verify that our proposal improves precision and recall up to 83% when used together with a variety of existing ER solutions, but never worsens them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "3179332",
                    "name": "Ergin Elmacioglu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144323862",
                    "name": "Jaewoo Kang"
                },
                {
                    "authorId": "145525190",
                    "name": "J. Pei"
                }
            ]
        },
        {
            "paperId": "b9efea59d10e0e9961c9a56cbb72aa8ede7a26bf",
            "title": "An effective approach to entity resolution problem using quasi-clique and its application to digital libraries",
            "abstract": "We study how to resolve entities that contain a group of related elements in them (e.g., an author entity with a list of citations or an intermediate result by GROUP BY SQL query). Such entities, named as grouped-entities, frequently occur in many applications. By exploiting contextual information mined from the group of elements per entity in addition to syntactic similarity, we show that our approach, Quasi-Clique, improves precision and recall unto 91% when used together with a variety of existing entity resolution solutions, but never worsens them",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "3179332",
                    "name": "Ergin Elmacioglu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144323862",
                    "name": "Jaewoo Kang"
                },
                {
                    "authorId": "145525190",
                    "name": "J. Pei"
                }
            ]
        },
        {
            "paperId": "c4facaf3edb3a02c449101e376027a10fd18b946",
            "title": "A comparative illustration of AI planning-based web services composition",
            "abstract": "As the number of available web services proliferates, finding right web services to fulfill a given goal becomes an important task. In particular, a problem of combining multiple web services to satisfy a single task, known as web services composition problem, has received much attention recently, and various solutions have been proposed. Among many proposed solutions, however, it is not clear to use which one in what scenarios. In this paper, to this end, we present: (1) a taxonomy and decision guideline of available solution spaces; (2) an overview of syntactic and semantic matching approaches, and (3) a comparative illustration of three representative solutions from the perspective of e-service workflows.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1749112",
                    "name": "S. Kumara"
                }
            ]
        },
        {
            "paperId": "efbb48a3c2e4673f69fd07530a43d494a8240f79",
            "title": "Search engine driven author disambiguation",
            "abstract": "In scholarly digital libraries, author disambiguation is an important task that attributes a scholarly work with specific authors. This is critical when individuals share the same name. We present an approach to this task that analyzes the results of automatically-crafted Web searches. A key observation is that pages from rare Web sites are stronger source of evidence than pages from common Web sites, which we model as inverse host frequency (IHF). Our system is able to achieve an average accuracy of 0.836",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2889313",
                    "name": "Yee Fan Tan"
                },
                {
                    "authorId": "37596605",
                    "name": "Min-Yen Kan"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "0f9bcf904b0481c4bc6ab9ae2ccf98d268fc93d0",
            "title": "Establishing value mappings using statistical models and user feedback",
            "abstract": "In this paper, we present a \"value mapping\" algorithm that does not rely on syntactic similarity or semantic interpretation of the values. The algorithm first constructs a statistical model (e.g., co-occurrence frequency or entropy vector) that captures the unique characteristics of values and their co-occurrence. It then finds the matching values by computing the distances between the models while refining the models using user feedback through iterations. Our experimental results suggest that our approach successfully establishes value mappings even in the presence of opaque data values and thus can be a useful addition to the existing data integration techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144323862",
                    "name": "Jaewoo Kang"
                },
                {
                    "authorId": "2075302335",
                    "name": "Tae Sik Han"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "143930195",
                    "name": "P. Mitra"
                }
            ]
        },
        {
            "paperId": "1414e05323d3b4ff214bed91f30ed153c2d8ef44",
            "title": "MISQ: a UML-based analytical modeling methodology for optimizing web service composition",
            "abstract": "A novel UML-based analytical modeling methodology, named MISQ, is presented for optimizing web service composition in Business Service Networks. MISQ enables functional and temporal analyses at a high-level design stage so that web service composition can be systematically optimized. Furthermore, MISQ provides an automatic generation of web service implementations for improving productivity and reliability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1749112",
                    "name": "S. Kumara"
                }
            ]
        },
        {
            "paperId": "14ec89c517bd528a157babd28af90a6011f02b1c",
            "title": "On six degrees of separation in DBLP-DB and more",
            "abstract": "An extensive bibliometric study on the db community using the collaboration network constructed from DBLP data is presented. Among many, we have found that (1) the average distance of all db scholars in the network has been stabilized to about 6 for the last 15 years, coinciding with the so-called six degrees of separation phenomenon; (2) In sync with Lotka's law on the frequency of publications, the db community also shows that a few number of scholars publish a large number of papers, while the majority of authors publish a small number of papers (i.e., following the power-law with exponent about -2); and (3) with the increasing demand to publish more, scholars collaborate more often than before (i.e., 3.93 collaborators per scholar and with steadily increasing clustering coefficients).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3179332",
                    "name": "Ergin Elmacioglu"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "1a4cef1124d1f8c9b747b86401be797d1eca0432",
            "title": "Report on the 6th ACM international workshop on web information and data management (WIDM 2004) held at CIKM 2004",
            "abstract": "The 6th ACM International Workshop on Web Information and Data Management (WIDM 2004) was held at the Hyatt Arlington Hotel, in Washington DC, on November 12-13, 2004, in conjunction with the 13th ACM International Conference on Information and Knowledge Management (CIKM 2004). The main objective of the workshop was to bring together researchers, industrial practitioners, and developers to discuss how Web information can be extracted, stored, analyzed, and processed to provide useful knowledge to the end users for various advanced database and Web applications. As for previous years, WIDM 2004 was sponsored by ACM SIGIR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1764577",
                    "name": "Alberto H. F. Laender"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "228c6e8e20ce4db7a9fbefb4e0104ba59d7b2acd",
            "title": "Deep Set Operators for XQuery",
            "abstract": "There are three set operators defined in XQuery, namely union, intersect and except. They take node sequences as operands, in which each node is identified by its node-ID and treated as an atomic entity. However, according to XML semantics, each node is \u201ca set of set(s)\u201d, which have descendants in a tree-structured hierarchy. Unfortunately, the regular set operators as described above ignored this structural feature of XML data. On the other hand, some XML applications can be benefited from set operators with different semantics considering ancestor-descendant relationships between nodes. In this extended semantics, the comparison during query processing are conducted not only on nodes of both operands, but also on their descendants, in a \u201cdeep\u201d manner. In this paper, we identify the needs of such \u201cdeep\u201d set operators and propose the deep-union, deep-intersect and deep-except operators. We further explore their properties as well as relationships to regular set operators, and present a preliminary experience on implementing them as user-defined functions of XQuery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2075399930",
                    "name": "Bo Luo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1686360",
                    "name": "Wang-Chien Lee"
                },
                {
                    "authorId": "39336958",
                    "name": "Peng Liu"
                }
            ]
        },
        {
            "paperId": "41b6eb2469c0be7640358c40998cfe15c65e9774",
            "title": "Pollock: automatic generation of virtual web services from web sites",
            "abstract": "As the usage of Web Services proliferates dramatically, new tools to help quickly generate web services are needed. In this paper, we propose a methodology that helps to automatically generate Web Services from the FORM-based query interfaces of a web site. Since the majority of web data are rather \"hidden\" behind such a FORM interface, we believe turning such a human-oriented query interface into machine-oriented web services is an important problem. Toward this goal, we adopt the Wrapper technology successfully developed and deployed in Database community, and demonstrate how to generate Web Services components (e.g., WSDL, UDDI, SOAP) automatically. We present the overall architecture of our developed prototype and a few showcases based on real web sites.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50028642",
                    "name": "Yi-Hsuan Lu"
                },
                {
                    "authorId": "2080375",
                    "name": "Yoojin Hong"
                },
                {
                    "authorId": "2266165",
                    "name": "J. Varia"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "42a449135630d2a5521015a771e4de91bbe3cf54",
            "title": "Taxonomy of XML schema languages using formal language theory",
            "abstract": "On the basis of regular tree grammars, we present a formal framework for XML schema languages. This framework helps to describe, compare, and implement such schema languages in a rigorous manner. Our main results are as follows: (1) a simple framework to study three classes of tree languages (local, single-type, and regular); (2) classification and comparison of schema languages (DTD, W3C XML Schema, and RELAX NG) based on these classes; (3) efficient document validation algorithms for these classes; and (4) other grammatical concepts and advanced validation algorithms relevant to an XML model (e.g., binarization, derivative-based validation).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153835257",
                    "name": "M. Murata"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144520541",
                    "name": "Murali Mani"
                }
            ]
        },
        {
            "paperId": "4ed72e4f77982c05fed0b2834273dc559c48b1b4",
            "title": "Blocking-aware private record linkage",
            "abstract": "In this paper, the problem of quickly matching records (i.e., record linkage problem) from two autonomous sources without revealing privacy to the other parties is considered. In particular, our focus is to devise secure blocking scheme to improve the performance of record linkage significantly while being secure. Although there have been works on private record linkage, none has considered adopting the blocking framework. Therefore, our proposed blocking-aware private record linkage can perform large-scale record linkage without revealing privacy. Preliminary experimental results showing the potential of the proposal are reported.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1411006756",
                    "name": "A. Al-Lawati"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144061974",
                    "name": "P. Mcdaniel"
                }
            ]
        },
        {
            "paperId": "4f54082655b5be9347cb7eb4b603a93c034734e3",
            "title": "Proceedings of the 7th annual ACM international workshop on Web information and data management",
            "abstract": "The 2005 International Workshop on Web Information and Data Management (WIDM 2005) is the seventh in a series of workshops on Web Information and Data Management held in conjunction with the International Conference on Information and Knowledge Management (CIKM). The objective of the workshop is to bring together researchers, industrial practitioners, and developers to study how Web information can be extracted, stored, analyzed, and processed to provide useful knowledge to the end users for various advanced database applications. WIDM 2005 has received the sponsorship from ACM SIGIR and the cooperation of ACM SIGMOD.The call for papers resulted in the submission of 44 papers from 15 countries around the world. Starting from this year, a one-day workshop schedule lets accommodate regular papers (up to 8 pages long) along with a few short papers (up to 6 pages long). All papers were thoroughly reviewed by the program committee and external reviewers. The program committee accepted 12 papers (8 full and 4 short papers) for this year novel one-day program, resulting in competitive 27% acceptance rate. The authors of these papers are from 7 countries. The 12 accepted papers were divided into 3 sessions: \"Web Ranking and Retrieval,\" \"XML Data Management and Web Discovery,\" and \"Web Clustering, Filtering and Applications\". In addition, the WIDM 2005 program also includes an invited talk on \"A Web of Data: New Architectures for New Technology?\" by Prof. Donald Kossmann, from ETH Zurich (Switzerland).The workshop would not be possible without the support from the NIKE (Nittany Information, Knowledge and wEb) Research Group of The Pennsylvania State University. The group provided both the manpower and computing resources to host the workshop Web site and to run the ConfMan paper submission and review system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "57f995fae9680adcc3e11fcfa11a06caa018a4e5",
            "title": "Comparative study of name disambiguation problem using a scalable blocking-based framework",
            "abstract": "In this paper, we consider the problem of ambiguous author names in bibliographic citations, and comparatively study alternative approaches to identify and correct such name variants (e.g., \"Vannevar Bush\" and \"V. Vush\"). Our study is based on a scalable two-step framework, where step 1 is to substantially reduce the number of candidates via blocking, and step 2 is to measure the distance of two names via coauthor information. Combining four blocking methods and seven distance measures on four data sets, we present extensive experimental results, and identify combinations that are scalable and effective to disambiguate author names in citations",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144323862",
                    "name": "Jaewoo Kang"
                },
                {
                    "authorId": "143930195",
                    "name": "P. Mitra"
                }
            ]
        },
        {
            "paperId": "c6032d26ba57b8dc5f6901de9737f7ccd2d639aa",
            "title": "BF*: Web services discovery and composition as graph search problem",
            "abstract": "When there are a large number of Web services available (e.g., in the range of 1,000-10,000), it is non-trivial to quickly find Web services satisfying the given request. Furthermore, when no single Web service satisfies the given request fully, one needs to \"compose\" multiple Web services to fulfill the goal. Since the search space for such a composition problem is in general exponentially increasing, it is important to have wise decision on underlying data structures and search algorithms. Toward this problem, in this paper, we present a novel solution, named as BF* (BF-Star), that adopts the competitive A* as a search algorithm while utilizing the Bloom Filter as a succinct data structure.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                },
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "2052551983",
                    "name": "Eric J. Larson"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "c701c349ac0049c7be1b811b8b3aa7b299c26711",
            "title": "MISQ: A framework to analyze and optimize web service composition in business service networks",
            "abstract": "A novel UML-based analytical modeling methodology, named MISQ, is presented for optimizing web service composition in Business Service Networks. MISQ enables functional and temporal analyses at a high-level design stage so that web service composition can be systematically optimized. Furthermore, MISQ provides an automatic generation of web service implementations for improving productivity and reliability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2996903",
                    "name": "Seog-Chan Oh"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1749112",
                    "name": "S. Kumara"
                }
            ]
        },
        {
            "paperId": "d19940d3d900dad3ea2cabd7a95842c88d783138",
            "title": "Adaptive framework for multivariate stream data processing in data-centric sensor applications",
            "abstract": "We introduce an adaptive framework for multivariate sensor stream data reduction. The proposed method takes as input a sliding window of multivariate stream data, classifies the data in each window, and chooses reduction strategies that are most appropriate for the window. In the classification step, it discretizes the stream data into a string of symbols that characterize the signal changes and then applies classification algorithms to classify the transformed sensor stream data. In the second step, depending on the classification labels assigned to each window, it applies most appropriate data reduction techniques and reduction ratios to the window. For classification, we considered supervised methods including Na\u00efve Bayes Model and SVM, and unsupervised methods including Jaccard, TFIDF, Jaro and JaroWinkler. For data reduction, we compared Wavelet, Sampling, SVD and Hierarchical clustering. In our experiments, SVM and TFIDF outperformed the other classification methods and SVD and Sampling showed the best result in data reduction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3183650",
                    "name": "Sungbo Seo"
                },
                {
                    "authorId": "144323862",
                    "name": "Jaewoo Kang"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1694153",
                    "name": "K. Ryu"
                }
            ]
        },
        {
            "paperId": "d6689410e75dff2d37473822cb8775b3fc5f3d99",
            "title": "Effective and scalable solutions for mixed and split citation problems in digital libraries",
            "abstract": "In this paper, we consider two important problems that commonly occur in bibliographic digital libraries, which seriously degrade their data qualities: Mixed Citation (MC) problem (i.e., citations of different scholars with their names being homonyms are mixed together) and Split Citation (SC) problem (i.e., citations of the same author appear under different name variants). In particular, we investigate an effective yet scalable solution since citations in such digital libraries tend to be large-scale. After formally defining the problems and accompanying challenges, we present an effective solution that is based on the state-of-the-art sampling-based approximate join algorithm. Our claim is verified through preliminary experimental results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "144323862",
                    "name": "Jaewoo Kang"
                },
                {
                    "authorId": "2115285735",
                    "name": "Sanghyun Park"
                }
            ]
        },
        {
            "paperId": "0166c44b7d0e3e2a5bdb4298aac8fb7c16b79428",
            "title": "QFilter: fine-grained run-time XML access control via NFA-based query rewriting",
            "abstract": "At present, most of the state-of-the-art solutions for XML access controls are either (1) document-level access control techniques that are too limited to support fine-grained security enforcement; (2) view-based approaches that are often expensive to create and maintain; or (3) impractical proposals that require substantial security-related support from underlying XML databases. In this paper, we take a different approach that assumes no security support from underlying XML databases and examine three alternative fine-grained XML access control solutions, namely <i>primitive, pre-processing</i> and <i>post-processing</i> approaches. In particular, we advocate a pre-processing method called <i>QFilter</i> that uses Non-deterministic Finite Automata (NFA) to rewrite user's query such that any parts violating access control rules are pruned. We show the construction and execution of a QFilter and demonstrate its superiority to other competing methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2075399930",
                    "name": "Bo Luo"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1686360",
                    "name": "Wang-Chien Lee"
                },
                {
                    "authorId": "39336958",
                    "name": "Peng Liu"
                }
            ]
        },
        {
            "paperId": "0a2347cc638df6e6bb3208965f3e5c59f51df5a2",
            "title": "Proceedings of the 6th annual ACM international workshop on Web information and data management",
            "abstract": "The 2004 International Workshop on Web Information and Data Management (WIDM 2004) is the sixth in a series of workshops on Web Information and Data Management held in conjunction with the International Conference on Information and Knowledge Management (CIKM). The objective of the workshop is to bring together researchers, industrial practitioners, and developers to study how Web information can be extracted, stored, analyzed, and processed to provide useful knowledge to the end users for various advanced database applications. WIDM 2004 has received the sponsorship from ACM SIGIR. \n \nThe call for papers resulted in the submission of 61 papers from 20 countries around the world. All papers were thoroughly reviewed by the program committee and external reviewers. The program committee accepted 20 papers for the two-days' program. The authors of these papers are from 10 countries. The 20 accepted papers were divided into 7 sessions: \"Web Crawling and Exploration\", \"XML Processing\", \"XML and Semistructured Data Querying\", \"Web Personalization\", \"Web Searching\", \"Web Mining and Clustering\", and \"User Interfaces and Services\". In addition, the WIDM 2004 program also includes an invited talk on \"Next Generation CiteSeer\" by Dr. C. Lee Giles from the Pennsylvania State University.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1764577",
                    "name": "Alberto H. F. Laender"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "68b67c910873bcfb24d8b11a43afbb2c6cfed617",
            "title": "Proceedings of the 5th ACM international workshop on Web information and data management",
            "abstract": "The 2003 International Workshop on Web Information and Data Management (WIDM 2003) is the fifth in a series of workshops on Web Information and Data Management held in conjunction with the International Conference on Information and Knowledge Management (CIKM). The objective of the workshop is to bring together researchers, industrial practitioners, and developers to study how web information can be extracted, stored, analyzed, and processed to provide useful knowledge to the end users for various advanced database applications. WIDM 2003 has received sponsorship from both ACM SIGIR and ACM SIGMIS.The call for papers resulted in the submission of 54 papers from 18 countries around the world. All papers were thoroughly reviewed by the program committee and external reviewers. The program committee accepted 15 full papers and 10 short papers for the two-days' program. The authors of these papers are from 12 countries. The 25 accepted papers were divided into 7 sessions: \"Web Data Extraction and Structure Mining\", \"XML Data Modeling and Storage\", \"Tools for Integrating and Querying Web Information\", \"Web Clustering and Usage Mining\", \"XML and Information Integration\", \"Intelligent Web Information Access\", and \"Query and View Processing.\" In addition, the WIDM 2003 program also includes an invited talk on \"Business-Aware Management: the Next Frontier for Web Services\" by Dr Fabio Casati from Palo Alto HP Labs.The workshop would not be a success without the support from the Centre for Advanced Information Systems of the Nanyang Technological University, Singapore. The center provided both the manpower and computing resources to host the workshop web site. Zehua Liu, a graduate student at the Nanyang Technological University, did a wonderful job running the ConfMan paper submission and review system. We would like express our appreciation to his assistance.We are also indebted to the CIKM 2003's organizing committee for its support. We are particularly thankful to Eun-Kyo Park for managing the proceedings coordination as well as financial and registration matters, and to Il-Yeol Song for coordinating WIDM 2003 with the other workshops.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1764577",
                    "name": "Alberto H. F. Laender"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "be26b9e547d59adb269a4ed84e90b99e4dff8819",
            "title": "Schema Conversion Methods between XML and Relational Models",
            "abstract": "In this chapter, three semantics-based schema conversion methods are presented: 1) CPI converts an XML schema to a relational schema while preserving semantic constraints of the original XML schema, 2) NeT derives a nested structured XML schema from a flat relational schema by repeatedly applying the nest operator so that the resulting XML schema becomes hierarchical, and 3) CoT takes a relational schema as input, where multiple tables are interconnected through inclusion dependencies and generates an equivalent XML schema as output.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144520541",
                    "name": "Murali Mani"
                },
                {
                    "authorId": "1724907",
                    "name": "W. Chu"
                }
            ]
        },
        {
            "paperId": "17011788ef68d9fb51d0155ab870ab4640a587f4",
            "title": "NeT & CoT: translating relational schemas to XML schemas using semantic constraints",
            "abstract": "Two algorithms, called NeT and CoT, to translate relational schemas to XML schemas using various semantic constraints are presented. The XML schema representation we use is a language-independent formalism named XSchema, that is both precise and concise. A given XSchema can be mapped to a schema in any of the existing XML schema language proposals. Our proposed algorithms have the following characteristics: (1) NeT derives a nested structure from a flat relational model by repeatedly applying the nest operator on each table so that the resulting XML schema becomes hierarchical, and (2) CoT considers not only the structure of relational schemas, but also semantic constraints such as inclusion dependencies during the translation. It takes as input a relational schema where multiple tables are interconnected through inclusion dependencies and converts it into a good XSchema. To validate our proposals, we present experimental results using both real schemas from the UCI repository and synthetic schemas from TPC-H.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144520541",
                    "name": "Murali Mani"
                },
                {
                    "authorId": "1934258",
                    "name": "Frank Chiu"
                },
                {
                    "authorId": "1724907",
                    "name": "W. Chu"
                }
            ]
        },
        {
            "paperId": "73f4a6d3414ff98c8ab30a3cc1ec8ee057ef88db",
            "title": "NeT and CoT: inferring XML schemas from relational world",
            "abstract": "Two conversion algorithms, called NeT and COT, to translate relational schemas to XML schemas using various semantic constraints are presented. We first present a language-independent formalism named XSchema so that our algorithms are able to generate output schema in various XML schema language proposals. The benefits of such a formalism are that it is both precise and concise. Based on the XSchema formalism, our proposed algorithms have the following characteristics: (1) NeT derives a nested structure from a flat relational model by repeatedly applying the nest operator so that the resulting XML schema becomes hierarchical, and (2) COT considers not only the structure of relational schemas, but also inclusion dependencies during the translation so that relational schemas where multiple tables are interconnected through inclusion dependencies can also be handled.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144520541",
                    "name": "Murali Mani"
                },
                {
                    "authorId": "1934258",
                    "name": "Frank Chiu"
                },
                {
                    "authorId": "1724907",
                    "name": "W. Chu"
                }
            ]
        },
        {
            "paperId": "8ba8d971e5e825f2f43729919bb2e5d652c2489c",
            "title": "Effective Schema Conversions between XML and Relational Models",
            "abstract": "As Extensible Markup Language (XML) is emerging as the data format of the Internet era, there is an increasing need to efficiently store and query XML data. At the same time, as requirements change, we expect a substantial amount of conventional relational data to be converted or published as XML data. One path to accommodate these changes is to transform XML data into relational format (and vice versa) to use the mature relational database technology. In this paper, we present three semantics-based schema transformation algorithms towards this goal: 1) CPI converts an XML schema to a relational schema while preserving semantic constraints of the original XML schema, 2) NeT derives a nested structured XML schema from a flat relational schema by repeatedly applying the nest operator so that the resulting XML schema becomes hierarchical, and 3) CoT takes a relational schema as input, where multiple tables are interconnected through inclusion dependencies and generates an equivalent XML schema as output.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "144520541",
                    "name": "Murali Mani"
                },
                {
                    "authorId": "1724907",
                    "name": "W. Chu"
                }
            ]
        },
        {
            "paperId": "a7cad0cafa6d66f9c5abf17963c3cf1e0d49cc39",
            "title": "Query relaxation for xml model",
            "abstract": "of the Dissertation Query Relaxation for XML Model",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "0c2e252df11d03365dd83c2f6b1c0b3c7fb1077d",
            "title": "Extracting semantic metadata and its visualization",
            "abstract": "Introduction Reverse engineering examines the problem of understanding an existing system and recovering essential design specifications [5]. Chiang et al. define database reverse engineering (DBRE) as a process that obtains domain semantics about an existing database, then converts the schema from relational to conceptual, and finally represents the results as a conceptual schema [4]. The objectives of the DBRE process are to improve the understanding of data semantics [12], to mechanically reuse past development outcomes, to reduce maintenance cost and improve software flexibility [13], and to integrate several databases [3].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2639676",
                    "name": "Yousub Hwang"
                }
            ]
        },
        {
            "paperId": "3fa7b2fb0429f683cd92f75ef4a7cc482a75fd77",
            "title": "AN ADAPTIVE AGENT-BASED FRAMEWORK FOR KNOWLEDGE MANAGEMENT AND SHARING",
            "abstract": "Abstract This paper presents an adaptive agent-based framework for knowledge sharing. The framework supports semantic-based knowledge retrieval and filtering through the usage of ontologies, case-based reasoning, and genetic algorithms. First, the concept of ontologies is applied to build users\u2019 profiles and resolve semantic conflicts among multiple knowledge sources \u2013 User Profile Ontology (UPROL), Semantic Conflict Resolution Ontology (SCROL), and Domain ontology. UPROL is used by the profile agent to map users\u2019 interests into a common terminology. SCROL allows multiple views and interpretations of a given terminology by different users and applications. Domain ontology is also used to organize information sources and direct search processes. We also utilize collaborative filtering through case-based reasoning techniques for knowledge filtering and recommendation. Finally, by adopting the survival-of-the-fittest rule of genetic algorithms, we make the system continuously adapt to changing users\u2019 profiles. In conclusion, our adaptive agent-based approach allows knowledge seekers to access tacit yet interoperable knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50878985",
                    "name": "Jinsoo Park"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "43263ddab88f2fa258c8c48e6c66c7f13a5ba3ef",
            "title": "On the Explanation of Factors Affecting E-Commerce Adoption",
            "abstract": "The Internet has grown at a remarkable pace since the emergence of the World Wide Web in the early 1990s. While electronic commerce (e-Commerce) has become an important issue with the growth of the Internet, there has been insufficient empirical research concerning its adoption by Internet users. In this paper, we propose the e-Commerce Adoption Model (e-CAM), which attempts to examine important factors that predict a consumer\u2019s online purchasing behavior. e-CAM integrates the technology acceptance model with the theories of perceived risk to explain the adoption of e-Commerce. Specifically, we examine the impact of the following factors on the consumer\u2019s purchasing behavior: perceived ease of use, perceived usefulness, perceived risk with products/services, and perceived risk in the context of online transaction. We test the e-CAM model using the structural equation modeling technique. Most of the causal relationships between the constructs postulated by our model are well supported, accounting for 33.4% of the total variance in e-Commerce adoption. In sum, our study finds that all of the antecedent constructs directly and/or indirectly affect the consumer\u2019s adoption of e-Commerce. Therefore, the findings suggest that firms providing products/services through eCommerce should consider these contextual factors in order to facilitate consumers\u2019 adoption behavior.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "50878985",
                    "name": "Jinsoo Park"
                },
                {
                    "authorId": "33809893",
                    "name": "Joongho Ahn"
                }
            ]
        },
        {
            "paperId": "0c4cffe3f4190cc754d867713579bb1ea9a28caf",
            "title": "Comparative analysis of six XML schema languages",
            "abstract": "As XML [5] is emerging as the data format of the internet era, there is an substantial increase of the amount of data in XML format. To better describe such XML data structures and constraints, several XML schema languages have been proposed. In this paper, we present a comparative analysis of six noteworthy XML schema languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1724907",
                    "name": "W. Chu"
                }
            ]
        },
        {
            "paperId": "12de2be984f537e198ebfcd94a678b460f79970b",
            "title": "Constraints-Preserving Transformation from XML Document Type Denition to",
            "abstract": "As Extensible Markup Language (XML) (5) is emerging as the data format of the internet era, there are increasing needs to e- ciently store and query XML data. One way towards this goal is using relational database by transforming XML data into relational format. In this paper, we argue that existing transformation algorithms are not complete in the sense that they focus only on structural aspects and ig- noring semantic aspects. We present the semantic knowledge that needs to be captured during the transformation to ensure a correct relational schema. Further, we show a simple algorithm that can 1) derive such semantic knowledge from the given XML Document Type Denition (DTD) and 2) preserve the knowledge by representing them in terms of semantic constraints in relational database terms. By combining the existing transformation algorithms and our constraints-preserving algo- rithm, one can transform XML DTD to relational schema where correct semantics and behaviors are guaranteed by the preserved constraints. Experimental results are also presented.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "71797794",
                    "name": "Relational Schema"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1724907",
                    "name": "W. Chu"
                }
            ]
        },
        {
            "paperId": "026ee44893eb9e77e5f652c83c280e7bd815117b",
            "title": "Semantic caching via query matching for web sources",
            "abstract": "A semantic caching scheme suitable for wrappers wrapping web sources is presented. Since the web sources have typically weaker querying capabilities than conventional databases, existing semantic caching schemes cannot be applied directly. A seamlessly integrated query translation and capability mapping between the wrappers and web sources in semantic caching is described. In addition, an analysis on the match types between the user's input query and cached queries is presented. Semantic knowledge acquired from the data can be used to avoid unnecessary access to the web sources by transforming the cache miss to the cache hit. A polynomial time algorithm based on the proposed query matching technique is presented to find the best matched query in the cache. Experimental results reveal the effectiveness of the proposed semantic caching scheme.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1724907",
                    "name": "W. Chu"
                }
            ]
        },
        {
            "paperId": "2bdb499d3d05f69421b3eacf176659bc03251e76",
            "title": "A Semantic Caching Scheme for Wrappers in Web Databases",
            "abstract": "We present a new semantic caching scheme suitable for wrappers in web databases. Since the web sources in web databases have typically weaker querying capabilities than conventional databases, it is not trivial to apply existing semantic caching schemes directly. We provide a seamlessly integrated query translation and capability mapping between the wrappers and web sources in the semantic caching to cope with such di culties and describe several related issues. In addition, an analysis on the match types between the user's input query and queries stored in the cache is presented. We show how to use semantic knowledge acquired from the data to avoid unnecessary access to web sources by transforming the cache miss to the cache hit. Further, a polynomial time algorithm based on the extended and knowledge-based matching is proposed to nd the best matched query in the cache. Finally, experimental results are presented to illustrate the e ectiveness of our proposed semantic caching scheme.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1724907",
                    "name": "W. Chu"
                }
            ]
        },
        {
            "paperId": "e92a33d29466f5d66d282187ee6f116d728e675f",
            "title": "Conjunctive Point Predicate-based Semantic Caching for Wrappers in Web Databases",
            "abstract": "A semantic caching scheme suitable for web database environments is proposed. In our scheme, tasks for query translation/capability mapping (named as query naturalization) between wrappers and web sources and tasks for semantic caching are seamlessly integrated, resulting in easier query optimization. A semantic cache consists of three components: 1) semantic view , a description of the contents in the cache using sub-expressions of the previous queries, 2) semantic index , an index for the tuple IDs that satisfy the semantic view, and 3) physical storage, a storage containing the tuples (or objects) that are shared by all semantic views in the cache. Types of matching between the native query and cache query are discussed. Algorithms for nding the optimal match of the input query in semantic cache and for cache replacement are presented. The proposed techniques are being implemented in a cooperative web database (CoWeb) prototype at UCLA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1724907",
                    "name": "W. Chu"
                }
            ]
        }
    ]
}