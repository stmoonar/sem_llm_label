{
    "authorId": "2188792890",
    "papers": [
        {
            "paperId": "13f81979372850d9ab9d386c35bb00b4cc0e35f1",
            "title": "Position: Graph Foundation Models Are Already Here",
            "abstract": "Graph Foundation Models (GFMs) are emerging as a significant research topic in the graph domain, aiming to develop graph models trained on extensive and diverse data to enhance their applicability across various tasks and domains. Developing GFMs presents unique challenges over traditional Graph Neural Networks (GNNs), which are typically trained from scratch for specific tasks on particular datasets. The primary challenge in constructing GFMs lies in effectively leveraging vast and diverse graph data to achieve positive transfer. Drawing inspiration from existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, expressiveness, and stability. Such a vocabulary perspective can potentially advance the future GFM design in line with the neural scaling laws. All relevant resources with GFM design can be found here.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2125202063",
                    "name": "Haitao Mao"
                },
                {
                    "authorId": "2257089588",
                    "name": "Zhikai Chen"
                },
                {
                    "authorId": "2188792890",
                    "name": "Wenzhuo Tang"
                },
                {
                    "authorId": "2282988161",
                    "name": "Jianan Zhao"
                },
                {
                    "authorId": "2254813628",
                    "name": "Yao Ma"
                },
                {
                    "authorId": "2256340293",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "2253409421",
                    "name": "Neil Shah"
                },
                {
                    "authorId": "2066369448",
                    "name": "Mikhail Galkin"
                },
                {
                    "authorId": "2283301882",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "1e8fcf495dbc386591fcbab75df75ac41a503859",
            "title": "Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation",
            "abstract": "Spatially resolved transcriptomics brings exciting breakthroughs to single-cell analysis by providing physical locations along with gene expression. However, as a cost of the extremely high spatial resolution, the cellular level spatial transcriptomic data suffer significantly from missing values. While a standard solution is to perform imputation on the missing values, most existing methods either overlook spatial information or only incorporate localized spatial context without the ability to capture long-range spatial information. Using multi-head self-attention mechanisms and positional encoding, transformer models can readily grasp the relationship between tokens and encode location information. In this paper, by treating single cells as spatial tokens, we study how to leverage transformers to facilitate spatial tanscriptomics imputation. In particular, investigate the following two key questions: (1) $\\textit{how to encode spatial information of cells in transformers}$, and (2) $\\textit{ how to train a transformer for transcriptomic imputation}$. By answering these two questions, we present a transformer-based imputation framework, SpaFormer, for cellular-level spatial transcriptomic data. Extensive experiments demonstrate that SpaFormer outperforms existing state-of-the-art imputation algorithms on three large-scale datasets while maintaining superior computational efficiency.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "30580446",
                    "name": "Haifang Wen"
                },
                {
                    "authorId": "2188792890",
                    "name": "Wenzhuo Tang"
                },
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "46496977",
                    "name": "Jiayuan Ding"
                },
                {
                    "authorId": "50268352",
                    "name": "Renming Liu"
                },
                {
                    "authorId": "2199025797",
                    "name": "Feng Shi"
                },
                {
                    "authorId": "2154871510",
                    "name": "Yuying Xie"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "b1e90b67675b6d7ae88b563a93cb4d375857cb15",
            "title": "CellPLM: Pre-training of Cell Language Model Beyond Single Cells",
            "abstract": "The current state-of-the-art single-cell pre-trained models are greatly inspired by the success of large language models. They trained transformers by treating genes as tokens and cells as sentences. However, three fundamental differences between single-cell data and natural language data are overlooked: (1) scRNA-seq data are presented as bag-of-genes instead of sequences of RNAs; (2) Cell-cell relations are more intricate and important than inter-sentence relations; and (3) The quantity of single-cell data is considerably inferior to text data, and they are very noisy. In light of these characteristics, we propose a new pre-trained model CellPLM, which takes cells as tokens and tissues as sentences. In addition, we leverage spatially-resolved transcriptomic data in pre-training to facilitate learning cell-cell relationships and introduce a Gaussian mixture prior distribution as an additional inductive bias to overcome data limitation. CellPLM is the first single-cell pre-trained transformer that encodes cell-cell relations and it consistently outperforms existing pre-trained and non-pre-trained models in diverse downstream tasks, with 100x times higher inference speed compared to existing pre-trained models.",
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256788829",
                    "name": "Hongzhi Wen"
                },
                {
                    "authorId": "2188792890",
                    "name": "Wenzhuo Tang"
                },
                {
                    "authorId": "2257010522",
                    "name": "Xinnan Dai"
                },
                {
                    "authorId": "46496977",
                    "name": "Jiayuan Ding"
                },
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "2154871510",
                    "name": "Yuying Xie"
                },
                {
                    "authorId": "2256937217",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "c194de20b4e1c5cd1978a05f192d492d2699db90",
            "title": "Single-Cell Multimodal Prediction via Transformers",
            "abstract": "The recent development of multimodal single-cell technology has made the possibility of acquiring multiple omics data from individual cells, thereby enabling a deeper understanding of cellular states and dynamics. Nevertheless, the proliferation of multimodal single-cell data also introduces tremendous challenges in modeling the complex interactions among different modalities. The recently advanced methods focus on constructing static interaction graphs and applying graph neural networks (GNNs) to learn from multimodal data. However, such static graphs can be suboptimal as they do not take advantage of the downstream task information; meanwhile GNNs also have some inherent limitations when deeply stacking GNN layers. To tackle these issues, in this work, we investigate how to leverage transformers for multimodal single-cell data in an end-to-end manner while exploiting downstream task information. In particular, we propose a scMoFormer framework which can readily incorporate external domain knowledge and model the interactions within each modality and cross modalities. Extensive experiments demonstrate that scMoFormer achieves superior performance on various benchmark datasets. Remarkably, scMoFormer won a Kaggle silver medal with the rank of 24/1221 (Top 2%) without ensemble in a NeurIPS 2022 competition1. Our implementation is publicly available at Github2.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2188792890",
                    "name": "Wenzhuo Tang"
                },
                {
                    "authorId": "30580446",
                    "name": "Haifang Wen"
                },
                {
                    "authorId": "50268352",
                    "name": "Renming Liu"
                },
                {
                    "authorId": "46496977",
                    "name": "Jiayuan Ding"
                },
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "2154871510",
                    "name": "Yuying Xie"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "b1b4309c9837d16bb8f696a8f2ede0c1b1931c8b",
            "title": "Deep Learning in Single-cell Analysis",
            "abstract": "Single-cell technologies are revolutionizing the entire field of biology. The large volumes of data generated by single-cell technologies are high dimensional, sparse, and heterogeneous and have complicated dependency structures, making analyses using conventional machine learning approaches challenging and impractical. In tackling these challenges, deep learning often demonstrates superior performance compared to traditional machine learning methods. In this work, we give a comprehensive survey on deep learning in single-cell analysis. We first introduce background on single-cell technologies and their development, as well as fundamental concepts of deep learning including the most popular deep architectures. We present an overview of the single-cell analytic pipeline pursued in research applications while noting divergences due to data sources or specific applications. We then review seven popular tasks spanning different stages of the single-cell analysis pipeline, including multimodal integration, imputation, clustering, spatial domain identification, cell-type deconvolution, cell segmentation, and cell-type annotation. Under each task, we describe the most recent developments in classical and deep learning methods and discuss their advantages and disadvantages. Deep learning tools and benchmark datasets are also summarized for each task. Finally, we discuss the future directions and the most recent challenges. This survey will serve as a reference for biologists and computer scientists, encouraging collaborations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2188737365",
                    "name": "Dylan Molho"
                },
                {
                    "authorId": "46496977",
                    "name": "Jiayuan Ding"
                },
                {
                    "authorId": "2188802257",
                    "name": "Zhaoheng Li"
                },
                {
                    "authorId": "30580446",
                    "name": "Haifang Wen"
                },
                {
                    "authorId": "2188792890",
                    "name": "Wenzhuo Tang"
                },
                {
                    "authorId": "2143443305",
                    "name": "Yixin Wang"
                },
                {
                    "authorId": "1768191205",
                    "name": "Julian Venegas"
                },
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "50268352",
                    "name": "Renming Liu"
                },
                {
                    "authorId": "1943369921",
                    "name": "Runze Su"
                },
                {
                    "authorId": "2011416",
                    "name": "P. Danaher"
                },
                {
                    "authorId": "2115688334",
                    "name": "Robert Yang"
                },
                {
                    "authorId": "71157169",
                    "name": "Y. Lei"
                },
                {
                    "authorId": "2154871510",
                    "name": "Yuying Xie"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        }
    ]
}