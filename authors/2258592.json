{
    "authorId": "2258592",
    "papers": [
        {
            "paperId": "10e0eb48710d7a98f284e41ec27f9a3c09f9b841",
            "title": "Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?",
            "abstract": "Large Language Models (LLMs) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various LLMs, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the LLM-KG-Bench. The evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of LLMs in relation to their application within RDF knowledge graph engineering workflows utilizing Turtle representation. While our findings show that the latest commercial models outperform their forerunners in terms of proficiency with the Turtle language, they also reveal an apparent weakness. These models fall short when it comes to adhering strictly to the output formatting constraints, a crucial requirement in this context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "34886987",
                    "name": "Lars-Peter Meyer"
                },
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "2236686278",
                    "name": "Felix Brei"
                },
                {
                    "authorId": "2108312",
                    "name": "Kirill Bulert"
                }
            ]
        },
        {
            "paperId": "d64398a12dee441da3a86b1e00f12e9369280217",
            "title": "Reproducibility Crisis in the LOD Cloud? Studying the Impact of Ontology Accessibility and Archiving as a Counter Measure",
            "abstract": ". The reproducibility crisis is an ongoing problem that a\ufb00ects data-driven science to a big extent. The highly connected decentral Web of Ontologies represents the backbone for semantic data and the Linked Open Data Cloud and provides terminological context information crucial for the usage and interpretation of the data, which in turn is key for the reproducibility of research results making use of it. In this paper, we identify, analyze, and quantify reproducibility issues related to capturing terminological context (e.g. caused by unavailable ontologies) and delineate the impact on the reproducibility crisis in the Linked Open Data Cloud. Our examinations are backed by a frequent and ongoing monitoring of online available vocabularies and ontologies that results in the DBpedia Archivo dataset. We also show the extent to which the reproducibility crisis can be countered with the aid of ontology archiving in DBpedia Archivo and the Linked Open Vocabularies platforms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32114346",
                    "name": "Johannes Frey"
                },
                {
                    "authorId": "2003784069",
                    "name": "Denis Streitmatter"
                },
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "21785c2ecacfd60ecf5c74703ed74a14a2d55106",
            "title": "A Visual SHACL Shapes Editor Based On OntoPad",
            "abstract": "On the Semantic Web, vocabularies and ontologies play a fundamental role to express the terminology and rules of certain domains. New technologies like SHACL provide the possibility to express data schemata specific to certain data sets, applications, and domains. However, the domain modeling process is collaborative and when using RDF, it requires technical knowledge. In this paper, we present a tool to support a two-step-process to model a terminology and a schema with a combined graphical RDF Schema editor and visual SHACL editor. This tool allows domain experts to create a terminology and schema without the need for a deep understanding of RDF Schema or SHACL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "39460549",
                    "name": "Andr\u00e9 Valdestilhas"
                },
                {
                    "authorId": "3489323",
                    "name": "G. Publio"
                },
                {
                    "authorId": "2016113016",
                    "name": "Andrea Cimmino Arriaga"
                },
                {
                    "authorId": "1749965",
                    "name": "Konrad H\u00f6ffner"
                },
                {
                    "authorId": "29356676",
                    "name": "Thomas Riechert"
                }
            ]
        },
        {
            "paperId": "c56898a1cb81dabba16bb75b7266933eb0e5a8c8",
            "title": "Doctoral Symposium on Research on Online Databases in History (RODBH 2019)",
            "abstract": "This editorial provides an introduction to the field of research of the Doctoral Symposium on Research on Online Databases in History (RODBH 2019) which was collocated with the 3rd Data for History workshop. The workshop series is situated in the field of digital humanities and targets the interconnection of subjects of historical research, knowledge engineering, and information science. The common interlink of this disciplines is the use of research data, data management, and all accompanying activities as well as the organization of collaborative community processes.",
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology"
            ],
            "authors": [
                {
                    "authorId": "29356676",
                    "name": "Thomas Riechert"
                },
                {
                    "authorId": "2083428007",
                    "name": "F. Beretta"
                },
                {
                    "authorId": "3371108",
                    "name": "G. Bruseker"
                },
                {
                    "authorId": "2522516",
                    "name": "Edgard Marx"
                },
                {
                    "authorId": "48891400",
                    "name": "J. Blanke"
                },
                {
                    "authorId": "2104928185",
                    "name": "Vincent Alamercery"
                },
                {
                    "authorId": "2055417491",
                    "name": "Tracy Hoffmann"
                },
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                }
            ]
        },
        {
            "paperId": "3d2ade0f41fa8ca85c52593ea4c97c8ed06b50b5",
            "title": "Decentralized Collaborative Knowledge Management using Git",
            "abstract": "Apart from documents, datasets are gaining more attention on the World Wide Web. An increasing number of the datasets on the Web are available as Linked Data, also called the Linked Open Data Cloud1 or Giant Global Graph2. Collaboration of people and machines is a major aspect of the World Wide Web and as well of the Semantic Web. Currently, the access to RDF data on the Semantic Web is possible by applying the Linked Data principles3, and the SPARQL specification4, which enables clients to access and retrieve data stored and published via SPARQL endpoints. RDF resources in the Semantic Web are interconnected and often correspond to previously created vocabularies and patterns. This way of reusing existing knowledge facilitates the modeling and representation of information and may optimally reduce the development costs of a knowledge base. As a result of the collaborative reuse process, structural and content interferences as well as varying models and contradictory statements are inevitable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "2249143718",
                    "name": "Michael Martin"
                }
            ]
        },
        {
            "paperId": "6578b32effd60495de15793103516cba97fe4f9d",
            "title": "Conflict Detection, Avoidance, and Resolution in a Non-Linear RDF Version Control System",
            "abstract": "The Semantic Web is about collaboration and exchange of information. While the data on the Semantic Web is constantly evolving and meant to be collaboratively edited there is no practical transactional concept or method to control concurrent writes to a dataset and avoid conflicts. Thus, we follow the question, how can we ensure a controlled state of a SPARQL Store when performing non transactional write operations? Based on the Distributed Version Control System for RDF data implemented in the Quit Store we present the Quit Editor Interface Concurrency Control (QEICC). QEICC provides a protocol on top of the SPARQL 1.1 standard to identify, avoid, and resolve conflicts. The strategies reject, branch, and merge are presented to allow different levels of control over the conflict resolution. While the reject strategy gives full control to the client, with branch and merge it is even possible to postpone the conflict resolution and integrate it into the date engineering process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "2096905450",
                    "name": "Norman Radtke"
                }
            ]
        },
        {
            "paperId": "83161c1c7250a453178f86417df9440be44a91c0",
            "title": "A Method for Distributed and Collaborative Curation of RDF Datasets Utilizing the Quit Stack",
            "abstract": "Knowledge engineering is becoming more and more important and collaborative approaches are promising. Especially in science, collaborative knowledge engineering on research data is a key factor for success. We propose a three layered method for distributed collaboration in curation of RDF datasets with the aim to bring domain experts into the role to command the process. The method builds on the existing infrastructure and work-flows of software engineering. By adding an RDF layer on top of the Git infrastructure, the method is flexible in its adaption to various domains using domain specific editing interfaces.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "2096905450",
                    "name": "Norman Radtke"
                }
            ]
        },
        {
            "paperId": "b8006352546cbde6958663c3a2d03c17d1977c3c",
            "title": "CubeViz.js: A Lightweight Framework for Discovering and Visualizing RDF Data Cubes",
            "abstract": "In this paper we present CubeViz.js, the successor of CubeViz, as an approach for lightweight visualization and exploration of statistical data using the RDF Data Cube vocabulary. In several use cases, such as the European Unions Open Data Portal, in which we deployed CubeViz, we were able to gather various requirements that eventually led to the decision of reimplementing CubeViz as JavaScript-only application. As part of this paper we showcase major functionalities of CubeViz.js and its improvements in comparison to the prior version.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49988346",
                    "name": "Konrad Abicht"
                },
                {
                    "authorId": "2083989823",
                    "name": "Georges Alkhouri"
                },
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "36476994",
                    "name": "R. Meissner"
                },
                {
                    "authorId": "2110607609",
                    "name": "Michael Martin"
                }
            ]
        },
        {
            "paperId": "e188ffdcf49e779570b136bf69db4168c311fccb",
            "title": "Exploring the Evolution and Provenance of Git Versioned RDF Data",
            "abstract": "The distributed character and the manifold possibilities for interchanging data on the Web lead to the problem of getting hold of the provenance of the data. Especially in the domain of digital humanities and when dealing with Linked Data in an enterprise context provenance information is needed to support the collaborative process of data management. We are proposing a possibility for capturing and exploring provenance information, based on the methodology of managing RDF data in a tool stack on top of the decentralized source code management system Git. This comprises a queriable history graph, the possibility to query arbitrary revisions of a Git versioned store and in the minimal granularity the possibility to annotate individual statements with their provenance information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "2094248803",
                    "name": "Patrick Naumann"
                },
                {
                    "authorId": "2522516",
                    "name": "Edgard Marx"
                }
            ]
        },
        {
            "paperId": "112c69a54ac945f240a07f08dfa2ca3cda0c036d",
            "title": "Distributed Collaboration on RDF Datasets Using Git: Towards the Quit Store",
            "abstract": "Collaboration is one of the most important topics regarding the evolution of the World Wide Web and thus also for the Web of Data. In scenarios of distributed collaboration on datasets it is necessary to provide support for multiple different versions of datasets to exist simultaneously, while also providing support for merging diverged datasets. In this paper we present an approach that uses SPARQL 1.1 in combination with the version control system Git, that creates commits for all changes applied to an RDF dataset containing multiple named graphs. Further the operations provided by Git are used to distribute the commits among collaborators and merge diverged versions of the dataset. We show the advantages of (public) Git repositories for RDF datasets and how this represents a way to collaborate on RDF data and consume it. With SPARQL 1.1 and Git in combination, users are given several opportunities to participate in the evolution of RDF data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "2096905450",
                    "name": "Norman Radtke"
                },
                {
                    "authorId": "2110607609",
                    "name": "Michael Martin"
                }
            ]
        }
    ]
}