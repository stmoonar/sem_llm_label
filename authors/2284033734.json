{
    "authorId": "2284033734",
    "papers": [
        {
            "paperId": "14e9ad1f2f107c450ce8dadf5907c2edea38e9ab",
            "title": "Practical and Robust Safety Guarantees for Advanced Counterfactual Learning to Rank",
            "abstract": "Counterfactual learning to rank (CLTR) can be risky and, in various circumstances, can produce sub-optimal models that hurt performance when deployed. Safe CLTR was introduced to mitigate these risks when using inverse propensity scoring to correct for position bias. However, the existing safety measure for CLTR is not applicable to state-of-the-art CLTR methods, cannot handle trust bias, and relies on specific assumptions about user behavior. Our contributions are two-fold. First, we generalize the existing safe CLTR approach to make it applicable to state-of-the-art doubly robust CLTR and trust bias. Second, we propose a novel approach, proximal ranking policy optimization (PRPO), that provides safety in deployment without assumptions about user behavior. PRPO removes incentives for learning ranking behavior that is too dissimilar to a safe ranking model. Thereby, PRPO imposes a limit on how much learned models can degrade performance metrics, without relying on any specific user assumptions. Our experiments show that both our novel safe doubly robust method and PRPO provide higher performance than the existing safe inverse propensity scoring approach. However, in unexpected circumstances, the safe doubly robust approach can become unsafe and bring detrimental performance. In contrast, PRPO always maintains safety, even in maximally adversarial situations. By avoiding assumptions, PRPO is the first method with unconditional safety in deployment that translates to robust safety for real-world applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284033734",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "143624743",
                    "name": "Harrie Oosterhuis"
                },
                {
                    "authorId": "2265490493",
                    "name": "M. D. Rijke"
                }
            ]
        },
        {
            "paperId": "3100dacd80b4cff78e773c766c9ecf740d8861cc",
            "title": "Proximal Ranking Policy Optimization for Practical Safety in Counterfactual Learning to Rank",
            "abstract": "Counterfactual learning to rank (CLTR) can be risky and, in various circumstances, can produce sub-optimal models that hurt performance when deployed. Safe CLTR was introduced to mitigate these risks when using inverse propensity scoring to correct for position bias. However, the existing safety measure for CLTR is not applicable to state-of-the-art CLTR methods, cannot handle trust bias, and relies on specific assumptions about user behavior. We propose a novel approach, proximal ranking policy optimization (PRPO), that provides safety in deployment without assumptions about user behavior. PRPO removes incentives for learning ranking behavior that is too dissimilar to a safe ranking model. Thereby, PRPO imposes a limit on how much learned models can degrade performance metrics, without relying on any specific user assumptions. Our experiments show that PRPO provides higher performance than the existing safe inverse propensity scoring approach. PRPO always maintains safety, even in maximally adversarial situations. By avoiding assumptions, PRPO is the first method with unconditional safety in deployment that translates to robust safety for real-world applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284033734",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "143624743",
                    "name": "Harrie Oosterhuis"
                },
                {
                    "authorId": "2265490493",
                    "name": "M. D. Rijke"
                }
            ]
        },
        {
            "paperId": "3d377006a2fba9e3c8f1cf8b4f382405497d7312",
            "title": "Optimal Baseline Corrections for Off-Policy Contextual Bandits",
            "abstract": "The off-policy learning paradigm allows for recommender systems and general ranking applications to be framed as decision-making problems, where we aim to learn decision policies that optimize an unbiased offline estimate of an online reward metric. With unbiasedness comes potentially high variance, and prevalent methods exist to reduce estimation variance. These methods typically make use of control variates, either additive (i.e., baseline corrections or doubly robust methods) or multiplicative (i.e., self-normalisation). Our work unifies these approaches by proposing a single framework built on their equivalence in learning scenarios. The foundation of our framework is the derivation of an equivalent baseline correction for all of the existing control variates. Consequently, our framework enables us to characterize the variance-optimal unbiased estimator and provide a closed-form solution for it. This optimal estimator brings significantly improved performance in both evaluation and learning, and minimizes data requirements. Empirical observations corroborate our theoretical findings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284033734",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "2299944027",
                    "name": "Olivier Jeunen"
                },
                {
                    "authorId": "143624743",
                    "name": "Harrie Oosterhuis"
                },
                {
                    "authorId": "2265490493",
                    "name": "M. D. Rijke"
                }
            ]
        },
        {
            "paperId": "d416754ea0d310f7cab5803798d5b121122c3ff5",
            "title": "A Simpler Alternative to Variational Regularized Counterfactual Risk Minimization",
            "abstract": "Variance regularized counterfactual risk minimization (VRCRM) has been proposed as an alternative off-policy learning (OPL) method. VRCRM method uses a lower-bound on the $f$-divergence between the logging policy and the target policy as regularization during learning and was shown to improve performance over existing OPL alternatives on multi-label classification tasks. In this work, we revisit the original experimental setting of VRCRM and propose to minimize the $f$-divergence directly, instead of optimizing for the lower bound using a $f$-GAN approach. Surprisingly, we were unable to reproduce the results reported in the original setting. In response, we propose a novel simpler alternative to f-divergence optimization by minimizing a direct approximation of f-divergence directly, instead of a $f$-GAN based lower bound. Experiments showed that minimizing the divergence using $f$-GANs did not work as expected, whereas our proposed novel simpler alternative works better empirically.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2321406763",
                    "name": "Hua Chang Bakker"
                },
                {
                    "authorId": "2284033734",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "143624743",
                    "name": "Harrie Oosterhuis"
                }
            ]
        },
        {
            "paperId": "d458f18ec6a763ab49bf1d3d613affa4de2e2f99",
            "title": "A First Look at Selection Bias in Preference Elicitation for Recommendation",
            "abstract": "Preference elicitation explicitly asks users what kind of recommendations they would like to receive. It is a popular technique for conversational recommender systems to deal with cold-starts. Previous work has studied selection bias in implicit feedback, e.g., clicks, and in some forms of explicit feedback, i.e., ratings on items. Despite the fact that the extreme sparsity of preference elicitation interactions make them severely more prone to selection bias than natural interactions, the effect of selection bias in preference elicitation on the resulting recommendations has not been studied yet. To address this gap, we take a first look at the effects of selection bias in preference elicitation and how they may be further investigated in the future. We find that a big hurdle is the current lack of any publicly available dataset that has preference elicitation interactions. As a solution, we propose a simulation of a topic-based preference elicitation process. The results from our simulation-based experiments indicate (i) that ignoring the effect of selection bias early in preference elicitation can lead to an exacerbation of overrepresentation in subsequent item recommendations, and (ii) that debiasing methods can alleviate this effect, which leads to significant improvements in subsequent item recommendation performance. Our aim is for the proposed simulator and initial results to provide a starting point and motivation for future research into this important but overlooked problem setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284033734",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "143624743",
                    "name": "Harrie Oosterhuis"
                },
                {
                    "authorId": "2265490493",
                    "name": "M. D. Rijke"
                }
            ]
        },
        {
            "paperId": "d7dd592e4af74e4abf90d8e86de136f931a8610a",
            "title": "Unbiased Learning to Rank: On Recent Advances and Practical Applications",
            "abstract": "Since its inception, the field of unbiased learning to rank (ULTR) has remained very active and has seen several impactful advancements in recent years. This tutorial provides both an introduction to the core concepts of the field and an overview of recent advancements in its foundations, along with several applications of its methods. The tutorial is divided into four parts: Firstly, we give an overview of the different forms of bias that can be addressed with ULTR methods. Secondly, we present a comprehensive discussion of the latest estimation techniques in the ULTR field. Thirdly, we survey published results of ULTR in real-world applications. Fourthly, we discuss the connection between ULTR and fairness in ranking. We end by briefly reflecting on the future of ULTR research and its applications. This tutorial is intended to benefit both researchers and industry practitioners interested in developing new ULTR solutions or utilizing them in real-world applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284033734",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "2283939214",
                    "name": "Philipp Hager"
                },
                {
                    "authorId": "2261085212",
                    "name": "Jin Huang"
                },
                {
                    "authorId": "3096714",
                    "name": "Ali Vardasbi"
                },
                {
                    "authorId": "143624743",
                    "name": "Harrie Oosterhuis"
                }
            ]
        },
        {
            "paperId": "33e37c3bdbfb144c954122048f9822e91ca814bb",
            "title": "Recent Advancements in Unbiased Learning to Rank",
            "abstract": "Since its inception, the field of unbiased learning to rank (ULTR) has remained very active and has seen several impactful advancements in recent years. This tutorial provides both an introduction to the core concepts of the field and an overview of recent advancements in its foundations along with several applications of its methods. The tutorial is divided into four parts: Firstly, we give an overview of the different forms of bias that can be addressed with ULTR methods. Secondly, we present a comprehensive discussion of the latest estimation techniques in the ULTR field. Thirdly, we survey published results of ULTR in real-world applications. Fourthly, we discuss the connection between ULTR and fairness in ranking. We end by briefly reflecting on the future of ULTR research and its applications. This tutorial is intended to benefit both researchers and industry practitioners who are interested in developing new ULTR solutions or utilizing them in real-world applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284033734",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "2283939214",
                    "name": "Philipp Hager"
                },
                {
                    "authorId": "143624743",
                    "name": "Harrie Oosterhuis"
                }
            ]
        }
    ]
}