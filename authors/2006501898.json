{
    "authorId": "2006501898",
    "papers": [
        {
            "paperId": "2d19ea2b8099ca0c104ad05e38a169b500f1295a",
            "title": "Focal Cosine Metric and Adaptive Attention Module for Remote Sensing Scene Classification With Siamese Convolutional Neural Networks",
            "abstract": "Convolutional neural networks (CNNs) have been widely used in remote sensing (RS) scene classification tasks due to their remarkable feature representation and inference capability. The complexity of RS images not only brings the challenges of high inter-class similarity and large intra-class diversity, but also introduces the problem that category-relevant regions are insufficiently prominent in feature extraction. Siamese CNNs with feature similarity measurement are chosen in some applications to overcome the former issue, but most ignore the randomness of input sample pairs. This makes the Siamese CNNs not focus enough on challenging samples, which limits the training efficiency. We propose the focal cosine metric (FCM) block that combines the cosine similarity metric and the threshold control to achieve sample selection, thereby completing network learning more efficiently. FCM only permits the misclassified focal samples to participate in similarity measurement based on Siamese CNN. It flexibly mitigates the misclassification caused by the high inter-class similarity and large intra-class diversity. Moreover, the adaptive attention (AA) module is designed to stress the pivotal target regions and assist in the similarity measurement of Siamese CNN. This is realized by adaptively assigning high weights to key targets with learnable guided vectors. It enables the model to focus on the details of intra-class similarities or inter-class differences in sample pairs, and thus reduces the difficulty of model optimization. Encouraging experimental results on three public data sets demonstrate the effectiveness of the novel Siamese CNN-based method with FCM and AA and show its superiority compared to other state-of-the-art scene classification methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2006501898",
                    "name": "Lei Min"
                },
                {
                    "authorId": "46812189",
                    "name": "Kun Gao"
                },
                {
                    "authorId": "49528487",
                    "name": "Hong Wang"
                },
                {
                    "authorId": "2130182524",
                    "name": "Yutong Liu"
                },
                {
                    "authorId": "2109262818",
                    "name": "Zhenzhou Zhang"
                },
                {
                    "authorId": "15368955",
                    "name": "Zibo Hu"
                },
                {
                    "authorId": "51161322",
                    "name": "Xiaodian Zhang"
                }
            ]
        },
        {
            "paperId": "8fc2548ae8f31a148b29281e359ea3151d2115ed",
            "title": "Triplet-Metric-Guided Multi-Scale Attention for Remote Sensing Image Scene Classification with a Convolutional Neural Network",
            "abstract": "Remote sensing image scene classification (RSISC) plays a vital role in remote sensing applications. Recent methods based on convolutional neural networks (CNNs) have driven the development of RSISC. However, these approaches are not adequate considering the contributions of different features to the global decision. In this paper, triplet-metric-guided multi-scale attention (TMGMA) is proposed to enhance task-related salient features and suppress task-unrelated salient and redundant features. Firstly, we design the multi-scale attention module (MAM) guided by multi-scale feature maps to adaptively emphasize salient features and simultaneously fuse multi-scale and contextual information. Secondly, to capture task-related salient features, we use the triplet metric (TM) to optimize the learning of MAM under the constraint that the distance of the negative pair is supposed to be larger than the distance of the positive pair. Notably, the MAM and TM collaboration can enforce learning a more discriminative model. As such, our TMGMA can avoid the classification confusion caused by only using the attention mechanism and the excessive correction of features caused by only using the metric learning. Extensive experiments demonstrate that our TMGMA outperforms the ResNet50 baseline by 0.47% on the UC Merced, 1.46% on the AID, and 1.55% on the NWPU-RESISC45 dataset, respectively, and achieves performance that is competitive with other state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144273636",
                    "name": "Hong Wang"
                },
                {
                    "authorId": "46812189",
                    "name": "Kun Gao"
                },
                {
                    "authorId": "2006501898",
                    "name": "Lei Min"
                },
                {
                    "authorId": "1574346529",
                    "name": "Yuxuan Mao"
                },
                {
                    "authorId": "51161322",
                    "name": "Xiaodian Zhang"
                },
                {
                    "authorId": "2110276903",
                    "name": "Junwei Wang"
                },
                {
                    "authorId": "15368955",
                    "name": "Zibo Hu"
                },
                {
                    "authorId": "2130182524",
                    "name": "Yutong Liu"
                }
            ]
        },
        {
            "paperId": "a71a1697ec8bcc114f9cc17774088cddade7f394",
            "title": "Lung CT image enhancement based on total variational frame and wavelet transform",
            "abstract": "Benefitting from the development of computer vision, computed tomography (CT) images have been used for assisting doctor's clinical diagnosis and improving the diagnostic efficiency. However, there exist some issues in medical images, such as low contrast, obscure detail, and complex noise due to the restriction of the system and the equipment in the process of imaging, medical images. To resolve these issues, a novel lung CT image enhancement method based on total variational framework combined with wavelet transform is proposed. Firstly, low\u2010frequency structure layer with low contrast and high\u2010frequency details layer with complex noise signals are acquired by decomposing the original image using total variational framework. Then, through the analysis of the histogram distribution characteristics of CT image, structure layer requires contrast enhancement; at the same time, the detail layer performs wavelet transform adaptive threshold denoising to remove noise. Finally, weight fusion of processed structure layer and details layer is performed to obtain the final fusion enhancement CT images. Experimental results show that the proposed method can enhance the contrast of clinical lung CT images, improve the clarity of details, and effectively suppress artifacts and complex noises. Contrast and sharpness\u2014objective indicators\u2014prove the proposed method's advantages. Subjectively, the proposed method performs superior over other existing CT enhancement methods, which achieves a better visual recognition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113217294",
                    "name": "Hongfei Wang"
                },
                {
                    "authorId": "2115894587",
                    "name": "P. Yang"
                },
                {
                    "authorId": "2157488922",
                    "name": "Chuan Xu"
                },
                {
                    "authorId": "2006501898",
                    "name": "Lei Min"
                },
                {
                    "authorId": "2118511114",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "144862247",
                    "name": "Bing Xu"
                }
            ]
        },
        {
            "paperId": "b238a7513097c03a2d29d8a1044efb6c546fadfb",
            "title": "Gradient Enhanced Dual Regression Network: Perception-Preserving Super-Resolution for Multi-Sensor Remote Sensing Imagery",
            "abstract": "Most existing learning-based single image super-resolution (SISR) methods mainly focus on improving reconstruction accuracy, but they always generate overly smoothed results that fail to match the visual perception. Although perceptual quality can be greatly improved via introducing adversarial loss, image fidelity may decrease to some extent. Moreover, most methods are trained and evaluated on simulated datasets and their performance would drop significantly on real remote sensing imagery. To solve the above problems, we propose a new SISR algorithm named gradient enhanced dual regression network (GEDRN). Based on the dual regression framework, we use share-source residual structure and non-local operation to learn abundant low-frequency information and long-distance spatial correlations. Besides, we not only introduce additional gradient information to avoid blurry results but also apply gradient loss and perceptual loss to further improve the perceptual quality. Our GEDRN is trained and tested on real-world multi-sensor satellite images. Experimental results demonstrate the superiority of the proposed method in achieving much better perceptual quality and ensuring high fidelity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109262818",
                    "name": "Zhenzhou Zhang"
                },
                {
                    "authorId": "46812189",
                    "name": "Kun Gao"
                },
                {
                    "authorId": "2110276903",
                    "name": "Junwei Wang"
                },
                {
                    "authorId": "2006501898",
                    "name": "Lei Min"
                },
                {
                    "authorId": "2151129927",
                    "name": "Shijing Ji"
                },
                {
                    "authorId": "2064176746",
                    "name": "Chong Ni"
                },
                {
                    "authorId": "2133809172",
                    "name": "Dayu Chen"
                }
            ]
        },
        {
            "paperId": "a3bed1b880728d3685b79591c392ca852751c9b8",
            "title": "Remote sensing image scene classification using deep combinative feature learning",
            "abstract": "Scene classification shows pivotal role in remote sensing image researches. Since challenges of large similarity between classes, high diversity in each class and huge variations in background, spatial resolution, translation, etc., remote sensing image scene classification still urgently need development. In this paper, we propose a novel method named deep combinative feature learning (DCFL) to extract low-level texture and high-level semantic information from different network layers. First, feature encoder VGGNet-16 is fine-tuned for subsequent multi-scale feature extraction. And two shallow convolutional (Conv) layers are selected for convolutional feature summing maps (CFSM), from which we extract uniform LBP with rotation invariance to excavate detailed texture. Deep semantic features from fully-connected (FC) layer concatenated with shallow detailed features constitute deep combinative features, which are thrown into support vector machine (SVM) classifier for final classification. Extensive experiments are carried out and results prove the comparable advantages and effectiveness of the proposed DCFL contrasting with different state-of-art methods.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2006501898",
                    "name": "Lei Min"
                },
                {
                    "authorId": "46812189",
                    "name": "Kun Gao"
                },
                {
                    "authorId": "48016237",
                    "name": "Hong Wang"
                },
                {
                    "authorId": "2110276903",
                    "name": "Junwei Wang"
                },
                {
                    "authorId": "2027169833",
                    "name": "Peilin Yu"
                },
                {
                    "authorId": "2118549112",
                    "name": "Ting Li"
                },
                {
                    "authorId": "2111498596",
                    "name": "Zhuoyi Chen"
                }
            ]
        },
        {
            "paperId": "1573756710401694acef85c605b6ef6701cd4ad2",
            "title": "Extraction of Structured Light Method in Machine Vision",
            "abstract": "In the line structured light three-dimensional measurement system based on trigonometry,how quickly and accurately from the complicated background of rapid extraction of structured light,and extracting the light image light strip Center location is key to real-time precision distance measuring problems.Based on the analysis of line-structured laser light source selection,environmental noise and other factors affecting the light extraction based on to the already existing linestructured laser light extraction method are reviewed,and the strengths and weaknesses of existing algorithms are compared.Finally,this paper presents a preliminary extraction of structured light HSV color space,then uses the threshold and light Center of gravity method to determine two-step extraction algorithm.This algorithm converts the RGB space into HSV space for initial processing to extract structured light,then the thresholds and focus are combined to accurately determine centre position of the light.Experimental results show that the algorithm can accurately extract stripe Center,and have a strong ability to withstand noise,sub-pixel precision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2006501898",
                    "name": "Lei Min"
                }
            ]
        }
    ]
}