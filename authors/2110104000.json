{
    "authorId": "2110104000",
    "papers": [
        {
            "paperId": "6b9699058152912efedd9a6d724ec08e0c4c9319",
            "title": "Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset",
            "abstract": "Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relevant to the dialogue context than those in prior datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117956135",
                    "name": "Minjin Kim"
                },
                {
                    "authorId": "2110104000",
                    "name": "Minju Kim"
                },
                {
                    "authorId": "2281098420",
                    "name": "Hana Kim"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2290185769",
                    "name": "Soyeon Chun"
                },
                {
                    "authorId": "2290213007",
                    "name": "Hyunseo Kim"
                },
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "2258802492",
                    "name": "Youngjae Yu"
                },
                {
                    "authorId": "2258712913",
                    "name": "Jinyoung Yeo"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                }
            ]
        },
        {
            "paperId": "c86eba7860f37c47b97fda9d607b5386b7a1cb15",
            "title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics",
            "abstract": "The idea of personality in descriptive psychology, traditionally defined through observable behavior, has now been extended to Large Language Models (LLMs) to better understand their behavior. This raises a question: do LLMs exhibit distinct and consistent personality traits, similar to humans? Existing self-assessment personality tests, while applicable, lack the necessary validity and reliability for precise personality measurements. To address this, we introduce TRAIT, a new tool consisting of 8K multi-choice questions designed to assess the personality of LLMs with validity and reliability. TRAIT is built on the psychometrically validated human questionnaire, Big Five Inventory (BFI) and Short Dark Triad (SD-3), enhanced with the ATOMIC10X knowledge graph for testing personality in a variety of real scenarios. TRAIT overcomes the reliability and validity issues when measuring personality of LLM with self-assessment, showing the highest scores across three metrics: refusal rate, prompt sensitivity, and option order sensitivity. It reveals notable insights into personality of LLM: 1) LLMs exhibit distinct and consistent personality, which is highly influenced by their training data (i.e., data used for alignment tuning), and 2) current prompting techniques have limited effectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in this direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307987815",
                    "name": "Seungbeen Lee"
                },
                {
                    "authorId": "2220330035",
                    "name": "Seungwon Lim"
                },
                {
                    "authorId": "2423429",
                    "name": "Seungju Han"
                },
                {
                    "authorId": "2307917646",
                    "name": "Giyeong Oh"
                },
                {
                    "authorId": "2184029886",
                    "name": "Hyungjoo Chae"
                },
                {
                    "authorId": "2004821977",
                    "name": "Jiwan Chung"
                },
                {
                    "authorId": "2110104000",
                    "name": "Minju Kim"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2307982267",
                    "name": "Yeonsoo Lee"
                },
                {
                    "authorId": "2258907627",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "2258712913",
                    "name": "Jinyoung Yeo"
                },
                {
                    "authorId": "2258802490",
                    "name": "Youngjae Yu"
                }
            ]
        },
        {
            "paperId": "1afeaef110134ebbba11604a206a54be547fdee0",
            "title": "Dual Task Framework for Improving Persona-grounded Dialogue Dataset",
            "abstract": "This paper introduces a simple yet effective data-centric approach for the task of improving persona-conditioned dialogue agents. Prior model-centric approaches unquestioningly depend on the raw crowdsourced benchmark datasets such as Persona-Chat. In contrast, we aim to fix annotation artifacts in benchmarking, which is orthogonally applicable to any dialogue model. Specifically, we augment relevant personas to improve dialogue dataset/agent, by leveraging the primal-dual structure of the two tasks, predicting dialogue responses and personas based on each other. Experiments on Persona-Chat show that our approach outperforms pre-trained LMs by an 11.7 point gain in terms of accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110104000",
                    "name": "Minju Kim"
                },
                {
                    "authorId": "2151245501",
                    "name": "Beong-woo Kwak"
                },
                {
                    "authorId": "2116682476",
                    "name": "Youngwook Kim"
                },
                {
                    "authorId": "2155252638",
                    "name": "Hong-in Lee"
                },
                {
                    "authorId": "2153642272",
                    "name": "Seung-won Hwang"
                },
                {
                    "authorId": "1898428",
                    "name": "Jinyoung Yeo"
                }
            ]
        },
        {
            "paperId": "dba29a38290ff3dfbb8646ac976e48506a5f5101",
            "title": "BotsTalk: Machine-sourced Framework for Automatic Curation of Large-scale Multi-skill Dialogue Datasets",
            "abstract": "To build open-domain chatbots that are able to use diverse communicative skills, we propose a novel framework BotsTalk, where multiple agents grounded to the specific target skills participate in a conversation to automatically annotate multi-skill dialogues. We further present Blended Skill BotsTalk (BSBT), a large-scale multi-skill dialogue dataset comprising 300K conversations. Through extensive experiments, we demonstrate that our dataset can be effective for multi-skill dialogue systems which require an understanding of skill blending as well as skill grounding. Our code and data are available at https://github.com/convei-lab/BotsTalk.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110104000",
                    "name": "Minju Kim"
                },
                {
                    "authorId": "2184057435",
                    "name": "Chaehyeong Kim"
                },
                {
                    "authorId": "2188768271",
                    "name": "Yongho Song"
                },
                {
                    "authorId": "2153642272",
                    "name": "Seung-won Hwang"
                },
                {
                    "authorId": "1898428",
                    "name": "Jinyoung Yeo"
                }
            ]
        }
    ]
}