{
    "authorId": "1762931",
    "papers": [
        {
            "paperId": "374804565d6c1f8a4abbbe3edb08a293082426e1",
            "title": "Online Learning of Decision Trees with Thompson Sampling",
            "abstract": "Decision Trees are prominent prediction models for interpretable Machine Learning. They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART. Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees. Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream. To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting. We analyse our algorithm and prove its almost sure convergence to the optimal tree. Furthermore, we conduct extensive experiments to validate our findings empirically. The proposed TSDT outperforms existing algorithms on several benchmarks, all while presenting the practical advantage of being tailored to the online setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1557555325",
                    "name": "Ayman Chaouki"
                },
                {
                    "authorId": "2277294960",
                    "name": "Jesse Read"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                }
            ]
        },
        {
            "paperId": "b485949d70d10b9c8743b8c60cf9780067387f3b",
            "title": "Branches: A Fast Dynamic Programming and Branch & Bound Algorithm for Optimal Decision Trees",
            "abstract": "Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine Learning, yet it poses a formidable optimisation challenge. Despite numerous efforts dating back to the early 1990's, practical algorithms have only recently emerged, primarily leveraging Dynamic Programming (DP) and Branch&Bound (B&B) techniques. These methods fall into two categories: algorithms like DL8.5, MurTree and STreeD utilise an efficient DP strategy but lack effective bounds for pruning the search space; while algorithms like OSDT and GOSDT employ more efficient pruning bounds but at the expense of a less refined DP strategy. We introduce Branches, a new algorithm that combines the strengths of both approaches. Using DP and B&B with a novel analytical bound for efficient pruning, Branches offers both speed and sparsity optimisation. Unlike other methods, it also handles non-binary features. Theoretical analysis shows its lower complexity compared to existing methods, and empirical results confirm that Branches outperforms the state-of-the-art in speed, iterations, and optimality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1557555325",
                    "name": "Ayman Chaouki"
                },
                {
                    "authorId": "2277294960",
                    "name": "Jesse Read"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                }
            ]
        },
        {
            "paperId": "120d671f6aabbc4920306c3f22cf828aa433ba8a",
            "title": "StreamAI: Dealing with Challenges of Continual Learning Systems for Serving AI in Production",
            "abstract": "How to build, deploy, update & maintain dynamic models which continuously learn from streaming data? This paper covers the industrialization aspects of these questions in production systems. In today\u2019s fast-changing environments, organizations are faced with the crucial challenge of predictive analytics in online fashion from big data and deploying Artificial Intelligence models at scale. Applications include cyber-security, cloud infrastructure, social networks and financial markets. Online learning models that learn continuously and adapt to the potentially evolving data distributions have demonstrated efficiency for big data stream learning. However, the challenges of deploying and maintaining such models in production (serving) have stalled their adoption. In this paper, we first categorize key challenges faced by the R&D, MLOps and governance teams for deploying automated and self-training AI models in production. Next, we highlight the challenges related to stream-based online machine-learning systems. Finally, we propose StreamAI, a technology-agnostic architecture to deal with the MLOps journey (learning, serving, maintenance) of online models in production. We conclude with open research questions for AI, MLOps and software engineering to bridge the gaps between industry needs and research-oriented development.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122673503",
                    "name": "Mariam Barry"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                },
                {
                    "authorId": "2223124959",
                    "name": "Jean-Luc Billy"
                }
            ]
        },
        {
            "paperId": "3a9129dc97b60f2ade7a1d29088d914cce2d7d17",
            "title": "BELLA: Black box model Explanations by Local Linear Approximations",
            "abstract": "In recent years, understanding the decision-making process of black-box models has become not only a legal requirement but also an additional way to assess their performance. However, the state of the art post-hoc interpretation approaches rely on synthetic data generation. This introduces uncertainty and can hurt the reliability of the interpretations. Furthermore, they tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model applies, so that the explanations are accurate, simple, general, and robust. BELLA can produce both factual and counterfactual explanations. Our user study confirms the importance of the desiderata we optimize, and our experiments show that BELLA outperforms the state-of-the-art approaches on these desiderata.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1505817544",
                    "name": "N. Radulovic"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "4cf55d69e775fcffc16d8f15bf8bd95aabc6d221",
            "title": "Aging and rejuvenating strategies for fading windows in multi-label classification on data streams",
            "abstract": "Combining the challenges of streaming data and multi-label learning, the task of mining a drifting, multi-label data stream requires methods that can accurately predict labelsets, adapt to various types of concept drift and run fast enough to process each data point before the next arrives. To achieve greater accuracy, many multi-label algorithms use computationally expensive techniques, such as multiple adaptive windows, with little concern for runtime and memory complexity. We present Aging and Rejuvenating kNN (ARkNN) which uses simple resources and efficient strategies to weight instances based on age, predictive performance, and similarity to the incoming data. We break down ARkNN into its component strategies to show the impact of each and experimentally compare ARkNN to seven state-of-the-art methods for learning from multi-label data streams. We demonstrate that it is possible to achieve competitive performance in multi-label classification on streams without sacrificing runtime and memory use, and without using complex and computationally expensive dual memory strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50427803",
                    "name": "M. Roseberry"
                },
                {
                    "authorId": "1693549",
                    "name": "S. D\u017eeroski"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                },
                {
                    "authorId": "143947197",
                    "name": "Alberto Cano"
                }
            ]
        },
        {
            "paperId": "69cdf85dc1f37c1539afa53a36a6ed53a76921d7",
            "title": "Survey on Online Streaming Continual Learning",
            "abstract": "Stream Learning (SL) attempts to learn from a data stream efficiently. A data stream learning algorithm should adapt to input data distribution shifts without sacrificing accuracy. These distribution shifts are known as \u201dconcept drifts\u201d in the literature. SL provides many supervised, semi-supervised, and unsupervised methods for detecting and adjusting to concept drift. On the other hand, Continual Learning (CL) attempts to preserve previous knowledge while performing well on the current concept when confronted with concept drift. In Online Continual Learning (OCL), this learning happens online. This survey explores the intersection of those two online learning paradigms to find synergies. We identify this intersection as Online Streaming Continual Learning (OSCL). The study starts with a gentle introduction to SL and then explores CL. Next, it explores OSCL from SL and OCL perspectives to point out new research trends and give directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144534729",
                    "name": "N. Gunasekara"
                },
                {
                    "authorId": "1737420",
                    "name": "Bernhard Pfahringer"
                },
                {
                    "authorId": "13645563",
                    "name": "Heitor Murilo Gomes"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                }
            ]
        },
        {
            "paperId": "a6638ab745e49f6884ab2beb5b07301ea78a0e10",
            "title": "FALL: A Modular Adaptive Learning Platform for Streaming Data",
            "abstract": "A growing number of tasks require adaptive machine learning systems capable of learning continuously from incoming data and adapting to changes in their environment. In order to enable the widespread adoption of machine learning for streaming data, it is crucial that practitioners and researchers have the tools to efficiently build and evaluate adaptive learning systems. In this paper we demonstrate FALL, a Framework for Adaptive Life-long Learning, which we have developed to enable the full adaptive learning pipeline to be built using modular, reusable components, enabling users to easily and efficiently develop, implement, and evaluate state-of-the-art adaptive learning systems. Source code, documentation, and examples may be found at https://benhalstead.dev/FALL/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087669179",
                    "name": "B. Halstead"
                },
                {
                    "authorId": "34930533",
                    "name": "Yun Sing Koh"
                },
                {
                    "authorId": "2066092410",
                    "name": "Patricia J. Riddle"
                },
                {
                    "authorId": "1691997",
                    "name": "Mykola Pechenizkiy"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                }
            ]
        },
        {
            "paperId": "ad0c099882776661495ce3a17e4c7f5a6343d216",
            "title": "Wangiri Fraud: Pattern Analysis and Machine-Learning-Based Detection",
            "abstract": "The rapid growth of the telecommunication landscape leads to a rapid rise of frauds in such networks. In this article, Wangiri fraud in which users are deceived by being charged for services without their knowledge during a call is tackled. In fact, Wangiri fraud has significant negative financial and reputation consequences for the mobile service providers and also has a bad psychological impact on the victims. In order to identify this fraudulent behavior, three Wangiri fraud patterns are defined by analyzing call records of over a year. Then, the security and performance of unsupervised and supervised machine learning (ML) methods in detecting one Wangiri pattern are evaluated using a large real-world Call Detail Records (CDRs) data set. In the context of Wangiri fraud detection, classification algorithms outperformed the others based on the chosen security and performance metrics. Finally, the performance evaluation of these algorithms is extended in detecting the other two real-world Wangiri fraud patterns. This article provides a detailed definition of the Wangiri fraud patterns and outlines the implementation and evaluation of ML algorithms in the context of detecting Wangiri fraud. The security analysis and experimental results demonstrate that depending on fraud patterns the best ML algorithm to detect Wangiri fraud may also vary.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054844626",
                    "name": "Akshaya Ravi"
                },
                {
                    "authorId": "3199375",
                    "name": "M. Msahli"
                },
                {
                    "authorId": "51189305",
                    "name": "Han Qiu"
                },
                {
                    "authorId": "1749624",
                    "name": "G. Memmi"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                },
                {
                    "authorId": "1471335567",
                    "name": "M. Qiu"
                }
            ]
        },
        {
            "paperId": "c4add496e8035a836f8e5a93d4c52f23b024abf5",
            "title": "Combining Diverse Meta-Features to Accurately Identify Recurring Concept Drift in Data Streams",
            "abstract": "Learning from streaming data is challenging as the distribution of incoming data may change over time, a phenomenon known as concept drift. The predictive patterns, or experience learned under one distribution may become irrelevant as conditions change under concept drift, but may become relevant once again when conditions reoccur. Adaptive learning methods adapt a classifier to concept drift by identifying which distribution, or concept, is currently present in order to determine which experience is relevant. Identifying a concept requires some representation to be stored for comparison, with the quality of the representation being key to accurate identification. Existing concept representations are based on meta-features, efficient univariate summaries of a concept. However, no single meta-feature can fully represent a concept, leading to severe accuracy loss when existing representations cannot describe concept drift. To avoid these failure cases, we propose the first general framework for combining a diverse range of meta-features into a single representation. We solve two main challenges, first presenting a method of efficiently computing, storing, and querying an arbitrary set of meta-features as a single representation, showing that a combination of meta-features may successfully avoid failure cases seen with existing methods. Second, we present the first method for dynamically learning which meta-features distinguish concepts in any given dataset, significantly improving performance. Our proposed approach enables state-of-the-art feature selection methods, such as mutual information, to be applied to concept representation meta-features for the first time. We investigate tradeoffs between memory budget and classification performance, observing accuracy increases of up to 16% by dynamically weighting the contribution of each meta-feature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087669179",
                    "name": "B. Halstead"
                },
                {
                    "authorId": "34930533",
                    "name": "Yun Sing Koh"
                },
                {
                    "authorId": "34429919",
                    "name": "Patricia J. Riddle"
                },
                {
                    "authorId": "1691997",
                    "name": "Mykola Pechenizkiy"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                }
            ]
        },
        {
            "paperId": "dc4c9eacc731c49e8acd0e63931ef799ae01e664",
            "title": "Preventing Discriminatory Decision-making in Evolving Data Streams",
            "abstract": "Bias in machine learning has rightly received significant attention over the past decade. However, most fair machine learning (fair-ML) works to address bias in decision-making systems has focused solely on the offline setting. Despite the wide prevalence of online systems in the real world, work on identifying and correcting bias in the online setting is severely lacking. The unique challenges of the online environment make addressing bias more difficult than in the offline setting. First, Streaming Machine Learning (SML) algorithms must deal with the constantly evolving real-time data stream. Secondly, they need to adapt to changing data distributions (concept drift) to make accurate predictions on new incoming data. Incorporating fairness constraints into this already intricate task is not straightforward. In this work, we focus on the challenges of achieving fairness in biased data streams while accounting for the presence of concept drift, accessing one sample at a time. We present Fair Sampling over Stream (FS2), a novel fair rebalancing approach capable of being integrated with SML classification algorithms. Furthermore, we devise the first unified performance-fairness metric, Fairness Bonded Utility (FBU), to efficiently evaluate and compare the trade-offs between performance and fairness across various bias mitigation methods. FBU simplifies the comparison of fairness-performance trade-offs of multiple techniques through one unified and intuitive evaluation, allowing model designers to easily choose a technique. Overall, extensive evaluations show our measures surpass those of other fair online techniques previously reported in the literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2206777360",
                    "name": "Zichong Wang"
                },
                {
                    "authorId": "51884035",
                    "name": "N. Saxena"
                },
                {
                    "authorId": "2206041831",
                    "name": "Tongjia Yu"
                },
                {
                    "authorId": "39222240",
                    "name": "Sneha Karki"
                },
                {
                    "authorId": "2206151224",
                    "name": "Tyler Zetty"
                },
                {
                    "authorId": "2323060",
                    "name": "I. Haque"
                },
                {
                    "authorId": "2164739874",
                    "name": "Shanlin Zhou"
                },
                {
                    "authorId": "3313608",
                    "name": "Dukka B Kc"
                },
                {
                    "authorId": "17078039",
                    "name": "I. Stockwell"
                },
                {
                    "authorId": "2213119699",
                    "name": "Xuyu Wang"
                },
                {
                    "authorId": "1762931",
                    "name": "A. Bifet"
                },
                {
                    "authorId": "2027662138",
                    "name": "Wenbin Zhang"
                }
            ]
        }
    ]
}