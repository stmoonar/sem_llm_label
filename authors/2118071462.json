{
    "authorId": "2118071462",
    "papers": [
        {
            "paperId": "0855cbf4dd3289145f65d1a3c33ebb55139af03b",
            "title": "LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned Image-to-Animation",
            "abstract": "Revolutionary advancements in text-to-image models have unlocked new dimensions for sophisticated content creation, e.g., text-conditioned image editing, allowing us to edit the diverse images that convey highly complex visual concepts according to the textual guidance. Despite being promising, existing methods focus on texture- or non-rigid-based visual manipulation, which struggles to produce the fine-grained animation of smooth text-conditioned image morphing without fine-tuning, i.e., due to their highly unstructured latent space. In this paper, we introduce a tuning-free LLM-driven attention control framework, encapsulated by the progressive process of LLM planning, prompt-Aware editing, StablE animation geneRation, abbreviated as LASER. LASER employs a large language model (LLM) to refine coarse descriptions into detailed prompts, guiding pre-trained text-to-image models for subsequent image generation. We manipulate the model's spatial features and self-attention mechanisms to maintain animation integrity and enable seamless morphing directly from text prompts, eliminating the need for additional fine-tuning or annotations. Our meticulous control over spatial features and self-attention ensures structural consistency in the images. This paper presents a novel framework integrating LLMs with text-to-image models to create high-quality animations from a single text input. We also propose a Text-conditioned Image-to-Animation Benchmark to validate the effectiveness and efficacy of LASER. Extensive experiments demonstrate that LASER produces impressive, consistent, and efficient results in animation generation, positioning it as a powerful tool for advanced digital content creation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2297809282",
                    "name": "Haoyu Zheng"
                },
                {
                    "authorId": "2108125912",
                    "name": "Wenqiao Zhang"
                },
                {
                    "authorId": "2257134292",
                    "name": "Yaoke Wang"
                },
                {
                    "authorId": "2267730291",
                    "name": "Hao Zhou"
                },
                {
                    "authorId": "2292279007",
                    "name": "Jiang Liu"
                },
                {
                    "authorId": "2261788275",
                    "name": "Juncheng Li"
                },
                {
                    "authorId": "2267726979",
                    "name": "Zheqi Lv"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                }
            ]
        },
        {
            "paperId": "22ffbc1864b2ed2d848c54619416a741b144e067",
            "title": "Auto-Encoding Morph-Tokens for Multimodal LLM",
            "abstract": "For multimodal LLMs, the synergy of visual comprehension (textual output) and generation (visual output) presents an ongoing challenge. This is due to a conflicting objective: for comprehension, an MLLM needs to abstract the visuals; for generation, it needs to preserve the visuals as much as possible. Thus, the objective is a dilemma for visual-tokens. To resolve the conflict, we propose encoding images into morph-tokens to serve a dual purpose: for comprehension, they act as visual prompts instructing MLLM to generate texts; for generation, they take on a different, non-conflicting role as complete visual-tokens for image reconstruction, where the missing visual cues are recovered by the MLLM. Extensive experiments show that morph-tokens can achieve a new SOTA for multimodal comprehension and generation simultaneously. Our project is available at https://github.com/DCDmllm/MorphTokens.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212175601",
                    "name": "Kaihang Pan"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2261788275",
                    "name": "Juncheng Li"
                },
                {
                    "authorId": "2300334499",
                    "name": "Zhaoyu Fan"
                },
                {
                    "authorId": "2299941238",
                    "name": "Wei Chow"
                },
                {
                    "authorId": "2300336453",
                    "name": "Shuicheng Yan"
                },
                {
                    "authorId": "2281747744",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                },
                {
                    "authorId": "2257264007",
                    "name": "Hanwang Zhang"
                }
            ]
        },
        {
            "paperId": "237bfa636f1a575f4784d2ae81a47ac29fa38522",
            "title": "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning",
            "abstract": "Large Language Models (LLMs) demonstrate remarkable proficiency in comprehending and handling text-based tasks. Many efforts are being made to transfer these attributes to video modality, which are termed Video-LLMs. However, existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments. In light of these challenges, we propose Momentor, a Video-LLM capable of accomplishing fine-grained temporal understanding tasks. To support the training of Momentor, we design an automatic data generation engine to construct Moment-10M, a large-scale video instruction dataset with segment-level instruction data. We train Momentor on Moment-10M, enabling it to perform segment-level reasoning and localization. Zero-shot evaluations on several tasks demonstrate that Momentor excels in fine-grained temporally grounded comprehension and localization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159713431",
                    "name": "Long Qian"
                },
                {
                    "authorId": "2249538542",
                    "name": "Juncheng Li"
                },
                {
                    "authorId": "2279975706",
                    "name": "Yu Wu"
                },
                {
                    "authorId": "2284935143",
                    "name": "Yaobo Ye"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2281747744",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                }
            ]
        },
        {
            "paperId": "5df1d39d1bdf0129250f1858f9d91acf66f08b88",
            "title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization",
            "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. With the advent of large language models (LLMs), shows their impressive capability of textual understanding through large-scale pretraining, which implies the great potential of extractive snippet generation. In this paper, we systematically investigated two indispensable characteristics that the LLMs-based QFS models should be harnessed, Lengthy Document Summarization and Efficiently Fine-grained Query-LLM Alignment, respectively. Correspondingly, we propose two modules called Query-aware HyperExpert and Query-focused Infini-attention to access the aforementioned characteristics. These innovations pave the way for broader application and accessibility in the field of QFS technology. Extensive experiments conducted on existing QFS benchmarks indicate the effectiveness and generalizability of the proposed approach. Our code is publicly available at https://github.com/DCDmllm/IDEAL_Summary.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109811486",
                    "name": "Jie Cao"
                },
                {
                    "authorId": "2302798653",
                    "name": "Dian Jiao"
                },
                {
                    "authorId": "2311684610",
                    "name": "Qiang Yan"
                },
                {
                    "authorId": "2108125912",
                    "name": "Wenqiao Zhang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                }
            ]
        },
        {
            "paperId": "69239ee34b04c5503774ffc0235bc1b239cca4fa",
            "title": "Data Shunt: Collaboration of Small and Large Models for Lower Costs and Better Performance",
            "abstract": "Pretrained large models, particularly large language models, have garnered increasing attention, as they have demonstrated remarkable abilities through contextual learning. Pretrained large models are increasingly recognized as fundamental tools for solving various tasks. However, the substantial computational demands of large models have dissuaded most product teams and individuals from running them. In such scenarios, to leverage the exceptional performance of large models, one must solely depend on costly APIs, further burdening product teams and individuals. On the other hand, despite the overall inferior performance of small models compared to large models, there are certain distributions where small models can achieve comparable or even superior results. For instance, during training, small models may become trapped in a local optimum that is unique to certain distributions, leading to superior performance. Hence, we propose Data Shunt (DS), a general paradigm for collaboration of small and large models. DS not only substantially reduces the cost associated with deploying large models but also effectively enhances overall performance. Specifically, DS determines the shunting direction by evaluating the confidence level of small models. When the confidence level falls below a specific threshold, the input data is forwarded to large models. To further leverage the advantages of the small and large models, we introduce Prompt Pruning (PP) and 2-Stage Confidence Distillation (2CD), which facilitate mutual collaboration, leading to better results and less cost. \nThe remarkable performance across diverse modalities and tasks demonstrates the superiority of the proposed DS over large models. For instance, ChatGPT achieves an accuracy of 94.43% on Amazon Product sentiment analysis, and DS achieves an accuracy of 95.64%, while the cost has been reduced to only 31.18%. The code for the proposed method are provided for research purposes https://github.com/Anfeather/Data-Shunt.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2253878180",
                    "name": "Dong Chen"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                },
                {
                    "authorId": "2293553363",
                    "name": "Shuo Zhang"
                },
                {
                    "authorId": "2293551690",
                    "name": "Jinfeng Liu"
                },
                {
                    "authorId": "2293739122",
                    "name": "Su Dong"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                }
            ]
        },
        {
            "paperId": "be225532eaa857743a7aea38de99e90a1cb4c528",
            "title": "Logic Distillation: Learning from Code Function by Function for Planning and Decision-making",
            "abstract": "Large language models (LLMs) have garnered increasing attention owing to their powerful logical reasoning capabilities. Generally, larger LLMs (L-LLMs) that require paid interfaces exhibit significantly superior performance compared to smaller LLMs (S-LLMs) that can be deployed on a variety of devices. Knowledge distillation (KD) aims to empower S-LLMs with the capabilities of L-LLMs, while S-LLMs merely mimic the outputs of L-LLMs, failing to get the powerful logical reasoning capabilities. Consequently, S-LLMs are helpless when it comes to planning and decision-making tasks that require logical reasoning capabilities. To tackle the identified challenges, we propose a novel framework called Logic Distillation (LD). Initially, LD employs L-LLMs to instantiate complex instructions into discrete functions and illustrates their usage to establish a function base. Subsequently, based on the function base, LD fine-tunes S-LLMs to learn the logic employed by L-LLMs in planning and decision-making. During testing, LD utilizes a retriever to identify the top-$K$ relevant functions based on instructions and current states, which will be selected and invoked by S-LLMs. Ultimately, S-LLMs yield planning and decision-making outcomes, function by function. Relevant experiments demonstrate that with the assistance of LD, S-LLMs can achieve outstanding results in planning and decision-making tasks, comparable to, or even surpassing, those of L-LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2253878180",
                    "name": "Dong Chen"
                },
                {
                    "authorId": "2313645573",
                    "name": "Shilin Zhang"
                },
                {
                    "authorId": "2313910365",
                    "name": "Fei Gao"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2277988909",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2299586680",
                    "name": "Mingliang Xu"
                }
            ]
        },
        {
            "paperId": "c150af9681789f4d3b58178eb5e343916a607756",
            "title": "RustGraph: Robust Anomaly Detection in Dynamic Graphs by Jointly Learning Structural-Temporal Dependency",
            "abstract": "Dynamic graph-based data are ubiquitous in the real world, such as social networks, finance systems, and traffic flow. Fast and accurately detecting anomalies in these dynamic graphs is of vital importance. However, despite promising results the current anomaly detection methods have achieved, there are two major limitations when coping with dynamic graphs. The first limitation is that the topological structures and the temporal dynamics have been modeled separately, resulting in less expressive features for detection. The second limitation is that the models have been trained by unreliable noisy labels generated by random negative sampling, rendering it severely vulnerable to subtle perturbations. To overcome the above limitations, we propose RustGraph, a robust anomaly detection framework by jointly learning structural-temporal dependency in dynamic graphs. To this end, we design a variational graph auto-encoder with informative prior that simultaneously encodes both graph structural and temporal information. Then we introduce a fine-grained contrastive learning method to learn better node representations by utilizing the temporal consistency between two snapshots. Furthermore, we formulate the noisy label learning problem for anomaly detection in dynamic graph, and then propose a robust anomaly detector to improve the model performance by leveraging learned graph structure signal. Our extensive experiments on six real-world datasets demonstrate the proposed RustGraph method achieves state-of-the-art performance with an average of 3.64% improvement on AUC-ROC metric compared with all baselines. The codes are available at https://github.com/aubreygjh/RustGraph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157893400",
                    "name": "Jianhao Guo"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2261788275",
                    "name": "Juncheng Li"
                },
                {
                    "authorId": "2212175601",
                    "name": "Kaihang Pan"
                },
                {
                    "authorId": "2263988064",
                    "name": "Lingfei Wu"
                }
            ]
        },
        {
            "paperId": "c311a89de3f869ec97973ad68c7a267cbfd23971",
            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
            "abstract": "Retrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks. However, contemporary RAG approaches suffer from irrelevant knowledge retrieval issues in complex domain questions (e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to low-quality generations. To address this issue, we propose a novel Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our bootstrapping philosophy is to simultaneously integrate the domain fintuning and RAG models to improve the knowledge retrieval quality, thereby enhancing generation quality. Finally, we demonstrate DuetRAG' s matches with expert human researchers on HotPot QA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2302798653",
                    "name": "Dian Jiao"
                },
                {
                    "authorId": "2303434387",
                    "name": "Li Cai"
                },
                {
                    "authorId": "2303044665",
                    "name": "Jingsheng Huang"
                },
                {
                    "authorId": "2108125912",
                    "name": "Wenqiao Zhang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                }
            ]
        },
        {
            "paperId": "e5c1e15e24708d169ef30c28d18a6d5cf3bd0836",
            "title": "WorldGPT: Empowering LLM as Multimodal World Model",
            "abstract": "World models are progressively being employed across diverse fields, extending from basic environment simulation to complex scenario construction. However, existing models are mainly trained on domain-specific states and actions, and confined to single-modality state representations. In this paper, We introduce WorldGPT, a generalist world model built upon Multimodal Large Language Model (MLLM). WorldGPT acquires an understanding of world dynamics through analyzing millions of videos across various domains. To further enhance WorldGPT's capability in specialized scenarios and long-term tasks, we have integrated it with a novel cognitive architecture that combines memory offloading, knowledge retrieval, and context reflection. As for evaluation, we build WorldNet, a multimodal state transition prediction benchmark encompassing varied real-life scenarios. Conducting evaluations on WorldNet directly demonstrates WorldGPT's capability to accurately model state transition patterns, affirming its effectiveness in understanding and predicting the dynamics of complex scenarios. We further explore WorldGPT's emerging potential in serving as a world simulator, helping multimodal agents generalize to unfamiliar domains through efficiently synthesising multimodal instruction instances which are proved to be as reliable as authentic data for fine-tuning purposes. The project is available on \\url{https://github.com/DCDmllm/WorldGPT}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49931194",
                    "name": "Zhiqi Ge"
                },
                {
                    "authorId": "2298945815",
                    "name": "Hongzhe Huang"
                },
                {
                    "authorId": "2298947834",
                    "name": "Mingze Zhou"
                },
                {
                    "authorId": "2261788275",
                    "name": "Juncheng Li"
                },
                {
                    "authorId": "2195413103",
                    "name": "Guoming Wang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                }
            ]
        },
        {
            "paperId": "e8bb2c1a0043ba4e16e08a942507fa11518ae3ee",
            "title": "Improving Large Models with Small models: Lower Costs and Better Performance",
            "abstract": "Pretrained large models (PLMs), such as ChatGPT, have demonstrated remarkable performance across diverse tasks. However, the significant computational requirements of PLMs have discouraged most product teams from running or fine-tuning them. In such cases, to harness the exceptional performance of PLMs, one must rely on expensive APIs, thereby exacerbating the economic burden. Despite the overall inferior performance of small models, in specific distributions, they can achieve comparable or even superior results. Consequently, some input can be processed exclusively by small models. On the other hand, certain tasks can be broken down into multiple subtasks, some of which can be completed without powerful capabilities. Under these circumstances, small models can handle the simple subtasks, allowing large models to focus on challenging subtasks, thus improving the performance. We propose Data Shunt$^+$ (DS$^+$), a general paradigm for collaboration of small and large models. DS$^+$ not only substantially reduces the cost associated with querying large models but also effectively improves large models' performance. For instance, ChatGPT achieves an accuracy of $94.43\\%$ on Amazon Product sentiment analysis, and DS$^+$ achieves an accuracy of $95.64\\%$, while the cost has been reduced to only $31.18\\%$. Besides, experiments also prove that the proposed collaborative-based paradigm can better inject specific task knowledge into PLMs compared to fine-tuning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2253878180",
                    "name": "Dong Chen"
                },
                {
                    "authorId": "2293553363",
                    "name": "Shuo Zhang"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2277988909",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2308069489",
                    "name": "Hua Wang"
                },
                {
                    "authorId": "2299586680",
                    "name": "Mingliang Xu"
                }
            ]
        }
    ]
}