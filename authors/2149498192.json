{
    "authorId": "2149498192",
    "papers": [
        {
            "paperId": "75ba0b92bcf095e7cd1544425f1818fed195f83f",
            "title": "KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge",
            "abstract": "Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149498192",
                    "name": "Pengcheng Jiang"
                },
                {
                    "authorId": "2283520647",
                    "name": "Lang Cao"
                },
                {
                    "authorId": "2273321244",
                    "name": "Cao Xiao"
                },
                {
                    "authorId": "2303385525",
                    "name": "Parminder Bhatia"
                },
                {
                    "authorId": "2249887353",
                    "name": "Jimeng Sun"
                },
                {
                    "authorId": "2284641156",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "79030ade9b06efac725626c1d0f097ba776dd081",
            "title": "TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale",
            "abstract": "The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs\u2019 text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summarization rationale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149498192",
                    "name": "Pengcheng Jiang"
                },
                {
                    "authorId": "2273321244",
                    "name": "Cao Xiao"
                },
                {
                    "authorId": "2108733162",
                    "name": "Zifeng Wang"
                },
                {
                    "authorId": "50339091",
                    "name": "Parminder Bhatia"
                },
                {
                    "authorId": "2253820975",
                    "name": "Jimeng Sun"
                },
                {
                    "authorId": "2284641156",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "e12aec7435bd348ec4357faae7d7ccb8d83d954d",
            "title": "GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models",
            "abstract": "The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities can cause hallucinations. Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent with human preferences for RE quality. Last, we made a comprehensive evaluation of fourteen leading LLMs using GenRES across document, bag, and sentence level RE datasets, respectively, to set the benchmark for future research in GRE",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149498192",
                    "name": "Pengcheng Jiang"
                },
                {
                    "authorId": "2284615476",
                    "name": "Jiacheng Lin"
                },
                {
                    "authorId": "2108733162",
                    "name": "Zifeng Wang"
                },
                {
                    "authorId": "2253820975",
                    "name": "Jimeng Sun"
                },
                {
                    "authorId": "2284641156",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "09a0143f23ec20aae2a8863437de92e973440ecc",
            "title": "Gode - Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network",
            "abstract": "The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Re-sults from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149498192",
                    "name": "Pengcheng Jiang"
                }
            ]
        },
        {
            "paperId": "2a2073aa908d7ed608bc7a6f1c76a3c65da4423b",
            "title": "GraphCare: Enhancing Healthcare Predictions with Open-World Personalized Knowledge Graphs",
            "abstract": "Clinical predictive models often rely on patients\u2019 electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are dif\ufb01cult to generate from patient EHR data. To address this, we propose GraphCare , an open-world framework that leverages external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to generate patient-speci\ufb01c KGs, which are then used to train our proposed Bi-attention Aug-menTed ( BAT ) graph neural network (GNN) for healthcare predictions. We evaluate GraphCare on two public datasets: MIMIC-III and MIMIC-IV. Our method outperforms baseline models in four vital healthcare prediction tasks: mortality, readmission, length-of-stay, and drug recommendation, improving AUROC on MIMIC-III by average margins of 10.4%, 3.8%, 2.0%, and 1.5%, respectively. Notably, GraphCare demonstrates a substantial edge in scenarios with limited data availability. Our \ufb01ndings highlight the potential of using external KGs in health-care prediction tasks and demonstrate the promise of GraphCare in generating personalized KGs for promoting personalized medicine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149498192",
                    "name": "Pengcheng Jiang"
                },
                {
                    "authorId": "145781464",
                    "name": "Cao Xiao"
                },
                {
                    "authorId": "2179107330",
                    "name": "Adam Cross"
                },
                {
                    "authorId": "49991208",
                    "name": "Jimeng Sun"
                }
            ]
        },
        {
            "paperId": "81001bd9f3753012815af4c7279d35a12432aa2f",
            "title": "Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models",
            "abstract": "The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TAGREAL that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TAGREAL achieves state-of-the-art performance on two benchmark datasets. We find that TAGREAL has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149498192",
                    "name": "Pengcheng Jiang"
                },
                {
                    "authorId": "1923351",
                    "name": "Shivam Agarwal"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2154990549",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "1738536",
                    "name": "Jimeng Sun"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "89d33d534fd94dc7bf0b4ecee11c8068c24ed6c5",
            "title": "Bi-level Contrastive Learning for Knowledge-Enhanced Molecule Representations",
            "abstract": "Molecule representation learning is crucial for various downstream applications, such as understanding and predicting molecular properties and side effects. In this paper, we propose a novel method called GODE, which takes into account the two-level structure of individual molecules. We recognize that molecules have an intrinsic graph structure as well as being a node in a larger molecule knowledge graph. GODE integrates graph representations of individual molecules with multidomain biochemical data from knowledge graphs. By pre-training two graph neural networks (GNNs) on different graph structures, combined with contrastive learning, GODE fuses molecular structures with their corresponding knowledge graph substructures. This fusion results in a more robust and informative representation, which enhances molecular property prediction by harnessing both chemical and biological information. When fine-tuned across 11 chemical property tasks, our model outperforms existing benchmarks, registering an average ROC-AUC uplift of 13.8% for classification tasks and an average RMSE/MAE enhancement of 35.1% for regression tasks. Impressively, it surpasses the current leading model in molecule property predictions with average advancements of 2.1% in classification and 6.4% in regression tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2149498192",
                    "name": "Pengcheng Jiang"
                },
                {
                    "authorId": "145781464",
                    "name": "Cao Xiao"
                },
                {
                    "authorId": "2427076",
                    "name": "Tianfan Fu"
                },
                {
                    "authorId": "1738536",
                    "name": "Jimeng Sun"
                }
            ]
        },
        {
            "paperId": "9b3f3f647cc5d6b7b7f6b2ab9849e1285351b474",
            "title": "GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
            "abstract": "Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose \\textsc{GraphCare}, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, \\textsc{GraphCare} surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\\% and 6.6\\% for mortality and readmission, and F1-score by 7.9\\% and 10.8\\% for LOS and drug recommendation, respectively. Notably, \\textsc{GraphCare} demonstrates a substantial edge in scenarios with limited data availability. Our findings highlight the potential of using external KGs in healthcare prediction tasks and demonstrate the promise of \\textsc{GraphCare} in generating personalized KGs for promoting personalized medicine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149498192",
                    "name": "Pengcheng Jiang"
                },
                {
                    "authorId": "145781464",
                    "name": "Cao Xiao"
                },
                {
                    "authorId": "2179107330",
                    "name": "Adam Cross"
                },
                {
                    "authorId": "2249887353",
                    "name": "Jimeng Sun"
                }
            ]
        },
        {
            "paperId": "91418983590db4ed9f3ae5796c16a9c7a42d5ba6",
            "title": "OACAL: Finding Module-consistent Specifications to Secure Systems from Weakened User Obligations",
            "abstract": "Users interacting with a system through UI are typically obliged to perform their actions in a pre-determined order, to successfully achieve certain functional goals. However, such obligations are often not followed strictly by users, which may lead to the violation to security properties, especially in security-critical systems. To improve the security with the awareness of unexpected user behaviors, a system can be redesigned to a more robust one by changing the order of actions in its specification. Meanwhile, we anticipate that the functionalities would remain consistent following the modifications. In this paper, we propose an efficient algorithm to automatically produce specification revisions tackling the attack scenarios caused by weakened user obligations. By our algorithm, all the revisions would be generated to maintain the integrity of the functionalities using a novel recomposition approach. Then, the eligible revisions that can satisfy the security requirements would be efficiently spotted by a hybrid approach combining model checking and machine learning techniques. We evaluate our algorithm by comparing its performance with a state-of-the-art approach regarding their coverage and searching speed of the desirable revisions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149498192",
                    "name": "Pengcheng Jiang"
                },
                {
                    "authorId": "2267460",
                    "name": "K. Tei"
                }
            ]
        }
    ]
}