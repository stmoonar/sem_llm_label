{
    "authorId": "48515097",
    "papers": [
        {
            "paperId": "0557067ac9a61c7cfbfe539f20b82236df5be2f1",
            "title": "Probing into the Fairness of Large Language Models: A Case Study of ChatGPT",
            "abstract": "Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited number of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT\u2019s performance in high-takes fields including education, criminology, finance and healthcare. To conduct a thorough evaluation, we consider both group fairness and individual fairness metrics. We also observe the disparities in ChatGPT\u2019s outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs\u2019 fairness performance, facilitates bias mitigation and fosters the development of responsible AI systems. Code and data are open-sourced on GitHub 1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "2294817498",
                    "name": "Lanjing Zhang"
                },
                {
                    "authorId": "2294814971",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "d8a54917133e34922e6fa1eca60cd11ab29d7b1b",
            "title": "Fairness in Survival Outcome Prediction for Medical Treatments",
            "abstract": "Cancer is a life-threatening disease and affects millions of people globally. Predicting the survival outcome of patients after receiving a cancer diagnosis is of great importance for personalized treatment planning. While predictive machine learning models have shown promising results in predicting survival outcomes, they may result in unfair treatments against certain groups, such as those based on age, gender, race and socioeconomic status. In this paper, we analyze and identify what kind of fairness criterion should be developed in cancer survival predictions. Considering the high stakes involved, we believe the basis for developing fairness in this field should be assisting to improve the model performance of diverse patients, rather than hurting the performance of certain patients in exchange for better performance of other patients. We suggest min-max fairness as an appropriate concept that seeks to minimize the group error of the worst-off group, instead of pursuing a strict equal error rate across various patient groups which would reduce the utility of the advantaged groups. We investigate the application of min-max fairness through active sampling in predicting the survival outcome of cancer patients using machine learning models, and evaluate the fairness-aware method on several publicly available cancer datasets. The experimental results demonstrate that the fairness-aware predictive method can help improve the model performance of the disadvantaged patient group without hurting the advantaged patient group, and thus increase the overall model performance for all patients.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "67228325",
                    "name": "H. Chen"
                },
                {
                    "authorId": "2294817498",
                    "name": "Lanjing Zhang"
                },
                {
                    "authorId": "2294814971",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "3e82d12dbce6becdcae94e3ebb01dd569bdb734f",
            "title": "Causal Inference for Recommendation: Foundations, Methods and Applications",
            "abstract": "Recommender systems are important and powerful tools for various personalized services. Traditionally, these systems use data mining and machine learning techniques to make recommendations based on correlations found in the data. However, relying solely on correlation without considering the underlying causal mechanism may lead to various practical issues such as fairness, explainability, robustness, bias, echo chamber and controllability problems. Therefore, researchers in related area have begun incorporating causality into recommendation systems to address these issues. In this survey, we review the existing literature on causal inference in recommender systems. We discuss the fundamental concepts of both recommender systems and causal inference as well as their relationship, and review the existing work on causal methods for different problems in recommender systems. Finally, we discuss open problems and future directions in the field of causal inference for recommendations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2111810606",
                    "name": "Jianchao Ji"
                },
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "d98d77857760a1150d55304f81387c0fee3a4d97",
            "title": "Transferable Fairness for Cold-Start Recommendation",
            "abstract": "With the increasing use and impact of recommender systems in our daily lives, how to achieve fairness in recommendation has become an important problem. Previous works on fairness-aware recommendation mainly focus on a predefined set of (usually warm-start) users. However, recommender systems often face more challenging fairness issues for new users or cold-start users due to their insuf-ficient amount of interactions. Therefore, it is essential to study whether the trained model still performs fairly for a new set of cold-start users. This paper considers the scenario where the recommender system meets new users who only have limited or even no interaction with the platform, and aims at providing high-quality and fair recommendations to such users effectively. The sufficient interaction data from warm users is treated as the source user domain, while the data from new users is treated as the target user domain, and we consider to transfer the counterfactual fairness from the source users to the target users. To this end, we introduce a framework to achieve transferable counterfactual fairness in recommendation. The proposed method is able to transfer the knowledge of a fair model learned from the source users to the target users with the hope of improving the recommendation performance and keeping the fairness property on the target users. Experiments on two real-world datasets with representative recommendation algorithms show that our method not only promotes fairness for the target users, but also outperforms comparative models in terms of recommendation performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "2506522",
                    "name": "Dingxian Wang"
                },
                {
                    "authorId": "67228325",
                    "name": "H. Chen"
                },
                {
                    "authorId": "1591136873",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "f6a1f8ca2f88e61c727bbdf7a77020dfacc399cd",
            "title": "Fairness of ChatGPT",
            "abstract": "Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited number of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To conduct a thorough evaluation, we consider both group fairness and individual fairness metrics. We also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible AI systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "2108026332",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "397542cddbac98639a8d9938ac63feff7a6eacd1",
            "title": "Fairness in Recommendation: Foundations, Methods, and Applications",
            "abstract": "As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision-making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This survey focuses on the foundations for fairness in recommendation literature. It first presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking to provide a general overview of fairness research, as well as introduce the more complex situations and challenges that need to be considered when studying fairness in recommender systems. After that, the survey will introduce fairness in recommendation with a focus on the taxonomies of current fairness definitions, the typical techniques for improving fairness, as well as the datasets for fairness studies in recommendation. The survey also talks about the challenges and opportunities in fairness research with the hope of promoting the fair recommendation research area and beyond.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "67228325",
                    "name": "H. Chen"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "4177661bbf202215be362c5e79dab45cb6cfaf49",
            "title": "Learn Basic Skills and Reuse: Modularized Adaptive Neural Architecture Search (MANAS)",
            "abstract": "Human intelligence is able to first learn some basic skills for solving basic problems and then assemble such basic skills into complex skills for solving complex or new problems. For example, the basic skills \"dig hole,'' \"put tree,'' \"backfill'' and \"watering'' compose a complex skill \"plant a tree''. Besides, some basic skills can be reused for solving other problems. For example, the basic skill \"dig hole'' not only can be used for planting a tree, but also can be used for mining treasures, building a drain, or landfilling. The ability to learn basic skills and reuse them for various tasks is very important for humans because it helps to avoid learning too many skills for solving each individual task, and makes it possible to solve a compositional number of tasks by learning just a few number of basic skills, which saves a considerable amount of memory and computational power in the human brain. We believe that machine intelligence should also capture the ability of learning basic skills and reusing them by composing into complex skills. In computer science language, each basic skill is a \"module'', which is a reusable network that has a concrete meaning and performs a concrete basic operation. The modules are assembled into a bigger \"model'' for doing a more complex task. The assembling procedure is adaptive to the input or task, i.e., for a given task, the modules should be assembled into the most suitable model for solving the given task. As a result, different inputs/tasks could have different assembled models. In this work, we take recommender system as an example and propose Modularized Adaptive Neural Architecture Search (MANAS) to demonstrate the above idea. Neural Architecture Search (NAS) has shown its power in discovering superior neural architectures. However, existing NAS mostly focus on searching for a global architecture regardless of the specific input, i.e., the architecture is not adaptive to the input. In this work, we borrow the idea from modularized neural logic reasoning and consider three basic logical operation modules: AND, OR, NOT. Meanwhile, making recommendations for each user is considered as a task. MANAS automatically assembles the logical operation modules into a network architecture tailored for the given user. As a result, a personalized neural architecture is assembled for each user to make recommendations for the user, which means that the resulting neural architecture is adaptive to the model's input (i.e., the user's past behaviors). Experiments on different datasets show that the adaptive architecture assembled by MANAS outperforms static global architectures. Further experiments and empirical analysis provide insights to the effectiveness of MANAS. The code is open-source at https://github.com/TalonCB/MANAS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67228325",
                    "name": "H. Chen"
                },
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "2117693954",
                    "name": "He Zhu"
                },
                {
                    "authorId": "1591136873",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "4f0925684db82985f9c48566065d4ead5e00a16b",
            "title": "Learning and Evaluating Graph Neural Network Explanations based on Counterfactual and Factual Reasoning",
            "abstract": "Structural data well exists in Web applications, such as social networks in social media, citation networks in academic websites, and threads data in online forums. Due to the complex topology, it is difficult to process and make use of the rich information within such data. Graph Neural Networks (GNNs) have shown great advantages on learning representations for structural data. However, the non-transparency of the deep learning models makes it non-trivial to explain and interpret the predictions made by GNNs. Meanwhile, it is also a big challenge to evaluate the GNN explanations, since in many cases, the ground-truth explanations are unavailable. In this paper, we take insights of Counterfactual and Factual (CF2) reasoning from causal inference theory, to solve both the learning and evaluation problems in explainable GNNs. For generating explanations, we propose a model-agnostic framework by formulating an optimization problem based on both of the two casual perspectives. This distinguishes CF2 from previous explainable GNNs that only consider one of them. Another contribution of the work is the evaluation of GNN explanations. For quantitatively evaluating the generated explanations without the requirement of ground-truth, we design metrics based on Counterfactual and Factual reasoning to evaluate the necessity and sufficiency of the explanations. Experiments show that no matter ground-truth explanations are available or not, CF2 generates better explanations than previous state-of-the-art methods on real-world datasets. Moreover, the statistic analysis justifies the correlation between the performance on ground-truth evaluation and our proposed metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "1947101",
                    "name": "Shijie Geng"
                },
                {
                    "authorId": "2011378",
                    "name": "Zuohui Fu"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "1591136873",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "633b93ecfe1b4fa5a80b8d71d952b81c25e8898d",
            "title": "Fairness in Recommendation: A Survey",
            "abstract": "Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This survey focuses on the foundations for fairness in recommendation literature. It first presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking in order to provide a general overview of fairness research, as well as introduce the more complex situations and challenges that need to be considered when studying fairness in recommender systems. After that, the survey will introduce fairness in recommendation with a focus on the taxonomies of current fairness definitions, the typical techniques for improving fairness, as well as the datasets for fairness studies in recommendation. The survey also talks about the challenges and opportunities in fairness research with the hope of promoting the fair recommendation research area and beyond.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "67228325",
                    "name": "H. Chen"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "1591136873",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "b20c87e2925a81f57d9c913423e9e74c14de5341",
            "title": "A Survey on Trustworthy Recommender Systems",
            "abstract": "Recommender systems (RS), serving at the forefront of Human-centered AI, are widely deployed in almost every corner of the web and facilitate the human decision-making process. However, despite their enormous capabilities and potential, RS may also lead to undesired effects on users, items, producers, platforms, or even the society at large, such as compromised user trust due to non-transparency, unfair treatment of different consumers, or producers, privacy concerns due to extensive use of user\u2019s private data for personalization, just to name a few. All of these create an urgent need for Trustworthy Recommender Systems (TRS) so as to mitigate or avoid such adverse impacts and risks. In this survey, we will introduce techniques related to trustworthy recommendation, including but not limited to explainable recommendation, fairness in recommendation, privacy-aware recommendation, robustness in recommendation, user-controllable recommendation, as well as the relationship between these different perspectives in terms of trustworthy recommendation. Through this survey, we hope to deliver readers with a comprehensive view of the research area and raise attention to the community about the importance, existing research achievements, and future research directions on trustworthy recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2011378",
                    "name": "Zuohui Fu"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "2109968285",
                    "name": "Zelong Li"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "2885287",
                    "name": "Yikun Xian"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        }
    ]
}