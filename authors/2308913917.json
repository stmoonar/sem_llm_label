{
    "authorId": "2308913917",
    "papers": [
        {
            "paperId": "1432133bf51be0bcd42676ca916143e321e101f0",
            "title": "DEJA VU: Continual Model Generalization For Unseen Domains",
            "abstract": "In real-world applications, deep learning models often run in non-stationary environments where the target data distribution continually shifts over time. There have been numerous domain adaptation (DA) methods in both online and offline modes to improve cross-domain adaptation ability. However, these DA methods typically only provide good performance after a long period of adaptation, and perform poorly on new domains before and during adaptation - in what we call the\"Unfamiliar Period\", especially when domain shifts happen suddenly and significantly. On the other hand, domain generalization (DG) methods have been proposed to improve the model generalization ability on unadapted domains. However, existing DG works are ineffective for continually changing domains due to severe catastrophic forgetting of learned knowledge. To overcome these limitations of DA and DG in handling the Unfamiliar Period during continual domain shift, we propose RaTP, a framework that focuses on improving models' target domain generalization (TDG) capability, while also achieving effective target domain adaptation (TDA) capability right after training on certain domains and forgetting alleviation (FA) capability on past domains. RaTP includes a training-free data augmentation module to prepare data for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for TDA, and a prototype contrastive alignment algorithm to align different domains for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and DomainNet demonstrate that RaTP significantly outperforms state-of-the-art works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG and Unified DA&DG in TDG, and achieves comparable TDA and FA capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308913917",
                    "name": "Chenxi Liu"
                },
                {
                    "authorId": "2108631414",
                    "name": "Lixu Wang"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2118831761",
                    "name": "Chen Sun"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2152206866",
                    "name": "Qi Zhu"
                }
            ]
        },
        {
            "paperId": "544cecd9102ea3cff07b033e695f55e343daffdd",
            "title": "Federated Continual Novel Class Learning",
            "abstract": "In a privacy-focused era, Federated Learning (FL) has emerged as a promising machine learning technique. However, most existing FL studies assume that the data distribution remains nearly fixed over time, while real-world scenarios often involve dynamic and continual changes. To equip FL systems with continual model evolution capabilities, we focus on an important problem called Federated Continual Novel Class Learning (FedCN) in this work. The biggest challenge in FedCN is to merge and align novel classes that are discovered and learned by different clients without compromising privacy. To address this, we propose a Global Alignment Learning (GAL) framework that can accurately estimate the global novel class number and provide effective guidance for local training from a global perspective, all while maintaining privacy protection. Specifically, GAL first locates high-density regions in the representation space through a bi-level clustering mechanism to estimate the novel class number, with which the global prototypes corresponding to novel classes can be constructed. Then, GAL uses a novel semantic weighted loss to capture all possible correlations between these prototypes and the training data for mitigating the impact of pseudo-label noise and data heterogeneity. Extensive experiments on various datasets demonstrate GAL's superior performance over state-of-the-art novel class discovery methods. In particular, GAL achieves significant improvements in novel-class performance, increasing the accuracy by 5.1% to 10.6% in the case of one novel class learning stage and by 7.8% to 17.9% in the case of two novel class learning stages, without sacrificing known-class performance. Moreover, GAL is shown to be effective in equipping a variety of different mainstream FL algorithms with novel class discovery and learning capability, highlighting its potential for many real-world applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108631414",
                    "name": "Lixu Wang"
                },
                {
                    "authorId": "2308913917",
                    "name": "Chenxi Liu"
                },
                {
                    "authorId": "2275765198",
                    "name": "Junfeng Guo"
                },
                {
                    "authorId": "2275767389",
                    "name": "Jiahua Dong"
                },
                {
                    "authorId": "2276121035",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2261394090",
                    "name": "Heng Huang"
                },
                {
                    "authorId": "2275773112",
                    "name": "Qi Zhu"
                }
            ]
        }
    ]
}