{
    "authorId": "144986814",
    "papers": [
        {
            "paperId": "5066fd66ed9c52b8ae9ed157e7849c1d8a22262b",
            "title": "Analyzing and explaining privacy risks on time series data: ongoing work and challenges",
            "abstract": "Currently, privacy risks assessment is mainly performed as audits conducted by data privacy analysts. In the TAILOR project, we promote a more systematic and automatic approach based on interpretable metrics and formal methods to evaluate privacy risks and to control the tension between data privacy and utility. In this paper, we focus on privacy risks raised by publishing time series datasets, and we survey the methods developed in TAILOR to analyze and quantify privacy risks depending on different publisher and attacker models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2313239994",
                    "name": "Tristan Allard"
                },
                {
                    "authorId": "3409084",
                    "name": "Hira Asghar"
                },
                {
                    "authorId": "1693350",
                    "name": "G. Avoine"
                },
                {
                    "authorId": "2313240259",
                    "name": "Christophe Bobineau"
                },
                {
                    "authorId": "90231593",
                    "name": "P. Cauchois"
                },
                {
                    "authorId": "2313243428",
                    "name": "E. Fromont"
                },
                {
                    "authorId": "2258258846",
                    "name": "Anna Monreale"
                },
                {
                    "authorId": "1998031158",
                    "name": "Francesca Naretto"
                },
                {
                    "authorId": "24269254",
                    "name": "Roberto Pellungrini"
                },
                {
                    "authorId": "33769943",
                    "name": "Francesca Pratesi"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1586585755",
                    "name": "Antonin Voyez"
                }
            ]
        },
        {
            "paperId": "2afc64dc92c2d3f740bd01e6ef075259d6201193",
            "title": "Sanity checks for patch visualisation in prototype-based image classification",
            "abstract": "In this work, we perform an analysis of the visualisation methods implemented in ProtoPNet and ProtoTree, two self-explaining visual classifiers based on prototypes. We show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour, which can create a false sense of bias in the model. We also demonstrate quantitatively that this issue can be mitigated by using other saliency methods that provide more faithful image patches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2173711438",
                    "name": "Romain Xu-Darme"
                },
                {
                    "authorId": "1693391",
                    "name": "G. Qu\u00e9not"
                },
                {
                    "authorId": "1945451",
                    "name": "Zakaria Chihani"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "a90a5fbafd7684c74c9c48963a7f7b1a775765d5",
            "title": "On the stability, correctness and plausibility of visual explanation methods based on feature importance",
            "abstract": "In the field of Explainable AI, multiples evaluation metrics have been proposed in order to assess the quality of explanation methods w.r.t. a set of desired properties. In this work, we study the articulation between the stability, correctness and plausibility of explanations based on feature importance for image classifiers. We show that the existing metrics for evaluating these properties do not always agree, raising the issue of what constitutes a good evaluation metric for explanations. Finally, in the particular case of stability and correctness, we show the possible limitations of some evaluation metrics and propose new ones that take into account the local behaviour of the model under test.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2173711438",
                    "name": "Romain Xu-Darme"
                },
                {
                    "authorId": "2267749587",
                    "name": "Jenny Benois-Pineau"
                },
                {
                    "authorId": "2267725712",
                    "name": "R. Giot"
                },
                {
                    "authorId": "2173711447",
                    "name": "Georges Qu'enot"
                },
                {
                    "authorId": "1945451",
                    "name": "Zakaria Chihani"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "2267727356",
                    "name": "Alexey Zhukov"
                }
            ]
        },
        {
            "paperId": "a9b1da32ec6f52ed2a21fb58d576fdd0861af196",
            "title": "Sanity checks and improvements for patch visualisation in prototype-based image classification",
            "abstract": "In this work, we perform an in-depth analysis of the visualisation methods implemented in two popular self-explaining models for visual classification based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets (CUB-200-2011 and Stanford Cars), we first show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour. Secondly, using a deletion metric, we demonstrate quantitatively that saliency methods such as Smoothgrads or PRP provide more faithful image patches. We also propose a new relevance metric based on the segmentation of the object provided in some datasets (e.g. CUB-200-2011) and show that the imprecise patch visualisations generated by ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated by the use of more faithful methods. Finally, we discuss the implications of our findings for other prototype-based models sharing the same visualisation method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2173711438",
                    "name": "Romain Xu-Darme"
                },
                {
                    "authorId": "2173711447",
                    "name": "Georges Qu'enot"
                },
                {
                    "authorId": "1945451",
                    "name": "Zakaria Chihani"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "616c81590b7f09c04c13b205c611c240e6941492",
            "title": "PARTICUL: Part Identification with Confidence measure using Unsupervised Learning",
            "abstract": "In this paper, we present PARTICUL, a novel algorithm for unsupervised learning of part detectors from datasets used in fine-grained recognition. It exploits the macro-similarities of all images in the training set in order to mine for recurring patterns in the feature space of a pre-trained convolutional neural network. We propose new objective functions enforcing the locality and unicity of the detected parts. Additionally, we embed our detectors with a confidence measure based on correlation scores, allowing the system to estimate the visibility of each part. We apply our method on two public fine-grained datasets (Caltech-UCSD Bird 200 and Stanford Cars) and show that our detectors can consistently highlight parts of the object while providing a good measure of the confidence in their prediction. We also demonstrate that these detectors can be directly used to build part-based fine-grained classifiers that provide a good compromise between the transparency of prototype-based approaches and the performance of non-interpretable methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2173711438",
                    "name": "Romain Xu-Darme"
                },
                {
                    "authorId": "2173711447",
                    "name": "Georges Qu'enot"
                },
                {
                    "authorId": "1945451",
                    "name": "Zakaria Chihani"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "6d597ad136e5bd978ebe9d4a7bd5e61faace4690",
            "title": "OntoSAMSEI: Interactive ontology engineering for supporting simulation-based training in Medicine",
            "abstract": "Simulation-based training is becoming a central need in medical education. So far, only a few pioneering teachers have developed and documented a pedagogical expertise for setting up training sessions based on simulation, making difficult to share it with less experienced teachers. In this paper, we describe an interactive and incremental ontology modeling approach in order to model such ill-defined domains related to pedagogy. We have built the OntoSAMSEI ontology for simulation-based medical education domain, and developed a new tool to automatically generate pre-filled forms in order to share the acquired knowledge among domain experts, and collect new information from them to enrich the ontology. We also report on the evaluation by domain experts of the completeness and the accuracy of the OntoSAMSEI ontology resulting from this incremental methodology supported by a pre-filled graphical user interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130058773",
                    "name": "Shadi Baghernezhad-Tabasi"
                },
                {
                    "authorId": "1483576501",
                    "name": "Loic Druette"
                },
                {
                    "authorId": "2683468",
                    "name": "F. Jouanot"
                },
                {
                    "authorId": "2130034739",
                    "name": "Celine Meurger"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "c617b8507bb5e6698f6c0e15b197de171890a5c3",
            "title": "IOPE: Interactive Ontology Population and Enrichment",
            "abstract": ". We demonstrate IOPE , a framework for the automatic construction of a Graphical User Interface (GUI) from the ontological constraints of an input ontology. The resulting GUI functions as a guidance for the experts with no knowledge of OWL/RDFS which enables them to easily explore and update their ontologies. The GUI also helps the experts to easily curate their own knowledge, and share it with peers. We illustrate the functionality of IOPE on an ontology for simulation-based medical workshops called OntoSAMSEI , and demonstrate two key scenarios on the GUI: semester planning , where an instructor in Medicine department uses the GUI to populate the ontology with information about target groups for di\ufb00erent training workshops, and robotic surgery evaluation , where a laboratory director employs the GUI to enrich the ontology with new assessment techniques to evaluate surgical skills.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130058773",
                    "name": "Shadi Baghernezhad-Tabasi"
                },
                {
                    "authorId": "1483576501",
                    "name": "Loic Druette"
                },
                {
                    "authorId": "2683468",
                    "name": "F. Jouanot"
                },
                {
                    "authorId": "2130034739",
                    "name": "Celine Meurger"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "6d5582208b4dee5c8cdedddb74a5e543dae0b442",
            "title": "Ontology-based Learning Analytics in Medicine",
            "abstract": "Through SIDES 3.0, we are developing an ontology-based e-learning platform in medicine to make learning analytics transparent and explainable to end-users (learners and teachers). In this project, the educational content, the traces of students' activities and the correction of exams are linked and related to items of an official reference program in a unified data model. As a result, an integrated access to useful information is provided for student progress monitoring and equipped with a powerful query language allowing users to express their specific needs relating to data exploration and analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2683468",
                    "name": "F. Jouanot"
                },
                {
                    "authorId": "1724968",
                    "name": "O. Palombi"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "9f6b732727a2f5e708350919b91b2db58cb90208",
            "title": "Workshop Chairs' Welcome",
            "abstract": "It is our great pleasure to welcome you to the WWW 2018 Workshops. This year's workshops of WWW 2018 feature a number of co-located workshops that are intended to provide a forum for researchers and practitioners in Web technologies to discuss and exchange positions on current and emergent Web topics. We received forty proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty selecting eighteen full-day workshops and ten half-day workshops. We also took into account the coverage of the different areas related to WWW as well as the potential audience, to schedule them in two consecutive days with the minimal audience interest overlap.",
            "fieldsOfStudy": [
                "Sociology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403840199",
                    "name": "Eyhab Al-Masri"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "1a9b59a9dde5059f7833ed6a26425d52ee62de36",
            "title": "Ontology-Mediated Queries for NOSQL Databases",
            "abstract": "\n \n Ontology-Based Data Access has been studied so far for relational structures and deployed on top of relational databases. This paradigm enables a uniform access to heterogeneous data sources, also coping with incomplete information. Whether OBDA is suitable also for non-relational structures, like those shared by increasingly popular NOSQL languages, is still an open question. In this paper, we study the problem of answering ontology-mediated queries on top of key-value stores. We formalize the data model and core queries of these systems, and introduce a rule language to express lightweight ontologies on top of data. We study the decidability and data complexity of query answering in this setting.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2506185",
                    "name": "M. Mugnier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "2871294",
                    "name": "F. Ulliana"
                }
            ]
        },
        {
            "paperId": "d96093add0207bb866c72007a225f189d6aa7c1f",
            "title": "Uncertainty-Sensitive Reasoning for Inferring sameAs Facts in Linked Data",
            "abstract": "Discovering whether or not two URIs described in Linked Data -- in the same or different RDF datasets -- refer to the same real-world entity is crucial for building applications that exploit the cross-referencing of open data. A major challenge in data interlinking is to design tools that effectively deal with incomplete and noisy data, and exploit uncertain knowledge. In this paper, we model data interlinking as a reasoning problem with uncertainty. We introduce a probabilistic framework for modelling and reasoning over uncertain RDF facts and rules that is based on the semantics of probabilistic Datalog. We have designed an algorithm, ProbFR, based on this framework. Experiments on real-world datasets have shown the usefulness and effectiveness of our approach for data linkage and disambiguation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403913920",
                    "name": "Mustafa Al-Bakri"
                },
                {
                    "authorId": "2124409",
                    "name": "Manuel Atencia"
                },
                {
                    "authorId": "2066561507",
                    "name": "J\u00e9r\u00f4me David"
                },
                {
                    "authorId": "2095176046",
                    "name": "Steffen Lalande"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "36b3361e14ec0d06fc931c1ba80dc29cef3f307f",
            "title": "Extracting Bounded-Level Modules from Deductive RDF Triplestores",
            "abstract": "\n \n We present a novel semantics for extracting bounded-level modules from RDF ontologies and databases augmented with safe inference rules, a la Datalog. Dealing with a recursive rule language poses challenging issues for defining the module semantics, and also makes module extraction algorithmically unsolvable in some cases. Our results include a set of module extraction algorithms compliant with the novel semantics. Experimental results show that the resulting framework is effective in extracting expressive modules from RDF datasets with formal guarantees, whilst controlling their succinctness.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "2871294",
                    "name": "F. Ulliana"
                }
            ]
        },
        {
            "paperId": "b2c13900454dbf0fd5d7879729cec1df6a912d19",
            "title": "Inferring Same-As Facts from Linked Data: An Iterative Import-by-Query Approach",
            "abstract": "\n \n In this paper we model the problem of data linkage in Linked Data as a reasoning problem on possibly decentralized data. We describe a novel import-by-query algorithm that alternates steps of sub-query rewriting and of tailored querying the Linked Data cloud in order to import data as specific as possible for inferring or contradicting given target same-as facts. Experiments conducted on a real-world dataset have demonstrated the feasibility of this approach and its usefulness in practice for data linkage and disambiguation.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403913920",
                    "name": "Mustafa Al-Bakri"
                },
                {
                    "authorId": "2124409",
                    "name": "Manuel Atencia"
                },
                {
                    "authorId": "2095176046",
                    "name": "Steffen Lalande"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "2f27039f2349bf5d5d49950385bfe94e770612c1",
            "title": "Combining 3D Models and Functions through Ontologies to Describe Man-made Products and Virtual Humans: Toward a Common Framework",
            "abstract": "Products and virtual humans are commonly described using 3D models of their components and anatomical entities, respectively. Functions stand for symbolic information bringing a high level description of product subsets and apply also to human body anatomical entities. Ontology-based approaches bring new means to improve the efficiency of these digital models. Here, the purpose is to identify similarities and differences between existing ontology based descriptions of products and virtual humans. From this analysis, it is shown how compatible descriptions can be obtained and how a common framework can be derived where products and virtual humans can be both incorporated for various applications involving 3D models. The proposed framework contains an ontology-based description of products and virtual humans enabling the access to 3D models while accessing high-level semantic information through the use of inference mechanisms. Biomechanical simulations of virtual humans and structural behavior simulations of products is the context used to setup this common framework such that products and virtual humans can take part to these simulations where products and virtual humans are involved.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2871294",
                    "name": "F. Ulliana"
                },
                {
                    "authorId": "144735282",
                    "name": "J. L\u00e9on"
                },
                {
                    "authorId": "1724968",
                    "name": "O. Palombi"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "145124560",
                    "name": "F. Faure"
                }
            ]
        },
        {
            "paperId": "e9da636d2ae63cfda98291c11e9862bf50957c59",
            "title": "Semantic Filtering of Scientific Articles guided by a Domain Ontology",
            "abstract": "The problem that we address in this paper is how to improve the accuracy of retrieving specialized information within a textual scientific corpus. We present a new approach in which the keywords expressing the bibliographical needs of a researcher are related to a domain ontology. We illustrate how such a declarative ontolology-based approach can be used both for computing varied statistics, and also for helping experts to find useful fine-grained information within a textual corpus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2683468",
                    "name": "F. Jouanot"
                },
                {
                    "authorId": "2026267633",
                    "name": "C. Labb\u00e9"
                },
                {
                    "authorId": "144368193",
                    "name": "E. Michael"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "153098464",
                    "name": "M. Tauber"
                },
                {
                    "authorId": "2871294",
                    "name": "F. Ulliana"
                }
            ]
        },
        {
            "paperId": "fbf19529e7e48cd48151550efebb445559be74d6",
            "title": "SOCLE: Towards a framework for data preparation in social applications",
            "abstract": "The web has evolved from a technological platform to a real social milieu thereby be- coming a continuous source of Big Social Data (BSD). BSD is characterized by a combination of factual content such as the coordinates of a restaurant, the content of a webpage or the title of a movie, behavior data such as exchanges between social relationships, as well as subjective data such as users' opinions, reviews, and tags. The goal of a social application is to analyze BSD and process it in order to understand it and transform it into valuable content to users. Building social applications requires an essential data preparation step during which raw BSD is sanitized, normalized, enriched, pruned, and transformed making it readily available for fur- ther processing. We argue for the need to formalize data preparation and develop appropriate tools to enable easy prototyping of social applications. We describe SOCLE, our framework for BSD preparation. We provide an architecture inspired from typical social applications, the state of the art of existing languages and algebras for manipulating BSD, and the scientific challenges and opportunities underlying the development of SOCLE. RESUME.Le web, initialement une plateforme technologique, est devenu un veritable eco-systeme socialetunesourcecontinuedeBigDatasociales(BSD).Nousytrouvonsdesdonneesfactuelles et objectives telles que les coordonnees d'un restaurant, des donnees comportementales telles que les echanges entre amis et enfin, des donnees subjectives comme les revues, les etiquettes, les notes ou les etoiles. Ces donnees brutes ont besoin d'etre filtrees et organisees pour en extraire des informations utiles et permettre le developpement d'applications sociales qui ap- portent une valeur ajoutee aux utilisateurs. Dans cet article, nous motivons le besoin de for- maliser l'etape de preparation des BSD et decrivons SOCLE, notre plateforme pour le faire. Nous presentons une architecture inspiree d'applications sociales types, un etat de l'art des langages et algebres existants et les defis scientifiques du developpement de SOCLE.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2052763654",
                    "name": "Noha Ibrahim"
                },
                {
                    "authorId": "2309554",
                    "name": "C. Kengne"
                },
                {
                    "authorId": "2871294",
                    "name": "F. Ulliana"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "97565646cc34540f0a45a64f71b76db583d12a56",
            "title": "Robust Module-Based Data Management",
            "abstract": "The current trend for building an ontology-based data management system (DMS) is to capitalize on efforts made to design a preexisting well-established DMS (a reference system). The method amounts to extracting from the reference DMS a piece of schema relevant to the new application needs-a module-, possibly personalizing it with extra constraints w.r.t. the application under construction, and then managing a data set using the resulting schema. In this paper, we extend the existing definitions of modules and we introduce novel properties of robustness that provide means for checking easily that a robust module-based DMS evolves safely w.r.t. both the schema and the data of the reference DMS. We carry out our investigations in the setting of description logics which underlie modern ontology languages, like RDFS, OWL, and OWL2 from W3C. Notably, we focus on the DL-liteA dialect of the DL-lite family, which encompasses the foundations of the QL profile of OWL2 (i.e., DL-liteR): the W3C recommendation for efficiently managing large data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273645",
                    "name": "Fran\u00e7ois Goasdou\u00e9"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "a5f36ee00dd518dac08aca7d79c9f84225450f53",
            "title": "Towards a Framework for Semantic Exploration of Frequent Patterns",
            "abstract": "Mining frequent patterns is an essential task in discovering hidden correlations in datasets. Although frequent patterns unveil valuable information, there are some challenges which limits their usability. First, the number of possible patterns is often very large which hinders their eff ective exploration. Second, patterns with many items are hard to read and the analyst may be unable to understand their meaning. In addition, the only available information about patterns is their support, a very coarse piece of information. In this paper, we are particularly interested in mining datasets that reflect usage patterns of users moving in space and time and for whom demographics attributes are available (age, occupation, etc). Such characteristics are typical of data collected from smart phones, whose analysis has critical business applications nowadays. We propose pattern exploration primitives, abstraction and refinement, that use hand-crafted taxonomies on time, space and user demographics. We show on two real datasets, Nokia and MovieLens, how the use of such taxonomies reduces the size of the pattern space and how demographics enable their semantic exploration. This work opens new perspectives in the semantic exploration of frequent patterns that reflect the behavior of di fferent user communities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "3228199",
                    "name": "Aur\u00e9lie Bertaux"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "a74f203cb24e1a35dc0ce4431df0bab6d7d86138",
            "title": "Distance-Based Trace Diagnosis for Multimedia Applications: Help Me TED!",
            "abstract": "Execution traces have become essential resources that many developers analyze to debug their applications. Ideally, a developer wants to quickly detect whether there are anomalies on his application or not. However, in practice, the size of multimedia applications trace can reach gigabytes, which makes their exploitation very complex. Usually, developers use visualization tools before stating a hypothesis. In this paper, we argue that this solution is not satisfactory and propose to automatically provide a diagnosis by comparing execution traces. We use distance-based models and conduct a user case to show how TED, our automatic trace diagnosis tool, provides semantic added-value information to the developer. Performance evaluation over real world data shows that our approach is scalable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309554",
                    "name": "C. Kengne"
                },
                {
                    "authorId": "2052763654",
                    "name": "Noha Ibrahim"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1779483",
                    "name": "M. Tchuent\u00e9"
                }
            ]
        },
        {
            "paperId": "b491bb7330c157eb3a7703a79969f8d0df95d763",
            "title": "Efficiently rewriting large multimedia application execution traces with few event sequences",
            "abstract": "The analysis of multimedia application traces can reveal important information to enhance program execution comprehension. However typical size of traces can be in gigabytes, which hinders their effective exploitation by application developers. In this paper, we study the problem of finding a set of sequences of events that allows a reduced-size rewriting of the original trace. These sequences of events, that we call blocks, can simplify the exploration of large execution traces by allowing application developers to see an abstraction instead of low-level events. The problem of computing such set of blocks is NP-hard and naive approaches lead to prohibitive running times that prevent analysing real world traces. We propose a novel algorithm that directly mines the set of blocks. Our experiments show that our algorithm can analyse real traces of up to two hours of video. We also show experimentally the quality of the set of blocks proposed, and the interest of the rewriting to understand actual trace data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309554",
                    "name": "C. Kengne"
                },
                {
                    "authorId": "35438184",
                    "name": "L. Fopa"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "2052763654",
                    "name": "Noha Ibrahim"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1704749",
                    "name": "T. Washio"
                },
                {
                    "authorId": "145264159",
                    "name": "M. Santana"
                }
            ]
        },
        {
            "paperId": "3f03b628cf05a40150ada6221220ab81dbb90516",
            "title": "Proceedings of the 5th International Conference on Information Systems and Economic Intelligence (SIIE'2012)",
            "abstract": "To set up an economic strategy, one needs to have a maximum of relevant information. The role of \" Business Intelligence \" (BI) is to provide relevant and oriented information to the decision-makers. This information is an essential source for making the right decision to guarantee the durability of an organization. Several powerful computer tools are available to collect the necessary significant information. Business Intelligence is based on data processing collected from various cross economic sources. The competitive market makes the acquisition of knowledge necessary. However, to get information is not sufficient, one has to be able to exploit it and to transform it into knowledge. SIIE'2012 conference supports several topics concerned by: - Information System and Economic Intelligence (EI) - Collaborative Information retrieval - Language and knowledge industries - Economic Intelligence and Management - Cognitive and social dimensions in watch and EI processes - Information Management and knowledge sharing - Intelligent e-Technology - Information system governance - System Development and community applications",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709137",
                    "name": "Sahbi Sidhom"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "2773645",
                    "name": "M. Ghenima"
                }
            ]
        },
        {
            "paperId": "e489509a7643d1cfc1c6de79dcaff8ee36bc47ca",
            "title": "Enhancing the Analysis of Large Multimedia Applications Execution Traces with FrameMiner",
            "abstract": "The analysis of multimedia application traces can reveal important information to enhance program comprehension. However traces can be very large, which hinders their effective exploitation. In this paper, we study the problem of finding a k-golden set of blocks that best characterize data. Sequential pattern mining can help to automatically discover the blocks, and we called k-golden set, a set of k blocks that maximally covers the trace. These kind of blocks can simplify the exploration of large traces by allowing programmers to see an abstraction instead of low-level events. We propose an approach for mining golden blocks and finding coverage of frames. The experiments carried out on video and audio application decoding show very promising results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309554",
                    "name": "C. Kengne"
                },
                {
                    "authorId": "35438184",
                    "name": "L. Fopa"
                },
                {
                    "authorId": "2052763654",
                    "name": "Noha Ibrahim"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1704749",
                    "name": "T. Washio"
                }
            ]
        },
        {
            "paperId": "030d3934872fab914b54adaed699991ec22cfe2f",
            "title": "Web Data Management: Ontologies, RDF, and OWL",
            "abstract": "INTRODUCTION The vision of the Semantic Web is that of a world-wide distributed architecture where data and services easily interoperate. This vision is not yet a reality in the Web of today, in which given a particular need, it is difficult to find a resource that is appropriate to it. Also, given a relevant resource, it is not easy to understand what it provides and how to use it. To solve such limitations, facilitate interoperability, and thereby enable the Semantic Web vision, the key idea is to also publish semantics descriptions of Web resources. These descriptions rely on semantic annotations , typically on logical assertions that relate resources to some terms in predefined ontologies . This is the topic of the chapter. An ontology is a formal description providing human users a shared understanding of a given domain. The ontologies we consider here can also be interpreted and processed by machines thanks to a logical semantics that enables reasoning. Ontologies provide the basis for sharing knowledge, and, as such, they are very useful for a number of reasons: Organizing data. It is very easy to get lost in large collections of documents. An ontology is a natural means of \u201corganizing\u201d (structuring) it and thereby facilitates browsing through it to find interesting information. It provides an organization that is flexible, and that naturally structures the information in multidimensional ways. For instance, an ontology may allow browsing through the courses offered by a university by topic or department, by quarter or time, by level, and so forth.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1739309",
                    "name": "I. Manolescu"
                },
                {
                    "authorId": "1744491",
                    "name": "P. Rigaux"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "256eb13b660dff2a8cdac37e8b92a9f465607c46",
            "title": "Web data management",
            "abstract": "Web Data Management (or WDM) refers to a body of work concerned with leveraging the large collections of structured data that can be extracted from the Web. Over the past few years, several research and commercial efforts have explored these collections of data with the goal of improving Web search and developing mechanisms for surfacing different kinds of search answers. This work has leveraged (1) collections of structured data such as HTML tables, lists and forms, (2) recent ontologies and knowledge bases created by crowd-sourcing, such as Wikipedia and its derivatives, DBPedia, YAGO and Freebase, and (3) the collection of text documents from the Web, from which facts could be extracted in a domain-independent fashion. The promise of this line of work is based on the observation that new kinds of results can be obtained by leveraging a huge collection of independently created fragments of data, and typically in ways that are wholly unrelated to the authors' original intent. For example, we might use many database schemas to compute a schema thesaurus. Or we might examine many spreadsheets of scientific data that reveal the aggregate practice of an entire scientific field. As such, WDM is tightly linked to Web-enabled collaboration, even (or especially) if the collaborators are unwitting ones. We will cover the key techniques, principles and insights obtained so far in the area of Web Data Management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1739309",
                    "name": "I. Manolescu"
                },
                {
                    "authorId": "1744491",
                    "name": "P. Rigaux"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "436e3b0beb3fe4e00ba241baa1fa4e05a14a0a9f",
            "title": "Exploiting ontologies and alignments for trust in semantic P2P networks",
            "abstract": "In a semantic P2P network, peers use separate ontologies and rely on alignments between their ontologies for translating queries. However, alignments may be limited \u2014unsound or incomplete\u2014 and generate flawed translations, and thereby produce unsatisfactory answers. In this paper we propose a trust mechanism that can assist peers to select those in the network that are better suited to answer their queries. The trust that a peer has towards another peer is subject to a specific query and approximates the probability that the latter peer will provide a satisfactory answer. In order to compute trust, we exploit the information provided by peers\u2019 ontologies and alignments, along with the information that comes from peers\u2019 experience. Trust values are refined over time as more queries are sent and answers received, and we prove that these approximations converge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2124409",
                    "name": "Manuel Atencia"
                },
                {
                    "authorId": "1800811",
                    "name": "J. Euzenat"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "742cb74392e7fd949321222b9abe0d3742b3488e",
            "title": "Distributed Computing with MapReduce and Pig",
            "abstract": "For personal use only, not for distribution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1739309",
                    "name": "I. Manolescu"
                },
                {
                    "authorId": "1744491",
                    "name": "P. Rigaux"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "75eb332a42eed3dec4ad7320e6afa571f6eeb510",
            "title": "Querying data through ontologies",
            "abstract": "Description explicite de connaissances partag\u00e9es entre diff\u00e9rents acteurs (personnes, applications, agents) Repr\u00e9sentation abstraite et simplifi\u00e9e du monde r\u00e9el avec un but (une application) pr\u00e9cis(e)",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "1739309",
                    "name": "I. Manolescu"
                },
                {
                    "authorId": "116325032",
                    "name": "Cachan Inria"
                },
                {
                    "authorId": "1744491",
                    "name": "P. Rigaux"
                },
                {
                    "authorId": "2063030427",
                    "name": "Cnam Paris"
                },
                {
                    "authorId": "66876582",
                    "name": "Inria Saclay"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                }
            ]
        },
        {
            "paperId": "951cb3425ca81fe12317f07c18aa2bc4cee79ef3",
            "title": "Conquer, an RDFS-based model for context querying",
            "abstract": "We propose a declarative approach to modeling context data based on the RDFS[6] language which enables to declare in a ?exible way classes and properties, and organize them in hierarchies. We show that queries on context data can then be expressed at a high level of abstraction and remain applicable when the class hierarchy is extended.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "73652510",
                    "name": "Anis Benyelloul"
                },
                {
                    "authorId": "2683468",
                    "name": "F. Jouanot"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "b2f916d8231602fcfa123bb9b5720fa247c98583",
            "title": "ProbaMap: a scalable tool for discovering probabilistic mappings between taxonomies",
            "abstract": "In this paper, we investigate a principled approach for defining and discovering probabilistic mappings between two taxonomies. First, we compare two ways of modeling probabilistic mappings which are compatible with the logical constraints declared in each taxonomy. Then we describe a generate and test algorithm (called ProbaMap) which minimizes the number of calls to the probability estimator for determining those mappings whose probability exceeds a certain threshold. Finally, we provide an experimental analysis of this approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2129862",
                    "name": "R\u00e9mi Tournaire"
                },
                {
                    "authorId": "1852297",
                    "name": "Jean-Marc Petit"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "fee9968851230ef998430eae87e1fcc603a9c54e",
            "title": "LN2R a knowledge based reference reconciliation system: OAEI 2010 results",
            "abstract": "This paper presents the first participation of LN2R system in IM@OAEI2010, the Instance Matching track of Ontology Alignment Evaluation Initiative 2010 Campaign. In particular, we participated in OWL data track by performing LN2R system on Person-Restaurant data set. We obtained very good results on person data sets and reasonable results on restaurant data set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7291776",
                    "name": "Fatiha Sa\u00efs"
                },
                {
                    "authorId": "1874535",
                    "name": "Nobal B. Niraula"
                },
                {
                    "authorId": "1718164",
                    "name": "Nathalie Pernelle"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "15c98875a92ad43ef619aae77606cb1b69ba6137",
            "title": "Semantic oriented data spaces",
            "abstract": "A data space system is a multi-model data management system that manages data sourced from a variety of local or external sources. The management of data spaces raises new challenges for representing, indexing, sharing and querying possibly distributed collections of heterogeneous and ill-structured data. Semantic approaches are promising for providing the glue between heterogeneous data. The goal of this course is to give an overview of the challenges and existing solutions for representing and exploiting the data semantics in data spaces. We will distinguish centralized approaches based on the mediator model from decentralized approaches of P2P data sharing for online communities connected through overlay networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "2a6ac1bc6ef60581d3d55d9b03ede8308bd4db7a",
            "title": "DL-LITER in the Light of Propositional Logic for Decentralized Data Management",
            "abstract": "This paper provides a decentralized data model and associated algorithms for peer data management systems (PDMS) based on the DL-LITER description logic. Our approach relies on reducing query reformulation and consistency checking for DL-LITER into reasoning in propositional logic. This enables a straightforward deployment of DL-LITER PDMSs on top of Some Where, a scalable propositional peer-to-peer inference system. We also show how to use the state-of-the-art Minicon algorithm for rewriting queries using views in DL-LITER in the centralized and decentralized cases.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2073078391",
                    "name": "Nada Abdallah"
                },
                {
                    "authorId": "2273645",
                    "name": "Fran\u00e7ois Goasdou\u00e9"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "6e17288d23841c5922d53cd79ef5ef0a5f94a0e6",
            "title": "Reasoning Web. Semantic Technologies for Information Systems: 5th International Summer School 2009, Brixen-Bressanone, Italy, August 30 - September 4, ... Applications, incl. Internet/Web, and HCI",
            "abstract": "This book contains a collection of seven thoroughly revised tutorial papers based on lectures given by leading researchers at the 5th International Summer School on the Reasoning Web, held in Brixen-Bressanone, Italy, from August 30 to September 4, 2009. The objective of the book is to provide a coherent introduction to semantic web methods and research issues with a particular emphasis on reasoning. The focus of this years event was on the use of semantic technologies to enhance data access on the web. Topics covered include design and analysis of reasoning procedures for description logics; answer set programming basics, its modeling methodology and its principal extensions tailored for semantic web applications; languages for constraining and querying XML data; RDF database theory and efficient and scalable support for RDF/OWL data storage, loading, inferencing and querying; tractable description logics and their use for ontology-based data access; and the social semantic desktop, which defines a users personal information environment as a source and end-point of the semantic web.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182320",
                    "name": "Sergio Tessaris"
                },
                {
                    "authorId": "1809261",
                    "name": "Enrico Franconi"
                },
                {
                    "authorId": "72431510",
                    "name": "Thomas Eiter"
                },
                {
                    "authorId": "144768011",
                    "name": "Claudio Guti\u00e9rrez"
                },
                {
                    "authorId": "1789102",
                    "name": "S. Handschuh"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1711430",
                    "name": "R. Schmidt"
                }
            ]
        },
        {
            "paperId": "f456b332775f619185253b83022c97e897e303b1",
            "title": "Data Extraction, Transformation and Integration guided by an Ontology",
            "abstract": "This chapter deals with integration of XML heterogeneous information sources into a data warehouse with data defined in terms of a global abstract schema or ontology. The authors present an approach supporting the acquisition of data from a set of external sources available for an application of interest including data extraction, data transformation and data integration or reconciliation. The integration middleware that the authors propose extracts data from external XML sources which are relevant according to an RDFS+ ontology, transforms returned XML data into RDF facts conformed to the ontology and reconciles RDF data in order to resolve possible redundancies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1762236",
                    "name": "C. Reynaud"
                },
                {
                    "authorId": "1718164",
                    "name": "Nathalie Pernelle"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "3087001",
                    "name": "B. Safar"
                },
                {
                    "authorId": "7291776",
                    "name": "Fatiha Sa\u00efs"
                }
            ]
        },
        {
            "paperId": "3b9ce0423519a5eceb24ac797271845e20b91f56",
            "title": "A probabilistic trust model for semantic peer to peer systems",
            "abstract": "Semantic peer to peer (P2P) systems are fully decentralized overlay networks of people or machines (called peers) sharing and searching varied resources (documents, videos, photos, data, services) based on their semantic annotations using ontologies. They provide a support for the emergence of open and decentralized electronic social networks, in which no central or external authority can control the reliability of the peers participating to the network. This lack of control may however cause some of the results provided by some peers to be unsatisfactory, because of inadequate or obsolete annotations.\n In this paper, we propose a probabilistic model to handle trust in a P2P setting. It supports a local computation and a simple form of propagation of the trust of peers into classes of other peers. We claim that it is well appropriate to the dynamics of P2P networks and to the freedom of each peer within the network to have different viewpoints towards the peers with which it interacts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2359818",
                    "name": "G. H. Nguyen"
                },
                {
                    "authorId": "2690085",
                    "name": "P. Chatalic"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "ae71aec87f39826a7a03118b7caf98228eb39acf",
            "title": "DryadeParent, An Efficient and Robust Closed Attribute Tree Mining Algorithm",
            "abstract": "In this paper, we present a new tree mining algorithm, DryadeParent, based on the hooking principle first introduced in DRYADE. In the experiments, we demonstrate that the branching factor and depth of the frequent patterns to find are key factors of complexity for tree mining algorithms, even if often overlooked in previous work. We show that DryadeParent outperforms the current fastest algorithm, CMTreeMiner, by orders of magnitude on data sets where the frequent tree patterns have a high branching factor.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                },
                {
                    "authorId": "1736451",
                    "name": "K. Ohara"
                },
                {
                    "authorId": "1704749",
                    "name": "T. Washio"
                },
                {
                    "authorId": "1748072",
                    "name": "H. Motoda"
                }
            ]
        },
        {
            "paperId": "22e9d4b67459ae360099c8d56e89fed5ae603851",
            "title": "Mining XML Documents",
            "abstract": "XML documents are becoming ubiquitous because of their rich and flexible format that can be used for a variety of applications. Giving the increasing size of XML collections as information sources, mining techniques that traditionally exist for text collections or databases need to be adapted and new methods to be invented to exploit the particular structure of XML documents. Basically XML documents can be seen as trees, which are well known to be complex structures. This chapter describes various ways of using and simplifying this tree structure to model documents and support efficient mining algorithms. We focus on three mining tasks: classification and clustering which are standard for text collections; discovering of frequent tree structure which is especially important for heterogeneous collection. This chapter presents some recent approaches and algorithms to support these tasks together with experimental evaluation on a variety of large XML collections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1727520",
                    "name": "Laurent Candillier"
                },
                {
                    "authorId": "8905591",
                    "name": "Ludovic Denoyer"
                },
                {
                    "authorId": "1741426",
                    "name": "P. Gallinari"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "1685821",
                    "name": "A. Vercoustre"
                }
            ]
        },
        {
            "paperId": "313d6edd4ae0c2617c62b1871dbcef24d45ba04c",
            "title": "Alogic-based language fordescribing device interoperability",
            "abstract": "services andphysical smartobjects asdevices (logical or physical devices). Itmodels service composition asdevice Technology 'sconstant progress makesusbuild moreand composition, including bothWebservice composition, and moreindependent devices dedicated todifferent goals. In composition ofservices provided byphysical devices. this way,wemakeitmorecomplex forhumanstomake Intheremainder ofthis paper, wepresent themaincharuseofthose devices. Inparticular; thefunctionalities re- acteristics oftheformal-logic-based language that weproquired bysomeusermaybeachievable onlybycombiningpose.Thespecification ofthis language hasbeenalready functionalities ofseveral devices currently accessible over sketched in[6]. achanging network. Wetherefore propose alanguage for describing devices andtheir functionalities inawaythat 2.Thelanguage enables theinference, definition andquerying ofpotential compositions. Wedefine this language, anddemonstrate 2.1.Basiccomponents practical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2100746157",
                    "name": "Sattisvar Tandabany"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "70211710",
                    "name": "Y. Tanakat"
                }
            ]
        },
        {
            "paperId": "65aa90873486353ad423b038028c91203b591abc",
            "title": "L2R: A Logical Method for Reference Reconciliation",
            "abstract": "The reference reconciliation problem consists in deciding whether different identifiers refer to the same data, i.e., correspond to the same world entity. The L2R system exploits the semantics of a rich data model, which extends RDFS by a fragment of OWL-DL and SWRL rules. In L2R, the semantics of the schema is translated into a set of logical rules of reconciliation, which are then used to infer correct decisions both of reconciliation and no reconciliation. In contrast with other approaches, the L2R method has a precision of 100% by construction. First experiments show promising results for recall, and most importantly significant increases when rules are added.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7291776",
                    "name": "Fatiha Sa\u00efs"
                },
                {
                    "authorId": "1718164",
                    "name": "Nathalie Pernelle"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "8255fe1b751167ee96a62fab9c195b027b90d171",
            "title": "Building Scalable Semantic Peer-to-Peer Data Management Systems: The SomeWhere Approach",
            "abstract": "In this talk, I present the SomeWhere approach and infrastructure for building semantic peer-to-peer data management systems based on simple personalized ontologies but which are distributed at a large scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "85f00e079e2ad5315a51b330411f03fefe780ad1",
            "title": "A logic-based language for describing device interoperability",
            "abstract": "Technology's constant progress makes us build more and more independent devices dedicated to different goals. In this way, we make it more complex for humans to make use of those devices. In particular; the functionalities required by some user may be achievable only by combining functionalities of several devices currently accessible over a changing network. We therefore propose a language for describing devices and their functionalities in a way that enables the inference, definition and querying of potential compositions. We define this language, and demonstrate practical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2100746157",
                    "name": "Sattisvar Tandabany"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "144865865",
                    "name": "Yuzuru Tanaka"
                }
            ]
        },
        {
            "paperId": "15adaae1455dfc2d29525ee6e60f2e286f88dfcf",
            "title": "Reasoning with Inconsistencies in Propositional Peer-to-Peer Inference Systems",
            "abstract": "In a peer-to-peer inference system, there is no centralized control or hierarchical organization: each peer is equivalent in functionality and cooperates with other peers in order to solve a collective reasoning task. Since peer theories model possibly different viewpoints, even if each local theory is consistent, the global theory may be inconsistent. We exhibit a distributed algorithm detecting inconsistencies in a fully decentralized setting. We provide a fully distributed reasoning algorithm, which computes only well-founded consequences of a formula, i.e., with a consistent set of support.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2690085",
                    "name": "P. Chatalic"
                },
                {
                    "authorId": "2359818",
                    "name": "G. H. Nguyen"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "201338ebb5b92e17be84fdf1c43d3b349b9a7e70",
            "title": "A Compact Representation for Least Common Subsumers in the description logic ALE",
            "abstract": "This paper introduces a compact representation which helps to avoid the exponential blow-up in space of the Least Common Subsumer (lcs) of two ALE-concept descriptions. Based on the compact representation we define a space of specific graphs which represents all ALE-concept descriptions including the lcs. Next, we propose an algorithm exponential in time and polynomial in space for deciding subsumption between concept descriptions represented by graphs in this space. These results provide better understanding of the double exponential blow-up of the approximation of ALC-concept descriptions by ALE-concept descriptions: double exponential size of the approximation in the ordinary representation is unavoidable in the worst case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108164",
                    "name": "C. L. Duc"
                },
                {
                    "authorId": "1726160",
                    "name": "Nhan Le Thanh"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "4b2ad4bb735c1312b56213216478cc32cf4466b9",
            "title": "Efficient mining of high branching factor attribute trees",
            "abstract": "In this paper, we present a new tree mining algorithm, DryadeParent, based on the hooking principle first introduced in Dryade (Termier et al, 2004). In the experiments, we demonstrate that the branching factor and depth of the frequent patterns to find are key factor of complexity for tree mining algorithms. We show that DryadeParent outperforms the current fastest algorithm, CMTreeMiner, by orders of magnitude on datasets where the frequent patterns have a high branching factor.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                },
                {
                    "authorId": "1736451",
                    "name": "K. Ohara"
                },
                {
                    "authorId": "1704749",
                    "name": "T. Washio"
                },
                {
                    "authorId": "1748072",
                    "name": "H. Motoda"
                }
            ]
        },
        {
            "paperId": "c46fbaab59119953a0dbe82aeba2dced100d9ace",
            "title": "Scalability Study of Peer-to-Peer Consequence Finding",
            "abstract": "In peer-to-peer inference systems, each peer can reason locally but also solicit some of its acquaintances, sharing part of its vocabulary. This paper studies both theoretically and experimentally the problem of computing proper prime implicates for propositional peer-to-peer systems, the global theory (union of all peer theories) of which is not known (as opposed to partition-based reasoning).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2708313",
                    "name": "P. Adjiman"
                },
                {
                    "authorId": "2690085",
                    "name": "P. Chatalic"
                },
                {
                    "authorId": "2273645",
                    "name": "Fran\u00e7ois Goasdou\u00e9"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "40774846",
                    "name": "Laurent Simon"
                }
            ]
        },
        {
            "paperId": "658cf324b706aaafd9bb7dba8c4be203fccbb559",
            "title": "Answering queries using views: A KRDB perspective for the semantic Web",
            "abstract": "In this article, we investigate a first step towards the long-term vision of the Semantic Web by studying the problem of answering queries posed through a mediated ontology to multiple information sources whose content is described as views over the ontology relations. The contributions of this paper are twofold. We first offer a uniform logical setting which allows us to encompass and to relate the existing work on answering and rewriting queries using views. In particular, we make clearer the connection between the problem of rewriting queries using views and the problem of answering queries using extensions of views. Then we focus on an instance of the problem of rewriting conjunctive queries using views through an ontology expressed in a description logic, for which we exhibit a complete algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273645",
                    "name": "Fran\u00e7ois Goasdou\u00e9"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "8a3fb668771742c8d767298413c7d0626335083b",
            "title": "Dryade: a new approach for discovering closed frequent trees in heterogeneous tree databases",
            "abstract": "In this paper we present a novel algorithm for discovering tree patterns in a tree database. This algorithm uses a relaxed tree inclusion definition, making the problem more complex (checking tree inclusion is NP-complete), but allowing to mine highly heterogeneous databases. To obtain good performances, our DRYADE algorithm, discovers only closed frequent tree patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                }
            ]
        },
        {
            "paperId": "8ec61085ad5130e1328111d36ffa27216437540c",
            "title": "Distributed Reasoning in a Peer-to-Peer Setting",
            "abstract": "In a peer-to-peer system, there is no centralized control or hierarchical organization: each peer is equivalent in functionality and cooperates with other peers in order to solve a collective task. Such systems have evolved from simple keyword-based peer-to-peer file sharing systems like Napster and Gnutella to schema-based peer data management systems like Edutella [3] or Piazza [2], which handle semantic data description and support complex queries for data retrieval. In this paper, we are interested in peer-to-peer inference systems in which each peer can answer queries by reasoning from its local (propositional) theory but can also ask queries to some other peers with which it is semantically related by sharing part of its vocabulary. This framework encompasses several applications like peer-to-peer information integration systems or intelligent agents, in which each peer has its own knowledge (about its data or its expertise domain) and some partial knowledge about some other peers. In this setting, when it is solicited to perform a reasoning task and if it cannot solve completely that task locally, a peer must be able to distribute appropriate reasoning subtasks among its acquainted peers. The contribution of this paper is the first consequence finding algorithm in a peer-to-peer setting: it is anytime and computes consequences gradually from the solicited peer to peers that are more and more distant. We have exhibited a sufficient condition on the acquaintance graph of the peer-to-peer inference system for guaranteeing the completeness of this algorithm. Our algorithm splits clauses if they involve vocabularies of several peers. Each piece of a splitted clause is transmitted to the corresponding theory to find its consequences. The consequences that are found for each piece of splitted clause must be recomposed to get the consequences of that clause.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2708313",
                    "name": "P. Adjiman"
                },
                {
                    "authorId": "2690085",
                    "name": "P. Chatalic"
                },
                {
                    "authorId": "2273645",
                    "name": "Fran\u00e7ois Goasdou\u00e9"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "40774846",
                    "name": "Laurent Simon"
                }
            ]
        },
        {
            "paperId": "c34348b22821fd20dd8260e7069e0de97ca166a6",
            "title": "Highlighting Latent Structure in Documents",
            "abstract": "Extensible Markup Language (XML) is playing an increasingly important role in the exchange of a wide variety of data on the Web and elsewhere. It is a simple, very flexible text format, used to annotate data by means of markup. XML documents can be checked for syntactic well-formedness and semantic coherence through DTD and schema validation which makes their processing easier. In particular, data with nested structure can be easily represented with embedded tags. This structured representation should be used in information retrieval models which take structure into account. As such, it is meta-data and therefore a contribution to the Semantic Web. However, nowadays, there exists huge quantities of raw texts and the issue is how to find an easy way to provide these texts with sensible XML structure. Here we present an automatic method to extract tree structure from raw texts. This work has been supported by the Paris XI University (BQR2002 project, Paris-XI University).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2085023006",
                    "name": "Helka Folch"
                },
                {
                    "authorId": "36722443",
                    "name": "B. Habert"
                },
                {
                    "authorId": "2432809",
                    "name": "M. Jardino"
                },
                {
                    "authorId": "1718164",
                    "name": "Nathalie Pernelle"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                }
            ]
        },
        {
            "paperId": "2e415e16dad366856f1a2878cba5916861069277",
            "title": "Querying Distributed Data through Distributed Ontologies: A Simple but Scalable Approach",
            "abstract": "This framework for peer-to-peer data-sharing systems allows efficient query answering over a network of semantically related peers. A simple class-based language appropriate for practical applications defines peer schemas as hierarchies of atomic classes and mappings as inclusions of logical combinations of atomic classes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273645",
                    "name": "Fran\u00e7ois Goasdou\u00e9"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "1dfe4d4809bff855141572178876310f1f565b0a",
            "title": "TreeFinder: a first step towards XML data mining",
            "abstract": "In this paper we consider the problem of searching frequent trees from a collection of tree-structured data modeling XML data. The TreeFinder algorithm aims at finding trees, such that their exact or perturbed copies are frequent in a collection of labelled trees. To cope with complexity issues, TreeFinder is correct but not complete: it finds a subset of actually frequent trees. The default of completeness is experimentally investigated on artificial medium size datasets; it is shown that TreeFinder reaches completeness or falls short for a range of experimental settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                }
            ]
        },
        {
            "paperId": "31c1a1d129f4fb0f36893c704306aa04f8a26d25",
            "title": "ZooM: a nested Galois lattices-based system for conceptual clustering",
            "abstract": "This paper deals with the representation of multi-valued data by clustering them in a small number of classes organized in a hierarchy and described at an appropriate level of abstraction. The contribution of this paper is three fold. First, we investigate a partial order, namely nesting, relating Galois lattices. A nested Galois lattice is obtained by reducing (through projections) the original lattice. As a consequence it makes coarser the equivalence relations defined on extents and intents. Second we investigate the intensional and extensional aspects of the languages used in our system ZooM. In particular we discuss the notion of \u03b1-extension of terms of a class language \u00a3. We also present our most expressive language \u00a33, close to a description logic, and which expresses optionality or/and multi-valuation of attributes. Finally, the nesting order between the Galois lattices corresponding to various languages and extensions is exploited in the interactive system ZooM. Typically a ZooM session starts from a propositional language \u00a32 and a coarse view of the data (through \u03b1-extension). Then the user selects two ordered nodes in the lattice and ZooM constructs a fine-grained lattice between the antecedents of these nodes. So the general purpose of ZooM is to give a general view of concepts addressing a large data set, then focussing on part of this coarse taxonomy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1718164",
                    "name": "Nathalie Pernelle"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1727401",
                    "name": "H. Soldano"
                },
                {
                    "authorId": "2003545",
                    "name": "V. Ventos"
                }
            ]
        },
        {
            "paperId": "8b4c59bfce6322dd20d171d27202fcc2672341e4",
            "title": "Compilation and Approximation of Conjunctive Queries by Concept Descriptions",
            "abstract": "In this paper, we characterize the logical correspondence between conjunctive queries and concept descriptions. We exhibit a necessary and sufficient condition for the compilation of a conjunctive query into an equivalent AL\u2107 concept description. We provide a necessary and sufficient condition for the approximation of conjunctive queries by maximally subsumed ALN concept descriptions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273645",
                    "name": "Fran\u00e7ois Goasdou\u00e9"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "29619b912da497f85dd19bbd83e2254c8782896c",
            "title": "Xyleme: A Dynamic Warehouse for XML Data of the Web",
            "abstract": "Xyleme is a dynamic warehouse for XML data of the Web supporting query evaluation, change control and data integration. We briefly present our motivations, the general architecture and some aspects of Xyleme. The project we describe here was completed at the end of 2000. A prototype has been implemented. This prototype is now being turned into a product by a start-up company also called Xyleme [14]. Xyleme: a complex tissue of wood cells, functions in conduction and storage ...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "2094231873",
                    "name": "V. Aguilera"
                },
                {
                    "authorId": "3249888",
                    "name": "S. Ailleret"
                },
                {
                    "authorId": "34990659",
                    "name": "Bernhard Amann"
                },
                {
                    "authorId": "2083374957",
                    "name": "F. Arambarri"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "2355686",
                    "name": "G. Cobena"
                },
                {
                    "authorId": "2070259970",
                    "name": "G. Corona"
                },
                {
                    "authorId": "2494831",
                    "name": "G. Ferran"
                },
                {
                    "authorId": "145714962",
                    "name": "A. Galland"
                },
                {
                    "authorId": "147485636",
                    "name": "M. Hascoet"
                },
                {
                    "authorId": "1692764",
                    "name": "C. Kanne"
                },
                {
                    "authorId": "13150703",
                    "name": "B. Koechlin"
                },
                {
                    "authorId": "150077717",
                    "name": "D. Le Niniven"
                },
                {
                    "authorId": "144173226",
                    "name": "A. Marian"
                },
                {
                    "authorId": "2727167",
                    "name": "L. Mignet"
                },
                {
                    "authorId": "1809585",
                    "name": "G. Moerkotte"
                },
                {
                    "authorId": "67044763",
                    "name": "B. Nguyen"
                },
                {
                    "authorId": "103197975",
                    "name": "M. Preda"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                },
                {
                    "authorId": "150163759",
                    "name": "J. Sirot"
                },
                {
                    "authorId": "1699308",
                    "name": "P. Veltri"
                },
                {
                    "authorId": "1757944",
                    "name": "D. Vodislav"
                },
                {
                    "authorId": "150106162",
                    "name": "F. Watez"
                },
                {
                    "authorId": "2830802",
                    "name": "T. Westmann"
                }
            ]
        },
        {
            "paperId": "495fb222857f94d31aa6a1ad96d8b2229c1d3ff0",
            "title": "Xyleme, a dynamic warehouse for XML data of the Web",
            "abstract": "The current development of the Web and the generalization of XML technology provide a major opportunity which can radically change the face of the Web. Xyleme intends to be a leader of this revolution by providing database services over the XML data of the Web. Originally, Xyleme was a research project functioning as an open, loosely coupled network of researchers. At the end of 2000, a prototype had been implemented. A start-up company, also called Xyleme, is now turning into a product. The authors summarize the main research efforts of the Xyleme team. They concern: a scalable architecture; the efficient storage of huge quantities of XML data (hundreds of millions of pages); XML query processing with full-text and structural indexing; data acquisition strategies to build the repository and keep it up-to-date; change control with services such as query subscription; and semantic data integration to free users from having to deal with many specific DTDs when expressing queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241613162",
                    "name": "Serge Abiteboul"
                },
                {
                    "authorId": "2094231873",
                    "name": "V. Aguilera"
                },
                {
                    "authorId": "3249888",
                    "name": "S. Ailleret"
                },
                {
                    "authorId": "2258052126",
                    "name": "Bernd Amann"
                },
                {
                    "authorId": "2083374957",
                    "name": "F. Arambarri"
                },
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "2355686",
                    "name": "G. Cobena"
                },
                {
                    "authorId": "2070259970",
                    "name": "G. Corona"
                },
                {
                    "authorId": "2494831",
                    "name": "G. Ferran"
                },
                {
                    "authorId": "145714962",
                    "name": "A. Galland"
                },
                {
                    "authorId": "147485636",
                    "name": "M. Hascoet"
                },
                {
                    "authorId": "1692764",
                    "name": "C. Kanne"
                },
                {
                    "authorId": "2258057557",
                    "name": "B. Koechlin"
                },
                {
                    "authorId": "2263209400",
                    "name": "D. L. Niniven"
                },
                {
                    "authorId": "2257166106",
                    "name": "Am\u00e9lie Marian"
                },
                {
                    "authorId": "2727167",
                    "name": "L. Mignet"
                },
                {
                    "authorId": "1809585",
                    "name": "G. Moerkotte"
                },
                {
                    "authorId": "2257985316",
                    "name": "Benjamin Nguyen"
                },
                {
                    "authorId": "103197975",
                    "name": "M. Preda"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "2249255783",
                    "name": "Mich\u00e8le Sebag"
                },
                {
                    "authorId": "150163759",
                    "name": "J. Sirot"
                },
                {
                    "authorId": "2263173877",
                    "name": "Pierangelo Veltri"
                },
                {
                    "authorId": "1757944",
                    "name": "D. Vodislav"
                },
                {
                    "authorId": "150106162",
                    "name": "F. Watez"
                },
                {
                    "authorId": "2830802",
                    "name": "T. Westmann"
                }
            ]
        },
        {
            "paperId": "7403aff432aebd3cbed36940923020af9c7562dd",
            "title": "Automatic Construction and Refinement of a Class Hierarchy over Semi-Structured Data",
            "abstract": "In many applications, it becomes crucial to help users to access to a huge amount of data by clustering them in a small number of classes described at an appropriate level of abstraction. In this paper, we present an approach based on the use of two languages of description of classes for the automatic clustering of semistructured data. The rst language of classes has a high power of abstraction and guides the construction of a lattice of classes covering the whole set of the data. The second language of classes, more expressive and more precise, is the basis for the re nement of a part of the lattice that the user wants to focus on. Our approach has been implemented and experimented on real data in the setting of the GAEL project 1 which aims at building exible electronic catalogs organized as a hierarchy of classes of products. Our experiments have been conducted on real data coming from the C/Net (http://www.cnet.com) electronic catalog of computer products.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1718164",
                    "name": "Nathalie Pernelle"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "2003545",
                    "name": "V. Ventos"
                }
            ]
        },
        {
            "paperId": "8cf37a07871bec0a5c70a8294ed852141727a9d8",
            "title": "Combining Statistics and Semantics for Word and Document Clustering",
            "abstract": "A new approach for constructing pseudo-keywords, referred to as Sense Units, is proposed. Sense Units are obtained by a word clustering process, where the underlying similarity reflects both statistical and semantic properties, respectively detected through Latent Semantic Analysis and WordNet. Sense Units are used to recode documents and are evaluated from the performance increase they permit in classification tasks. \n \nExperimental results show that accounting for semantic information in fact decreases the performances compared to LSI standalone. \n \nThe main weakenesses of the current hybrid scheme are discussed and several tracks for improvement are sketched.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "11a8d56136601206fe0473b2afd7f14cb31168ce",
            "title": "The Use of CARIN Language and Algorithms for Information Integration: The PICSEL System",
            "abstract": "PICSEL is an information integration system over sources that are distributed and possibly heterogeneous. The approach which has been chosen in PICSEL is to define an information server as a knowledge-based mediator in which CARIN is used as the core logical formalism to represent both the domain of application and the contents of information sources relevant to that domain. In this paper, we describe the way the expressive power of the CARIN language is exploited in the PICSEL information integration system, while maintaining the decidability of query answering. We illustrate it on examples coming from the tourism domain, which is the first real case that we have to consider in PICSEL, in collaboration with the travel agency Degriftour. see",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273645",
                    "name": "Fran\u00e7ois Goasdou\u00e9"
                },
                {
                    "authorId": "1398233401",
                    "name": "V. Latt\u00e8s"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "d38682571ece7475e123fe121968e43caaeae90a",
            "title": "Workshop on Intelligent Information Integration (III-99)",
            "abstract": "The Workshop on Intelligent Information Integration (III), organized in conjunction with the Sixteenth International Joint Conference on Artificial Intelligence, was held on 31 July 1999 in Stockholm, Sweden. Approximately 40 people participated, and nearly 20 papers were presented. This packed workshop schedule resulted from a large number of submissions that made it difficult to reserve discussion time without rejecting an unproportionately large number of papers. Participants included scientists and practitioners from industry and academia. Topics included query planning, applications of III, mediator architectures, and the use of ontologies for III.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1766239",
                    "name": "D. Fensel"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                },
                {
                    "authorId": "8551365",
                    "name": "N. Kushmerick"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "71d6d68c51bfadad7e2cae464f872983e4bd70dc",
            "title": "Backward Reasoning in Aboxes for Query Answering",
            "abstract": "In this paper, we address the problem of answering (unions of) conjunctive queries on Aboxes. We propose a backward-chaining query evaluation based on query expansion. Expanding a query consists of determining all the ways of deriving it from atoms built on some distinguished predicates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "2db175290d46d7f7b1e1ae23bc7384d3649853ae",
            "title": "Verifying the World Wide Web: a Position Statement",
            "abstract": "This paper investigates the issues raised by checking potential update anomalies of a web site. It is of course impossible to control the updates for the whole World Wide Web. However, it is conceivable to provide tools for pointing out potential missing updates for a given web site (i.e, a set of Web pages, for instance related to a given matter or to a given institution, which is under the control of a web site manager). The assumption that we make in this paper is that modeling the semantics of the considered Web site is possible, while modeling the semantics of the sources outside of it, which can be connected to it, is not possible. Therefore, two diierent update problems have to be distinguished, depending on whether the updates that have to be done inside the web site are triggered by changes occuring inside or outside the web site.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "40483bc238a8016ca0058d8229a263c3575202ce",
            "title": "Verification of Knowledge Bases: a Unifying Logical View",
            "abstract": "Notions of correctness and completeness of a KB are impossible to capture completely by a formal deenition. However, when the knowledge base is represented in a declarative logical formalism, they can be approached by a logical analysis of its contents. A logical analysis of the knowledge base and constraints that are known to hold on the domain enables us to detect anomalies or discrepancies between the knowledge represented in the KB and the domain. This paper describes a uniied logical framework for the veriication problem of knowledge bases represented by logical rules (i.e., Horn rules and some extensions). We consider several instances of the veriication problem, describe algorithms for veriication, and establish the computational complexity of the veriication problem. In particular, we consider the veriication w.r.t. consistency constraints that are speciied separately on the inputs and on the outputs. Next, we consider dependency constraints on the relations between inputs and outputs. Finally, we consider several extensions to our logical formalism that enable us to express richer classes of constraints. The extensions involve the use of negation and the use of constructors of description logics in the constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "ea06a1b9120fa79a5ed3ceca3743a5fd9c909398",
            "title": "ECAI-96 workshop on validation, verification and refinement of KBS: A short report",
            "abstract": "Ensuring reliability and enhancing quality of Knowledge Based Systems (KBS) are critical factors for their successful deployment in real-world applications. This is a broad task involving both methodological and formal approaches for designing rigorous Validation, Verification and Testing (VVT) methods and tools. Some of these can be adapted from conventional software engineering, while others rely on specific aspects of KBS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "2257301493",
                    "name": "Susan Craw"
                }
            ]
        },
        {
            "paperId": "f1ea620de49d886f317e76a9aa695175c3b95cce",
            "title": "Rewriting queries using views in description logics",
            "abstract": "The problem of rewriting queries using views is to iind a query expression that uses only a set of views V and is equivalent to (or maximally contained in) a given query Q. Rewriting queries using views is important for query optimization and for applications such as information integration and data warehousing. Description logics are a family of logics that were developed for modeling complex hierarchical structures, and can also be viewed as a query language with an interesting tradeoff between complexity and expressive power. We consider the problem of rewriting queries using views expressed in description logics and conjunctive queries over description logics. We show that if the view definitions do not contain existential variables, then it is always possible to find a rewriting that is a union of conjunctive queries, and furthermore, this rewriting produces the maximal set of answers possible from the views. If the views have existential variables, the rewriting may be recursive. We present an algorithm for producing a recursive rewriting, that is guaranteed to be a maximal one when the underlying database forms a tree of constants. We show that in general, it is not always be possible to find a maximal rewriting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "306c0b659b0c892a68148659105615d13898433f",
            "title": "A Proposal for a Glass-Box Approach for Subsumption Checking",
            "abstract": "AProp osalforaGlass-BoxApproachSubsumptionCheckingQuentinElhaik,Marie-ChristineRousset,Marie-ClaudeGaudelL.R.IU.R.AC.N.R.S,UniversityofParis-SudBuilding490,91405OrsayCedex,Francemcr@lri.lri.fr1MotivationDescriptionlogicshavego o drepresentationalprop-ertiesandtheircomputationalprop ertiesarewellundersto o d.However,oneoftheirmaindrawbackisthattheircurrentreasoningalgorithmshaveaninherentblack-b oxnature,i.etheydon'tmakeex-plicithowsubsumptioninferencesareobtained.Thisisanimp ortantdrawbackforprovidingex-planationordebuggingservices.Descriptionlogicswithareasonableexpressivep owercontaindi erentconstructorsthecombinationofwhichcanleadtonumerous,complexandvariedformsofsubsump-tionrelation.Consequently,somesubsumptionre-lationsarecomplexandneedtob eexplainedauserortoaterminologydesigner.Thep ointisnottoexplainusersordesignershowsubsumptioniscomputedbuthowacomplexsubsumptioninfer-encecanb edecomp osedintodi erentsimplerinfer-encesteps.ForKBS,ruleshaveshowntob eafor-malismthatiswellappropriatetosupp orttraceca-pabilitiesforb othexplanationanddebuggingpur-p oses.Moregenerally,aspreviouslyadvo catedby[1]and[6],sp ecifyingthesubsumptionrelationwithasetofinferenceruleswouldhavemanyadvantagesforcharacterizing,customizingoroptimizingimple-mentationsofsubsumptionalgorithms.Inaddition,itcouldb ethebasisforde ningdi erentkindsofrelevance(e.g,ofafacttoquery)andnewwaysofreasoning(e.g,ab ductivereasoning)inadescriptionlogicssetting.Thema jordi\u000ecultyistoobtainacompletesetofinferencerulesharac-terizingthesubsumptionrelation.Inthispap er,weconsideraratherexpressivde-scriptionlogicslanguage,includingthetwodesig-natedprimitiveconcepts>(top)and?(b ottom),1corresp ondencehastob esentthisauthorandtheconstructorsu(conjunctionofconcepts),:(negation,forprimitiveconceptsonly),8(uni-versalrolequanti cation),9(existentalquan-ti cation)andquali ednumberrestrictions.Thep ossibilitytoexpressquali ednumb errestrictions(denoted(\u0015n R C)and\u0014))hasb eenveryusefulforarealapplicationofmo delingcomp onentsofnuclearplants([5]).Notethatquali edumb errestrictionsaremoreexpressivethansimplenumb errestrictions.Insection2,wesummarizetheglass-b oxconsis-tencycheckingmetho dthatweprop osedin[5],ordertoprovideatraceofsomethesubsump-tionstepsleadingtoaninconsistencyasatrace-tre.Inthesections3and4wdescrib earecentwork[2],basedonestablishingacorresp ondenceb e-tweencomplexconceptsinourlanguageandrecordtyp es,whichenabledustoobtainasoundandcom-pletesetofinferencerulestocharacterizesubsump-tioninourlanguage.2Aglass-b oxconsistencycheckingmetho dbasedonrulesTheglass-b oxapproachthatweprop osedin[5]isquitesimilarinspiritto[3]providingexplanationsforinferencesinCLASSIC.Ourapproachcanb eusedasacomplementofexistingsubsumptional-gorithms,b othtop ossiblycompleteexistingsub-sumptionalgorithmsandtoprovidetracesforthemostsubtlesubsumptioninferences.Thebasicas-sumptionofourapproachisthatthesubsumptionrelationsthatarethemostdi\u000eculttounderstandforaterminologydesignerarethesubsumptionre-lationsthatarenotstructural,inparticularthosestatingthataconjunctionofconceptsissubsumedbyaconceptthatdo esnotsubsumeanofthe",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3030580",
                    "name": "Quentin Elhaik"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "1684909",
                    "name": "M. Gaudel"
                }
            ]
        },
        {
            "paperId": "3e02b2eb9ffc7f751fafdb8dfb01c86787e08054",
            "title": "Switching Terminologies - Creating a New View of An Old World: Preliminary Report",
            "abstract": "Several advanced applications of database systems require the modeling, maintenance, and usage of large collections of views. Prime examples include mediator systems that provide access to multiple information sources, data mining and archeology, mobile databases, data warehouses, and decision support systems. Furthermore, some database vendors are considering the maintenance of materialized views also as a means for query optimization. As a result, problems concerning materialized views have recently received a lot of attention in the database community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "4078f09f5b1d69dfbbc11629d8ed607e7da5bdc9",
            "title": "The Limits on Combining Recursive Horn Rules with Description Logics",
            "abstract": "Horn rule languages have formed the basis for many Artificial Intelligence application languages, but are not expressive enough to model domains with a rich hierarchical structure. Description logics have been designed especially to model rich hierarchies. Several applications would significantly benefit from combining the expressive power of both formalisms. This paper focuses on combining recursive function-free Horn rules with the expressive description logic ALCNR, and shows exactly when a hybrid language with decidable inference can be obtained. First, we show that several of the core constructors of description logics lead by themselves to undecidability of inference when combined with recursive function-free Horn rules. We then show that without these constructors we obtain a maximal subset of ALCNR that yields a decidable hybrid language. Finally, we describe a restriction on the Horn rules that guarantees decidable inference when combined with all of ALCNR, and covers many of the common usages of recursive rules.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "7a195957eca847e32b82d4dabc8d7447a4d80ac8",
            "title": "CARIN: A Representation Language Combining Horn Rules and Description Logics",
            "abstract": "We describe CARIN, a novel family of representation languages, which integrate the expressive power of Horn rules and of description logics. We address the key issue in designing such a language, namely, providing a sound and complete inference procedure. We identify existential entailment as a core problem in reasoning in CARIN, and describe an existential entailment algorithm for CARIN languages whose description logic component is ALCNR. This algorithm entails several important results for reasoning in CARIN, most notably: (1) a sound and complete inference procedure for non recursive CARIN-ALCNR, and (2) an algorithm for determining rule subsumption overALCNR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "bd9dc44e3c15fc8dc484c405ac18c2a133deeb2b",
            "title": "Merging test and verification for rule base debugging",
            "abstract": "A way of formally but partially characterizing knowledge base correctness is to define knowledge base coherency. The first contribution of this paper is to show how taking into account test cases can lead to a new definition of rule base coherency that is better than existing ones. Our second contribution is that we propose extensions of model-based diagnosis that enable the complete characterization of rule base incoherencies and their possible causes. As a result, we obtain an algorithm for both debugging rule bases and detecting incoherencies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721687",
                    "name": "F. Bouali"
                },
                {
                    "authorId": "1883021",
                    "name": "S. Loiseau"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "c98f0d40e87c2e4e34f251c2f3ffa0ffd41a150e",
            "title": "Modeling and Verifying Complex Objects: A Declarative Approach Based on Description Logics",
            "abstract": "Description logics are good candidates for representing co mplex objects. Their expressive and computational properties ha ve been extensively studied and are well understood. However, from a knowledge engineering perspective, the current reasoning servi ces provided by description logic systems do not suffice for representing objects. Adding services (e.g help for debugging) is necessar y to make description logic systems useful tools for building object -centered knowledge bases. In this paper, we present a principled glass-box approach both for trace-based debugging and for supporting transparent modeling of whole-part relationship, using descrip tion logics. This approach has been proven very useful for a real application of modeling components of power plants.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "2102609988",
                    "name": "Pascale Hors"
                }
            ]
        },
        {
            "paperId": "e35450060883f451ac9af3c9fd7b13def9dacd94",
            "title": "Verification of Knowledge Bases Based on Containment Checking",
            "abstract": "Building complex knowledge based applications requires encoding large amounts of domain knowledge. After acquiring knowledge from domain experts, much of the effort in building a knowledge base goes into verifying that the knowledge is encoded correctly. We consider the problem of verifying hybrid knowledge bases that contain both Horn rules and a terminology in a description logic. Our approach to the verification problem is based on showing a close relationship to the problem of query containment. Our first contribution, based on this relationship, is presenting a thorough analysis of the decidability and complexity of the verification problem, for knowledge bases containing recursive rules and the interpreted predicates =, \u2264, < and \u2260. Second, we show that important new classes of constraints on correct inputs and outputs can be expressed in a hybrid setting, in which a description logic class hierarchy is also considered, and we present the first complete algorithm for verifying such hybrid knowledge bases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "a6f8877b3aee52f73644bc9d050ec004fc07fa78",
            "title": "An event-driven causal simulation applied to geoforecasting analysis",
            "abstract": "A qualitative-quantitative formalism centered around the process paradigm is presented. To model the causal interactions among the parameters of a system, an influence graph is used. The exploitation of the formalism consists of a causal simulation driven by dynamic occurrence of interesting events. The results of the simulation consists of the approximate evolution over time of the parameters. Because time is central in the reasoning used, evolution is explicitly represented over time of the parameters by piecewise linear functions. This approach has been applied to geoforecasting analysis to simulate plausible scenarios of natural evolution over time of deep radioactive waste-disposal sites.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "70211344",
                    "name": "A. Dejerroud"
                },
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "100758996",
                    "name": "P. Godefroy"
                },
                {
                    "authorId": "33620234",
                    "name": "M. Garcin"
                }
            ]
        },
        {
            "paperId": "1dd70c99c74362eff70ad3ce3997176172768391",
            "title": "On the consistency of knowledge bases: the COVADIS system",
            "abstract": "It is currently thought in the knowledge\u2010based systems (KBS) domain that sophisticated tools are necessary for helping an expert with the difficult task of knowledge acquisition. The problem of detecting inconsistencies is especially crucial. The risk of inconsistencies increases with the size of the knowledge base; for large knowledge bases, detecting inconsistencies \u201cby hand\u201d or even by a superficial survey of the knowledge base is impossible. Indeed, most inconsistencies are due to the interaction between several rules via often deep deductions. In this paper, we first state the problem and define our approach in the framework of classical logic. We then describe a complete method to prove the consistency (or the inconsistency) of knowledge bases that we have implemented in the COVADIS system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                }
            ]
        },
        {
            "paperId": "7d40b530853faf7b4c6cc298a61881c1174c7360",
            "title": "Negative and positive explanations in expert",
            "abstract": "Abstract This paper presents an explanation module implemented in the Computer Science Department of the University of Orsay. It was designed to be used after a session with an expert system and to answer two types of questions: why does some fact have a particular value (deduced by the expert system in the session) and why does some fact not have a particular value (after the expert system has given a value different from the one the user expected) ? The answers to these questions are referred to as positive and negative explanations, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144986814",
                    "name": "M. Rousset"
                },
                {
                    "authorId": "3087001",
                    "name": "B. Safar"
                }
            ]
        }
    ]
}