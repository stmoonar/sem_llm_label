{
    "authorId": "82737548",
    "papers": [
        {
            "paperId": "43f9eafa33aaee285655fcda0c2f50fd0b5fc899",
            "title": "All Birds with One Stone: Multi-task Text Classification for Efficient Inference with One Forward Pass",
            "abstract": "Multi-Task Learning (MTL) models have shown their robustness, effectiveness, and efficiency for transferring learned knowledge across tasks. In real industrial applications such as web content classification, multiple classification tasks are predicted from the same input text such as a web article. However, at the serving time, the existing multitask transformer models such as prompt or adaptor based approaches need to conduct N forward passes for N tasks with O(N) computation cost. To tackle this problem, we propose a scalable method that can achieve stronger performance with close to O(1) computation cost via only one forward pass. To illustrate real application usage, we release a multitask dataset on news topic and style classification. Our experiments show that our proposed method outperforms strong baselines on both the GLUE benchmark and our news dataset. Our code and dataset are publicly available at https://bit.ly/mtop-code.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3488341",
                    "name": "Jiaxin Huang"
                },
                {
                    "authorId": "2115346248",
                    "name": "Tianqi Liu"
                },
                {
                    "authorId": "2746747",
                    "name": "Jialu Liu"
                },
                {
                    "authorId": "143828990",
                    "name": "\u00c1. Lelkes"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "121ec105fed37e0f40deba6915a5b2a57e17e337",
            "title": "Quiz-Style Question Generation for News Stories",
            "abstract": "A large majority of American adults get at least some of their news from the Internet. Even though many online news products have the goal of informing their users about the news, they lack scalable and reliable tools for measuring how well they are achieving this goal, and therefore have to resort to noisy proxy metrics (e.g., click-through rates or reading time) to track their performance. As a first step towards measuring news informedness at a scale, we study the problem of quiz-style multiple-choice question generation, which may be used to survey users about their knowledge of recent news. In particular, we formulate the problem as two sequence-to-sequence tasks: question-answer generation (QAG) and distractor, or incorrect answer, generation (DG). We introduce NewsQuizQA, the first dataset intended for quiz-style question-answer generation, containing 20K human written question-answer pairs from 5K news article summaries. Using this dataset, we propose a series of novel techniques for applying large pre-trained Transformer encoder-decoder models, namely PEGASUS and T5, to the tasks of question-answer generation and distractor generation. We show that our models outperform strong baselines using both automated metrics and human raters. We provide a case study of running weekly quizzes on real-world users via the Google Surveys platform over the course of two months. We found that users generally found the automatically generated questions to be educational and enjoyable. Finally, to serve the research community, we are releasing the NewsQuizQA dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143828990",
                    "name": "\u00c1. Lelkes"
                },
                {
                    "authorId": "2057663102",
                    "name": "Vinh Q. Tran"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "1b7799a4cfbbb99b29b27f8c2fab3fba610e79b2",
            "title": "NewsEmbed: Modeling News through Pre-trained Document Representations",
            "abstract": "Effectively modeling text-rich fresh content such as news articles at document-level is a challenging problem. To ensure a content-based model generalize well to a broad range of applications, it is critical to have a training dataset that is large beyond the scale of human labels while achieving desired quality. In this work, we address those two challenges by proposing a novel approach to mine semantically-relevant fresh documents, and their topic labels, with little human supervision. Meanwhile, we design a multitask model called NewsEmbed that alternatively trains a contrastive learning with a multi-label classification to derive a universal document encoder. We show that the proposed approach can provide billions of high quality organic training examples and can be naturally extended to multilingual setting where texts in different languages are encoded in the same semantic space. We experimentally demonstrate NewsEmbed's competitive performance across multiple natural language understanding tasks, both supervised and unsupervised.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2746747",
                    "name": "Jialu Liu"
                },
                {
                    "authorId": "2115346248",
                    "name": "Tianqi Liu"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "27794bca3b7327aff29e2593e8b989b6a5af678b",
            "title": "Training ELECTRA Augmented with Multi-word Selection",
            "abstract": "Pre-trained text encoders such as BERT and its variants have recently achieved state-of-the-art performances on many NLP tasks. While being effective, these pre-training methods typically demand massive computation resources. To accelerate pre-training, ELECTRA trains a discriminator that predicts whether each input token is replaced by a generator. However, this new task, as a binary classification, is less semantically informative. In this study, we present a new text encoder pre-training method that improves ELECTRA based on multi-task learning. Specifically, we train the discriminator to simultaneously detect replaced tokens and select original tokens from candidate sets. We further develop two techniques to effectively combine all pre-training tasks: (1) using attention-based networks for task-specific heads, and (2) sharing bottom layers of the generator and the discriminator. Extensive experiments on GLUE and SQuAD datasets demonstrate both the effectiveness and the efficiency of our proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "2746747",
                    "name": "Jialu Liu"
                },
                {
                    "authorId": "2115346248",
                    "name": "Tianqi Liu"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "4170608b18671e5258584788af3e0a571d8aa39c",
            "title": "Collocating News Articles with Structured Web Tables\u2731",
            "abstract": "In today\u2019s news deluge, it can often be overwhelming to understand the significance of a news article or verify the facts within. One approach to address this challenge is to identify relevant data so that crucial statistics or facts can be highlighted for the user to easily digest, and thus improve the user\u2019s comprehension of the news story in a larger context. In this paper, we look toward structured tables on the Web, especially the high quality data tables from Wikipedia, to assist in news understanding. Specifically, we aim to automatically find tables related to a news article. For that, we leverage the content and entities extracted from news articles and their matching tables to fine-tune a Bidirectional Transformers (BERT) model. The resulting model is, therefore, an encoder tailored for article-to-table match. To find the matching tables for a given news article, the fine-tuned BERT model encodes each table in the corpus and the news article into their respective embedding vectors. The tables with the highest cosine similarities to the news article in this new representation space are considered the possible matches. Comprehensive experimental analyses show that the new approach significantly outperforms the baselines over a large, weakly-labeled, dataset obtained from Web click logs as well as a small, crowdsourced, evaluation set. Specifically, our approach achieves near 90% accuracy@5 as opposed to baselines varying between 30% and 64%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49982610",
                    "name": "Alyssa Lees"
                },
                {
                    "authorId": "144251706",
                    "name": "Luciano Barbosa"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "2110800905",
                    "name": "L. Silva"
                },
                {
                    "authorId": "1557391861",
                    "name": "You Wu"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "e79d1206292bc5e67ba19737d87d4b2ea4a37105",
            "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization",
            "abstract": "State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce Charformer, a deep Transformer model that integrates GBST and operates on the byte level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that Charformer outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, Charformer is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28%-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144447820",
                    "name": "Yi Tay"
                },
                {
                    "authorId": "2057663102",
                    "name": "Vinh Q. Tran"
                },
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "143702064",
                    "name": "Jai Gupta"
                },
                {
                    "authorId": "3351938",
                    "name": "Hyung Won Chung"
                },
                {
                    "authorId": "11774695",
                    "name": "Dara Bahri"
                },
                {
                    "authorId": "145144957",
                    "name": "Zhen Qin"
                },
                {
                    "authorId": "2054255425",
                    "name": "Simon Baumgartner"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "1680617",
                    "name": "Donald Metzler"
                }
            ]
        },
        {
            "paperId": "f87a921fe6984953634356b5cd895cc43dbdd2c9",
            "title": "AgreeSum: Agreement-Oriented Multi-Document Summarization",
            "abstract": "We aim to renew interest in a particular multi-document summarization (MDS) task which we call AgreeSum: agreement-oriented multi-document summarization. Given a cluster of articles, the goal is to provide abstractive summaries that represent information common and faithful to all input articles. Given the lack of existing datasets, we create a dataset for AgreeSum, and provide annotations on article-summary entailment relations for a subset of the clusters in the dataset. We aim to create strong baselines for the task by applying the top-performing pretrained single-document summarization model PEGASUS onto AgreeSum, leveraging both annotated clusters by supervised losses, and unannotated clusters by T5-based entailment-related and language-related losses. Compared to other baselines, both automatic evaluation and human evaluation show better article-summary and cluster-summary entailment in generated summaries. On a separate note, we hope that our article-summary entailment annotations contribute to the community's effort in improving abstractive summarization faithfulness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46230016",
                    "name": "Richard Yuanzhe Pang"
                },
                {
                    "authorId": "143828990",
                    "name": "\u00c1. Lelkes"
                },
                {
                    "authorId": "2057663102",
                    "name": "Vinh Q. Tran"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "22530c6a79d405308d68f4aa0cbce62d1376a4a8",
            "title": "A Generative Approach to Titling and Clustering Wikipedia Sections",
            "abstract": "We evaluate the performance of transformer encoders with various decoders for information organization through a new task: generation of section headings for Wikipedia articles. Our analysis shows that decoders containing attention mechanisms over the encoder output achieve high-scoring results by generating extractive text. In contrast, a decoder without attention better facilitates semantic encoding and can be used to generate section embeddings. We additionally introduce a new loss function, which further encourages the decoder to generate high-quality embeddings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49713890",
                    "name": "Anjalie Field"
                },
                {
                    "authorId": "2204815",
                    "name": "S. Rothe"
                },
                {
                    "authorId": "2054255425",
                    "name": "Simon Baumgartner"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "3407108",
                    "name": "Abe Ittycheriah"
                }
            ]
        },
        {
            "paperId": "5b3d791caf682998bbd96ce08a98bfc95a86b3a6",
            "title": "TURL",
            "abstract": "Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in a self-supervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145924070",
                    "name": "Xiang Deng"
                },
                {
                    "authorId": "1515546612",
                    "name": "Huan Sun"
                },
                {
                    "authorId": "49982610",
                    "name": "Alyssa Lees"
                },
                {
                    "authorId": "1557391861",
                    "name": "You Wu"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "9aeb7be90e304ad9b915bd6f86304f85402c84d5",
            "title": "Factoring Fact-Checks: Structured Information Extraction from Fact-Checking Articles",
            "abstract": "Fact-checking, which investigates claims made in public to arrive at a verdict supported by evidence and logical reasoning, has long been a significant form of journalism to combat misinformation in the news ecosystem. Most of the fact-checks share common structured information (called factors) such as claim, claimant, and verdict. In recent years, the emergence of ClaimReview as the standard schema for annotating those factors within fact-checking articles has led to wide adoption of fact-checking features by online platforms (e.g., Google, Bing). However, annotating fact-checks is a tedious process for fact-checkers and distracts them from their core job of investigating claims. As a result, less than half of the fact-checkers worldwide have adopted ClaimReview as of mid-2019. In this paper, we propose the task of factoring fact-checks for automatically extracting structured information from fact-checking articles. Exploring a public dataset of fact-checks, we empirically show that factoring fact-checks is a challenging task, especially for fact-checkers that are under-represented in the existing dataset. We then formulate the task as a sequence tagging problem and fine-tune the pre-trained BERT models with a modification made from our observations to approach the problem. Through extensive experiments, we demonstrate the performance of our models for well-known fact-checkers and promising initial results for under-represented fact-checkers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388034901",
                    "name": "Shan Jiang"
                },
                {
                    "authorId": "2054255425",
                    "name": "Simon Baumgartner"
                },
                {
                    "authorId": "3407108",
                    "name": "Abe Ittycheriah"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "d309117204d25e752d29d5a55785ccf942e43146",
            "title": "On detecting cherry-picked trendlines",
            "abstract": "Poorly supported stories can be told based on data by cherry-picking the data points included. While such stories may be technically accurate, they are misleading. In this paper, we build a system for detecting cherry-picking, with a focus on trendlines extracted from temporal data. We define a support metric for detecting such trendlines. Given a dataset and a statement made based on a trendline, we compute a support score that indicates how cherry-picked it is. Studying different types of trendlines and formalizing terms, we propose efficient and effective algorithms for computing the support measure. We also study the problem of discovering the most supported statements. Besides theoretical analysis, we conduct extensive experiments on real-world data, that demonstrate the validity of our proposed techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1717283",
                    "name": "Abolfazl Asudeh"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1557391861",
                    "name": "You Wu"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "ef57ad148ec2eeef5eb3467f3e37e30042b2c7bd",
            "title": "Generating Representative Headlines for News Stories",
            "abstract": "Millions of news articles are published online every day, which can be overwhelming for readers to follow. Grouping articles that are reporting the same event into news stories is a common way of assisting readers in their news consumption. However, it remains a challenging research problem to efficiently and effectively generate a representative headline for each story. Automatic summarization of a document set has been studied for decades, while few studies have focused on generating representative headlines for a set of articles. Unlike summaries, which aim to capture most information with least redundancy, headlines aim to capture information jointly shared by the story articles in short length and exclude information specific to each individual article. In this work, we study the problem of generating representative headlines for news stories. We develop a distant supervision approach to train large-scale generation models without any human annotation. The proposed approach centers on two technical components. First, we propose a multi-level pre-training framework that incorporates massive unlabeled corpus with different quality-vs.-quantity balance at different levels. We show that models trained within the multi-level pre-training framework outperform those only trained with human-curated corpus. Second, we propose a novel self-voting-based article attention layer to extract salient information shared by multiple articles. We show that models that incorporate this attention layer are robust to potential noises in news stories and outperform existing baselines on both clean and noisy datasets. We further enhance our model by incorporating human labels, and show that our distant supervision approach significantly reduces the demand on labeled data. Finally, to serve the research community, we publish the first manually curated benchmark dataset on headline generation for news stories, NewSHead, which contains 367K stories (each with 3-5 articles), 6.5 times larger than the current largest multi-document summarization dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7787721",
                    "name": "Xiaotao Gu"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2746747",
                    "name": "Jialu Liu"
                },
                {
                    "authorId": "40244451",
                    "name": "Hongkun Yu"
                },
                {
                    "authorId": "1557391861",
                    "name": "You Wu"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "1490886761",
                    "name": "Daniel Finnie"
                },
                {
                    "authorId": "48017650",
                    "name": "Jiaqi Zhai"
                },
                {
                    "authorId": "1490887487",
                    "name": "Nicholas Zukoski"
                }
            ]
        },
        {
            "paperId": "09c09b0d9e17ba4b61712c036043bd36ea584732",
            "title": "Contextual Fact Ranking and Its Applications in Table Synthesis and Compression",
            "abstract": "Modern search engines increasingly incorporate tabular content, which consists of a set of entities each augmented with a small set of facts. The facts can be obtained from multiple sources: an entity's knowledge base entry, the infobox on its Wikipedia page, or its row within a WebTable. Crucially, the informativeness of a fact depends not only on the entity but also the specific context(e.g., the query).To the best of our knowledge, this paper is the first to study the problem of contextual fact ranking: given some entities and a context (i.e., succinct natural language description), identify the most informative facts for the entities collectively within the context.We propose to contextually rank the facts by exploiting deep learning techniques. In particular, we develop pointwise and pair-wise ranking models, using textual and statistical information for the given entities and context derived from their sources. We enhance the models by incorporating entity type information from an IsA (hypernym) database. We demonstrate that our approaches achieve better performance than state-of-the-art baselines in terms of MAP, NDCG, and recall. We further conduct user studies for two specific applications of contextual fact ranking-table synthesis and table compression-and show that our models can identify more informative facts than the baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2641121",
                    "name": "Silu Huang"
                },
                {
                    "authorId": "2746747",
                    "name": "Jialu Liu"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "1524732527",
                    "name": "Xuezhi Wang"
                },
                {
                    "authorId": "1557391861",
                    "name": "You Wu"
                },
                {
                    "authorId": "2070782694",
                    "name": "Dale Markowitz"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "185a17a95a3ab77f2f7850d5f977abaa032098e3",
            "title": "Automatically Generating Interesting Facts from Wikipedia Tables",
            "abstract": "Modern search engines provide contextual information surrounding query entities beyond ten blue links in the form of information cards. Among the various attributes displayed about entities there has been recent interest in providing fun facts. Obtaining such trivia at a large scale is, however, non-trivial: hiring professional content creators is expensive and extracting statements from the Web is prone to uninteresting, out-of-context and/or unreliable facts. In this paper we show how fun facts can be mined from superlative tables in Wikipedia, whose rows are ranked according to some statistics, to provide a large volume of reliable and interesting content. We employ a template-based approach to semi-automatically generate natural language statements as fun facts. We show how to bootstrap and streamline the process for faster and cheaper task completion. However, the content contained in these tables is dynamic. Therefore, we address the problem of automatically maintaining the pairing of templates to tables as the tables are updated over time. Fun facts produced by our work is now part of Google's production search results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "1524732527",
                    "name": "Xuezhi Wang"
                },
                {
                    "authorId": "1557391861",
                    "name": "You Wu"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "1d63e7131c504949eef4aae1dc3349179ec8439a",
            "title": "Introduction to the Special Issue on Combating Digital Misinformation and Disinformation",
            "abstract": "We are delighted to present this special issue of the Journal of Data and Information Quality (ACM JDIQ) on Combating Digital Misinformation and Disinformation. This issue presents an overview of innovative research primarily at the intersection of information credibility, machine learning, and data science, from theory to practice, with a focus on combating misinformation and disinformation. Spread of misinformation and disinformation is one of the most serious challenges facing the news industry, and a threat to democracy worldwide. The problem has reached an unprecedented level via social media, where contents can be created and disseminated to a large audience with little to zero cost and revenues are driven by clicks. Researchers from multiple disciplines have proposed various strategies, built automated and semiautomated systems [1, 3], and recommended policy changes across the media ecosystem [2, 4]. Recently, researchers also explored how artificial intelligence techniques, particularly machine learning and natural language processing, can be leveraged to combat falsehoods online. In this special issue of JDIQ, we provide a representative collection of insightful articles at the intersection of data quality and credibility, from theory to practice, with a focus on improvements in veracity and value. The articles went through a rigorous procedure of review involving at least three expert reviewers for each article. After two rounds of review, we selected five articles that made contributions to both research and practice. Zannettou et al., in \u201cThe Web of False Information: Rumors, Fake News, Hoaxes, Clickbait, and Various Other Shenanigans,\u201d provide a typology of the false information content on the Web and surveys the latest research directions. It identifies several lines of works in the false information ecosystem. In particular, it surveys the research works from false information propagation, perception, and identification perspectives. Then, the authors specifically attend the false information spread in the political domain and investigate the velocity and consequence of the spread in communities. Finally, the authors delineate several future research directions that can help understand and mitigate this misinformation problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2789540",
                    "name": "Naeemul Hassan"
                },
                {
                    "authorId": "2128664093",
                    "name": "Chengkai Li"
                },
                {
                    "authorId": "2146157506",
                    "name": "Jun Yang"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "752f7d080ba12d604da487863895f9fc11e07e11",
            "title": "AggChecker: A Fact-Checking System for Text Summaries of Relational Data Sets",
            "abstract": "We demonstrate AggChecker, a novel tool for verifying textual summaries of relational data sets. The system automatically veri\ufb01es natural language claims about numerical aggregates against the underlying raw data. The system incorporates a combination of natural language processing, information retrieval, machine learning",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32184052",
                    "name": "Saehan Jo"
                },
                {
                    "authorId": "2636156",
                    "name": "Immanuel Trummer"
                },
                {
                    "authorId": "31003906",
                    "name": "Weicheng Yu"
                },
                {
                    "authorId": "1524732527",
                    "name": "Xuezhi Wang"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "2118822440",
                    "name": "Daniel Liu"
                },
                {
                    "authorId": "153784958",
                    "name": "Niyati Mehta"
                }
            ]
        },
        {
            "paperId": "809fa5f2ae9bd78b93ac038e33528296413b134c",
            "title": "Structured Data Meets News",
            "abstract": "The news ecosystem is going through profound changes that will have long-lasting impact on our civic society. One of the key challenges in the news ecosystem is how to encourage content consumption that will bring informational values to the users instead of purely consuming users' attention via any (i.e., unhealthy) means necessary. As part of the talk, I will describe how structured data can play an important role in helping users consume content in a more healthy way and present my long term vision on how our research community can contribute to this important cause. In the rest of the talk, I will describe various technical efforts, within the Structured Data Research Group at Google AI and in partnership with many product teams, on improving news consumption at Google. One specific example is the question and answering summarization of news articles, where we combine structured data, machine learning, and natural language processing techniques to help users understand news articles quickly. Short Bio: Cong Yu is a research scientist and manager at Google Research in New York City and leads the Structured Data Research Group. The group\u2019s mission is to understand and leverage structured data on the Web to enhance user experience for Google products and has been responsible for several impactful products such as Web Tables, Structured Snippets, and Fact Checking at Google. Currently, his group focuses on technical research for news and has been partnering with journalists and policy advisors to combat online misinformation and polarization and to improve news consumption. His research interests are structured data exploration and mining, computational journalism, machine learning, natural language processing, and scalable data analysis. He was a keynote speaker for VLDB 2019 and twice served as an industrial program co-chair for VLDB (2014 and 2018). Outside of Google, he periodically teaches at NYU Courant's Department of Computer Science. Before Google, Cong was a Research Scientist at Yahoo! Research, also in NYC. He has a PhD from the University of Michigan, Ann Arbor, advised by Prof. H.V. Jagadish. ceur-ws.org/Vol-2399/keynote2.pdf",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "e57d937679672971c7150644743724a0856227a5",
            "title": "Mining an \"Anti-Knowledge Base\" from Wikipedia Updates with Applications to Fact Checking and Beyond",
            "abstract": "We introduce the problem of anti-knowledge mining. Our goal is to create an \"anti-knowledge base\" that contains factual mistakes. The resulting data can be used for analysis, training, and benchmarking in the research domain of automated fact checking. Prior data sets feature manually generated fact checks of famous misclaims. Instead, we focus on the long tail of factual mistakes made by Web authors, ranging from erroneous sports results to incorrect capitals.\n We mine mistakes automatically, by an unsupervised approach, from Wikipedia updates that correct factual mistakes. Identifying such updates (only a small fraction of the total number of updates) is one of the primary challenges. We mine anti-knowledge by a multi-step pipeline. First, we filter out candidate updates via several simple heuristics. Next, we correlate Wikipedia updates with other statements made on the Web. Using claim occurrence frequencies as input to a probabilistic model, we infer the likelihood of corrections via an iterative expectation-maximization approach. Finally, we extract mistakes in the form of subject-predicate-object triples and rank them according to several criteria. Our end result is a data set containing over 110,000 ranked mistakes with a precision of 85% in the top 1% and a precision of over 60% in the top 25%. We demonstrate that baselines achieve significantly lower precision. Also, we exploit our data to verify several hypothesis on why users make mistakes. We finally show that the AKB can be used to find mistakes on the entire Web.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065163409",
                    "name": "Georgios T. Karagiannis"
                },
                {
                    "authorId": "2636156",
                    "name": "Immanuel Trummer"
                },
                {
                    "authorId": "32184052",
                    "name": "Saehan Jo"
                },
                {
                    "authorId": "2068653411",
                    "name": "Shubham Khandelwal"
                },
                {
                    "authorId": "1524732527",
                    "name": "Xuezhi Wang"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "5c5a252bc679b8078ac17d8fca7991810beb770e",
            "title": "Investigating Rumor News Using Agreement-Aware Search",
            "abstract": "Recent years have witnessed a widespread increase of rumor news generated by humans and machines. Therefore, tools for investigating rumor news have become an urgent necessity. One useful function of such tools is to see ways a specific topic or event is represented by presenting different points of view from multiple sources. In this paper, we propose Maester, a novel agreement-aware search framework for investigating rumor news. Given an investigative question, Maester will retrieve related articles to that question, assign and display top articles from agree, disagree, and discuss categories to users. Splitting the results into these three categories provides the user a holistic view towards the investigative question. We build Maester based on the following two key observations: (1) relatedness can commonly be determined by keywords and entities occurring in both questions and articles, and (2) the level of agreement between the investigative question and the related news article can often be decided by a few key sentences. Accordingly, we use gradient boosting tree models with keyword/entity matching features for relatedness detection, and leverage recurrent neural network to infer the level of agreement. Our experiments on the Fake News Challenge (FNC) dataset demonstrate up to an order of magnitude improvement of Maester over the original FNC winning solution, for agreement-aware search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "116654429",
                    "name": "Tianhang Sun"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "30575502",
                    "name": "Xingbang Liu"
                },
                {
                    "authorId": "3202556",
                    "name": "Anja Gruenheid"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "143828990",
                    "name": "\u00c1. Lelkes"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "e95cfc2233db60317809f86e41f3694d71e69e80",
            "title": "Verifying Text Summaries of Relational Data Sets",
            "abstract": "We present a novel natural language query interface, the AggChecker, aimed at text summaries of relational data sets. The tool focuses on natural language claims that translate into an SQL query and a claimed query result. Similar in spirit to a spell checker, the AggChecker marks up text passages that seem to be inconsistent with the actual data. At the heart of the system is a probabilistic model that reasons about the input document in a holistic fashion. Based on claim keywords and the document structure, it maps each text claim to a probability distribution over associated query translations. By efficiently executing tens to hundreds of thousands of candidate translations for a typical input document, the system maps text claims to correctness probabilities. This process becomes practical via a specialized processing backend, avoiding redundant work via query merging and result caching. Verification is an interactive process in which users are shown tentative results, enabling them to take corrective actions if necessary. We tested our system on 53 publicly available articles containing 392 claims. Our tool revealed erroneous claims in roughly a third of test cases. Also, AggChecker compares favorably against several automated and semi-automated fact checking baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32184052",
                    "name": "Saehan Jo"
                },
                {
                    "authorId": "2636156",
                    "name": "Immanuel Trummer"
                },
                {
                    "authorId": "31003906",
                    "name": "Weicheng Yu"
                },
                {
                    "authorId": "1524732527",
                    "name": "Xuezhi Wang"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "2118822440",
                    "name": "Daniel Liu"
                },
                {
                    "authorId": "153784958",
                    "name": "Niyati Mehta"
                }
            ]
        },
        {
            "paperId": "2ae8e295b94b154d944e60110d9277b9348f2efd",
            "title": "Exploiting group recommendation functions for flexible preferences",
            "abstract": "We examine the problem of enabling the flexibility of updating one's preferences in group recommendation. In our setting, any group member can provide a vector of preferences that, in addition to past preferences and other group members' preferences, will be accounted for in computing group recommendation. This functionality is essential in many group recommendation applications, such as travel planning, online games, book clubs, or strategic voting, as it has been previously shown that user preferences may vary depending on mood, context, and company (i.e., other people in the group). Preferences are enforced in an feedback box that replaces preferences provided by the users by a potentially different feedback vector that is better suited for maximizing the individual satisfaction when computing the group recommendation. The feedback box interacts with a traditional recommendation box that implements a group consensus semantics in the form of Aggregated Voting or Least Misery, two popular aggregation functions for group recommendation. We develop efficient algorithms to compute robust group recommendations that are appropriate in situations where users have changing preferences. Our extensive empirical study on real world data-sets validates our findings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "227a4163f6702aadc0978d339f1cc2e966577f6a",
            "title": "MapRat: Meaningful Explanation, Interactive Exploration and Geo-Visualization of Collaborative Ratings",
            "abstract": "Collaborative rating sites such as IMDB and Yelp have become rich resources that users consult to form judgments about and choose from among competing items. Most of these sites either provide a plethora of information for users to interpret all by themselves or a simple overall aggregate information. Such aggregates (e.g., average rating over all users who have rated an item, aggregates along pre-defined dimensions, etc.) can not help a user quickly decide the desirability of an item. In this paper, we build a system MapRat that allows a user to explore multiple carefully chosen aggregate analytic details over a set of user demographics that meaningfully explain the ratings associated with item(s) of interest. MapRat allows a user to systematically explore, visualize and understand user rating patterns of input item(s) so as to make an informed decision quickly. In the demo, participants are invited to explore collaborative movie ratings for popular movies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "40308435",
                    "name": "Mahashweta Das"
                },
                {
                    "authorId": "2067694313",
                    "name": "Shrikant Desai"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "532ce3b2a17aed5908e332816eec70f1960fc39a",
            "title": "Who Tags What? An Analysis Framework",
            "abstract": "The rise of Web 2.0 is signaled by sites such as Flickr, del.icio.us, and YouTube, and social tagging is essential to their success. A typical tagging action involves three components, user, item (e.g., photos in Flickr), and tags (i.e., words or phrases). Analyzing how tags are assigned by certain users to certain items has important implications in helping users search for desired information. In this paper, we explore common analysis tasks and propose a dual mining framework for social tagging behavior mining. This framework is centered around two opposing measures, similarity and diversity, being applied to one or more tagging components, and therefore enables a wide range of analysis scenarios such as characterizing similar users tagging diverse items with similar tags, or diverse users tagging similar items with diverse tags, etc. By adopting different concrete measures for similarity and diversity in the framework, we show that a wide range of concrete analysis problems can be defined and they are NP-Complete in general. We design efficient algorithms for solving many of those problems and demonstrate, through comprehensive experiments over real data, that our algorithms significantly out-perform the exact brute-force approach without compromising analysis result quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40308435",
                    "name": "Mahashweta Das"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "1ef6d442bd3928c54b162d3cb3c65f04b54c8975",
            "title": "MRI",
            "abstract": "Collaborative rating sites have become essential resources that many users consult to make purchasing decisions on various items. Ideally, a user wants to quickly decide whether an item is desirable, especially when many choices are available. In practice, however, a user either spends a lot of time examining reviews before making an informed decision, or simply trusts overall rating aggregations associated with an item. In this paper, we argue that neither option is satisfactory and propose a novel and powerful third option, Meaningful Ratings Interpretation (MRI), that automatically provides a meaningful interpretation of ratings associated with the input items. As a simple example, given the movie \"Usual Suspects,\" instead of simply showing the average rating of 8.7 from all reviewers, MRI produces a set of meaningful factoids such as \"male reviewers under 30 from NYC love this movie\". We define the notion of meaningful interpretation based on the idea of data cube, and formalize two important sub-problems, meaningful description mining and meaningful difference mining. We show that these problems are NP-hard and design randomized hill exploration algorithms to solve them efficiently. We conduct user studies to show that MRI provides more helpful information to users than simple average ratings. Performance evaluation over real data shows that our algorithms perform much faster and generate equally good interpretations as brute-force algorithms.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "40308435",
                    "name": "Mahashweta Das"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "281a8e360e3dccf986483a772ecc6fd4fa2a030e",
            "title": "Interactive itinerary planning",
            "abstract": "Planning an itinerary when traveling to a city involves substantial effort in choosing Points-of-Interest (POIs), deciding in which order to visit them, and accounting for the time it takes to visit each POI and transit between them. Several online services address different aspects of itinerary planning but none of them provides an interactive interface where users give feedbacks and iteratively construct their itineraries based on personal interests and time budget. In this paper, we formalize interactive itinerary planning as an iterative process where, at each step: (1) the user provides feedback on POIs selected by the system, (2) the system recommends the best itineraries based on all feedback so far, and (3) the system further selects a new set of POIs, with optimal utility, to solicit feedback for, at the next step. This iterative process stops when the user is satisfied with the recommended itinerary. We show that computing an itinerary is NP-complete even for simple itinerary scoring functions, and that POI selection is NP-complete. We develop heuristics and optimizations for a specific case where the score of an itinerary is proportional to the number of desired POIs it contains. Our extensive experiments show that our algorithms are efficient and return high quality itineraries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "598eb6f20fa5488865425332944b7d889561dd46",
            "title": "Repeatability and workability evaluation of SIGMOD 2011",
            "abstract": "SIGMOD has offered, since 2008, to verify the experiments published in the papers accepted at the conference. This year, we have been in charge of reproducing the experiments provided by the authors (repeatability), and exploring changes to experiment parameters (workability). In this paper, we assess the SIGMOD repeatability process in terms of participation, review process and results. While the participation is stable in terms of number of submissions, we find this year a sharp contrast between the high participation from Asian authors and the low participation from American authors. We also find that most experiments are distributed as Linux packages accompanied by instructions on how to setup and run the experiments. We are still far from the vision of executable papers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1748177",
                    "name": "Philippe Bonnet"
                },
                {
                    "authorId": "1702550",
                    "name": "S. Manegold"
                },
                {
                    "authorId": "1403904607",
                    "name": "Matias Bj\u00f8rling"
                },
                {
                    "authorId": "2075436748",
                    "name": "Wei Cao"
                },
                {
                    "authorId": "2115205453",
                    "name": "Javier Gonz\u00e1lez"
                },
                {
                    "authorId": "1758178",
                    "name": "Joel A. Granados"
                },
                {
                    "authorId": "2053923028",
                    "name": "Nancy E. Hall"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "2065721373",
                    "name": "M. Ivanova"
                },
                {
                    "authorId": "40240897",
                    "name": "Ryan Johnson"
                },
                {
                    "authorId": "1766755",
                    "name": "D. Koop"
                },
                {
                    "authorId": "1746961",
                    "name": "Tim Kraska"
                },
                {
                    "authorId": "145013980",
                    "name": "Ren\u00e9 M\u00fcller"
                },
                {
                    "authorId": "1756801",
                    "name": "Dan Olteanu"
                },
                {
                    "authorId": "1802817",
                    "name": "Paolo Papotti"
                },
                {
                    "authorId": "2060683457",
                    "name": "Christine Reilly"
                },
                {
                    "authorId": "3221184",
                    "name": "Dimitris Tsirogiannis"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "1695878",
                    "name": "D. Shasha"
                }
            ]
        },
        {
            "paperId": "28761b000695ed7bdd3171e7e78eb0ae7619a573",
            "title": "Entity-relationship queries over wikipedia",
            "abstract": "Wikipedia is the largest user-generated knowledge base. We propose a structured query mechanism, entity-relationship query, for searching entities in Wikipedia corpus by their properties and inter-relationships. An entity-relationship query consists of arbitrary number of predicates on desired entities. The semantics of each predicate is specified with keywords. Entity-relationship query searches entities directly over text rather than pre-extracted structured data stores. This characteristic brings two benefits: (1) Query semantics can be intuitively expressed by keywords; (2) It avoids information loss that happens during extraction. We present a ranking framework for general entity-relationship queries and a position-based Bounded Cumulative Model for accurate ranking of query answers. Experiments on INEX benchmark queries and our own crafted queries show the effectiveness and accuracy of our ranking method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2136101243",
                    "name": "Xiaonan Li"
                },
                {
                    "authorId": "2128664093",
                    "name": "Chengkai Li"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "7268638e1dd65368dbab0fa344bda112b74b6474",
            "title": "Structured Querying of Annotation-Rich Web Text with Shallow Semantics",
            "abstract": "Information discovery on the Web has so far been dominated by keyword-based document search. However, recent years have witnessed arising needs from Web users to search for named entities, e.g., finding all Silicon Valley companies. With existing Web search e ngines, users have to digest returned Web pages by themselves to find the answers. En tity search has been introduced as a solution to this problem. However, existing entity search sys tems are limited in their capability to address complex information needs that involve multiple entitie s and their interrelationships. In this report, we introduce a novel entity-centric structure d querying mechanism called Shallow Semantic Query (SSQ) to overcome this limitation. We cover two key technical issues with regard to SSQ, ranking and query processing. Comprehensive experiments show that (1) our ranking model beats state-of-the-art entity ranking method s; (2) the proposed query processing algorithm based on our new Entity-Centric Index is more effic ient than a baseline extended from existing entity search systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2136101243",
                    "name": "Xiaonan Li"
                },
                {
                    "authorId": "2128664093",
                    "name": "Chengkai Li"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                }
            ]
        }
    ]
}