{
    "authorId": "2149237236",
    "papers": [
        {
            "paperId": "0b74ad893b8b5f6054d9e05ae706edd53b26d6a5",
            "title": "Masked Graph Transformer for Large-Scale Recommendation",
            "abstract": "Graph Transformers have garnered significant attention for learning graph-structured data, thanks to their superb ability to capture long-range dependencies among nodes. However, the quadratic space and time complexity hinders the scalability of Graph Transformers, particularly for large-scale recommendation. Here we propose an efficient Masked Graph Transformer, named MGFormer, capable of capturing all-pair interactions among nodes with a linear complexity. To achieve this, we treat all user/item nodes as independent tokens, enhance them with positional embeddings, and feed them into a kernelized attention module. Additionally, we incorporate learnable relative degree information to appropriately reweigh the attentions. Experimental results show the superior performance of our MGFormer, even with a single attention layer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2243519158",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "3056465",
                    "name": "Chin-Chia Michael Yeh"
                },
                {
                    "authorId": "2223764628",
                    "name": "Vivian Lai"
                },
                {
                    "authorId": "2185013996",
                    "name": "Yan Zheng"
                },
                {
                    "authorId": "2285001003",
                    "name": "Minghua Xu"
                },
                {
                    "authorId": "2278450099",
                    "name": "Hanghang Tong"
                }
            ]
        },
        {
            "paperId": "4df52956b2801e4d216edc0ef0ea806e5ff2b59b",
            "title": "Discrete-state Continuous-time Diffusion for Graph Generation",
            "abstract": "Graph is a prevalent discrete data structure, whose generation has wide applications such as drug discovery and circuit design. Diffusion generative models, as an emerging research focus, have been applied to graph generation tasks. Overall, according to the space of states and time steps, diffusion generative models can be categorized into discrete-/continuous-state discrete-/continuous-time fashions. In this paper, we formulate the graph diffusion generation in a discrete-state continuous-time setting, which has never been studied in previous graph diffusion models. The rationale of such a formulation is to preserve the discrete nature of graph-structured data and meanwhile provide flexible sampling trade-offs between sample quality and efficiency. Analysis shows that our training objective is closely related to generation quality, and our proposed generation framework enjoys ideal invariant/equivariant properties concerning the permutation of node ordering. Our proposed model shows competitive empirical performance against state-of-the-art graph generation solutions on various benchmarks and, at the same time, can flexibly trade off the generation quality and efficiency in the sampling phase.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "2187298875",
                    "name": "Ruizhong Qiu"
                },
                {
                    "authorId": "2215477428",
                    "name": "Yuzhong Chen"
                },
                {
                    "authorId": "2284864789",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "2302364032",
                    "name": "Xiran Fan"
                },
                {
                    "authorId": "2265584592",
                    "name": "Menghai Pan"
                },
                {
                    "authorId": "2215437587",
                    "name": "Zhichen Zeng"
                },
                {
                    "authorId": "2273930014",
                    "name": "Mahashweta Das"
                },
                {
                    "authorId": "2278450099",
                    "name": "Hanghang Tong"
                }
            ]
        },
        {
            "paperId": "6d1d4c3d591a9a6cc020082ff588107c4a6ae450",
            "title": "Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-Link Diagrams",
            "abstract": "Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2152520175",
                    "name": "Qing Chen"
                },
                {
                    "authorId": "2118768185",
                    "name": "Nan Chen"
                },
                {
                    "authorId": "2005512798",
                    "name": "W. Shuai"
                },
                {
                    "authorId": "2153300135",
                    "name": "Guande Wu"
                },
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "2058143613",
                    "name": "H. Tong"
                },
                {
                    "authorId": "2059201609",
                    "name": "Nana Cao"
                }
            ]
        },
        {
            "paperId": "96a21ab57b483325c5f67d26bac7b2884ae2b104",
            "title": "Kernel Ridge Regression-Based Graph Dataset Distillation",
            "abstract": "The huge volume of emerging graph datasets has become a double-bladed sword for graph machine learning. On the one hand, it empowers the success of a myriad of graph neural networks (GNNs) with strong empirical performance. On the other hand, training modern graph neural networks on huge graph data is computationally expensive. How to distill the given graph dataset while retaining most of the trained models' performance is a challenging problem. Existing efforts try to approach this problem by solving meta-learning-based bilevel optimization objectives. A major hurdle lies in that the exact solutions of these methods are computationally intensive and thus, most, if not all, of them are solved by approximate strategies which in turn hurt the distillation performance. In this paper, inspired by the recent advances in neural network kernel methods, we adopt a kernel ridge regression-based meta-learning objective which has a feasible exact solution. However, the computation of graph neural tangent kernel is very expensive, especially in the context of dataset distillation. As a response, we design a graph kernel, named LiteGNTK, tailored for the dataset distillation problem which is closely related to the classic random walk graph kernel. An effective model named Kernel r\u0131dge regression-based graph Dataset Distillation (KIDD) and its variants are proposed. KIDD shows nice efficiency in both the forward and backward propagation processes. At the same time, KIDD shows strong empirical performance over 7 real-world datasets compared with the state-of-the-art distillation methods. Thanks to the ability to find the exact solution of the distillation objective, the learned training graphs by KIDD can sometimes even outperform the original whole training set with as few as 1.65% training graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "2215477428",
                    "name": "Yuzhong Chen"
                },
                {
                    "authorId": "29913565",
                    "name": "Menghai Pan"
                },
                {
                    "authorId": "1504511015",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "40308435",
                    "name": "Mahashweta Das"
                },
                {
                    "authorId": "2145058012",
                    "name": "Hao Yang"
                },
                {
                    "authorId": "2058143613",
                    "name": "H. Tong"
                }
            ]
        },
        {
            "paperId": "9abcfea0022abe29e6c1fc855c0782701ee8c9e2",
            "title": "Invariant Graph Transformer",
            "abstract": "Rationale discovery is defined as finding a subset of the input data that maximally supports the prediction of downstream tasks. In graph machine learning context, graph rationale is defined to locate the critical subgraph in the given graph topology, which fundamentally determines the prediction results. In contrast to the rationale subgraph, the remaining subgraph is named the environment subgraph. Graph rationalization can enhance the model performance as the mapping between the graph rationale and prediction label is viewed as invariant, by assumption. To ensure the discriminative power of the extracted rationale subgraphs, a key technique named\"intervention\"is applied. The core idea of intervention is that given any changing environment subgraphs, the semantics from the rationale subgraph is invariant, which guarantees the correct prediction result. However, most, if not all, of the existing rationalization works on graph data develop their intervention strategies on the graph level, which is coarse-grained. In this paper, we propose well-tailored intervention strategies on graph data. Our idea is driven by the development of Transformer models, whose self-attention module provides rich interactions between input nodes. Based on the self-attention module, our proposed invariant graph Transformer (IGT) can achieve fine-grained, more specifically, node-level and virtual node-level intervention. Our comprehensive experiments involve 7 real-world datasets, and the proposed IGT shows significant performance advantages compared to 13 baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "2265584592",
                    "name": "Menghai Pan"
                },
                {
                    "authorId": "2215477428",
                    "name": "Yuzhong Chen"
                },
                {
                    "authorId": "2274061283",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "2274018166",
                    "name": "Yuchen Yan"
                },
                {
                    "authorId": "2273930014",
                    "name": "Mahashweta Das"
                },
                {
                    "authorId": "2278450099",
                    "name": "Hanghang Tong"
                }
            ]
        },
        {
            "paperId": "b42eb1d02f6ba5f9dc61251399cca91d7d723b0b",
            "title": "Node Classification Beyond Homophily: Towards a General Solution",
            "abstract": "Graph neural networks (GNNs) have become core building blocks behind a myriad of graph learning tasks. The vast majority of the existing GNNs are built upon, either implicitly or explicitly, the homophily assumption, which is not always true and could heavily degrade the performance of learning tasks. In response, GNNs tailored for heterophilic graphs have been developed. However, most of the existing works are designed for the specific GNN models to address heterophily, which lacks generality. In this paper, we study the problem from the structure learning perspective and propose a family of general solutions named ALT. It can work hand in hand with most of the existing GNNs to handle graphs with either low or high homophily. At the core of our method is learning to (1) decompose a given graph into two components, (2) extract complementary graph signals from these two components, and (3) adaptively integrate the graph signals for node classification. Moreover, analysis based on graph signal processing shows that our framework can empower a broad range of existing GNNs to have adaptive filter characteristics and further modulate the input graph signals, which is critical for handling complex homophilic/heterophilic patterns. The proposed ALT brings significant and consistent performance improvement in node classification for a wide range of GNNs over a variety of real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "2215477428",
                    "name": "Yuzhong Chen"
                },
                {
                    "authorId": "1491243049",
                    "name": "Qinghai Zhou"
                },
                {
                    "authorId": "2461369",
                    "name": "Yuhang Wu"
                },
                {
                    "authorId": "29913565",
                    "name": "Menghai Pan"
                },
                {
                    "authorId": "2145058012",
                    "name": "Hao Yang"
                },
                {
                    "authorId": "2058143613",
                    "name": "H. Tong"
                }
            ]
        },
        {
            "paperId": "e99320299c32316d2bb2cc875017210378c8b8b8",
            "title": "Class-Imbalanced Graph Learning without Class Rebalancing",
            "abstract": "Class imbalance is prevalent in real-world node classification tasks and poses great challenges for graph learning models. Most existing studies are rooted in a class-rebalancing (CR) perspective and address class imbalance with class-wise reweighting or resampling. In this work, we approach the root cause of class-imbalance bias from an topological paradigm. Specifically, we theoretically reveal two fundamental phenomena in the graph topology that greatly exacerbate the predictive bias stemming from class imbalance. On this basis, we devise a lightweight topological augmentation framework BAT to mitigate the class-imbalance bias without class rebalancing. Being orthogonal to CR, BAT can function as an efficient plug-and-play module that can be seamlessly combined with and significantly boost existing CR techniques. Systematic experiments on real-world imbalanced graph learning tasks show that BAT can deliver up to 46.27% performance gain and up to 72.74% bias reduction over existing techniques. Code, examples, and documentations are available at https://github.com/ZhiningLiu1998/BAT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "70995412",
                    "name": "Zhining Liu"
                },
                {
                    "authorId": "2215437587",
                    "name": "Zhichen Zeng"
                },
                {
                    "authorId": "2187298875",
                    "name": "Ruizhong Qiu"
                },
                {
                    "authorId": "2154454915",
                    "name": "Hyunsik Yoo"
                },
                {
                    "authorId": "2235338301",
                    "name": "David Zhou"
                },
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "24018646",
                    "name": "Yada Zhu"
                },
                {
                    "authorId": "2181934308",
                    "name": "Kommy Weldemariam"
                },
                {
                    "authorId": "37395525",
                    "name": "Jingrui He"
                },
                {
                    "authorId": "2058143613",
                    "name": "H. Tong"
                }
            ]
        },
        {
            "paperId": "ead3c3c9861cfcb8e10ba46f493add0d3782ddfd",
            "title": "Topological Augmentation for Class-Imbalanced Node Classification",
            "abstract": "Class imbalance is prevalent in real-world node classification tasks and often biases graph learning models toward majority classes. Most existing studies root from a node-centric perspective and aim to address the class imbalance in training data by node/class-wise reweighting or resampling. In this paper, we approach the source of the class-imbalance bias from an under-explored topology-centric perspective. Our investigation reveals that beyond the inherently skewed training class distribution, the graph topology also plays an important role in the formation of predictive bias: we identify two fundamental challenges, namely ambivalent and distant message-passing , that can exacerbate the bias by aggravating majority-class over-generalization and minority-class misclassification. In light of these findings, we devise a lightweight topological augmentation method T O BA to dynamically rectify the nodes influenced by ambivalent/distant message-passing during graph learning, so as to mitigate the class-imbalance bias. We highlight that T O BA is a model-agnostic, efficient, and versatile solution that can be seamlessly combined with and further boost other imbalance-handling techniques. Systematic experiments validate the superior performance of T O BA in both promoting imbalanced node classification and mitigating the prediction bias between different classes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257083296",
                    "name": "Zhining Liu"
                },
                {
                    "authorId": "2260609300",
                    "name": "Z. Zeng"
                },
                {
                    "authorId": "2187298875",
                    "name": "Ruizhong Qiu"
                },
                {
                    "authorId": "2154454915",
                    "name": "Hyunsik Yoo"
                },
                {
                    "authorId": "2235338301",
                    "name": "David Zhou"
                },
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "2297766807",
                    "name": "Yada Zhu"
                },
                {
                    "authorId": "2181934308",
                    "name": "Kommy Weldemariam"
                },
                {
                    "authorId": "2303885774",
                    "name": "Jingrui He"
                },
                {
                    "authorId": "2294365662",
                    "name": "Hanghang Tong"
                }
            ]
        },
        {
            "paperId": "f86beb070f8d4d1ae6d40b012a9f63c087b3bb05",
            "title": "Natural and Artificial Dynamics in GNNs: A Tutorial",
            "abstract": "In the big data era, the relationship between entities becomes more complex. Therefore, graph (or network) data attracts increasing research attention for carrying complex relational information. For a myriad of graph mining/learning tasks, graph neural networks (GNNs) have been proven as effective tools for extracting informative node and graph representations, which empowers a broad range of applications such as recommendation, fraud detection, molecule design, and many more. However, real-world scenarios bring pragmatic challenges to GNNs. First, the input graphs are evolving, i.e., the graph structure and node features are time-dependent. Integrating temporal information into the GNNs to enhance their representation power requires additional ingenious designs. Second, the input graphs may be unreliable, noisy, and suboptimal for a variety of downstream graph mining/learning tasks. How could end-users deliberately modify the given graphs (e.g., graph topology and node features) to boost GNNs' utility (e.g., accuracy and robustness)? Inspired by the above two kinds of dynamics, in this tutorial, we focus on topics of natural dynamics and artificial dynamics in GNNs and introduce the related works systematically. After that, we point out some promising but under-explored research problems in the combination of these two dynamics. We hope this tutorial could be beneficial to researchers and practitioners in areas including data mining, machine learning, and general artificial intelligence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1893402501",
                    "name": "Dongqi Fu"
                },
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "2058143613",
                    "name": "H. Tong"
                },
                {
                    "authorId": "31108652",
                    "name": "Jingrui He"
                }
            ]
        },
        {
            "paperId": "0c9acd8719380c40760dab42c16072c0c43b91d9",
            "title": "Learning Optimal Propagation for Graph Neural Networks",
            "abstract": "Graph Neural Networks (GNNs) have achieved tremendous success in a variety of real-world applications by relying on the \ufb01xed graph data as input. However, the initial input graph might not be optimal in terms of speci\ufb01c downstream tasks, because of information scarcity, noise, adversarial attacks, or discrepancies between the distribution in graph topology, features, and groundtruth labels. In this paper, we propose a bi-level optimization-based approach for learning the optimal graph structure via directly learning the Personalized PageRank propagation matrix as well as the downstream semi-supervised node classi\ufb01cation simultaneously. We also explore a low-rank approximation model for further reducing the time complexity. Empirical evaluations show the superior ef\ufb01cacy and robustness of the proposed model over all baseline meth-ods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1959090837",
                    "name": "Beidi Zhao"
                },
                {
                    "authorId": "22607329",
                    "name": "Boxin Du"
                },
                {
                    "authorId": "2149237236",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "2897748",
                    "name": "Liangyue Li"
                },
                {
                    "authorId": "2256983432",
                    "name": "Hanghang Tong"
                }
            ]
        }
    ]
}