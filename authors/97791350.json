{
    "authorId": "97791350",
    "papers": [
        {
            "paperId": "3ca5af81c40fad6d61a4a8bc22c5fb83f4bb3dce",
            "title": "Multilingual Neural Machine Translation with Deep Encoder and Multiple Shallow Decoders",
            "abstract": "Recent work in multilingual translation advances translation quality surpassing bilingual baselines using deep transformer models with increased capacity. However, the extra latency and memory costs introduced by this approach may make it unacceptable for efficiency-constrained applications. It has recently been shown for bilingual translation that using a deep encoder and shallow decoder (DESD) can reduce inference latency while maintaining translation quality, so we study similar speed-accuracy trade-offs for multilingual translation. We find that for many-to-one translation we can indeed increase decoder speed without sacrificing quality using this approach, but for one-to-many translation, shallow decoders cause a clear quality drop. To ameliorate this drop, we propose a deep encoder with multiple shallow decoders (DEMSD) where each shallow decoder is responsible for a disjoint subset of target languages. Specifically, the DEMSD model with 2-layer decoders is able to obtain a 1.8x speedup on average compared to a standard transformer model with no drop in translation quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "97791350",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "3286437",
                    "name": "Adithya Renduchintala"
                },
                {
                    "authorId": "2059363961",
                    "name": "James Cross"
                },
                {
                    "authorId": "1825565215",
                    "name": "Yuqing Tang"
                },
                {
                    "authorId": "3016273",
                    "name": "Jiatao Gu"
                },
                {
                    "authorId": "2116235416",
                    "name": "Xian Li"
                }
            ]
        },
        {
            "paperId": "70e91e16eb321067d9402710e14a40cf28311f73",
            "title": "Mega: Moving Average Equipped Gated Attention",
            "abstract": "The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "2110714400",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "97791350",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "6215698",
                    "name": "Junxian He"
                },
                {
                    "authorId": "1970583",
                    "name": "Liangke Gui"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "143823227",
                    "name": "Jonathan May"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ]
        },
        {
            "paperId": "3e384108a9cb09acfaa96c38aa1502fbf08ab771",
            "title": "Decoupling Global and Local Representations via Invertible Generative Flows",
            "abstract": "In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative \ufb02ow in the VAE framework to model the decoder. Speci\ufb01cally, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a \ufb02ow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at https://github.com/XuezheMax/wolf .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "97791350",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "2437353",
                    "name": "Shanghang Zhang"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        },
        {
            "paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
            "title": "Luna: Linear Unified Nested Attention",
            "abstract": "The quadratic computational and memory complexities of the Transformer's attention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. Specifically, with the first attention function, Luna packs the input sequence into a sequence of fixed length. Then, the packed sequence is unpacked using the second attention function. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efficiency of Luna compared to a variety",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "97791350",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "2116420716",
                    "name": "Sinong Wang"
                },
                {
                    "authorId": "2110714400",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "143823227",
                    "name": "Jonathan May"
                },
                {
                    "authorId": "2110815489",
                    "name": "Hao Ma"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ]
        },
        {
            "paperId": "0805cb1b26577f08f84190445992f7f0584e4742",
            "title": "OPERA: Operations-oriented Probabilistic Extraction, Reasoning, and Analysis",
            "abstract": "The OPERA system of CMU and USC/ISI performs end-to-end information extraction from multiple media and languages (English, Russian, Ukrainian), integrates the results, builds Knowledge Bases about the domain, and does hypothesis creation and reasoning to answer questions. ",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                },
                {
                    "authorId": "2284176",
                    "name": "Hans Chalupsky"
                },
                {
                    "authorId": "145001267",
                    "name": "A. Gershman"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "1706595",
                    "name": "T. Mitamura"
                },
                {
                    "authorId": "38599655",
                    "name": "Zaid A. W. Sheikh"
                },
                {
                    "authorId": "1741515",
                    "name": "Ankit Dangi"
                },
                {
                    "authorId": "51250894",
                    "name": "Aditi Chaudhary"
                },
                {
                    "authorId": "2135112672",
                    "name": "Xianyang Chen"
                },
                {
                    "authorId": "97791350",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "1410241246",
                    "name": "Bernie Huang"
                },
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "2109279237",
                    "name": "H. Liu"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "1410648718",
                    "name": "Maria Ryskina"
                },
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "2126048085",
                    "name": "Varun Gangal"
                }
            ]
        },
        {
            "paperId": "8b0582bc8cf882ecaebeb0336b2b3876e790c98b",
            "title": "Decompressing Knowledge Graph Representations for Link Prediction",
            "abstract": "This paper studies the problem of predicting missing relationships between entities in knowledge graphs through learning their representations. Currently, the majority of existing link prediction models employ simple but intuitive scoring functions and relatively small embedding size so that they could be applied to large-scale knowledge graphs. However, these properties also restrict the ability to learn more expressive and robust features. Therefore, diverging from most of the prior works which focus on designing new objective functions, we propose, DeCom, a simple but effective mechanism to boost the performance of existing link predictors such as DistMult, ComplEx, etc, through extracting more expressive features while preventing overfitting by adding just a few extra parameters. Specifically, embeddings of entities and relationships are first decompressed to a more expressive and robust space by decompressing functions, then knowledge graph embedding models are trained in this new feature space. Experimental results on several benchmark knowledge graphs and advanced link prediction systems demonstrate the generalization and effectiveness of our method. Especially, RESCAL + DeCom achieves state-of-the-art performance on the FB15k-237 benchmark across all evaluation metrics. In addition, we also show that compared with DeCom, explicitly increasing the embedding size significantly increase the number of parameters but could not achieve promising performance improvement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "97791350",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "2135112672",
                    "name": "Xianyang Chen"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        }
    ]
}