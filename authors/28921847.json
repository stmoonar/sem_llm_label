{
    "authorId": "28921847",
    "papers": [
        {
            "paperId": "ad4eb803ff485acdac07d3358bfb097694003159",
            "title": "Data-Efficient Low-Complexity Acoustic Scene Classification in the DCASE 2024 Challenge",
            "abstract": "This article describes the Data-Efficient Low-Complexity Acoustic Scene Classification Task in the DCASE 2024 Challenge and the corresponding baseline system. The task setup is a continuation of previous editions (2022 and 2023), which focused on recording device mismatches and low-complexity constraints. This year's edition introduces an additional real-world problem: participants must develop data-efficient systems for five scenarios, which progressively limit the available training data. The provided baseline system is based on an efficient, factorized CNN architecture constructed from inverted residual blocks and uses Freq-MixStyle to tackle the device mismatch problem. The task received 37 submissions from 17 teams, with the large majority of systems outperforming the baseline. The top-ranked system's accuracy ranges from 54.3% on the smallest to 61.8% on the largest subset, corresponding to relative improvements of approximately 23% and 9% over the baseline system on the evaluation set.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261390466",
                    "name": "Florian Schmid"
                },
                {
                    "authorId": "12746771",
                    "name": "Paul Primus"
                },
                {
                    "authorId": "2373836",
                    "name": "Toni Heittola"
                },
                {
                    "authorId": "145022667",
                    "name": "A. Mesaros"
                },
                {
                    "authorId": "2301453752",
                    "name": "Irene Mart'in-Morat'o"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "2261390402",
                    "name": "Gerhard Widmer"
                }
            ]
        },
        {
            "paperId": "03627e32a048ba71ad6ed632df2f3669464a7dae",
            "title": "Domain Information Control at Inference Time for Acoustic Scene Classification",
            "abstract": "Domain shift is considered a challenge in machine learning as it causes significant degradation of model performance. In the Acoustic Scene Classification task (ASC), domain shift is mainly caused by different recording devices. Several studies have already targeted domain generalization to improve the performance of ASC models on unseen domains, such as new devices. Recently, the Controllable Gate Adapter (CONGATER) has been proposed in Natural Language Processing to address the biased training data problem. CONGATER allows controlling the debiasing process at inference time. CONGATER's main advantage is the continuous and selective debiasing of a trained model, during inference. In this work, we adapt CONGATER to the audio spectrogram transformer for an acoustic scene classification task. We show that CONGATER can be used to selectively adapt the learned representations to be invariant to device domain shifts such as recording devices. Our analysis shows that CONGATER can progressively remove device information from the learned representations and improve the model generalization, especially under domain shift conditions (e.g. unseen devices). We show that information removal can be extended to both device and location domain. Finally, we demonstrate CONGATER's ability to enhance specific device performance without further training11Source Code: https://github.com/ShawMaskldcase22_CONGATER.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2184114298",
                    "name": "Shahed Masoudian"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "144125621",
                    "name": "M. Schedl"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                },
                {
                    "authorId": "2844293",
                    "name": "Navid Rekabsaz"
                }
            ]
        },
        {
            "paperId": "3a2605cec29c0439a4d7975cf4188b82ffd82327",
            "title": "Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets",
            "abstract": "This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12746771",
                    "name": "Paul Primus"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "a031181a2902e1236aeda7e36e6d012a0d5ee0df",
            "title": "Dynamic Convolutional Neural Networks as Efficient Pre-Trained Audio Models",
            "abstract": "The introduction of large-scale audio datasets, such as AudioSet, paved the way for Transformers to conquer the audio domain and replace CNNs as the state-of-the-art neural network architecture for many tasks. Audio Spectrogram Transformers are excellent at exploiting large datasets, creating powerful pre-trained models that surpass CNNs when fine-tuned on downstream tasks. However, current popular Audio Spectrogram Transformers are demanding in terms of computational complexity compared to CNNs. Recently, we have shown that, by employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch up with and even outperform Transformers on large datasets. In this work, we extend this line of research and increase the capacity of efficient CNNs by introducing dynamic CNN blocks constructed of dynamic convolutions, a dynamic ReLU activation function, and Coordinate Attention. We show that these dynamic CNNs outperform traditional efficient CNNs, such as MobileNets, in terms of the performance\u2013complexity trade-off at the task of audio tagging on the large-scale AudioSet. Our experiments further indicate that the proposed dynamic CNNs achieve competitive performance with Transformer-based models for end-to-end fine-tuning on downstream tasks while being much more computationally efficient.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2261390466",
                    "name": "Florian Schmid"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "2261390402",
                    "name": "Gerhard Widmer"
                }
            ]
        },
        {
            "paperId": "d494ec73981928cd7b15484a27ef493ff5e31728",
            "title": "Device-Robust Acoustic Scene Classification via Impulse Response Augmentation",
            "abstract": "The ability to generalize to a wide range of recording devices is a crucial performance factor for audio classification models. The characteristics of different types of microphones introduce distributional shifts in the digitized audio signals due to their varying frequency responses. If this domain shift is not taken into account during training, the model's performance could degrade severely when it is applied to signals recorded by unseen devices. In particular, training a model on audio signals recorded with a small number of different microphones can make generalization to unseen devices difficult. To tackle this problem, we convolve audio signals in the training set with pre-recorded device impulse responses (DIRs) to artificially increase the diversity of recording devices. We systematically study the effect of DIR augmentation on the task of Acoustic Scene Classification using CNNs and Audio Spectrogram Transformers. The results show that DIR augmentation in isolation performs similarly to the state-of-the-art method Freq-MixStyle. However, we also show that DIR augmentation and Freq-MixStyle are complementary, achieving a new state-of-the-art performance on signals recorded by devices unseen during training.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2103245223",
                    "name": "Tobias Morocutti"
                },
                {
                    "authorId": "49739808",
                    "name": "Florian Schmid"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "f7acab7b005915cc086137f87e265242da6ae3eb",
            "title": "Low-Complexity Audio Embedding Extractors",
            "abstract": "Solving tasks such as speaker recognition, music classification, or semantic audio event tagging with deep learning models typically requires computationally demanding networks. General-purpose audio embed dings (GPAEs) are dense representations of audio signals that allow lightweight, shallow classifiers to tackle various audio tasks. The idea is that a single complex feature extractor would extract dense GPAEs, while shallow MLPs can produce task-specific predictions. If the extracted dense representations are general enough to allow the simple downstream classifiers to generalize to a variety of tasks in the audio domain, a single costly forward pass suffices to solve multiple tasks in parallel. In this work, we try to reduce the cost of GPAE extractors to make them suitable for resource-constrained devices. We use efficient MobileNets trained on AudioSet using Knowledge Distillation from a Transformer ensemble as efficient GPAE extractors. We explore how to obtain high-quality GPAEs from the model, study how model complexity relates to the quality of extracted GPAEs, and conclude that low-complexity models can generate competitive GPAEs, paving the way for analyzing audio streams on edge devices w.r.t. multiple audio classification and recognition tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "49739808",
                    "name": "Florian Schmid"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "206d8afa93a3afcd1d6f327bc898eb41aa8b3e59",
            "title": "Knowledge Distillation from Transformers for Low-Complexity Acoustic Scene Classification",
            "abstract": "Knowledge Distillation (KD) is known for its ability to compress large models into low-complexity solutions while preserving high predictive performance. In Acoustic Scene Classification (ASC), this ability has recently been exploited successfully, as underlined by three of the top four systems in the low-complexity ASC task of the DCASE\u201821 challenge [1] relying on KD. Current KD solutions for ASC mainly use large-scale CNNs or specialist ensembles to derive superior teacher predictions. In this work, we use the Audio Spectrogram Transformer model PaSST , pre-trained on Audioset, as a teacher model. We show how the pre-trained PaSST model can be properly trained downstream on the TAU Urban Acoustic Scenes 2022 Mobile development dataset [2] and how to distill the knowledge into a low-complexity CNN student. We study the effect of using teacher ensembles, using teacher predictions on extended audio sequences, and using Audioset as an additional dataset for knowledge transfer. Additionally, we compare the effectiveness of Mixup and Freq-MixStyle to improve performance and enhance device generalization. The described system achieved rank 1 in the Low-complexity ASC Task of the DCASE\u201822 challenge [3] 1 .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49739808",
                    "name": "Florian Schmid"
                },
                {
                    "authorId": "2184114298",
                    "name": "Shahed Masoudian"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "65f1595bad9c04a01c2ec3da87467970ab363ded",
            "title": "Learning General Audio Representations with Large-Scale Training of Patchout Audio Transformers",
            "abstract": "The success of supervised deep learning methods is largely due to their ability to learn relevant features from raw data. Deep Neural Networks (DNNs) trained on large-scale datasets are capable of capturing a diverse set of features, and learning a representation that can generalize onto unseen tasks and datasets that are from the same domain. Hence, these models can be used as powerful feature extractors, in combination with shallower models as classifiers, for smaller tasks and datasets where the amount of training data is insufficient for learning an end-to-end model from scratch. During the past years, Convolutional Neural Networks (CNNs) have largely been the method of choice for audio processing. However, recently attention-based transformer models have demonstrated great potential in supervised settings, outperforming CNNs. In this work, we investigate the use of audio transformers trained on large-scale datasets to learn general-purpose representations. We study how the different setups in these audio transformers affect the quality of their embeddings. We experiment with the models' time resolution, extracted embedding level, and receptive fields in order to see how they affect performance on a variety of tasks and datasets, following the HEAR 2021 NeurIPS challenge evaluation setup. Our results show that representations extracted by audio transformers outperform CNN representations. Furthermore, we will show that transformers trained on Audioset can be extremely effective representation extractors for a wide range of downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "2184114298",
                    "name": "Shahed Masoudian"
                },
                {
                    "authorId": "49739808",
                    "name": "Florian Schmid"
                },
                {
                    "authorId": "1406798986",
                    "name": "Hamid Eghbalzadeh"
                },
                {
                    "authorId": "1389602735",
                    "name": "Jan Schluter"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "91e9ea2d23018ed62c5675e0abee226c2c08eee9",
            "title": "Efficient Large-Scale Audio Tagging Via Transformer-to-CNN Knowledge Distillation",
            "abstract": "Audio Spectrogram Transformer models rule the field of Audio Tagging, outrunning previously dominating Convolutional Neural Networks (CNNs). Their superiority is based on the ability to scale up and exploit large-scale datasets such as AudioSet. However, Transformers are demanding in terms of model size and computational requirements compared to CNNs. We propose a training procedure for efficient CNNs based on offline Knowledge Distillation (KD) from high-performing yet complex transformers. The proposed training schema and the efficient CNN design based on MobileNetV3 results in models outperforming previous solutions in terms of parameter and computational efficiency and prediction performance. We provide models of different complexity levels, scaling from low-complexity models up to a new state-of-the-art performance of .483 mAP on AudioSet.1",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "49739808",
                    "name": "Florian Schmid"
                },
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        },
        {
            "paperId": "528d1f017cf87144aecb43834b2712a2300a9190",
            "title": "Over-Parameterization and Generalization in Audio Classification",
            "abstract": "Convolutional Neural Networks (CNNs) have been dominating classification tasks in various domains, such as machine vision, machine listening, and natural language processing. In machine listening, while generally exhibiting very good generalization capabilities, CNNs are sensitive to the specific audio recording device used, which has been recognized as a substantial problem in the acoustic scene classification (DCASE) community. In this study, we investigate the relationship between over-parameterization of acoustic scene classification models, and their resulting generalization abilities. Our results indicate that increasing width improves generalization to unseen devices, even without an increase in the number of parameters.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "28921847",
                    "name": "Khaled Koutini"
                },
                {
                    "authorId": "1406798986",
                    "name": "Hamid Eghbalzadeh"
                },
                {
                    "authorId": "50509045",
                    "name": "Florian Henkel"
                },
                {
                    "authorId": "1389602735",
                    "name": "Jan Schluter"
                },
                {
                    "authorId": "145964711",
                    "name": "G. Widmer"
                }
            ]
        }
    ]
}