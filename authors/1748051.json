{
    "authorId": "1748051",
    "papers": [
        {
            "paperId": "c8525b6119a6ec89bd64a97a6ee5d7d377f8841c",
            "title": "CGF: Constrained Generation Framework for Query Rewriting in Conversational AI",
            "abstract": "In conversational AI agents, Query Rewriting 001 (QR) plays a crucial role in reducing users fric- 002 tions and satisfying their daily demands. Users 003 frictions are caused by various reasons, such 004 as errors in the spoken dialogue system, users\u2019 005 accent or their abridged language. In this work, 006 we present a novel Constrained Generation 007 Framework (CGF) for query rewriting at both 008 global and personalized level. The proposed 009 framework is based on the encoder-decoder 010 framework and consists of a context-enhanced 011 encoding and constrained generation decoding 012 phrases. The model takes the query and its 013 previous dialogue context information as the 014 encoder input, then the decoder relies on the 015 pre-defined global or personalized constrained 016 decoding space to generate the rewrites. Ex- 017 tensive offline and online A/B experimental re- 018 sults show that the proposed CGF significantly 019 boosts the query rewriting performance. 020",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                },
                {
                    "authorId": "2152797245",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2117800368",
                    "name": "Xing Fan"
                },
                {
                    "authorId": "2118973203",
                    "name": "Saurabh Gupta"
                },
                {
                    "authorId": "2805456",
                    "name": "Saleh Soltan"
                },
                {
                    "authorId": "147887099",
                    "name": "Rakesh Chada"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "1941997",
                    "name": "Chenlei Guo"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                }
            ]
        },
        {
            "paperId": "a4bc44a41b0ddfb70d3a1a189127991cd3d68b7c",
            "title": "Correcting Automated and Manual Speech Transcription Errors using Warped Language Models",
            "abstract": "Masked language models have revolutionized natural language processing systems in the past few years. A recently introduced generalization of masked language models called warped language models are trained to be more robust to the types of errors that appear in automatic or manual transcriptions of spoken language by exposing the language model to the same types of errors during training. In this work we propose a novel approach that takes advantage of the robustness of warped language models to transcription noise for correcting transcriptions of spoken language. We show that our proposed approach is able to achieve up to 10% reduction in word error rates of both automatic and manual transcriptions of spoken language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171886",
                    "name": "Mahdi Namazifar"
                },
                {
                    "authorId": "47566430",
                    "name": "John Malik"
                },
                {
                    "authorId": "144180616",
                    "name": "Erran L. Li"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "a54caa4ec733b8ee5cec433861e223cc153e0a3c",
            "title": "Are We There Yet? Learning to Localize in Embodied Instruction Following",
            "abstract": "Embodied instruction following is a challenging problem requiring an agent to infer a sequence of primitive actions to achieve a goal environment state from complex language and visual inputs. Action Learning From Realistic Environments and Directives (ALFRED) is a recently proposed benchmark for this problem consisting of step-by-step natural language instructions to achieve subgoals which compose to an ultimate high-level goal. Key challenges for this task include localizing target locations and navigating to them through visual inputs, and grounding language instructions to visual appearance of objects. To address these challenges, in this study, we augment the agent's field of view during navigation subgoals with multiple viewing angles, and train the agent to predict its relative spatial relation to the target location at each timestep. We also improve language grounding by introducing a pre-trained object detection module to the model pipeline. Empirical studies show that our approach exceeds the baseline model performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "89093987",
                    "name": "Shane Storks"
                },
                {
                    "authorId": "3193409",
                    "name": "Qiaozi Gao"
                },
                {
                    "authorId": "2028300167",
                    "name": "G. Thattai"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                }
            ]
        },
        {
            "paperId": "42dd6654fbbca89f011b4986fba97362bd86739a",
            "title": "Language Model is all You Need: Natural Language Understanding as Question Answering",
            "abstract": "Different flavors of transfer learning have shown tremendous impact in advancing research and applications of machine learning. In this work we study the use of a certain family of transfer learning, where the target domain is mapped to the source domain. Specifically we map Natural Language Understanding (NLU) problems to Question Answering (QA) problems and we show that in low data regimes this approach offers significant improvements compared to other approaches to NLU. Moreover, we show that these gains could be increased through sequential transfer learning across NLU problems from different domains. We show that our approach could reduce the amount of required data for the same performance by up to a factor of 10.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171886",
                    "name": "Mahdi Namazifar"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "691b7d216440921dd03a98f8496bfe28530ef3da",
            "title": "Warped Language Models for Noise Robust Language Understanding",
            "abstract": "Masked Language Models (MLM) are self-supervised neural networks trained to fill in the blanks in a given sentence with masked tokens. Despite the tremendous success of MLMs for various text based tasks, they are not robust for spoken language understanding, especially for spontaneous conversational speech recognition noise. In this work we introduce Warped Language Models (WLM) in which input sentences at training time go through the same modifications as in MLM, plus two additional modifications, namely inserting and dropping random tokens. These two modifications extend and contract the sentence in addition to the modifications in MLMs, hence the word \"warped\" in the name. The insertion and drop modification of the input text during training of WLM resemble the types of noise due to Automatic Speech Recognition (ASR) errors, and as a result WLMs are likely to be more robust to ASR noise. Through computational results we show that natural language understanding systems built on top of WLMs perform better compared to those built based on MLMs, especially in the presence of ASR errors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171886",
                    "name": "Mahdi Namazifar"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "86fdbc4540d146b0a2d7d61bf9f0109fa1331dac",
            "title": "LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular Supervision for Visual Question Answering",
            "abstract": "The predominant approach to visual question answering (VQA) relies on encoding the image and question with a \"black-box\" neural encoder and decoding a single token as the answer like \"yes\" or \"no\". Despite this approach's strong quantitative results, it struggles to come up with intuitive, human-readable forms of justification for the prediction process. To address this insufficiency, we reformulate VQA as a full answer generation task, which requires the model to justify its predictions in natural language. We propose LRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning framework for visual question answering that solves the problem step-by-step like humans and provides human-readable form of justification at each step. Specifically, LRTA learns to first convert an image into a scene graph and parse a question into multiple reasoning instructions. It then executes the reasoning instructions one at a time by traversing the scene graph using a recurrent neural-symbolic execution module. Finally, it generates a full answer to the given question with natural language justifications. Our experiments on GQA dataset show that LRTA outperforms the state-of-the-art model by a large margin (43.1% v.s. 28.0%) on the full answer generation task. We also create a perturbed GQA test set by removing linguistic cues (attributes and relations) in the questions for analyzing whether a model is having a smart guess with superficial data correlations. We show that LRTA makes a step towards truly understanding the question while the state-of-the-art model tends to learn superficial correlations from the training data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151253861",
                    "name": "Weixin Liang"
                },
                {
                    "authorId": "2058222200",
                    "name": "Fei Niu"
                },
                {
                    "authorId": "8856206",
                    "name": "Aishwarya N. Reganti"
                },
                {
                    "authorId": "2028300167",
                    "name": "G. Thattai"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                }
            ]
        },
        {
            "paperId": "ddfd4b0929c7999f57fda22179b09c49f360d472",
            "title": "Can You be More Social? Injecting Politeness and Positivity into Task-Oriented Conversational Agents",
            "abstract": "Goal-oriented conversational agents are becoming prevalent in our daily lives. For these systems to engage users and achieve their goals, they need to exhibit appropriate social behavior as well as provide informative replies that guide users through tasks. The first component of the research in this paper applies statistical modeling techniques to understand conversations between users and human agents for customer service. Analyses show that social language used by human agents is associated with greater users' responsiveness and task completion. The second component of the research is the construction of a conversational agent model capable of injecting social language into an agent's responses while still preserving content. The model uses a sequence-to-sequence deep learning architecture, extended with a social language understanding element. Evaluation in terms of content preservation and social language level using both human judgment and automatic linguistic measures shows that the model can generate responses that enable agents to address users' issues in a more socially appropriate way.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116640035",
                    "name": "Yi-Chia Wang"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "49908029",
                    "name": "Runze Wang"
                },
                {
                    "authorId": "2382475",
                    "name": "Zhaleh Feizollahi"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                },
                {
                    "authorId": "1702853",
                    "name": "R. Kraut"
                }
            ]
        },
        {
            "paperId": "e3440a85ea9887af01e2831696e0dc5538abaa15",
            "title": "Multi-Task Siamese Neural Network for Improving Replay Attack Detection",
            "abstract": "Automatic speaker verification systems are vulnerable to audio replay attacks which bypass security by replaying recordings of authorized speakers. Replay attack detection (RA) detection systems built upon Residual Neural Networks (ResNet)s have yielded astonishing results on the public benchmark ASVspoof 2019 Physical Access challenge. With most teams using fine-tuned feature extraction pipelines and model architectures, the generalizability of such systems remains questionable though. In this work, we analyse the effect of discriminative feature learning in a multi-task learning (MTL) setting can have on the generalizability and discriminability of RA detection systems. We use a popular ResNet architecture optimized by the cross-entropy criterion as our baseline and compare it to the same architecture optimized by MTL using Siamese Neural Networks (SNN). It can be shown that SNN outperform the baseline by relative 26.8 % Equal Error Rate (EER). We further enhance the model's architecture and demonstrate that SNN with additional reconstruction loss yield another significant improvement of relative 13.8 % EER.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "138609838",
                    "name": "Patrick von Platen"
                },
                {
                    "authorId": "2064877277",
                    "name": "Fei Tao"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                }
            ]
        },
        {
            "paperId": "ffb32ee364dc38d54111978df5cac89d36e28cfc",
            "title": "Improving Embedding Extraction for Speaker Verification with Ladder Network",
            "abstract": "Speaker verification is an established yet challenging task in speech processing and a very vibrant research area. Recent speaker verification (SV) systems rely on deep neural networks to extract high-level embeddings which are able to characterize the users' voices. Most of the studies have investigated on improving the discriminability of the networks to extract better embeddings for performances improvement. However, only few research focus on improving the generalization. In this paper, we propose to apply the ladder network framework in the SV systems, which combines the supervised and unsupervised learning fashions. The ladder network can make the system to have better high-level embedding by balancing the trade-off to keep/discard as much useful/useless information as possible. We evaluated the framework on two state-of-the-art SV systems, d-vector and x-vector, which can be used for different use cases. The experiments showed that the proposed approach relatively improved the performance by 10% at most without adding parameters and augmented data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2064877277",
                    "name": "Fei Tao"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                }
            ]
        },
        {
            "paperId": "3ad4f202d040851c3b8d19e37ae7f83b88adc961",
            "title": "Flexibly-Structured Model for Task-Oriented Dialogues",
            "abstract": "This paper proposes a novel end-to-end architecture for task-oriented dialogue systems. It is based on a simple and practical yet very effective sequence-to-sequence approach, where language understanding and state tracking tasks are modeled jointly with a structured copy-augmented sequential decoder and a multi-label decoder for each slot. The policy engine and language generation tasks are modeled jointly following that. The copy-augmented sequential decoder deals with new or unknown values in the conversation, while the multi-label decoder combined with the sequential decoder ensures the explicit assignment of values to slots. On the generation part, slot binary classifiers are used to improve performance. This architecture is scalable to real-world scenarios and is shown through an empirical evaluation to achieve state-of-the-art performance on both the Cambridge Restaurant dataset and the Stanford in-car assistant dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145142456",
                    "name": "Lei Shu"
                },
                {
                    "authorId": "34890911",
                    "name": "Piero Molino"
                },
                {
                    "authorId": "2171886",
                    "name": "Mahdi Namazifar"
                },
                {
                    "authorId": "145902876",
                    "name": "Hu Xu"
                },
                {
                    "authorId": "47655430",
                    "name": "Bing Liu"
                },
                {
                    "authorId": "2115689465",
                    "name": "H. Zheng"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                }
            ]
        }
    ]
}