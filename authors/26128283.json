{
    "authorId": "26128283",
    "papers": [
        {
            "paperId": "0ff029e91b8185739646a4ac6ab0713909d31d16",
            "title": "Large Language Models of Code Fail at Completing Code with Potential Bugs",
            "abstract": "Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a significant gap in post-mitigation performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Tuan Dinh"
                },
                {
                    "authorId": "26128283",
                    "name": "Jinman Zhao"
                },
                {
                    "authorId": "145814654",
                    "name": "Samson Tan"
                },
                {
                    "authorId": "1905927",
                    "name": "Renato M. P. Negrinho"
                },
                {
                    "authorId": "8789103",
                    "name": "Leonard Lausen"
                },
                {
                    "authorId": "40881843",
                    "name": "Sheng Zha"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "65f86451e96ad61ffca50eed6a007a19bc03093d",
            "title": "Better Context Makes Better Code Language Models: A Case Study on Function Call Argument Completion",
            "abstract": "Pretrained code language models have enabled great progress towards program synthesis. However, common approaches only consider in-file local context and thus miss information and constraints imposed by other parts of the codebase and its external dependencies. Existing code completion benchmarks also lack such context. To resolve these restrictions we curate a new dataset of permissively licensed Python packages that includes full projects and their dependencies and provide tools to extract non-local information with the help of program analyzers. We then focus on the task of function call argument completion which requires predicting the arguments to function calls. We show that existing code completion models do not yield good results on our completion task. To better solve this task, we query a program analyzer for information relevant to a given function call, and consider ways to provide the analyzer results to different code completion models during inference and training. Our experiments show that providing access to the function implementation and function usages greatly improves the argument completion performance. Our ablation study provides further insights on how different types of information available from the program analyzer and different ways of incorporating the information affect the model performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "146922081",
                    "name": "Hengzhi Pei"
                },
                {
                    "authorId": "26128283",
                    "name": "Jinman Zhao"
                },
                {
                    "authorId": "8789103",
                    "name": "Leonard Lausen"
                },
                {
                    "authorId": "40881843",
                    "name": "Sheng Zha"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "21b11793b960a3e37c0eab7aae6127c28fd38e5c",
            "title": "Code Prediction by Feeding Trees to Transformers",
            "abstract": "Code prediction, more specifically autocomplete, has become an essential feature in modern IDEs. Autocomplete is more effective when the desired next token is at (or close to) the top of the list of potential completions offered by the IDE at cursor position. This is where the strength of the underlying machine learning system that produces a ranked order of potential completions comes into play. We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. Our work uses Transformers as the base neural architecture. We show that by making the Transformer architecture aware of the syntactic structure of code, we increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of several state-of-the-art next token prediction systems by margins ranging from 14% to 18%. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132103197",
                    "name": "Seohyun Kim"
                },
                {
                    "authorId": "26128283",
                    "name": "Jinman Zhao"
                },
                {
                    "authorId": "22472831",
                    "name": "Yuchi Tian"
                },
                {
                    "authorId": "145486355",
                    "name": "S. Chandra"
                }
            ]
        },
        {
            "paperId": "378fa903f330e83ec4c3200d273c25bdcb191923",
            "title": "PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding",
            "abstract": "We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the spellings of words and propose a model, along with an efficient algorithm, that simultaneously models subword segmentation and computes subword-based compositional word embedding. We call the model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords for all possible segmentations based on their likelihood. Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge. Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword-level models in the quality of generated word embeddings across languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26128283",
                    "name": "Jinman Zhao"
                },
                {
                    "authorId": "1999912074",
                    "name": "Shawn Zhong"
                },
                {
                    "authorId": "2141864084",
                    "name": "Xiaomin Zhang"
                },
                {
                    "authorId": "40609253",
                    "name": "Yingyu Liang"
                }
            ]
        },
        {
            "paperId": "5fc7b4dbc154bbbf26d8cee2f18f31ecbf286bcf",
            "title": "Generalizing Word Embeddings using Bag of Subwords",
            "abstract": "We approach the problem of generalizing pre-trained word embeddings beyond fixed-size vocabularies without using additional contextual information. We propose a subword-level word vector generation model that views words as bags of character n-grams. The model is simple, fast to train and provides good vectors for rare or unseen words. Experiments show that our model achieves state-of-the-art performances in English word similarity task and in joint prediction of part-of-speech tag and morphosyntactic attributes in 23 languages, suggesting our model\u2019s ability in capturing the relationship between words\u2019 textual representations and their embeddings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26128283",
                    "name": "Jinman Zhao"
                },
                {
                    "authorId": "40171292",
                    "name": "Sidharth Mudgal"
                },
                {
                    "authorId": "40609253",
                    "name": "Yingyu Liang"
                }
            ]
        },
        {
            "paperId": "6de321c11eddd5349a842899bcadc05a4b5f2bf5",
            "title": "Neural-augmented static analysis of Android communication",
            "abstract": "We address the problem of discovering communication links between applications in the popular Android mobile operating system, an important problem for security and privacy in Android. Any scalable static analysis in this complex setting is bound to produce an excessive amount of false-positives, rendering it impractical. To improve precision, we propose to augment static analysis with a trained neural-network model that estimates the probability that a communication link truly exists. We describe a neural-network architecture that encodes abstractions of communicating objects in two applications and estimates the probability with which a link indeed exists. At the heart of our architecture are type-directed encoders (TDE), a general framework for elegantly constructing encoders of a compound data type by recursively composing encoders for its constituent types. We evaluate our approach on a large corpus of Android applications, and demonstrate that it achieves very high accuracy. Further, we conduct thorough interpretability studies to understand the internals of the learned neural networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26128283",
                    "name": "Jinman Zhao"
                },
                {
                    "authorId": "1893193",
                    "name": "Aws Albarghouthi"
                },
                {
                    "authorId": "35915880",
                    "name": "Vaibhav Rastogi"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "1793187",
                    "name": "Damien Octeau"
                }
            ]
        },
        {
            "paperId": "a138415032b1ee0cb2fa2763cba1f3f9e2d9bbe7",
            "title": "The Effect of Network Width on the Performance of Large-batch Training",
            "abstract": "Distributed implementations of mini-batch stochastic gradient descent (SGD) suffer from communication overheads, attributed to the high frequency of gradient updates inherent in small-batch training. Training with large batches can reduce these overheads; however, large batches can affect the convergence properties and generalization performance of SGD. In this work, we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that--for a fixed number of parameters--wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down, unlike their deeper variants.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119322954",
                    "name": "Lingjiao Chen"
                },
                {
                    "authorId": "2109798334",
                    "name": "Hongyi Wang"
                },
                {
                    "authorId": "26128283",
                    "name": "Jinman Zhao"
                },
                {
                    "authorId": "1740595",
                    "name": "Dimitris Papailiopoulos"
                },
                {
                    "authorId": "1713920",
                    "name": "Paraschos Koutris"
                }
            ]
        },
        {
            "paperId": "d1863797b67de81ae04103f7e3fb672135d059c4",
            "title": "Counting Hypergraph Matchings up to Uniqueness Threshold",
            "abstract": "We study the problem of approximately counting matchings in hypergraphs of bounded maximum degree and maximum size of hyperedges. With an activity parameter $\\lambda$, each matching $M$ is assigned a weight $\\lambda^{|M|}$. The counting problem is formulated as computing a partition function that gives the sum of the weights of all matchings in a hypergraph. This problem unifies two extensively studied statistical physics models in approximate counting: the hardcore model (graph independent sets) and the monomer-dimer model (graph matchings). \nFor this model, the critical activity $\\lambda_c= \\frac{d^d}{k (d-1)^{d+1}}$ is the threshold for the uniqueness of Gibbs measures on the infinite $(d+1)$-uniform $(k+1)$-regular hypertree. Consider hypergraphs of maximum degree at most $k+1$ and maximum size of hyperedges at most $d+1$. We show that when $\\lambda 2\\lambda_c$, there is no PRAS for the partition function or the log-partition function unless NP$=$RP. \nTowards obtaining a sharp transition of computational complexity of approximate counting, we study the local convergence from a sequence of finite hypergraphs to the infinite lattice with specified symmetry. We show a surprising connection between the local convergence and the reversibility of a natural random walk. This leads us to a barrier for the hardness result: The non-uniqueness of infinite Gibbs measure is not realizable by any finite gadgets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2067622935",
                    "name": "Renjie Song"
                },
                {
                    "authorId": "9489786",
                    "name": "Yitong Yin"
                },
                {
                    "authorId": "26128283",
                    "name": "Jinman Zhao"
                }
            ]
        }
    ]
}