{
    "authorId": "2378954",
    "papers": [
        {
            "paperId": "e9484cf633985ccef70b0ce46866ce232a81ca4b",
            "title": "Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length",
            "abstract": "The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, a neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code: https://github.com/XuezheMax/megalodon",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "2296722524",
                    "name": "Xiaomeng Yang"
                },
                {
                    "authorId": "2296721483",
                    "name": "Wenhan Xiong"
                },
                {
                    "authorId": "2296721324",
                    "name": "Beidi Chen"
                },
                {
                    "authorId": "2296724476",
                    "name": "Lili Yu"
                },
                {
                    "authorId": "2296716359",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2296718960",
                    "name": "Jonathan May"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2296719126",
                    "name": "Omer Levy"
                },
                {
                    "authorId": "2110714400",
                    "name": "Chunting Zhou"
                }
            ]
        },
        {
            "paperId": "0a90d5f56150eafd3ac88e4406b17496fedf529a",
            "title": "Look-back Decoding for Open-Ended Text Generation",
            "abstract": "Given a prefix (context), open-ended generation aims to decode texts that are coherent, which do not abruptly drift from previous topics, and informative, which do not suffer from undesired repetitions. In this paper, we propose Look-back, an improved decoding algorithm that leverages the Kullback-Leibler divergence to track the distribution distance between current and historical decoding steps. Thus Look-back can automatically predict potential repetitive phrase and topic drift, and remove tokens that may cause the failure modes, restricting the next token probability distribution within a plausible distance to the history. We perform decoding experiments on document continuation and story generation, and demonstrate that Look-back is able to generate more fluent and coherent text, outperforming other strong decoding methods significantly in both automatic and human evaluations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072804453",
                    "name": "Nan Xu"
                },
                {
                    "authorId": null,
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                }
            ]
        },
        {
            "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
            "title": "LIMA: Less Is More for Alignment",
            "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2384711",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "144118452",
                    "name": "Pengfei Liu"
                },
                {
                    "authorId": "2214843767",
                    "name": "Puxin Xu"
                },
                {
                    "authorId": "1900163",
                    "name": "Srini Iyer"
                },
                {
                    "authorId": "145478138",
                    "name": "Jiao Sun"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "1388010852",
                    "name": "Avia Efrat"
                },
                {
                    "authorId": "2114104308",
                    "name": "Ping Yu"
                },
                {
                    "authorId": "49297123",
                    "name": "L. Yu"
                },
                {
                    "authorId": "2108244542",
                    "name": "Susan Zhang"
                },
                {
                    "authorId": "134007132",
                    "name": "Gargi Ghosh"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "39455775",
                    "name": "Omer Levy"
                }
            ]
        },
        {
            "paperId": "8511ea96d61593de57cbc2e996910e5cb3dbfe84",
            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
            "abstract": "FlashAttention (Dao, 2023) effectively reduces the quadratic peak memory usage to linear in training transformer-based large language models (LLMs) on a single GPU. In this paper, we introduce DISTFLASHATTN, a distributed memory-efficient attention mechanism optimized for long-context LLMs training. We propose three key techniques: token-level workload balancing, overlapping key-value communication, and a rematerialization-aware gradient checkpointing algorithm. We evaluate DISTFLASHATTN on Llama-7B and variants with sequence lengths from 32K to 512K. DISTFLASHATTN achieves 8x longer sequences, 4.45 - 5.64x speedup compared to Ring Self-Attention, 2 - 8x longer sequences, 1.24 - 2.01x speedup compared to Megatron-LM with FlashAttention. It achieves 1.67x and 1.26 - 1.88x speedup compared to recent Ring Attention and DeepSpeed-Ulysses. Code is available at https://github.com/RulinShao/LightSeq.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117961435",
                    "name": "Dacheng Li"
                },
                {
                    "authorId": "2254264970",
                    "name": "Rulin Shao"
                },
                {
                    "authorId": "2254221083",
                    "name": "Anze Xie"
                },
                {
                    "authorId": "2243336934",
                    "name": "Eric P. Xing"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "2055174324",
                    "name": "Ion Stoica"
                },
                {
                    "authorId": "2254681613",
                    "name": "Joseph E. Gonzalez"
                },
                {
                    "authorId": "2257340589",
                    "name": "Hao Zhang"
                }
            ]
        },
        {
            "paperId": "9cab8c423d9c13d4f35beb97a7f823c250e8f059",
            "title": "RECAP: Retrieval-Enhanced Context-Aware Prefix Encoder for Personalized Dialogue Response Generation",
            "abstract": "Endowing chatbots with a consistent persona is essential to an engaging conversation, yet it remains an unresolved challenge. In this work, we propose a new retrieval-enhanced approach for personalized response generation. Specifically, we design a hierarchical transformer retriever trained on dialogue domain data to perform personalized retrieval and a context-aware prefix encoder that fuses the retrieved information to the decoder more effectively. Extensive experiments on a real-world dataset demonstrate the effectiveness of our model at generating more fluent and personalized responses. We quantitatively evaluate our model\u2019s performance under a suite of human and automatic metrics and find it to be superior compared to state-of-the-art baselines on English Reddit conversations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156268596",
                    "name": "Shuai Liu"
                },
                {
                    "authorId": "91009922",
                    "name": "Hyundong Justin Cho"
                },
                {
                    "authorId": "2052513135",
                    "name": "Marjorie Freedman"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "143823227",
                    "name": "Jonathan May"
                }
            ]
        },
        {
            "paperId": "a83ddab9d09192daadef387ca6571d8c44d71703",
            "title": "Challenges in Context-Aware Neural Machine Translation",
            "abstract": "Context-aware neural machine translation involves leveraging information beyond sentence-level context to resolve inter-sentential discourse dependencies and improve document-level translation quality, and has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most context-aware translation models show only modest improvements over sentence-level systems. In this work, we investigate several challenges that impede progress within this field, relating to discourse phenomena, context usage, model architectures, and document-level evaluation. To address these problems, we propose a more realistic setting for document-level translation, called paragraph-to-paragraph (para2para) translation, and collect a new dataset of Chinese-English novels to promote future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2000231390",
                    "name": "Linghao Jin"
                },
                {
                    "authorId": "2188998958",
                    "name": "Jacqueline He"
                },
                {
                    "authorId": "143823227",
                    "name": "Jonathan May"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                }
            ]
        },
        {
            "paperId": "bfa6d9c1ad7ba2c4f5b1806021f3e1999d0e5d7e",
            "title": "MIDDAG: Where Does Our News Go? Investigating Information Diffusion via Community-Level Information Pathways",
            "abstract": "We present MIDDAG, an intuitive, interactive system that visualizes the information propagation paths on social media triggered by COVID-19-related news articles accompanied by comprehensive insights including user/community susceptibility level, as well as events and popular opinions raised by the crowd while propagating the information. Besides discovering information flow patterns among users, we construct communities among users and develop the propagation forecasting capability, enabling tracing and understanding of how information is disseminated at a higher level. A demo video and more are available at https://info-pathways.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2223976510",
                    "name": "Alexander K. Taylor"
                },
                {
                    "authorId": "2253657042",
                    "name": "Nuan Wen"
                },
                {
                    "authorId": "2108082280",
                    "name": "Yanchen Liu"
                },
                {
                    "authorId": "2008339028",
                    "name": "Po-Nien Kung"
                },
                {
                    "authorId": "2253469011",
                    "name": "Wenna Qin"
                },
                {
                    "authorId": "2253451145",
                    "name": "Shicheng Wen"
                },
                {
                    "authorId": "2253463535",
                    "name": "Azure Zhou"
                },
                {
                    "authorId": "2254124345",
                    "name": "Diyi Yang"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "2253599901",
                    "name": "Nanyun Peng"
                },
                {
                    "authorId": "2256611613",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "048a71c2b2b1fca75feff0914a375ffe5aa023c9",
            "title": "Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping",
            "abstract": "Fine-tuning over large pretrained language models (PLMs) has established many state-of-the-art results. Despite its superior performance, such fine-tuning can be unstable, resulting in significant variance in performance and potential risks for practical applications. Previous works have attributed such instability to the catastrophic forgetting problem in the top layers of PLMs, which indicates iteratively fine-tuning layers in a top-down manner is a promising solution. In this paper, we first point out that this method does not always work out due to the different convergence speeds of different layers/modules. Inspired by this observation, we propose a simple component-wise gradient norm clipping method to adjust the convergence speed for different components. Experiment results demonstrate that our method achieves consistent improvements in terms of generalization performance, convergence speed, and training stability. The codebase can be found at https://github.com/yangalan123/FineTuningStability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46962597",
                    "name": "Chenghao Yang"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                }
            ]
        },
        {
            "paperId": "07c70ca55793984ffdf31582a05170ef3d62381a",
            "title": "Prompt Consistency for Zero-Shot Task Generalization",
            "abstract": "One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zero-shot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al., 2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110714400",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "6215698",
                    "name": "Junxian He"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "165231cedd0f52953a4d11bf359a26738b3e5020",
            "title": "Better May Not Be Fairer: Can Data Augmentation Mitigate Subgroup Degradation?",
            "abstract": "It is no secret that deep learning models exhibit undesirable behaviors such as learning spurious correlations instead of learning correct relationships between input/output pairs. Prior works on robustness study datasets that mix low-level features to quantify how spurious correlations affect predictions instead of considering natural semantic factors due to limitations in accessing realistic datasets for comprehensive evaluation. To bridge this gap, in this paper we \ufb01rst investigate how natural background colors play a role as spurious features in image classi\ufb01cation tasks by manually splitting the test sets of CIFAR10 and CIFAR100 into subgroups based on the background color of each image. We name our datasets CIFAR10-B and CIFAR100-B . We \ufb01nd that while standard CNNs achieve human-level accuracy, the subgroup performances are not consistent, and the phenomenon remains even after data augmentation (DA). To alleviate this issue, we propose FlowAug , a semantic DA method that leverages the decoupled semantic representations captured by a pre-trained generative \ufb02ow. Experimental results show that FlowAug achieves more consistent results across subgroups than other types of DA methods on CIFAR10 and CIFAR100. Additionally, it shows better generalization performance. Furthermore, we propose a generic metric for studying model robustness to spurious correlations, where we take a macro average on the weighted standard deviations across different classes. Per our metric, FlowAug demonstrates less reliance on spurious correlations. Although this metric is proposed to study our curated datasets, it applies to all datasets that have subgroups or subclasses. Lastly, aside from less dependence on spurious correlations and better generalization on in-distribution test sets, we also show superior out-of-distribution results on CIFAR10.1 and competitive performances on CIFAR10-C and CIFAR100-C.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "116137272",
                    "name": "Ming-Chang Chiu"
                },
                {
                    "authorId": "2158177948",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                }
            ]
        }
    ]
}