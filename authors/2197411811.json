{
    "authorId": "2197411811",
    "papers": [
        {
            "paperId": "47d69cf2a9d8acaefab20b70d70a6df9e8011a0a",
            "title": "Efficient Link Prediction via GNN Layers Induced by Negative Sampling",
            "abstract": "Graph neural networks (GNNs) for link prediction can loosely be divided into two broad categories. First, \\emph{node-wise} architectures pre-compute individual embeddings for each node that are later combined by a simple decoder to make predictions. While extremely efficient at inference time (since node embeddings are only computed once and repeatedly reused), model expressiveness is limited such that isomorphic nodes contributing to candidate edges may not be distinguishable, compromising accuracy. In contrast, \\emph{edge-wise} methods rely on the formation of edge-specific subgraph embeddings to enrich the representation of pair-wise relationships, disambiguating isomorphic nodes to improve accuracy, but with the cost of increased model complexity. To better navigate this trade-off, we propose a novel GNN architecture whereby the \\emph{forward pass} explicitly depends on \\emph{both} positive (as is typical) and negative (unique to our approach) edges to inform more flexible, yet still cheap node-wise embeddings. This is achieved by recasting the embeddings themselves as minimizers of a forward-pass-specific energy function (distinct from the actual training loss) that favors separation of positive and negative samples. As demonstrated by extensive empirical evaluations, the resulting architecture retains the inference speed of node-wise models, while producing competitive accuracy with edge-wise alternatives.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2258757827",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "2197411811",
                    "name": "Xiannian Hu"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2256635267",
                    "name": "David Wipf"
                }
            ]
        },
        {
            "paperId": "bf195fed7592cc364abb7352ad0e41c175031305",
            "title": "Graph Structure Learning via Lottery Hypothesis at Scale",
            "abstract": "Graph Neural Networks (GNNs) are commonly applied to analyze real-world graph-structured data. However, GNNs are sensitive to the given graph structure, which cast importance on graph structure learning to find optimal graph structures and representations. Previous methods have been restricted from large graphs due to high computational complexity. Lottery ticket hypothesis suggests that there exists a subnetwork that has comparable or better performance with proto-networks, which has been transferred to suit for pruning GNNs recently. There are few studies that address lottery ticket hypothesis\u2019s performance on defense in graphs. In this paper, we propose a scalable graph structure learning method leveraging lottery (ticket) hypothesis : GSL-LH. Our experiments show that GSL-LH can outperform its backbone model without attack and show better robustness against attack, achieving state-of-the-art performances in regular-size graphs compared to other graph structure learning methods without feature augmentation. In large graphs, GSL-LH can have comparable results with state-of-the-art defense methods other than graph structure learning, while bringing some insights into explanation of robustness. 1 2 3",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258757827",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "2197411811",
                    "name": "Xiannian Hu"
                },
                {
                    "authorId": "2301074845",
                    "name": "Jiaqing Xie"
                },
                {
                    "authorId": "2155273086",
                    "name": "Zhangyue Yin"
                },
                {
                    "authorId": "2118117212",
                    "name": "Yunhua Zhou"
                },
                {
                    "authorId": "2282972251",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2284750473",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "6a3f7dfd36f2ae3d2882393b87353f66f190ee87",
            "title": "Rethinking Label Smoothing on Multi-hop Question Answering",
            "abstract": "\u201cMulti-Hop Question Answering (MHQA) is a significant area in question answering, requiringmultiple reasoning components, including document retrieval, supporting sentence prediction,and answer span extraction. In this work, we present the first application of label smoothing tothe MHQA task, aiming to enhance generalization capabilities in MHQA systems while miti-gating overfitting of answer spans and reasoning paths in the training set. We introduce a novellabel smoothing technique, F1 Smoothing, which incorporates uncertainty into the learning pro-cess and is specifically tailored for Machine Reading Comprehension (MRC) tasks. Moreover,we employ a Linear Decay Label Smoothing Algorithm (LDLA) in conjunction with curricu-lum learning to progressively reduce uncertainty throughout the training process. Experimenton the HotpotQA dataset confirms the effectiveness of our approach in improving generaliza-tion and achieving significant improvements, leading to new state-of-the-art performance on theHotpotQA leaderboard.\u201d",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155273086",
                    "name": "Zhangyue Yin"
                },
                {
                    "authorId": "2187160099",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "2197486172",
                    "name": "Yiguang Wu"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2197411811",
                    "name": "Xiannian Hu"
                },
                {
                    "authorId": null,
                    "name": "Xinyu Zhang"
                },
                {
                    "authorId": "2106400572",
                    "name": "Zhao Cao"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                }
            ]
        }
    ]
}