{
    "authorId": "2122143",
    "papers": [
        {
            "paperId": "475fc9568fb9d19313976a0ad7ba3fb90b75f02b",
            "title": "Search Result Diversification Using Query Aspects as Bottlenecks",
            "abstract": "We address some of the limitations of coverage-based search result diversification models, which often consist of separate components and rely on external systems for query aspects. To overcome these challenges, we introduce an end-to-end learning framework called DUB. Our approach preserves the intrinsic interpretability of coverage-based methods while enhancing diversification performance. Drawing inspiration from the information bottleneck method, we propose an aspect extractor that generates query aspect embeddings optimized as information bottlenecks for the task of diversified document re-ranking. Experimental results demonstrate that DUB outperforms state-of-the-art diversification models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51029950",
                    "name": "Puxuan Yu"
                },
                {
                    "authorId": "38543401",
                    "name": "Razieh Rahimi"
                },
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                },
                {
                    "authorId": "144890574",
                    "name": "James Allan"
                }
            ]
        },
        {
            "paperId": "6e8627662e66a797be56f473f9bee76451d5eb48",
            "title": "Soft Prompt Decoding for Multilingual Dense Retrieval",
            "abstract": "In this work, we explore a Multilingual Information Retrieval (MLIR) task, where the collection includes documents in multiple languages. We demonstrate that applying state-of-the-art approaches developed for cross-lingual information retrieval to MLIR tasks leads to sub-optimal performance. This is due to the heterogeneous and imbalanced nature of multilingual collections -- some languages are better represented in the collection and some benefit from large-scale training data. To address this issue, we present KD-SPD, a novel soft prompt decoding approach for MLIR that implicitly \"translates'' the representation of documents in different languages into the same embedding space. To address the challenges of data scarcity and imbalance, we introduce a knowledge distillation strategy. The teacher model is trained on rich English retrieval data, and by leveraging bi-text data, our distillation framework transfers its retrieval knowledge to the multilingual document encoder. Therefore, our approach does not require any multilingual retrieval training data. Extensive experiments on three MLIR datasets with a total of 15 languages demonstrate that KD-SPD significantly outperforms competitive baselines in all cases. We conduct extensive analyses to show that our method has less language bias and better zero-shot transfer ability towards new languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                },
                {
                    "authorId": "2029235362",
                    "name": "Hansi Zeng"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                },
                {
                    "authorId": "121300608",
                    "name": "J. Allan"
                }
            ]
        },
        {
            "paperId": "709fb9ddc54f048f255d814738008aab1ae314ba",
            "title": "Cross-lingual Knowledge Transfer via Distillation for Multilingual Information Retrieval",
            "abstract": "In this paper, we introduce the approach behind our submission for the MIRACL challenge, a WSDM 2023 Cup competition that centers on ad-hoc retrieval across 18 diverse languages. Our solution contains two neural-based models. The first model is a bi-encoder re-ranker, on which we apply a cross-lingual distillation technique to transfer ranking knowledge from English to the target language space. The second model is a cross-encoder re-ranker trained on multilingual retrieval data generated using neural machine translation. We further fine-tune both models using MIRACL training data and ensemble multiple rank lists to obtain the final result. According to the MIRACL leaderboard, our approach ranks 8th for the Test-A set and 2nd for the Test-B set among the 16 known languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                },
                {
                    "authorId": "51029950",
                    "name": "Puxuan Yu"
                },
                {
                    "authorId": "121300608",
                    "name": "J. Allan"
                }
            ]
        },
        {
            "paperId": "c49930b60d4b29a6d1e0249fa9eef4b937b0394e",
            "title": "Hierarchical Transformer-based Query by Multiple Documents",
            "abstract": "It is often difficult for users to form keywords to express their information needs, especially when they are not familiar with the domain of the articles of interest. Moreover, in some search scenarios, there is no explicit query for the search engine to work with. Query-By-Multiple-Documents (QBMD), in which the information needs are implicitly represented by a set of relevant documents addresses these retrieval scenarios. Unlike the keyword-based retrieval task, the query documents are treated as exemplars of a hidden query topic, but it is often the case that they can be relevant to multiple topics. In this paper, we present a Hierarchical Interaction-based (HINT) bi-encoder retrieval architecture that encodes a set of query documents and retrieval documents separately for the QBMD task. We design a hierarchical attention mechanism that allows the model to 1) encode long sequences efficiently and 2) learn the interactions at low-level and high-level semantics (e.g., tokens and paragraphs) across multiple documents. With contextualized representations, the final scoring is calculated based on a stratified late interaction, which ensures each query document contributes equally to the matching against the candidate document. We build a large-scale, weakly supervised QBMD retrieval dataset based on Wikipedia for model training. We evaluate the proposed model on both Query-By-Single-Document (QBSD) and QBMD tasks. For QBSD, we use a benchmark dataset for legal case retrieval. For QBMD, we transform standard keyword-based retrieval datasets into the QBMD setting. Our experimental results show that HINT significantly outperforms all competitive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                },
                {
                    "authorId": "2721029",
                    "name": "Sheikh Muhammad Sarwar"
                }
            ]
        },
        {
            "paperId": "fe6fa381e6b826c8411d5c4e6d717f0e4ef23755",
            "title": "Improving Cross-lingual Information Retrieval on Low-Resource Languages via Optimal Transport Distillation",
            "abstract": "Benefiting from transformer-based pre-trained language models, neural ranking models have made significant progress. More recently, the advent of multilingual pre-trained language models provides great support for designing neural cross-lingual retrieval models. However, due to unbalanced pre-training data in different languages, multilingual language models have already shown a performance gap between high and low-resource languages in many downstream tasks. And cross-lingual retrieval models built on such pre-trained models can inherit language bias, leading to suboptimal result for low-resource languages. Moreover, unlike the English-to-English retrieval task, where large-scale training collections for document ranking such as MS MARCO are available, the lack of cross-lingual retrieval data for low-resource language makes it more challenging for training cross-lingual retrieval models. In this work, we propose OPTICAL: Optimal Transport distillation for low-resource Cross-lingual information retrieval. To transfer a model from high to low resource languages, OPTICAL forms the cross-lingual token alignment task as an optimal transport problem to learn from a well-trained monolingual retrieval model. By separating the cross-lingual knowledge from knowledge of query document matching, OPTICAL only needs bitext data for distillation training, which is more feasible for low-resource languages. Experimental results show that, with minimal training data, OPTICAL significantly outperforms strong baselines on low-resource languages, including neural machine translation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                }
            ]
        },
        {
            "paperId": "1e883502e757b818b2820593882fe4e053547da2",
            "title": "Mixed Attention Transformer for Leveraging Word-Level Knowledge to Neural Cross-Lingual Information Retrieval",
            "abstract": "Pre-trained contextualized representations offer great success for many downstream tasks, including document ranking. The multilingual versions of such pre-trained representations provide a possibility of jointly learning many languages with the same model. Although it is expected to gain big with such joint training, in the case of cross-lingual information retrieval (CLIR), the models under a multilingual setting are not achieving the same level of performance as those under a monolingual setting. We hypothesize that the performance drop is due to thetranslation gap between query and documents. In the monolingual retrieval task, because of the same lexical inputs, it is easier for model to identify the query terms that occurred in documents. However, in the multilingual pre-trained models that the words in different languages are projected into the same hyperspace, the model tends to \"translate\" query terms into related terms - i.e., terms that appear in a similar context - in addition to or sometimes rather than synonyms in the target language. This property is creating difficulties for the model to connect terms that co-occur in both query and document. To address this issue, we propose a novel Mixed Attention Transformer (MAT) that incorporates external word-level knowledge, such as a dictionary or translation table. We design a sandwich-like architecture to embed MAT into the recent transformer-based deep neural models. By encoding the translation knowledge into an attention matrix, the model with MAT is able to focus on the mutually translated words in the input sequence. Experimental results demonstrate the effectiveness of the external knowledge and the significant improvement of MAT-embedded neural reranking model on CLIR task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                },
                {
                    "authorId": "2131141811",
                    "name": "Hamed Bonab"
                },
                {
                    "authorId": "2721029",
                    "name": "Sheikh Muhammad Sarwar"
                },
                {
                    "authorId": "38543401",
                    "name": "Razieh Rahimi"
                },
                {
                    "authorId": "121300608",
                    "name": "J. Allan"
                }
            ]
        },
        {
            "paperId": "9f2ff252b79bc951857cf6faf4a6e9ae53d28e94",
            "title": "AutoName: A Corpus-Based Set Naming Framework",
            "abstract": "We propose AutoName, an unsupervised framework that extracts a name for a set of query entities from a large-scale text corpus. Entity-set naming is useful in many tasks related to natural language processing and information retrieval such as session-based and conversational information seeking. Previous studies mainly extract set names from knowledge bases which provide highly reliable entity relations, but suffer from limited coverage of entities and set names that represent broad semantic classes. To address these problems, AutoName generates hypernym-anchored candidate phrases via probing a pre-trained language model and the entities' context in documents. Phrases are then clustered to identify ones that describe common concepts among query entities. Finally, AutoName ranks refined phrases based on the co-occurrences of their words with query entities and the conceptual integrity of their respective clusters. We built a new benchmark dataset for this task, consisting of 130 entity sets with name labels. Experimental results show that AutoName generates coherent and meaningful set names and significantly outperforms all baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                },
                {
                    "authorId": "38543401",
                    "name": "Razieh Rahimi"
                },
                {
                    "authorId": "51029950",
                    "name": "Puxuan Yu"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "144890574",
                    "name": "James Allan"
                }
            ]
        },
        {
            "paperId": "ca32689a9336ce845cdd50abe52d86d251d68aaa",
            "title": "Audio-Oriented Multimodal Machine Comprehension: Task, Dataset and Model",
            "abstract": "While Machine Comprehension (MC) has attracted extensive research interests in recent years, existing approaches mainly belong to the category of Machine Reading Comprehension task which mines textual inputs (paragraphs and questions) to predict the answers (choices or text spans). However, there are a lot of MC tasks that accept audio input in addition to the textual input, e.g. English listening comprehension test. In this paper, we target the problem of Audio-Oriented Multimodal Machine Comprehension, and its goal is to answer questions based on the given audio and textual information. To solve this problem, we propose a Dynamic Inter- and Intra-modality Attention (DIIA) model to effectively fuse the two modalities (audio and textual). DIIA can work as an independent component and thus be easily integrated into existing MC models. Moreover, we further develop a Multimodal Knowledge Distillation (MKD) module to enable our multimodal MC model to accurately predict the answers based only on either the text or the audio. As a result, the proposed approach can handle various tasks including: Audio-Oriented Multimodal Machine Comprehension, Machine Reading Comprehension and Machine Listening Comprehension, in a single model, making fair comparisons possible between our model and the existing unimodal MC models. Experimental results and analysis prove the effectiveness of the proposed approaches. First, the proposed DIIA boosts the baseline models by up to 21.08% in terms of accuracy; Second, under the unimodal scenarios, the MKD module allows our multimodal MC model to significantly outperform the unimodal models by up to 18.87%, which are trained and tested with only audio or textual data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                },
                {
                    "authorId": "1927674",
                    "name": "Fenglin Liu"
                },
                {
                    "authorId": "144620586",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "36263371",
                    "name": "Shen Ge"
                },
                {
                    "authorId": "3408469",
                    "name": "Helin Wang"
                },
                {
                    "authorId": "3228071",
                    "name": "Wei Fan"
                },
                {
                    "authorId": "26981150",
                    "name": "Yuexian Zou"
                }
            ]
        },
        {
            "paperId": "0fe3395afcf426a61b3f90f553a1bd9aecbf6f70",
            "title": "Corpus-based Set Expansion with Lexical Features and Distributed Representations",
            "abstract": "Corpus-based set expansion refers to mining \"sibling\" entities of some given seed entities from a corpus. Previous works are limited to using either textual context matching or semantic matching to fulfill this task. Neither matching method takes full advantage of the rich information in free text. We present CaSE, an efficient unsupervised corpus-based set expansion framework that leverages lexical features as well as distributed representations of entities for the set expansion task. Experiments show that CaSE outperforms state-of-the-art set expansion algorithms in terms of expansion accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51029950",
                    "name": "Puxuan Yu"
                },
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                },
                {
                    "authorId": "38543401",
                    "name": "Razieh Rahimi"
                },
                {
                    "authorId": "144890574",
                    "name": "James Allan"
                }
            ]
        },
        {
            "paperId": "b58bbd8044d2bc8f8fea37ca005f854a0eedad9f",
            "title": "PSynDB: Accurate and Accessible Private Data Generation",
            "abstract": "Across many application domains, trusted parties who collect sensitive information need mechanisms to safely disseminate data. A favored approach is to generate synthetic data: a dataset similar to the original, hopefully retaining its statistical features, but one that does not reveal the private information of contributors to the data. \n \nWe present PSynDB, a web-based synthetic table generator that is built on recent privacy technologies [10,11,15]. PSynDB satisfies the formal guarantee of differential privacy and generates synthetic tables with high accuracy for tasks that the user specifies as important. PSynDB allows users to browse expected error rates before running the mechanism, a useful feature for making important policy decisions, such as setting the privacy loss budget. When the user has finished configuration, the tool outputs a data synthesis program that can be ported to a trusted environment. There it can be safely executed on the private data to produce the private synthetic dataset for broad dissemination.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122143",
                    "name": "Zhiqi Huang"
                },
                {
                    "authorId": "35836504",
                    "name": "Ryan McKenna"
                },
                {
                    "authorId": "3416325",
                    "name": "G. Bissias"
                },
                {
                    "authorId": "1729605",
                    "name": "G. Miklau"
                },
                {
                    "authorId": "145256244",
                    "name": "Michael Hay"
                },
                {
                    "authorId": "2357165",
                    "name": "Ashwin Machanavajjhala"
                }
            ]
        }
    ]
}