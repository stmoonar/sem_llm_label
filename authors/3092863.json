{
    "authorId": "3092863",
    "papers": [
        {
            "paperId": "e65f6420f0876d4681dc11a0163e12a8a236dc5a",
            "title": "Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning",
            "abstract": "Dialogue state tracking (DST) is an important step in dialogue management to keep track of users' beliefs. Existing works fine-tune all language model (LM) parameters to tackle the DST task, which requires significant data and computing resources for training and hosting. The cost grows exponentially in the real-world deployment where dozens of fine-tuned LM are used for different domains and tasks. To reduce parameter size and better utilize cross-task shared information, we propose to use soft prompt token embeddings to learn task properties. Without tuning LM parameters, our method drastically reduces the number of parameters needed to less than 0.5% of prior works while achieves better low-resource DST performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "1927a2b40a55ef64eb392719bfb7bd5843cd74d5",
            "title": "Towards Textual Out-of-Domain Detection Without In-Domain Labels",
            "abstract": "In many real-world settings, machine learning models need to identify user inputs that are out-of-domain (OOD) so as to avoid performing wrong actions. This work focuses on a challenging case of OOD detection, where no labels for in-domain data are accessible (e.g., no intent labels for the intent classification task). To this end, we first evaluate different language model based approaches that predict likelihood for a sequence of tokens. Furthermore, we propose a novel representation learning based method by combining unsupervised clustering and contrastive learning so that better data representations for OOD detection can be learned. Through extensive experiments, we demonstrate that this method can significantly outperform likelihood-based methods and can be even competitive to the state-of-the-art supervised approaches with label information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "7e5712806ec37a554914dc8dae19fe57b9645763",
            "title": "GRAVL-BERT: Graphical Visual-Linguistic Representations for Multimodal Coreference Resolution",
            "abstract": "Learning from multimodal data has become a popular research topic in recent years. Multimodal coreference resolution (MCR) is an important task in this area. MCR involves resolving the references across different modalities, e.g., text and images, which is a crucial capability for building next-generation conversational agents. MCR is challenging as it requires encoding information from different modalities and modeling associations between them. Although significant progress has been made for visual-linguistic tasks such as visual grounding, most of the current works involve single turn utterances and focus on simple coreference resolutions. In this work, we propose an MCR model that resolves coreferences made in multi-turn dialogues with scene images. We present GRAVL-BERT, a unified MCR framework which combines visual relationships between objects, background scenes, dialogue, and metadata by integrating Graph Neural Networks with VL-BERT. We present results on the SIMMC 2.0 multimodal conversational dataset, achieving the rank-1 on the DSTC-10 SIMMC 2.0 MCR challenge with F1 score 0.783. Our code is available at https://github.com/alexa/gravl-bert.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068107537",
                    "name": "Danfeng Guo"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "150290293",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2052990580",
                    "name": "Arijit Biswas"
                },
                {
                    "authorId": "2116675467",
                    "name": "Chien-Wei Lin"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ]
        },
        {
            "paperId": "a66b091b71f862dc8f8865e9e1cc1249a3d13252",
            "title": "Context-Situated Pun Generation",
            "abstract": "Previous work on pun generation commonly begins with a given pun word (a pair of homophones for heterographic pun generation and a polyseme for homographic pun generation) and seeks to generate an appropriate pun. While this may enable efficient pun generation, we believe that a pun is most entertaining if it fits appropriately within a given context, e.g., a given situation or dialogue. In this work, we propose a new task, context-situated pun generation, where a specific context represented by a set of keywords is provided, and the task is to first identify suitable pun words that are appropriate for the context, then generate puns based on the context keywords and the identified pun words. We collect a new dataset, CUP (Context-sitUated Pun), containing 4.5k tuples of context words and pun pairs. Based on the new data and setup, we propose a pipeline system for context-situated pun generation, including a pun word retrieval module that identifies suitable pun words for a given context, and a pun generation module that generates puns from context keywords and pun words. Human evaluation shows that 69% of our top retrieved pun words can be used to generate context-situated puns, and our generation module yields successful puns 31% of the time given a plausible tuple of context words and pun pair, almost tripling the yield of a state-of-the-art pun generation model. With an end-to-end evaluation, our pipeline system with the top-1 retrieved pun pair for a given context can generate successful puns 40% of the time, better than all other modeling variations but 32% lower than the human success rate. This highlights the difficulty of the task, and encourages more research in this direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145478138",
                    "name": "Jiao Sun"
                },
                {
                    "authorId": "1414599717",
                    "name": "Anjali Narayan-Chen"
                },
                {
                    "authorId": "3114640",
                    "name": "Shereen Oraby"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "49025105",
                    "name": "Jing-huan Huang"
                },
                {
                    "authorId": "2152802662",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "2c498b6504ca7f5fb156a994aa2e68a51156237f",
            "title": "Alexa Conversations: An Extensible Data-driven Approach for Building Task-oriented Dialogue Systems",
            "abstract": "Traditional goal-oriented dialogue systems rely on various components such as natural language understanding, dialogue state tracking, policy learning and response generation. Training each component requires annotations which are hard to obtain for every new domain, limiting scalability of such systems. Similarly, rule-based dialogue systems require extensive writing and maintenance of rules and do not scale either. End-to-End dialogue systems, on the other hand, do not require module-specific annotations but need a large amount of data for training. To overcome these problems, in this demo, we present Alexa Conversations, a new approach for building goal-oriented dialogue systems that is scalable, extensible as well as data efficient. The components of this system are trained in a data-driven manner, but instead of collecting annotated conversations for training, we generate them using a novel dialogue simulator based on a few seed dialogues and specifications of APIs and entities provided by the developer. Our approach provides out-of-the-box support for natural conversational phenomenon like entity sharing across turns or users changing their mind during conversation without requiring developers to provide any such dialogue flows. We exemplify our approach using a simple pizza ordering task and showcase its value in reducing the developer burden for creating a robust experience. Finally, we evaluate our system using a typical movie ticket booking task integrated with live APIs and show that the dialogue simulator is an essential component of the system that leads to over 50% improvement in turn-level action signature prediction accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40026200",
                    "name": "Anish Acharya"
                },
                {
                    "authorId": "2025630077",
                    "name": "Suranjit Adhikari"
                },
                {
                    "authorId": "150290293",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "2131423",
                    "name": "Vincent Auvray"
                },
                {
                    "authorId": "2025627813",
                    "name": "Nehal Belgamwar"
                },
                {
                    "authorId": "2052990580",
                    "name": "Arijit Biswas"
                },
                {
                    "authorId": "2059656807",
                    "name": "Shubhra Chandra"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "1399159921",
                    "name": "Maryam Fazel-Zarandi"
                },
                {
                    "authorId": "39303368",
                    "name": "Raefer Gabriel"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2061600973",
                    "name": "Rahul Goel"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "2078502127",
                    "name": "Jan Jezabek"
                },
                {
                    "authorId": "1879594",
                    "name": "Abhay Jha"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "2068122300",
                    "name": "Prakash Krishnan"
                },
                {
                    "authorId": "2052970003",
                    "name": "Peter Ku"
                },
                {
                    "authorId": "2057019",
                    "name": "Anuj Goyal"
                },
                {
                    "authorId": "2116675467",
                    "name": "Chien-Wei Lin"
                },
                {
                    "authorId": "2116621357",
                    "name": "Qing Liu"
                },
                {
                    "authorId": "33638380",
                    "name": "Arindam Mandal"
                },
                {
                    "authorId": "47851995",
                    "name": "A. Metallinou"
                },
                {
                    "authorId": "41035810",
                    "name": "V. Naik"
                },
                {
                    "authorId": "2115430213",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "31593543",
                    "name": "Shachi Paul"
                },
                {
                    "authorId": "38686805",
                    "name": "Vittorio Perera"
                },
                {
                    "authorId": "14214710",
                    "name": "Abhishek Sethi"
                },
                {
                    "authorId": "98920799",
                    "name": "Minmin Shen"
                },
                {
                    "authorId": "1805735",
                    "name": "N. Strom"
                },
                {
                    "authorId": "2087051541",
                    "name": "Eddie Wang"
                }
            ]
        },
        {
            "paperId": "667e67fd51109391fd76f0ebd311e1184bc8c179",
            "title": "Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems",
            "abstract": "Most prior work on task-oriented dialogue systems is restricted to supporting domain APIs. However, users may have requests that are out of the scope of these APIs. This work focuses on identifying such user requests. Existing methods for this task mainly rely on fine-tuning pre-trained models on large annotated data. We propose a novel method, REDE, based on adaptive representation learning and density estimation. REDE can be applied to zero-shot cases, and quickly learns a high-performing detector with only a few shots by updating less than 3K parameters. We demonstrate REDE\u2019s competitive performance on DSTC9 data and our newly collected test set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "e510f923d3a34adc727f43203e06526a0e33c1d8",
            "title": "Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE",
            "abstract": "InfoNCE-based contrastive representation learners, such as SimCLR, have been tremendously successful in recent years. However, these contrastive schemes are notoriously resource demanding, as their effectiveness breaks down with small-batch training (i.e., the log-K curse, whereas K is the batch-size). In this work, we reveal mathematically why contrastive learners fail in the small-batch-size regime, and present a novel simple, non-trivial contrastive objective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no longer explicitly appeals to a discriminative classification goal for contrastive learning. Theoretically, we show FlatNCE is the mathematical dual formulation of InfoNCE, thus bridging the classical literature on energy modeling; and empirically, we demonstrate that, with minimal modification of code, FlatNCE enables immediate performance boost independent of the subject-matter engineering efforts. The significance of this work is furthered by the powerful generalization of contrastive learning techniques, and the introduction of new tools to monitor and diagnose contrastive training. We substantiate our claims with empirical evidence on CIFAR10, ImageNet, and other datasets, where FlatNCE consistently outperforms InfoNCE.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2108169368",
                    "name": "Junya Chen"
                },
                {
                    "authorId": "144702900",
                    "name": "Zhe Gan"
                },
                {
                    "authorId": "2144461823",
                    "name": "Xuan Li"
                },
                {
                    "authorId": "2153686863",
                    "name": "Qing Guo"
                },
                {
                    "authorId": "2109077977",
                    "name": "Liqun Chen"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2110290078",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "1723706",
                    "name": "Wenlian Lu"
                },
                {
                    "authorId": "2146329185",
                    "name": "Fan Li"
                },
                {
                    "authorId": "145006560",
                    "name": "L. Carin"
                },
                {
                    "authorId": "46387857",
                    "name": "Chenyang Tao"
                }
            ]
        },
        {
            "paperId": "f34c7d022028dab2c10565880032884d9e30cb35",
            "title": "Building Goal-Oriented Dialogue Systems with Situated Visual Context",
            "abstract": "Goal-oriented dialogue agents can comfortably utilize the conversational context and understand its users' goals. However, in visually driven user experiences, these conversational agents are also required to make sense of the screen context in order to provide a proper interactive experience. In this paper, we propose a novel multimodal conversational framework where the dialogue agent's next action and their arguments are derived jointly conditioned both on the conversational and the visual context. We demonstrate the proposed approach via a prototypical furniture shopping experience for a multimodal virtual assistant.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150290293",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "2078502127",
                    "name": "Jan Jezabek"
                },
                {
                    "authorId": "2052990580",
                    "name": "Arijit Biswas"
                },
                {
                    "authorId": "34974515",
                    "name": "Emre Barut"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                }
            ]
        },
        {
            "paperId": "4d0146679527de1208d801d253ab30f9de92bd39",
            "title": "Modeling Dialogues with Hashcode Representations: A Nonparametric Approach",
            "abstract": "We propose a novel dialogue modeling framework, the first-ever nonparametric kernel functions based approach for dialogue modeling, which learns hashcodes as text representations; unlike traditional deep learning models, it handles well relatively small datasets, while also scaling to large ones. We also derive a novel lower bound on mutual information, used as a model-selection criterion favoring representations with better alignment between the utterances of participants in a collaborative dialogue setting, as well as higher predictability of the generated responses. As demonstrated on three real-life datasets, including prominently psychotherapy sessions, the proposed approach significantly outperforms several state-of-art neural network based dialogue systems, both in terms of computational efficiency, reducing training time from days or weeks to hours, and the response quality, achieving an order of magnitude improvement over competitors in frequency of being chosen as the best model by human evaluators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144529280",
                    "name": "S. Garg"
                },
                {
                    "authorId": "2109771",
                    "name": "I. Rish"
                },
                {
                    "authorId": "40193335",
                    "name": "G. Cecchi"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "3022427",
                    "name": "Sarik Ghazarian"
                },
                {
                    "authorId": "1719898",
                    "name": "G. V. Steeg"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                }
            ]
        },
        {
            "paperId": "6f66e5260574727cfb46c097bfcaf1763e488bf0",
            "title": "From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap",
            "abstract": "Dialogue state tracking (DST) is at the heart of task-oriented dialogue systems. However, the scarcity of labeled data is an obstacle to building accurate and robust state tracking systems that work across a variety of domains. Existing approaches generally require some dialogue data with state information and their ability to generalize to unknown domains is limited. In this paper, we propose using machine reading comprehension (RC) in state tracking from two perspectives: model architectures and datasets. We divide the slot types in dialogue state into categorical or extractive to borrow the advantages from both multiple-choice and span-based reading comprehension models. Our method achieves near the current state-of-the-art in joint goal accuracy on MultiWOZ 2.1 given full training data. More importantly, by leveraging machine reading comprehension datasets, our method outperforms the existing approaches by many a large margin in few-shot scenarios when the availability of in-domain data is limited. Lastly, even without any state tracking data, i.e., zero-shot scenario, our proposed approach achieves greater than 90% average slot accuracy in 12 out of 30 slots in MultiWOZ 2.1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "150290293",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        }
    ]
}