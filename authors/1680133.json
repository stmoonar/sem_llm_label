{
    "authorId": "1680133",
    "papers": [
        {
            "paperId": "102991981ecba8265cbf45643b128faee7778d25",
            "title": "Introduction to the Special Issue on Automotive CPS Safety & Security: Part 1",
            "abstract": "One might argue that automotive and allied domains like robotics serve as the best possible examples of what \u201ccyber-physical systems\u201d (CPS) are. Here, the correctness of the underlying electronics and software (or cyber) components are defined by the dynamics of the vehicle or the robot, viz., the physical components of the system. This shift in perspective on how electronics and software should be modeled and synthesized, and how their correctness should be defined, has led to a tremendous volume of research on CPS in recent times [7, 8, 43, 56]. At the same time, the volume of electronics and software in modern cars has also grown tremendously. Today, high-end cars have more than 100 control computers or electronic control units (ECUs) embedded in them, that run hundreds of millions of lines of software code implementing a range of diverse functions. These functions span across engine and brake control, to the body and entertainment domains. Cars are also equipped with a variety of cameras, radars, and lidar sensors that are used to perceive the external world and take the appropriate control actions as a part of driver assistance features that are common today. As such features continue to accelerate the evolution and adoption of fully autonomous vehicles, the role of electronics and software in the automotive domain is increasing at an unprecedented pace, and modern automobiles are now aptly referred",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144637469",
                    "name": "S. Chakraborty"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "1720382",
                    "name": "Soheil Samii"
                },
                {
                    "authorId": "2266206",
                    "name": "Philipp Mundhenk"
                }
            ]
        },
        {
            "paperId": "217d5e0c5ed75f45256e14e122035eeb9af1722b",
            "title": "ASPEST: Bridging the Gap Between Active Learning and Selective Prediction",
            "abstract": "Selective prediction aims to learn a reliable model that abstains from making predictions when uncertain. These predictions can then be deferred to humans for further evaluation. As an everlasting challenge for machine learning, in many real-world scenarios, the distribution of test data is different from the training data. This results in more inaccurate predictions, and often increased dependence on humans, which can be difficult and expensive. Active learning aims to lower the overall labeling effort, and hence human dependence, by querying the most informative examples. Selective prediction and active learning have been approached from different angles, with the connection between them missing. In this work, we introduce a new learning paradigm, active selective prediction, which aims to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new paradigm, we propose a simple yet effective approach, ASPEST, that utilizes ensembles of model snapshots with self-training with their aggregated outputs as pseudo labels. Extensive experiments on numerous image, text and structured datasets, which suffer from domain shifts, demonstrate that ASPEST can significantly outperform prior work on selective prediction and active learning (e.g. on the MNIST$\\to$SVHN benchmark with the labeling budget of 100, ASPEST improves the AUACC metric from 79.36% to 88.84%) and achieves more optimal utilization of humans in the loop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152425941",
                    "name": "Jiefeng Chen"
                },
                {
                    "authorId": "2144029",
                    "name": "Jinsung Yoon"
                },
                {
                    "authorId": "27556211",
                    "name": "Sayna Ebrahimi"
                },
                {
                    "authorId": "2676352",
                    "name": "Sercan \u00d6. Arik"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "1945962",
                    "name": "Tomas Pfister"
                }
            ]
        },
        {
            "paperId": "349062d82b56fb516b0c7061940470f8d9c256a6",
            "title": "Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems",
            "abstract": "Model-serving systems have become increasingly popular, especially in real-time web applications. In such systems, users send queries to the server and specify the desired performance metrics (e.g., desired accuracy, latency). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks assume a single model can be repeatedly selected for serving inference requests. Modern inference serving systems break this assumption. Thus, they cannot be directly applied to extract a victim model, as models are hidden behind a layer of abstraction exposed by the serving system. An attacker can no longer identify which model she is interacting with. To this end, we first propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accuracy scores within $1\\%$ of the scores obtained when attacking a single, explicitly specified model, as well as up to $14.6\\%$ gain in accuracy and up to $7.7\\%$ gain in fidelity compared to the naive attack. Second, we counter the proposed attack with a noise-based defense mechanism that thwarts fingerprinting by adding noise to the specified performance metrics. The proposed defense strategy reduces the attack's accuracy and fidelity by up to $9.8\\%$ and $4.8\\%$, respectively (on medium-sized model extraction). Third, we show that the proposed defense induces a fundamental trade-off between the level of protection and system goodput, achieving configurable and significant victim model extraction protection while maintaining acceptable goodput ($>80\\%$). We implement the proposed defense in a real system with plans to open source.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140297212",
                    "name": "Debopam Sanyal"
                },
                {
                    "authorId": "2221123728",
                    "name": "Jui-Tse Hung"
                },
                {
                    "authorId": "70486241",
                    "name": "Manavi Agrawal"
                },
                {
                    "authorId": "2221130492",
                    "name": "Prahlad Jasti"
                },
                {
                    "authorId": "2216984120",
                    "name": "Shahab Nikkhoo"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "49980880",
                    "name": "Tianhao Wang"
                },
                {
                    "authorId": "1689767",
                    "name": "Sibin Mohan"
                },
                {
                    "authorId": "144312193",
                    "name": "Alexey Tumanov"
                }
            ]
        },
        {
            "paperId": "34b0e55dcccab464f08b985ccd97984cd9ec7d3b",
            "title": "Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks",
            "abstract": "Adversarial examples threaten the integrity of machine learning systems with alarming success rates even under constrained black-box conditions. Stateful defenses have emerged as an effective countermeasure, detecting potential attacks by maintaining a buffer of recent queries and detecting new queries that are too similar. However, these defenses fundamentally pose a trade-off between attack detection and false positive rates, and this trade-off is typically optimized by hand-picking feature extractors and similarity thresholds that empirically work well. There is little current understanding as to the formal limits of this trade-off and the exact properties of the feature extractors/underlying problem domain that influence it. This work aims to address this gap by offering a theoretical characterization of the trade-off between detection and false positive rates for stateful defenses. We provide upper bounds for detection rates of a general class of feature extractors and analyze the impact of this trade-off on the convergence of black-box attacks. We then support our theoretical findings with empirical evaluations across multiple datasets and stateful defenses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144148953",
                    "name": "Ashish Hooda"
                },
                {
                    "authorId": "2007928171",
                    "name": "Neal Mangaokar"
                },
                {
                    "authorId": "8785459",
                    "name": "Ryan Feng"
                },
                {
                    "authorId": "1910642",
                    "name": "Kassem Fawaz"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "49428285",
                    "name": "Atul Prakash"
                }
            ]
        },
        {
            "paperId": "3c5404de53ad173dc0f8c8261333ce4649eb4890",
            "title": "Stateful Defenses for Machine Learning Models Are Not Yet Secure Against Black-box Attacks",
            "abstract": "Recent work has proposed stateful defense models (SDMs) as a compelling strategy to defend against a black-box attacker who only has query access to the model, as is common for online machine learning platforms. Such stateful defenses aim to defend against black-box attacks by tracking the query history and detecting and rejecting queries that are \"similar\" and thus preventing black-box attacks from finding useful gradients and making progress towards finding adversarial attacks within a reasonable query budget. Recent SDMs (e.g., Blacklight and PIHA) have shown remarkable success in defending against state-of-the-art black-box attacks. In this paper, we show that SDMs are highly vulnerable to a new class of adaptive black-box attacks. We propose a novel adaptive black-box attack strategy called Oracle-guided Adaptive Rejection Sampling (OARS) that involves two stages: (1) use initial query patterns to infer key properties about an SDM's defense; and, (2) leverage those extracted properties to design subsequent query patterns to evade the SDM's defense while making progress towards finding adversarial inputs. OARS is broadly applicable as an enhancement to existing black-box attacks - we show how to apply the strategy to enhance six common black-box attacks to be more effective against current class of SDMs. For example, OARS-enhanced versions of black-box attacks improved attack success rate against recent stateful defenses from almost 0% to to almost 100% for multiple datasets within reasonable query budgets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8785459",
                    "name": "Ryan Feng"
                },
                {
                    "authorId": "144148953",
                    "name": "Ashish Hooda"
                },
                {
                    "authorId": "2007928171",
                    "name": "Neal Mangaokar"
                },
                {
                    "authorId": "1910642",
                    "name": "Kassem Fawaz"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "49428285",
                    "name": "Atul Prakash"
                }
            ]
        },
        {
            "paperId": "51510b3de4823ac647b78939be51e183e5b9b0d5",
            "title": "Learning Modulo Theories",
            "abstract": "Recent techniques that integrate \\emph{solver layers} into Deep Neural Networks (DNNs) have shown promise in bridging a long-standing gap between inductive learning and symbolic reasoning techniques. In this paper we present a set of techniques for integrating \\emph{Satisfiability Modulo Theories} (SMT) solvers into the forward and backward passes of a deep network layer, called SMTLayer. Using this approach, one can encode rich domain knowledge into the network in the form of mathematical formulas. In the forward pass, the solver uses symbols produced by prior layers, along with these formulas, to construct inferences; in the backward pass, the solver informs updates to the network, driving it towards representations that are compatible with the solver's theory. Notably, the solver need not be differentiable. We implement \\layername as a Pytorch module, and our empirical results show that it leads to models that \\emph{1)} require fewer training samples than conventional models, \\emph{2)} that are robust to certain types of covariate shift, and \\emph{3)} that ultimately learn representations that are consistent with symbolic knowledge, and thus naturally interpretable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2623167",
                    "name": "Matt Fredrikson"
                },
                {
                    "authorId": "51153248",
                    "name": "Kaiji Lu"
                },
                {
                    "authorId": "2203247479",
                    "name": "Saranya Vijayakumar"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "1482593422",
                    "name": "Vijay Ganesh"
                },
                {
                    "authorId": "2117418493",
                    "name": "Zifan Wang"
                }
            ]
        },
        {
            "paperId": "5f4b51ce315d6c8048694899b2e481f775276ea4",
            "title": "Stratified Adversarial Robustness with Rejection",
            "abstract": "Recently, there is an emerging interest in adversarially training a classifier with a rejection option (also known as a selective classifier) for boosting adversarial robustness. While rejection can incur a cost in many applications, existing studies typically associate zero cost with rejecting perturbed inputs, which can result in the rejection of numerous slightly-perturbed inputs that could be correctly classified. In this work, we study adversarially-robust classification with rejection in the stratified rejection setting, where the rejection cost is modeled by rejection loss functions monotonically non-increasing in the perturbation magnitude. We theoretically analyze the stratified rejection setting and propose a novel defense method -- Adversarial Training with Consistent Prediction-based Rejection (CPR) -- for building a robust selective classifier. Experiments on image datasets demonstrate that the proposed method significantly outperforms existing methods under strong adaptive attacks. For instance, on CIFAR-10, CPR reduces the total robust loss (for different rejection losses) by at least 7.3% under both seen and unseen attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152425941",
                    "name": "Jiefeng Chen"
                },
                {
                    "authorId": "12998514",
                    "name": "Jayaram Raghuram"
                },
                {
                    "authorId": "2116551496",
                    "name": "Jihye Choi"
                },
                {
                    "authorId": "122112968",
                    "name": "Xi Wu"
                },
                {
                    "authorId": "40609253",
                    "name": "Yingyu Liang"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                }
            ]
        },
        {
            "paperId": "9af40cfce9bd4adc3eb874062b38a23faadc0bb7",
            "title": "Rethinking Diversity in Deep Neural Network Testing",
            "abstract": "Motivated by the success of traditional software testing, numerous diversity measures have been proposed for testing deep neural networks (DNNs). In this study, we propose a shift in perspective, advocating for the consideration of DNN testing as directed testing problems rather than diversity-based testing tasks. We note that the objective of testing DNNs is specific and well-defined: identifying inputs that lead to misclassifications. Consequently, a more precise testing approach is to prioritize inputs with a higher potential to induce misclassifications, as opposed to emphasizing inputs that enhance\"diversity.\"We derive six directed metrics for DNN testing. Furthermore, we conduct a careful analysis of the appropriate scope for each metric, as applying metrics beyond their intended scope could significantly diminish their effectiveness. Our evaluation demonstrates that (1) diversity metrics are particularly weak indicators for identifying buggy inputs resulting from small input perturbations, and (2) our directed metrics consistently outperform diversity metrics in revealing erroneous behaviors of DNNs across all scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117421590",
                    "name": "Zi Wang"
                },
                {
                    "authorId": "2116551496",
                    "name": "Jihye Choi"
                },
                {
                    "authorId": "2260760035",
                    "name": "Keming Wang"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                }
            ]
        },
        {
            "paperId": "9d3463da77f6288da6fa16631034293a733bd719",
            "title": "The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning",
            "abstract": "Pre-training representations (a.k.a. foundation models) has recently become a prevalent learning paradigm, where one first pre-trains a representation using large-scale unlabeled data, and then learns simple predictors on top of the representation using small labeled data from the downstream tasks. There are two key desiderata for the representation: label efficiency (the ability to learn an accurate classifier on top of the representation with a small amount of labeled data) and universality (usefulness across a wide range of downstream tasks). In this paper, we focus on one of the most popular instantiations of this paradigm: contrastive learning with linear probing, i.e., learning a linear predictor on the representation pre-trained by contrastive learning. We show that there exists a trade-off between the two desiderata so that one may not be able to achieve both simultaneously. Specifically, we provide analysis using a theoretical data model and show that, while more diverse pre-training data result in more diverse features for different tasks (improving universality), it puts less emphasis on task-specific features, giving rise to larger sample complexity for down-stream supervised tasks, and thus worse prediction performance. Guided by this analysis, we propose a contrastive regularization method to improve the trade-off. We validate our analysis and method empirically with systematic experiments using real-world datasets and foundation models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "113515522",
                    "name": "Zhenmei Shi"
                },
                {
                    "authorId": "152425941",
                    "name": "Jiefeng Chen"
                },
                {
                    "authorId": "2183338596",
                    "name": "Kunyang Li"
                },
                {
                    "authorId": "12998514",
                    "name": "Jayaram Raghuram"
                },
                {
                    "authorId": "122112968",
                    "name": "Xi Wu"
                },
                {
                    "authorId": "40609253",
                    "name": "Yingyu Liang"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                }
            ]
        },
        {
            "paperId": "a5ab110e6b58f3360e569704d11e4af1681f87e0",
            "title": "Formal Analysis of the API Proxy Problem",
            "abstract": "Implementing a security mechanism on top of APIs requires clear understanding of the semantics of each API, to ensure that security entitlements are enforced consistently and completely across all APIs that could perform the same function for an attacker. Unfortunately, APIs are not designed to be\"semantically orthogonal\"and they often overlap, for example by offering different performance points for the same functionality. This leaves it to the security mechanism to discover and account for API proxies, i.e., groups of APIs which together approximate the functionality of some other API. Lacking a complete view of the structure of the API-proxy relationship, current security mechanisms address it in an ad-hoc and reactive manner, by updating the implementation when new API proxies are uncovered and abused by attackers. We analyze the problem of discovering API-proxy relationships and show that its complexity makes it NP-complete, which makes computing exact information about API proxies prohibitively expensive for modern API surfaces that consist of tens of thousands of APIs. We then propose a simple heuristic algorithm to approximate the same API-proxy information and argue that this overapproximation can be safely used for security purposes, with only the downside of some utility loss. We conclude with a number of open problems of both theoretical and practical interest and with potential directions towards new solutions for the API-proxy problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "8200989",
                    "name": "Mihai Christodorescu"
                },
                {
                    "authorId": "152500280",
                    "name": "A. Pham"
                }
            ]
        }
    ]
}