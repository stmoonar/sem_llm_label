{
    "authorId": "2152122353",
    "papers": [
        {
            "paperId": "665b51c39c17db2758bc5e48fda532ff6872a087",
            "title": "Masked Image Modeling Knowledge Distillation Based on Mutual Learning",
            "abstract": "Abstract: With the rapid improvement of the performance of large models, pre-training a large-scale model and fine-tuning on downstream has become a trend. However, previous knowledge distillation methods mainly concentrates on a specific downstream task. This paper proposes a novel distillation framework Mutual Distillation for Masked Autoencoder Model(MMIM) which transfers knowledge from large-scale pre-trained visual models to provide smaller backbones for downstream tasks, thereby saving time. In our model MMIM, to maintain the advantages of mutual learning, two different student models learn collaboratively to boost both feature representation capabilities simultaneously. At the same time, in order to utilize powerful feature representation capability of pre-trained teacher model, we use the intermediate features of the teacher model to supervise the learning of the student models. Experiments on ImageNet demonstrates the compressed model obtained by our framework possesses powerful feature representation comparable to teacher model. In addition, MMIM also outperforms state-of-the-art knowledge distillation methods which directly distills fine-tuned models. Keywords: Masked Image Modeling; pre-trained Model; Knowledge Distillation; Vision Transformers",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2296752039",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2152122353",
                    "name": "Zexu Du"
                },
                {
                    "authorId": "2149193442",
                    "name": "Wei-Wei Wei"
                },
                {
                    "authorId": "2296276405",
                    "name": "Zongbo Chu"
                }
            ]
        },
        {
            "paperId": "cbe20bb5e40c4fa12d312001906b96a8ba400e18",
            "title": "Lightweight Method for Adaptive Power Grid Operation Safety Control Model Based on Knowledge Distillation",
            "abstract": "Safety control is one of the most important tasks for power grid operation. However, safety control and risk warning models with high accuracy always have multitudinous parameters and need scaled resources for calculation. So as to sort out this problem, we proposed a novel and adaptive model lightweight method based on knowledge distillation. In our method, the intermediate features of the teacher network is utilized to guide the learning of the student network and a regressor is introduced behind the selected middle layer of teacher network to realize dimension match. The prediction confidence of teacher network is mapped to the feature space for the regressor initialization. Besides, the prediction confidence is used to construct the distillation temperature to improve the adaptability of information transfer. Experiments on ImageNet, CIFAR 100 and Grid-specific datasets demonstrate the lightweight model obtained by our method possesses powerful and adaptive feature extraction and representation ability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149193442",
                    "name": "Wei-Wei Wei"
                },
                {
                    "authorId": "2296752039",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2152122353",
                    "name": "Zexu Du"
                },
                {
                    "authorId": "2296276405",
                    "name": "Zongbo Chu"
                },
                {
                    "authorId": "2116019811",
                    "name": "Guoliang Zhang"
                }
            ]
        },
        {
            "paperId": "5a135c3a51f762d52b80edb6ba341b667face748",
            "title": "Information Semantic Mining Method for Risk Warning Scenarios of Electric Power Field Operations",
            "abstract": "Risk management and control of on-site power operations is an important link in the power production process. Traditional methods that rely on manual execution of safety rules lack proactive prevention and control capabilities, and insufficient efficiency leads to an increasing frequency of safety issues in power production. Therefore, it is urgent to adopt intelligent methods to improve the level of safety risk management and control. In response to this issue, this paper proposes a multimodal causal knowledge graph based auxiliary decision-making method for safety risk management in power field operations. Based on the characteristics and requirements of safety risk management for on-site power operations, first obtain multimodal data related to safety risk management for on-site power operations, including video, photo, and text information collected by end devices, as well as historical archive data in various system databases. Then, based on historical risk management rules, the causal relationships of the core elements of on-site work safety issues are sorted out, and the data involved in the multiple types of antecedent influencing factors of historical faults and safety accidents are taken as the causes, and the faults and safety issues are taken as the results, forming a complete causal knowledge sample and constructing a causal effect estimation learning model. This model can output the safety risks of on-site power operations, forming a causal knowledge graph to provide auxiliary support for early warning and pre control decision-making of safety risks in on-site power operations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2296276405",
                    "name": "Zongbo Chu"
                },
                {
                    "authorId": "2149193442",
                    "name": "Wei-Wei Wei"
                },
                {
                    "authorId": "2296752039",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2152122353",
                    "name": "Zexu Du"
                }
            ]
        },
        {
            "paperId": "981691abcfc3ad075f38bb86946cf8d4bca8d050",
            "title": "Cross-Domain Character Recognition through Latent Space Alignment",
            "abstract": "Deep neural networks have proved its capability in many machine learning tasks. The effectiveness of deep neural networks in real-world applications, however, is greatly affected by the distribution discrepancy between the training and testing data. To address the issue, domain adaptation methods have been studied. In this work, we propose a novel unsupervised domain adaptation method which combines the feature learning and the distribution estimation into one learning framework, enabling automatic update of feature representations through fine-tuning parameterized distributions. As such, our model can produce an unified distribution to represent both source and target samples. Furthermore, two new regularizers are integrated into the optimization objective to minimize the divergence of the unified distribution from those of source and target domains. Experiments on character reconstruction show that our method demonstrates much better learning ability compared to the existing variational autoencoder. More importantly, our method improves recognition accuracy by more than 5% from that of the state-of-the-art methods in domain adaptation tasks built upon popular character datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3207491",
                    "name": "Chunpeng Wu"
                },
                {
                    "authorId": "2152122353",
                    "name": "Zexu Du"
                },
                {
                    "authorId": "2055055020",
                    "name": "Jinrui Gan"
                },
                {
                    "authorId": "2116019811",
                    "name": "Guoliang Zhang"
                },
                {
                    "authorId": "2153604287",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2153691345",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "47561503",
                    "name": "Peng Wu"
                },
                {
                    "authorId": "2190851126",
                    "name": "Fei Zhou"
                }
            ]
        }
    ]
}