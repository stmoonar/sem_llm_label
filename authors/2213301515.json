{
    "authorId": "2213301515",
    "papers": [
        {
            "paperId": "1f6f0f535d240fe8270c3ef65a051f604b404d04",
            "title": "WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions",
            "abstract": "Language Models (LMs) are being proposed for mental health applications where the heightened risk of adverse outcomes means predictive performance may not be a sufficient litmus test of a model's utility in clinical practice. A model that can be trusted for practice should have a correspondence between explanation and clinical determination, yet no prior research has examined the attention fidelity of these models and their effect on ground truth explanations. We introduce an evaluation design that focuses on the robustness and explainability of LMs in identifying Wellness Dimensions (WDs). We focus on two existing mental health and well-being datasets: (a) Multi-label Classification-based MultiWD, and (b) WellXplain for evaluating attention mechanism veracity against expert-labeled explanations. The labels are based on Halbert Dunn's theory of wellness, which gives grounding to our evaluation. We reveal four surprising results about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on WellXplain fails to deliver any remarkable improvements in performance or explanations. (2) Re-examining LMs' predictions based on a confidence-oriented loss function reveals a significant performance drop. (3) Across all LMs/LLMs, the alignment between attention and explanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific knowledge and undervalue explanations, causing these discrepancies. This study highlights the need for further research into their consistency and explanations in mental health and well-being.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307078365",
                    "name": "Seyedali Mohammadi"
                },
                {
                    "authorId": "2300175110",
                    "name": "Edward Raff"
                },
                {
                    "authorId": "2213301515",
                    "name": "Jinendra Malekar"
                },
                {
                    "authorId": "2257214339",
                    "name": "Booz Allen"
                },
                {
                    "authorId": "2257208467",
                    "name": "Hamilton"
                },
                {
                    "authorId": "2304551858",
                    "name": "Francis Ferraro"
                },
                {
                    "authorId": "2307092529",
                    "name": "Manas Gaur"
                },
                {
                    "authorId": "2296100360",
                    "name": "Yu Sun"
                },
                {
                    "authorId": "104463827",
                    "name": "Shuohuan Wang"
                },
                {
                    "authorId": "2307219223",
                    "name": "Yukun Li"
                },
                {
                    "authorId": "1718657",
                    "name": "Shikun Feng"
                },
                {
                    "authorId": "2109214103",
                    "name": "Xuyi Chen"
                },
                {
                    "authorId": "2307629857",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "2307084716",
                    "name": "Xin Tian"
                },
                {
                    "authorId": "152867082",
                    "name": "Danxiang Zhu"
                },
                {
                    "authorId": "2307258766",
                    "name": "Hao Tian"
                },
                {
                    "authorId": "2307077763",
                    "name": "Hua Wu. 2019"
                },
                {
                    "authorId": "2307091973",
                    "name": "Ernie"
                },
                {
                    "authorId": "2290402489",
                    "name": "Hugo Touvron"
                },
                {
                    "authorId": "46183616",
                    "name": "Thibaut Lavril"
                },
                {
                    "authorId": "2261981869",
                    "name": "Gautier Izacard"
                },
                {
                    "authorId": "1490887583",
                    "name": "Xavier Martinet"
                },
                {
                    "authorId": "114952298",
                    "name": "Marie-Anne Lachaux"
                },
                {
                    "authorId": "47733973",
                    "name": "Timoth\u00e9e Lacroix"
                },
                {
                    "authorId": "3361236",
                    "name": "Baptiste Rozi\u00e8re"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2072738644",
                    "name": "Eric Hambro"
                }
            ]
        },
        {
            "paperId": "d79d7d6f47aebf062186589256800e860372a2d1",
            "title": "Matmul or No Matmul in the Era of 1-bit LLMs",
            "abstract": "The advent of 1-bit large language models (LLMs) has attracted considerable attention and opened up new research opportunities. However, 1-bit LLMs only improve a fraction of models by applying extreme quantization to the projection layers while leaving attention heads unchanged. Therefore, to avoid fundamentally wrong choices of goals in future research, it is crucial to understand the actual improvements in computation and memory usage that 1-bit LLMs can deliver. In this work, we present an adaptation of Amdahl's Law tailored for the 1-bit LLM context, which illustrates how partial improvements in 1-bit LLMs impact overall model performance. Through extensive experiments, we uncover key nuances across different model architectures and hardware configurations, offering a roadmap for future research in the era of 1-bit LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2213301515",
                    "name": "Jinendra Malekar"
                },
                {
                    "authorId": "2220459202",
                    "name": "Mohammed E. Elbtity"
                },
                {
                    "authorId": "2806817",
                    "name": "Ramtin Zand"
                }
            ]
        },
        {
            "paperId": "6b0cb6f3f6a644087353e577d17057fadb62e836",
            "title": "Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case",
            "abstract": "After the pandemic, artificial intelligence (AI) powered support for mental health care has become increasingly important. The breadth and complexity of significant challenges required to provide adequate care involve:\n(a) Personalized patient understanding, (b) Safety-constrained and medically validated chatbot patient interactions, and (c) Support for continued feedback-based refinements in design using chatbot-patient interactions. \nWe propose Alleviate, a chatbot designed to assist patients suffering from mental health challenges with personalized care and assist clinicians with understanding their patients better. Alleviate draws from an array of publicly available clinically valid mental-health texts and databases, allowing Alleviate to make medically sound and informed decisions. In addition, Alleviate's modular design and explainable decision-making lends itself to robust and continued feedback-based refinements to its design. In this paper, we explain the different modules of Alleviate and submit a short video demonstrating Alleviate's capabilities to help patients and clinicians understand each other better to facilitate optimal care strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "50840713",
                    "name": "Vedant Khandelwal"
                },
                {
                    "authorId": "46178306",
                    "name": "Raxit Goswami"
                },
                {
                    "authorId": "2092591535",
                    "name": "Nathan Dolbir"
                },
                {
                    "authorId": "2213301515",
                    "name": "Jinendra Malekar"
                },
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                }
            ]
        },
        {
            "paperId": "853fc07ed068a0e58ef6c80186eb080df73251ce",
            "title": "Process Knowledge-infused Learning for Clinician-friendly Explanations",
            "abstract": "Language models have the potential to assess mental health using social media data. By analyzing online posts and conversations, these models can detect patterns indicating mental health conditions like depression, anxiety, or suicidal thoughts. They examine keywords, language markers, and sentiment to gain insights into an individual\u2019s mental well-being. This information is crucial for early detection, intervention, and support, improving mental health care and prevention strategies. However, using language models for mental health assessments from social media has two limitations: (1) They do not compare posts against clinicians\u2019 diagnostic processes, and (2) It\u2019s challenging to explain language model outputs using concepts that the clinician can understand, i.e., clinician-friendly explanations. In this study, we introduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm that layers clinical process knowledge structures on language model outputs, enabling clinician-friendly explanations of the underlying language model predictions. We rigorously test our methods on existing benchmark datasets, augmented with such clinical process knowledge, and release a new dataset for assessing suicidality. PKiL performs competitively, achieving a 70% agreement with users, while other XAI methods only achieve 47% agreement (average inter-rater agreement of 0.72). Our evaluations demonstrate that PK-iL effectively explains model predictions to clinicians.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "2187301061",
                    "name": "Yuxin Zi"
                },
                {
                    "authorId": "1491238594",
                    "name": "Manas Gaur"
                },
                {
                    "authorId": "2213301515",
                    "name": "Jinendra Malekar"
                },
                {
                    "authorId": "2145908831",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2068096390",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        }
    ]
}