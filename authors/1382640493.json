{
    "authorId": "1382640493",
    "papers": [
        {
            "paperId": "0fbcc72af00baf20e28e5aaa6374e7026dd153bc",
            "title": "Toward High Performance, Programmable Extreme-Edge Intelligence for Neuromorphic Vision Sensors utilizing Magnetic Domain Wall Motion-based MTJ",
            "abstract": "The desire to empower resource-limited edge devices with computer vision (CV) must overcome the high energy consumption of collecting and processing vast sensory data. To address the challenge, this work proposes an energy-efficient non-von-Neumann in-pixel processing solution for neuromorphic vision sensors employing emerging (X) magnetic domain wall magnetic tunnel junction (MDWMTJ) for the first time, in conjunction with CMOS-based neuromorphic pixels. Our hybrid CMOS+X approach performs in-situ massively parallel asynchronous analog convolution, exhibiting low power consumption and high accuracy across various CV applications by leveraging the non-volatility and programmability of the MDWMTJ. Moreover, our developed device-circuit-algorithm co-design framework captures device constraints (low tunnel-magnetoresistance, low dynamic range) and circuit constraints (non-linearity, process variation, area consideration) based on monte-carlo simulations and device parameters utilizing GF22nm FD-SOI technology. Our experimental results suggest we can achieve an average of 45.3% reduction in backend-processor energy, maintaining similar front-end energy compared to the state-of-the-art and high accuracy of 79.17% and 95.99% on the DVS-CIFAR10 and IBM DVS128-Gesture datasets, respectively.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2181915328",
                    "name": "Md. Abdullah-Al Kaiser"
                },
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                },
                {
                    "authorId": "2468479",
                    "name": "Akhilesh R. Jaiswal"
                }
            ]
        },
        {
            "paperId": "436e9ba3db298820949bc4f62a50b919bed0a97d",
            "title": "Energy-Efficient&Real-Time Computer Vision with Intelligent Skipping via Reconfigurable CMOS Image Sensors",
            "abstract": "Current video-based computer vision (CV) applications typically suffer from high energy consumption due to reading and processing all pixels in a frame, regardless of their significance. While previous works have attempted to reduce this energy by skipping input patches or pixels and using feedback from the end task to guide the skipping algorithm, the skipping is not performed during the sensor read phase. As a result, these methods can not optimize the front-end sensor energy. Moreover, they may not be suitable for real-time applications due to the long latency of modern CV networks that are deployed in the back-end. To address this challenge, this paper presents a custom-designed reconfigurable CMOS image sensor (CIS) system that improves energy efficiency by selectively skipping uneventful regions or rows within a frame during the sensor's readout phase, and the subsequent analog-to-digital conversion (ADC) phase. A novel masking algorithm intelligently directs the skipping process in real-time, optimizing both the front-end sensor and back-end neural networks for applications including autonomous driving and augmented/virtual reality (AR/VR). Our system can also operate in standard mode without skipping, depending on application needs. We evaluate our hardware-algorithm co-design framework on object detection based on BDD100K and ImageNetVID, and gaze estimation based on OpenEDS, achieving up to 53% reduction in front-end sensor energy while maintaining state-of-the-art (SOTA) accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181915328",
                    "name": "Md. Abdullah-Al Kaiser"
                },
                {
                    "authorId": "30641664",
                    "name": "Sreetama Sarkar"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                },
                {
                    "authorId": "2316433373",
                    "name": "Akhilesh Jaiswal"
                },
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                }
            ]
        },
        {
            "paperId": "7840d718b1730e98d09ce774462fe907aaad4cb0",
            "title": "Training Ultra-Low-Latency Spiking Neural Networks from Scratch",
            "abstract": "Spiking Neural networks (SNN) have emerged as an attractive spatio-temporal computing paradigm for a wide range of low-power vision tasks. However, state-of-the-art (SOTA) SNN models either incur multiple time steps which hinder their deployment in real-time use cases or increase the training complexity significantly. To mitigate this concern, we present a training framework (from scratch) for SNNs with ultra-low (down to 1) time steps that leverages the Hoyer regularizer. We calculate the threshold for each BANN layer as the Hoyer extremum of a clipped version of its activation map. The clipping value is determined through training using gradient descent with our Hoyer regularizer. We evaluate the efficacy of our training framework on large-scale vision tasks, including traditional and event-based image recognition and object detection. Our experiments demonstrate up to 34\u00d7 increase in compute efficiency with a marginal accuracy/mAP drop compared to non-spiking networks. Finally, we implement our framework in the Lava-DL library, thereby enabling the deployment of our SNN models in the Loihi neuromorphic chip.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "2243367696",
                    "name": "Zeyu Liu"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                }
            ]
        },
        {
            "paperId": "9e80727551b36ff076e85d20b7b674d62bc8119f",
            "title": "LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units",
            "abstract": "Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and SOTA performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LMUFormer, which augments the LMU with convolutional patch embedding and convolutional channel mixer. Moreover, we present a spiking version of this architecture, which introduces the benefit of states within the patch embedding and channel mixer modules while simultaneously reducing the computing complexity. We evaluated our architectures on multiple sequence datasets. In comparison to SOTA transformer-based models within the ANN domain on the SCv2 dataset, our LMUFormer demonstrates comparable performance while necessitating a remarkable 53 times reduction in parameters and a substantial 65 times decrement in FLOPs. Additionally, owing to our model's proficiency in real-time data processing, we can achieve a 32.03% reduction in sequence length, all while incurring an inconsequential decline in performance. Our code is publicly available at https://github.com/zeyuliu1037/LMUFormer.git.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2243367696",
                    "name": "Zeyu Liu"
                },
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "2269117149",
                    "name": "Anni Li"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                }
            ]
        },
        {
            "paperId": "b133323076568301c36ceab22104df20484eedfa",
            "title": "MaskVD: Region Masking for Efficient Video Object Detection",
            "abstract": "Video tasks are compute-heavy and thus pose a challenge when deploying in real-time applications, particularly for tasks that require state-of-the-art Vision Transformers (ViTs). Several research efforts have tried to address this challenge by leveraging the fact that large portions of the video undergo very little change across frames, leading to redundant computations in frame-based video processing. In particular, some works leverage pixel or semantic differences across frames, however, this yields limited latency benefits with significantly increased memory overhead. This paper, in contrast, presents a strategy for masking regions in video frames that leverages the semantic information in images and the temporal correlation between frames to significantly reduce FLOPs and latency with little to no penalty in performance over baseline models. In particular, we demonstrate that by leveraging extracted features from previous frames, ViT backbones directly benefit from region masking, skipping up to 80% of input regions, improving FLOPs and latency by 3.14x and 1.5x. We improve memory and latency over the state-of-the-art (SOTA) by 2.3x and 1.14x, while maintaining similar detection performance. Additionally, our approach demonstrates promising results on convolutional neural networks (CNNs) and provides latency improvements over the SOTA up to 1.3x using specialized computational kernels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30641664",
                    "name": "Sreetama Sarkar"
                },
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "2965493",
                    "name": "Souvik Kundu"
                },
                {
                    "authorId": "2302328543",
                    "name": "Kai Zheng"
                },
                {
                    "authorId": "1857633658",
                    "name": "Chirayata Bhattacharyya"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                }
            ]
        },
        {
            "paperId": "1e9591635b3558b4192853ff6169b747e7470f56",
            "title": "Technology-Circuit-Algorithm Tri-Design for Processing-in-Pixel-in-Memory (P2M)",
            "abstract": "The massive amounts of data generated by camera sensors motivate data processing inside pixel arrays, i.e., at the extreme-edge. Several critical developments have fueled recent interest in the processing-in-pixel-in-memory paradigm for a wide range of visual machine intelligence tasks, including (1) advances in 3D integration technology to enable complex processing inside each pixel in a 3D integrated manner while maintaining pixel density, (2) analog processing circuit techniques for massively parallel low-energy in-pixel computations, and (3) algorithmic techniques to mitigate non-idealities associated with analog processing through hardware-aware training schemes. This article presents a comprehensive technology-circuit-algorithm landscape that connects technology capabilities, circuit design strategies, and algorithmic optimizations to power, performance, area, bandwidth reduction, and application-level accuracy metrics. We present our results using a comprehensive co-design framework incorporating hardware and algorithmic optimizations for various complex real-life visual intelligence tasks mapped onto our P2M paradigm.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2181915328",
                    "name": "Md. Abdullah-Al Kaiser"
                },
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "30641664",
                    "name": "Sreetama Sarkar"
                },
                {
                    "authorId": "2965493",
                    "name": "Souvik Kundu"
                },
                {
                    "authorId": "2069534525",
                    "name": "Zihan Yin"
                },
                {
                    "authorId": "1699602385",
                    "name": "Manas Garg"
                },
                {
                    "authorId": "2167031766",
                    "name": "Ajey P. Jacob"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                },
                {
                    "authorId": "2468479",
                    "name": "Akhilesh R. Jaiswal"
                }
            ]
        },
        {
            "paperId": "37dc332349d1d10b3c3d4d5dbd31801ecc7957c8",
            "title": "FireFly: A Synthetic Dataset for Ember Detection in Wildfire",
            "abstract": "This paper presents \"FireFly\", a synthetic dataset for ember detection created using Unreal Engine 4 (UE4), designed to overcome the current lack of ember-specific training resources. To create the dataset, we present a tool that allows the automated generation of the synthetic labeled dataset with adjustable parameters, enabling data diversity from various environmental conditions, making the dataset both diverse and customizable based on user requirements. We generated a total of 19,273 frames that have been used to evaluate FireFly on four popular object detection models. Further to minimize human intervention, we leveraged a trained model to create a semi-automatic labeling process for real-life ember frames. Moreover, we demonstrated an up to 8.57% improvement in mean Average Precision (mAP) in real-world wildfire scenarios compared to models trained exclusively on a small real dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145171629",
                    "name": "Yue Hu"
                },
                {
                    "authorId": "2147245139",
                    "name": "Xin-Yu Ye"
                },
                {
                    "authorId": "2201282948",
                    "name": "Yifei Liu"
                },
                {
                    "authorId": "2965493",
                    "name": "Souvik Kundu"
                },
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "2230108904",
                    "name": "Srikar Mutnuri"
                },
                {
                    "authorId": "2148629286",
                    "name": "Namo Asavisanu"
                },
                {
                    "authorId": "2247162",
                    "name": "Nora Ayanian"
                },
                {
                    "authorId": "9313028",
                    "name": "K. Psounis"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                }
            ]
        },
        {
            "paperId": "56045e6ec8ac23cc7954fdb14d8e16dfb9667a3b",
            "title": "Bridging the Gap Between Spiking Neural Networks & LSTMs for Latency & Energy Efficiency",
            "abstract": "Spiking Neural Networks (SNNs) have emerged as an attractive spatio-temporal computing paradigm for complex vision tasks. However, most existing works yield models that require many time steps and do not leverage the inherent temporal dynamics of spiking neural networks, even for sequential tasks. Motivated by this observation, we propose an optimized spiking long short-term memory networks (LSTM) training framework that involves a novel ANN-to-SNN conversion framework, followed by SNN fine-tuning via backpropagation through time (BPTT). In particular, we propose novel activation functions in the source LSTM architecture and convert a judiciously selected subset of them to leaky-integrate-and-fire (LIF) activations with optimal bias shifts. Moreover, we propose a pipelined parallel processing scheme that hides the SNN time steps, significantly improving system latency, especially for long sequences. The resulting SNNs have high activation sparsity and require only accumulate operations (AC), in contrast to expensive multiply-and-accumulates (MAC) needed for ANNs, except for the input layer when using direct encoding, yielding significant improvements in energy efficiency. We evaluate our framework on sequential learning tasks including temporal MNIST, Google Speech Commands (GSC), and UCI Smartphone datasets on different LSTM architectures. We obtain test accuracy of 94.75 % with only 2 time steps on the GSC dataset with $\\sim 4.1\\times$ lower energy than an iso-architecture standard LSTM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "1514604984",
                    "name": "Haoqing Deng"
                },
                {
                    "authorId": "2242939423",
                    "name": "Robert S. Aviles"
                },
                {
                    "authorId": "2243367696",
                    "name": "Zeyu Liu"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                }
            ]
        },
        {
            "paperId": "5f14ea2e200b8419addcad8fce21dbfa427778f2",
            "title": "Design Considerations for 3D Heterogeneous Integration Driven Analog Processing-in-Pixel for Extreme-Edge Intelligence",
            "abstract": "Given the progress in computer vision, image sensors are broadening their capabilities, which requires adding data processing close to or within the pixel chips. In this context, in-pixel computing has emerged as a notable paradigm, offering the capability to process data within the pixel unit itself. Interestingly, state-of-art in-pixel paradigms rely on high-density 3D heterogeneous integration to establish a per-pixel connection with vertically aligned analog processing units. This article provides a comprehensive review of the most recent developments in in-pixel computing and its relation to 3D heterogeneous integration. It offers an in-depth examination of innovative circuit design, adaptations in algorithms, and the challenges in 3D integration technology for sensor chips, thereby presenting a holistic perspective on the future trajectory of in-pixel computing driven by advances in 3D integration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2069534525",
                    "name": "Zihan Yin"
                },
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "2181915328",
                    "name": "Md. Abdullah-Al Kaiser"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                },
                {
                    "authorId": "2167031766",
                    "name": "Ajey P. Jacob"
                },
                {
                    "authorId": "2468479",
                    "name": "Akhilesh R. Jaiswal"
                }
            ]
        },
        {
            "paperId": "6bdafb965e94c5240db2c30f20c37c4b4dd0e451",
            "title": "ViTA: A Vision Transformer Inference Accelerator for Edge Applications",
            "abstract": "Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2208654904",
                    "name": "Shashank Nag"
                },
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "2965493",
                    "name": "Souvik Kundu"
                },
                {
                    "authorId": "2953173",
                    "name": "N. Chandrachoodan"
                },
                {
                    "authorId": "2658716",
                    "name": "P. Beerel"
                }
            ]
        }
    ]
}