{
    "authorId": "24868638",
    "papers": [
        {
            "paperId": "067fea2029918b7b46e70b327aab8da9502ff515",
            "title": "Leveraging Open Information Extraction for Improving Few-Shot Trigger Detection Domain Transfer",
            "abstract": "Event detection is a crucial information extraction task in many domains, such as Wikipedia or news. The task typically relies on trigger detection (TD) \u2013 identifying token spans in the text that evoke specific events. While the notion of triggers should ideally be universal across domains, domain transfer for TD from high-to low-resource domains results in significant performance drops. We address the problem of negative transfer for TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system. We demonstrate that relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero-and few-shot TD domain transfer and reducing negative transfer, in particular when transferring from a high-resource source Wikipedia domain to a low-resource target news domain. Additionally, we combine the extracted relations with masked language modeling on the target domain and obtain further TD performance gains. Finally, we demonstrate that the results are robust to the choice of the OIE system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218665654",
                    "name": "David Duki'c"
                },
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "1666177566",
                    "name": "Goran Glavavs"
                },
                {
                    "authorId": "1621659226",
                    "name": "Jan vSnajder"
                }
            ]
        },
        {
            "paperId": "1d5a3c90e9a1d6309ef6b59296fb650826542219",
            "title": "Robust Text Classification: Analyzing Prototype-Based Networks",
            "abstract": "Downstream applications often require text classification models to be accurate and robust. While the accuracy of the state-of-the-art Language Models (LMs) approximates human performance, they often exhibit a drop in performance on noisy data found in the real world. This lack of robustness can be concerning, as even small perturbations in the text, irrelevant to the target task, can cause classifiers to incorrectly change their predictions. A potential solution can be the family of Prototype-Based Networks (PBNs) that classifies examples based on their similarity to prototypical examples of a class (prototypes) and has been shown to be robust to noise for computer vision tasks. In this paper, we study whether the robustness properties of PBNs transfer to text classification tasks under both targeted and static adversarial attack settings. Our results show that PBNs, as a mere architectural variation of vanilla LMs, offer more robustness compared to vanilla LMs under both targeted and static settings. We showcase how PBNs' interpretability can help us to understand PBNs' robustness properties. Finally, our ablation studies reveal the sensitivity of PBNs' robustness to how strictly clustering is done in the training phase, as tighter clustering results in less robust PBNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2196148017",
                    "name": "Zhivar Sourati"
                },
                {
                    "authorId": "2069729843",
                    "name": "D. Deshpande"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "3467930",
                    "name": "S. Saralajew"
                }
            ]
        },
        {
            "paperId": "48a3e2cfca3552ae04a021c7b0e6fec1a320a1eb",
            "title": "Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection",
            "abstract": "Event detection is a crucial information extraction task in many domains, such as Wikipedia or news. The task typically relies on trigger detection (TD) \u2013 identifying token spans in the text that evoke specific events. While the notion of triggers should ideally be universal across domains, domain transfer for TD from high- to low-resource domains results in significant performance drops. We address the problem of negative transfer in TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system. We demonstrate that OIE relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero- and few-shot TD domain transfer and reducing performance drops, in particular when transferring from a high-resource source domain (Wikipedia) to a low(er)-resource target domain (news). Additionally, we combine this improved transfer with masked language modeling on the target domain, observing further TD transfer gains. Finally, we demonstrate that the gains are robust to the choice of the OIE system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218665654",
                    "name": "David Duki'c"
                },
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "1666177566",
                    "name": "Goran Glavavs"
                },
                {
                    "authorId": "1621659226",
                    "name": "Jan vSnajder"
                }
            ]
        },
        {
            "paperId": "59f3b696530ebe1599e7f653f175a95349babd89",
            "title": "Linking Surface Facts to Large-Scale Knowledge Graphs",
            "abstract": "Open Information Extraction (OIE) methods extract facts from natural language text in the form of (\"subject\";\"relation\";\"object\") triples. These facts are, however, merely surface forms, the ambiguity of which impedes their downstream usage; e.g., the surface phrase\"Michael Jordan\"may refer to either the former basketball player or the university professor. Knowledge Graphs (KGs), on the other hand, contain facts in a canonical (i.e., unambiguous) form, but their coverage is limited by a static schema (i.e., a fixed set of entities and predicates). To bridge this gap, we need the best of both worlds: (i) high coverage of free-text OIEs, and (ii) semantic precision (i.e., monosemy) of KGs. In order to achieve this goal, we propose a new benchmark with novel evaluation protocols that can, for example, measure fact linking performance on a granular triple slot level, while also measuring if a system has the ability to recognize that a surface form has no match in the existing KG. Our extensive evaluation of several baselines show that detection of out-of-KG entities and predicates is more difficult than accurate linking to existing ones, thus calling for more research efforts on this difficult task. We publicly release all resources (data, benchmark and code) on https://github.com/nec-research/fact-linking.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51016420",
                    "name": "Gorjan Radevski"
                },
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "11618346",
                    "name": "Chia-Chien Hung"
                },
                {
                    "authorId": "2261363939",
                    "name": "Carolin Lawrence"
                },
                {
                    "authorId": "1666177566",
                    "name": "Goran Glavavs"
                }
            ]
        },
        {
            "paperId": "8e8a1489bf4d782d2435cdeb93f7d1f165747c63",
            "title": "Large Language Models Enable Few-Shot Clustering",
            "abstract": "Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user\u2019s intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model (LLM) can amplify an expert\u2019s guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find that incorporating LLMs in the first two stages routinely provides significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2061499362",
                    "name": "Vijay Viswanathan"
                },
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "19752252",
                    "name": "Carolin (Haas) Lawrence"
                },
                {
                    "authorId": "35232494",
                    "name": "Tongshuang Sherry Wu"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "55f44d9024463486955151c4e75b520cf56b7697",
            "title": "A Human-Centric Assessment Framework for AI",
            "abstract": "With the rise of AI systems in real-world applications comes the need for reliable and trustworthy AI. An essential aspect of this are explainable AI systems. However, there is no agreed standard on how explainable AI systems should be assessed. Inspired by the Turing test, we introduce a human-centric assessment framework where a leading domain expert accepts or rejects the solutions of an AI system and another domain expert. By comparing the acceptance rates of provided solutions, we can assess how the AI system performs compared to the domain expert, and whether the AI system's explanations (if provided) are human-understandable. This setup -- comparable to the Turing test -- can serve as a framework for a wide range of human-centric AI system assessments. We demonstrate this by presenting two instantiations: (1) an assessment that measures the classification accuracy of a system with the option to incorporate label uncertainties; (2) an assessment where the usefulness of provided explanations is determined in a human-centric manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3467930",
                    "name": "S. Saralajew"
                },
                {
                    "authorId": "40515722",
                    "name": "Ammar Shaker"
                },
                {
                    "authorId": "2166335112",
                    "name": "Zhao Xu"
                },
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "2105201",
                    "name": "Bhushan Kotnis"
                },
                {
                    "authorId": "2166312218",
                    "name": "Wiem Ben-Rim"
                },
                {
                    "authorId": "152845986",
                    "name": "J\u00fcrgen Quittek"
                },
                {
                    "authorId": "19752252",
                    "name": "Carolin (Haas) Lawrence"
                }
            ]
        },
        {
            "paperId": "c33e99e90d066319866de9e0768e01b83360d1ab",
            "title": "KGxBoard: Explainable and Interactive Leaderboard for Evaluation of Knowledge Graph Completion Models",
            "abstract": "Knowledge Graphs (KGs) store information in the form of (head, predicate, tail)-triples. To augment KGs with new knowledge, researchers proposed models for KG Completion (KGC) tasks such as link prediction; i.e., answering (h; p; ?) or (?; p; t) queries. Such models are usually evaluated with averaged metrics on a held-out test set. While useful for tracking progress, averaged single-score metrics cannot reveal what exactly a model has learned -- or failed to learn. To address this issue, we propose KGxBoard: an interactive framework for performing fine-grained evaluation on meaningful subsets of the data, each of which tests individual and interpretable capabilities of a KGC model. In our experiments, we highlight the findings that we discovered with the use of KGxBoard, which would have been impossible to detect with standard averaged single-score metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091675826",
                    "name": "Haris Widjaja"
                },
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "2037487498",
                    "name": "Wiem Ben Rim"
                },
                {
                    "authorId": "144118452",
                    "name": "Pengfei Liu"
                },
                {
                    "authorId": "3200180",
                    "name": "Christopher Malon"
                },
                {
                    "authorId": "8792454",
                    "name": "Daniel Ruffinelli"
                },
                {
                    "authorId": "19752252",
                    "name": "Carolin (Haas) Lawrence"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "f4c63c64eb3a29630a73753a1d8185f004a167df",
            "title": "Human-Centric Research for NLP: Towards a Definition and Guiding Questions",
            "abstract": "With Human-Centric Research (HCR) we can steer research activities so that the research outcome is bene\ufb01cial for human stakeholders, such as end users. But what exactly makes research human-centric ? We address this question by providing a working de\ufb01nition and de\ufb01ne how a research pipeline can be split into different stages in which human-centric components can be added. Additionally, we discuss existing NLP with HCR components and de\ufb01ne a series of guiding questions , which can serve as starting points for researchers interested in exploring human-centric research approaches. We hope that this work would inspire researchers to re\ufb01ne the proposed de\ufb01nition and to pose other questions that might be meaningful for achieving HCR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2105201",
                    "name": "Bhushan Kotnis"
                },
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "108564082",
                    "name": "J. Gastinger"
                },
                {
                    "authorId": "2275344",
                    "name": "G. Serra"
                },
                {
                    "authorId": "2819104",
                    "name": "F. Alesiani"
                },
                {
                    "authorId": "2315527",
                    "name": "T. Sztyler"
                },
                {
                    "authorId": "40515722",
                    "name": "Ammar Shaker"
                },
                {
                    "authorId": "2175653032",
                    "name": "Na Gong"
                },
                {
                    "authorId": "19752252",
                    "name": "Carolin (Haas) Lawrence"
                },
                {
                    "authorId": "2166335112",
                    "name": "Zhao Xu"
                }
            ]
        },
        {
            "paperId": "0a0bbbdc195185b2aab93e99ddaeb12bed43d5ee",
            "title": "AnnIE: An Annotation Platform for Constructing Complete Open Information Extraction Benchmark",
            "abstract": "Open Information Extraction (OIE) is the task of extracting facts from sentences in the form of relations and their corresponding arguments in schema-free manner. Intrinsic performance of OIE systems is difficult to measure due to the incompleteness of existing OIE benchmarks: ground truth extractions do not group all acceptable surface realizations of the same fact that can be extracted from a sentence. To measure performance of OIE systems more realistically, it is necessary to manually annotate complete facts (i.e., clusters of all acceptable surface realizations of the same fact) from input sentences. We propose AnnIE: an interactive annotation platform that facilitates such challenging annotation tasks and supports creation of complete fact-oriented OIE evaluation benchmarks. AnnIE is modular and flexible in order to support different use case scenarios (i.e., benchmarks covering different types of facts) and different languages. We use AnnIE to build two complete OIE benchmarks: one with verb-mediated facts and another with facts encompassing named entities. We evaluate several OIE systems on our complete benchmarks created with AnnIE. We publicly release AnnIE (and all gold datasets generated with it) under non-restrictive license.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2062924459",
                    "name": "Niklas Friedrich"
                },
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "98885420",
                    "name": "Mingying Yu"
                },
                {
                    "authorId": "2105201",
                    "name": "Bhushan Kotnis"
                },
                {
                    "authorId": "2056385822",
                    "name": "Caroline V. Lawrence"
                },
                {
                    "authorId": "2780262",
                    "name": "Mathias Niepert"
                },
                {
                    "authorId": "1666177566",
                    "name": "Goran Glavavs"
                }
            ]
        },
        {
            "paperId": "2c51f0a3a7e4f4730805a2cf786d43f011d1007b",
            "title": "BenchIE: Open Information Extraction Evaluation Based on Facts, Not Tokens",
            "abstract": "Intrinsic evaluations of OIE systems are carried out either manually\u2014with human evaluators judging the correctness of extractions\u2014 or automatically, on standardized benchmarks. The latter, while much more cost-effective, is less reliable, primarily because of the in-completeness of the existing OIE benchmarks: the ground truth extractions do not include all acceptable variants of the same fact, leading to unreliable assessment of models\u2019 performance. Moreover, the existing OIE benchmarks are available for English only. In this work, we introduce BenchIE: a benchmark and evaluation framework for comprehensive evaluation of OIE systems for English, Chinese and German. In contrast to existing OIE benchmarks, BenchIE takes into account informational equivalence of extractions: our gold standard consists of fact synsets , clusters in which we exhaustively list all surface forms of the same fact. We benchmark several state-of-the-art OIE systems using BenchIE and demonstrate that these systems are signi\ufb01-cantly less effective than indicated by existing OIE benchmarks. We make BenchIE (data and evaluation code) publicly available 1 .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24868638",
                    "name": "Kiril Gashteovski"
                },
                {
                    "authorId": "98885420",
                    "name": "Mingying Yu"
                },
                {
                    "authorId": "2105201",
                    "name": "Bhushan Kotnis"
                },
                {
                    "authorId": "2056385822",
                    "name": "Caroline V. Lawrence"
                },
                {
                    "authorId": "2472657",
                    "name": "Goran Glavas"
                },
                {
                    "authorId": "2780262",
                    "name": "Mathias Niepert"
                }
            ]
        }
    ]
}