{
    "authorId": "1397442049",
    "papers": [
        {
            "paperId": "7cc05a25874ab6bad78973062ccc838c8060d160",
            "title": "Overview of the CLEF-2024 CheckThat! Lab Task 2 on Subjectivity in News Articles",
            "abstract": "We present an overview of Task 2 of the seventh edition of the CheckThat! lab at the 2024 iteration of the Conference and Labs of the Evaluation Forum (CLEF). The task focuses on subjectivity detection in news articles and was o\u200cered in \uf6d5ve languages: Arabic, Bulgarian, English, German, and Italian, as well as in a multilingual setting. The datasets for each language were carefully curated and annotated, comprising over 10,000 sentences from news articles. The task challenged participants to develop systems capable of distinguishing between subjective statements (re\uf6d6ecting personal opinions or biases) and objective ones (presenting factual information) at the sentence level. A total of 15 teams participated in the task, submitting 36 valid runs across all language tracks. The participants used a variety of approaches, with transformer-based models being the most popular choice. Strategies included \uf6d5ne-tuning monolingual and multilingual models, and leveraging English models with automatic translation for the non-English datasets. Some teams also explored ensembles, feature engineering, and innovative techniques such as few-shot learning and in-context learning with large language models. The evaluation was based on macro-averaged F1 score. The results varied across languages, with the best performance achieved for Italian and German, followed by English. The Arabic track proved particularly challenging, with no team surpassing an F1 score of 0.50. This task contributes to the broader goal of enhancing the reliability of automated content analysis in the context of misinformation detection and fact-checking. The paper provides detailed insights into the datasets, participant approaches, and results, o\u200cering a benchmark for the current state of subjectivity detection across multiple languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2264714",
                    "name": "Julia Maria Stru\u00df"
                },
                {
                    "authorId": "2241527223",
                    "name": "Federico Ruggeri"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "2291964834",
                    "name": "Dimitar Dimitrov"
                },
                {
                    "authorId": "2316056652",
                    "name": "Andrea Galassi"
                },
                {
                    "authorId": "2239200277",
                    "name": "Georgi Pachov"
                },
                {
                    "authorId": "52553663",
                    "name": "Ivan Koychev"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2261403474",
                    "name": "Melanie Siegel"
                },
                {
                    "authorId": "2261403527",
                    "name": "Michael Wiegand"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "3378399",
                    "name": "Reem Suwaileh"
                },
                {
                    "authorId": "2034351",
                    "name": "W. Zaghouani"
                }
            ]
        },
        {
            "paperId": "b07e43ce063bb745c2156dac6223029d95161010",
            "title": "UniBO at CheckThat! 2024: Multi-lingual and Multi-label Persuasion Technique Detection in News with Data Augmentation and Sequence-Token Classifiers",
            "abstract": "With the widespread use of the Internet and the rise of algorithmic journalism, consumers of news are exposed more than ever before to manipulative, propagandistic, and deceptive content. As a result, major public events and debates on relevant topics can be significantly influenced. This creates an increasing demand for automated tools that help experts analyze the news ecosystem. We explored persuasion technique detection in multi-lingual news as part of the CheckThat! Lab Task 3. Our pipeline comprises two parts. The first part is a data augmentation module, which uses a BERT-based model fine-tuned for word-alignment to project labels from source texts to machine-translated target texts. The second one is a persuasion technique classification module, leveraging two fine-tuned BERT-based models: a sequence classifier for detecting sentences containing persuasion techniques and a set of 23 token-level classifiers for specific techniques. Our approach, trained on augmented multilingual data with class weighting and a high decision threshold of 0.9, is competitive in all language settings, showing hints of cross-lingual transfer. Despite the research efforts in this direction, exemplified by shared tasks, detecting persuasion techniques, especially across languages, remains challenging due to their implicit and subtle nature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265758070",
                    "name": "Paolo Gajo"
                },
                {
                    "authorId": "2316057370",
                    "name": "Luca Giordano"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                }
            ]
        },
        {
            "paperId": "fab90c57448d2619136f3479fac094b2353e487d",
            "title": "Overview of the CLEF-2024 CheckThat! Lab Task 1 on Check-Worthiness Estimation of Multigenre Content",
            "abstract": "We present an overview of the CheckThat! Lab 2024 Task 1, part of CLEF 2024. Task 1 involves determining whether a text item is check-worthy, with a special emphasis on COVID-19, political news, and political debates and speeches. It is conducted in three languages: Arabic, Dutch, and English. Additionally, Spanish was offered for extra training data during the development phase. A total of 75 teams registered, with 37 teams submitting 236 runs and 17 teams submitting system description papers. Out of these, 13, 15 and 26 teams participated for Arabic, Dutch and English, respectively. Among these teams, the use of transformer pre-trained language models (PLMs) was the most frequent. A few teams also employed Large Language Models (LLMs). We provide a description of the dataset, the task setup, including evaluation settings, and a brief overview of the participating systems. As is customary in the CheckThat! Lab, we release all the datasets as well as the evaluation scripts to the research community. This will enable further research on identifying relevant check-worthy content that can assist various stakeholders, such as fact-checkers, journalists, and policymakers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "3378399",
                    "name": "Reem Suwaileh"
                },
                {
                    "authorId": "2221317616",
                    "name": "Sanne Weering"
                },
                {
                    "authorId": "2241675070",
                    "name": "Chengkai Li"
                },
                {
                    "authorId": "2261403233",
                    "name": "Tommaso Caselli"
                },
                {
                    "authorId": "2034351",
                    "name": "W. Zaghouani"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "1b50303ddbce01279a72443cece0b0be625cc17c",
            "title": "On the Definition of Prescriptive Annotation Guidelines for Language-Agnostic Subjectivity Detection",
            "abstract": "Defining subjectivity indicators without relying on domain-specific assumptions or incurring interpretation biases is a well-known challenge. To account for these limitations, recent work is shifting toward annotation procedures for subjectivity detection that are not limited to language-specific cues. Nonetheless, developing a rigorous methodology to address edge cases and annotators\u2019 bias, while maintaining desired properties like language agnosticism, is yet an open problem. In this work, we rely on the prescriptive annotation paradigm and propose a methodology based on three key aspects. We present a case study on subjectivity detection for fact-checking in English and Italian news to evaluate the efficacy of the proposed methodology and discuss the open challenges",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39921439",
                    "name": "Federico Ruggeri"
                },
                {
                    "authorId": "2127115553",
                    "name": "Francesco Antici"
                },
                {
                    "authorId": "143978279",
                    "name": "Andrea Galassi"
                },
                {
                    "authorId": "2093365921",
                    "name": "Katerina Korre"
                },
                {
                    "authorId": "2038102772",
                    "name": "Arianna Muti"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                }
            ]
        },
        {
            "paperId": "29745c2f62e65af4cc3f2516ce2ae010cae317d2",
            "title": "Overview of the CLEF-2023 CheckThat! Lab: Task 2 on Subjectivity Detection",
            "abstract": "We describe the outcome of the 2023 edition of the CheckThat! Lab at CLEF. We focus on subjectivity (Task 2), which has been proposed for the first time. It aims at fostering the technology for the identification of subjective text fragments in news articles. For that, we produced corpora consisting of 9,530 manually-annotated sentences, covering six languages \u2014Arabic, Dutch, English, German, Italian, and Turkish. Task 2 attracted 12 teams, which submitted a total of 40 final runs covering all languages. The most successful approaches addressed the task using state-of-the-art multilingual transformer models, which were fine-tuned on language-specific data. Teams also experimented with a rich set of other neural architectures, including foundation models, zero-shot classifiers, and standard transformers, mainly coupled with data augmentation and multilingual training strategies to address class imbalance. We publicly release all the datasets and evaluation scripts, with the purpose of promoting further research on this topic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143978279",
                    "name": "Andrea Galassi"
                },
                {
                    "authorId": "2241527223",
                    "name": "Federico Ruggeri"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "2261403233",
                    "name": "Tommaso Caselli"
                },
                {
                    "authorId": "38330039",
                    "name": "Mucahid Kutlu"
                },
                {
                    "authorId": "2264714",
                    "name": "Julia Maria Stru\u00df"
                },
                {
                    "authorId": "2127115553",
                    "name": "Francesco Antici"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2181067879",
                    "name": "Juliane K\u00f6hler"
                },
                {
                    "authorId": "2093365921",
                    "name": "Katerina Korre"
                },
                {
                    "authorId": "2121382337",
                    "name": "Folkert Leistra"
                },
                {
                    "authorId": "2038102772",
                    "name": "Arianna Muti"
                },
                {
                    "authorId": "2261403474",
                    "name": "Melanie Siegel"
                },
                {
                    "authorId": "2235689778",
                    "name": "Mehmet Deniz T\u00fcrkmen"
                },
                {
                    "authorId": "2261403527",
                    "name": "Michael Wiegand"
                },
                {
                    "authorId": "2034351",
                    "name": "W. Zaghouani"
                }
            ]
        },
        {
            "paperId": "3eb0ecb3ad7432447af87e072b0ae812bdc52a80",
            "title": "Overview of the CLEF-2023 CheckThat! Lab Task 1 on Check-Worthiness in Multimodal and Multigenre Content",
            "abstract": "We present an overview of CheckThat! Lab\u2019s 2023 Task 1, which is part of CLEF-2023. Task 1 asks to determine whether a text item, or a text coupled with an image, is check-worthy. This task places a special emphasis on COVID-19, political debates and transcriptions, and it is conducted in three languages: Arabic, English, and Spanish. A total of 15 teams participated, and most submissions managed to achieve significant improvements over the baselines using Transformer-based models. Out of these, seven teams participated in the multimodal subtask (1A), and 12 teams participated in the Multigenre subtask (1B), collectively submitting 155 official runs for both subtasks. Across both subtasks, approaches that targeted multiple languages, either individually or in conjunction, generally achieved the best performance. We provide a description of the dataset and the task setup, including the evaluation settings, and we briefly overview the participating systems. As is customary in the CheckThat! lab, we have release all datasets from the lab as well as the evaluation scripts to the research community. This will enable further research on finding relevant check-worthy content that can assist various stakeholders such as fact-checkers, journalists, and policymakers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "39866663",
                    "name": "Gullal S. Cheema"
                },
                {
                    "authorId": "3376145",
                    "name": "Gautam Kishore Shahi"
                },
                {
                    "authorId": "2079305",
                    "name": "Sherzod Hakimov"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2241675070",
                    "name": "Chengkai Li"
                },
                {
                    "authorId": "31329752",
                    "name": "Rub\u00e9n M\u00edguez"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2034351",
                    "name": "W. Zaghouani"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "c44bb8391861ba10935165d02462805d6f16761c",
            "title": "UniBoe\u2019s at SemEval-2023 Task 10: Model-Agnostic Strategies for the Improvement of Hate-Tuned and Generative Models in the Classification of Sexist Posts",
            "abstract": "We present our submission to SemEval-2023 Task 10: Explainable Detection of Online Sexism (EDOS). We address all three tasks: Task A consists of identifying whether a post is sexist. If so, Task B attempts to assign it one of four categories: threats, derogation, animosity, and prejudiced discussions. Task C aims for an even more fine-grained classification, divided among 11 classes. Our team UniBoe\u2019s experiments with fine-tuning of hate-tuned Transformer-based models and priming for generative models. In addition, we explore model-agnostic strategies, such as data augmentation techniques combined with active learning, as well as obfuscation of identity terms. Our official submissions obtain an F1_score of 0.83 for Task A, 0.58 for Task B and 0.32 for Task C.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2038102772",
                    "name": "Arianna Muti"
                },
                {
                    "authorId": "2186553188",
                    "name": "Francesco Fernicola"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                }
            ]
        },
        {
            "paperId": "2a75cf81f1cafbcdc99d15467bdd4b8fca9a6902",
            "title": "Overview of the CLEF-2022 CheckThat! Lab Task 1 on Identifying Relevant Claims in Tweets",
            "abstract": "We present an overview of CheckThat! lab 2022 Task 1, part of the 2022 Conference and Labs of the Evaluation Forum (CLEF). Task 1 asked to predict which posts in a Twitter stream are worth fact-checking, focusing on COVID-19 and politics in six languages: Arabic, Bulgarian, Dutch, English, Spanish, and Turkish. A total of 19 teams participated and most submissions managed to achieve sizable improvements over the baselines using Transformer-based models such as BERT and GPT-3. Across the four subtasks, approaches that targetted multiple languages (be it individually or in conjunction, in general obtained the best performance. We describe the dataset and the task setup, including the evaluation settings, and we give a brief overview of the participating systems. As usual in the CheckThat! lab, we release to the research community all datasets from the lab as well as the evaluation scripts, which should enable further research on finding relevant tweets that can help different stakeholders such as fact-checkers, journalists, and policymakers. \u00a9 2022 Copyright for this paper by its authors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "34086979",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "31329752",
                    "name": "Rub\u00e9n M\u00edguez"
                },
                {
                    "authorId": "1864635",
                    "name": "Tommaso Caselli"
                },
                {
                    "authorId": "38330039",
                    "name": "Mucahid Kutlu"
                },
                {
                    "authorId": "2034351",
                    "name": "W. Zaghouani"
                },
                {
                    "authorId": "2128664093",
                    "name": "Chengkai Li"
                },
                {
                    "authorId": "65877664",
                    "name": "Shaden Shaar"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "47195288",
                    "name": "Alex Nikolov"
                },
                {
                    "authorId": "1413965894",
                    "name": "Yavuz Selim Kartal"
                }
            ]
        },
        {
            "paperId": "2b768ad30497c7f1545f8542ec5eb64d1cd73eb8",
            "title": "Report on the 13th Conference and Labs of the Evaluation Forum (CLEF 2022)",
            "abstract": "This is a report on the thirteenth edition of the Conference and Labs of the Evaluation Forum (CLEF 2022), held on September 5--8, 2022, in Bologna, Italy. CLEF was a four-day hybrid event combining a conference and an evaluation forum. The conference featured keynotes by Benno Stein and Rita Cucchiara, and presentation of peer-reviewed research papers covering a wide range of topics, in addition to many posters. The evaluation forum consisted of fourteen labs: ARQMath, BioASQ, CheckThat!, ChEMU, eRisk, HIPE, iDPP, ImageCLEF, JokeR, LeQua, LifeCLEF, PAN, SimpleText, and Touch\u00e9, addressing a wide range of tasks, media, languages, and ways to go beyond standard test collections. Date: 5--8 September, 2022. Website: https://clef2022.clef-initiative.eu/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                },
                {
                    "authorId": "134000266",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "33831241",
                    "name": "M. Esposti"
                },
                {
                    "authorId": "80808662",
                    "name": "G. Faggioli"
                },
                {
                    "authorId": "2046170685",
                    "name": "N. Ferro"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "145645971",
                    "name": "G. Pasi"
                },
                {
                    "authorId": "3046200",
                    "name": "Martin Potthast"
                },
                {
                    "authorId": "145077269",
                    "name": "F. Sebastiani"
                }
            ]
        },
        {
            "paperId": "53ec78653f92b4a679486dda207dea43cabe8611",
            "title": "The (Undesired) Attenuation of Human Biases by Multilinguality",
            "abstract": "Some human preferences are universal. The odor of vanilla is perceived as pleasant all around the world. We expect neural models trained on human texts to exhibit these kind of preferences, i.e. biases, but we show that this is not always the case. We explore 16 static and contextual embedding models in 9 languages and, when possible, compare them under similar training conditions. We introduce and release CA-WEAT, multilingual cultural aware tests to quantify biases, and compare them to previous English-centric tests. Our experiments confirm that monolingual static embeddings do exhibit human biases, but values differ across languages, being far from universal. Biases are less evident in contextual models, to the point that the original human association might be reversed. Multilinguality proves to be another variable that attenuates and even reverses the effect of the bias, specially in contextual multilingual models. In order to explain this variance among models and languages, we examine the effect of asymmetries in the training corpus, departures from isomorphism in multilingual embedding spaces and discrepancies in the testing measures between languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1402921001",
                    "name": "C. Espa\u00f1a-Bonet"
                },
                {
                    "authorId": "1397442049",
                    "name": "Alberto Barr\u00f3n-Cede\u00f1o"
                }
            ]
        }
    ]
}