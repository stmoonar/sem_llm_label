{
    "authorId": "2146059429",
    "papers": [
        {
            "paperId": "575d626c47d1b6b74246d7ce168a64f6bcafbe4c",
            "title": "Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations",
            "abstract": "The rapid expansion of Location-Based Social Networks (LBSNs) has highlighted the importance of effective next Point-of-Interest (POI) recommendations, which leverage historical check-in data to predict users' next POIs to visit. Traditional centralized deep neural networks (DNNs) offer impressive POI recommendation performance but face challenges due to privacy concerns and limited timeliness. In response, on-device POI recommendations have been introduced, utilizing federated learning (FL) and decentralized approaches to ensure privacy and recommendation timeliness. However, these methods often suffer from computational strain on devices and struggle to adapt to new users and regions. This paper introduces a novel collaborative learning framework, Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations (DCPR), leveraging the diffusion model known for its success across various domains. DCPR operates with a cloud-edge-device architecture to offer region-specific and highly personalized POI recommendations while reducing on-device computational burdens. DCPR minimizes on-device computational demands through a unique blend of global and local learning processes. Our evaluation with two real-world datasets demonstrates DCPR's superior performance in recommendation accuracy, efficiency, and adaptability to new users and regions, marking a significant step forward in on-device POI recommendation technology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161605203",
                    "name": "Jing Long"
                },
                {
                    "authorId": "1849611",
                    "name": "Guanhua Ye"
                },
                {
                    "authorId": "2280284086",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "8347844388486bfaff10eaa3294bb490e29e622d",
            "title": "Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting",
            "abstract": "Denoising diffusion probabilistic models (DDPMs) for image inpainting aim to add the noise to the texture of the image during the forward process and recover the masked regions with the unmasked ones of the texture via the reverse denoising process. Despite the meaningful semantics gen-eration, the existing arts suffer from the semantic discrep-ancy between the masked and unmasked regions, since the semantically dense unmasked texture fails to be completely degraded while the masked regions turn to the pure noise in diffusion process, leading to the large discrepancy between them. In this paper, we aim to answer how the unmasked se-mantics guide the texture denoising process; together with how to tackle the semantic discrepancy, to facilitate the con-sistent and meaningful semantics generation. To this end, we propose a novel structure-guided diffusion model for image inpainting named StrDiffusion, to reformulate the conventional texture denoising process under the structure guidance to derive a simplified denoising objective for im-age inpainting, while revealing: 1) the semantically sparse structure is beneficial to tackle the semantic discrepancy in the early stage, while the dense texture generates the rea-sonable semantics in the late stage; 2) the semantics from the unmasked regions essentially offer the time-dependent structure guidance for the texture denoising process, ben-efiting from the time-dependent sparsity of the structure semantics. For the denoising process, a structure-guided neural network is trained to estimate the simplified denoising objective by exploiting the consistency of the denoised structure between masked and unmasked regions. Besides, we devise an adaptive resampling strategy as aformal criterion as whether the structure is competent to guide the texture denoising process, while regulate their semantic corre-lations. Extensive experiments validate the merits of StrDif-fusion over the state-of-the-arts. Our code is available at https://github.com/htyjers/StrDiffusion.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155522807",
                    "name": "Haipeng Liu"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2064306184",
                    "name": "Biao Qian"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2292379158",
                    "name": "Yong Rui"
                }
            ]
        },
        {
            "paperId": "9f77870cbec30806e448292ce10ba1343b91e507",
            "title": "Unpacking the Gap Box Against Data-Free Knowledge Distillation",
            "abstract": "Data-free knowledge distillation (DFKD) improves the student model (S) by mimicking the class probability from a pre-trained teacher model (T) without training data. Under such setting, an ideal scenario is that T can help generate \u201dgood\u201d samples from a generator (G) to maximally benefit S. However, existing arts suffer from the non-ideal generated samples under the disturbance of the gap (i.e., either too large or small) between the class probabilities of T and S; for example, the generated samples with too large gap may exhibit <italic>excessive</italic> information for S, while too small gap leads to the <italic>limited</italic> knowledge in the samples, resulting into the poor generalization. Meanwhile, they fail to judge the \u201cgoodness\u201d of the generated samples for S since the <italic>fixed</italic> T is not necessarily ideal. In this paper, we aim to answer <italic>what is inside the gap box</italic>; together with <italic>how to yield \u201dgood\u201d generated samples for DFKD?</italic> To this end, we propose a <italic>Gap</italic>-<italic>S</italic>ensitive <italic>S</italic>ample <italic>G</italic>eneration (GapSSG) approach, by revisiting the empirical distilled risk from a data-free perspective, which confirms the existence of an ideal teacher (T<inline-formula><tex-math notation=\"LaTeX\">$^*$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"wang-ieq1-3379505.gif\"/></alternatives></inline-formula>), while theoretically implying: (1) the gap disturbance originates from the <italic>mismatch</italic> between T and T<inline-formula><tex-math notation=\"LaTeX\">$^*$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"wang-ieq2-3379505.gif\"/></alternatives></inline-formula>, hence the class probabilities of T enable the approximation to those of T<inline-formula><tex-math notation=\"LaTeX\">$^*$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"wang-ieq3-3379505.gif\"/></alternatives></inline-formula>; and (2) \u201dgood\u201d samples should maximally benefit S via T's class probabilities, owing to unknown T<inline-formula><tex-math notation=\"LaTeX\">$^*$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"wang-ieq4-3379505.gif\"/></alternatives></inline-formula>. To this end, we unpack the gap box between T and S as two findings: <italic>inherent</italic> gap to perceive T and T<inline-formula><tex-math notation=\"LaTeX\">$^*$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"wang-ieq5-3379505.gif\"/></alternatives></inline-formula>; <italic>derived</italic> gap to monitor S and T<inline-formula><tex-math notation=\"LaTeX\">$^*$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"wang-ieq6-3379505.gif\"/></alternatives></inline-formula>. Benefiting from the <italic>derived</italic> gap that focuses on the adaptability of generated sample to S, we attempt to track student's training route (a series of training epochs) to capture the category distribution of S; upon which, a regulatory factor is further devised to approximate T<inline-formula><tex-math notation=\"LaTeX\">$^*$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>*</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"wang-ieq7-3379505.gif\"/></alternatives></inline-formula> over <italic>inherent</italic> gap, so as to generate \u201dgood\u201d samples to S. Furthermore, during the distillation process, a sample-balanced strategy comes up to tackle the overfitting and missing knowledge issues between the generated partial and critical samples by training G. The theoretical and empirical studies verify the advantages of GapSSG over the state-of-the-arts.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155651697",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2064306184",
                    "name": "Biao Qian"
                },
                {
                    "authorId": "2155522807",
                    "name": "Haipeng Liu"
                },
                {
                    "authorId": "2292379158",
                    "name": "Yong Rui"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                }
            ]
        },
        {
            "paperId": "a5adf2881910398948f3c7cdbcf73ef2dfa4e1e7",
            "title": "Lightweight Embeddings for Graph Collaborative Filtering",
            "abstract": "Graph neural networks (GNNs) are currently one of the most performant collaborative filtering methods. Meanwhile, owing to the use of an embedding table to represent each user/item as a distinct vector, GNN-based recommenders have inherited the long-standing defect of parameter inefficiency. As a common practice for scalable embeddings, parameter sharing enables the use of fewer embedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most existing methods are a heuristically designed, predefined mapping from each user's/item's ID to the corresponding meta-embedding indexes, thus simplifying the optimization problem into learning only the meta-embeddings. However, in the context of GNN-based collaborative filtering, such a fixed mapping omits the semantic correlations between entities that are evident in the user-item interaction graph, leading to suboptimal recommendation performance. To this end, we propose Lightweight Embeddings for Graph Collaborative Filtering (LEGCF), a parameter-efficient embedding framework dedicated to GNN-based recommenders. LEGCF innovatively introduces an assignment matrix as an extra learnable component on top of meta-embeddings. To jointly optimize these two heavily entangled components, aside from learning the meta-embeddings by minimizing the recommendation loss, LEGCF further performs efficient assignment update by enforcing a novel semantic similarity constraint and finding its closed-form solution based on matrix pseudo-inverse. The meta-embeddings and assignment matrix are alternately updated, where the latter is sparsified on the fly to ensure negligible storage overhead. Extensive experiments on three benchmark datasets have verified LEGCF's smallest trade-off between size and performance, with consistent accuracy gain over state-of-the-art baselines. The codebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238394188",
                    "name": "Xurong Liang"
                },
                {
                    "authorId": "1490931831",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2281001972",
                    "name": "Lizhen Cui"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2267513105",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "1523851527c5ee1dc499f015dbfebcf000fca9cb",
            "title": "Macroscopic-and-Microscopic Rain Streaks Disentanglement Network for Single-Image Deraining",
            "abstract": "Single-image deraining aims to restore the image that is degraded by the rain streaks, where the long-standing bottleneck lies in how to disentangle the rain streaks from the given rainy image. Despite the progress made by substantial existing works, several crucial questions \u2014 e.g., How to distinguish rain streaks and clean image, while how to disentangle rain streaks from low-frequency pixels, and further prevent the blurry edges \u2014 have not been well investigated. In this paper, we attempt to solve all of them under one roof. Our observation is that rain streaks are bright stripes with higher pixel values that are evenly distributed in each color channel of the rainy image, while the disentanglement of the high-frequency rain streaks is equivalent to decreasing the standard deviation of the pixel distribution for the rainy image. To this end, we propose a self-supervised rain streaks learning network to characterize the similar pixel distribution of the rain streaks from a macroscopic viewpoint over various low-frequency pixels of gray-scale rainy images, coupling with a supervised rain streaks learning network to explore the specific pixel distribution of the rain streaks from a microscopic viewpoint between each paired rainy and clean images. Building on this, a self-attentive adversarial restoration network comes up to prevent the further blurry edges. These networks compose an end-to-end <underline>M</underline>acroscopic-and-<underline>M</underline>icroscopic <underline>R</underline>ain <underline>S</underline>treaks <underline>D</underline>isentanglement <underline>N</underline>etwork, named <inline-formula> <tex-math notation=\"LaTeX\">$\\text{M}^{2}$ </tex-math></inline-formula>RSD-Net, to learn rain streaks, which is further removed for single image deraining. The experimental results validate its advantages on deraining benchmarks against the state-of-the-arts. The code is available at: <uri>https://github.com/xinjiangaohfut/MMRSD-Net</uri>",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1994707",
                    "name": "Xinjian Gao"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                }
            ]
        },
        {
            "paperId": "3feda432725f4066754325e889316658313eb050",
            "title": "Fine-grained Cross-modal Fusion based Refinement for Text-to-Image Synthesis",
            "abstract": "Text-to-image synthesis refers to generating visual-realistic and semantically consistent images from given textual descriptions. Previous approaches generate an initial low-resolution image and then refine it to be high-resolution. Despite the remarkable progress, these methods are limited in fully utilizing the given texts and could generate text-mismatched images, especially when the text description is complex. We propose a novel Fine-grained text-image Fusion based Generative Adversarial Networks, dubbed FF-GAN, which consists of two modules: Fine-grained text-image Fusion Block (FF-Block) and Global Semantic Refinement (GSR). The proposed FF-Block integrates an attention block and several convolution layers to effectively fuse the fine-grained word-context features into the corresponding visual features, in which the text information is fully used to refine the initial image with more details. And the GSR is proposed to improve the global semantic consistency between linguistic and visual features during the refinement process. Extensive experiments on CUB-200 and COCO datasets demonstrate the superiority of FF-GAN over other state-of-the-art approaches in generating images with semantic consistency to the given texts.Code is available at https://github.com/haoranhfut/FF-GAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Haoran Sun"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2155522807",
                    "name": "Haipeng Liu"
                },
                {
                    "authorId": "2064306184",
                    "name": "Biao Qian"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                }
            ]
        },
        {
            "paperId": "4e1e3631593f7334c5a3534b3de2e53680d5fd1a",
            "title": "Graph-based Text Classification by Contrastive Learning with Text-level Graph Augmentation",
            "abstract": "Text Classification (TC) is a fundamental task in the information retrieval community. Nowadays, the mainstay TC methods are built on the deep neural networks, which can learn much more discriminative text features than the traditional shallow learning methods. Among existing deep TC methods, the ones based on Graph Neural Network (GNN) have attracted more attention due to the superior performance. Technically, the GNN-based TC methods mainly transform the full training dataset to a graph of texts; however, they often neglect the dependency between words, so as to miss potential semantic information of texts, which may be significant to exactly represent them. To solve the aforementioned problem, we generate graphs of words instead, so as to capture the dependency information of words. Specifically, each text is translated into a graph of words, where neighboring words are linked. We learn the node features of words by a GNN-like procedure and then aggregate them as the graph feature to represent the current text. To further improve the text representations, we suggest a contrastive learning regularization term. Specifically, we generate two augmented text graphs for each original text graph, we constrain the representations of the two augmented graphs from the same text close and the ones from different texts far away. We propose various techniques to generate the augmented graphs. Upon those ideas, we develop a novel deep TC model, namely Text-level Graph Networks with Contrastive Learning (TGNcl). We conduct a number of experiments to evaluate the proposed TGNcl model. The empirical results demonstrate that TGNcl can outperform the existing state-of-the-art TC models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275287623",
                    "name": "Ximing Li"
                },
                {
                    "authorId": "2155840248",
                    "name": "Bing Wang"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                }
            ]
        },
        {
            "paperId": "afe7d5e4a309dea2ac2a8e53c3db61b135b85092",
            "title": "Rethinking Data-Free Quantization as a Zero-Sum Game",
            "abstract": "Data-free quantization (DFQ) recovers the performance of quantized network (Q) without accessing the real data, but generates the fake sample via a generator (G) by learning from full-precision network (P) instead. However, such sample generation process is totally independence of Q, specialized as failing to consider the adaptability of the generated samples, i.e., beneficial or adversarial, over the learning process of Q, resulting into non-ignorable performance loss. Building on this, several crucial questions --- how to measure and exploit the sample adaptability to Q under varied bit-width scenarios? how to generate the samples with desirable adaptability to benefit the quantized network? --- impel us to revisit DFQ. In this paper, we answer the above questions from a game-theory perspective to specialize DFQ as a zero-sum game between two players --- a generator and a quantized network, and further propose an Adaptability-aware Sample Generation (AdaSG) method. Technically, AdaSG reformulates DFQ as a dynamic maximization-vs-minimization game process anchored on the sample adaptability. The maximization process aims to generate the sample with desirable adaptability, such sample adaptability is further reduced by the minimization process after calibrating Q for performance recovery. The Balance Gap is defined to guide the stationarity of the game process to maximally benefit Q. The theoretical analysis and empirical studies verify the superiority of AdaSG over the state-of-the-arts. Our code is available at https://github.com/hfutqian/AdaSG.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064306184",
                    "name": "Biao Qian"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "48043335",
                    "name": "Richang Hong"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                }
            ]
        },
        {
            "paperId": "f9fc45bebc6568fbd40d9c0e016b8cf5a0ccd229",
            "title": "Adaptive Data-Free Quantization",
            "abstract": "Data-free quantization (DFQ) recovers the performance of quantized network (Q) without the original data, but generates the fake sample via a generator (G) by learning from full-precision network (P), which, however, is totally independent of Q, overlooking the adaptability of the knowledge from generated samples, i.e., informative or not to the learning process of Q, resulting into the overflow of generalization error. Building on this, several critical questions \u2014 how to measure the sample adaptability to Q under varied bit-width scenarios? whether the largest adaptability is the best? how to generate the samples with adaptive adaptability to improve Q's generalization? To answer the above questions, in this paper, we propose an Adaptive Data-Free Quantization (AdaDFQ) method, which revisits DFQ from a zero-sum game perspective upon the sample adaptability between two players \u2014 a generator and a quantized network. Following this viewpoint, we further define the disagreement and agreement samples to form two boundaries, where the margin between two boundaries is optimized to adaptively regulate the adaptability of generated samples to Q, so as to address the over-and-under fitting issues. Our AdaDFQ reveals: 1) the largest adaptability is NOT the best for sample generation to benefit Q's generalization; 2) the knowledge of the generated sample should not be informative to Q only, but also related to the category and distribution information of the training data for P. The theoretical and empirical analysis validate the advantages of AdaDFQ over the state-of-the-arts. Our code is available at https://github.com/hfutqian/AdaDFQ.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064306184",
                    "name": "Biao Qian"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "48043335",
                    "name": "Richang Hong"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                }
            ]
        },
        {
            "paperId": "7f962a0f7ef3beccd879ef6d4d732481ec887adb",
            "title": "Thinking inside The Box: Learning Hypercube Representations for Group Recommendation",
            "abstract": "As a step beyond traditional personalized recommendation, group recommendation is the task of suggesting items that can satisfy a group of users. In group recommendation, the core is to design preference aggregation functions to obtain a quality summary of all group members' preferences. Such user and group preferences are commonly represented as points in the vector space (i.e., embeddings), where multiple user embeddings are compressed into one to facilitate ranking for group-item pairs. However, the resulted group representations, as points, lack adequate flexibility and capacity to account for the multi-faceted user preferences. Also, the point embedding-based preference aggregation is a less faithful reflection of a group's decision-making process, where all users have to agree on a certain value in each embedding dimension instead of a negotiable interval. In this paper, we propose a novel representation of groups via the notion of hypercubes, which are subspaces containing innumerable points in the vector space. Specifically, we design the hypercube recommender (CubeRec) to adaptively learn group hypercubes from user embeddings with minimal information loss during preference aggregation, and to leverage a revamped distance metric to measure the affinity between group hypercubes and item points. Moreover, to counteract the long-standing issue of data sparsity in group recommendation, we make full use of the geometric expressiveness of hypercubes and innovatively incorporate self-supervision by intersecting two groups. Experiments on four real-world datasets have validated the superiority of CubeRec over state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1490931831",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "2161605203",
                    "name": "Jing Long"
                },
                {
                    "authorId": "144133815",
                    "name": "Q. Nguyen"
                },
                {
                    "authorId": "2155651371",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2146059429",
                    "name": "Meng Wang"
                }
            ]
        }
    ]
}