{
    "authorId": "1745117",
    "papers": [
        {
            "paperId": "283e8d16fb65d83383c1644185443407795d8d2f",
            "title": "Automatically Constructing Geospatial Feature Taxonomies from OpenStreetMap Data",
            "abstract": "This paper presents a method for constructing a lightweight taxonomy of geospatial features using OpenStreetMap (OSM) data. Leveraging the OSM data model, our process mines frequent tags to efficiently produce a structured hierarchy, enriching the semantic representation of geo-features. This data-driven taxonomy supports various geospatial analysis applications. Accompanying the methodology, we release the source code of our tool and demonstrate its practical application with tailored taxonomies for California (US) and Greece, underscoring our approach\u2019s adaptability and scalability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720805839",
                    "name": "Basel Shbita"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                }
            ]
        },
        {
            "paperId": "032e2a112d7b36456399ad3b2ef62a032e302056",
            "title": "Indirect Cooperation in Distributed Stationary-Resource Searching with Predefined Destinations",
            "abstract": "Private vehicles are a direct means to bring people from one place to their desired destinations. However, no omniscient dispatcher is handling the origin-destination of vehicles and the availability of stationary resources, such as parking spaces or charging stations. Competitive cruising for stationary resources leads to environmental pollution and is a waste of drivers' time. We focus on the problem of distributed stationary-resource searching with predefined destinations under a multi-agent scenario. It is a distributed route planning problem with global optimization objectives. We present a probabilistic approach to achieving indirect resource coordination and latent agent cooperation in a distributed manner. Our approach treats the estimated availability of stationary resources as a reference and guides each agent based on their preferences. We evaluate our approach on four real-world datasets. Our approach outperforms state-of-the-art methods by 5% in multi-criteria optimization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5961871",
                    "name": "Fandel Lin"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                }
            ]
        },
        {
            "paperId": "03c89b373681994d479c315fb9b9c1425e4c6a85",
            "title": "Detecting Semantic Errors in Tables using Textual Evidence",
            "abstract": "Tables can contain various types of errors, including both syntactic and semantic errors. Semantic errors relate to the meaning of the data and can be detrimental for downstream applications. The existing approaches for semantic error detection use structured knowledge sources such as Wikidata and DBpedia, but the coverage of such sources is quite limited. There is much more information available in free text to validate the contents of tables. In this paper, we present a novel semantic-error-detection approach that exploits open-domain textual data to verify the semantic correctness of tables. Our approach leverages contrastive learning, table linearization, and pre-trained language models to implement the error detection process. We implement our approach in a system called SEED and show in the evaluation that it significantly outperforms the other competing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280436529",
                    "name": "Minh Pham"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                },
                {
                    "authorId": "2280378308",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "94a758a4f435280d32c9851f3ac83d1e51dcb0de",
            "title": "Exploiting Polygon Metadata to Understand Raster Maps - Accurate Polygonal Feature Extraction",
            "abstract": "Locating undiscovered deposits of critical minerals requires accurate geological data. However, most of the 100,000 historical geological maps of the United States Geological Survey (USGS) are in raster format. This hinders critical mineral assessment. We target the problem of extracting geological features represented as polygons from raster maps. We exploit the polygon metadata that provides information on the geological features, such as the map keys indicating how the polygon features are represented, to extract the features. We present a metadata-driven machine-learning approach that encodes the raster map and map key into a series of bitmaps and uses a convolutional model to learn to recognize the polygon features. We evaluated our approach on USGS geological maps; our approach achieves a median F1 score of 0.809 and outperforms state-of-the-art methods by 4.52%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5961871",
                    "name": "Fandel Lin"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                },
                {
                    "authorId": "1720805839",
                    "name": "Basel Shbita"
                },
                {
                    "authorId": "2275821878",
                    "name": "Binh Vu"
                },
                {
                    "authorId": "2261450824",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2266801014",
                    "name": "Yao-Yi Chiang"
                }
            ]
        },
        {
            "paperId": "42f6681a18a4d042609e7b8698cd4fa74903592f",
            "title": "Building spatio-temporal knowledge graphs from vectorized topographic historical maps",
            "abstract": "Historical maps provide rich information for researchers in many areas, including the social and natural sciences. These maps contain detailed documentation of a wide variety of natural and human-made features and their changes over time, such as changes in transportation networks or the decline of wetlands or forest areas. Analyzing changes over time in such maps can be labor-intensive for a scientist, even after the geographic features have been digitized and converted to a vector format. Knowledge Graphs (KGs) are the appropriate representations to store and link such data and support semantic and temporal querying to facilitate change analysis. KGs combine expressivity, interoperability, and standardization in the Semantic Web stack, thus providing a strong foundation for querying and analysis. In this paper, we present an automatic approach to convert vector geographic features extracted from multiple historical maps into contextualized spatio-temporal KGs. The resulting graphs can be easily queried and visualized to understand the changes in different regions over time. We evaluate our technique on railroad networks and wetland areas extracted from the United States Geological Survey (USGS) historical topographic maps for several regions over multiple map sheets and editions. We also demonstrate how the automatically constructed linked data (i.e., KGs) enable effective querying and visualization of changes over different points in time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720805839",
                    "name": "Basel Shbita"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                },
                {
                    "authorId": "1410108610",
                    "name": "Weiwei Duan"
                },
                {
                    "authorId": "40532403",
                    "name": "Yao-Yi Chiang"
                },
                {
                    "authorId": "47352285",
                    "name": "Johannes H. Uhl"
                },
                {
                    "authorId": "2051975",
                    "name": "S. Leyk"
                }
            ]
        },
        {
            "paperId": "2e28cced9fa7ac1ee94f6a3b22a016e7e4e6fcd0",
            "title": "A Label Correction Algorithm Using Prior Information for Automatic and Accurate Geospatial Object Recognition",
            "abstract": "Thousands of scanned historical topographic maps contain valuable information covering long periods of time, such as how the hydrography of a region has changed over time. Efficiently unlocking the information in these maps requires training a geospatial objects recognition system, which needs a large amount of annotated data. Overlapping geo-referenced external vector data with topographic maps according to their coordinates can annotate the desired objects\u2019 locations in the maps automatically. However, directly overlapping the two datasets causes misaligned and false annotations because the publication years and coordinate projection systems of topographic maps are different from the external vector data. We propose a label correction algorithm, which leverages the color information of maps and the prior shape information of the external vector data to reduce misaligned and false annotations. The experiments show that the precision of annotations from the proposed algorithm is 10% higher than the annotations from a state-of-the-art algorithm. Consequently, recognition results using the proposed algorithm\u2019s annotations achieve 9% higher correctness than using the annotations from the state-of-the-art algorithm.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1410108610",
                    "name": "Weiwei Duan"
                },
                {
                    "authorId": "40532403",
                    "name": "Yao-Yi Chiang"
                },
                {
                    "authorId": "2051975",
                    "name": "S. Leyk"
                },
                {
                    "authorId": "47352285",
                    "name": "Johannes H. Uhl"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                }
            ]
        },
        {
            "paperId": "63a4bca007437909b64d1791e399c9ef19eb63e6",
            "title": "Synthetic Map Generation to Provide Unlimited Training Data for Historical Map Text Detection",
            "abstract": "Many historical map sheets are publicly available for studies that require long-term historical geographic data. The cartographic design of these maps includes a combination of map symbols and text labels. Automatically reading text labels from map images could greatly speed up the map interpretation and helps generate rich metadata describing the map content. Many text detection algorithms have been proposed to locate text regions in map images automatically, but most of the algorithms are trained on out-of-domain datasets (e.g., scenic images). Training data determines the quality of machine learning models, and manually annotating text regions in map images is labor-extensive and time-consuming. On the other hand, existing geographic data sources, such as Open-StreetMap (OSM), contain machine-readable map layers, which allow us to separate out the text layer and obtain text label annotations easily. However, the cartographic styles between OSM map tiles and historical maps are significantly different. This paper proposes a method to automatically generate an unlimited amount of annotated historical map images for training text detection models. We use a style transfer model to convert contemporary map images into historical style and place text labels upon them. We show that the state-of-the-art text detection models (e.g., PSENet) can benefit from the synthetic historical maps and achieve significant improvement for historical map text detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145250935",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2138130333",
                    "name": "Runyu Guan"
                },
                {
                    "authorId": "2153795471",
                    "name": "Qianmu Yu"
                },
                {
                    "authorId": "40532403",
                    "name": "Yao-Yi Chiang"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                }
            ]
        },
        {
            "paperId": "c23b9223dd4a8a1e23e7686f8c9d9816958874b0",
            "title": "Building Survivable Software Systems by Automatically Adapting to Sensor Changes",
            "abstract": "Many software systems run on long-lifespan platforms that operate in diverse and dynamic environments. If these software systems could automatically adapt to hardware changes, it would significantly reduce the maintenance cost and enable rapid upgrade. In this paper, we study the problem of how to automatically adapt to sensor changes, as an important step towards building such long-lived, survivable software systems. We address challenges in sensor adaptation when a set of sensors are replaced by new sensors. Our approach reconstructs sensor values of replaced sensors by preserving distributions of sensor values before and after the sensor change, thereby not warranting a change in higher-layer software. Compared to existing work, our approach has the following advantages: (a) ability to exploit new sensors without requiring an overlapping period of time between the new sensors and the old ones; (b) ability to provide an estimation of adaptation quality; and (c) ability to scale to a large number of sensors. Experiments on weather data and Unmanned Undersea Vehicle (UUV) data demonstrate that our approach can automatically adapt to sensor changes with 5.7% higher accuracy compared to baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118758172",
                    "name": "Yuan Shi"
                },
                {
                    "authorId": "2108667795",
                    "name": "Ang Li"
                },
                {
                    "authorId": "2066755908",
                    "name": "T. Kumar"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                }
            ]
        },
        {
            "paperId": "c2ef2277a72933d2cdf5a1410182cabbd28af3e0",
            "title": "Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making",
            "abstract": "Major societal and environmental challenges involve complex systems that have diverse multi-scale interacting processes. Consider, for example, how droughts and water reserves affect crop production and how agriculture and industrial needs affect water quality and availability. Preventive measures, such as delaying planting dates and adopting new agricultural practices in response to changing weather patterns, can reduce the damage caused by natural processes. Understanding how these natural and human processes affect one another allows forecasting the effects of undesirable situations and study interventions to take preventive measures. For many of these processes, there are expert models that incorporate state-of-the-art theories and knowledge to quantify a system's response to a diversity of conditions. A major challenge for efficient modeling is the diversity of modeling approaches across disciplines and the wide variety of data sources available only in formats that require complex conversions. Using expert models for particular problems requires integration of models with third-party data as well as integration of models across disciplines. Modelers face significant heterogeneity that requires resolving semantic, spatiotemporal, and execution mismatches, which are largely done by hand today and may take more than 2 years of effort. We are developing a modeling framework that uses artificial intelligence (AI) techniques to reduce modeling effort while ensuring utility for decision making. Our work to date makes several innovative contributions: (1) an intelligent user interface that guides analysts to frame their modeling problem and assists them by suggesting relevant choices and automating steps along the way; (2) semantic metadata for models, including their modeling variables and constraints, that ensures model relevance and proper use for a given decision-making problem; and (3) semantic representations of datasets in terms of modeling variables that enable automated data selection and data transformations. This framework is implemented in the MINT (Model INTegration) framework, and currently includes data and models to analyze the interactions between natural and human systems involving climate, water availability, agricultural production, and markets. Our work to date demonstrates the utility of AI techniques to accelerate modeling to support decision-making and uncovers several challenging directions for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145526918",
                    "name": "Y. Gil"
                },
                {
                    "authorId": "1398926410",
                    "name": "D. Garijo"
                },
                {
                    "authorId": "26901841",
                    "name": "D. Khider"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                },
                {
                    "authorId": "1712372",
                    "name": "V. Ratnakar"
                },
                {
                    "authorId": "34258886",
                    "name": "Maximiliano Osorio"
                },
                {
                    "authorId": "2059350960",
                    "name": "H. Vargas"
                },
                {
                    "authorId": "144843028",
                    "name": "Minh Pham"
                },
                {
                    "authorId": "2634786",
                    "name": "J. Pujara"
                },
                {
                    "authorId": "1720805839",
                    "name": "Basel Shbita"
                },
                {
                    "authorId": "144023413",
                    "name": "Binh Vu"
                },
                {
                    "authorId": "40532403",
                    "name": "Yao-Yi Chiang"
                },
                {
                    "authorId": "97577305",
                    "name": "D. Feldman"
                },
                {
                    "authorId": "46394784",
                    "name": "Yijun Lin"
                },
                {
                    "authorId": "2158090180",
                    "name": "Haoting Song"
                },
                {
                    "authorId": "2107978833",
                    "name": "Vipin Kumar"
                },
                {
                    "authorId": "2533995",
                    "name": "A. Khandelwal"
                },
                {
                    "authorId": "1707756",
                    "name": "M. Steinbach"
                },
                {
                    "authorId": "7885684",
                    "name": "Kshitij Tayal"
                },
                {
                    "authorId": "4632515",
                    "name": "Shaoming Xu"
                },
                {
                    "authorId": "35691547",
                    "name": "S. Pierce"
                },
                {
                    "authorId": "122989628",
                    "name": "L. Pearson"
                },
                {
                    "authorId": "1409945973",
                    "name": "Daniel Hardesty-Lewis"
                },
                {
                    "authorId": "1687896",
                    "name": "E. Deelman"
                },
                {
                    "authorId": "2956722",
                    "name": "R. Mayani"
                },
                {
                    "authorId": "34608794",
                    "name": "Yuning Shi"
                },
                {
                    "authorId": "153095581",
                    "name": "Lorne Leonard"
                },
                {
                    "authorId": "144520374",
                    "name": "K. Cobourn"
                },
                {
                    "authorId": "2118690204",
                    "name": "Zeya Zhang"
                },
                {
                    "authorId": "144235920",
                    "name": "C. Duffy"
                }
            ]
        },
        {
            "paperId": "d22d5e54626f81688eefb96eb0216a657f451321",
            "title": "Combining Remote-Sensing-Derived Data and Historical Maps for Long-Term Back-Casting of Urban Extents",
            "abstract": "Spatially explicit, fine-grained datasets describing historical urban extents are rarely available prior to the era of operational remote sensing. However, such data are necessary to better understand long-term urbanization and land development processes and for the assessment of coupled nature\u2013human systems (e.g., the dynamics of the wildland\u2013urban interface). Herein, we propose a framework that jointly uses remote-sensing-derived human settlement data (i.e., the Global Human Settlement Layer, GHSL) and scanned, georeferenced historical maps to automatically generate historical urban extents for the early 20th century. By applying unsupervised color space segmentation to the historical maps, spatially constrained to the urban extents derived from the GHSL, our approach generates historical settlement extents for seamless integration with the multitemporal GHSL. We apply our method to study areas in countries across four continents, and evaluate our approach against historical building density estimates from the Historical Settlement Data Compilation for the US (HISDAC-US), and against urban area estimates from the History Database of the Global Environment (HYDE). Our results achieve Area-under-the-Curve values > 0.9 when comparing to HISDAC-US and are largely in agreement with model-based urban areas from the HYDE database, demonstrating that the integration of remote-sensing-derived observations and historical cartographic data sources opens up new, promising avenues for assessing urbanization and long-term land cover change in countries where historical maps are available.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47352285",
                    "name": "Johannes H. Uhl"
                },
                {
                    "authorId": "2051975",
                    "name": "S. Leyk"
                },
                {
                    "authorId": "2145250935",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "1410108610",
                    "name": "Weiwei Duan"
                },
                {
                    "authorId": "1720805839",
                    "name": "Basel Shbita"
                },
                {
                    "authorId": "40532403",
                    "name": "Yao-Yi Chiang"
                },
                {
                    "authorId": "1745117",
                    "name": "Craig A. Knoblock"
                }
            ]
        }
    ]
}