{
    "authorId": "1751914",
    "papers": [
        {
            "paperId": "417960fc4cd0d6f672a33778bb9a4b95dc4f1554",
            "title": "Analyzing the Support Level for Tips Extracted from Product Reviews",
            "abstract": "Useful tips extracted from product reviews assist customers to take a more informed purchase decision, as well as making a better, easier, and safer usage of the product. In this work we argue that extracted tips should be examined based on the amount of support and opposition they receive from all product reviews. A classifier, developed for this purpose, determines the degree to which a tip is supported or contradicted by a single review sentence. These support-levels are then aggregated over all review sentences, providing a global support score, and a global contradiction score, reflecting the support-level of all reviews to the given tip, thus improving the customer confidence in the tip validity. By analyzing a large set of tips extracted from product reviews, we propose a novel taxonomy for categorizing tips as highly-supported, highly-contradicted, controversial (supported and contradicted), and anecdotal (neither supported nor contradicted).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056901323",
                    "name": "Miriam Farber"
                },
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                },
                {
                    "authorId": "1617901165",
                    "name": "Lital Kuchy"
                },
                {
                    "authorId": "1723339",
                    "name": "Avihai Mejer"
                }
            ]
        },
        {
            "paperId": "656b8ffc95d7febbfacfc28fe346d9f4ddfe88ac",
            "title": "Alexa, Let's Work Together: Introducing the First Alexa Prize TaskBot Challenge on Conversational Task Assistance",
            "abstract": "Since its inception in 2016, the Alexa Prize program has enabled hundreds of university students to explore and compete to develop conversational agents through the SocialBot Grand Challenge. The goal of the challenge is to build agents capable of conversing coherently and engagingly with humans on popular topics for 20 minutes, while achieving an average rating of at least 4.0/5.0. However, as conversational agents attempt to assist users with increasingly complex tasks, new conversational AI techniques and evaluation platforms are needed. The Alexa Prize TaskBot challenge, established in 2021, builds on the success of the SocialBot challenge by introducing the requirements of interactively assisting humans with real-world Cooking and Do-It-Yourself tasks, while making use of both voice and visual modalities. This challenge requires the TaskBots to identify and understand the user's need, identify and integrate task and domain knowledge into the interaction, and develop new ways of engaging the user without distracting them from the task at hand, among other challenges. This paper provides an overview of the TaskBot challenge, describes the infrastructure support provided to the teams with the CoBot Toolkit, and summarizes the approaches the participating teams took to overcome the research challenges. Finally, it analyzes the performance of the competing TaskBots during the first year of the competition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1411423941",
                    "name": "Anna Gottardi"
                },
                {
                    "authorId": "2167638287",
                    "name": "Osman Ipek"
                },
                {
                    "authorId": "3285152",
                    "name": "Giuseppe Castellucci"
                },
                {
                    "authorId": "2122825525",
                    "name": "Shui Hu"
                },
                {
                    "authorId": "2167643644",
                    "name": "Lavina Vaz"
                },
                {
                    "authorId": "2167771392",
                    "name": "Yao Lu"
                },
                {
                    "authorId": "2077475879",
                    "name": "Anju Khatri"
                },
                {
                    "authorId": "2077475884",
                    "name": "Anjali Chadha"
                },
                {
                    "authorId": "2187068732",
                    "name": "Desheng Zhang"
                },
                {
                    "authorId": "2167648845",
                    "name": "Sattvik Sahai"
                },
                {
                    "authorId": "72881692",
                    "name": "Prerna Dwivedi"
                },
                {
                    "authorId": "2167861070",
                    "name": "Hangjie Shi"
                },
                {
                    "authorId": "2144596247",
                    "name": "Lu Hu"
                },
                {
                    "authorId": "2167641762",
                    "name": "Andy Huang"
                },
                {
                    "authorId": "2115177932",
                    "name": "Luke Dai"
                },
                {
                    "authorId": "2119660672",
                    "name": "Bo Yang"
                },
                {
                    "authorId": "46569381",
                    "name": "Varun Somani"
                },
                {
                    "authorId": "29153729",
                    "name": "Pankaj Rajan"
                },
                {
                    "authorId": "2167648862",
                    "name": "Ron Rezac"
                },
                {
                    "authorId": "2078507351",
                    "name": "Michael Johnston"
                },
                {
                    "authorId": "2167638459",
                    "name": "Savanna Stiff"
                },
                {
                    "authorId": "2167616711",
                    "name": "Leslie Ball"
                },
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3046332",
                    "name": "Oleg Rokhlenko"
                },
                {
                    "authorId": "35834027",
                    "name": "Kate Bland"
                },
                {
                    "authorId": "1685296",
                    "name": "Eugene Agichtein"
                },
                {
                    "authorId": "3306272",
                    "name": "R. Ghanadan"
                },
                {
                    "authorId": "1781257",
                    "name": "Y. Maarek"
                }
            ]
        },
        {
            "paperId": "c076190daa73d76c8861d6c613407e30ad6ab7bd",
            "title": "IR Evaluation and Learning in the Presence of Forbidden Documents",
            "abstract": "Many IR collections contain forbidden documents (F-docs), i.e. documents that should not be retrieved to the searcher. In an ideal scenario F-docs are clearly flagged, hence the ranker can filter them out, guaranteeing that no F-doc will be exposed. However, in real-world scenarios, filtering algorithms are prone to errors. Therefore, an IR evaluation system should also measure filtering quality in addition to ranking quality. Typically, filtering is considered as a classification task and is evaluated independently of the ranking quality. However, due to the mutual affinity between the two, it is desirable to evaluate ranking quality while filtering decisions are being made. In this work we propose nDCGf, a novel extension of the nDCGmin metric[14], which measures both ranking and filtering quality of the search results. We show both theoretically and empirically that while nDCGmin is not suitable for the simultaneous ranking and filtering task, nDCGf is a reliable metric in this case. We experiment with three datasets for which ranking and filtering are both required. In the PR dataset our task is to rank product reviews while filtering those marked as spam. Similarly, in the CQA dataset our task is to rank a list of human answers per question while filtering bad answers. We also experiment with the TREC web-track datasets, where F-docs are explicitly labeled, sorting participant runs according to their ranking and filtering quality, demonstrating the stability, sensitivity, and reliability of nDCGf for this task. We propose a learning to rank and filter (LTRF) framework that is specifically designed to optimize nDCGf, by learning a ranking model and optimizing a filtering threshold used for discarding documents with lower scores. We experiment with several loss functions demonstrating their success in learning an effective LTRF model for the simultaneous learning and filtering task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                },
                {
                    "authorId": "2898845",
                    "name": "Nachshon Cohen"
                },
                {
                    "authorId": "34766724",
                    "name": "A. Ingber"
                },
                {
                    "authorId": "1681183",
                    "name": "Elad Kravi"
                }
            ]
        },
        {
            "paperId": "44afb29a41a8f65c73898d537b9939ac69c42abf",
            "title": "\"Did you buy it already?\", Detecting Users Purchase-State From Their Product-Related Questions",
            "abstract": "In this study we address the problem of identifying the purchase-state of users, based on product-related questions they ask on an eCommerce website. We differentiate between questions asked before buying a product (pre-purchase) and after (post-purchase). At first, we study the ambiguity that exists in purchase-states' definition, and then investigate the linguistic characteristics of the questions in each state. We analyze the discrepancy between the language models of pre- and post-purchase questions, and offer two classification schemes for this task, both outperform human judgments. We additionally show the effectiveness of our classification models in improving real world applications for both consumers and sellers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1617901165",
                    "name": "Lital Kuchy"
                },
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                },
                {
                    "authorId": "2118919858",
                    "name": "Thomas Huet"
                },
                {
                    "authorId": "1681183",
                    "name": "Elad Kravi"
                }
            ]
        },
        {
            "paperId": "631ec25bc744cfaccea99f394640271c50d79401",
            "title": "WSDM'21",
            "abstract": "The ACM International Conference on Web Search and Data Mining (WSDM) is one of the premier conferences on web-related research involving web search and data mining, with a dynamic and growing community from academia and industry. This year, WSDM was held virtually on March 8th -- 12th, 2021, due to the Covid-19 pandemic, instead of the originally-planned location in Jerusalem, Israel. WSDM'21 program reflects the breadth and diversity of research in the field and showcases the latest developments in these domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                },
                {
                    "authorId": "1402943721",
                    "name": "L. Lewin-Eytan"
                },
                {
                    "authorId": "1388775854",
                    "name": "E. Yom-Tov"
                },
                {
                    "authorId": "1685296",
                    "name": "Eugene Agichtein"
                },
                {
                    "authorId": "1718798",
                    "name": "E. Gabrilovich"
                }
            ]
        },
        {
            "paperId": "f63fd9b84c7e99884742ae7decce8e9cb0f40afb",
            "title": "Answering Product-Questions by Utilizing Questions from Other Contextually Similar Products",
            "abstract": "Predicting the answer to a product-related question is an emerging field of research that recently attracted a lot of attention. Answering subjective and opinion-based questions is most challenging due to the dependency on customer generated content. Previous works mostly focused on review-aware answer prediction; however, these approaches fail for new or unpopular products, having no (or only a few) reviews at hand. In this work, we propose a novel and complementary approach for predicting the answer for such questions, based on the answers for similar questions asked on similar products. We measure the contextual similarity between products based on the answers they provide for the same question. A mixture-of-expert framework is used to predict the answer by aggregating the answers from contextually similar products. Empirical results demonstrate that our model outperforms strong baselines on some segments of questions, namely those that have roughly ten or more similar resolved questions in the corpus. We additionally publish two large-scale datasets used in this work, one is of similar product question pairs, and the second is of product question-answer pairs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1379931325",
                    "name": "Ohad Rozen"
                },
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                },
                {
                    "authorId": "1723339",
                    "name": "Avihai Mejer"
                },
                {
                    "authorId": "2218832",
                    "name": "Vitaly Mirkis"
                },
                {
                    "authorId": "7264689",
                    "name": "Yftah Ziser"
                }
            ]
        },
        {
            "paperId": "5099c0cdf29d97774a0d34287915c51c46f6f9a1",
            "title": "Scalable top-k retrieval with Sparta",
            "abstract": "Many big data processing applications rely on a top-k retrieval building block, which selects (or approximates) the k highest-scoring data items based on an aggregation of features. In web search, for instance, a document's score is the sum of its scores for all query terms. Top-k retrieval is often used to sift through massive data and identify a smaller subset of it for further analysis. Because it filters out the bulk of the data, it often constitutes the main performance bottleneck. Beyond the rise in data sizes, today's data processing scenarios also increase the number of features contributing to the overall score. In web search, for example, verbose queries are becoming mainstream, while state-of-the-art algorithms fail to process long queries in real-time. We present Sparta, a practical parallel algorithm that exploits multi-core hardware for fast (approximate) top-k retrieval. Thanks to lightweight coordination and judicious context sharing among threads, Sparta scales both in the number of features and in the searched index size. In our web search case study on 50M documents, Sparta processes 12-term queries more than twice as fast as the state-of-the-art. On a tenfold bigger index, Sparta processes queries at the same speed, whereas the average latency of existing algorithms soars to be an order-of-magnitude larger than Sparta's.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51117498",
                    "name": "Gali Sheffi"
                },
                {
                    "authorId": "2470974",
                    "name": "Dmitry Basin"
                },
                {
                    "authorId": "1685240",
                    "name": "Edward Bortnikov"
                },
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                },
                {
                    "authorId": "1777373",
                    "name": "I. Keidar"
                }
            ]
        },
        {
            "paperId": "552afce0f1400e3790dca7161126f9c8e53428d1",
            "title": "Humor Detection in Product Question Answering Systems",
            "abstract": "Community question-answering (CQA) has been established as a prominent web service enabling users to post questions and get answers from the community. Product Question Answering (PQA) is a special CQA framework where questions are asked (and are answered) in the context of a specific product. Naturally, humorous questions are integral part of such platforms, especially as some products attract humor due to their unreasonable price, their peculiar functionality, or in cases that users emphasize their critical point-of-view through humor. Detecting humorous questions in such systems is important for sellers, to better understand user engagement with their products. It is also important to signal users about flippancy of humorous questions, and that answers for such questions should be taken with a grain of salt. In this study we present a deep-learning framework for detecting humorous questions in PQA systems. Our framework utilizes two properties of the questions - Incongruity and Subjectivity, demonstrating their contribution for humor detection. We evaluate our framework over a real-world dataset, demonstrating an accuracy of 90.8%, up to 18.3% relative improvement over baseline methods. We then demonstrate the existence of product bias in PQA platforms, when some products attract more humorous questions than others. A classifier trained over unbiased data is outperformed by the biased classifier, however, it excels in the task of differentiating between humorous and non-humorous questions that are both related to the same product. To the best of our knowledge this work is the first to detect humor in PQA setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7264689",
                    "name": "Yftah Ziser"
                },
                {
                    "authorId": "1681183",
                    "name": "Elad Kravi"
                },
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                }
            ]
        },
        {
            "paperId": "570927ab717b3271045e3a935bd228b6bd5deb19",
            "title": "Voice-based Reformulation of Community Answers",
            "abstract": "Community Question Answering (CQA) websites, such as Stack Exchange1 or Quora2, allow users to freely ask questions and obtain answers from other users, i.e., the community. Personal assistants, such as Amazon Alexa or Google Home, can also exploit CQA data to answer a broader range of questions and increase customers\u2019 engagement. However, the voice-based interaction poses new challenges to the Question Answering scenario. Even assuming that we are able to retrieve a previously asked question that perfectly matches the user\u2019s query, we cannot simply read its answer to the user. A major limitation is the answer length. Reading these answers to the user is cumbersome and boring. Furthermore, many answers contain non-voice-friendly parts, such as images, or URLs. In this paper, we define the Answer Reformulation task and propose a novel solution to automatically reformulate a community provided answer making it suitable for a voice interaction. Results on a manually annotated dataset3 extracted from Stack Exchange show that our models improve strong baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2778426",
                    "name": "Simone Filice"
                },
                {
                    "authorId": "2898845",
                    "name": "Nachshon Cohen"
                },
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                }
            ]
        },
        {
            "paperId": "9350eac23da04bb4ebfd4f0afd14c0d14668b0dc",
            "title": "Multi-Objective Ranking Optimization for Product Search Using Stochastic Label Aggregation",
            "abstract": "Learning a ranking model in product search involves satisfying many requirements such as maximizing the relevance of retrieved products with respect to the user query, as well as maximizing the purchase likelihood of these products. Multi-Objective Ranking Optimization (MORO) is the task of learning a ranking model from training examples while optimizing multiple objectives simultaneously. Label aggregation is a popular solution approach for multi-objective optimization, which reduces the problem into a single objective optimization problem, by aggregating the multiple labels of the training examples, related to the different objectives, to a single label. In this work we explore several label aggregation methods for MORO in product search. We propose a novel stochastic label aggregation method which randomly selects a label per training example according to a given distribution over the labels. We provide a theoretical proof showing that stochastic label aggregation is superior to alternative aggregation approaches, in the sense that any optimal solution of the MORO problem can be generated by a proper parameter setting of the stochastic aggregation process. We experiment on three different datasets: two from the voice product search domain, and one publicly available dataset from the Web product search domain. We demonstrate empirically over these three datasets that MORO with stochastic label aggregation provides a family of ranking models that fully dominates the set of MORO models built using deterministic label aggregation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1751914",
                    "name": "David Carmel"
                },
                {
                    "authorId": "2723826",
                    "name": "Elad Haramaty"
                },
                {
                    "authorId": "2079007233",
                    "name": "Arnon Lazerson"
                },
                {
                    "authorId": "1402943721",
                    "name": "L. Lewin-Eytan"
                }
            ]
        }
    ]
}