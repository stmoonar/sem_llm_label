{
    "authorId": "1812965",
    "papers": [
        {
            "paperId": "0130f2fab75c30c26635be32fdafa6a03c35af6a",
            "title": "GraphStorm: all-in-one graph machine learning framework for industry applications",
            "abstract": "Graph machine learning (GML) is effective in many business applications. However, making GML easy to use and applicable to industry applications with massive datasets remain challenging. We developed GraphStorm, which provides an end-to-end solution for scalable graph construction, graph model training and inference. GraphStorm has the following desirable properties: (a) Easy to use: it can perform graph construction and model training and inference with just a single command; (b) Expert-friendly: GraphStorm contains many advanced GML modeling techniques to handle complex graph data and improve model performance; (c) Scalable: every component in GraphStorm can operate on graphs with billions of nodes and can scale model training and inference to different hardware without changing any code. GraphStorm has been used and deployed for over a dozen billion-scale industry applications after its release in May 2023. It is open-sourced in Github: https://github.com/awslabs/graphstorm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283934850",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2284037254",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2299062897",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "2284069643",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "2146354747",
                    "name": "Runjie Ma"
                },
                {
                    "authorId": "2280741034",
                    "name": "Houyu Zhang"
                },
                {
                    "authorId": "2255392614",
                    "name": "Zichen Wang"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "10429687",
                    "name": "Israt Nisa"
                },
                {
                    "authorId": "3125115",
                    "name": "Alejandro Mottini"
                },
                {
                    "authorId": "2305618086",
                    "name": "Qingjun Cui"
                },
                {
                    "authorId": "145344187",
                    "name": "H. Rangwala"
                },
                {
                    "authorId": "2305616323",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "db46dd642e9c03ccc2186c6dd65611f79f220062",
            "title": "GraphStorm an Easy-to-use and Scalable Graph Neural Network Framework: From Beginners to Heroes",
            "abstract": "Applying Graph Neural Networks (GNNs) to real-world problems is challenging for machine learning (ML) practitioners due to two major obstacles. The first hurdle is the high barrier to learn programming GNNs from scratch. The second challenge lies in overcoming engineering difficulties when scaling GNN models for large graphs at an industry-level. To address these challenges, GraphStorm, an open-source framework, offers a solution by providing an easy-to-use user interface and an end-to-end GNN training/inference pipeline that seamlessly handles extremely large graphs in a distributed manner This tutorial aims to provide participants with a comprehensive understanding of GraphStorm, including its design principles, target users, and use cases, through presentations. The hands-on sections will enable attendees to walk through four practical GraphStorm use cases that can assist them in leveraging GNNs to address real-world business problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151810148",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "10429687",
                    "name": "Israt Nisa"
                },
                {
                    "authorId": "2227670581",
                    "name": "Jim Lu"
                }
            ]
        },
        {
            "paperId": "d3a7664b314f6fc9d0374c74592523afb70077bd",
            "title": "BRR: Preserving Privacy of Text Data Efficiently on Device",
            "abstract": "With the use of personal devices connected to the Internet for tasks such as searches and shopping becoming ubiquitous, ensuring the privacy of the users of such services has become a requirement in order to build and maintain customer trust. While text privatization methods exist, they require the existence of a trusted party that collects user data before applying a privatization method to preserve users' privacy. In this work we propose an efficient mechanism to provide metric differential privacy for text data on-device. With our solution, sensitive data never leaves the device and service providers only have access to privatized data to train models on and analyze. We compare our algorithm to the state-of-the-art for text privatization, showing similar or better utility for the same privacy guarantees, while reducing the storage costs by orders of magnitude, enabling on-device text privatization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145262240",
                    "name": "Ricardo Silva Carvalho"
                },
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "2067883",
                    "name": "Oluwaseyi Feyisetan"
                }
            ]
        },
        {
            "paperId": "e7b1e34113ccf40eebee5dac5357246eda585152",
            "title": "TEM: High Utility Metric Differential Privacy on Text",
            "abstract": "Ensuring the privacy of users whose data are used to train Natural Language Processing (NLP) models is necessary to build and maintain customer trust. Differential Privacy (DP) has emerged as the most successful method to protect the privacy of individuals. However, applying DP to the NLP domain comes with unique challenges. The most successful previous methods use a generalization of DP for metric spaces, and apply the privatization by adding noise to inputs in the metric space of word embeddings. However, these methods assume that one specific distance measure is being used, ignore the density of the space around the input, and assume the embeddings used have been trained on non-sensitive data. In this work we propose Truncated Exponential Mechanism (TEM), a general method that allows the privatization of words using any distance metric, on embeddings that can be trained on sensitive data. Our method makes use of the exponential mechanism to turn the privatization step into a \\emph{selection problem}. This allows the noise applied to be calibrated to the density of the embedding space around the input, and makes domain adaptation possible for the embeddings. In our experiments, we demonstrate that our method significantly outperforms the state-of-the-art in terms of utility for the same level of privacy, while providing more flexibility in the metric selection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145262240",
                    "name": "Ricardo Silva Carvalho"
                },
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "2067883",
                    "name": "Oluwaseyi Feyisetan"
                }
            ]
        },
        {
            "paperId": "153f7a8fc8b0b111f6b1859b613d21cd33e3fadc",
            "title": "Block-distributed Gradient Boosted Trees",
            "abstract": "The Gradient Boosted Tree (GBT) algorithm is one of the most popular machine learning algorithms used in production, for tasks that include Click-Through Rate (CTR) prediction and learning-to-rank. To deal with the massive datasets available today, many distributed GBT methods have been proposed. However, they all assume a row-distributed dataset, addressing scalability only with respect to the number of data points and not the number of features, and increasing communication cost for high-dimensional data. In order to allow for scalability across both the data point and feature dimensions, and reduce communication cost, we propose block-distributed GBTs. We achieve communication efficiency by making full use of the data sparsity and adapting the Quickscorer algorithm to the block-distributed setting. We evaluate our approach using datasets with millions of features, and demonstrate that we are able to achieve multiple orders of magnitude reduction in communication cost for sparse data, with no loss in accuracy, while providing a more scalable design. As a result, we are able to reduce the training time for high-dimensional data, and allow more cost-effective scale-out without the need for expensive network communication.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "2288374",
                    "name": "Hyunsu Cho"
                },
                {
                    "authorId": "1777235",
                    "name": "Henrik Bostr\u00f6m"
                }
            ]
        },
        {
            "paperId": "86af68e2010e1ce42bd0b0c2f326a869696f646d",
            "title": "Scalable Machine Learning through Approximation and Distributed Computing",
            "abstract": "Machine learning algorithms are now being deployed in practically all areas of our lives. Part of this success can be attributed to the ability to learn complex representations from massive dataset ...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                }
            ]
        },
        {
            "paperId": "9477d0fdff319ec97222e25c3c5eb41056d4aa40",
            "title": "Quantifying Uncertainty in Online Regression Forests",
            "abstract": "Accurately quantifying uncertainty in predictions is essential for the deployment of machine learning algorithms in critical applications where mistakes are costly. Most approaches to quantifying p ...",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                },
                {
                    "authorId": "1777235",
                    "name": "Henrik Bostr\u00f6m"
                }
            ]
        },
        {
            "paperId": "60c2bb59e0f765fa4ea64a43f2a861a632df033d",
            "title": "BoostVHT: Boosting Distributed Streaming Decision Trees",
            "abstract": "Online boosting improves the accuracy of classifiers for unbounded streams of data by chaining them into an ensemble. Due to its sequential nature, boosting has proven hard to parallelize, even more so in the online setting. This paper introduces BoostVHT, a technique to parallelize online boosting algorithms. Our proposal leverages a recently-developed model-parallel learning algorithm for streaming decision trees as a base learner. This design allows to neatly separate the model boosting from its training. As a result, BoostVHT provides a flexible learning framework which can employ any existing online boosting algorithm, while at the same time it can leverage the computing power of modern parallel and distributed cluster environments. We implement our technique on Apache SAMOA, an open-source platform for mining big data streams that can be run on several distributed execution engines, and demonstrate order of magnitude speedups compared to the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "28067242",
                    "name": "F. Beligianni"
                },
                {
                    "authorId": "35168570",
                    "name": "G. D. F. Morales"
                }
            ]
        },
        {
            "paperId": "a68cb0d4b8fc2b71327feab589c93e321383e047",
            "title": "Predicting Session Length in Media Streaming",
            "abstract": "Session length is a very important aspect in determining a user's satisfaction with a media streaming service. Being able to predict how long a session will last can be of great use for various downstream tasks, such as recommendations and ad scheduling. Most of the related literature on user interaction duration has focused on dwell time for websites, usually in the context of approximating post-click satisfaction either in search results, or display ads. In this work we present the first analysis of session length in a mobile-focused online service, using a real world data-set from a major music streaming service. We use survival analysis techniques to show that the characteristics of the length distributions can differ significantly between users, and use gradient boosted trees with appropriate objectives to predict the length of a session using only information available at its beginning. Our evaluation on real world data illustrates that our proposed technique outperforms the considered baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "2507979",
                    "name": "H. Vahabi"
                },
                {
                    "authorId": "40630797",
                    "name": "Ross Kravitz"
                },
                {
                    "authorId": "21500225",
                    "name": "V. Rashkov"
                }
            ]
        },
        {
            "paperId": "d34443f6cb4ab3086ff5915aa142e4447a97c29d",
            "title": "Knowing an Object by the Company it Keeps: A Domain-Agnostic Scheme for Similarity Discovery",
            "abstract": "Appropriately defining and then efficiently calculating similarities from large data sets are often essential in data mining, both for building tractable representations and for gaining understanding of data and generating processes. Here we rely on the premise that given a set of objects and their correlations, each object is characterized by its context, i.e. its correlations to the other objects, and that the similarity between two objects therefore can be expressed in terms of the similarity between their respective contexts. Resting on this principle, we propose a data-driven and highly scalable approach for discovering similarities from large data sets by representing objects and their relations as a correlation graph that is transformed to a similarity graph. Together these graphs can express rich structural properties among objects. Specifically, we show that concepts -- representations of abstract ideas and notions -- are constituted by groups of similar objects that can be identified by clustering the objects in the similarity graph. These principles and methods are applicable in a wide range of domains, and will here be demonstrated for three distinct types of objects: codons, artists and words, where the numbers of objects and correlations range from small to very large.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1810096",
                    "name": "Olof G\u00f6rnerup"
                },
                {
                    "authorId": "2338283",
                    "name": "D. Gillblad"
                },
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                }
            ]
        }
    ]
}