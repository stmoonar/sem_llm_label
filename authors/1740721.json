{
    "authorId": "1740721",
    "papers": [
        {
            "paperId": "04cab71e7ff997d32332a633d17abbaa54589764",
            "title": "AudioTagging Done Right: 2nd comparison of deep learning methods for environmental sound classification",
            "abstract": "After its sweeping success in vision and language tasks, pure attention-based neural architectures (e.g. DeiT) [1] are emerg-ing to the top of audio tagging (AT) leaderboards [2], which seemingly obsoletes traditional convolutional neural networks (CNNs), feed-forward networks or recurrent networks. How-ever, taking a closer look, there is great variability in pub-lished research, for instance, performances of models initialized with pretrained weights differ drastically from without pretraining [2], training time for a model varies from hours to weeks, and often, essences are hidden in seemingly trivial details. This urgently calls for a comprehensive study since our 1st comparison [3] is half-decade old. In this work, we perform extensive experiments on AudioSet [4] which is the largest weakly-labeled sound event dataset available, we also did analysis based on the data quality and efficiency. We compare a few state-of-the-art baselines on the AT task, and study the performance and efficiency of 2 major categories of neural architectures: CNN variants and attention-based variants. We also closely examine their optimization procedures. Our open-sourced experimental results 1 provide insights to trade off between performance, efficiency, optimization process, for both practitioners and researchers.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "3428237",
                    "name": "Juncheng Billy Li"
                },
                {
                    "authorId": "3188437",
                    "name": "Shuhui Qu"
                },
                {
                    "authorId": "2319973",
                    "name": "Po-Yao (Bernie) Huang"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                }
            ]
        },
        {
            "paperId": "0649abfb8f157dc6b6befbc4143bb04418cc6eed",
            "title": "Error-aware Quantization through Noise Tempering",
            "abstract": "Quantization has become a predominant approach for model compression, enabling deployment of large models trained on GPUs onto smaller form-factor devices for inference. Quantization-aware training (QAT) optimizes model parameters with respect to the end task while simulating quantization error, leading to better performance than post-training quantization. Approximation of gradients through the non-differentiable quantization operator is typically achieved using the straight-through estimator (STE) or additive noise. However, STE-based methods suffer from instability due to biased gradients, whereas existing noise-based methods cannot reduce the resulting variance. In this work, we incorporate exponentially decaying quantization-error-aware noise together with a learnable scale of task loss gradient to approximate the effect of a quantization operator. We show this method combines gradient scale and quantization noise in a better optimized way, providing \ufb01ner-grained estimation of gradients at each weight and activation layer\u2019s quantizer bin size. Our controlled noise also contains an implicit curvature term that could encourage \ufb02atter minima, which we show is indeed the case in our experiments. Experiments training ResNet architectures on the CIFAR-10, CIFAR-100 and ImageNet benchmarks show that our method obtains state-of-the-art top-1 classi\ufb01cation accuracy for uniform (non mixed-precision) quantization, out-performing previous methods by 0.5-1.2% absolute.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157155590",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "3428237",
                    "name": "Juncheng Billy Li"
                },
                {
                    "authorId": "3188437",
                    "name": "Shuhui Qu"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                }
            ]
        },
        {
            "paperId": "10672baf790962195677c7581a2fe984032e7f98",
            "title": "Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models",
            "abstract": "End-to-end spoken language understanding (SLU) systems are gaining popularity over cascaded approaches due to their simplicity and ability to avoid error propagation. However, these systems model sequence labeling as a sequence prediction task causing a divergence from its well-established token-level tagging formulation. We build compositional end-to-end SLU systems that explicitly separate the added complexity of recognizing spoken mentions in SLU from the NLU task of sequence labeling. By relying on intermediate decoders trained for ASR, our end-to-end systems transform the input modality from speech to token-level representations that can be used in the traditional sequence labeling framework. This composition of ASR and NLU formulations in our end-to-end SLU system offers direct compatibility with pre-trained ASR and NLU systems, allows performance monitoring of individual components and enables the use of globally normalized losses like CRF, making them attractive in practical scenarios. Our models outperform both cascaded and direct end-to-end models on a labeling task of named entity recognition across SLU benchmarks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "72401599",
                    "name": "Siddhant Arora"
                },
                {
                    "authorId": "35186886",
                    "name": "Siddharth Dalmia"
                },
                {
                    "authorId": "2087059555",
                    "name": "Brian Yan"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "1690706",
                    "name": "A. Black"
                },
                {
                    "authorId": "1746678",
                    "name": "Shinji Watanabe"
                }
            ]
        },
        {
            "paperId": "11311c3688963a993c079ca3078c1fcf36b987e5",
            "title": "Phone Inventories and Recognition for Every Language",
            "abstract": "Identifying phone inventories is a crucial component in language documentation and the preservation of endangered languages. However, even the largest collection of phone inventory only covers about 2000 languages, which is only 1/4 of the total number of languages in the world. A majority of the remaining languages are endangered. In this work, we attempt to solve this problem by estimating the phone inventory for any language listed in Glottolog, which contains phylogenetic information regarding 8000 languages. In particular, we propose one probabilistic model and one non-probabilistic model, both using phylogenetic trees (\u201clanguage family trees\u201d) to measure the distance between languages. We show that our best model outperforms baseline models by 6.5 F1. Furthermore, we demonstrate that, with the proposed inventories, the phone recognition model can be customized for every language in the set, which improved the PER (phone error rate) in phone recognition by 25%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47058260",
                    "name": "Xinjian Li"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "1690706",
                    "name": "A. Black"
                },
                {
                    "authorId": "1746678",
                    "name": "Shinji Watanabe"
                }
            ]
        },
        {
            "paperId": "3083d25a984cfaf5520aed08849d1b71b71070d8",
            "title": "Robustness of Neural Architectures for Audio Event Detection",
            "abstract": "Traditionally, in Audio Recognition pipeline, noise are sup-pressed by the \u201cfrontend\u201d, relying on preprocessing techniques such as speech enhancement. However, it is not guaranteed that noise will not cascade into downstream pipelines. To understand the actual in\ufb02uence of noise on the entire audio pipeline, in this paper, we directly investigate the impact of noise on different type of neural models without the preprocessing step. We measure the recognition performances of 4 different neural net-work models on the task of environment sound classi\ufb01cation under the 3 types of noises: occlusion (to emulate intermittent noise), Gaussian noise (models continuous noise), and adversarial perturbations (worst case scenario). Our intuition is that the different ways in which these models process their input (i.e. CNNs have strong locality inductive biases, which Transformers do not have) should lead to observable differences in performance and/ or robustness, an understanding of which will en-able further improvements. We perform extensive experiments on AudioSet [1] which is the largest weakly-labeled sound event dataset available. We also seek to explain the behaviors of different models through output distribution change and weight vi-sualization. 1",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "3428237",
                    "name": "Juncheng Billy Li"
                },
                {
                    "authorId": "3188437",
                    "name": "Shuhui Qu"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                }
            ]
        },
        {
            "paperId": "34f5bd178d863e6ca1af5d74beae003f921e594f",
            "title": "ASR2K: Speech Recognition for Around 2000 Languages without Audio",
            "abstract": "Most recent speech recognition models rely on large supervised datasets, which are unavailable for many low-resource languages. In this work, we present a speech recognition pipeline that does not require any audio for the target language. The only assumption is that we have access to raw text datasets or a set of n-gram statistics. Our speech pipeline consists of three components: acoustic, pronunciation, and language models. Unlike the standard pipeline, our acoustic and pronunciation models use multilingual models without any supervision. The language model is built using n-gram statistics or the raw text dataset. We build speech recognition for 1909 languages by combining it with Crubadan: a large endangered languages n-gram database. Furthermore, we test our approach on 129 languages across two datasets: Common Voice and CMU Wilderness dataset. We achieve 50% CER and 74% WER on the Wilderness dataset with Crubadan statistics only and improve them to 45% CER and 69% WER when using 10000 raw text utterances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47058260",
                    "name": "Xinjian Li"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "1690706",
                    "name": "A. Black"
                },
                {
                    "authorId": "1746678",
                    "name": "Shinji Watanabe"
                }
            ]
        },
        {
            "paperId": "58e3a1d38e9960a2bd59fbcc840981886306f9c6",
            "title": "LegoNN: Building Modular Encoder-Decoder Models",
            "abstract": "State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or automatic speech recognition (ASR)) are constructed and trained end-to-end as an atomic unit. No component of the model can be (re-)used without the others, making it impossible to share parts, e.g. a high resourced decoder, across tasks. We describe LegoNN, a procedure for building encoder-decoder architectures in a way so that its parts can be applied to other tasks without the need for any fine-tuning. To achieve this reusability, the interface between encoder and decoder modules is grounded to a sequence of marginal distributions over a pre-defined discrete vocabulary. We present two approaches for ingesting these marginals; one is differentiable, allowing the flow of gradients across the entire network, and the other is gradient-isolating. To enable the portability of decoder modules between MT tasks for different source languages and across other tasks like ASR, we introduce a modality agnostic encoder which consists of a length control mechanism to dynamically adapt encoders' output lengths in order to match the expected input length range of pre-trained decoders. We present several experiments to demonstrate the effectiveness of LegoNN models: a trained language generation LegoNN decoder module from German-English (De-En) MT task can be reused without any fine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT tasks, matching or beating the performance of baseline. After fine-tuning, LegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5% relative WER reduction on the Europarl ASR task. To show how the approach generalizes, we compose a LegoNN ASR model from three modules \u2013 each has been learned within different end-to-end trained models on three different datasets \u2013 achieving an overall WER reduction of 19.5%.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "35186886",
                    "name": "Siddharth Dalmia"
                },
                {
                    "authorId": "113568063",
                    "name": "Dmytro Okhonko"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "2068070",
                    "name": "Sergey Edunov"
                },
                {
                    "authorId": "1746678",
                    "name": "Shinji Watanabe"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "40360972",
                    "name": "Abdel-rahman Mohamed"
                }
            ]
        },
        {
            "paperId": "794c5d5ca20e71eae416da91cf1fed0a8ef15658",
            "title": "Masked Autoencoders that Listen",
            "abstract": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2319973",
                    "name": "Po-Yao (Bernie) Huang"
                },
                {
                    "authorId": "2109084085",
                    "name": "Hu Xu"
                },
                {
                    "authorId": "3428237",
                    "name": "Juncheng Billy Li"
                },
                {
                    "authorId": "51428394",
                    "name": "Alexei Baevski"
                },
                {
                    "authorId": "2325985",
                    "name": "Michael Auli"
                },
                {
                    "authorId": "2247475926",
                    "name": "Wojciech Galuba"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "2322150",
                    "name": "Christoph Feichtenhofer"
                }
            ]
        },
        {
            "paperId": "97ec7cf78c6f0b8a9cd9f4afdccc369ec3898f19",
            "title": "On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization",
            "abstract": "Combining the visual modality with pretrained language models has been surprisingly effective for simple descriptive tasks such as image captioning. More general text generation however remains elusive. We take a step back and ask: How do these models work for more complex generative tasks, i.e. conditioning on both text and images? Are multimodal models simply visually adapted language models, or do they combine they reason jointly over modalities? We investigate these questions in the context of self-rationalization (jointly generating task labels/answers and free-text explanations) of three tasks: (i) visual question answering in VQA-X, (ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment in e-SNLI-VE. We show that recent unimodal advances, CLIP image representations and scaling of language models, do not consistently improve self-rationalization in multimodal tasks. We find that no single model type works universally best across tasks, datasets, and finetuning data sizes. Our findings motivate the need for novel general backbones approach that move text generation from images and text beyond image captioning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "2166136235",
                    "name": "Akshita Bhagia"
                },
                {
                    "authorId": "3312309",
                    "name": "Yonatan Bisk"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "1690706",
                    "name": "A. Black"
                },
                {
                    "authorId": "3451494",
                    "name": "Ana Marasovi\u0107"
                }
            ]
        },
        {
            "paperId": "9bf72f03a0fd82990c63831cc5f2cf9056cf2ab6",
            "title": "SQuAT: Sharpness- and Quantization-Aware Training for BERT",
            "abstract": "Quantization is an effective technique to reduce memory footprint, inference latency, and power consumption of deep learning models. However, existing quantization methods suffer from accuracy degradation compared to full-precision (FP) models due to the errors introduced by coarse gradient estimation through non-differentiable quantization layers. The existence of sharp local minima in the loss landscapes of overparameterized models (e.g., Transformers) tends to aggravate such performance penalty in low-bit (2, 4 bits) settings. In this work, we propose sharpness- and quantization-aware training (SQuAT), which would encourage the model to converge to flatter minima while performing quantization-aware training. Our proposed method alternates training between sharpness objective and step-size objective, which could potentially let the model learn the most suitable parameter update magnitude to reach convergence near-flat minima. Extensive experiments show that our method can consistently outperform state-of-the-art quantized BERT models under 2, 3, and 4-bit settings on GLUE benchmarks by 1%, and can sometimes even outperform full precision (32-bit) models. Our experiments on empirical measurement of sharpness also suggest that our method would lead to flatter minima compared to other quantization methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157155590",
                    "name": "Zheng Wang"
                },
                {
                    "authorId": "3428237",
                    "name": "Juncheng Billy Li"
                },
                {
                    "authorId": "3188437",
                    "name": "Shuhui Qu"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                }
            ]
        }
    ]
}