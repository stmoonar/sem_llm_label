{
    "authorId": "2256661980",
    "papers": [
        {
            "paperId": "002a2f9da012bcf37e59c7ea0406f6bf11e5ab55",
            "title": "In-Memory Learning: A Declarative Learning Framework for Large Language Models",
            "abstract": "The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic. Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework. The agents adeptly distill insights from past experiences, refining and updating existing notes to enhance their performance in the environment. This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning. We also delve into the key features of benchmarks designed to evaluate the self-improvement process. Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290050829",
                    "name": "Bo Wang"
                },
                {
                    "authorId": "153345698",
                    "name": "Tianxiang Sun"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2182224120",
                    "name": "Siyin Wang"
                },
                {
                    "authorId": "2290056525",
                    "name": "Qingyuan Cheng"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                }
            ]
        },
        {
            "paperId": "14191e9f12913ad8c7ac6e1188682afac04aad09",
            "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
            "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2217531907",
                    "name": "Jun Zhan"
                },
                {
                    "authorId": "2087363104",
                    "name": "Junqi Dai"
                },
                {
                    "authorId": "2153258452",
                    "name": "Jiasheng Ye"
                },
                {
                    "authorId": "2118117212",
                    "name": "Yunhua Zhou"
                },
                {
                    "authorId": "2109797247",
                    "name": "Dong Zhang"
                },
                {
                    "authorId": "2284732560",
                    "name": "Zhigeng Liu"
                },
                {
                    "authorId": "2278615698",
                    "name": "Xin Zhang"
                },
                {
                    "authorId": "2032236274",
                    "name": "Ruibin Yuan"
                },
                {
                    "authorId": "2143853895",
                    "name": "Ge Zhang"
                },
                {
                    "authorId": "2107897400",
                    "name": "Linyang Li"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2276508494",
                    "name": "Jie Fu"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                },
                {
                    "authorId": "153345698",
                    "name": "Tianxiang Sun"
                },
                {
                    "authorId": "2284724292",
                    "name": "Yugang Jiang"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                }
            ]
        },
        {
            "paperId": "4b84736c87d9d927cc2b68a9fdc769f91dc56d71",
            "title": "Identifying Semantic Induction Heads to Understand In-Context Learning",
            "abstract": "Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284823035",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "3187768",
                    "name": "Qipeng Guo"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2284861897",
                    "name": "Dongrui Liu"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2258618409",
                    "name": "Dahua Lin"
                }
            ]
        },
        {
            "paperId": "4e68cc2d6cb0c60883307de05ac248f89cd51b6d",
            "title": "Efficient Training of Large Language Models on Distributed Infrastructures: A Survey",
            "abstract": "Large Language Models (LLMs) like GPT and LLaMA are revolutionizing the AI industry with their sophisticated capabilities. Training these models requires vast GPU clusters and significant computing time, posing major challenges in terms of scalability, efficiency, and reliability. This survey explores recent advancements in training systems for LLMs, including innovations in training infrastructure with AI accelerators, networking, storage, and scheduling. Additionally, the survey covers parallelism strategies, as well as optimizations for computation, communication, and memory in distributed LLM training. It also includes approaches of maintaining system reliability over extended training periods. By examining current innovations and future directions, this survey aims to provide valuable insights towards improving LLM training systems and tackling ongoing challenges. Furthermore, traditional digital circuit-based computing systems face significant constraints in meeting the computational demands of LLMs, highlighting the need for innovative solutions such as optical computing and optical networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268410645",
                    "name": "Jiangfei Duan"
                },
                {
                    "authorId": "2257086624",
                    "name": "Shuo Zhang"
                },
                {
                    "authorId": "2200108715",
                    "name": "Zerui Wang"
                },
                {
                    "authorId": "2196217535",
                    "name": "Lijuan Jiang"
                },
                {
                    "authorId": "2313633991",
                    "name": "Wenwen Qu"
                },
                {
                    "authorId": "2150570711",
                    "name": "Qi Hu"
                },
                {
                    "authorId": "2263696698",
                    "name": "Guoteng Wang"
                },
                {
                    "authorId": "2257054500",
                    "name": "Qizhen Weng"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2283503847",
                    "name": "Xingcheng Zhang"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2258618409",
                    "name": "Dahua Lin"
                },
                {
                    "authorId": "2114783855",
                    "name": "Yonggang Wen"
                },
                {
                    "authorId": "2279869979",
                    "name": "Xin Jin"
                },
                {
                    "authorId": "2146333441",
                    "name": "Tianwei Zhang"
                },
                {
                    "authorId": "2300809135",
                    "name": "Peng Sun"
                }
            ]
        },
        {
            "paperId": "656f4a76bcbc1a3a032ef5cf284909ef1bb58156",
            "title": "Turn Waste into Worth: Rectifying Top-k Router of MoE",
            "abstract": "Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectification effectively handle dropped tokens and padding, respectively. Furthermore, the combination of them achieves superior performance, surpassing the accuracy of the vanilla top-1 router by 4.7%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281709181",
                    "name": "Zhiyuan Zeng"
                },
                {
                    "authorId": "3187768",
                    "name": "Qipeng Guo"
                },
                {
                    "authorId": "2132200788",
                    "name": "Zhaoye Fei"
                },
                {
                    "authorId": "2155273086",
                    "name": "Zhangyue Yin"
                },
                {
                    "authorId": "2118117212",
                    "name": "Yunhua Zhou"
                },
                {
                    "authorId": "2107897400",
                    "name": "Linyang Li"
                },
                {
                    "authorId": "153345698",
                    "name": "Tianxiang Sun"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2258618409",
                    "name": "Dahua Lin"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                }
            ]
        },
        {
            "paperId": "6a932dd04c39c361bcacf730b3812b56df0e9b8a",
            "title": "Case2Code: Learning Inductive Reasoning with Synthetic Data",
            "abstract": "Complex reasoning is an impressive ability shown by large language models (LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought prompting or iterative tool-using to solve challenging tasks step-by-step. In this paper, we hope to focus on evaluating and teaching LLMs to conduct inductive reasoning, that is, LLMs are supposed to infer underlying rules by observing examples or sequential transformations. However, collecting large-scale and diverse human-generated inductive data is challenging. We focus on data synthesis in the code domain and propose a \\textbf{Case2Code} task by exploiting the expressiveness and correctness of programs. Specifically, we collect a diverse set of executable programs, synthesize input-output transformations for each program, and force LLMs to infer the underlying code implementations based on the synthetic I/O cases. We first evaluate representative LLMs on the synthesized Case2Code task and demonstrate that the Case-to-code induction is challenging for LLMs. Then, we synthesize large-scale Case2Code training samples to train LLMs to perform inductive reasoning. Experimental results show that such induction training benefits not only in distribution Case2Code performance but also enhances various coding abilities of trained LLMs, demonstrating the great potential of learning inductive reasoning via synthetic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "95329799",
                    "name": "Yunfan Shao"
                },
                {
                    "authorId": "2107897400",
                    "name": "Linyang Li"
                },
                {
                    "authorId": "2283837879",
                    "name": "Yichuan Ma"
                },
                {
                    "authorId": "2311748053",
                    "name": "Peiji Li"
                },
                {
                    "authorId": "2042948910",
                    "name": "Demin Song"
                },
                {
                    "authorId": "1834133",
                    "name": "Qinyuan Cheng"
                },
                {
                    "authorId": "2278813969",
                    "name": "Shimin Li"
                },
                {
                    "authorId": "50080067",
                    "name": "Xiaonan Li"
                },
                {
                    "authorId": "2199600227",
                    "name": "Pengyu Wang"
                },
                {
                    "authorId": "2303800191",
                    "name": "Qipeng Guo"
                },
                {
                    "authorId": "2302528360",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2284750473",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "2258618409",
                    "name": "Dahua Lin"
                }
            ]
        },
        {
            "paperId": "7b4005e0c624750252260bcac1ef67c9b804ec97",
            "title": "Balanced Data Sampling for Language Model Training with Clustering",
            "abstract": "Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and large language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "95329799",
                    "name": "Yunfan Shao"
                },
                {
                    "authorId": "2107897400",
                    "name": "Linyang Li"
                },
                {
                    "authorId": "2132200788",
                    "name": "Zhaoye Fei"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2258618409",
                    "name": "Dahua Lin"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                }
            ]
        },
        {
            "paperId": "7c16ef4e3c13265307c3569cc8f8ec5b0f7b0991",
            "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling",
            "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training. In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental results confirm that data with varying preference strengths have different impacts on reward model performance. We introduce a series of novel methods to mitigate the influence of incorrect and ambiguous preferences in the dataset and fully leverage high-quality preference data. (2) From an algorithmic standpoint, we introduce contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses, thereby improving model generalization. Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples, and this approach can be utilized for iterative RLHF optimization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188630983",
                    "name": "Bing Wang"
                },
                {
                    "authorId": "2058585152",
                    "name": "Rui Zheng"
                },
                {
                    "authorId": "2115386043",
                    "name": "Luyao Chen"
                },
                {
                    "authorId": "2275033850",
                    "name": "Yan Liu"
                },
                {
                    "authorId": "2042683163",
                    "name": "Shihan Dou"
                },
                {
                    "authorId": "2277419264",
                    "name": "Caishuang Huang"
                },
                {
                    "authorId": "2248291262",
                    "name": "Wei Shen"
                },
                {
                    "authorId": "2219131195",
                    "name": "Senjie Jin"
                },
                {
                    "authorId": "2240446306",
                    "name": "Enyu Zhou"
                },
                {
                    "authorId": "2279171753",
                    "name": "Chenyu Shi"
                },
                {
                    "authorId": "2181306462",
                    "name": "Songyang Gao"
                },
                {
                    "authorId": "2279134354",
                    "name": "Nuo Xu"
                },
                {
                    "authorId": "2212175381",
                    "name": "Yuhao Zhou"
                },
                {
                    "authorId": "2241140630",
                    "name": "Xiaoran Fan"
                },
                {
                    "authorId": "2190751523",
                    "name": "Zhiheng Xi"
                },
                {
                    "authorId": "2265759508",
                    "name": "Jun Zhao"
                },
                {
                    "authorId": "2118451107",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2279023445",
                    "name": "Tao Ji"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2279352722",
                    "name": "Lixing Shen"
                },
                {
                    "authorId": "2279039039",
                    "name": "Zhan Chen"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                },
                {
                    "authorId": "2256972361",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2257129989",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "3099139",
                    "name": "Zuxuan Wu"
                },
                {
                    "authorId": "2269831666",
                    "name": "Yuanyuan Jiang"
                }
            ]
        },
        {
            "paperId": "82f041ed71f8ef1cf462fa03a7e732e440259bd7",
            "title": "LongWanjuan: Towards Systematic Measurement for Long Text Quality",
            "abstract": "The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated, and chaotic types, enabling a detailed analysis of long-text quality. Furthermore, we devise a data mixture recipe that strategically balances different types of long texts within LongWanjuan, leading to significant improvements in model performance on long-text tasks. The code and dataset are available at https://github.com/OpenLMLab/LongWanjuan.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055634356",
                    "name": "Kai Lv"
                },
                {
                    "authorId": "2257094943",
                    "name": "Xiaoran Liu"
                },
                {
                    "authorId": "3187768",
                    "name": "Qipeng Guo"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2267889334",
                    "name": "Conghui He"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2258618409",
                    "name": "Dahua Lin"
                }
            ]
        },
        {
            "paperId": "893004b6a8a8c4455cf8e2d4e46ea1ee13cf3d48",
            "title": "Data-freeWeight Compress and Denoise for Large Language Models",
            "abstract": "Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains. Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed. To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization. Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise. In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices. Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in conjunction with pruning and quantization methods. We achieve a model pruning of 80% parameters while retaining 93.43% of the original performance without any calibration data. Additionally, we explore the fundamental properties of the weight matrix of LLMs undergone Rank-k Approximation and conduct comprehensive experiments to elucidate our hypothesis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2286883177",
                    "name": "Runyu Peng"
                },
                {
                    "authorId": "2118117212",
                    "name": "Yunhua Zhou"
                },
                {
                    "authorId": "3187768",
                    "name": "Qipeng Guo"
                },
                {
                    "authorId": "2287681221",
                    "name": "Yang Gao"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2258618409",
                    "name": "Dahua Lin"
                }
            ]
        }
    ]
}