{
    "authorId": "2112304965",
    "papers": [
        {
            "paperId": "67c0b0f0b1efb2e25084d23ed310109fb3c451aa",
            "title": "Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via Compositional Uncertainty Quantification",
            "abstract": "Pre-trained seq2seq models excel at graph semantic parsing with rich annotated data, but generalize worse to out-of-distribution (OOD) and long-tail examples. In comparison, symbolic parsers under-perform on population-level metrics, but exhibit unique strength in OOD and tail generalization. In this work, we study compositionality-aware approach to neural-symbolic inference informed by model confidence, performing fine-grained neural-symbolic reasoning at subgraph level (i.e., nodes and edges) and precisely targeting subgraph components with high uncertainty in the neural parser. As a result, the method combines the distinct strength of the neural and symbolic approaches in capturing different aspects of the graph prediction, leading to well-rounded generalization performance both across domains and in the tail. We empirically investigate the approach in the English Resource Grammar (ERG) parsing problem on a diverse suite of standard in-domain and seven OOD corpora. Our approach leads to 35.26% and 35.60% error reduction in aggregated SMATCH score over neural and symbolic approaches respectively, and 14% absolute accuracy gain in key tail linguistic categories over the neural model, outperforming prior state-of-art methods that do not account for compositionality or uncertainty.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112304965",
                    "name": "Zi Lin"
                },
                {
                    "authorId": "2108345570",
                    "name": "J. Liu"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "758eb8b631ca1aa485d88e2a2e70706d5256f050",
            "title": "Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty",
            "abstract": "Retrieval augmentation enhances generative language models by retrieving informative ex-emplars relevant for output prediction. However, in realistic graph parsing problems where the output space is large and complex, classic retrieval methods based on input-sentence similarity can fail to identify the most informative exemplars that target graph elements the model is most struggling about, leading to suboptimal retrieval and compromised prediction under limited retrieval budget. In this work, we improve retrieval-augmented parsing for complex graph problems by exploiting two unique sources of information (1) structural similarity and (2) model uncertainty. We pro-pose S tructure-aware and U ncertainty-G uided A daptive R etrieval (SUGAR) that first quantify the model uncertainty in graph prediction and identify its most uncertain subgraphs, and then retrieve exemplars based on their structural similarity with the identified uncertain subgraphs. On a suite of real-world parsing benchmarks with non-trivial graph structure (SMCalflow and E-commerce), SUGAR exhibits a strong advantage over its classic counterparts that do not leverage structure or model uncertainty.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112304965",
                    "name": "Zi Lin"
                },
                {
                    "authorId": "2273593708",
                    "name": "Quan Yuan"
                },
                {
                    "authorId": "2616463",
                    "name": "Panupong Pasupat"
                },
                {
                    "authorId": "2273531502",
                    "name": "J. Z. Liu"
                },
                {
                    "authorId": "2257005005",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "ea68ba918a92fd4460be9dddc7326724cdccbb5e",
            "title": "Large-Scale Generative Data-Free Distillation",
            "abstract": "Knowledge distillation is one of the most popular and effective techniques for knowledge transfer, model compression and semi-supervised learning. Most existing distillation approaches require the access to original or augmented training samples. But this can be problematic in practice due to privacy, proprietary and availability concerns. Recent work has put forward some methods to tackle this problem, but they are either highly time-consuming or unable to scale to large datasets. To this end, we propose a new method to train a generative image model by leveraging the intrinsic normalization layers' statistics of the trained teacher network. This enables us to build an ensemble of generators without training data that can efficiently produce substitute inputs for subsequent distillation. The proposed method pushes forward the data-free distillation performance on CIFAR-10 and CIFAR-100 to 95.02% and 77.02% respectively. Furthermore, we are able to scale it to ImageNet dataset, which to the best of our knowledge, has never been done using generative models in a data-free setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51225788",
                    "name": "Liangchen Luo"
                },
                {
                    "authorId": "144882893",
                    "name": "M. Sandler"
                },
                {
                    "authorId": "2112304965",
                    "name": "Zi Lin"
                },
                {
                    "authorId": "3422677",
                    "name": "A. Zhmoginov"
                },
                {
                    "authorId": "144727050",
                    "name": "Andrew G. Howard"
                }
            ]
        }
    ]
}