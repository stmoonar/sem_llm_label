{
    "authorId": "35660011",
    "papers": [
        {
            "paperId": "02a278b42385a490d4309ecfa9ad9dacb1cfc37d",
            "title": "KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations",
            "abstract": "While recent works have been considerably improving the quality of the natural language explanations (NLEs) generated by a model to justify its predictions, there is very limited research in detecting and alleviating inconsistencies among generated NLEs. In this work, we leverage external knowledge bases to significantly improve on an existing adversarial attack for detecting inconsistent NLEs. We apply our attack to high-performing NLE models and show that models with higher NLE quality do not necessarily generate fewer inconsistencies. Moreover, we propose an off-the-shelf mitigation method to alleviate inconsistencies by grounding the model into external background knowledge. Our method decreases the inconsistencies of previous high-performing NLE models as detected by our attack.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35756238",
                    "name": "Myeongjun Jang"
                },
                {
                    "authorId": "3165738",
                    "name": "Bodhisattwa Prasad Majumder"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                },
                {
                    "authorId": "1690572",
                    "name": "Thomas Lukasiewicz"
                },
                {
                    "authorId": "2115552858",
                    "name": "Oana-Maria Camburu"
                }
            ]
        },
        {
            "paperId": "1673b72a4ebb56ae3775131937c40afae3853e9b",
            "title": "Data Distillation: A Survey",
            "abstract": "The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40705044",
                    "name": "Noveen Sachdeva"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "277e0ed89014d76452b0e8d4eb7ef26c8547f56d",
            "title": "3rd Workshop on Online and Adaptive Recommender Systems (OARS)",
            "abstract": "Recommender systems (RecSys) play important roles in helping users navigate, discover, and consume large and highly dynamic information. Today, many RecSys solutions deployed in the real world rely on categorical user-profiles and/or pre-calculated recommendation actions that stay static during a user session. However, recent trends suggest that RecSys need to model user intent in real time and constantly adapt to meet user needs at the moment or change user behavior in situ. There are three primary drivers for this emerging need of online adaptation. First, in order to meet the increasing demand for a better personalized experience, the personalization dimensions and space will grow larger and larger. It would not be feasible to pre-compute recommended actions for all personalization scenarios beyond a certain scale. Second, in many settings the system does not have user prior history to leverage. Estimating user intent in real time is the only feasible way to personalize. As various consumer privacy laws tighten, it is foreseeable that many businesses will reduce their reliance on static user profiles. Therefore, it makes the modeling of user intent in real time an important research topic. Third, a user's intent often changes within a session and between sessions, and user behavior could shift significantly during dramatic events. A RecSys should adapt in real time to meet user needs and be robust against distribution shifts. The online and adaptive recommender systems (OARS) workshop offers a focused discussion of the study and application of OARS, and will bring together an interdisciplinary community of researchers and practitioners from both industry and academia. KDD, as the premier data science conference, is an ideal venue to gather leaders in the field to further research into OARS and promote its adoption. This workshop is complementary to several sessions of the main conference (e.g., recommendation, reinforcement learning, etc.) and brings them together using a practical and focused application.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087078033",
                    "name": "Xiquan Cui"
                },
                {
                    "authorId": "1942166",
                    "name": "Vachik S. Dave"
                },
                {
                    "authorId": "144908640",
                    "name": "Yi-Hsun Su"
                },
                {
                    "authorId": "2123018867",
                    "name": "Khalifeh Al-Jadda"
                },
                {
                    "authorId": "2109543348",
                    "name": "Srijan Kumar"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                },
                {
                    "authorId": "2112409071",
                    "name": "Tao Ye"
                },
                {
                    "authorId": "1721424",
                    "name": "Stephen D. Guo"
                },
                {
                    "authorId": "2227573033",
                    "name": "Chip Huyen"
                }
            ]
        },
        {
            "paperId": "2a09ebbfcca1a6994eeb472cd4159f5f3858dbf9",
            "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion",
            "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference. All the codes and data are available at https://github.com/microsoft/CodeBERT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2278834796",
                    "name": "Daya Guo"
                },
                {
                    "authorId": "66247317",
                    "name": "Canwen Xu"
                },
                {
                    "authorId": "46429989",
                    "name": "Nan Duan"
                },
                {
                    "authorId": "2152938031",
                    "name": "Jian Yin"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "34d9ed6c9313f2811f629a4a57231bf95b041b5b",
            "title": "Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Reviews",
            "abstract": "It is time-consuming to find the best product among many similar alternatives. Comparative sentences can help to contrast one item from others in a way that highlights important features of an item that stand out. Given reviews of one or multiple items and relevant item features, we generate comparative review sentences to aid users to find the best fit. Specifically, our model consists of three successive components in a transformer: (i) an item encoding module to encode an item for comparison, (ii) a comparison generation module that generates comparative sentences in an autoregressive manner, (iii) a novel decoding method for user personalization. We show that our pipeline generates fluent and diverse comparative sentences. We run experiments on the relevance and fidelity of our generated sentences in a human evaluation study and find that our algorithm creates comparative review sentences that are relevant and truthful.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80743661",
                    "name": "J. Echterhoff"
                },
                {
                    "authorId": "47837986",
                    "name": "An Yan"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "34fd95dd4dd32e704d4284fc31165e85b303bb1e",
            "title": "CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets",
            "abstract": "Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps: (i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest; (iii) map the predicted subclass back to its parent to produce the final prediction. Across numerous datasets with underlying hierarchical structure, CHiLS leads to improved accuracy in situations both with and without ground-truth hierarchical information. CHiLS is simple to implement within existing zero-shot pipelines and requires no additional training cost. Code is available at: https://github.com/acmi-lab/CHILS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189130909",
                    "name": "Zachary Novack"
                },
                {
                    "authorId": "144901377",
                    "name": "S. Garg"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                },
                {
                    "authorId": "32219137",
                    "name": "Zachary Chase Lipton"
                }
            ]
        },
        {
            "paperId": "4f1a26b055d9973853c039bf554b19a6587ab286",
            "title": "Jointly modeling products and resource pages for task-oriented recommendation",
            "abstract": "Modeling high-level user intent in recommender systems can improve performance, although it is often difficult to obtain a ground truth measure of this intent. In this paper, we investigate a novel way to obtain such an intent signal by leveraging resource pages associated with a particular task. We jointly model product interactions and resource page interactions to create a system which can recommend both products and resource pages to users. Our experiments consider the domain of home improvement product recommendation, where resource pages are DIY (do-it-yourself) project pages from Lowes.com. Each DIY page provides a list of tools, materials, and step-by-step instructions to complete a DIY project, such as building a deck, installing cabinets, and fixing a leaking pipe. We use this data as an indicator of the intended project, which is a natural high-level intent signal for home improvement shoppers. We then extend a state-of-the-art system to incorporate this new intent data, and show a significant improvement in the ability of the system to recommend products. We further demonstrate that our system can be used to successfully recommend DIY project pages to users. We have taken initial steps towards deploying our method for project recommendation in production on the Lowe\u2019s website and for recommendations through marketing emails.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32370715",
                    "name": "B. Duncan"
                },
                {
                    "authorId": "3378098",
                    "name": "Surya Kallumadi"
                },
                {
                    "authorId": "1400419309",
                    "name": "Taylor Berg-Kirkpatrick"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "54a86a3042591b9757c292630594b6487a2fc82c",
            "title": "Reinforcement Learning for Generative AI: A Survey",
            "abstract": "Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning has become a trending research field and has stretched the limits of generative AI in both model design and application. It is reasonable to summarize and conclude advances in recent years with a comprehensive review. Although there are surveys in different application areas recently, this survey aims to shed light on a high-level review that spans a range of application areas. We provide a rigorous taxonomy in this area and make sufficient coverage on various models and applications. Notably, we also surveyed the fast-developing large language model area. We conclude this survey by showing the potential directions that might tackle the limit of current models and expand the frontiers for generative AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388993143",
                    "name": "Yuanjiang Cao"
                },
                {
                    "authorId": "2115514578",
                    "name": "Lina Yao"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                },
                {
                    "authorId": "120607997",
                    "name": "Quan.Z Sheng"
                }
            ]
        },
        {
            "paperId": "6028780bfb3728292d37c07951e3f463fae0981e",
            "title": "Unsupervised Improvement of Factual Knowledge in Language Models",
            "abstract": "Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge. In this work, we propose an approach for influencing MLM pretraining in a way that can improve language model performance on a variety of knowledge-intensive tasks. We force the language model to prioritize informative words in a fully unsupervised way. Experiments demonstrate that the proposed approach can significantly improve the performance of pretrained language models on tasks such as factual recall, question answering, sentiment analysis, and natural language inference in a closed-book setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "68972893",
                    "name": "Nafis Sadeq"
                },
                {
                    "authorId": "2428654",
                    "name": "Byungkyu Kang"
                },
                {
                    "authorId": "2213358377",
                    "name": "Prarit Lamba"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "62f559604c2869bd97d4bdc6c64645e8518c207c",
            "title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems",
            "abstract": "Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48283085",
                    "name": "Xiaocong Chen"
                },
                {
                    "authorId": "2128046810",
                    "name": "Siyu Wang"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                },
                {
                    "authorId": "1705282",
                    "name": "D. Jannach"
                },
                {
                    "authorId": "2106357243",
                    "name": "Lina Yao"
                }
            ]
        }
    ]
}