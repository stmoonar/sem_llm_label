{
    "authorId": "1405511901",
    "papers": [
        {
            "paperId": "21b4777948797377deedf4a9f1f58ad13f6b8b5d",
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2887412",
                    "name": "Jinchao Li"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2066415714",
                    "name": "Zhengcong Fei"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2257374643",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2116575668",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "1979505",
                    "name": "A. Geramifard"
                },
                {
                    "authorId": "1765212",
                    "name": "Chiori Hori"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "2cdfff602a5710f0ed1aa652dd51f6e4088d731a",
            "title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models",
            "abstract": "Various audio-LLMs (ALLMs) have been explored recently for tackling different audio tasks simultaneously using a single, unified model. While existing evaluations of ALLMs primarily focus on single-audio tasks, real-world applications often involve processing multiple audio streams simultaneously. To bridge this gap, we propose the first multi-audio evaluation (MAE) benchmark that consists of 20 datasets from 11 multi-audio tasks encompassing both speech and sound scenarios. Comprehensive experiments on MAE demonstrate that the existing ALLMs, while being powerful in comprehending primary audio elements in individual audio inputs, struggling to handle multi-audio scenarios. To this end, we propose a novel multi-audio-LLM (MALLM) to capture audio context among multiple similar audios using discriminative learning on our proposed synthetic data. The results demonstrate that the proposed MALLM outperforms all baselines and achieves high data efficiency using synthetic data without requiring human annotations. The proposed MALLM opens the door for ALLMs towards multi-audio processing era and brings us closer to replicating human auditory capabilities in machines.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2309430062",
                    "name": "Yiming Chen"
                },
                {
                    "authorId": "10675491",
                    "name": "Xianghu Yue"
                },
                {
                    "authorId": "2149396823",
                    "name": "Xiaoxue Gao"
                },
                {
                    "authorId": "2321413639",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1726720",
                    "name": "R. Tan"
                },
                {
                    "authorId": "2287269266",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "3aabd69e13f64f10fd210e4e9e6b2e75c0e734d1",
            "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
            "abstract": "Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305602940",
                    "name": "David Romero"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "49918371",
                    "name": "Haryo Akbarianto Wibowo"
                },
                {
                    "authorId": "2298756162",
                    "name": "Teresa Lynn"
                },
                {
                    "authorId": "3248560",
                    "name": "Injy Hamed"
                },
                {
                    "authorId": "2305619033",
                    "name": "Aditya Nanda Kishore"
                },
                {
                    "authorId": "2305622255",
                    "name": "Aishik Mandal"
                },
                {
                    "authorId": "2305619182",
                    "name": "Alina Dragonetti"
                },
                {
                    "authorId": "1396213362",
                    "name": "Artem Abzaliev"
                },
                {
                    "authorId": "2148631756",
                    "name": "A. Tonja"
                },
                {
                    "authorId": "2305622331",
                    "name": "Bontu Fufa Balcha"
                },
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "51124788",
                    "name": "Christian Salamea"
                },
                {
                    "authorId": "1994718316",
                    "name": "Dan John Velasco"
                },
                {
                    "authorId": "2273673245",
                    "name": "D. Adelani"
                },
                {
                    "authorId": "70145452",
                    "name": "D. Meur"
                },
                {
                    "authorId": "2183780558",
                    "name": "Emilio Villa-Cueva"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "2266756359",
                    "name": "Fauzan Farooqui"
                },
                {
                    "authorId": "1738707459",
                    "name": "Frederico Belcavello"
                },
                {
                    "authorId": "2151970366",
                    "name": "Ganzorig Batnasan"
                },
                {
                    "authorId": "2305623074",
                    "name": "Gisela Vallejo"
                },
                {
                    "authorId": "2305619231",
                    "name": "Grainne Caulfield"
                },
                {
                    "authorId": "2213060824",
                    "name": "Guido Ivetta"
                },
                {
                    "authorId": "2980506",
                    "name": "Haiyue Song"
                },
                {
                    "authorId": "2305619369",
                    "name": "Henok Biadglign Ademtew"
                },
                {
                    "authorId": "2139773809",
                    "name": "Hern\u00e1n Maina"
                },
                {
                    "authorId": "116344405",
                    "name": "Holy Lovenia"
                },
                {
                    "authorId": "2304752238",
                    "name": "Israel Abebe Azime"
                },
                {
                    "authorId": "2282499634",
                    "name": "Jan Christian Blaise Cruz"
                },
                {
                    "authorId": "1992915388",
                    "name": "Jay Gala"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "1724941617",
                    "name": "Jes\u00fas-Germ\u00e1n Ortiz-Barajas"
                },
                {
                    "authorId": "90765684",
                    "name": "Jinheon Baek"
                },
                {
                    "authorId": "2305082736",
                    "name": "Jocelyn Dunstan"
                },
                {
                    "authorId": "2276687",
                    "name": "L. A. Alemany"
                },
                {
                    "authorId": "2290013575",
                    "name": "Kumaranage Ravindu Yasas Nagasinghe"
                },
                {
                    "authorId": "2066254822",
                    "name": "Luciana Benotti"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1738707461",
                    "name": "Marcelo Viridiano"
                },
                {
                    "authorId": "2168569460",
                    "name": "Marcos Estecha-Garitagoitia"
                },
                {
                    "authorId": "2305622553",
                    "name": "Maria Camila Buitrago Cabrera"
                },
                {
                    "authorId": "2220406508",
                    "name": "Mario Rodr'iguez-Cantelar"
                },
                {
                    "authorId": "71090258",
                    "name": "M\u00e9lanie Jouitteau"
                },
                {
                    "authorId": "121947924",
                    "name": "M. Mihaylov"
                },
                {
                    "authorId": "2305619376",
                    "name": "Mohamed Fazli Mohamed Imam"
                },
                {
                    "authorId": "2191731497",
                    "name": "Muhammad Farid Adilazuarda"
                },
                {
                    "authorId": "66556569",
                    "name": "Munkhjargal Gochoo"
                },
                {
                    "authorId": "2159634278",
                    "name": "Munkh-Erdene Otgonbold"
                },
                {
                    "authorId": "1742219452",
                    "name": "Naome A. Etori"
                },
                {
                    "authorId": "2305623065",
                    "name": "Olivier Niyomugisha"
                },
                {
                    "authorId": "2307313242",
                    "name": "Paula M'onica Silva"
                },
                {
                    "authorId": "2040713514",
                    "name": "Pranjal A. Chitale"
                },
                {
                    "authorId": "3209719",
                    "name": "Raj Dabre"
                },
                {
                    "authorId": "2148764367",
                    "name": "Rendi Chevi"
                },
                {
                    "authorId": "49775305",
                    "name": "Ruochen Zhang"
                },
                {
                    "authorId": "2197070752",
                    "name": "Ryandito Diandaru"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "2305622481",
                    "name": "Santiago G'ongora"
                },
                {
                    "authorId": "8599185",
                    "name": "Soyeong Jeong"
                },
                {
                    "authorId": "152881983",
                    "name": "Sukannya Purkayastha"
                },
                {
                    "authorId": "83446147",
                    "name": "Tatsuki Kuribayashi"
                },
                {
                    "authorId": "2219413815",
                    "name": "Thanmay Jayakumar"
                },
                {
                    "authorId": "2244512282",
                    "name": "T. Torrent"
                },
                {
                    "authorId": "2305621229",
                    "name": "Toqeer Ehsan"
                },
                {
                    "authorId": "2283931820",
                    "name": "Vladimir Araujo"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "2305621242",
                    "name": "Zara Burzo"
                },
                {
                    "authorId": "2305621264",
                    "name": "Zheng Wei Lim"
                },
                {
                    "authorId": "2282475073",
                    "name": "Zheng-Xin Yong"
                },
                {
                    "authorId": "2293317558",
                    "name": "Oana Ignat"
                },
                {
                    "authorId": "2218338376",
                    "name": "Joan Nwatu"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                },
                {
                    "authorId": "1794626",
                    "name": "T. Solorio"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                }
            ]
        },
        {
            "paperId": "8325631bba5ef8492d7171c35dbd2b64fef11897",
            "title": "Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial Framework Driven by Large Language Models",
            "abstract": "The automatic evaluation of natural language generation (NLG) systems presents a long-lasting challenge. Recent studies have highlighted various neural metrics that align well with human evaluations. Yet, the robustness of these evaluators against adversarial perturbations remains largely under-explored due to the unique challenges in obtaining adversarial data for different NLG evaluation tasks. To address the problem, we introduce AdvEval, a novel black-box adversarial framework against NLG evaluators. AdvEval is specially tailored to generate data that yield strong disagreements between human and victim evaluators. Specifically, inspired by the recent success of large language models (LLMs) in text generation and evaluation, we adopt strong LLMs as both the data generator and gold evaluator. Adversarial data are automatically optimized with feedback from the gold and victim evaluator. We conduct experiments on 12 victim evaluators and 11 NLG datasets, spanning tasks including dialogue, summarization, and question evaluation. The results show that AdvEval can lead to significant performance degradation of various victim metrics, thereby validating its efficacy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188651547",
                    "name": "Yiming Chen"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2292191740",
                    "name": "Danqing Luo"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1726720",
                    "name": "R. Tan"
                },
                {
                    "authorId": "2302787029",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "d05e4e4d05c09d90c56cfa5ff903c8bfc4e605ef",
            "title": "Awareness in robotics: An early perspective from the viewpoint of the EIC Pathfinder Challenge \"Awareness Inside\"",
            "abstract": "Consciousness has been historically a heavily debated topic in engineering, science, and philosophy. On the contrary, awareness had less success in raising the interest of scholars in the past. However, things are changing as more and more researchers are getting interested in answering questions concerning what awareness is and how it can be artificially generated. The landscape is rapidly evolving, with multiple voices and interpretations of the concept being conceived and techniques being developed. The goal of this paper is to summarize and discuss the ones among these voices connected with projects funded by the EIC Pathfinder Challenge called ``Awareness Inside'', a nonrecurring call for proposals within Horizon Europe designed specifically for fostering research on natural and synthetic awareness. In this perspective, we dedicate special attention to challenges and promises of applying synthetic awareness in robotics, as the development of mature techniques in this new field is expected to have a special impact on generating more capable and trustworthy embodied systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35178897",
                    "name": "C. D. Santina"
                },
                {
                    "authorId": "72825942",
                    "name": "C. H. Corbato"
                },
                {
                    "authorId": "48696566",
                    "name": "Burak Sisman"
                },
                {
                    "authorId": "2284062894",
                    "name": "Luis A. Leiva"
                },
                {
                    "authorId": "2280254525",
                    "name": "Ioannis Arapakis"
                },
                {
                    "authorId": "2280251154",
                    "name": "Michalis Vakalellis"
                },
                {
                    "authorId": "2274388506",
                    "name": "Jean Vanderdonckt"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "2284062891",
                    "name": "Guido Manzi"
                },
                {
                    "authorId": "2290845562",
                    "name": "Cristina Becchio"
                },
                {
                    "authorId": "2284062996",
                    "name": "Aida Elamrani"
                },
                {
                    "authorId": "2284063385",
                    "name": "Mohsen Alirezaei"
                },
                {
                    "authorId": "2284062896",
                    "name": "Ginevra Castellano"
                },
                {
                    "authorId": "1722151",
                    "name": "Dimos V. Dimarogonas"
                },
                {
                    "authorId": "2261744171",
                    "name": "Arabinda Ghosh"
                },
                {
                    "authorId": "3064175",
                    "name": "S. Haesaert"
                },
                {
                    "authorId": "47567935",
                    "name": "S. Soudjani"
                },
                {
                    "authorId": "3022522",
                    "name": "S. Stroeve"
                },
                {
                    "authorId": "2237971575",
                    "name": "Paul Verschure"
                },
                {
                    "authorId": "3224102",
                    "name": "D. Bacciu"
                },
                {
                    "authorId": "1999550",
                    "name": "Oph\u00e9lia Deroy"
                },
                {
                    "authorId": "1935873",
                    "name": "B. Bahrami"
                },
                {
                    "authorId": "2270670823",
                    "name": "Claudio Gallicchio"
                },
                {
                    "authorId": "2242951786",
                    "name": "Sabine Hauert"
                },
                {
                    "authorId": "2284063001",
                    "name": "Ricardo Sanz"
                },
                {
                    "authorId": "2258948686",
                    "name": "Pablo Lanillos"
                },
                {
                    "authorId": "2271913473",
                    "name": "G. Iacca"
                },
                {
                    "authorId": "1710108",
                    "name": "S. Sigg"
                },
                {
                    "authorId": "1885752",
                    "name": "M. Gasulla"
                },
                {
                    "authorId": "2250016929",
                    "name": "Luc Steels"
                },
                {
                    "authorId": "2284063003",
                    "name": "Carles Sierra"
                }
            ]
        },
        {
            "paperId": "90d04e5f1cb6efb12e14e388730cf58666325edb",
            "title": "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators",
            "abstract": "Automatic evaluation is an integral aspect of dialogue system research. The traditional reference-based NLG metrics are generally found to be unsuitable for dialogue assessment. Consequently, recent studies have suggested various unique, reference-free neural metrics that better align with human evaluations. Notably among them, large language models (LLMs), particularly the instruction-tuned variants like ChatGPT, are shown to be promising substitutes for human judges. Yet, existing works on utilizing LLMs for automatic dialogue evaluation are limited in their scope in terms of the number of meta-evaluation datasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains inconclusive how effective these LLMs are. To this end, we conduct a comprehensive study on the application of LLMs for automatic dialogue evaluation. Specifically, we analyze the multi-dimensional evaluation capability of 30 recently emerged LLMs at both turn and dialogue levels, using a comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the robustness of the LLMs in handling various adversarial perturbations at both turn and dialogue levels. Finally, we explore how model-level and dimension-level ensembles impact the evaluation performance. All resources are available at https://github.com/e0397123/comp-analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "2188651547",
                    "name": "Yiming Chen"
                },
                {
                    "authorId": "2145120631",
                    "name": "Malu Zhang"
                },
                {
                    "authorId": "2258605037",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "9799c17fd287bb9e8d231fe032c6dbf9c0c9d675",
            "title": "Overview of Robust and Multilingual Automatic Evaluation Metrics\n\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4",
            "abstract": "The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220406508",
                    "name": "Mario Rodr'iguez-Cantelar"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1672552269",
                    "name": "Chengguang Tang"
                },
                {
                    "authorId": "2026468506",
                    "name": "Ke Shi"
                },
                {
                    "authorId": "3022427",
                    "name": "Sarik Ghazarian"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "b64b2d28e00e1bdf35393856707cbd133058abab",
            "title": "xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark",
            "abstract": "Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined. This is largely due to the absence of a multilingual dialogue evaluation benchmark. To address the issue, we introduce xDial-Eval, built on top of open-source English dialogue evaluation datasets. xDial-Eval includes 12 turn-level and 6 dialogue-level English datasets, comprising 14930 annotated turns and 8691 annotated dialogues respectively. The English dialogue data are extended to nine other languages with commercial machine translation systems. On xDial-Eval, we conduct comprehensive analyses of previous BERT-based metrics and the recently-emerged large language models. Lastly, we establish strong self-supervised and multilingual baselines. In terms of average Pearson correlations over all datasets and languages, the best baseline outperforms OpenAI's ChatGPT by absolute improvements of 6.5% and 4.6% at the turn and dialogue levels respectively, albeit with much fewer parameters. The data and code are publicly available at https://github.com/e0397123/xDial-Eval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "2258775321",
                    "name": "Chengguang Tang"
                },
                {
                    "authorId": "2258553844",
                    "name": "Ke Shi"
                },
                {
                    "authorId": "2065465380",
                    "name": "Guohua Tang"
                },
                {
                    "authorId": "2258605037",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "1e4f7b2156dd6c37401afbc36bd0c19bf62cf8d5",
            "title": "Automatic Detection of Inconsistencies in Open-Domain Chatbots",
            "abstract": "Current pre-trained Large Language Models applied to chat-bots are capable of producing good quality sentences, handling different conversation topics, and larger interaction times. Unfortunately, the generated responses highly depend on the data on which the chatbot has been trained on, the specific dialogue history and current turn used for guiding the response, the internal decoding mechanisms, ranking strategies, among others. Therefore, it may happen that for the same question asked by the user, the chatbot may provide a different answer, which in a long-term interaction may produce confusion. In this paper, we propose a new methodology based on three phases: a) automatic detection of dialogue topics using zero-shot learning approaches, b) automatic clustering of distinctive questions, and c) detecting inconsistent answers using K-Means clustering and the Silhouette coefficient. To test our proposal, we used the DailyDialog dataset to detect up to 13 different topics. To detect inconsistencies, we manually generated multiple paraphrased questions. Then, we used multiple pre-trained chatbots to answer those questions. Our results in topic detection show a weighted F-1 value of 0.658, and a 3.4 MSE to predict the number of different responses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2190335041",
                    "name": "Jorge Mira Prats"
                },
                {
                    "authorId": "2168569460",
                    "name": "Marcos Estecha-Garitagoitia"
                },
                {
                    "authorId": "2210935527",
                    "name": "Mario Rodr\u00edguez-Cantelar"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                }
            ]
        },
        {
            "paperId": "45c55fe92abc0ed4c2190fe039c9a23d70a0e33a",
            "title": "Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges",
            "abstract": "This is a report on the NSF Future Directions Workshop on Automatic Evaluation of Dialog. The workshop explored the current state of the art along with its limitations and suggested promising directions for future work in this important and very rapidly changing area of research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32251567",
                    "name": "Shikib Mehri"
                },
                {
                    "authorId": "2155313947",
                    "name": "Jinho Choi"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "1716325",
                    "name": "M. Esk\u00e9nazi"
                },
                {
                    "authorId": "1768624",
                    "name": "Milica Gasic"
                },
                {
                    "authorId": "3194430",
                    "name": "Kallirroi Georgila"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "1681799",
                    "name": "Verena Rieser"
                },
                {
                    "authorId": "39049552",
                    "name": "Samira Shaikh"
                },
                {
                    "authorId": "144518646",
                    "name": "D. Traum"
                },
                {
                    "authorId": "47999368",
                    "name": "Yi-Ting Yeh"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2159589560",
                    "name": "Yizhe Zhang"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                }
            ]
        }
    ]
}