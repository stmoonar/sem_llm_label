{
    "authorId": "2172078823",
    "papers": [
        {
            "paperId": "340a2513a7d807361c1b122092f16d04e9aa8fab",
            "title": "DecomFormer: Decompose Self-Attention Via Fourier Transform for VHR Aerial Image Scene Classification",
            "abstract": "Very high-resolution (VHR) aerial image scene classification is an essential task for aerial image understanding. Although transformer-based models have demonstrated strong ability in natural image classification, transformer-based methods on VHR aerial image tasks are still lack of concern because the complexity of self-attention in the transformer grows quadratically with the image resolution. To address this issue, we decompose the self-attention via Fourier Transform and propose a novel Fourier self-attention (FSA) mechanism. Based on FSA, we design a highly efficient network named DecomFormer, which learns contextual relationships in the real part and imaginary part of the Fourier field, respectively. Theoretically, the DecomFormer reduces the complexity of the naive self-attention mechanism from O(n2) to O(nlog(n)). Universal experiments on public VHR aerial image classification benchmarks demonstrated the DecomFormer\u2019s efficiency, especially on images with very high-resolution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2172078823",
                    "name": "Xiyuan Gao"
                },
                {
                    "authorId": "2187308496",
                    "name": "Xiao Pu"
                },
                {
                    "authorId": "41154933",
                    "name": "Tao Wang"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "0fcfc3c0d13facbfec603cccbee9eec40a73aea8",
            "title": "Contextual Learning in Fourier Complex Field for VHR Remote Sensing Images",
            "abstract": "Very high-resolution (VHR) remote sensing (RS) image classification is the fundamental task for RS image analysis and understanding. Recently, Transformer-based models demonstrated outstanding potential for learning high-order contextual relationships from natural images with general resolution ( \u2248 224 \u00d7 224 pixels) and achieved remarkable results on general image classification tasks. However, the complexity of the naive Transformer grows quadratically with the increase in image size, which prevents Transformer-based models from VHR RS image ( \u2265 500 \u00d7 500 pixels) classification and other computationally expensive downstream tasks. To this end, we propose to decompose the expensive self-attention (SA) into real and imaginary parts via discrete Fourier transform (DFT) and, therefore, propose an efficient complex SA (CSA) mechanism. Benefiting from the conjugated symmetric property of DFT, CSA is capable to model the high-order contextual information with less than half computations of naive SA. To overcome the gradient explosion in Fourier complex field, we replace the Softmax function with the carefully designed Logmax function to normalize the attention map of CSA and stabilize the gradient propagation. By stacking various layers of CSA blocks, we propose the Fourier complex Transformer (FCT) model to learn global contextual information from VHR aerial images following the hierarchical manners. Universal experiments conducted on commonly used RS classification datasets demonstrate the effectiveness and efficiency of FCT, especially on VHR RS images. The source code of FCT will be available at https://github.com/Gao-xiyuan/FCT.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2172078823",
                    "name": "Xiyuan Gao"
                },
                {
                    "authorId": "3440080",
                    "name": "Qingyan Duan"
                },
                {
                    "authorId": "40939264",
                    "name": "Jiaxu Leng"
                },
                {
                    "authorId": "2187308496",
                    "name": "Xiao Pu"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "11926237f16c11db6bd50d9818847b6af81372bc",
            "title": "DHT: Deformable Hybrid Transformer for Aerial Image Segmentation",
            "abstract": "Due to the strong ability to model global information, the transformer-based methods have shown remarkable improvements in the image segmentation tasks. However, the self-attention mechanism in the transformer is computationally expensive and relies on pretrained parameters. Moreover, the transformer method is weak in modeling local information, which is unfavorable for accurately segmenting objects from high-resolution aerial images. To this end, an efficient deformable orientational self-attention (DoA) is proposed to simultaneously extract the global information and the local information. Besides, for parameter efficiency, we design a depthwise channel self-attention (DcA) to model the contextual information among channels. Combining with the DoA and DcA, we propose the deformable hybrid transformer (DHT) to perform high-quality object segmentation on aerial images. Experiments on the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Wuhan University (WHU) building datasets illustrate that the proposed DHT can not only achieve the state-of-the-art (SOTA) results but also markedly reduce the dependence of the transformer on pretrained parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2172078823",
                    "name": "Xiyuan Gao"
                },
                {
                    "authorId": "3440080",
                    "name": "Qingyan Duan"
                },
                {
                    "authorId": "2152194037",
                    "name": "Lin Yuan"
                },
                {
                    "authorId": "2164214077",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "d700dd7957ccb2cf356dec85c8659d9cea8923f8",
            "title": "Deep CNN-based Inductive Transfer Learning for Sarcasm Detection in Speech",
            "abstract": "Sarcasm is a frequently used linguistic device which is expressed in a multitude of ways, both with acoustic cues (including pitch, intonation, intensity, etc.) and visual cues (including facial expression, eye gaze, etc.). While cues used in the expression of sarcasm are well-described in the literature, there is a striking paucity of attempts to perform automatic sarcasm detection in speech. To explore this gap, we elaborate a methodology of implementing Inductive Transfer Learning (ITL) based on pre-trained Deep Convolutional Neural Networks (DCNNs) to detect sarcasm in speech. To those ends, the multimodal dataset MUStARD is used as a target dataset in this study. The two selected pre-trained DCNN models used are Xception and VGGish, which we trained on visual and audio datasets. Re-sults show that VGGish, which is applied as a feature extractor in the experiment, performs better than Xception, which has its convolutional layers and pooling layers retrained. Both models achieve a higher F-score compared to the baseline Support Vector Machines (SVM) model by 7% and 5% in unimodal sarcasm detection in speech.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2172078823",
                    "name": "Xiyuan Gao"
                },
                {
                    "authorId": "3262083",
                    "name": "Shekhar Nayak"
                },
                {
                    "authorId": "65906957",
                    "name": "M. Coler"
                }
            ]
        }
    ]
}