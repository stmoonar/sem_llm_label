{
    "authorId": "2247662718",
    "papers": [
        {
            "paperId": "563e59d708457c1271fbc1998faf224565aad2c1",
            "title": "Improving Black-box Robustness with In-Context Rewriting",
            "abstract": "Machine learning models for text classification often excel on in-distribution (ID) data but struggle with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a black box, such as when the weights are frozen, retraining is costly, or the model is leveraged via an API. Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps black-box constraints by aggregating predictions across multiple augmentations of the test input. TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations. In this work, we propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for BERT and T5 models, with BERT's OOD robustness improving by an average of 4.48 percentage points without regressing average ID performance. We explore selectively augmenting inputs based on prediction entropy to reduce the rate of expensive LLM augmentations, allowing us to maintain performance gains while reducing the average number of generated augmentations by 57.74\\%. LLM-TTA is agnostic to the task model architecture, does not require OOD labels, and is effective across low and high-resource settings. We share our data, models, and code for reproducibility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212970046",
                    "name": "Kyle O'Brien"
                },
                {
                    "authorId": "2283934556",
                    "name": "Nathan Ng"
                },
                {
                    "authorId": "65971246",
                    "name": "Isha Puri"
                },
                {
                    "authorId": "2283926120",
                    "name": "Jorge Mendez"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2260289990",
                    "name": "Yoon Kim"
                },
                {
                    "authorId": "2275548863",
                    "name": "Marzyeh Ghassemi"
                },
                {
                    "authorId": "1452598350",
                    "name": "Tom Hartvigsen"
                }
            ]
        },
        {
            "paperId": "be7122f7b2db3bce3137519b1f81e79fa57c9eaa",
            "title": "Eureka: Evaluating and Understanding Large Foundation Models",
            "abstract": "Rigorous and reproducible evaluation is critical for assessing the state of the art and for guiding scientific advances in Artificial Intelligence. Evaluation is challenging in practice due to several reasons, including benchmark saturation, lack of transparency in methods used for measurement, development challenges in extracting measurements for generative tasks, and, more generally, the extensive number of capabilities required for a well-rounded comparison across models. We make three contributions to alleviate the above challenges. First, we present Eureka, an open-source framework for standardizing evaluations of large foundation models beyond single-score reporting and rankings. Second, we introduce Eureka-Bench as an extensible collection of benchmarks testing capabilities that (i) are still challenging for state-of-the-art models and (ii) represent fundamental but overlooked language and multimodal capabilities. The inherent space for improvement in non-saturated benchmarks enables us to discover meaningful differences between models at a capability level. Third, using Eureka, we conduct an analysis of 12 state-of-the-art models, providing in-depth insights into failure understanding and model comparison, which can be leveraged to plan targeted improvements. In contrast to recent trends in reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for some capabilities. Despite the recent improvements, current models still struggle with several fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2321484904",
                    "name": "Jingya Chen"
                },
                {
                    "authorId": "2250480908",
                    "name": "Neel Joshi"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2321455461",
                    "name": "Eduardo Salinas"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                },
                {
                    "authorId": "2321455359",
                    "name": "James Woffinden-Luey"
                },
                {
                    "authorId": "2670023",
                    "name": "Safoora Yousefi"
                }
            ]
        },
        {
            "paperId": "0244aeb7c6927e2fb0c2e668687e160a00737dbe",
            "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
            "abstract": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "2146720788",
                    "name": "Arindam Mitra"
                },
                {
                    "authorId": "2065043351",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "2211923024",
                    "name": "Sahaj Agarwal"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ]
        },
        {
            "paperId": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
            "abstract": "Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2247370218",
                    "name": "Erik Jones"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2256999153",
                    "name": "Clarisse Simoes"
                },
                {
                    "authorId": "2247670741",
                    "name": "Varun Chandrasekaran"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "2256988075",
                    "name": "Arindam Mitra"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                },
                {
                    "authorId": "2247662058",
                    "name": "Ece Kamar"
                }
            ]
        },
        {
            "paperId": "0d943aa547690c40aff35b4e0b329bf04aedc59d",
            "title": "Diversity of Thought Improves Reasoning Abilities of LLMs",
            "abstract": "Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance. However, these methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we discuss how one can create and leverage variations of the input prompt as a means of diversity of thought. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that are apt for the problem. We then ensemble the diverse prompts in our method DIVSE (DIVerse reasoning path Self-Ensemble) across multiple inference calls, or use diverse approaches within a single inference call; we call the latter IDIV-SE (In-call DIVerse reasoning path Self-Ensemble). Apart from our approaches outperforming prior work, DIV-SE(in particular) advances state-of-the-art performance on the challenging planning and graph coloring benchmarks. Our results improve the Pareto frontier of the accuracy-cost trade-off.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205397794",
                    "name": "Ranjita Naik"
                },
                {
                    "authorId": "2247670741",
                    "name": "Varun Chandrasekaran"
                },
                {
                    "authorId": "2282542500",
                    "name": "Mert Yuksekgonul"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                }
            ]
        },
        {
            "paperId": "38a2c434ad10f26e2b1f970d944d862da74f9e71",
            "title": "Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning",
            "abstract": "Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the model's activation maps towards the actual class rather than the spurious attribute when present. In particular, on the Waterbirds dataset, our algorithm achieved a worst-group accuracy 23% higher than ERM on CLIP with a ResNet-50 backbone, and 32% higher on CLIP with a ViT backbone, while maintaining the same average accuracy as ERM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116468252",
                    "name": "Yu Yang"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2389094",
                    "name": "Baharan Mirzasoleiman"
                }
            ]
        },
        {
            "paperId": "5970fd7b57a0a9ad317a59498719bee747dd30b6",
            "title": "An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models",
            "abstract": "Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive human-written data which contains latent societal biases and toxic contents. In this paper, we leverage the primary task of PTLMs, i.e., language modeling, and propose a new metric to quantify manifested implicit representational harms in PTLMs towards 13 marginalized demographics. Using this metric, we conducted an empirical analysis of 24 widely used PTLMs. Our analysis provides insights into the correlation between the proposed metric in this work and other related metrics for representational harm. We observe that our metric correlates with most of the gender-specific metrics in the literature. Through extensive experiments, we explore the connections between PTLMs architectures and representational harms across two dimensions: depth and width of the networks. We found that prioritizing depth over width, mitigates representational harms in some PTLMs. Our code and data can be found at [place holder].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2195458",
                    "name": "Saghar Hosseini"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ]
        },
        {
            "paperId": "8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
            "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
            "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "121645690",
                    "name": "S\u00e9bastien Bubeck"
                },
                {
                    "authorId": "143754359",
                    "name": "Varun Chandrasekaran"
                },
                {
                    "authorId": "2315830",
                    "name": "Ronen Eldan"
                },
                {
                    "authorId": "120962807",
                    "name": "J. Gehrke"
                },
                {
                    "authorId": "2064595436",
                    "name": "Eric Horvitz"
                },
                {
                    "authorId": "1783184",
                    "name": "Ece Kamar"
                },
                {
                    "authorId": "2212084492",
                    "name": "Peter Lee"
                },
                {
                    "authorId": "2109308930",
                    "name": "Y. Lee"
                },
                {
                    "authorId": "152244300",
                    "name": "Yuan-Fang Li"
                },
                {
                    "authorId": "23451726",
                    "name": "Scott M. Lundberg"
                },
                {
                    "authorId": "40900039",
                    "name": "Harsha Nori"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "78846919",
                    "name": "Marco Tulio Ribeiro"
                },
                {
                    "authorId": "144884116",
                    "name": "Yi Zhang"
                }
            ]
        },
        {
            "paperId": "d2ea161e6b2114529d875d16cfaad4c824e17a8c",
            "title": "Orca 2: Teaching Small Language Models How to Reason",
            "abstract": "Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256988075",
                    "name": "Arindam Mitra"
                },
                {
                    "authorId": "1875906",
                    "name": "Luciano Del Corro"
                },
                {
                    "authorId": "2054111097",
                    "name": "Shweti Mahajan"
                },
                {
                    "authorId": "2267341665",
                    "name": "Andres Codas"
                },
                {
                    "authorId": "2256999153",
                    "name": "Clarisse Simoes"
                },
                {
                    "authorId": "2267374957",
                    "name": "Sahaj Agrawal"
                },
                {
                    "authorId": "2267429167",
                    "name": "Xuxi Chen"
                },
                {
                    "authorId": "2267341963",
                    "name": "Anastasia Razdaibiedina"
                },
                {
                    "authorId": "2247370218",
                    "name": "Erik Jones"
                },
                {
                    "authorId": "2267341270",
                    "name": "Kriti Aggarwal"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2267344201",
                    "name": "Guoqing Zheng"
                },
                {
                    "authorId": "41016119",
                    "name": "Corby Rosset"
                },
                {
                    "authorId": "7156764",
                    "name": "Hamed Khanpour"
                },
                {
                    "authorId": "113916198",
                    "name": "Ahmed Awadallah"
                }
            ]
        },
        {
            "paperId": "f6007270b8aba3a69eac7c98375c99b7ca26c9b0",
            "title": "Diversity of Thought Improves Reasoning Abilities of Large Language Models",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205397794",
                    "name": "Ranjita Naik"
                },
                {
                    "authorId": "2247670741",
                    "name": "Varun Chandrasekaran"
                },
                {
                    "authorId": "2186981598",
                    "name": "Mert Yuksekgonul"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                }
            ]
        }
    ]
}