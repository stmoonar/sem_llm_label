{
    "authorId": "2115437382",
    "papers": [
        {
            "paperId": "02fdffee1dcc17b69c10e18512b4916e8b79d42e",
            "title": "Towards Better Graph Representation Learning with Parameterized Decomposition & Filtering",
            "abstract": "Proposing an effective and flexible matrix to represent a graph is a fundamental challenge that has been explored from multiple perspectives, e.g., filtering in Graph Fourier Transforms. In this work, we develop a novel and general framework which unifies many existing GNN models from the view of parameterized decomposition and filtering, and show how it helps to enhance the flexibility of GNNs while alleviating the smoothness and amplification issues of existing models. Essentially, we show that the extensively studied spectral graph convolutions with learnable polynomial filters are constrained variants of this formulation, and releasing these constraints enables our model to express the desired decomposition and filtering simultaneously. Based on this generalized framework, we develop models that are simple in implementation but achieve significant improvements and computational efficiency on a variety of graph learning tasks. Code is available at https://github.com/qslim/PDF.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150426854",
                    "name": "Mingqi Yang"
                },
                {
                    "authorId": "145948356",
                    "name": "Wenjie Feng"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "2019961",
                    "name": "Bryan Hooi"
                }
            ]
        },
        {
            "paperId": "72d33fc03ab754bab15f3a43f8a3374890c848e0",
            "title": "Breaking the Expression Bottleneck of Graph Neural Networks",
            "abstract": "Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressiveness of graph neural networks (GNNs), showing that the neighborhood aggregation GNNs were at most as powerful as 1-WL test in distinguishing graph structures. There were also improvements proposed in analogy to <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"shen-ieq1-3168070.gif\"/></alternatives></inline-formula>-WL test (<inline-formula><tex-math notation=\"LaTeX\">$k>1$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>k</mml:mi><mml:mo>></mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"shen-ieq2-3168070.gif\"/></alternatives></inline-formula>). However, the aggregations in these GNNs are far from injective as required by the WL test, and suffer from weak distinguishing strength, making it become the expression bottleneck. In this paper, we improve the expressiveness by exploring powerful aggregations. We reformulate an aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements on this matrix for building more powerful and even injective aggregations. We also show the necessity of applying nonlinear units ahead of aggregations, which is different from most existing GNNs. Based on our theoretical analysis, we develop ExpandingConv. Experimental results show that our model significantly boosts performance, especially for large and densely connected graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150426854",
                    "name": "Mingqi Yang"
                },
                {
                    "authorId": "2163064417",
                    "name": "Renjian Wang"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "2072590343",
                    "name": "Heng Qi"
                },
                {
                    "authorId": "1714354",
                    "name": "Baocai Yin"
                }
            ]
        },
        {
            "paperId": "81fef5548a8a8c823201878ba3afb23a2d98fbb8",
            "title": "A Continuous Encoder-Decoder Method for Spatial-Temporal Forecasting",
            "abstract": "Spatial-temporal forecasting (e.g., traffic flow forecasting) plays a vital role in various applications and has important research significance. Most existing spatial-temporal forecasting methods improve the forecasting accuracy by stacking discrete modules. However, the discontinuous hidden state trajectories between discrete modules may lead to high numerical errors, parameter redundancy, and model complexity. Neural Controlled Differential Equations (NCDEs) can solve the above issues effectively, which have a mechanism that can continuously adjust the hidden state trajectories according to controlled signals. In this paper, we propose a novel Spatial-Temporal Continuous Encoder-Decoder (STCED) method for spatial-temporal forecasting. Specifically, we first propose an overall spatial-temporal continuous encoder-decoder architecture based on NCDEs, which can not only promote the spatial-temporal message passing, but also capture the periodicity without introducing multiple modules or extra parameters. Then, we refactor the core of NCDEs based on Fast Weight Programmers (FWPs) and Graph Neural Networks (GNNs) to overcome the limitations that previous NCDEs cannot capture long-range spatial-temporal dependencies. We conduct experiments on two representative spatial-temporal datasets, demonstrating the effectiveness and superiority of our proposed algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2213808446",
                    "name": "Lei Liu"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "2072590343",
                    "name": "Heng Qi"
                }
            ]
        },
        {
            "paperId": "f7e78f1a1b436d28bf8761380b91bff7d2f83c4a",
            "title": "To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion",
            "abstract": "Embedding models have shown great power in knowledge graph completion (KGC) task. By learning structural constraints for each training triple, these methods implicitly memorize intrinsic relation rules to infer missing links. However, this paper points out that the multi-hop relation rules are hard to be reliably memorized due to the inherent deficiencies of such implicit memorization strategy, making embedding models underperform in predicting links between distant entity pairs. To alleviate this problem, we present Vertical Learning Paradigm (VLP), which extends embedding models by allowing to explicitly copy target information from related factual triples for more accurate prediction. Rather than solely relying on the implicit memory, VLP directly provides additional cues to improve the generalization ability of embedding models, especially making the distant link prediction significantly easier. Moreover, we also propose a novel relative distance based negative sampling technique (ReD) for more effective optimization. Experiments demonstrate the validity and generality of our proposals on two standard benchmarks. Our code is available at https://github.com/rui9812/VLP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "2144230222",
                    "name": "Xu Chen"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "48018967",
                    "name": "Jianan Zhao"
                },
                {
                    "authorId": "46394401",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2114924471",
                    "name": "Weihao Han"
                },
                {
                    "authorId": "2118180377",
                    "name": "Hao Sun"
                },
                {
                    "authorId": "2066621592",
                    "name": "Weiwei Deng"
                },
                {
                    "authorId": "2145908764",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                }
            ]
        },
        {
            "paperId": "fdfa32206cef7ea5be913fadc704de06f1607260",
            "title": "Triplet-contrastive Periodical Siamese Graph Networks for Travel Time Estimation",
            "abstract": "Accurate travel time estimation (TTE) is a critical component of intelligent transportation systems (ITSs). However, due to the high variation and complexity of road networks, the majority of prior methods are inefficient to extracting both spatial and temporal dynamic correlations. Additionally, most of them often use simplistic graph representations to depict the traffic nodes' interconnections and separate temporal modules to capture time variations. Thus, taking into account the semantic relationships among traffic variables is a difficult task that plays a crucial role in gaining a comprehensive understanding of traffic networks' topology. To overcome these limitations, we propose a new deep learning framework called Triplet-Contrastive Periodical Siamese Graph networks (TCPSG). This framework incorporates a novel combination of Siamese graph networks and triplet contrastive learning, as well as considering periodicity patterns in traffic. Specifically, we construct semantical and periodical graph adjacency matrices that represent latent spatial correlations between nodes and periodicity dependencies of recent, daily, and weekly time-periods. Then, we devise a contrastive-based learning method that combines a triplet siamese graph structure with a dual-gated Temporal Convolutional Network (TCN) based module to simultaneously learn traffic patterns' similarities among those three periodic components and long-term dynamic temporal dependencies. Extensive experiments on real-world traffic datasets demonstrate that TCPSG consistently outperforms baselines in all prediction horizons.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140941414",
                    "name": "Alfateh M. Tag Elsir"
                },
                {
                    "authorId": "2140937107",
                    "name": "Alkilane Khaled"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                }
            ]
        },
        {
            "paperId": "63836e669416668744c3676a831060e8de3f58a1",
            "title": "HousE: Knowledge Graph Embedding with Householder Parameterization",
            "abstract": "The effectiveness of knowledge graph embedding (KGE) largely depends on the ability to model intrinsic relation patterns and mapping properties. However, existing approaches can only capture some of them with insufficient modeling capacity. In this work, we propose a more powerful KGE framework named HousE, which involves a novel parameterization based on two kinds of Householder transformations: (1) Householder rotations to achieve superior capacity of modeling relation patterns; (2) Householder projections to handle sophisticated relation mapping properties. Theoretically, HousE is capable of modeling crucial relation patterns and mapping properties simultaneously. Besides, HousE is a generalization of existing rotation-based models while extending the rotations to high-dimensional spaces. Empirically, HousE achieves new state-of-the-art performance on five benchmark datasets. Our code is available at https://github.com/anrep/HousE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "48018967",
                    "name": "Jianan Zhao"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2064929062",
                    "name": "Di He"
                },
                {
                    "authorId": "2108941389",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "2108104523",
                    "name": "Yuming Liu"
                },
                {
                    "authorId": "2118180377",
                    "name": "Hao Sun"
                },
                {
                    "authorId": "3210262",
                    "name": "Senzhang Wang"
                },
                {
                    "authorId": "2066621592",
                    "name": "Weiwei Deng"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2145908764",
                    "name": "Qi Zhang"
                }
            ]
        },
        {
            "paperId": "688465f03f131c258a9242240c95253d4ea22953",
            "title": "TCSA-Net: A Temporal-Context-Based Self-Attention Network for Next Location Prediction",
            "abstract": "Next location prediction aims to find the location that the user will visit next. It plays a fundamental role for location-based applications. However, the heterogeneity and sparsity of the trajectory data pose great challenges to the task. Recently, RNN-based methods have shown promising performance in learining the spatio-temporal characteristics of the trajectory. While the effectiveness of location prediction has been improved, the computational efficiency and the long-term preferences still leave space for further research. The self-attention mechanism is viewed as a promising solution for parallel computation and exploiting sequential regularities from sparse data. But the huge memory cost and the neglect of temporal information make it infeasible to directly modeling human mobility regularities. In this paper, we propose a temporal-context-based self-attention network named TCSA-Net, which can simultaneously exploit long- and short-term mvoement preferences from sparse and long trajectories. In particular, we design a novel two-stage self-attention architecture that can learn long-term dependency under constrained memory budget. Further, we propose a multi-modal embedding layer to model two complementary temporal contexts and provide more abundant temporal and sequential information. Extensive experiments on two real-life datasets show that the TCSA-Net significantly outperforms the state-of-the-art methods in terms of standard evaluation metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2169870851",
                    "name": "Guiming Sun"
                },
                {
                    "authorId": "2002488",
                    "name": "Heng Qi"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "1714354",
                    "name": "Baocai Yin"
                }
            ]
        },
        {
            "paperId": "dad4f99d0175fafec01e6a5d34bfe26aa6ccc38e",
            "title": "NodeTrans: A Graph Transfer Learning Approach for Traffic Prediction",
            "abstract": "\u2014Recently, deep learning methods have made great progress in traf\ufb01c prediction, but their performance depends on a large amount of historical data. In reality, we may face the data scarcity issue. In this case, deep learning models fail to obtain satisfactory performance. Transfer learning is a promising approach to solve the data scarcity issue. However, existing transfer learning approaches in traf\ufb01c prediction are mainly based on regular grid data, which is not suitable for the inherent graph data in the traf\ufb01c network. Moreover, existing graph- based models can only capture shared traf\ufb01c patterns in the road network, and how to learn node-speci\ufb01c patterns is also a challenge. In this paper, we propose a novel transfer learning approach to solve the traf\ufb01c prediction with few data, which can transfer the knowledge learned from a data-rich source domain to a data-scarce target domain. First, a spatial-temporal graph neural network is proposed, which can capture the node-speci\ufb01c spatial-temporal traf\ufb01c patterns of different road networks. Then, to improve the robustness of transfer, we design a pattern-based transfer strategy, where we leverage a clustering-based mechanism to distill common spatial-temporal patterns in the source domain, and use these knowledge to further improve the prediction performance of the target domain. Experiments on real-world datasets verify the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152544049",
                    "name": "Xueyan Yin"
                },
                {
                    "authorId": "46493888",
                    "name": "Fei Li"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "2072590761",
                    "name": "Heng Qi"
                },
                {
                    "authorId": "1714354",
                    "name": "Baocai Yin"
                }
            ]
        },
        {
            "paperId": "28da08d10d371914bf79aea0c846110de2294629",
            "title": "A New Perspective on the Effects of Spectrum in Graph Neural Networks",
            "abstract": "Many improvements on GNNs can be deemed as operations on the spectrum of the underlying graph matrix, which motivates us to directly study the characteristics of the spectrum and their effects on GNN performance. By generalizing most existing GNN architectures, we show that the correlation issue caused by the $unsmooth$ spectrum becomes the obstacle to leveraging more powerful graph filters as well as developing deep architectures, which therefore restricts GNNs' performance. Inspired by this, we propose the correlation-free architecture which naturally removes the correlation issue among different channels, making it possible to utilize more sophisticated filters within each channel. The final correlation-free architecture with more powerful filters consistently boosts the performance of learning graph representations. Code is available at https://github.com/qslim/gnn-spectrum.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150426854",
                    "name": "Mingqi Yang"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "2072590343",
                    "name": "Heng Qi"
                },
                {
                    "authorId": "1737486",
                    "name": "Qian Zhang"
                },
                {
                    "authorId": "1714354",
                    "name": "Baocai Yin"
                }
            ]
        },
        {
            "paperId": "47ae807cd511b35e78a2cd4e198283dea6dafd41",
            "title": "Do Transformers Really Perform Bad for Graph Representation?",
            "abstract": "The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051552141",
                    "name": "Chengxuan Ying"
                },
                {
                    "authorId": "123970124",
                    "name": "Tianle Cai"
                },
                {
                    "authorId": "2108801920",
                    "name": "Shengjie Luo"
                },
                {
                    "authorId": "150311931",
                    "name": "Shuxin Zheng"
                },
                {
                    "authorId": "35286545",
                    "name": "Guolin Ke"
                },
                {
                    "authorId": "1391126980",
                    "name": "Di He"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                }
            ]
        }
    ]
}