{
    "authorId": "36737249",
    "papers": [
        {
            "paperId": "8c68c6f45d4f8af0536dd7a401fe9333eda7e1be",
            "title": "Aligning Large Language Models by On-Policy Self-Judgment",
            "abstract": "Existing approaches for aligning large language models with human preferences face a trade-off that requires a separate reward model (RM) for on-policy learning. In this paper, we present a novel alignment framework, SELF-JUDGE that (1) does on-policy learning and 2) is parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model to act as both a policy and a judge. Specifically, we view the pairwise judgment task, choosing the better response from a response pair, as a special case of the instruction-following task. The resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines in preference benchmarks. We also show that the rejecting sampling by itself can improve performance further without an additional evaluator.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284690114",
                    "name": "Sangkyu Lee"
                },
                {
                    "authorId": "2829848",
                    "name": "Sungdong Kim"
                },
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2266468609",
                    "name": "Minjoon Seo"
                },
                {
                    "authorId": "31760501",
                    "name": "Kang Min Yoo"
                },
                {
                    "authorId": "2284703785",
                    "name": "Youngjae Yu"
                }
            ]
        },
        {
            "paperId": "a83af24920fac48e66e7cdbf5ed9fc77b19a210a",
            "title": "Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding",
            "abstract": "Visual arguments, often used in advertising or social causes, rely on images to persuade viewers to do or believe something. Understanding these arguments requires selective vision: only specific visual stimuli within an image are relevant to the argument, and relevance can only be understood within the context of a broader argumentative structure. While visual arguments are readily appreciated by human audiences, we ask: are today's AI capable of similar understanding? We present VisArgs, a dataset of 1,611 images annotated with 5,112 visual premises (with regions), 5,574 commonsense premises, and reasoning trees connecting them into structured arguments. We propose three tasks for evaluating visual argument understanding: premise localization, premise identification, and conclusion deduction. Experiments show that 1) machines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy, while humans reached 98.0%. Models also performed 19.5% worse when distinguishing between irrelevant objects within the image compared to external objects. 2) Providing relevant visual premises improved model performance significantly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2004821977",
                    "name": "Jiwan Chung"
                },
                {
                    "authorId": "2309177771",
                    "name": "Sungjae Lee"
                },
                {
                    "authorId": "2308561234",
                    "name": "Minseo Kim"
                },
                {
                    "authorId": "2423429",
                    "name": "Seungju Han"
                },
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2689239",
                    "name": "Jack Hessel"
                },
                {
                    "authorId": "2258802490",
                    "name": "Youngjae Yu"
                }
            ]
        },
        {
            "paperId": "8f5eee0ab94a613cb30722e6573585e44cdd5aee",
            "title": "Green Federated Learning",
            "abstract": "The rapid progress of AI is fueled by increasingly large and computationally intensive machine learning models and datasets. As a consequence, the amount of compute used in training state-of-the-art models is exponentially increasing (doubling every 10 months between 2015 and 2022), resulting in a large carbon footprint. Federated Learning (FL) - a collaborative machine learning technique for training a centralized model using data of decentralized entities - can also be resource-intensive and have a significant carbon footprint, particularly when deployed at scale. Unlike centralized AI that can reliably tap into renewables at strategically placed data centers, cross-device FL may leverage as many as hundreds of millions of globally distributed end-user devices with diverse energy sources. Green AI is a novel and important research area where carbon footprint is regarded as an evaluation criterion for AI, alongside accuracy, convergence speed, and other metrics. In this paper, we propose the concept of Green FL, which involves optimizing FL parameters and making design choices to minimize carbon emissions consistent with competitive performance and training time. The contributions of this work are two-fold. First, we adopt a data-driven approach to quantify the carbon emissions of FL by directly measuring real-world at-scale FL tasks running on millions of phones. Second, we present challenges, guidelines, and lessons learned from studying the trade-off between energy efficiency, performance, and time-to-train in a production FL system. Our findings offer valuable insights into how FL can reduce its carbon footprint, and they provide a foundation for future research in the area of Green AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2155249906",
                    "name": "Sheng Guo"
                },
                {
                    "authorId": "2052661789",
                    "name": "Ashish Shenoy"
                },
                {
                    "authorId": "2041115682",
                    "name": "Sayan Ghosh"
                },
                {
                    "authorId": "37502184",
                    "name": "Pierre Stock"
                },
                {
                    "authorId": "10995410",
                    "name": "Kiwan Maeng"
                },
                {
                    "authorId": "2212361386",
                    "name": "Schalk-Willem Kruger"
                },
                {
                    "authorId": "2066127975",
                    "name": "Michael G. Rabbat"
                },
                {
                    "authorId": "2797270",
                    "name": "Carole-Jean Wu"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                }
            ]
        },
        {
            "paperId": "42e685d9997e579746b4e2f39988e3ddb59899f3",
            "title": "Reconciling Security and Communication Efficiency in Federated Learning",
            "abstract": "Cross-device Federated Learning is an increasingly popular machine learning setting to train a model by leveraging a large population of client devices with high privacy and security guarantees. However, communication efficiency remains a major bottleneck when scaling federated learning to production environments, particularly due to bandwidth constraints during uplink communication. In this paper, we formalize and address the problem of compressing client-to-server model updates under the Secure Aggregation primitive, a core component of Federated Learning pipelines that allows the server to aggregate the client updates without accessing them individually. In particular, we adapt standard scalar quantization and pruning methods to Secure Aggregation and propose Secure Indexing, a variant of Secure Aggregation that supports quantization for extreme compression. We establish state-of-the-art results on LEAF benchmarks in a secure Federated Learning setup with up to 40$\\times$ compression in uplink communication with no meaningful loss in utility compared to uncompressed baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "2041115682",
                    "name": "Sayan Ghosh"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "37502184",
                    "name": "Pierre Stock"
                }
            ]
        },
        {
            "paperId": "38fca90fef15b226dbae8db408c5efe8066dde1c",
            "title": "Federated Learning with Buffered Asynchronous Aggregation",
            "abstract": "Scalability and privacy are two critical concerns for cross-device federated learning (FL) systems. In this work, we identify that synchronous FL - synchronized aggregation of client updates in FL - cannot scale efficiently beyond a few hundred clients training in parallel. It leads to diminishing returns in model performance and training speed, analogous to large-batch training. On the other hand, asynchronous aggregation of client updates in FL (i.e., asynchronous FL) alleviates the scalability issue. However, aggregating individual client updates is incompatible with Secure Aggregation, which could result in an undesirable level of privacy for the system. To address these concerns, we propose a novel buffered asynchronous aggregation method, FedBuff, that is agnostic to the choice of optimizer, and combines the best properties of synchronous and asynchronous FL. We empirically demonstrate that FedBuff is 3.3x more efficient than synchronous FL and up to 2.5x more efficient than asynchronous FL, while being compatible with privacy-preserving technologies such as Secure Aggregation and differential privacy. We provide theoretical convergence guarantees in a smooth non-convex setting. Finally, we show that under differentially private training, FedBuff can outperform FedAvgM at low privacy settings and achieve the same utility for higher privacy settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2131399147",
                    "name": "John Nguyen"
                },
                {
                    "authorId": "40243893",
                    "name": "Kshitiz Malik"
                },
                {
                    "authorId": "19277351",
                    "name": "Hongyuan Zhan"
                },
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2066127975",
                    "name": "Michael G. Rabbat"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "68973739",
                    "name": "Dzmitry Huba"
                }
            ]
        },
        {
            "paperId": "b87b5797b661d6137301dd3167d573a94493936e",
            "title": "Papaya: Practical, Private, and Scalable Federated Learning",
            "abstract": "Cross-device Federated Learning (FL) is a distributed learning paradigm with several challenges that differentiate it from traditional distributed learning, variability in the system characteristics on each device, and millions of clients coordinating with a central server being primary ones. Most FL systems described in the literature are synchronous - they perform a synchronized aggregation of model updates from individual clients. Scaling synchronous FL is challenging since increasing the number of clients training in parallel leads to diminishing returns in training speed, analogous to large-batch training. Moreover, stragglers hinder synchronous FL training. In this work, we outline a production asynchronous FL system design. Our work tackles the aforementioned issues, sketches of some of the system design challenges and their solutions, and touches upon principles that emerged from building a production FL system for millions of clients. Empirically, we demonstrate that asynchronous FL converges faster than synchronous FL when training across nearly one hundred million devices. In particular, in high concurrency settings, asynchronous FL is 5x faster and has nearly 8x less communication overhead than synchronous FL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "68973739",
                    "name": "Dzmitry Huba"
                },
                {
                    "authorId": "2131399147",
                    "name": "John Nguyen"
                },
                {
                    "authorId": "40243893",
                    "name": "Kshitiz Malik"
                },
                {
                    "authorId": "3252651",
                    "name": "Ruiyu Zhu"
                },
                {
                    "authorId": "2066127975",
                    "name": "Michael G. Rabbat"
                },
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2797270",
                    "name": "Carole-Jean Wu"
                },
                {
                    "authorId": "19277351",
                    "name": "Hongyuan Zhan"
                },
                {
                    "authorId": "2139735162",
                    "name": "Pavel Ustinov"
                },
                {
                    "authorId": "47490833",
                    "name": "H. Srinivas"
                },
                {
                    "authorId": "2150030991",
                    "name": "Kaikai Wang"
                },
                {
                    "authorId": "2139734446",
                    "name": "Anthony Shoumikhin"
                },
                {
                    "authorId": "2146537748",
                    "name": "Jesik Min"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                }
            ]
        },
        {
            "paperId": "bea1187a1f8a68f1a93f0c2fa10d31f93a30f84e",
            "title": "Opacus: User-Friendly Differential Privacy Library in PyTorch",
            "abstract": "We introduce Opacus, a free, open-source PyTorch library for training deep learning models with differential privacy (hosted at opacus.ai). Opacus is designed for simplicity, flexibility, and speed. It provides a simple and user-friendly API, and enables machine learning practitioners to make a training pipeline private by adding as little as two lines to their code. It supports a wide variety of layers, including multi-head attention, convolution, LSTM, GRU (and generic RNN), and embedding, right out of the box and provides the means for supporting other user-defined layers. Opacus computes batched per-sample gradients, providing higher efficiency compared to the traditional\"micro batch\"approach. In this paper we present Opacus, detail the principles that drove its implementation and unique features, and benchmark it against other frameworks for training models with differential privacy as well as standard PyTorch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2107059646",
                    "name": "I. Shilov"
                },
                {
                    "authorId": "2319225824",
                    "name": "Alexandre Sablayrolles"
                },
                {
                    "authorId": "1389630028",
                    "name": "Davide Testuggine"
                },
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "2131399147",
                    "name": "John Nguyen"
                },
                {
                    "authorId": "2129469303",
                    "name": "Sayan Gosh"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "2024689450",
                    "name": "Jessica Zhao"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                }
            ]
        },
        {
            "paperId": "d0270d8193510ae4f9df241c4e074183687892b0",
            "title": "ResiliNet: Failure-Resilient Inference in Distributed Neural Networks",
            "abstract": "Federated Learning aims to train distributed deep models without sharing the raw data with the centralized server. Similarly, in Split Learning, by partitioning a neural network and distributing it across several physical nodes, activations and gradients are exchanged between physical nodes, rather than raw data. Nevertheless, when a neural network is partitioned and distributed among physical nodes, failure of physical nodes causes the failure of the neural units that are placed on those nodes, which results in a significant performance drop. Current approaches focus on resiliency of training in distributed neural networks. However, resiliency of inference in distributed neural networks is less explored. We introduce ResiliNet, a scheme for making inference in distributed neural networks resilient to physical node failures. ResiliNet combines two concepts to provide resiliency: skip hyperconnection, a concept for skipping nodes in distributed neural networks similar to skip connection in resnets, and a novel technique called failout, which is introduced in this paper. Failout simulates physical node failure conditions during training using dropout, and is specifically designed to improve the resiliency of distributed neural networks. The results of the experiments and ablation studies using three datasets confirm the ability of ResiliNet to provide inference resiliency for distributed neural networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "153154714",
                    "name": "Brian Q. Nguyen"
                },
                {
                    "authorId": "117403158",
                    "name": "Siddartha Devic"
                },
                {
                    "authorId": "48738250",
                    "name": "Guanhua Wang"
                },
                {
                    "authorId": "27548550",
                    "name": "Aboudy Kreidieh"
                },
                {
                    "authorId": "46681782",
                    "name": "Hans Lobel"
                },
                {
                    "authorId": "1705102",
                    "name": "A. Bayen"
                },
                {
                    "authorId": "1741553",
                    "name": "J. Jue"
                }
            ]
        },
        {
            "paperId": "fc12af545f543d3c203d4ec09f27c61df10786d1",
            "title": "Failout: Achieving Failure-Resilient Inference in Distributed Neural Networks",
            "abstract": "When a neural network is partitioned and distributed across physical nodes, failure of physical nodes causes the failure of the neural units that are placed on those nodes, which results in a significant performance drop. Current approaches focus on resiliency of training in distributed neural networks. However, resiliency of inference in distributed neural networks is less explored. We introduce ResiliNet, a scheme for making inference in distributed neural networks resilient to physical node failures. ResiliNet combines two concepts to provide resiliency: skip connection in residual neural networks, and a novel technique called failout, which is introduced in this paper. Failout simulates physical node failure conditions during training using dropout, and is specifically designed to improve the resiliency of distributed neural networks. The results of the experiments and ablation studies using three datasets confirm the ability of ResiliNet to provide inference resiliency for distributed neural networks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "153154714",
                    "name": "Brian Q. Nguyen"
                },
                {
                    "authorId": "117403158",
                    "name": "Siddartha Devic"
                },
                {
                    "authorId": "48738250",
                    "name": "Guanhua Wang"
                },
                {
                    "authorId": "27548550",
                    "name": "Aboudy Kreidieh"
                },
                {
                    "authorId": "46681782",
                    "name": "Hans Lobel"
                },
                {
                    "authorId": "1705102",
                    "name": "A. Bayen"
                },
                {
                    "authorId": "1741553",
                    "name": "J. Jue"
                }
            ]
        },
        {
            "paperId": "6543df620844dd683b7251990312d14e6c137837",
            "title": "Guardians of the Deep Fog: Failure-Resilient DNN Inference from Edge to Cloud",
            "abstract": "Partitioning and distributing deep neural networks (DNNs) over physical nodes such as edge, fog, or cloud nodes, could enhance sensor fusion, and reduce bandwidth and inference latency. However, when a DNN is distributed over physical nodes, failure of the physical nodes causes the failure of the DNN units that are placed on these nodes. The performance of the inference task will be unpredictable, and most likely, poor, if the distributed DNN is not specifically designed and properly trained for failures. Motivated by this, we introduce deepFogGuard, a DNN architecture augmentation scheme for making the distributed DNN inference task failure-resilient. To articulate deepFogGuard, we introduce the elements and a model for the resiliency of distributed DNN inference. Inspired by the concept of residual connections in DNNs, we introduce skip hyperconnections in distributed DNNs, which are the basis of deepFogGuard's design to provide resiliency. Next, our extensive experiments using two existing datasets for the sensing and vision applications confirm the ability of deepFogGuard to provide resiliency for distributed DNNs in edge-cloud networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "117403158",
                    "name": "Siddartha Devic"
                },
                {
                    "authorId": "153154714",
                    "name": "Brian Q. Nguyen"
                },
                {
                    "authorId": "27548550",
                    "name": "Aboudy Kreidieh"
                },
                {
                    "authorId": "2070872379",
                    "name": "Alan Liao"
                },
                {
                    "authorId": "1705102",
                    "name": "A. Bayen"
                },
                {
                    "authorId": "1741553",
                    "name": "J. Jue"
                }
            ]
        }
    ]
}