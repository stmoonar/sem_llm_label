{
    "authorId": "103010588",
    "papers": [
        {
            "paperId": "a0173909e4515d6bd8de58752f17a457ba003667",
            "title": "Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models",
            "abstract": "Large-scale pre-trained models (PTMs) such as BERT and GPT have achieved great success in diverse fields. The typical paradigm is to pre-train a big deep learning model on large-scale data sets, and then fine-tune the model on small task-specific data sets for downstream tasks. Although PTMs have rapidly progressed with wide real-world applications, they also pose significant risks of potential attacks. Existing backdoor attacks or data poisoning methods often build up the assumption that the attacker invades the computers of victims or accesses the target data, which is challenging in real-world scenarios. In this paper, we propose a novel framework for an invisible attack on PTMs with enhanced MD5 collision. The key idea is to generate two equal-size models with the same MD5 checksum by leveraging the MD5 chosen-prefix collision. Afterwards, the two ``same\"models will be deployed on public websites to induce victims to download the poisoned model. Unlike conventional attacks on deep learning models, this new attack is flexible, covert, and model-independent. Additionally, we propose a simple defensive strategy for recognizing the MD5 chosen-prefix collision and provide a theoretical justification for its feasibility. We extensively validate the effectiveness and stealthiness of our proposed attack and defensive method on different models and data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152100364",
                    "name": "Minghan Deng"
                },
                {
                    "authorId": "103010588",
                    "name": "Zhong Zhang"
                },
                {
                    "authorId": "2111876441",
                    "name": "Junming Shao"
                }
            ]
        },
        {
            "paperId": "bf8c7ca2c4f66607b7d99e3598a6b03397af764b",
            "title": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models",
            "abstract": "Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "103010588",
                    "name": "Zhong Zhang"
                },
                {
                    "authorId": "2116441692",
                    "name": "Bang Liu"
                },
                {
                    "authorId": "2111876441",
                    "name": "Junming Shao"
                }
            ]
        },
        {
            "paperId": "0001bd26bba3e950c5846b6080815f4549705115",
            "title": "Community Detection and Link Prediction via Cluster-driven Low-rank Matrix Completion",
            "abstract": "Community detection and link prediction are highly dependent since knowing cluster structure as a priori will help identify missing links, and in return, clustering on networks with supplemented missing links will improve community detection performance. In this paper, we propose a Cluster-driven Low-rank Matrix Completion (CLMC), for performing community detection and link prediction simultaneously in a unified framework. To this end, CLMC decomposes the adjacent matrix of a target network as three additive matrices: clustering matrix, noise matrix and supplement matrix. The community-structure and low-rank constraints are imposed on the clustering matrix, such that the noisy edges between communities are removed and the resulting matrix is an ideal block-diagonal matrix. Missing edges are further learned via low-rank matrix completion. Extensive experiments show that CLMC achieves state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40161233",
                    "name": "Junming Shao"
                },
                {
                    "authorId": "103010588",
                    "name": "Zhong Zhang"
                },
                {
                    "authorId": "32206063",
                    "name": "Zhongjing Yu"
                },
                {
                    "authorId": "2153062990",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2143936507",
                    "name": "Yi Zhao"
                },
                {
                    "authorId": "1839095",
                    "name": "Qinli Yang"
                }
            ]
        }
    ]
}