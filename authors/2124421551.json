{
    "authorId": "2124421551",
    "papers": [
        {
            "paperId": "0c6a9ddaa0fef4138a03625beb927faec3d4760d",
            "title": "Multi-Modal Attribute Extraction for E-Commerce",
            "abstract": "To improve users' experience as they navigate the myriad of options offered by online marketplaces, it is essential to have well-organized product catalogs. One key ingredient to that is the availability of product attributes such as color or material. However, on some marketplaces such as Rakuten-Ichiba, which we focus on, attribute information is often incomplete or even missing. One promising solution to this problem is to rely on deep models pre-trained on large corpora to predict attributes from unstructured data, such as product descriptive texts and images (referred to as modalities in this paper). However, we find that achieving satisfactory performance with this approach is not straightforward but rather the result of several refinements, which we discuss in this paper. We provide a detailed description of our approach to attribute extraction, from investigating strong single-modality methods, to building a solid multimodal model combining textual and visual information. One key component of our multimodal architecture is a novel approach to seamlessly combine modalities, which is inspired by our single-modality investigations. In practice, we notice that this new modality-merging method may suffer from a modality collapse issue, i.e., it neglects one modality. Hence, we further propose a mitigation to this problem based on a principled regularization scheme. Experiments on Rakuten-Ichiba data provide empirical evidence for the benefits of our approach, which has been also successfully deployed to Rakuten-Ichiba. We also report results on publicly available datasets showing that our model is competitive compared to several recent multimodal and unimodal baselines.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2124421551",
                    "name": "Alo\u00efs de La Comble"
                },
                {
                    "authorId": "3294830",
                    "name": "Anuvabh Dutt"
                },
                {
                    "authorId": "2057535114",
                    "name": "Pablo Montalvo"
                },
                {
                    "authorId": "2911888",
                    "name": "Aghiles Salah"
                }
            ]
        },
        {
            "paperId": "103109fb901f8a5c9053bcdbd24009ec3ab211e6",
            "title": "Efficient transfer learning for multi-channel convolutional neural networks",
            "abstract": "Although most convolutional neural networks architectures for computer vision are built to process RGB images, more and more applications complete this information with additional input channels coming from different sensors and data sources. The current techniques for training models on such data, generally leveraging transfer learning, do not take into account the imbalance between RGB channels and additional channels. If no specific strategy is adopted, additional channels are underfitted. We propose to apply channel-wise dropout to inputs to reduce channel underfitting and improve performances. This improvement of performances may be linked to how much new information is brought by additional channels. We propose a method to evaluate this complementarity between additional and RGB channels. We test our approach on three different datasets: a multispectral dataset, a multi-channel PDF dataset and an RGB-D dataset. We find out that results are conclusive on the first two while there is no significant improvement on the last one. In all cases, we observe that additional channels underfitting decreases. We show that this difference of efficiency is linked to complementary between RGB and additional channels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2124421551",
                    "name": "Alo\u00efs de La Comble"
                },
                {
                    "authorId": "2181003",
                    "name": "K. Prepin"
                }
            ]
        }
    ]
}