{
    "authorId": "31017418",
    "papers": [
        {
            "paperId": "21b4777948797377deedf4a9f1f58ad13f6b8b5d",
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2887412",
                    "name": "Jinchao Li"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2066415714",
                    "name": "Zhengcong Fei"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2257374643",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2116575668",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "1979505",
                    "name": "A. Geramifard"
                },
                {
                    "authorId": "1765212",
                    "name": "Chiori Hori"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "2729d18c3906477489f2ddb9c7dd92c1d77b534d",
            "title": "Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments",
            "abstract": "Grounding navigational commands to linear temporal logic (LTL) leverages its unambiguous semantics for reasoning about long-horizon tasks and verifying the satisfaction of temporal constraints. Existing approaches require training data from the specific environment and landmarks that will be used in natural language to understand commands in those environments. We propose Lang2LTL, a modular system and a software package that leverages large language models (LLMs) to ground temporal navigational commands to LTL specifications in environments without prior language data. We comprehensively evaluate Lang2LTL for five well-defined generalization behaviors. Lang2LTL demonstrates the state-of-the-art ability of a single model to ground navigational commands to diverse temporal specifications in 21 city-scaled environments. Finally, we demonstrate a physical robot using Lang2LTL can follow 52 semantically diverse navigational commands in two indoor environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119081469",
                    "name": "Jason Liu"
                },
                {
                    "authorId": "2243392790",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "1587802238",
                    "name": "Ifrah Idrees"
                },
                {
                    "authorId": "2209471047",
                    "name": "Sam Liang"
                },
                {
                    "authorId": "114925292",
                    "name": "Benjamin Schornstein"
                },
                {
                    "authorId": "2913681",
                    "name": "Stefanie Tellex"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                }
            ]
        },
        {
            "paperId": "4628f0c28a8ed231168d1a27a93ddb938da4102d",
            "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
            "abstract": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were used to explain of perceptual quality and intelligibility. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2174668832",
                    "name": "Joseph Konan"
                },
                {
                    "authorId": "1649727619",
                    "name": "Ojas Bhargave"
                },
                {
                    "authorId": "2202224349",
                    "name": "Shikhar Agnihotri"
                },
                {
                    "authorId": "2206298901",
                    "name": "Shuo Han"
                },
                {
                    "authorId": "2111187803",
                    "name": "YUNYANG ZENG"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2240348802",
                    "name": "Bhiksha Raj"
                }
            ]
        },
        {
            "paperId": "593a603354c09d151440ae044de1d80324a2ab01",
            "title": "An Approach to Ontological Learning from Weak Labels",
            "abstract": "Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the \"Is A\" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. We also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best TNN-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline TNN but about the same as our models that do not use ontology information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2157796077",
                    "name": "Larry Tang"
                },
                {
                    "authorId": "2216451398",
                    "name": "Po Hao Chou"
                },
                {
                    "authorId": "2158586492",
                    "name": "Yilun Zheng"
                },
                {
                    "authorId": "2157426412",
                    "name": "Ziqian Ge"
                },
                {
                    "authorId": "1681921",
                    "name": "B. Raj"
                }
            ]
        },
        {
            "paperId": "6e3560353f5a6eded9a21fbc6d37b14675468984",
            "title": "Conformers are All You Need for Visual Speech Recogntion",
            "abstract": "Visual speech recognition models extract visual features in a hierarchical manner. At the lower level, there is a visual front-end with a limited temporal receptive field that processes the raw pixels depicting the lips or faces. At the higher level, there is an encoder that attends to the embeddings produced by the front-end over a large temporal receptive field. Previous work has focused on improving the visual front-end of the model to extract more useful features for speech recognition. Surprisingly, our work shows that complex visual front-ends are not necessary. Instead of allocating resources to a sophisticated visual front-end, we find that a linear visual front-end paired with a larger Conformer encoder results in lower latency, more efficient memory usage, and improved WER performance. We achieve a new state-of-the-art of 12.8% WER for visual speech recognition on the TED LRS3 dataset, which rivals the performance of audio-only models from just four years ago.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2134532944",
                    "name": "Oscar Chang"
                },
                {
                    "authorId": "39977619",
                    "name": "H. Liao"
                },
                {
                    "authorId": "1862138",
                    "name": "Dmitriy Serdyuk"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2523830",
                    "name": "Olivier Siohan"
                }
            ]
        },
        {
            "paperId": "7333be530df311b3148e9857ce9f481975cf0a9b",
            "title": "Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms",
            "abstract": "In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assistants, and telecommunication.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2174668832",
                    "name": "Joseph Konan"
                },
                {
                    "authorId": "1649727619",
                    "name": "Ojas Bhargave"
                },
                {
                    "authorId": "2202224349",
                    "name": "Shikhar Agnihotri"
                },
                {
                    "authorId": "2110019379",
                    "name": "Hojeong Lee"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2206298901",
                    "name": "Shuo Han"
                },
                {
                    "authorId": "2111187803",
                    "name": "YUNYANG ZENG"
                },
                {
                    "authorId": "2202226132",
                    "name": "Amanda Shu"
                },
                {
                    "authorId": "2143857486",
                    "name": "Haohui Liu"
                },
                {
                    "authorId": "8776560",
                    "name": "Xuankai Chang"
                },
                {
                    "authorId": "1500657253",
                    "name": "Hamza Khalid"
                },
                {
                    "authorId": "2031482648",
                    "name": "Minseon Gwak"
                },
                {
                    "authorId": "2204265592",
                    "name": "Kawon Lee"
                },
                {
                    "authorId": "2145949834",
                    "name": "Minjeong Kim"
                },
                {
                    "authorId": "1681921",
                    "name": "B. Raj"
                }
            ]
        },
        {
            "paperId": "740488982dee323d559f2dae70b1f4b3aa5f7171",
            "title": "Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms",
            "abstract": "General-purpose embedding is highly desirable for few-shot even zero-shot learning in many application scenarios, including audio tasks. In order to understand representations better, we conducted a thorough error analysis and visualization of HEAR 2021 submission results. Inspired by the analysis, this work experiments with different front-end audio preprocessing methods, including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT), and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover a more holistic simulation of the frequency information received by the human auditory system. We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks. Preliminary results show (1) the proposed BECR can incur a more dispersed embedding on the test set, (2) BECR improves the PaSST model without extra computation complexity, and (3) STFT preprocessing outperforms CQT in all tasks we tested. Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2108757768",
                    "name": "Shuyi Chen"
                },
                {
                    "authorId": "2210856438",
                    "name": "Kejun Zhou"
                },
                {
                    "authorId": "1681412",
                    "name": "Yue Chen"
                },
                {
                    "authorId": "1681921",
                    "name": "B. Raj"
                }
            ]
        },
        {
            "paperId": "b7e2074934985b6112b6bce8c3680b14e621fdfe",
            "title": "Importance of Negative Sampling in Weak Label Learning",
            "abstract": "Weak-label learning is a challenging task that requires learning from data \"bags\" containing positive and negative instances, but only the bag labels are known. The pool of negative instances is usually larger than positive instances, thus making selecting the most informative negative instance critical for performance. Such a selection strategy for negative instances from each bag is an open problem that has not been well studied for weak-label learning. In this paper, we study several sampling strategies that can measure the usefulness of negative instances for weak-label learning and select them accordingly. We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "48099799",
                    "name": "Fuyu Tang"
                },
                {
                    "authorId": "2246113411",
                    "name": "Zelin Ye"
                },
                {
                    "authorId": "2240446387",
                    "name": "Rita Singh"
                },
                {
                    "authorId": "2240348802",
                    "name": "Bhiksha Raj"
                }
            ]
        },
        {
            "paperId": "ee2e0077ec46704f2cb930958c9bf3739a904227",
            "title": "Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents",
            "abstract": "Recent advancements in large language models (LLMs) have enabled a new research domain, LLM agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of LLMs obtained during pretraining. However, while considerable effort has been made to teach the robot the \"dos\", the \"don\u2019ts\" received relatively less attention. We argue that, for any practical usage, it is as crucial to teach the robot the \"don\u2019ts\": conveying explicit instructions about prohibited actions, assessing the robot\u2019s comprehension of these restrictions, and, most importantly, ensuring compliance. Moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as ISO 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. Aiming at deploying the LLM agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (LTL) that simultaneously enables natural language (NL) to temporal constraints encoding, safety violation reasoning and explaining, and unsafe action pruning. To demonstrate the effectiveness of our system, we conducted experiments in VirtualHome environment and on a real robot. The experimental results show that our system strictly adheres to the safety constraints and scales well with complex safety constraints, highlighting its potential for practical utility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2243392790",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "2242964116",
                    "name": "S. S. Raman"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2913681",
                    "name": "Stefanie Tellex"
                }
            ]
        },
        {
            "paperId": "212d804e12100e93d986d978ef6d70b3f47f7a4a",
            "title": "Automated Audio Captioning and Language-Based Audio Retrieval",
            "abstract": "This project involved participation in the DCASE 2022 Competition (Task 6) which had two subtasks: (1) Automated Audio Captioning and (2) Language-Based Audio Retrieval. The first subtask involved the generation of a textual description for audio samples, while the goal of the second was to find audio samples within a fixed dataset that match a given description. For both subtasks, the Clotho dataset was used. The models were evaluated on BLEU1, BLEU2, BLEU3, ROUGEL, METEOR, CIDEr, SPICE, and SPIDEr scores for audio captioning and R1, R5, R10 and mARP10 scores for audio retrieval. We have conducted a handful of experiments that modify the baseline models for these tasks. Our final architecture for Automated Audio Captioning is close to the baseline performance, while our model for Language-Based Audio Retrieval has surpassed its counterpart.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2175651665",
                    "name": "Clive Gomes"
                },
                {
                    "authorId": "93544325",
                    "name": "Hyejin Park"
                },
                {
                    "authorId": "2175649876",
                    "name": "Patrick Kollman"
                },
                {
                    "authorId": "2115712433",
                    "name": "Yi-Zhe Song"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                }
            ]
        }
    ]
}