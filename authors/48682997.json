{
    "authorId": "48682997",
    "papers": [
        {
            "paperId": "43bafa19f94a42caea89b32a86489aa850317617",
            "title": "FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis",
            "abstract": "Diffusion models have transformed the image-to-image (I2I) synthesis and are now permeating into videos. How-ever, the advancement of video-to-video (V2V) synthesis has been hampered by the challenge of maintaining temporal consistency across video frames. This paper proposes a consistent V2V synthesis framework by jointly leveraging spatial conditions and temporal optical flow clues within the source video. Contrary to prior methods that strictly adhere to optical flow, our approach harnesses its benefits while handling the imperfection in flow estimation. We encode the optical flow via warping from the first frame and serve it as a supplementary reference in the diffusion model. This enables our model for video synthesis by editing the first frame with any prevalent I2I models and then propagating edits to successive frames. Our V2V model, Flow Vid, demon-strates remarkable properties: (1) Flexibility: Flow Vid works seamlessly with existing I2I models, facilitating various modifications, including stylization, object swaps, and local edits. (2) Efficiency: Generation of a 4-second video with 30 FPS and 512\u00d7512 resolution takes only 1.5 minutes, which is 3.1\u00d7, 7.2\u00d7, and 10.5\u00d7 faster than CoDeF, Rerender, and TokenFlow, respectively. (3) High-quality: In user studies, our FlowVid is preferred 45.7% of the time, outperforming CoDeF (3.5%), Rerender (10.2%), and TokenFlow (40.4%).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275604366",
                    "name": "Feng Liang"
                },
                {
                    "authorId": "2269776842",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "2247901055",
                    "name": "Jialiang Wang"
                },
                {
                    "authorId": "2269696579",
                    "name": "Licheng Yu"
                },
                {
                    "authorId": "2256701243",
                    "name": "Kunpeng Li"
                },
                {
                    "authorId": "2247515549",
                    "name": "Yinan Zhao"
                },
                {
                    "authorId": "2267241285",
                    "name": "Ishan Misra"
                },
                {
                    "authorId": "2238908906",
                    "name": "Jia-Bin Huang"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "92419662",
                    "name": "D. Marculescu"
                }
            ]
        },
        {
            "paperId": "4daaecd6230523ff346cacbea102dfde922bab0d",
            "title": "NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection",
            "abstract": "We present NeRF-Det, a novel method for indoor 3D detection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle to model scene geometry, our method makes novel use of NeRF in an end-to-end manner to explicitly estimate 3D geometry, thereby improving 3D detection performance. Specifically, to avoid the significant extra latency associated with per-scene optimization of NeRF, we introduce sufficient geometry priors to enhance the generalizability of NeRF-MLP. Furthermore, we subtly connect the detection and NeRF branches through a shared MLP, enabling an efficient adaptation of NeRF to detection and yielding geometry-aware volumetric representations for 3D detection. Our method outperforms state-of-the-arts by 3.9 mAP and 3.1 mAP on the ScanNet and ARKITScenes benchmarks, respectively. We provide extensive analysis to shed light on how NeRF-Det works. As a result of our joint-training design, NeRF-Det is able to generalize well to unseen scenes for object detection, view synthesis, and depth estimation tasks without requiring per-scene optimization. Code is available at https://github.com/facebookresearch/NeRF-Det.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1490695028",
                    "name": "Chenfeng Xu"
                },
                {
                    "authorId": "3130257",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "144129263",
                    "name": "Ji Hou"
                },
                {
                    "authorId": "2225238191",
                    "name": "Sam S. Tsai"
                },
                {
                    "authorId": "51104559",
                    "name": "Ruilong Li"
                },
                {
                    "authorId": "46584351",
                    "name": "Jialiang Wang"
                },
                {
                    "authorId": "144267500",
                    "name": "Wei Zhan"
                },
                {
                    "authorId": "2558787",
                    "name": "Zijian He"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "1732330",
                    "name": "K. Keutzer"
                },
                {
                    "authorId": "1680165",
                    "name": "M. Tomizuka"
                }
            ]
        },
        {
            "paperId": "4f4991f93ed86b777c8b0f192dac034a3144b165",
            "title": "Pruning Compact ConvNets for Efficient Inference",
            "abstract": "Neural network pruning is frequently used to compress over-parameterized networks by large amounts, while incurring only marginal drops in generalization performance. However, the impact of pruning on networks that have been highly optimized for efficient inference has not received the same level of attention. In this paper, we analyze the effect of pruning for computer vision, and study state-of-the-art ConvNets, such as the FBNetV3 family of models. We show that model pruning approaches can be used to further optimize networks trained through NAS (Neural Architecture Search). The resulting family of pruned models can consistently obtain better performance than existing FBNetV3 models at the same level of computation, and thus provide state-of-the-art results when trading off between computational complexity and generalization performance on the ImageNet benchmark. In addition to better generalization performance, we also demonstrate that when limited computation resources are available, pruning FBNetV3 models incur only a fraction of GPU-hours involved in running a full-scale NAS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143032877",
                    "name": "Sayan Ghosh"
                },
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "3130257",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                }
            ]
        },
        {
            "paperId": "5924535eaad89fbeb96ade32db50950476ea6e66",
            "title": "MixRT: Mixed Neural Representations For Real-Time NeRF Rendering",
            "abstract": "Neural Radiance Field (NeRF) has emerged as a leading technique for novel view synthesis, owing to its impressive photorealistic reconstruction and rendering capability. Nevertheless, achieving real-time NeRF rendering in large-scale scenes has presented challenges, often leading to the adoption of either intricate baked mesh representations with a substantial number of triangles or resource-intensive ray marching in baked representations. We challenge these conventions, observing that high-quality geometry, represented by meshes with substantial triangles, is not necessary for achieving photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF representation that includes a low-quality mesh, a view-dependent displacement map, and a compressed NeRF model. This design effectively harnesses the capabilities of existing graphics hardware, thus enabling real-time NeRF rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering framework, our proposed MixRT attains real-time rendering speeds on edge devices (over 30 FPS at a resolution of $1280 \\times 720$ on a MacBook M1 Pro laptop), better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360 datasets), and a smaller storage size (less than 80% compared to state-of-the-art methods).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28987646",
                    "name": "Chaojian Li"
                },
                {
                    "authorId": "2269776842",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "2198041939",
                    "name": "Yingyan Lin"
                }
            ]
        },
        {
            "paperId": "d37533f318857a480b52c258ae4581d84f5acffa",
            "title": "AVID: Any-Length Video Inpainting with Diffusion Model",
            "abstract": "Recent advances in diffusion models have successfully enabled text-guided image inpainting. While it seems straightforward to extend such editing capability into the video domain, there have been fewer works regarding textguided video inpainting. Given a video, a masked region at its initial frame, and an editing prompt, it requires a model to do infilling at each frame following the editing guidance while keeping the out-of-mask region intact. There are three main challenges in text-guided video inpainting: (i) temporal consistency of the edited video, (ii) supporting different inpainting types at different structural fidelity levels, and (iii) dealing with variable video length. To address these challenges, we introduce Any-Length Video Inpainting with Diffusion Model, dubbed as AVID. At its core, our model is equipped with effective motion modules and adjustable structure guidance, for fixed-length video inpainting. Building on top of that, we propose a novel Temporal MultiDiffusion sampling pipeline with a middle-frame attention guidance mechanism, facilitating the generation of videos with any desired duration. Our comprehensive experiments show our model can robustly deal with various inpainting types at different video duration ranges, with high quality11More visualization results are made publicly available here.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2128662401",
                    "name": "Zhixing Zhang"
                },
                {
                    "authorId": "2269776842",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "2271752219",
                    "name": "Xiaoyan Wang"
                },
                {
                    "authorId": "2271325625",
                    "name": "Yaqiao Luo"
                },
                {
                    "authorId": "2273376477",
                    "name": "Luxin Zhang"
                },
                {
                    "authorId": "2247515549",
                    "name": "Yinan Zhao"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "2265543956",
                    "name": "Dimitris N. Metaxas"
                },
                {
                    "authorId": "2269696579",
                    "name": "Licheng Yu"
                }
            ]
        },
        {
            "paperId": "d62daf809266e02a3e3be4bec160579ff3839cc9",
            "title": "An Investigation on Hardware-Aware Vision Transformer Scaling",
            "abstract": "Vision Transformer (ViT) has demonstrated promising performance in various computer vision tasks, and recently attracted a lot of research attention. Many recent works have focused on proposing new architectures to improve ViT and deploying it into real-world applications. However, little effort has been made to analyze and understand ViT\u2019s architecture design space and its implication for hardware costs on different devices. In this work, by simply scaling ViT\u2019s depth, width, input size, and other basic configurations, we show that a scaled vanilla ViT model without bells and whistles can achieve comparable or superior accuracy-efficiency trade-off than most of the latest ViT variants. Specifically, compared with DeiT-Tiny, our scaled model achieves a \u2191 1.9% higher ImageNet top-1 accuracy under the same FLOPs and a \u2191 3.7% better ImageNet top-1 accuracy under the same latency on an NVIDIA Edge GPU TX2. Motivated by this, we further investigate the extracted scaling strategies from the following two aspects: (1) can these scaling strategies be transferred across different real hardware devices? and (2) can these scaling strategies be transferred to different ViT variants and tasks?. For (1), our exploration, based on various devices with different resource budgets, indicates that the transferability effectiveness depends on the underlying device together with its corresponding deployment tool. For (2), we validate the effective transferability of the aforementioned scaling strategies obtained from a vanilla ViT model on top of an image classification task to the PiT model, a strong ViT variant targeting efficiency as well as object detection and video classification tasks. In particular, when transferred to PiT, our scaling strategies lead to a boosted ImageNet top-1 accuracy of from 74.6% to 76.7% (\u2191 2.1%) under the same 0.7G FLOPs. When transferred to the COCO object detection task, the average precision is boosted by \u2191 0.7% under a similar throughput on a V100 GPU.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28987646",
                    "name": "Chaojian Li"
                },
                {
                    "authorId": "2110017910",
                    "name": "Kyungmin Kim"
                },
                {
                    "authorId": "3130257",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "2232778335",
                    "name": "Hang Zhang"
                },
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "2198041939",
                    "name": "Yingyan Lin"
                }
            ]
        },
        {
            "paperId": "e04da3c945aae8e2211222d373e7bf771d6412a7",
            "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack",
            "abstract": "Training text-to-image models with web scale image-text pairs enables the generation of a wide range of visual concepts from text. However, these pre-trained models often face challenges when it comes to generating highly aesthetic images. This creates the need for aesthetic alignment post pre-training. In this paper, we propose quality-tuning to effectively guide a pre-trained model to exclusively generate highly visually appealing images, while maintaining generality across visual concepts. Our key insight is that supervised fine-tuning with a set of surprisingly small but extremely visually appealing images can significantly improve the generation quality. We pre-train a latent diffusion model on $1.1$ billion image-text pairs and fine-tune it with only a few thousand carefully selected high-quality images. The resulting model, Emu, achieves a win rate of $82.9\\%$ compared with its pre-trained only counterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred $68.4\\%$ and $71.3\\%$ of the time on visual appeal on the standard PartiPrompts and our Open User Input benchmark based on the real-world usage of text-to-image models. In addition, we show that quality-tuning is a generic approach that is also effective for other architectures, including pixel diffusion and masked generative transformer models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "2249723114",
                    "name": "Ji Hou"
                },
                {
                    "authorId": "2250489747",
                    "name": "Chih-Yao Ma"
                },
                {
                    "authorId": "2225238191",
                    "name": "Sam S. Tsai"
                },
                {
                    "authorId": "2247901055",
                    "name": "Jialiang Wang"
                },
                {
                    "authorId": "2248766592",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "83754395",
                    "name": "Simon Vandenhende"
                },
                {
                    "authorId": "2248423317",
                    "name": "Xiaofang Wang"
                },
                {
                    "authorId": "2479521",
                    "name": "Abhimanyu Dubey"
                },
                {
                    "authorId": "2110144262",
                    "name": "Matthew Yu"
                },
                {
                    "authorId": "89942851",
                    "name": "Abhishek Kadian"
                },
                {
                    "authorId": "2708577",
                    "name": "Filip Radenovic"
                },
                {
                    "authorId": "144542135",
                    "name": "D. Mahajan"
                },
                {
                    "authorId": "2256701243",
                    "name": "Kunpeng Li"
                },
                {
                    "authorId": "2248075665",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2162195471",
                    "name": "Vladan Petrovic"
                },
                {
                    "authorId": "2247874378",
                    "name": "Mitesh Kumar Singh"
                },
                {
                    "authorId": "121255235",
                    "name": "Simran Motwani"
                },
                {
                    "authorId": "148416622",
                    "name": "Yiqian Wen"
                },
                {
                    "authorId": "1705408",
                    "name": "Yi-Zhe Song"
                },
                {
                    "authorId": "1722889",
                    "name": "Roshan Sumbaly"
                },
                {
                    "authorId": "34066479",
                    "name": "Vignesh Ramanathan"
                },
                {
                    "authorId": "2558787",
                    "name": "Zijian He"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "2248278031",
                    "name": "Devi Parikh"
                }
            ]
        },
        {
            "paperId": "e0d62e25811018636c22d2cc76650b9d31968890",
            "title": "Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis",
            "abstract": "In this paper, we introduce Fairy, a minimalist yet ro-bust adaptation of image-editing diffusion models, enhancing them for video editing applications. Our approach centers on the concept of anchor-based cross-frame attention, a mechanism that implicitly propagates diffusion features across frames, ensuring superior temporal coherence and high-fidelity synthesis. Fairy not only addresses limitations of previous models on memory and processing speed, but also improves temporal consistency through a unique data augmentation strategy. This strategy renders the model equivariant to affine transformations in both source and target images. Remarkably efficient, Fairy generates 120- frame 512\u00d7384 videos (4-second duration at 30 FPS) in just 14 seconds, outpacing prior works by at least 44\u00d7. A comprehensive user study, involving 1000 generated samples, confirms that our approach delivers superior quality, decisively outperforming established methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2269776842",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "2275606869",
                    "name": "Ching-Yao Chuang"
                },
                {
                    "authorId": "2271752219",
                    "name": "Xiaoyan Wang"
                },
                {
                    "authorId": "2275766917",
                    "name": "Yichen Jia"
                },
                {
                    "authorId": "2075974852",
                    "name": "K. Krishnakumar"
                },
                {
                    "authorId": "2275602354",
                    "name": "Tong Xiao"
                },
                {
                    "authorId": "2275604366",
                    "name": "Feng Liang"
                },
                {
                    "authorId": "2269696579",
                    "name": "Licheng Yu"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                }
            ]
        },
        {
            "paperId": "e7ceec6f384db0ddf8c2b387c5172dee28326567",
            "title": "ControlRoom3D: Room Generation Using Semantic Proxy Rooms",
            "abstract": "Manually creating 3D environments for AR/VR applications is a complex process requiring expert knowledge in 3D modeling software. Pioneering works facilitate this process by generating room meshes conditioned on textual style descriptions. Yet, many of these automatically generated 3D meshes do not adhere to typical room layouts, compromising their plausibility, e.g., by placing several beds in one bedroom. To address these challenges, we present ControlRoom3D, a novel method to generate high-quality room meshes. Central to our approach is a user-defined 3D semantic proxy room that outlines a rough room layout based on semantic bounding boxes and a textual description of the overall room style. Our key insight is that when rendered to 2D, this 3D representation provides valuable geometric and semantic information to control powerful 2D models to generate 3D consistent textures and geometry that aligns well with the proxy room. Backed up by an extensive study including quantitative metrics and qualitative user evaluations, our method generates diverse and globally plausible 3D room meshes, thus empowering users to design 3D rooms effortlessly without specialized knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "79955689",
                    "name": "Jonas Schult"
                },
                {
                    "authorId": "2225238191",
                    "name": "Sam S. Tsai"
                },
                {
                    "authorId": "2142793267",
                    "name": "Lukas H\u00f6llein"
                },
                {
                    "authorId": "2271890529",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "2247901055",
                    "name": "Jialiang Wang"
                },
                {
                    "authorId": "2250489747",
                    "name": "Chih-Yao Ma"
                },
                {
                    "authorId": "2256701243",
                    "name": "Kunpeng Li"
                },
                {
                    "authorId": "2248423317",
                    "name": "Xiaofang Wang"
                },
                {
                    "authorId": "2028358105",
                    "name": "Felix Wimbauer"
                },
                {
                    "authorId": "2558787",
                    "name": "Zijian He"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "2064068973",
                    "name": "Bastian Leibe"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "2249723114",
                    "name": "Ji Hou"
                }
            ]
        },
        {
            "paperId": "feb49aec3ed748bb15c5375363b21f01baef8b24",
            "title": "Cache Me if You Can: Accelerating Diffusion Models through Block Caching",
            "abstract": "Diffusion models have recently revolutionized the field of image synthesis due to their ability to generate photorealistic images. However, one of the major drawbacks of diffusion models is that the image generation process is costly. A large image-to-image network has to be applied many times to iteratively refine an image from random noise. While many recent works propose techniques to reduce the number of required steps, they generally treat the underlying denoising network as a black box. In this work, we investigate the behavior of the layers within the network and find that 1) the layers' output changes smoothly over time, 2) the layers show distinct patterns of change, and 3) the change from step to step is often very small. We hypothesize that many layer computations in the denoising network are redundant. Leveraging this, we introduce block caching, in which we reuse outputs from layer blocks of previous steps to speed up inference. Furthermore, we propose a technique to automatically determine caching schedules based on each block's changes over timesteps. In our experiments, we show through FID, human evaluation and qualitative analysis that Block Caching allows to generate images with higher visual quality at the same computational cost. We demonstrate this for different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM). Project page: fwmb.github.io/blockcaching",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2028358105",
                    "name": "Felix Wimbauer"
                },
                {
                    "authorId": "2271890529",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "2270673864",
                    "name": "Edgar Schoenfeld"
                },
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "2249723114",
                    "name": "Ji Hou"
                },
                {
                    "authorId": "2558787",
                    "name": "Zijian He"
                },
                {
                    "authorId": "3451249",
                    "name": "A. Sanakoyeu"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "2225238191",
                    "name": "Sam S. Tsai"
                },
                {
                    "authorId": "2275352009",
                    "name": "Jonas Kohler"
                },
                {
                    "authorId": "2257034597",
                    "name": "Christian Rupprecht"
                },
                {
                    "authorId": "2269841264",
                    "name": "Daniel Cremers"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "2247901055",
                    "name": "Jialiang Wang"
                }
            ]
        }
    ]
}