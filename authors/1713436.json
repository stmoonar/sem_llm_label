{
    "authorId": "1713436",
    "papers": [
        {
            "paperId": "304408dca8e63e85ae2a096a0ec470b5317d9ad3",
            "title": "Multi-Conditional Ranking with Large Language Models",
            "abstract": "Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iteratively Ranking the items (EXSIR). Our extensive experiments show that this decomposed reasoning method enhances LLMs' performance significantly, achieving up to a 12% improvement over existing LLMs. We also provide a detailed analysis of LLMs performance across various condition categories, and examine the effectiveness of decomposition step. Furthermore, we compare our method with existing approaches such as Chain-of-Thought and existing ranking models, demonstrating the superiority of our approach and complexity of MCR task. We released our dataset and code.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                },
                {
                    "authorId": "2265753807",
                    "name": "Estevam R. Hruschka"
                }
            ]
        },
        {
            "paperId": "54644dfa759ca210e6c00dfe8943e37bca8a0ff0",
            "title": "Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions",
            "abstract": "Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration of constraints during optimization and establish connections among different components within the system, which also enable a more holistic and comprehensive approach to evaluation. We present a formal definition of reasoning capacity and illustrate its utility in identifying limitations within each component of the system. We then argue how these limitations can be addressed with a self-reflective process wherein human-feedback is used to alleviate shortcomings in reasoning and enhance overall consistency of the system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                },
                {
                    "authorId": "1781317",
                    "name": "Eser Kandogan"
                },
                {
                    "authorId": "29995869",
                    "name": "Nikita Bhutani"
                },
                {
                    "authorId": "37455401",
                    "name": "Sajjadur Rahman"
                },
                {
                    "authorId": "2282472829",
                    "name": "Tom Mitchell"
                },
                {
                    "authorId": "2265753807",
                    "name": "Estevam R. Hruschka"
                }
            ]
        },
        {
            "paperId": "69069bea10deaa4b6b95450420cb5564a4890aa6",
            "title": "A Blueprint Architecture of Compound AI Systems for Enterprise",
            "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities surpassing conventional NLP challenges, creating opportunities for use in production use cases. Towards this goal, there is a notable shift to building compound AI systems, wherein LLMs are integrated into an expansive software infrastructure with many components like models, retrievers, databases and tools. In this paper, we introduce a blueprint architecture for compound AI systems to operate in enterprise settings cost-effectively and feasibly. Our proposed architecture aims for seamless integration with existing compute and data infrastructure, with ``stream'' serving as the key orchestration concept to coordinate data and instructions among agents and other components. Task and data planners, respectively, break down, map, and optimize tasks and data to available agents and data sources defined in respective registries, given production constraints such as accuracy and latency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1781317",
                    "name": "Eser Kandogan"
                },
                {
                    "authorId": "37455401",
                    "name": "Sajjadur Rahman"
                },
                {
                    "authorId": "2284869489",
                    "name": "Nikita Bhutani"
                },
                {
                    "authorId": "2188417102",
                    "name": "Dan Zhang"
                },
                {
                    "authorId": "2199811208",
                    "name": "Rafael Li Chen"
                },
                {
                    "authorId": "2265753908",
                    "name": "Kushan Mitra"
                },
                {
                    "authorId": "2121386026",
                    "name": "Sairam Gurajada"
                },
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                },
                {
                    "authorId": "7782351",
                    "name": "Hayate Iso"
                },
                {
                    "authorId": "2304573633",
                    "name": "Yanlin Feng"
                },
                {
                    "authorId": "2143468335",
                    "name": "H. Kim"
                },
                {
                    "authorId": "2305077543",
                    "name": "Chen Shen"
                },
                {
                    "authorId": "2305332661",
                    "name": "Jin Wang"
                },
                {
                    "authorId": "2265753807",
                    "name": "Estevam R. Hruschka"
                }
            ]
        },
        {
            "paperId": "a763e20be42f01cb1492e24035786d96c419e3bb",
            "title": "LLMs Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree Benchmark for Comprehensive Evaluation of LLMs",
            "abstract": "Large language models (LLMs) demonstrate impressive capabilities in mathematical reasoning. However, despite these achievements, current evaluations are mostly limited to specific mathematical topics, and it remains unclear whether LLMs are genuinely engaging in reasoning. To address these gaps, we present the Mathematical Topics Tree (MaTT) benchmark, a challenging and structured benchmark that offers 1,958 questions across a wide array of mathematical subjects, each paired with a detailed hierarchical chain of topics. Upon assessing different LLMs using the MaTT benchmark, we find that the most advanced model, GPT-4, achieved a mere 54\\% accuracy in a multiple-choice scenario. Interestingly, even when employing Chain-of-Thought prompting, we observe mostly no notable improvement. Moreover, LLMs accuracy dramatically reduced by up to 24.2 percentage point when the questions were presented without providing choices. Further detailed analysis of the LLMs' performance across a range of topics showed significant discrepancy even for closely related subtopics within the same general mathematical area. In an effort to pinpoint the reasons behind LLMs performances, we conducted a manual evaluation of the completeness and correctness of the explanations generated by GPT-4 when choices were available. Surprisingly, we find that in only 53.3\\% of the instances where the model provided a correct answer, the accompanying explanations were deemed complete and accurate, i.e., the model engaged in genuine reasoning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2559595",
                    "name": "Arash Gholami Davoodi"
                },
                {
                    "authorId": "2305616156",
                    "name": "Seyed Pouyan Mousavi Davoudi"
                },
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                }
            ]
        },
        {
            "paperId": "5e096f65139e789fd3aa41de7e11bc9c04da79d5",
            "title": "Measuring and Modifying Factual Knowledge in Large Language Models",
            "abstract": "Large Language Models (LLMs) store an extensive amount of factual knowledge obtained from vast collections of text. To effectively utilize these models for downstream tasks, it is crucial to have reliable methods for measuring their knowledge. However, existing approaches for knowledge measurement have certain limitations, and despite recent efforts, they fail to provide accurate measurements and the necessary insights for modifying the knowledge within LLMs. In this work, we employ information theory-based measurements to provide a framework estimating the factual knowledge contained within large language models. More specifically, we measure knowledge by analyzing the LLM's prediction probability distribution before and after instilling the target knowledge, employing metrics such as entropy and KL-divergence. Introducing our metrics, we first assess their accuracy in comparison to previous ranking-based methods, surpassing them by around 30% in a synthetic experiment. Then, we explore two prominent methods of knowledge instillation, discovering that LLMs exhibit limitations in capturing new knowledge under specific circumstances for one of these methods. Lastly, we demonstrate the applicability of our methods in extracting unlearned and mislearned facts in LLMs through their application to in-context learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                }
            ]
        },
        {
            "paperId": "a925b55e0bf07a68ba7554beabda9fa88cd025c5",
            "title": "Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks",
            "abstract": "Numerous HR applications are centered around resumes and job descriptions. While they can benefit from advancements in NLP, particularly large language models, their real-world adoption faces challenges due to absence of comprehensive benchmarks for various HR tasks, and lack of smaller models with competitive capabilities. In this paper, we aim to bridge this gap by introducing the Resume-Job Description Benchmark (RJDB). We meticulously craft this benchmark to cater to a wide array of HR tasks, including matching and explaining resumes to job descriptions, extracting skills and experiences from resumes, and editing resumes. To create this benchmark, we propose to distill domain-specific knowledge from a large language model (LLM). We rely on a curated skill-occupation graph to ensure diversity and provide context for LLMs generation. Our benchmark includes over 50 thousand triples of job descriptions, matched resumes and unmatched resumes. Using RJDB, we train multiple smaller student models. Our experiments reveal that the student models achieve near/better performance than the teacher model (GPT-4), affirming the effectiveness of the benchmark. Additionally, we explore the utility of RJDB on out-of-distribution data for skill extraction and resume-job description matching, in zero-shot and weak supervision manner. We release our datasets and code to foster further research and industry applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                },
                {
                    "authorId": "7782351",
                    "name": "Hayate Iso"
                },
                {
                    "authorId": "67055103",
                    "name": "Thom Lake"
                },
                {
                    "authorId": "29995869",
                    "name": "Nikita Bhutani"
                },
                {
                    "authorId": "2265753807",
                    "name": "Estevam R. Hruschka"
                }
            ]
        },
        {
            "paperId": "d592d88bbb30a52bdac637f025a50c3aef07a89f",
            "title": "Less is More for Long Document Summary Evaluation by LLMs",
            "abstract": "Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked. To address these issues, this paper introduces a novel approach, Extract-then-Evaluate, which involves extracting key sentences from a long source document and then evaluating the summary by prompting LLMs. The results reveal that the proposed method not only significantly reduces evaluation costs but also exhibits a higher correlation with human evaluations. Furthermore, we provide practical recommendations for optimal document length and sentence extraction methods, contributing to the development of cost-effective yet more accurate methods for LLM-based text generation evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240802491",
                    "name": "Yunshu Wu"
                },
                {
                    "authorId": "7782351",
                    "name": "Hayate Iso"
                },
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                },
                {
                    "authorId": "29995869",
                    "name": "Nikita Bhutani"
                },
                {
                    "authorId": "1842532",
                    "name": "Estevam Hruschka"
                }
            ]
        },
        {
            "paperId": "ec101a7eff56e2a90bc0143f734fb282be757a5f",
            "title": "Rethinking Language Models as Symbolic Knowledge Graphs",
            "abstract": "Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric applications such as search, question answering and recommendation. As contemporary language models (LMs) trained on extensive textual data have gained prominence, researchers have extensively explored whether the parametric knowledge within these models can match up to that present in knowledge graphs. Various methodologies have indicated that enhancing the size of the model or the volume of training data enhances its capacity to retrieve symbolic knowledge, often with minimal or no human supervision. Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes. In this work, we provide an exhaustive evaluation of language models of varying sizes and capabilities. We construct nine qualitative benchmarks that encompass a spectrum of attributes including symmetry, asymmetry, hierarchy, bidirectionality, compositionality, paths, entity-centricity, bias and ambiguity. Additionally, we propose novel evaluation metrics tailored for each of these attributes. Our extensive evaluation of various LMs shows that while these models exhibit considerable potential in recalling factual information, their ability to capture intricate topological and semantic traits of KGs remains significantly constrained. We note that our proposed evaluation metrics are more reliable in evaluating these abilities than the existing metrics. Lastly, some of our benchmarks challenge the common notion that larger LMs (e.g., GPT-4) universally outshine their smaller counterparts (e.g., BERT).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51035076",
                    "name": "Vishwas Mruthyunjaya"
                },
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                },
                {
                    "authorId": "1842532",
                    "name": "Estevam Hruschka"
                },
                {
                    "authorId": "29995869",
                    "name": "Nikita Bhutani"
                }
            ]
        },
        {
            "paperId": "fd81018bc72b030545a2d3f3010f3758ec4d48c3",
            "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                },
                {
                    "authorId": "1842532",
                    "name": "Estevam Hruschka"
                }
            ]
        },
        {
            "paperId": "20a50d0dddf9046f335a7f555ad5fc6578feb4c6",
            "title": "The Extremal GDoF Gain of Optimal Versus Binary Power Control in K User Interference Networks is \u0398 (\u221aK)",
            "abstract": "Using ideas from Generalized Degrees of Freedom (GDoF) analyses and extremal network theory, this work studies the extremal gain of optimal power control over binary (on/off) power control, especially in large interference networks, in search of new theoretical insights. Whereas numerical studies have already established that in most practical settings binary power control is close to optimal, the extremal analysis shows not only that there exist settings where the gain from optimal power control can be quite significant, but also bounds the extremal values of such gains from a GDoF perspective. As its main contribution, this work explicitly characterizes the extremal GDoF gain of optimal over binary power control as <inline-formula> <tex-math notation=\"LaTeX\">$\\Theta (\\sqrt {K})$ </tex-math></inline-formula> for all <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula>. In particular, the extremal gain is bounded between <inline-formula> <tex-math notation=\"LaTeX\">$\\lfloor \\sqrt {K}\\rfloor $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$2.5\\sqrt {K}$ </tex-math></inline-formula> for every <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula>. For <inline-formula> <tex-math notation=\"LaTeX\">$K=2,3,4,5,6$ </tex-math></inline-formula> users, the precise extremal gain is found to be 1, 3/2, 2, 9/4 and 41/16, respectively. Networks shown to achieve the extremal gain may be interpreted as multi-tier heterogeneous networks. It is worthwhile to note that because of their focus on asymptotic analysis, the sharp characterizations of extremal gains are valuable primarily from a theoretical perspective, and not as contradictions to the conventional wisdom that binary power control is generally close to optimal in practical, non-asymptotic settings.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3140482",
                    "name": "Yao-Chia Chan"
                },
                {
                    "authorId": "1713436",
                    "name": "Pouya Pezeshkpour"
                },
                {
                    "authorId": "143724038",
                    "name": "Chunhua Geng"
                },
                {
                    "authorId": "145486824",
                    "name": "S. Jafar"
                }
            ]
        }
    ]
}