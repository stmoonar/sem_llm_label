{
    "authorId": "3115592",
    "papers": [
        {
            "paperId": "15c3d00f4719e27130f0122d70c8556686b1b6b8",
            "title": "SYMPTOMIFY: Transforming Symptom Annotations with Language Model Knowledge Harvesting",
            "abstract": "Given the high-stakes nature of healthcare decision-making, we aim to improve the efficiency of human annotators rather than re-placing them with fully automated solutions. We introduce a new comprehensive resource, S YMPTOMIFY , a dataset of annotated vaccine adverse reaction reports detailing individual vaccine reactions. The dataset, consisting of over 800 k reports, surpasses previous datasets in size. Notably, it features reasoning-based explanations alongside background knowledge obtained via language model knowledge harvesting. We assess data quality, and evaluate performance across various methods and learning paradigms, paving the way for future comparisons and benchmarking. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273756244",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "bbb938b69ee2bcf3b4833bd2b3b8016a353bb30c",
            "title": "Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision",
            "abstract": "Current medical question answering systems have difficulty processing long, detailed and informally worded questions submitted by patients, called Consumer Health Questions (CHQs). To address this issue, we introduce a medical question understanding and answering system with knowledge grounding and semantic self-supervision. Our system is a pipeline that first summarizes a long, medical, user-written question, using a supervised summarization loss. Then, our system performs a two-step retrieval to return answers. The system first matches the summarized user question with an FAQ from a trusted medical knowledge base, and then retrieves a fixed number of relevant sentences from the corresponding answer document. In the absence of labels for question matching or answer relevance, we design 3 novel, self-supervised and semantically-guided losses. We evaluate our model against two strong retrieval-based question answering baselines. Evaluators ask their own questions and rate the answers retrieved by our baselines and own system according to their relevance. They find that our system retrieves more relevant answers, while achieving speeds 20 times faster. Our self-supervised losses also help the summarizer achieve higher scores in ROUGE, as well as in human evaluation metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46180513",
                    "name": "Khalil Mrini"
                },
                {
                    "authorId": "2170328025",
                    "name": "Harpreet Singh"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2110654003",
                    "name": "Seunghyun Yoon"
                },
                {
                    "authorId": "145262461",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "2116858693",
                    "name": "Walter Chang"
                },
                {
                    "authorId": "1786321",
                    "name": "E. Farcas"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "c8ccb9ff52180cf8f03bb39ee131ccf0624f7874",
            "title": "Data Augmentation for Rare Symptoms in Vaccine Side-Effect Detection",
            "abstract": "We study the problem of entity detection and normalization applied to patient self-reports of symptoms that arise as side-effects of vaccines. Our application domain presents unique challenges that render traditional classification methods ineffective: the number of entity types is large; and many symptoms are rare, resulting in a long-tail distribution of training examples per entity type. We tackle these challenges with an autoregressive model that generates standardized names of symptoms. We introduce a data augmentation technique to increase the number of training examples for rare symptoms. Experiments on real-life patient vaccine symptom self-reports show that our approach outperforms strong baselines, and that additional examples improve performance on the long-tail entities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "ec8a0790f865a89f8e51d9b83bea80e43793406d",
            "title": "Structured Data Representation in Natural Language Interfaces",
            "abstract": "A Natural Language Interface (NLI) enables the use of human languages to interact with computer systems, including smart phones and robots. Compared to other types of interfaces, such as command line interfaces (CLIs) or graphical user interfaces (GUIs), NLIs stand to enable more people to have access to functionality behind databases or APIs as they only require knowledge of natural languages. Many NLI applications involve structured data for the domain (e.g., applications such as hotel booking, product search, and factual question answering.) Thus, to fully process user questions, in addition to natural language comprehension, understanding of structured data is also crucial for the model. In this paper, we study neural network methods for building Natural Language Interfaces (NLIs) with a focus on learning structure data representations that can generalize to novel data sources and schemata not seen at training time. Specifically, we review two tasks related to natural language interfaces: i) semantic parsing where we focus on text-to-SQL for database access, and ii) task-oriented dialog systems for API access. We survey representative methods for text-to-SQL and task-oriented dialog tasks, focusing on representing and incorporating structured data. Lastly, we present two of our original studies on structured data representation methods for NLIs to enable access to i) databases, and ii) visualization APIs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30046004",
                    "name": "Yutong Shao"
                },
                {
                    "authorId": "2155076799",
                    "name": "Arun Kumar"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "f29e5a78378bd3d8ae8ec7caecf564fe3701c1dd",
            "title": "Zero-shot Triplet Extraction by Template Infilling",
            "abstract": "The task of triplet extraction aims to extract pairs of entities and their corresponding relations from unstructured text. Most existing methods train an extraction model on training data involving specific target relations, and are incapable of extracting new relations that were not observed at training time. Generalizing the model to unseen relations typically requires fine-tuning on synthetic training data which is often noisy and unreliable. We show that by reducing triplet extraction to a template infilling task over a pre-trained language model (LM), we can equip the extraction model with zero-shot learning capabilities and eliminate the need for additional training data. We propose a novel framework, ZETT (ZEro-shot Triplet extraction by Template infilling), that aligns the task objective to the pre-training objective of generative transformers to generalize to unseen relations. Experiments on FewRel and Wiki-ZSL datasets demonstrate that ZETT shows consistent and stable performance, outperforming previous state-of-the-art methods, even when using automatically generated templates. https://github.com/megagonlabs/zett/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "7782351",
                    "name": "Hayate Iso"
                },
                {
                    "authorId": "29995869",
                    "name": "Nikita Bhutani"
                },
                {
                    "authorId": "1842532",
                    "name": "Estevam Hruschka"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "061c712d9bb32439c70d9d3c01882f4097fc3df3",
            "title": "Recursive Tree-Structured Self-Attention for Answer Sentence Selection",
            "abstract": "Syntactic structure is an important component of natural language text. Recent top-performing models in Answer Sentence Selection (AS2) use self-attention and transfer learning, but not syntactic structure. Tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness. We investigate whether tree structures can boost performance in AS2. We introduce the Tree Aggregation Transformer: a novel recursive, tree-structured self-attention model for AS2. The recursive nature of our model is able to represent all levels of syntactic parse trees with only one additional self-attention layer. Without transfer learning, we establish a new state of the art on the popular TrecQA and WikiQA benchmark datasets. Additionally, we evaluate our method on four Community Question Answering datasets, and find that tree-structured representations have limitations with noisy user-generated text. We conduct probing experiments to evaluate how our models leverage tree structures across datasets. Our findings show that the ability of tree-structured models to successfully absorb syntactic information is strongly correlated with a higher performance in AS2.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065526768",
                    "name": "K. Mrini"
                },
                {
                    "authorId": "1786321",
                    "name": "E. Farcas"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "1b60e235fb995d97a25f940f396a484b0ac7b8da",
            "title": "Joint Summarization-Entailment Optimization for Consumer Health Question Understanding",
            "abstract": "Understanding the intent of medical questions asked by patients, or Consumer Health Questions, is an essential skill for medical Conversational AI systems. We propose a novel data-augmented and simple joint learning approach combining question summarization and Recognizing Question Entailment (RQE) in the medical domain. Our data augmentation approach enables to use just one dataset for joint learning. We show improvements on both tasks across four biomedical datasets in accuracy (+8%), ROUGE-1 (+2.5%) and human evaluation scores. Human evaluation shows joint learning generates faithful and informative summaries. Finally, we release our code, the two question summarization datasets extracted from a large-scale medical dialogue dataset, as well as our augmented datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46180513",
                    "name": "Khalil Mrini"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2116858693",
                    "name": "Walter Chang"
                },
                {
                    "authorId": "1786321",
                    "name": "E. Farcas"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "321b3f1d6c17712d7344c526ac553fcf72486893",
            "title": "Interactive Plot Manipulation using Natural Language",
            "abstract": "We present an interactive Plotting Agent, a system that enables users to directly manipulate plots using natural language instructions within an interactive programming environment. The Plotting Agent maps language to plot updates. We formulate this problem as a slot-based task-oriented dialog problem, which we tackle with a sequence-to-sequence model. This plotting model while accurate in most cases, still makes errors, therefore, the system allows a feedback mode, wherein the user is presented with a top-k list of plots, among which the user can pick the desired one. From this kind of feedback, we can then, in principle, continuously learn and improve the system. Given that plotting is widely used across data-driven fields, we believe our demonstration will be of interest to both practitioners such as data scientists broadly defined, and researchers interested in natural language interfaces.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108927455",
                    "name": "Yihan Wang"
                },
                {
                    "authorId": "30046004",
                    "name": "Yutong Shao"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "4061f3c5e32589c0a9a6569708b48ec5378ed12a",
            "title": "UCSD-Adobe at MEDIQA 2021: Transfer Learning and Answer Sentence Selection for Medical Summarization",
            "abstract": "In this paper, we describe our approach to question summarization and multi-answer summarization in the context of the 2021 MEDIQA shared task (Ben Abacha et al., 2021). We propose two kinds of transfer learning for the abstractive summarization of medical questions. First, we train on HealthCareMagic, a large question summarization dataset collected from an online healthcare service platform. Second, we leverage the ability of the BART encoder-decoder architecture to model both generation and classification tasks to train on the task of Recognizing Question Entailment (RQE) in the medical domain. We show that both transfer learning methods combined achieve the highest ROUGE scores. Finally, we cast the question-driven extractive summarization of multiple relevant answer documents as an Answer Sentence Selection (AS2) problem. We show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46180513",
                    "name": "Khalil Mrini"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2110654003",
                    "name": "Seunghyun Yoon"
                },
                {
                    "authorId": "145262461",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "2116858693",
                    "name": "Walter Chang"
                },
                {
                    "authorId": "1786321",
                    "name": "E. Farcas"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "562f4062b43830115568f9552b140721c958a72c",
            "title": "A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding",
            "abstract": "Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval. To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding. We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain. Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss. We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer. We show through ablation studies that our proposed novelties improve performance. Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation. Finally, we show that our method fares better than single-task learning under 4 low-resource settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065526768",
                    "name": "K. Mrini"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2110654003",
                    "name": "Seunghyun Yoon"
                },
                {
                    "authorId": "145966986",
                    "name": "Trung H. Bui"
                },
                {
                    "authorId": "2116858693",
                    "name": "Walter Chang"
                },
                {
                    "authorId": "1786321",
                    "name": "E. Farcas"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        }
    ]
}