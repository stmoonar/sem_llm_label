{
    "authorId": "2054378953",
    "papers": [
        {
            "paperId": "185ace5661963e2e1eb998e739e4110272a6bb43",
            "title": "COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements",
            "abstract": "Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance\"your English is very good\"may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144101734",
                    "name": "Xuhui Zhou"
                },
                {
                    "authorId": "2122171840",
                    "name": "Haojie Zhu"
                },
                {
                    "authorId": "1388021166",
                    "name": "Akhila Yerukola"
                },
                {
                    "authorId": "2054378953",
                    "name": "Thomas Davidson"
                },
                {
                    "authorId": "2012510",
                    "name": "Jena D. Hwang"
                },
                {
                    "authorId": "2133324514",
                    "name": "Swabha Swayamdipta"
                },
                {
                    "authorId": "2729164",
                    "name": "Maarten Sap"
                }
            ]
        },
        {
            "paperId": "2d3ca953e2efb2e6b09d851e1649681ac549ae89",
            "title": "Partisan US News Media Representations of Syrian Refugees",
            "abstract": "We investigate how representations of Syrian refugees (2011-2021) differ across US partisan news outlets. We analyze 47,388 articles from the online US media about Syrian refugees to detail differences in reporting between left- and right-leaning media. We use various NLP techniques to understand these differences. Our polarization and question answering results indicated that left-leaning media tended to represent refugees as child victims, welcome in the US, and right-leaning media cast refugees as Islamic terrorists. We noted similar results with our sentiment and offensive speech scores over time, which detail possibly unfavorable representations of refugees in right-leaning media. A strength of our work is how the different techniques we have applied validate each other. Based on our results, we provide several recommendations. Stakeholders may utilize our findings to intervene around refugee representations, and design communications campaigns that improve the way society sees refugees and possibly aid refugee outcomes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152955806",
                    "name": "Keyu Chen"
                },
                {
                    "authorId": "51166170",
                    "name": "M. Babaeianjelodar"
                },
                {
                    "authorId": "2170852517",
                    "name": "Yiwen Shi"
                },
                {
                    "authorId": "1645061254",
                    "name": "Kamila Janmohamed"
                },
                {
                    "authorId": "2056781644",
                    "name": "Rupak Sarkar"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "2054378953",
                    "name": "Thomas Davidson"
                },
                {
                    "authorId": "2583473",
                    "name": "M. Choudhury"
                },
                {
                    "authorId": "2171736559",
                    "name": "Jonathan Y Huang"
                },
                {
                    "authorId": "46782621",
                    "name": "S. Yadav"
                },
                {
                    "authorId": "2170539086",
                    "name": "Ashique Khudabukhsh"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2736592",
                    "name": "C. Bauch"
                },
                {
                    "authorId": "20985588",
                    "name": "Orestis Papakyriakopoulos"
                },
                {
                    "authorId": "50417026",
                    "name": "K. Khoshnood"
                },
                {
                    "authorId": "50325647",
                    "name": "Navin Kumar"
                }
            ]
        },
        {
            "paperId": "c0b96f3dac60b65c7deff3ccb6a700844f0bff90",
            "title": "Examining Racial Bias in an Online Abuse Corpus with Structural Topic Modeling",
            "abstract": "We use structural topic modeling to examine racial bias in data collected to train models to detect hate speech and abusive language in social media posts. We augment the abusive language dataset by adding an additional feature indicating the predicted probability of the tweet being written in African-American English. We then use structural topic modeling to examine the content of the tweets and how the prevalence of different topics is related to both abusiveness annotation and dialect prediction. We find that certain topics are disproportionately racialized and considered abusive. We discuss how topic modeling may be a useful approach for identifying bias in annotated data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054378953",
                    "name": "Thomas Davidson"
                },
                {
                    "authorId": "2052797388",
                    "name": "Debasmita Bhattacharya"
                }
            ]
        },
        {
            "paperId": "5af1e8b2f546ab8dbac7a35e89e5a2b2af7968d7",
            "title": "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
            "abstract": "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "2054378953",
                    "name": "Thomas Davidson"
                },
                {
                    "authorId": "2052797388",
                    "name": "Debasmita Bhattacharya"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                }
            ]
        },
        {
            "paperId": "5e3067d9a604935ac10c14beb3f1540e5d062d8d",
            "title": "Identifying hate speech in social media",
            "abstract": "X R D S \u2022 W I N T E R 2 0 17 \u2022 V O L . 2 4 \u2022 N O . 2 Hate speech refers to statements that attack or delegitimize particular groups of people based on a demographic category\u2014race, gender, religion, sexual orientation, and so on. For social media platforms, like Facebook and Twitter, hate speech that specifically encourages violence against a group is explicitly prohibited in the terms of service. However, these media platforms are large and hard to moderate, so those rules cannot fully prevent hateful posts, nor do they regulate other kinds of hate speech. To mitigate this problem, machine learning can be used to identify potential hate speech in larger collections of text. This problem is closely tied to spam filtering, a classic problem in text classification and natural language processing. Based on past work in automatic hate speech detection [1], included is a brief tutorial on using Python, specifically scikit-learn [2], on a set of tweets to distinguish between hate speech and other text. We also describe some of the challenges in solving this task efficiently. In the process, we demonstrate the complexity of the problem; definitions of what specifically constitutes hate speech vary from person to person, and it can be tricky to find and represent the contextual elements that distinguish hate speech.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144627155",
                    "name": "Alexandra Schofield"
                },
                {
                    "authorId": "2054378953",
                    "name": "Thomas Davidson"
                }
            ]
        },
        {
            "paperId": "8dd6a2c9c88c9b3465484228c93f4dcc11cfeab9",
            "title": "Automated Hate Speech Detection and the Problem of Offensive Language",
            "abstract": "\n \n A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054378953",
                    "name": "Thomas Davidson"
                },
                {
                    "authorId": "9950697",
                    "name": "Dana Warmsley"
                },
                {
                    "authorId": "1863307",
                    "name": "M. Macy"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                }
            ]
        },
        {
            "paperId": "c7eba4ee84ce9d8db071e5d7922060ec3d8398c5",
            "title": "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
            "abstract": "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                },
                {
                    "authorId": "2054378953",
                    "name": "Thomas Davidson"
                },
                {
                    "authorId": "9950697",
                    "name": "Dana Warmsley"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                }
            ]
        }
    ]
}