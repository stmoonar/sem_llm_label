{
    "authorId": "2240559309",
    "papers": [
        {
            "paperId": "1bd1aff50c233b235e8a8d872896ebab36ee39ca",
            "title": "ControlTraj: Controllable Trajectory Generation with Topology-Constrained Diffusion Model",
            "abstract": "Generating trajectory data is among promising solutions to addressing privacy concerns, collection costs, and proprietary restrictions usually associated with human mobility analyses. However, existing trajectory generation methods are still in their infancy due to the inherent diversity and unpredictability of human activities, grappling with issues such as fidelity, flexibility, and generalizability. To overcome these obstacles, we propose ControlTraj, a Controllable Trajectory generation framework with the topology-constrained diffusion model. Distinct from prior approaches, ControlTraj utilizes a diffusion model to generate high-fidelity trajectories while integrating the structural constraints of road network topology to guide the geographical outcomes. Specifically, we develop a novel road segment autoencoder to extract fine-grained road segment embedding. The encoded features, along with trip attributes, are subsequently merged into the proposed geographic denoising UNet architecture, named GeoUNet, to synthesize geographic trajectories from white noise. Through experimentation across three real-world data settings, ControlTraj demonstrates its ability to produce human-directed, high-fidelity trajectory generation with adaptability to unexplored geographical contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292569421",
                    "name": "Yuanshao Zhu"
                },
                {
                    "authorId": "2261667444",
                    "name": "James J. Q. Yu"
                },
                {
                    "authorId": "2261673614",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "89987905",
                    "name": "Yongchao Ye"
                },
                {
                    "authorId": "2262453292",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2298640483",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2298206411",
                    "name": "Xuetao Wei"
                },
                {
                    "authorId": "2289093854",
                    "name": "Yuxuan Liang"
                }
            ]
        },
        {
            "paperId": "1fd57ba49bacc54358231931e1659648181e14c6",
            "title": "Bidirectional Gated Mamba for Sequential Recommendation",
            "abstract": "In various domains, Sequential Recommender Systems (SRS) have become essential due to their superior capability to discern intricate user preferences. Typically, SRS utilize transformer-based architectures to forecast the subsequent item within a sequence. Nevertheless, the quadratic computational complexity inherent in these models often leads to inefficiencies, hindering the achievement of real-time recommendations. Mamba, a recent advancement, has exhibited exceptional performance in time series prediction, significantly enhancing both efficiency and accuracy. However, integrating Mamba directly into SRS poses several challenges. Its inherently unidirectional nature may constrain the model's capacity to capture the full context of user-item interactions, while its instability in state estimation can compromise its ability to detect short-term patterns within interaction sequences. To overcome these issues, we introduce a new framework named Selective Gated Mamba (SIGMA) for Sequential Recommendation. This framework leverages a Partially Flipped Mamba (PF-Mamba) to construct a bidirectional architecture specifically tailored to improve contextual modeling. Additionally, an input-sensitive Dense Selective Gate (DS Gate) is employed to optimize directional weights and enhance the processing of sequential information in PF-Mamba. For short sequence modeling, we have also developed a Feature Extract GRU (FE-GRU) to efficiently capture short-term dependencies. Empirical results indicate that SIGMA outperforms current models on five real-world datasets. Our implementation code is available at https://github.com/ziwliu-cityu/SIMGA to ease reproducibility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316826974",
                    "name": "Ziwei Liu"
                },
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2211473272",
                    "name": "Wanyu Wang"
                },
                {
                    "authorId": "2264224432",
                    "name": "Pengyue Jia"
                },
                {
                    "authorId": "2262447141",
                    "name": "Maolin Wang"
                },
                {
                    "authorId": "2260835602",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "2316790244",
                    "name": "Yi Chang"
                },
                {
                    "authorId": "2243037657",
                    "name": "Xiangyu Zhao"
                }
            ]
        },
        {
            "paperId": "7c8609b93871c49e2e0cef2a0e11f9ec9b1ce921",
            "title": "Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item",
            "abstract": "Sequential recommendation systems (SRS) serve the purpose of predicting users' subsequent preferences based on their past interactions and have been applied across various domains such as e-commerce and social networking platforms. However, practical SRS encounters challenges due to the fact that most users engage with only a limited number of items, while the majority of items are seldom consumed. These challenges, termed as the long-tail user and long-tail item dilemmas, often create obstacles for traditional SRS methods. Mitigating these challenges is crucial as they can significantly impact user satisfaction and business profitability. While some research endeavors have alleviated these issues, they still grapple with issues such as seesaw or noise stemming from the scarcity of interactions. The emergence of large language models (LLMs) presents a promising avenue to address these challenges from a semantic standpoint. In this study, we introduce the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR), which leverages semantic embeddings from LLMs to enhance SRS performance without increasing computational overhead. To combat the long-tail item challenge, we propose a dual-view modeling approach that fuses semantic information from LLMs with collaborative signals from traditional SRS. To address the long-tail user challenge, we introduce a retrieval augmented self-distillation technique to refine user preference representations by incorporating richer interaction data from similar users. Through comprehensive experiments conducted on three authentic datasets using three widely used SRS models, our proposed enhancement framework demonstrates superior performance compared to existing methodologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2269463602",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2244621655",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "b0633ccf235e467c35b963ad012f6b8c54aba19f",
            "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
            "abstract": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262514619",
                    "name": "Derong Xu"
                },
                {
                    "authorId": "2030976630",
                    "name": "Ziheng Zhang"
                },
                {
                    "authorId": "2288043255",
                    "name": "Zhihong Zhu"
                },
                {
                    "authorId": "2258562926",
                    "name": "Zhenxi Lin"
                },
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2258675923",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2277237058",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2265580543",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "c1fc2546b1476b77448e01b9d7d0a50d1bf632d3",
            "title": "Large Language Model Distilling Medication Recommendation Model",
            "abstract": "The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the straightforward integration of LLMs into recommender systems leads to an out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a novel output layer and a refined tuning loss function. Although LLM-based models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector. To mitigate this, we have developed a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model. Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient. To ease the reproducibility of our experiments, we release the implementation code online.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2258675923",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2261673614",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2051691467",
                    "name": "Yuanshao Zhu"
                },
                {
                    "authorId": "2269463602",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2244621655",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "c497b1de58f9a53338ea3b7507cdc4d0056f48a1",
            "title": "M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework",
            "abstract": "Multi-domain recommendation and multi-task recommendation have demonstrated their effectiveness in leveraging common information from different domains and objectives for comprehensive user modeling. Nonetheless, the practical recommendation usually faces multiple domains and tasks simultaneously, which cannot be well-addressed by current methods. To this end, we introduce M3oE, an adaptive Multi-domain Multi-task Mixture-of-Experts recommendation framework. M3oE integrates multi-domain information, maps knowledge across domains and tasks, and optimizes multiple objectives. We leverage three mixture-of-experts modules to learn common, domain-aspect, and task-aspect user preferences respectively to address the complex dependencies among multiple domains and tasks in a disentangled manner. Additionally, we design a two-level fusion mechanism for precise control over feature extraction and fusion across diverse domains and tasks. The framework's adaptability is further enhanced by applying AutoML technique, which allows dynamic structure optimization. To the best of the authors' knowledge, our M3oE is the first effort to solve multi-domain multi-task recommendation self-adaptively. Extensive experiments on two benchmark datasets against diverse baselines demonstrate M3oE's superior performance. The implementation code is available to ensure reproducibility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298640483",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2244772979",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2298904141",
                    "name": "Jiaao Yu"
                },
                {
                    "authorId": "2244625023",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2244774353",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "7923634",
                    "name": "Chunxu Zhang"
                },
                {
                    "authorId": "2204700561",
                    "name": "Ziru Liu"
                },
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2244169478",
                    "name": "Hongwei Zhao"
                },
                {
                    "authorId": "2258334183",
                    "name": "Lantao Hu"
                },
                {
                    "authorId": "2244625137",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "2244624390",
                    "name": "Kun Gai"
                }
            ]
        },
        {
            "paperId": "f795f0b7380dbbbe8bd48f0c4505ba0d64155007",
            "title": "Large Language Model Empowered Embedding Generator for Sequential Recommendation",
            "abstract": "Sequential Recommender Systems (SRS) are extensively applied across various domains to predict users' next interaction by modeling their interaction sequences. However, these systems typically grapple with the long-tail problem, where they struggle to recommend items that are less popular. This challenge results in a decline in user discovery and reduced earnings for vendors, negatively impacting the system as a whole. Large Language Model (LLM) has the potential to understand the semantic connections between items, regardless of their popularity, positioning them as a viable solution to this dilemma. In our paper, we present LLMEmb, an innovative technique that harnesses LLM to create item embeddings that bolster the performance of SRS. To align the capabilities of general-purpose LLM with the needs of the recommendation domain, we introduce a method called Supervised Contrastive Fine-Tuning (SCFT). This method involves attribute-level data augmentation and a custom contrastive loss designed to tailor LLM for enhanced recommendation performance. Moreover, we highlight the necessity of incorporating collaborative filtering signals into LLM-generated embeddings and propose Recommendation Adaptation Training (RAT) for this purpose. RAT refines the embeddings to be optimally suited for SRS. The embeddings derived from LLMEmb can be easily integrated with any SRS model, showcasing its practical utility. Extensive experimentation on three real-world datasets has shown that LLMEmb significantly improves upon current methods when applied across different SRS models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2211473272",
                    "name": "Wanyu Wang"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2292569421",
                    "name": "Yuanshao Zhu"
                },
                {
                    "authorId": "2261673614",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2244621655",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "1fb8f2d080e965c833c777f06fccf09dc9856b91",
            "title": "MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications",
            "abstract": "\u2014The recent surge in the field of Large Language Models (LLMs) has gained significant attention in numerous domains. In order to tailor an LLM to a specific domain such as a web-based healthcare system, fine-tuning with domain knowledge is necessary. However, two issues arise during fine-tuning LLMs for medical applications. The first is the problem of task variety, where there are numerous distinct tasks in real-world medical scenarios. This diversity often results in suboptimal fine-tuning due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning can be prohibitive, impeding the application of LLMs. The large number of parameters in LLMs results in enormous time and computational consumption during fine-tuning, which is difficult to justify. To address these two issues simultaneously, we propose a novel parameter-efficient fine-tuning framework for multi-task medical applications called MOELoRA. The framework aims to capitalize on the benefits of both MOE for multi-task learning and LoRA for parameter-efficient fine-tuning. To unify MOE and LoRA, we devise multiple experts as the trainable parameters, where each expert consists of a pair of low-rank matrices to maintain a small number of trainable parameters. Additionally, we propose a task-motivated gate function for all MOELoRA layers that can regulate the contributions of each expert and generate distinct parameters for various tasks. To validate the effectiveness and practicality of the proposed method, we conducted comprehensive experiments on a public multi-task Chinese medical dataset. The experimental results demonstrate that MOELoRA outperforms existing parameter-efficient fine-tuning methods. The implementation is available online for convenient reproduction of our experiments 1 .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2290857499",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2261673614",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2292569421",
                    "name": "Yuanshao Zhu"
                },
                {
                    "authorId": "2262514619",
                    "name": "Derong Xu"
                },
                {
                    "authorId": "2244621655",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "2235030b6265e9b78924355fcbf8595fa664dafb",
            "title": "PromptST: Prompt-Enhanced Spatio-Temporal Multi-Attribute Prediction",
            "abstract": "In the era of information explosion, spatio-temporal data mining serves as a critical part of urban management. Considering the various fields demanding attention, e.g., traffic state, human activity, and social event, predicting multiple spatio-temporal attributes simultaneously can alleviate regulatory pressure and foster smart city construction. However, current research can not handle the spatio-temporal multi-attribute prediction well due to the complex relationships between diverse attributes. The key challenge lies in how to address the common spatio-temporal patterns while tackling their distinctions. In this paper, we propose an effective solution for spatio-temporal multi-attribute prediction, PromptST. We devise a spatio-temporal transformer and a parameter-sharing training scheme to address the common knowledge among different spatio-temporal attributes. Then, we elaborate a spatio-temporal prompt tuning strategy to fit the specific attributes in a lightweight manner. Through the pretrain and prompt tuning phases, our PromptST is able to enhance the specific spatio-temoral characteristic capture by prompting the backbone model to fit the specific target attribute while maintaining the learned common knowledge. Extensive experiments on real-world datasets verify that our PromptST attains state-of-the-art performance. Furthermore, we also prove PromptST owns good transferability on unseen spatio-temporal attributes, which brings promising application potential in urban computing. The implementation code is available to ease reproducibility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47295030",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2243037657",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "7923634",
                    "name": "Chunxu Zhang"
                },
                {
                    "authorId": "2244129522",
                    "name": "Qian Ma"
                },
                {
                    "authorId": "2211473272",
                    "name": "Wanyu Wang"
                },
                {
                    "authorId": "2146230626",
                    "name": "Hongwei Zhao"
                },
                {
                    "authorId": "2108941389",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "3195628",
                    "name": "Zitao Liu"
                }
            ]
        },
        {
            "paperId": "2bc796d5aff8e4323ebb7b2d62da34384f765e32",
            "title": "When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications",
            "abstract": "The recent surge in Large Language Models (LLMs) has garnered significant attention across numerous fields. Fine-tuning is often required to fit general LLMs for a specific domain, like the web-based healthcare system. However, two problems arise during fine-tuning LLMs for medical applications. One is the task variety problem, which involves distinct tasks in real-world medical scenarios. The variety often leads to sub-optimal fine-tuning for data imbalance and seesaw problems. Besides, the large amount of parameters in LLMs leads to huge time and computation consumption by fine-tuning. To address these two problems, we propose a novel parameter efficient fine-tuning framework for multi-task medical applications, dubbed as MOELoRA. The designed framework aims to absorb both the benefits of mixture-of-expert (MOE) for multi-task learning and low-rank adaptation (LoRA) for parameter efficient fine-tuning. For unifying MOE and LoRA, we devise multiple experts as the trainable parameters, where each expert consists of a pair of low-rank matrices to retain the small size of trainable parameters. Then, a task-motivated gate function for all MOELoRA layers is proposed, which can control the contributions of each expert and produce distinct parameters for various tasks. We conduct experiments on a multi-task medical dataset, indicating MOELoRA outperforms the existing parameter efficient fine-tuning methods. The code is available online.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2258675923",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2261673614",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2051691467",
                    "name": "Yuanshao Zhu"
                },
                {
                    "authorId": "2262514619",
                    "name": "Derong Xu"
                },
                {
                    "authorId": "2244621655",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        }
    ]
}