{
    "authorId": "151007075",
    "papers": [
        {
            "paperId": "5df7b276980dbd2987b0a0a42e1cc7100263f460",
            "title": "Ranking Across Different Content Types: The Robust Beauty of Multinomial Blending",
            "abstract": "An increasing number of media streaming services have expanded their offerings to include entities of multiple content types. For instance, audio streaming services that started by offering music only, now also offer podcasts, merchandise items, and videos. Ranking items across different content types into a single slate poses a significant challenge for traditional learning-to-rank (LTR) algorithms due to differing user engagement patterns for different content types. We explore a simple method for cross-content-type ranking, called multinomial blending (MB), which can be used in conjunction with most existing LTR algorithms. We compare MB to existing baselines not only in terms of ranking quality but also from other industry-relevant perspectives such as interpretability, ease-of-use, and stability in dynamic environments with changing user behavior and ranking model retraining. Finally, we report the results of an A/B test from an Amazon Music ranking use-case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060219456",
                    "name": "Jan Malte Lichtenberg"
                },
                {
                    "authorId": "151007075",
                    "name": "G. Benedetto"
                },
                {
                    "authorId": "39868478",
                    "name": "M. Ruffini"
                }
            ]
        },
        {
            "paperId": "df3e5ce919747174eb98747d7d8f7271b6ba6c77",
            "title": "Counterfactual Ranking Evaluation with Flexible Click Models",
            "abstract": "Evaluating a new ranking policy using data logged by a previously deployed policy requires a counterfactual (off-policy) estimator that corrects for presentation and selection biases. Some estimators (e.g., the position-based model ) perform this correction by making strong assumptions about user behavior, which can lead to high bias if the assumptions are not met. Other estimators (e.g., the item-position model ) rely on randomization to avoid these assumptions, but they often suffer from high variance. In this paper, we develop a new counterfactual estimator, called Interpol , that provides a tunable trade-off in the assumptions it makes, thus providing a novel ability to optimize the bias-variance trade-off. We analyze the bias of our estimator, both theoretically and empirically, and show that it achieves lower error than both the position-based model and the item-position model, on both synthetic and real datasets. This improvement in accuracy not only benefits offline evaluation of ranking policies, we also find that Interpol improves learning of new ranking policies when used as the training objective for learning-to-rank.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237805687",
                    "name": "Alexander Buchholz"
                },
                {
                    "authorId": "2237804555",
                    "name": "Ben London"
                },
                {
                    "authorId": "151007075",
                    "name": "G. Benedetto"
                },
                {
                    "authorId": "2060219456",
                    "name": "Jan Malte Lichtenberg"
                },
                {
                    "authorId": "2244753300",
                    "name": "Yannik Stein"
                },
                {
                    "authorId": "2237223150",
                    "name": "Thorsten Joachims"
                }
            ]
        },
        {
            "paperId": "9536a9b7b7b34fbefddc524ba1ddb90b65f23949",
            "title": "Double Clipping: Less-Biased Variance Reduction in Off-Policy Evaluation",
            "abstract": "\"Clipping\"(a.k.a. importance weight truncation) is a widely used variance-reduction technique for counterfactual off-policy estimators. Like other variance-reduction techniques, clipping reduces variance at the cost of increased bias. However, unlike other techniques, the bias introduced by clipping is always a downward bias (assuming non-negative rewards), yielding a lower bound on the true expected reward. In this work we propose a simple extension, called $\\textit{double clipping}$, which aims to compensate this downward bias and thus reduce the overall bias, while maintaining the variance reduction properties of the original estimator.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060219456",
                    "name": "Jan Malte Lichtenberg"
                },
                {
                    "authorId": "2237805687",
                    "name": "Alexander Buchholz"
                },
                {
                    "authorId": "151007075",
                    "name": "G. Benedetto"
                },
                {
                    "authorId": "39868478",
                    "name": "M. Ruffini"
                },
                {
                    "authorId": "2237804555",
                    "name": "Ben London"
                }
            ]
        },
        {
            "paperId": "1d74df29a192a33cf08f61d5eb5e09083e826f05",
            "title": "Low-variance estimation in the Plackett-Luce model via quasi-Monte Carlo sampling",
            "abstract": "The Plackett-Luce (PL) model is popular in learning-to-rank (LTR) because it provides a useful and intuitive probabilistic model for sampling ranked lists. Counterfactual offline evaluation and optimization of ranking metrics are pivotal for using LTR methods in production. When adopting the PL model as a ranking policy, both tasks require the computation of expectations with respect to the model. These are usually approximated via Monte-Carlo (MC) sampling, since the combinatorial scaling in the number of items to be ranked makes their analytical computation intractable. Despite recent advances in improving the computational efficiency of the sampling process via the Gumbel top-k trick [23], the MC estimates can suffer from high variance. We develop a novel approach to producing more sample-efficient estimators of expectations in the PL model by combining the Gumbel top-k trick with quasi-Monte Carlo (QMC) sampling, a well-established technique for variance reduction. We illustrate our findings both theoretically and empirically using real-world recommendation data from Amazon Music and the Yahoo learning-to-rank challenge.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "11843959",
                    "name": "Alexander Buchholz"
                },
                {
                    "authorId": "2060219456",
                    "name": "Jan Malte Lichtenberg"
                },
                {
                    "authorId": "151007075",
                    "name": "G. Benedetto"
                },
                {
                    "authorId": "3149848",
                    "name": "Yannik Stein"
                },
                {
                    "authorId": "31614629",
                    "name": "Vito Bellini"
                },
                {
                    "authorId": "39868478",
                    "name": "M. Ruffini"
                }
            ]
        },
        {
            "paperId": "3abf36a8f2a4d5ec1352f43d4b9e4e35670fee9d",
            "title": "Modeling Position Bias Ranking for Streaming Media Services",
            "abstract": "We tackle the problem of position bias estimation for streaming media services. Position bias is a widely studied topic in ranking literature and its impact on ranking quality is well understood. Although several methods exist to estimate position bias, their applicability to an industrial setting is limited, either because they require ad-hoc interventions that harm user experience, or because their learning accuracy is poor. In this paper, we present a novel position bias estimator that overcomes these limitations: it can be applied to streaming media services without manual interventions while delivering best in class estimation accuracy. We compare the proposed method against existing ones on real and synthetic data and illustrate its applicability to Amazon Music.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39868478",
                    "name": "M. Ruffini"
                },
                {
                    "authorId": "31614629",
                    "name": "Vito Bellini"
                },
                {
                    "authorId": "2064695624",
                    "name": "Alexander Buchholz"
                },
                {
                    "authorId": "151007075",
                    "name": "G. Benedetto"
                },
                {
                    "authorId": "3149848",
                    "name": "Yannik Stein"
                }
            ]
        },
        {
            "paperId": "863f76e1c7b9d73745a1077888ba53f82e36b5f7",
            "title": "Off-policy evaluation for learning-to-rank via interpolating the item-position model and the position-based model",
            "abstract": "A critical need for industrial recommender systems is the ability to evaluate recommendation policies offline, before deploying them to production. Unfortunately, widely used off-policy evaluation methods either make strong assumptions about how users behave that can lead to excessive bias, or they make fewer assumptions and suffer from large variance. We tackle this problem by developing a new estimator that mitigates the problems of the two most popular off-policy estimators for rankings, namely the position-based model and the item-position model. In particular, the new estimator, called INTERPOL, addresses the bias of a potentially misspecified position-based model, while providing an adaptable bias-variance trade-off compared to the item-position model. We provide theoretical arguments as well as empirical results that highlight the performance of our novel estimation approach. Off-policy for learning-to-rank interpolating the",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "11843959",
                    "name": "Alexander Buchholz"
                },
                {
                    "authorId": "2085850",
                    "name": "Ben London"
                },
                {
                    "authorId": "151007075",
                    "name": "G. Benedetto"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        },
        {
            "paperId": "0aa2b27dd0bb17c5bdd60ac5307c2fa431ebab7a",
            "title": "Ranker-agnostic Contextual Position Bias Estimation",
            "abstract": "Learning-to-rank (LTR) algorithms are ubiquitous and necessary to explore the extensive catalogs of media providers. To avoid the user examining all the results, its preferences are used to provide a subset of relatively small size. The user preferences can be inferred from the interactions with the presented content if explicit ratings are unavailable. However, directly using implicit feedback can lead to learning wrong relevance models and is known as biased LTR. The mismatch between implicit feedback and true relevances is due to various nuisances, with position bias one of the most relevant. Position bias models consider that the lack of interaction with a presented item is not only attributed to the item being irrelevant but because the item was not examined. This paper introduces a method for modeling the probability of an item being seen in different contexts, e.g., for different users, with a single estimator. Our suggested method, denoted as contextual (EM)-based regression, is ranker-agnostic and able to correctly learn the latent examination probabilities while only using implicit feedback. Our empirical results indicate that the method introduced in this paper outperforms other existing position bias estimators in terms of relative error when the examination probability varies across queries. Moreover, the estimated values provide a ranking performance boost when used to debias the implicit ranking data even if there is no context dependency on the examination probabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "115667891",
                    "name": "Oriol Barbany"
                },
                {
                    "authorId": "31614629",
                    "name": "Vito Bellini"
                },
                {
                    "authorId": "2064695624",
                    "name": "Alexander Buchholz"
                },
                {
                    "authorId": "151007075",
                    "name": "G. Benedetto"
                },
                {
                    "authorId": "10746977",
                    "name": "Diego Granziol"
                },
                {
                    "authorId": "39868478",
                    "name": "M. Ruffini"
                },
                {
                    "authorId": "3149848",
                    "name": "Yannik Stein"
                }
            ]
        },
        {
            "paperId": "bf9bee50d49cc40858a4959be378cf14ae5018d8",
            "title": "A Linear Bandit for Seasonal Environments",
            "abstract": "Contextual bandit algorithms are extremely popular and widely used in recommendation systems to provide online personalised recommendations. A recurrent assumption is the stationarity of the reward function, which is rather unrealistic in most of the real-world applications. In the music recommendation scenario for instance, people's music taste can abruptly change during certain events, such as Halloween or Christmas, and revert to the previous music taste soon after. \nWe would therefore need an algorithm which can promptly react to these changes. Moreover, we would like to leverage already observed rewards collected during different stationary periods which can potentially reoccur, without the need of restarting the learning process from scratch. A growing literature has addressed the problem of reward's non-stationarity, providing algorithms that could quickly adapt to the changing environment. However, up to our knowledge, there is no algorithm which deals with seasonal changes of the reward function. Here we present a contextual bandit algorithm which detects and adapts to abrupt changes of the reward function and leverages previous estimations whenever the environment falls back to a previously observed state. We show that the proposed method can outperform state-of-the-art algorithms for non-stationary environments. We ran our experiment on both synthetic and real datasets.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151007075",
                    "name": "G. Benedetto"
                },
                {
                    "authorId": "31614629",
                    "name": "Vito Bellini"
                },
                {
                    "authorId": "3024156",
                    "name": "Giovanni Zappella"
                }
            ]
        },
        {
            "paperId": "e74f4e9289660ab37cc662db78e47c3f3b142304",
            "title": "Non-exchangeable feature allocation models with sublinear growth of the feature sizes",
            "abstract": "Feature allocation models are popular models used in different applications such as unsupervised learning or network modeling. In particular, the Indian buffet process is a flexible and simple one-parameter feature allocation model where the number of features grows unboundedly with the number of objects. The Indian buffet process, like most feature allocation models, satisfies a symmetry property of exchangeability: the distribution is invariant under permutation of the objects. While this property is desirable in some cases, it has some strong implications. Importantly, the number of objects sharing a particular feature grows linearly with the number of objects. In this article, we describe a class of non-exchangeable feature allocation models where the number of objects sharing a given feature grows sublinearly, where the rate can be controlled by a tuning parameter. We derive the asymptotic properties of the model, and show that such model provides a better fit and better predictive performances on various datasets.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151007075",
                    "name": "G. Benedetto"
                },
                {
                    "authorId": "1774837",
                    "name": "F. Caron"
                },
                {
                    "authorId": "1725303",
                    "name": "Y. Teh"
                }
            ]
        }
    ]
}