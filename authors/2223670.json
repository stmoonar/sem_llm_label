{
    "authorId": "2223670",
    "papers": [
        {
            "paperId": "38d37fc0e471b7e7ade321bc839c04f4ab28a65a",
            "title": "MMoOn Core - the Multilingual Morpheme Ontology",
            "abstract": "In the last years a rapid emergence of lexical resources has evolved in the Semantic Web. Whereas most of the linguistic information is already machine-readable, we found that morphological information is mostly absent or only contained in semi-structured strings. An integration of morphemic data has not yet been undertaken due to the lack of existing domain-specific ontologies and explicit morphemic data. In this paper, we present the Multilingual Morpheme Ontology called MMoOn Core which can be regarded as the first comprehensive ontology for the linguistic domain of morphological language data. It will be described how crucial concepts like morphs, morphemes, word forms and meanings are represented and interrelated and how language-specific morpheme inventories can be created as a new possibility of morphological datasets. The aim of the MMoOn Core ontology is to serve as a shared semantic model for linguists and NLP researchers alike to enable the creation, conversion, exchange, reuse and enrichment of morphological language data across different data-dependent language sciences. Therefore, various use cases are illustrated to draw attention to the cross-disciplinary potential which can be realized with the MMoOn Core ontology in the context of the existing Linguistic Linked Data research landscape.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3448570",
                    "name": "Bettina Klimek"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "6016495c5a5d4040773395350a49150ada14e1c1",
            "title": "DBpedia Abstracts: A Large-Scale, Open, Multilingual NLP Training Corpus",
            "abstract": "The ever increasing importance of machine learning in Natural Language Processing is accompanied by an equally increasing need in large-scale training and evaluation corpora. Due to its size, its openness and relative quality, the Wikipedia has already been a source of such data, but on a limited scale. This paper introduces the DBpedia Abstract Corpus, a large-scale, open corpus of annotated Wikipedia texts in six languages, featuring over 11 million texts and over 97 million entity links. The properties of the Wikipedia texts are being described, as well as the corpus creation process, its format and interesting use-cases, like Named Entity Linking training and evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "8bee8378a10cb4ca545389e9d68646001a74648f",
            "title": "Assessing Quantity and Quality of Links Between Link Data Datasets",
            "abstract": "The Linked Data Web is growing and it becomes increasingly necessary to analyze the relationship between datasets to exploit its full value. LOD datasets can range from datasets with low cohesion \u2013 containing data from different Fully Qualified Domain Names (FQDN) and namespaces \u2013 to highly cohesive datasets. This paper evaluates the quantity and quality of links between distributions, datasets and ontologies categorizing and defining different types of links. We streamed and indexed 2.5 billion triples and extracted 0.5 billion links using probabilistic data structures. Our results show the analysis of datasets w.r.t. valid links, dead links, and number of namespaces described by distributions and datasets. Our results indicate that 7.9% of the links we indexed and verified are actually dead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32124073",
                    "name": "C. Baron"
                },
                {
                    "authorId": "2627116",
                    "name": "D. Kontokostas"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "2113612289",
                    "name": "Kay M\u00fcller"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                }
            ]
        },
        {
            "paperId": "e4d45a38390c892b8098899e25058b755fbd2608",
            "title": "LODVader: An Interface to LOD Visualization, Analyticsand DiscovERy in Real-time",
            "abstract": "The Linked Open Data (LOD) cloud is in danger of becoming a black box. Simple questions such as \"What kind of datasets are in the LOD cloud?\", \"In what way(s) are these datasets connected?\" -- albeit frequently asked -- are at the moment still difficult to answer due to the lack of proper tooling support. The infrequent update of the static LOD cloud diagram adds to the current dilemma, since there is neither reliable nor timely-updated information to perform an interactive search, analysis or in particular visualization in order to gain insight into the current state of Linked Open Data. In this paper, we propose a new hybrid system which combines LOD Visualisation, Analytics and DiscovERy (LODVader) to aid in answering the above questions. LODVader is equipped with (1) a multi-layer LOD cloud visualization component comprising datasets, subsets and vocabularies, (2) dataset analysis components that extend the state of the art with new similarity measures and efficient link extracting techniques and (3) a fast search index that is an entry point for dataset discovery. At its core, LODVader employs a timely-updated index using a complex cluster of Bloom filters as a fast search index with low memory footprint. This BF cluster is able to efficiently perform analysis on link and dataset similarities based on stored predicate and object information, which -- once inverted -- can be employed to discover invalid links by displaying the Dark LOD Cloud. By combining all these features, we allow for an up-to-date, multi-dimensional LOD cloud analysis, which -- to the best of our knowledge -- was not possible before.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32124073",
                    "name": "C. Baron"
                },
                {
                    "authorId": "2113612289",
                    "name": "Kay M\u00fcller"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "2627116",
                    "name": "D. Kontokostas"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "04dc1e72d8382cc3aa4fbbd5595e664b816d4981",
            "title": "Knowledge base shipping to the linked open data cloud",
            "abstract": "Popular knowledge bases that provide SPARQL endpoints for the web are usually experiencing a high number of requests, which often results in low availability of their interfaces. A common approach to counter the availability issue is to run a local mirror of the knowledge base. Running a SPARQL endpoint is currently a complex task which requires a lot of effort and technical support for domain experts who just want to use the SPARQL interface. With our approach of containerised knowledge base shipping we are introducing a simple to setup methodology for running a local mirror of an RDF knowledge base and SPARQL endpoint with interchangeable exploration components. The flexibility of the presented approach further helps maintaining the publication infrastructure for dataset projects. We are demonstrating and evaluating the presented methodology at the example of the dataset projects DBpedia, Catalogus Professorum Lipsiensium and S\u00e4chsisches Pfarrerbuch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258592",
                    "name": "Natanael Arndt"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "29356676",
                    "name": "Thomas Riechert"
                }
            ]
        },
        {
            "paperId": "0b9dea9414a3acbd5d00e00541f917f96be68e19",
            "title": "GERBIL: General Entity Annotator Benchmarking Framework",
            "abstract": "We present GERBIL, an evaluation framework for semantic entity annotation. The rationale behind our framework is to provide developers, end users and researchers with easy-to-use interfaces that allow for the agile, fine-grained and uniform evaluation of annotation tools on multiple datasets. By these means, we aim to ensure that both tool developers and end users can derive meaningful insights pertaining to the extension, integration and use of annotation applications. In particular, GERBIL provides comparable results to tool developers so as to allow them to easily discover the strengths and weaknesses of their implementations with respect to the state of the art. With the permanent experiment URIs provided by our framework, we ensure the reproducibility and archiving of evaluation results. Moreover, the framework generates data in machine-processable format, allowing for the efficient querying and post-processing of evaluation results. Finally, the tool diagnostics provided by GERBIL allows deriving insights pertaining to the areas in which tools should be further refined, thus allowing developers to create an informed agenda for extensions and end users to detect the right tools for their purposes. GERBIL aims to become a focal point for the state of the art, driving the research agenda of the community by presenting comparable objective evaluation results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2370666",
                    "name": "Ricardo Usbeck"
                },
                {
                    "authorId": "40506049",
                    "name": "Michael R\u00f6der"
                },
                {
                    "authorId": "1712107",
                    "name": "A. N. Ngomo"
                },
                {
                    "authorId": "32124073",
                    "name": "C. Baron"
                },
                {
                    "authorId": "1697447",
                    "name": "A. Both"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "40306775",
                    "name": "Diego Ceccarelli"
                },
                {
                    "authorId": "1843943",
                    "name": "M. Cornolti"
                },
                {
                    "authorId": "2946039",
                    "name": "D. Cherix"
                },
                {
                    "authorId": "49378270",
                    "name": "Bernd Eickmann"
                },
                {
                    "authorId": "1681278",
                    "name": "P. Ferragina"
                },
                {
                    "authorId": "12714015",
                    "name": "Christian Lemke"
                },
                {
                    "authorId": "144924686",
                    "name": "A. Moro"
                },
                {
                    "authorId": "1733928",
                    "name": "Roberto Navigli"
                },
                {
                    "authorId": "2174596",
                    "name": "Francesco Piccinno"
                },
                {
                    "authorId": "145971067",
                    "name": "Giuseppe Rizzo"
                },
                {
                    "authorId": "144196238",
                    "name": "Harald Sack"
                },
                {
                    "authorId": "1726618",
                    "name": "Ren\u00e9 Speck"
                },
                {
                    "authorId": "1684267",
                    "name": "Raphael Troncy"
                },
                {
                    "authorId": "2810514",
                    "name": "J. Waitelonis"
                },
                {
                    "authorId": "2613820",
                    "name": "L. Wesemann"
                }
            ]
        },
        {
            "paperId": "a49c7020e70009cf3f94b5b95c0268debed33700",
            "title": "Multilingual linked data",
            "abstract": "The interaction of natural language processing and the Semantic Web have lead to the creation of a new paradigm known as Linguistic Linked Open Data (LLOD), whereby traditional language resources are made available as linked data. Conversely, the publication of corpora, machine-readable dictionaries as linked data has opened new resources to Semantic Web researchers and allowed new tools to be developed. In this special issue, we present recent development of tools and resources for creating and publishing language resources as linked data and tools to exploit this data to enable a multilingual Semantic Web.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1689974",
                    "name": "John P. McCrae"
                },
                {
                    "authorId": "1491479767",
                    "name": "Steven Moran"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                }
            ]
        },
        {
            "paperId": "b182847f19e22823386e48f11c42c3f1bfaf223d",
            "title": "DataID: towards semantically rich metadata for complex datasets",
            "abstract": "The constantly growing amount of Linked Open Data (LOD) datasets constitutes the need for rich metadata descriptions, enabling users to discover, understand and process the available data. This metadata is often created, maintained and stored in diverse data repositories featuring disparate data models that are often unable to provide the metadata necessary to automatically process the datasets described. This paper proposes DataID, a best-practice for LOD dataset descriptions which utilize RDF files hosted together with the datasets, under the same domain. We are describing the data model, which is based on the widely used DCAT and VoID vocabularies, as well as supporting tools to create and publish DataIDs and use cases that show the benefits of providing semantically rich metadata for complex datasets. As a proof of concept, we generated a DataID for the DBpedia dataset, which we will present in the paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "32124073",
                    "name": "C. Baron"
                },
                {
                    "authorId": "3058606",
                    "name": "Ivan Ermilov"
                },
                {
                    "authorId": "36767711",
                    "name": "M. Freudenberg"
                },
                {
                    "authorId": "2627116",
                    "name": "D. Kontokostas"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "c7ee7a569d451ecd0a2fab9af5c168193580badb",
            "title": "NIF4OGGD - NLP Interchange Format for Open German Governmental Data",
            "abstract": "In the last couple of years the amount of structured open government data has increased significantly. Already now, citizens are able to leverage the advantages of open data through increased transparency and better opportunities to take part in governmental decision making processes. Our approach increases the interoperability of existing but distributed open governmental datasets by converting them to the RDF-based NLP Interchange Format (NIF). Furthermore, we integrate the converted data into a geodata store and present a user interface for querying this data via a keyword-based search. The language resource generated in this project is publicly available for download and also via a dedicated SPARQL endpoint.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37273320",
                    "name": "M. A. Sherif"
                },
                {
                    "authorId": "144318941",
                    "name": "Sandro Coelho"
                },
                {
                    "authorId": "2370666",
                    "name": "Ricardo Usbeck"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                },
                {
                    "authorId": "144568027",
                    "name": "Jens Lehmann"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                },
                {
                    "authorId": "1697447",
                    "name": "A. Both"
                }
            ]
        },
        {
            "paperId": "f35be0af7075d4ef6632ae41289ea79843c327ec",
            "title": "Lemon-aid: using Lemon to aid quantitative historical linguistic analysis",
            "abstract": "In this short paper, we describe how we converted dictionary and wordlist data made available by the QuantHistLing project into the Lexicon Model for Ontologies. By doing so, we leverage Linked Data to combine disparate lexical resources \u2013 more than fifty lexicons and dictionaries \u2013 by converting the lexical data into an RDFmodel that is specified by Lemon. The resulting new Linked Data resource, what we call the QHL dataset, provides researchers with a translation graph, which allows users to query across the underlying lexicons and dictionaries to extract semantically-aligned wordlists.",
            "fieldsOfStudy": [
                "Art",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491479767",
                    "name": "Steven Moran"
                },
                {
                    "authorId": "2223670",
                    "name": "Martin Br\u00fcmmer"
                }
            ]
        }
    ]
}