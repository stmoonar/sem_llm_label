{
    "authorId": "41209309",
    "papers": [
        {
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2203076",
                    "name": "Wenxuan Zhou"
                },
                {
                    "authorId": "41209309",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "227d0ec1231b951c06c8dc2992e4dcdd5cd62248",
            "title": "Interactive Span Recommendation for Biomedical Text",
            "abstract": "Motivated by the scarcity of high-quality labeled biomedical text, as well as the success of data programming, we introduce KRISS-Search. By leveraging the Unified Medical Language Systems (UMLS) ontology, KRISS-Search addresses an interactive few-shot span recommendation task that we propose. We first introduce unsupervised KRISS-Search and show that our method outperforms existing methods in identifying spans that are semantically similar to a given span of interest, with >50% AUPRC improvement relative to PubMedBERT. We then introduce supervised KRISS-Search, which leverages human interaction to improve the notion of similarity used by unsupervised KRISS-Search. Through simulated human feedback, we demonstrate an enhanced F1 score of 0.68 in classifying spans as semantically similar or different in the low-label setting, outperforming PubMedBERT by 2 F1 points. Finally, supervised KRISS-Search demonstrates competitive or superior performance compared to PubMedBERT in few-shot biomedical named entity recognition (NER) across five benchmark datasets, with an average improvement of 5.6 F1 points. We envision KRISS-Search increasing the efficiency of programmatic data labeling and also providing broader utility as an interactive biomedical search engine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "13794901",
                    "name": "Louis Blankemeier"
                },
                {
                    "authorId": "2220964895",
                    "name": "Theodore Zhao"
                },
                {
                    "authorId": "1846722967",
                    "name": "Robert Tinn"
                },
                {
                    "authorId": "39620434",
                    "name": "Sid Kiblawi"
                },
                {
                    "authorId": "2112677245",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2063969492",
                    "name": "Akshay Chaudhari"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "41209309",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "2059369124",
                    "name": "J. Preston"
                }
            ]
        },
        {
            "paperId": "3ede448b85b71790eda31de51ce663e5980eab78",
            "title": "Knowledge-Rich Self-Supervised Entity Linking",
            "abstract": "Entity linking faces signi\ufb01cant challenges, such as proli\ufb01c variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classi\ufb01cation approaches suffer from the annotation bottle-neck and cannot effectively handle unseen entities. Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia. In this paper, we explore Knowledge-RIch Self-Supervision ( KRISS ) for entity linking, by leveraging readily available domain knowledge. In training, it generates self-supervised mention examples on unlabeled text using a domain ontology and trains a contextual encoder using contrastive learning. For inference, it samples self-supervised mentions as prototypes for each entity and conducts linking by mapping the test mention to the most similar prototype. Our approach subsumes zero-shot and few-shot methods, and can easily incorporate entity descriptions and gold mention labels if available. Using biomedicine as a case study, we conducted extensive experiments on seven standard datasets spanning biomedical literature and clinical notes. With-out using any labeled information, our method produces KRISSBERT , a universal entity linker for four million UMLS entities, which attains new state of the art across the board, outper-forming prior best self-supervised methods by as much as over 20 absolute points in accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41209309",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "3404827",
                    "name": "Shikhar Vashishth"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "46851930",
                    "name": "Jinfeng Xiao"
                },
                {
                    "authorId": "2108860856",
                    "name": "Xiaodong Liu"
                },
                {
                    "authorId": "40466858",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        }
    ]
}