{
    "authorId": "2388459",
    "papers": [
        {
            "paperId": "1bb1606d09db36b2129355b44ca3b5fc0febd105",
            "title": "Diversity, Equity and Inclusion Activities in Database Conferences: A 2023 Report",
            "abstract": "The Diversity, Equity and Inclusion (DEI) initiative started as the Diversity/Inclusion initiative in 2020 [4]. The current report summarizes our activities in 2023.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "2302858948",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "1404555727",
                    "name": "Renata Borovica-Gajic"
                },
                {
                    "authorId": "2314293279",
                    "name": "Jes\u00fas Camacho-Rodr\u00edguez"
                },
                {
                    "authorId": "2314330906",
                    "name": "Jinli Cao"
                },
                {
                    "authorId": "2314297028",
                    "name": "Barbara Catania"
                },
                {
                    "authorId": "2249901748",
                    "name": "P. Chrysanthis"
                },
                {
                    "authorId": "2278429940",
                    "name": "Carlo Curino"
                },
                {
                    "authorId": "117266605",
                    "name": "A. El Abbadi"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2178387374",
                    "name": "Juliana Freire"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "51205357",
                    "name": "Sujaya Maiyya"
                },
                {
                    "authorId": "2266690266",
                    "name": "Alexandra Meliou"
                },
                {
                    "authorId": "37168010",
                    "name": "Madhulika Mohanty"
                },
                {
                    "authorId": "2257398736",
                    "name": "Fatma \u00d6zcan"
                },
                {
                    "authorId": "3139922",
                    "name": "L. Peterfreund"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "2314297947",
                    "name": "Sana Sellami"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2314670128",
                    "name": "Wang-Chiew Tan"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "2303255329",
                    "name": "Yuanyuan Tian"
                },
                {
                    "authorId": "1393643717",
                    "name": "Genoveva Vargas-Solar"
                },
                {
                    "authorId": "2314731432",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "2302886251",
                    "name": "Wenjie Zhang"
                }
            ]
        },
        {
            "paperId": "81b2834f3f884785e94a8c4c1b0bf66f858e0d97",
            "title": "The Image Calculator: 10x Faster Image-AI Inference by Replacing JPEG with Self-designing Storage Format",
            "abstract": "Numerous applications today rely on artificial intelligence over images. Image AI is, however, extremely expensive. In particular, the inference cost of image AI dominates the end-to-end cost. We observe that the image storage format lies at the root of the problem. Images today are predominantly stored in JPEG format. JPEG is a storage format designed for the human eye; it maximally compresses images without distorting the components of an image that are visible to the human eye. However, our observation is that during image AI, images are \"seen'' by algorithms, not humans. In addition, every AI application is different regarding which data components of the images are the most relevant. We present the Image Calculator, a self-designing image storage format that adapts to the given AI task, i.e., the specific neural network, the dataset, and the applications' specific accuracy, inference time, and storage requirements. Contrary to the state-of-the-art, the Image Calculator does not use a fixed storage format like JPEG. Instead, it designs and constructs a new storage format tailored to the context. It does so by constructing a massive design space of candidate storage formats from first principles, within which it searches efficiently using composite performance models (inference time, accuracy, storage). This way, it leverages the given AI task's unique characteristics to compress the data maximally. We evaluate the Image Calculator across a diverse set of data, image analysis tasks, AI models, and hardware. We show that the Image Calculator can generate image storage formats that reduce inference time by up to 14.2x and storage by up to 8.2x with a minimal loss in accuracy or gain, compared to JPEG and its state-of-the-art variants.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                }
            ]
        },
        {
            "paperId": "5215a52205943e08c886948dc7881938ec5ee230",
            "title": "Diversity, Equity and Inclusion Activities in Database Conferences: A 2022 Report",
            "abstract": "The Diversity, Equity and Inclusion (DEI) initiative started as the Diversity/Inclusion initiative in 2020 [4]. The current report summarizes our activities in 2022. Our responsibility as a community is to ensure that attendees of DB conferences feel included, irrespective of their scientific perspective and personal background. One of the first steps was to establish the role of the DEI chairs at DB Conferences, with the DEI team dedicated to providing leadership to help our community achieve this goal. In this leadership role, the DEI team is advising DEI chairs at DB conferences, serving as a memory of DEI events at conferences, building an agreed-upon vision, and committing to working together to devise a set of measures for achieving DEI. That is pursued via actions led by our core members (Figure 1) and liaisons of individual executive bodies (Figure 2): REACH OUT collects data and experiences from our community. INCLUDE monitors and recommends inclusion efforts. ORGANIZE focuses on in-conference organization efforts, such as adopting a code of conduct. INFORM communicates through various channels. SUPPORT coordinates DEI support from executive bodies and sponsors. SCOUT collates DEI efforts from other communities. COORDINATE manages all actions. Two new actions: MEDIA preserves and disseminates the digital media produced by DEI@DB events. ETHICS establishes and promotes ethics guidelines for publications in our community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "1401945543",
                    "name": "Jes\u00fas Camacho-Rodr\u00edguez"
                },
                {
                    "authorId": "1726425",
                    "name": "B. Catania"
                },
                {
                    "authorId": "2091879100",
                    "name": "K. Panos"
                },
                {
                    "authorId": "2081044488",
                    "name": "Chrysanthis"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "145025853",
                    "name": "J. Darmont"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                },
                {
                    "authorId": "1709353",
                    "name": "A. El Abbadi"
                },
                {
                    "authorId": "2223141115",
                    "name": "Avrilia"
                },
                {
                    "authorId": "2223141462",
                    "name": "Floratou"
                },
                {
                    "authorId": "2178387374",
                    "name": "Juliana Freire"
                },
                {
                    "authorId": "2153832",
                    "name": "Alekh Jindal"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "51205357",
                    "name": "Sujaya Maiyya"
                },
                {
                    "authorId": "2079019460",
                    "name": "Alexandra"
                },
                {
                    "authorId": "2223137576",
                    "name": "Meliou"
                },
                {
                    "authorId": "37168010",
                    "name": "Madhulika Mohanty"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "2149251297",
                    "name": "Fatma \u00d6zcan"
                },
                {
                    "authorId": "3139922",
                    "name": "L. Peterfreund"
                },
                {
                    "authorId": "145492471",
                    "name": "Wenny Rahayu"
                },
                {
                    "authorId": "2092368",
                    "name": "S. Sadiq"
                },
                {
                    "authorId": "2062657988",
                    "name": "Sana Sellami"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2131764025",
                    "name": "Wang-Chiew Tan"
                },
                {
                    "authorId": "2080239484",
                    "name": "Bhavani"
                },
                {
                    "authorId": "101679060",
                    "name": "Thuraisingham"
                },
                {
                    "authorId": "1392679676",
                    "name": "Neeraja"
                },
                {
                    "authorId": "2223141037",
                    "name": "Yadwadkar"
                },
                {
                    "authorId": "3010003",
                    "name": "Victor Zakhary"
                },
                {
                    "authorId": "2117848168",
                    "name": "Meihui Zhang"
                }
            ]
        },
        {
            "paperId": "ae728adf50ea299e480a6f7c28925baebb0f995d",
            "title": "Entropy-Learned Hashing: Constant Time Hashing with Controllable Uniformity",
            "abstract": "Hashing is a widely used technique for creating uniformly random numbers from arbitrary data. This is required in a large range of core data-driven operations including indexing, partitioning, filters, and sketches. As such, hashing is a core component in numerous systems including relational data systems, key-value stores, compilers, and networks. Due to both the computational and data heavy nature of hashing, it is a core systems bottleneck. For example, a typical database query in the standard TPC-H benchmark may spend 50% of its total cost in hash tables. Similarly, Google spends at least 2% of its total computational cost on C++ hash tables, resulting in a massive yearly cost footprint just from one hashing operation. We propose a new hashing method, called Entropy-Learned Hashing, which reduces the computational cost of hashing by up to an order of magnitude. We look at hashing from a pseudorandomness point of view and the key question we ask is ''how much randomness is needed?'' We show that state-of-the-art hash functions do too much work to perform their core task: extracting randomness from a data source to create random outputs. Entropy-Learned Hashing 1) models and estimates the randomness (entropy) of the input data, and then 2) creates data-specific hash functions that use only the parts of the data that are needed to differentiate the outputs. The resulting hash functions dramatically reduce the amount of computation needed while we prove their output is similarly uniform to that of traditional hash functions. We test Entropy-Learned Hashing across diverse and core hashing operations such as hash tables, Bloom filters, and partitioning and we demonstrate an increase in throughput in the order of 3.7x, 4.0x, and 14x respectively compared to the best in-class hash functions and implementations used at scale by Google and Meta. In this paper we propose a new method, called Entropy-Learned Hashing, which reduces the computational cost of hashing by up to an order of magnitude. The key question we ask is \"how much randomness is needed?'': We look at hashing from a pseudorandom point of view, wherein hashing is viewed as extracting randomness from a data source to create random outputs and we show that state-of-the-art hash functions do too much work. Entropy-Learned Hashing 1) models and estimates the randomness (entropy) of the input data, and then 2) creates data-specific hash functions that use only the parts of the data that are needed to differentiate the outputs. Thus the resulting hash functions can minimize the amount of computation needed while we prove that they act similarly to traditional hash functions in terms of the uniformity of their outputs. We test Entropy-Learned Hashing across diverse and core hashing operations such as hash tables, Bloom filters, and partitioning and we observe an increase in throughput in the order of 3.7X, 4.0X, and 14X respectively compared to the best in-class hash functions and implementations used at scale by Google and Meta.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064396353",
                    "name": "Brian Hentschel"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                }
            ]
        },
        {
            "paperId": "18f5511e59cdc5f9367eb55f775f58bce9f27378",
            "title": "Interference-aware Micro-architectural Resource Manager for Hybrid Workloads",
            "abstract": "Hybrid Transactional and Analytical Processing (HTAP) systems suffer from workload interference at the software and hardware level. We examine workload interference for HTAP systems and highlight investigation directions to mitigate the interference. We use the popular two-copy HTAP architecture. The OLTP and OLAP sides are independent components with their own private copies of the data. The OLTP side is a row-store, whereas the OLAP side is a column-store. The OLTP and OLAP sides are connected by means of an intermediate data structure, delta, that keeps track of the fresh tuples that are generated by the OLTP side, but not yet transferred to the OLAP side. OLTP transactions register their modifications to delta before committing. OLAP queries first propagate fresh tuples from the OLTP side to the OLAP side and then perform query execution over the data at the OLAP side [1]. Benchmarks & hardware. The OLTP benchmark is a transaction that randomly updates one row. The OLAP benchmark is either an aggregation or a join query. The database size is 30GB. We use a commodity Intel server with two CPU sockets, 14 cores per socket, 2 hyper-threads per core, and three levels of caches. The first two levels of caches are per-core, whereas the last-level cache (LLC) is per-socket, i.e., shared among the 14 cores of each socket. Software-level interference. OLAP execution time is composed of two parts: (i) fresh tuple propagation time and (ii) query processing time. Fresh tuple propagation time is considered software-level interference. We fix the number of fresh tuple propagation threads to 1 and the number of OLAP query processing threads to 10 and vary the number of OLTP threads. We place OLTP and OLAP threads and data on separate CPU sockets so that they do not interfere at the hardware level. The propagation thread and delta are placed on the same CPU socket as OLAP. For the aggregation query, fresh tuple propagation time is 2% and 23% of OLAP execution time with 1 and 7 OLTP threads. At these levels of OLTP throughput, software-level interference can be considered modest. Fresh tuple propagation time increases exponentially for 14 or more OLTP threads since the OLTP tuple generation throughput exceeds the fresh tuple propagation throughput of 1 propagation thread for 14 or more OLTP threads. In order to avoid this, software must ensure that tuple propagation throughput keeps pace with OLTP\u2019s tuple generation throughput. Hardware-level interference. Hardware-level interference is defined as the amount of throughput drop at the OLTP or OLAP side when running OLTP and OLAP concurrently on the same CPU socket compared to when running OLTP or OLAP alone.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "1727597",
                    "name": "S. Dwarkadas"
                },
                {
                    "authorId": "1728318",
                    "name": "A. Ailamaki"
                }
            ]
        },
        {
            "paperId": "77a315c3889767a4374781e75ceeb3cd751f860a",
            "title": "Performance Characterization of HTAP Workloads",
            "abstract": "Hybrid Transactional and Analytical Processing (HTAP) systems have become popular in the past decade. HTAP systems allow running transactional and analytical processing workloads on the same data and hardware. As a result, they suffer from workload interference. Despite the large body of existing work in HTAP systems and architectures, none of the existing work has systematically analyzed workload interference for HTAP systems.In this work, we characterize workload interference for HTAP systems. We show that the OLTP throughput drops by up to 42% due to sharing the hardware resources. Partitioning the last-level cache (LLC) among the OLTP and OLAP workloads can significantly improve the OLTP throughput without hurting the OLAP throughput. The OLAP throughput is significantly reduced due to sharing the data. The OLAP execution time is exponentially increased if the OLTP workload generates fresh tuples faster than the HTAP system propagates them. Therefore, in order to minimize the workload interference, HTAP systems should isolate the OLTP and OLAP workloads in the shared hardware resources and should allocate enough resources to fresh tuple propagation to propagate the fresh tuples faster than they are generated.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "1727597",
                    "name": "S. Dwarkadas"
                },
                {
                    "authorId": "1728318",
                    "name": "A. Ailamaki"
                }
            ]
        },
        {
            "paperId": "63977dbb1c332b37f939821777e54759ce1fa640",
            "title": "Micro-architectural analysis of OLAP",
            "abstract": "Understanding micro-architectural behavior is important for efficiently using hardware resources. Recent work has shown that in-memory online transaction processing (OLTP) systems severely underutilize their core micro-architecture resources [29]. Whereas, online analytical processing (OLAP) workloads exhibit a completely different computing pattern. OLAP workloads are read-only, bandwidth-intensive, and include various data access patterns. With the rise of column-stores, they run on high-performance engines that are tightly optimized for modern hardware. Consequently, micro-architectural behavior of modern OLAP systems remains unclear. This work presents a micro-architectural analysis of a set of OLAP systems. The results show that traditional commercial OLAP systems suffer from their long instruction footprint, which results in high response times. High-performance columnstores execute tight instruction streams; however, they spend 25 to 82% of their CPU cycles on stalls both for sequential- and random-access-heavy workloads. Concurrent query execution can improve the utilization, but it creates interference in the shared resources, which results in sub-optimal performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "1728318",
                    "name": "A. Ailamaki"
                }
            ]
        },
        {
            "paperId": "041a23aac72f9b7648dd09c123b7ca5df031dde0",
            "title": "Energy-efficient Database Machines",
            "abstract": "Database workloads severely underutilize their micro-architectural resources. This results in inefficient use of the chip's limited power budget. While ARM's server-grade processor is a promising energy-efficient alternative, it nevertheless wastes significant portion of the power-budget when running a database workload. In this paper, we systematically explore the ways of improving energy efficiency when running database workloads. We start by describing software-only and hardware-only optimizations, and then discuss two hardware/software co-design approaches. We finally propose combining a diverse set of hardware/software co-designed systems into a heterogeneous, energy-efficient database machine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                }
            ]
        },
        {
            "paperId": "5287c962a44335d3493f1dee6388fbbff27517ad",
            "title": "Batch Mode TD($\\lambda$ ) for Controlling Partially Observable Gene Regulatory Networks",
            "abstract": "External control of gene regulatory networks (GRNs) has received much attention in recent years. The aim is to find a series of actions to apply to a gene regulation system making it avoid its diseased states. In this work, we propose a novel method for controlling partially observable GRNs combining batch mode reinforcement learning (Batch RL) and TD(<inline-formula><tex-math notation=\"LaTeX\">$\\lambda$</tex-math><alternatives> <inline-graphic xlink:href=\"polat-ieq2-2595577.gif\"/></alternatives></inline-formula>) algorithms. Unlike the existing studies inferring a computational model from gene expression data, and obtaining a control policy over the constructed model, our idea is to interpret the time series gene expression data as a sequence of observations that the system produced, and obtain an approximate stochastic policy directly from the gene expression data without estimation of the internal states of the partially observable environment. Thereby, we get rid of the most time consuming phases of the existing studies, inferring a model and running the model for the control. Results show that our method is able to provide control solutions for regulation systems of several thousands of genes only in seconds, whereas existing studies cannot solve control problems of even a few dozens of genes. Results also show that our approximate stochastic policies are almost as good as the policies generated by the existing studies.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "1761620",
                    "name": "Faruk Polat"
                },
                {
                    "authorId": "144451975",
                    "name": "R. Alhajj"
                }
            ]
        },
        {
            "paperId": "766f77f05210fbdaf6be38503c1ce6637835205b",
            "title": "A methodology for OLTP micro-architectural analysis",
            "abstract": "Micro-architectural analysis is critical to investigate the interaction between workloads and processors. While today's aggressive out-of-order processors provide a rich set of performance events for deep execution cycle analysis, OLTP characterization studies usually use a cache-miss-based method (CMBM). In this work, we investigate the validity and the functionality of CMBM by comparing it with Intel's state-of-the-art Top-down Micro-architecture Analysis Method (TMAM) for OLTP workloads. We show that, while CMBM and TMAM provide a similar high-level micro-architectural behavior, it is inadequate for a fine-grained micro-architectural analysis. We further show that TMAM underestimates memory stalls. We optimize TMAM's execution cycle breakdown, and improve its estimation of memory stalls up to 50%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "143754027",
                    "name": "Ahmad Yasin"
                },
                {
                    "authorId": "1728318",
                    "name": "A. Ailamaki"
                }
            ]
        }
    ]
}