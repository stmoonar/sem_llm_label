{
    "authorId": "2144408471",
    "papers": [
        {
            "paperId": "667a30b3d77da9e8a6a0643ce9eca0204ebcf323",
            "title": "On the Efficiency and Robustness of Vibration-Based Foundation Models for IoT Sensing: A Case Study",
            "abstract": "This paper demonstrates the potential of vibration-based Foundation Models (FMs), pre-trained with unlabeled sensing data, to improve the robustness of run-time inference in (a class of) IoT applications. A case study is presented featuring a vehicle classification application using acoustic and seismic sensing. The work is motivated by the success of foundation models in the areas of natural language processing and computer vision, leading to generalizations of the FM concept to other domains as well, where significant amounts of unlabeled data exist that can be used for self-supervised pre-training. One such domain is IoT applications. Foundation models for selected sensing modalities in the IoT domain can be pre-trained in an environment-agnostic fashion using available unlabeled sensor data and then fine-tuned to the deployment at hand using a small amount of labeled data. The paper shows that the pre-training/fine-tuning approach improves the robustness of downstream inference and facilitates adaptation to different environmental conditions. More specifically, we present a case study in a real-world setting to evaluate a simple (vibration-based) FM-like model, called FOCAL, demonstrating its superior robustness and adaptation, compared to conventional supervised deep neural networks (DNNs). We also demonstrate its superior convergence over supervised solutions. Our findings highlight the advantages of vibration-based FMs (and FM-inspired self-supervised models in general) in terms of inference robustness, runtime efficiency, and model adaptation (via fine-tuning) in resource-limited IoT settings.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2263447798",
                    "name": "Tomoyoshi Kimura"
                },
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "49981269",
                    "name": "Tianshi Wang"
                },
                {
                    "authorId": "2202568267",
                    "name": "Denizhan Kara"
                },
                {
                    "authorId": "2237088718",
                    "name": "Yizhuo Chen"
                },
                {
                    "authorId": "2295021253",
                    "name": "Yigong Hu"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2276530877",
                    "name": "Maggie B. Wigness"
                },
                {
                    "authorId": "2237386361",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "2255351946",
                    "name": "Mani Srivastava"
                },
                {
                    "authorId": "2257216991",
                    "name": "Suhas Diggavi"
                },
                {
                    "authorId": "2287824378",
                    "name": "Tarek F. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "72c960bade39cefde0123d709ce5156dac80e086",
            "title": "Large Language Model-Guided Disentangled Belief Representation Learning on Polarized Social Graphs",
            "abstract": "The paper advances belief representation learning in polarized networks \u2013 the mapping of social beliefs espoused by users and posts in a polarized network into a disentangled latent space that separates (the members and beliefs of) each side. Our prior work embeds social interaction data, using non-negative variational graph auto-encoders, into a disentangled latent space. However, the interaction graphs alone may not adequately reflect similarity and/or disparity in beliefs, especially for those graphs with sparsity and outlier issues. In this paper, we investigate the impact of limited guidance from Large Language Models (LLMs) on the accuracy of belief separation. Specifically, we integrate social graphs with LLM-based soft labels as a novel weakly-supervised interpretable graph representation learning framework. This framework combines the strengths of graph-and text-based information, and is shown to maintain the interpretability of learned representations, where different axes in the latent space denote association with different sides of the divide. An evaluation on six real-world Twitter datasets illustrates the effectiveness of the proposed model at solving stance detection problems, demonstrating 5.9%-6.5% improvements in the accuracy, F1 score, and purity metrics, without introducing a significant computational overhead. An ablation study is also discussed to study the impact of different components of the proposed architecture.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2248195405",
                    "name": "Jinning Li"
                },
                {
                    "authorId": "2316747409",
                    "name": "Ruipeng Han"
                },
                {
                    "authorId": "2316893411",
                    "name": "Chenkai Sun"
                },
                {
                    "authorId": "1630209136",
                    "name": "Dachun Sun"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2316895195",
                    "name": "Jingying Zeng"
                },
                {
                    "authorId": "2274018166",
                    "name": "Yuchen Yan"
                },
                {
                    "authorId": "2278450099",
                    "name": "Hanghang Tong"
                },
                {
                    "authorId": "2158570329",
                    "name": "Tarek F. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "8525456077afdb1b4282132d70418c9eda062b2d",
            "title": "Acies-OS: A Content-Centric Platform for Edge AI Twinning and Orchestration",
            "abstract": "This paper describes Acies-OS, a content-centric platform for edge AI twinning and orchestration that allows easy deployment, re-configuration, and control of edge AI services, augmented by a digital twin. The work is motivated by the proliferation of edge AI in a plethora of IoT applications, ranging from home automation to military defense, and the emergence of digital twins that go beyond monitoring and emulation into configuration management and optimization of edge capabilities. While past work focused on either the edge capabilities themselves or the digital twin, this work focuses on their seamless interactions, offering abstractions that enable the digital twin to manage and optimize an increasingly diverse edge AI system. Acies-OS features a structured namespace, a thin client library with flexible pub/sub-based communication, health monitoring support, and a control plane for twin-based value-added analysis and optimization. To illustrate the use of Acies-OS, we implemented a multi-node multi-modality vehicle classification application and used Acies-OS to interface it to a digital twin. We then deployed the system in the field to showcase run-time twin-based optimizations of inference latency, classification accuracy, and robustness to failures in noisy and challenging conditions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "2237088718",
                    "name": "Yizhuo Chen"
                },
                {
                    "authorId": "2263447798",
                    "name": "Tomoyoshi Kimura"
                },
                {
                    "authorId": "49981269",
                    "name": "Tianshi Wang"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2202568267",
                    "name": "Denizhan Kara"
                },
                {
                    "authorId": "2295021253",
                    "name": "Yigong Hu"
                },
                {
                    "authorId": "2244138467",
                    "name": "Li Wu"
                },
                {
                    "authorId": "35723919",
                    "name": "Walid A. Hanafy"
                },
                {
                    "authorId": "2316790364",
                    "name": "Abel Souza"
                },
                {
                    "authorId": "2267768636",
                    "name": "Prashant Shenoy"
                },
                {
                    "authorId": "2276530877",
                    "name": "Maggie B. Wigness"
                },
                {
                    "authorId": "2174726260",
                    "name": "Joydeep Bhattacharyya"
                },
                {
                    "authorId": "2184218337",
                    "name": "Jae H. Kim"
                },
                {
                    "authorId": "2155124410",
                    "name": "Guijun Wang"
                },
                {
                    "authorId": "2274897599",
                    "name": "Greg Kimberly"
                },
                {
                    "authorId": "2202582720",
                    "name": "Josh D. Eckhardt"
                },
                {
                    "authorId": "9350980",
                    "name": "Denis Osipychev"
                },
                {
                    "authorId": "2158570329",
                    "name": "Tarek F. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "caeb485e5481f0d8285852acc1fdefa5d133c9e7",
            "title": "Data Augmentation for Human Activity Recognition via Condition Space Interpolation within a Generative Model",
            "abstract": "This paper presents a generative data augmentation approach for human activity recognition (HAR) to close the distribution gap between laboratory training and real-world deployment. Despite the recent success of deep learning methods in wearable sensor-based HAR tasks, performance degradation occurs during real-world deployment due to training data scarcity and the vast variability in human activities. In light of this, we aim to enhance the diversity of training datasets by generating new data points within the vicinity of existing samples, as informed by domain expertise. Unlike the commonly utilized methods that augment data by interpolating in data space or feature space, we innovate by applying interpolation in the condition space of a conditional generative model to augment HAR datasets. We use domain-specific knowledge to extract statistical metrics from sensor data, which serve as conditions to direct the generation process. We demonstrate how a conditional generative diffusion model, steered by interpolated conditions, can synthesize realistic new data with various high-level features that benefit the robustness of the downstream HAR models. Our methodology advances the use of interpolation in data augmentation by exploring the capability of a state-of-the-art generative model, offering novel perspectives for bolstering the robustness and generalizability of HAR systems. Experimental results demonstrate that condition space interpolation outperforms the conventional interpolation-based and generative model-based augmentation methods across various datasets and downstream classifier combinations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49981269",
                    "name": "Tianshi Wang"
                },
                {
                    "authorId": "2237088718",
                    "name": "Yizhuo Chen"
                },
                {
                    "authorId": "2249087809",
                    "name": "Qikai Yang"
                },
                {
                    "authorId": "1630209136",
                    "name": "Dachun Sun"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "2263447798",
                    "name": "Tomoyoshi Kimura"
                },
                {
                    "authorId": "2158570329",
                    "name": "Tarek F. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "e726fda0c22e1f8d554abd2e7e386b9bdaf1dfb2",
            "title": "FreqMAE: Frequency-Aware Masked Autoencoder for Multi-Modal IoT Sensing",
            "abstract": "This paper presents FreqMAE, a novel self-supervised learning framework that synergizes masked autoencoding (MAE) with physics-informed insights to capture feature patterns in multi-modal IoT sensor data. FreqMAE enhances latent space representation of sensor data, reducing reliance on data labeling and improving accuracy for AI tasks. Differing from data augmentation-based methods like contrastive learning, FreqMAE's approach eliminates the need for handcrafted transformations. Adapting MAE for IoT sensing signals, we present three contributions from frequency domain insights: First, a Temporal-Shifting Transformer (TS-T) encoder that enables temporal interactions while distinguishing different frequency bands; Second, a factorized multi-modal fusion mechanism for leveraging cross-modal correlations and preserving unique modality features; Third, a hierarchically weighted loss function that emphasizes important frequency components and high Signal-to-Noise Ratio (SNR) samples. Comprehensive evaluations on two sensing applications validate FreqMAE's proficiency in reducing labeling needs and enhancing resilience against domain shifts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2202568267",
                    "name": "Denizhan Kara"
                },
                {
                    "authorId": "2263447798",
                    "name": "Tomoyoshi Kimura"
                },
                {
                    "authorId": "2237386361",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "153626198",
                    "name": "Dongxin Liu"
                },
                {
                    "authorId": "49981269",
                    "name": "Tianshi Wang"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2237088718",
                    "name": "Yizhuo Chen"
                },
                {
                    "authorId": "2295021253",
                    "name": "Yigong Hu"
                },
                {
                    "authorId": "2287824378",
                    "name": "Tarek F. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "28ad57d03f523833a25565d84472e17bc040d380",
            "title": "Influence Pathway Discovery on Social Media",
            "abstract": "This paper addresses influence pathway discovery, a key emerging problem in today's online media. We propose a discovery algorithm that leverages recently published work on unsupervised interpretable ideological embedding, a mapping of ideological beliefs (done in a self-supervised fashion) into interpretable low-dimensional spaces. Computing the ideological embedding at scale allows one to analyze correlations between the ideological positions of leaders, influencers, news portals, or population segments, deriving potential influence pathways. The work is motivated by the importance of social media as the preeminent means for global interactions and collaborations on today's Internet, as well as their frequent (mis-)use to wield influence that targets social beliefs and attitudes of selected populations. Tools that enable the understanding and mapping of influence propagation through population segments on social media are therefore increasingly important. In this paper, influence is measured by the perceived ideological shift over time that is correlated with influencers' activity. Correlated shifts in ideological embed dings indicate changes, such as swings/switching (among competing ideologies), polarization (depletion of neutral ideological positions), escalation/radicalization (shifts to more extreme versions of the ideology), or unification/cooldown (shifts towards more neutral stances). Case-studies are presented to explore selected influence pathways (i) in a recent French election, (ii) during political discussions in the Philippines, and (iii) for some Russian messaging during the Russia/Ukraine conflict.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100683845",
                    "name": "Xinyi Liu"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "1630209136",
                    "name": "Dachun Sun"
                },
                {
                    "authorId": "2248195405",
                    "name": "Jinning Li"
                },
                {
                    "authorId": "2248177502",
                    "name": "Christina Youn"
                },
                {
                    "authorId": "2247787227",
                    "name": "You Lyu"
                },
                {
                    "authorId": "2247935542",
                    "name": "Jianyuan Zhan"
                },
                {
                    "authorId": "2248172859",
                    "name": "Dayou Wu"
                },
                {
                    "authorId": "2248489488",
                    "name": "Xinhe Xu"
                },
                {
                    "authorId": "2248464804",
                    "name": "Mingjun Liu"
                },
                {
                    "authorId": "2247792630",
                    "name": "Xinshuo Lei"
                },
                {
                    "authorId": "2248932602",
                    "name": "Zhihao Xu"
                },
                {
                    "authorId": "2247935201",
                    "name": "Yutong Zhang"
                },
                {
                    "authorId": "2248183359",
                    "name": "Zehao Li"
                },
                {
                    "authorId": "2249087809",
                    "name": "Qikai Yang"
                },
                {
                    "authorId": "2158570329",
                    "name": "Tarek F. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "5bba9dfb9b8f12ff105498fd192bb04f95ce6514",
            "title": "Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning",
            "abstract": "This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both \\textit{false negative issue} (i.e., potential true facts being excluded) and \\textit{false positive issue} (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as a noisy Positive-Unlabeled learning problem. We propose a variational framework, namely nPUGraph, that jointly estimates the correctness of both collected and uncollected facts (which we call \\textit{label posterior}) and updates model parameters during training. The label posterior estimation facilitates speculative reasoning from two perspectives. First, it improves the robustness of a label posterior-aware graph encoder against false positive links. Second, it identifies missing facts to provide high-quality grounds of reasoning. They are unified in a simple yet effective self-training procedure. Empirically, extensive experiments on three benchmark KG and one Twitter dataset with various degrees of false negative/positive cases demonstrate the effectiveness of nPUGraph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2118424577",
                    "name": "Baoyu Li"
                },
                {
                    "authorId": "2156141201",
                    "name": "Yichen Lu"
                },
                {
                    "authorId": "1630209136",
                    "name": "Dachun Sun"
                },
                {
                    "authorId": "2109377974",
                    "name": "Jinning Li"
                },
                {
                    "authorId": "2109341751",
                    "name": "Yuchen Yan"
                },
                {
                    "authorId": "1390628682",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "2058143613",
                    "name": "H. Tong"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "69828bf2eb40d7fbbdc2421d7ee60d838972f505",
            "title": "Unsupervised Image Classification by Ideological Affiliation from User-Content Interaction Patterns",
            "abstract": "The proliferation of political memes in modern information campaigns calls for efficient solutions for image classification by ideological affiliation. While significant advances have recently been made on text classification in modern natural language processing literature, understanding the political insinuation in imagery is less developed due to the hard nature of the problem. Unlike text, where meaning arises from juxtaposition of tokens (words) within some common linguistic structures, image semantics emerge from a much less constrained process of fusion of visual concepts. Thus, training a model to infer visual insinuation is possibly a more challenging problem. In this paper, we explore an alternative unsupervised approach that, instead, infers ideological affiliation from image propagation patterns on social media. The approach is shown to improve the F1-score by over 0.15 (nearly 25%) over previous unsupervised baselines, and then by another 0.05 (around 7%) in the presence of a small amount of supervision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100683845",
                    "name": "Xinyi Liu"
                },
                {
                    "authorId": "2109377974",
                    "name": "Jinning Li"
                },
                {
                    "authorId": "1630209136",
                    "name": "Dachun Sun"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                },
                {
                    "authorId": "2218438509",
                    "name": "Matt Brown"
                },
                {
                    "authorId": "2218883644",
                    "name": "Anthony Barricelli"
                },
                {
                    "authorId": "2113490",
                    "name": "Matthias Kirchner"
                },
                {
                    "authorId": "32865856",
                    "name": "Arslan Basharat"
                }
            ]
        },
        {
            "paperId": "9597d710c4b083baf1ae062b6ce9bdc619f91051",
            "title": "SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach",
            "abstract": "This paper introduces SudokuSens, a generative framework for automated generation of training data in machine-learning-based Internet-of-Things (IoT) applications, such that the generated synthetic data mimic experimental configurations not encountered during actual sensor data collection. The framework improves the robustness of resulting deep learning models, and is intended for IoT applications where data collection is expensive. The work is motivated by the fact that IoT time-series data entangle the signatures of observed objects with the confounding intrinsic properties of the surrounding environment and the dynamic environmental disturbances experienced. To incorporate sufficient diversity into the IoT training data, one therefore needs to consider a combinatorial explosion of training cases that are multiplicative in the number of objects considered and the possible environmental conditions in which such objects may be encountered. Our framework substantially reduces these multiplicative training needs. To decouple object signatures from environmental conditions, we employ a Conditional Variational Autoencoder (CVAE) that allows us to reduce data collection needs from multiplicative to (nearly) linear, while synthetically generating (data for) the missing conditions. To obtain robustness with respect to dynamic disturbances, a session-aware temporal contrastive learning approach is taken. Integrating the aforementioned two approaches, SudokuSens significantly improves the robustness of deep learning for IoT applications. We explore the degree to which SudokuSens benefits downstream inference tasks in different data sets and discuss conditions under which the approach is particularly effective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49981269",
                    "name": "Tianshi Wang"
                },
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2202568267",
                    "name": "Denizhan Kara"
                },
                {
                    "authorId": "1390628682",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "2282540347",
                    "name": "Davis Wertheimer"
                },
                {
                    "authorId": "2305841881",
                    "name": "Antoni Viros i Martin"
                },
                {
                    "authorId": "143642366",
                    "name": "R. Ganti"
                },
                {
                    "authorId": "1718467",
                    "name": "M. Srivatsa"
                },
                {
                    "authorId": "2158570329",
                    "name": "Tarek F. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "d11a83665286cccec8f56d632c0079c86ff240f3",
            "title": "FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space",
            "abstract": "This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a transformation-invariant objective. Second, we propose a temporal structural constraint for modality features, such that the average distance between temporally neighboring samples is no larger than that of temporally distant samples. Extensive evaluations are performed on four multimodal sensing datasets with two backbone encoders and two classifiers to demonstrate the superiority of FOCAL. It consistently outperforms the state-of-the-art baselines in downstream tasks with a clear margin, under different ratios of available labels. The code and self-collected dataset are available at https://github.com/tomoyoshki/focal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1390628682",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "2263447798",
                    "name": "Tomoyoshi Kimura"
                },
                {
                    "authorId": "153626198",
                    "name": "Dongxin Liu"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "2257216991",
                    "name": "Suhas Diggavi"
                },
                {
                    "authorId": "2255351946",
                    "name": "Mani Srivastava"
                },
                {
                    "authorId": "2158570329",
                    "name": "Tarek F. Abdelzaher"
                }
            ]
        }
    ]
}