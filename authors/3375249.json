{
    "authorId": "3375249",
    "papers": [
        {
            "paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
            "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113243762",
                    "name": "Hugo Touvron"
                },
                {
                    "authorId": "143792623",
                    "name": "Louis Martin"
                },
                {
                    "authorId": "2059203763",
                    "name": "Kevin R. Stone"
                },
                {
                    "authorId": "2214809450",
                    "name": "Peter Albert"
                },
                {
                    "authorId": "2634674",
                    "name": "Amjad Almahairi"
                },
                {
                    "authorId": "2223764353",
                    "name": "Yasmine Babaei"
                },
                {
                    "authorId": "2223756247",
                    "name": "Nikolay Bashlykov"
                },
                {
                    "authorId": "47505161",
                    "name": "Soumya Batra"
                },
                {
                    "authorId": "51229603",
                    "name": "Prajjwal Bhargava"
                },
                {
                    "authorId": "2116473",
                    "name": "Shruti Bhosale"
                },
                {
                    "authorId": "2023469",
                    "name": "D. Bikel"
                },
                {
                    "authorId": "2040305955",
                    "name": "Lukas Blecher"
                },
                {
                    "authorId": "66286536",
                    "name": "Cristian Cant\u00f3n Ferrer"
                },
                {
                    "authorId": "2108267192",
                    "name": "Moya Chen"
                },
                {
                    "authorId": "7153363",
                    "name": "Guillem Cucurull"
                },
                {
                    "authorId": "71039937",
                    "name": "David Esiobu"
                },
                {
                    "authorId": "2166312768",
                    "name": "Jude Fernandes"
                },
                {
                    "authorId": "2223974989",
                    "name": "Jeremy Fu"
                },
                {
                    "authorId": "2223742000",
                    "name": "Wenyin Fu"
                },
                {
                    "authorId": "2223748737",
                    "name": "Brian Fuller"
                },
                {
                    "authorId": "2107063269",
                    "name": "Cynthia Gao"
                },
                {
                    "authorId": "28554843",
                    "name": "Vedanuj Goswami"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "4305645",
                    "name": "A. Hartshorn"
                },
                {
                    "authorId": "2195458",
                    "name": "Saghar Hosseini"
                },
                {
                    "authorId": "2132302721",
                    "name": "Rui Hou"
                },
                {
                    "authorId": "2065277797",
                    "name": "Hakan Inan"
                },
                {
                    "authorId": "2059886128",
                    "name": "Marcin Kardas"
                },
                {
                    "authorId": "2190957318",
                    "name": "Viktor Kerkez"
                },
                {
                    "authorId": "2072010",
                    "name": "Madian Khabsa"
                },
                {
                    "authorId": "2207049",
                    "name": "Isabel M. Kloumann"
                },
                {
                    "authorId": "2135297476",
                    "name": "A. Korenev"
                },
                {
                    "authorId": "2146367061",
                    "name": "Punit Singh Koura"
                },
                {
                    "authorId": "114952298",
                    "name": "Marie-Anne Lachaux"
                },
                {
                    "authorId": "46183616",
                    "name": "Thibaut Lavril"
                },
                {
                    "authorId": "2223749565",
                    "name": "Jenya Lee"
                },
                {
                    "authorId": "2145259939",
                    "name": "Diana Liskovich"
                },
                {
                    "authorId": "1768032",
                    "name": "Yinghai Lu"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "1490887583",
                    "name": "Xavier Martinet"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "3047561",
                    "name": "Pushkar Mishra"
                },
                {
                    "authorId": "66839644",
                    "name": "Igor Molybog"
                },
                {
                    "authorId": "40383658",
                    "name": "Yixin Nie"
                },
                {
                    "authorId": "38579672",
                    "name": "Andrew Poulton"
                },
                {
                    "authorId": "39906022",
                    "name": "Jeremy Reizenstein"
                },
                {
                    "authorId": "150282885",
                    "name": "Rashi Rungta"
                },
                {
                    "authorId": "1859294",
                    "name": "Kalyan Saladi"
                },
                {
                    "authorId": "14279694",
                    "name": "Alan Schelten"
                },
                {
                    "authorId": "2214818043",
                    "name": "Ruan Silva"
                },
                {
                    "authorId": "51324296",
                    "name": "Eric Michael Smith"
                },
                {
                    "authorId": "2066074360",
                    "name": "R. Subramanian"
                },
                {
                    "authorId": "2112782199",
                    "name": "Xia Tan"
                },
                {
                    "authorId": "71292072",
                    "name": "Binh Tang"
                },
                {
                    "authorId": "2110697298",
                    "name": "Ross Taylor"
                },
                {
                    "authorId": "2110032535",
                    "name": "Adina Williams"
                },
                {
                    "authorId": "2223770369",
                    "name": "Jian Xiang Kuan"
                },
                {
                    "authorId": "2214843767",
                    "name": "Puxin Xu"
                },
                {
                    "authorId": "14701107",
                    "name": "Zhengxu Yan"
                },
                {
                    "authorId": "121929334",
                    "name": "Iliyan Zarov"
                },
                {
                    "authorId": "2108473229",
                    "name": "Yuchen Zhang"
                },
                {
                    "authorId": "144270981",
                    "name": "Angela Fan"
                },
                {
                    "authorId": "2272979",
                    "name": "Melanie Kambadur"
                },
                {
                    "authorId": "46617804",
                    "name": "Sharan Narang"
                },
                {
                    "authorId": "2166043087",
                    "name": "Aurelien Rodriguez"
                },
                {
                    "authorId": "1962768",
                    "name": "Robert Stojnic"
                },
                {
                    "authorId": "2068070",
                    "name": "Sergey Edunov"
                },
                {
                    "authorId": "2073456043",
                    "name": "Thomas Scialom"
                }
            ]
        },
        {
            "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
            "title": "LIMA: Less Is More for Alignment",
            "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2384711",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "144118452",
                    "name": "Pengfei Liu"
                },
                {
                    "authorId": "2214843767",
                    "name": "Puxin Xu"
                },
                {
                    "authorId": "1900163",
                    "name": "Srini Iyer"
                },
                {
                    "authorId": "145478138",
                    "name": "Jiao Sun"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "1388010852",
                    "name": "Avia Efrat"
                },
                {
                    "authorId": "2114104308",
                    "name": "Ping Yu"
                },
                {
                    "authorId": "49297123",
                    "name": "L. Yu"
                },
                {
                    "authorId": "2108244542",
                    "name": "Susan Zhang"
                },
                {
                    "authorId": "134007132",
                    "name": "Gargi Ghosh"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "39455775",
                    "name": "Omer Levy"
                }
            ]
        },
        {
            "paperId": "62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c",
            "title": "XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models",
            "abstract": "Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This \\textit{vocabulary bottleneck} limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn). XLM-V is particularly effective on low-resource language tasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and Americas NLI, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25130521",
                    "name": "Davis Liang"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2132302721",
                    "name": "Rui Hou"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "2072010",
                    "name": "Madian Khabsa"
                }
            ]
        },
        {
            "paperId": "709af143f78bc62413c50ea1a7ee75b0702c4f59",
            "title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming",
            "abstract": "Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them.In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM.Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning.On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2243417222",
                    "name": "Suyu Ge"
                },
                {
                    "authorId": "2266920681",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "2266467782",
                    "name": "Rui Hou"
                },
                {
                    "authorId": "2072010",
                    "name": "Madian Khabsa"
                },
                {
                    "authorId": "2266748478",
                    "name": "Yi-Chia Wang"
                },
                {
                    "authorId": "2266712798",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2253929707",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                }
            ]
        },
        {
            "paperId": "7e96a8bc938b47cf805383ef8c079cd852bd64ba",
            "title": "Representation Deficiency in Masked Language Modeling",
            "abstract": "Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically that MLM pretraining allocates some model dimensions exclusively for representing $\\texttt{[MASK]}$ tokens, resulting in a representation deficiency for real tokens and limiting the pretrained model's expressiveness when it is adapted to downstream data without $\\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM, which pretrains the Masked Autoencoder architecture with MLM where $\\texttt{[MASK]}$ tokens are excluded from the encoder. Empirically, we show that MAE-LM improves the utilization of model dimensions for real token representations, and MAE-LM consistently outperforms MLM-pretrained models across different pretraining settings and model sizes when fine-tuned on the GLUE and SQuAD benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "113293663",
                    "name": "Jitin Krishnan"
                },
                {
                    "authorId": "2116420716",
                    "name": "Sinong Wang"
                },
                {
                    "authorId": "2145778781",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2087117615",
                    "name": "Han Fang"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ]
        },
        {
            "paperId": "811b2a0677fda94ec1bf655ff9d3958d62474139",
            "title": "Generating Hashtags for Short-form Videos with Guided Signals",
            "abstract": "Short-form video hashtag recommendation (SVHR) aims to recommend hashtags to content creators from videos and corresponding descriptions. Most prior studies regard SVHR as a classification or ranking problem and select hashtags from a set of limited candidates. However, in reality, users can create new hashtags, and trending hashtags change rapidly over time on social media. Both of these properties cannot be easily modeled with classification approaches. To bridge this gap, we formulate SVHR as a generation task that better represents how hashtags are created naturally. Additionally, we propose the Guided Generative Model (GGM) where we augment the input features by retrieving relevant hashtags from a large-scale hashtag pool as extra guidance signals. Experimental results on two short-form video datasets show that our generative models outperform strong classification baselines, and the guidance signals further boost the performance by 8.11 and 2.17 absolute ROUGE-1 scores on average, respectively. We also perform extensive analyses including human evaluation, demonstrating that our generative model can create meaningful and relevant novel hashtags while achieving state-of-the-art performance on known hashtags",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1660855299",
                    "name": "Tiezheng Yu"
                },
                {
                    "authorId": "30863081",
                    "name": "Hanchao Yu"
                },
                {
                    "authorId": "25130521",
                    "name": "Davis Liang"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "35557488",
                    "name": "Shaoliang Nie"
                },
                {
                    "authorId": "2319973",
                    "name": "Po-Yao (Bernie) Huang"
                },
                {
                    "authorId": "2072010",
                    "name": "Madian Khabsa"
                },
                {
                    "authorId": "2057151752",
                    "name": "Pascale Fung"
                },
                {
                    "authorId": "2116640035",
                    "name": "Yi-Chia Wang"
                }
            ]
        },
        {
            "paperId": "82b17686abba18dfc821a262dab5fbb081aa2388",
            "title": "Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization",
            "abstract": "Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute<0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters. In this work, we introduce Residual Prompt Tuning - a simple and efficient method that significantly improves the performance and stability of prompt tuning. We propose to reparameterize soft prompt embeddings using a shallow network with a residual connection. Our experiments show that Residual Prompt Tuning significantly outperforms prompt tuning on SuperGLUE benchmark. Notably, our method reaches +7 points improvement over prompt tuning with T5-Base and allows to reduce the prompt length by 10x without hurting performance. In addition, we show that our approach is robust to the choice of learning rate and prompt initialization, and is effective in few-shot settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1397302840",
                    "name": "Anastasia Razdaibiedina"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2132302721",
                    "name": "Rui Hou"
                },
                {
                    "authorId": "2072010",
                    "name": "Madian Khabsa"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "2503659",
                    "name": "Jimmy Ba"
                },
                {
                    "authorId": "2634674",
                    "name": "Amjad Almahairi"
                }
            ]
        },
        {
            "paperId": "86478f285356b5c8d27423e6b939634d9e010fba",
            "title": "Progressive Prompts: Continual Learning for Language Models",
            "abstract": "We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement>20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1397302840",
                    "name": "Anastasia Razdaibiedina"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2132302721",
                    "name": "Rui Hou"
                },
                {
                    "authorId": "2072010",
                    "name": "Madian Khabsa"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "2634674",
                    "name": "Amjad Almahairi"
                }
            ]
        },
        {
            "paperId": "041f5dbfcd07a3369ac44a6b902ee4b145eccf2b",
            "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation",
            "abstract": "Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based metrics, and we lack a reliable framework for a more comprehensive evaluation of advanced models. In this paper, we propose a unified multi-dimensional evaluator UniEval for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task, and by guiding the model with different questions, we can use one evaluator to evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean QA format, we are able to introduce an intermediate learning phase that enables UniEval to incorporate external knowledge from multiple related tasks and gain further improvement. Experiments on three typical NLG tasks show that UniEval correlates substantially better with human judgments than existing metrics. Specifically, compared to the top-performing unified evaluators, UniEval achieves a 23% higher correlation on text summarization, and over 43% on dialogue response generation. Also, UniEval demonstrates a strong zero-shot learning ability for unseen evaluation dimensions and tasks. Source code, data, and all pre-trained evaluators are available at https://github.com/maszhongming/UniEval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "39798499",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "144508458",
                    "name": "Da Yin"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "1381900594",
                    "name": "Yizhu Jiao"
                },
                {
                    "authorId": "145779142",
                    "name": "Peng Liu"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "5eb38b667ef6e594670ead4d21f41922e9df80c1",
            "title": "CiteSum: Citation Text-guided Scientific Extreme Summarization and Low-resource Domain Adaptation",
            "abstract": "Scienti\ufb01c extreme summarization (TLDR) aims to form ultra-short summaries of scienti\ufb01c papers. Previous efforts on curating scienti\ufb01c TLDR datasets failed to scale up due to the heavy human annotation and domain ex-pertise required. In this paper, we propose a simple yet effective approach to automatically extracting TLDR summaries for scienti\ufb01c papers from their citation texts. Based on the proposed approach, we create a new benchmark CiteSum without human annotation, which is around 30 times larger than the previous human-curated dataset SciTLDR. We conduct a comprehensive analysis of CiteSum, exam-ining its data characteristics and establishing strong baselines. We further demonstrate the usefulness of CiteSum by adapting models pretrained on CiteSum (named C I T E S) to new tasks and domains with limited supervision. For scienti\ufb01c extreme summarization, C I T E S outperforms most fully-supervised methods on SciTLDR without any \ufb01ne-tuning and ob-tains state-of-the-art results with only 128 examples. For news extreme summarization, C I T E S achieves signi\ufb01cant gains on XSum over its base model (not pre-trained on Cite-Sum), e.g. , +7.2 ROUGE-1 zero-shot performance and state-of-the-art few-shot performance. For news headline generation, C I T E S performs the best among unsupervised and zero-shot methods on Gigaword. 1 Paper Abstract : We study the problem of transferring a sample in one domain to an analog sample in another domain . Given two related domains, S and T , we would like to learn a generative function G that maps an input sample from S to the domain T , such that the output of a given function f , which accepts inputs in either domains, would remain unchanged. Other than the function f , the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f -constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity. Congested states in empirical observations and microscopic simulations Paper Abstract : We present data from several German freeways showing different kinds of congested traf\ufb01c forming near road inhomogeneities, speci\ufb01cally lane closings, intersections, or uphill gradients. The states are localized or extended, homogeneous or oscillating. Combined states are observed as well, like the coexistence of moving localized clusters and clusters pinned at road inhomogeneities, or regions of oscillating congested traf\ufb01c upstream of nearly homogeneous congested traf\ufb01c. The experimental \ufb01ndings are consistent with a recently proposed theoretical phase diagram for traf\ufb01c near on-ramps [D. Helbing, A. Hennecke, and M. Treiber, Phys. Rev. Lett. 82, 4360 (1999)]. We simulate these situations with a novel continuous microscopic single-lane model, the \"intelligent driver model\" (IDM), using the empirical boundary conditions. All observations, including the coexistence of states, are qualitatively reproduced by describing inhomogeneities with local variations of one model parameter. We show that the results of the microscopic model can be understood by formulating the theoretical phase diagram for bottlenecks in a more general way. In particular, a local drop of the road capacity induced by parameter variations has practically the same effect as an on-ramp. In a \ufb01rst approach, we use the well-known \"intelligent driver model\" (IDM) REF to show that the method works. Paper Abstract : Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classi\ufb01er) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classi\ufb01ers and regressors in ambiguous few-shot learning problems. Method Paper Abstract : Light \ufb01eld cameras can capture both spatial and angular information of light rays, enabling 3D reconstruction by a single exposure. The geometry of 3D reconstruction is affected by intrinsic parameters of a light \ufb01eld camera signi\ufb01cantly. In the paper, we propose a multi-projection-center (MPC) model with 6 intrinsic parameters to characterize light \ufb01eld cameras based on traditional twoparallel-plane (TPP) representation. The MPC model can generally parameterize light \ufb01eld in different imaging formations, including conventional and focused light \ufb01eld cameras. By the constraints of 4D ray and 3D geometry, a 3D projective transformation is deduced to describe the relationship between geometric structure and the MPC coordinates. Based on the MPC model and projective transformation, we propose a calibration algorithm to verify our light \ufb01eld camera model. Our calibration method includes a close-form solution and a non-linear optimization by minimizing re-projection errors. Experimental results on both simulated and real scene data have veri\ufb01ed the performance of our algorithm. Citation Text : Zhang et al REF proposed a multi-projection-center (MPC) model with six intrinsic parameters to characterize both conventional and focused LF cameras. Abstract-Cloud computing, evolved computing, virtualisation",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        }
    ]
}