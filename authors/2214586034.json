{
    "authorId": "2214586034",
    "papers": [
        {
            "paperId": "8ee0d8f6a35f66d8bb97e3388b85dba10d8d22d2",
            "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science",
            "abstract": "Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, these agents, called scientific LLM agents, also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This perspective paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47274259",
                    "name": "Xiangru Tang"
                },
                {
                    "authorId": "2279754388",
                    "name": "Qiao Jin"
                },
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2279788413",
                    "name": "Tongxin Yuan"
                },
                {
                    "authorId": "2308223883",
                    "name": "Yichi Zhang"
                },
                {
                    "authorId": "2249891947",
                    "name": "Wangchunshu Zhou"
                },
                {
                    "authorId": "2282962319",
                    "name": "Meng Qu"
                },
                {
                    "authorId": "46316984",
                    "name": "Yilun Zhao"
                },
                {
                    "authorId": "2283301920",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "2279826813",
                    "name": "Zhuosheng Zhang"
                },
                {
                    "authorId": "2266838179",
                    "name": "Arman Cohan"
                },
                {
                    "authorId": "2237094367",
                    "name": "Zhiyong Lu"
                },
                {
                    "authorId": "2201323142",
                    "name": "Mark B. Gerstein"
                }
            ]
        },
        {
            "paperId": "e3e13a31a839912aa8d0b62bbcd9906ff49512c9",
            "title": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework",
            "abstract": "Retrieval-Augmented Generation (RAG) systems have demonstrated their advantages in alleviating the hallucination of Large Language Models (LLMs). Existing RAG benchmarks mainly focus on evaluating whether LLMs can correctly answer the general knowledge. However, they are unable to evaluate the effectiveness of the RAG system in dealing with the data from different vertical domains. This paper introduces RAGEval, a framework for automatically generating evaluation datasets to evaluate the knowledge usage ability of different LLMs in different scenarios. Specifically, RAGEval summarizes a schema from seed documents, applies the configurations to generate diverse documents, and constructs question-answering pairs according to both articles and configurations. We propose three novel metrics, Completeness, Hallucination, and Irrelevance, to carefully evaluate the responses generated by LLMs. By benchmarking RAG models in vertical domains, RAGEval has the ability to better evaluate the knowledge usage ability of LLMs, which avoids the confusion regarding the source of knowledge in answering question in existing QA datasets--whether it comes from parameterized memory or retrieval. The code and dataset will be released.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2314833035",
                    "name": "Yifan Luo"
                },
                {
                    "authorId": "2314779448",
                    "name": "Dingling Xu"
                },
                {
                    "authorId": "2314784069",
                    "name": "Ruobing Wang"
                },
                {
                    "authorId": "2314785970",
                    "name": "Shi Yu"
                },
                {
                    "authorId": "2267033597",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2277242040",
                    "name": "Yukun Yan"
                },
                {
                    "authorId": "49047064",
                    "name": "Zhenghao Liu"
                },
                {
                    "authorId": "48506411",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2301534001",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "fc7245085637d7bb60d5e8e029f752262ee0c65f",
            "title": "How Far Are We From AGI",
            "abstract": "The evolution of artificial intelligence (AI) has profoundly impacted human society, driving significant advancements in multiple sectors. Yet, the escalating demands on AI have highlighted the limitations of AI's current offerings, catalyzing a movement towards Artificial General Intelligence (AGI). AGI, distinguished by its ability to execute diverse real-world tasks with efficiency and effectiveness comparable to human intelligence, reflects a paramount milestone in AI evolution. While existing works have summarized specific recent advancements of AI, they lack a comprehensive discussion of AGI's definitions, goals, and developmental trajectories. Different from existing survey papers, this paper delves into the pivotal questions of our proximity to AGI and the strategies necessary for its realization through extensive surveys, discussions, and original perspectives. We start by articulating the requisite capability frameworks for AGI, integrating the internal, interface, and system dimensions. As the realization of AGI requires more advanced capabilities and adherence to stringent constraints, we further discuss necessary AGI alignment technologies to harmonize these factors. Notably, we emphasize the importance of approaching AGI responsibly by first defining the key levels of AGI progression, followed by the evaluation framework that situates the status-quo, and finally giving our roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible insights into the ubiquitous impact of the integration of AI, we outline existing challenges and potential pathways toward AGI in multiple domains. In sum, serving as a pioneering exploration into the current state and future trajectory of AGI, this paper aims to foster a collective comprehension and catalyze broader public discussions among researchers and practitioners on AGI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2300275413",
                    "name": "Tao Feng"
                },
                {
                    "authorId": "2232994133",
                    "name": "Chuanyang Jin"
                },
                {
                    "authorId": "2301500865",
                    "name": "Jingyu Liu"
                },
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2301455901",
                    "name": "Haoqin Tu"
                },
                {
                    "authorId": "2301540626",
                    "name": "Zirui Cheng"
                },
                {
                    "authorId": "2301577462",
                    "name": "Guanyu Lin"
                },
                {
                    "authorId": "2261495159",
                    "name": "Jiaxuan You"
                }
            ]
        },
        {
            "paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
            "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
            "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2163374235",
                    "name": "Shi Liang"
                },
                {
                    "authorId": "2114059497",
                    "name": "Yining Ye"
                },
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2214613855",
                    "name": "Lan Yan"
                },
                {
                    "authorId": "2191753738",
                    "name": "Ya-Ting Lu"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "47274259",
                    "name": "Xiangru Tang"
                },
                {
                    "authorId": "2226120351",
                    "name": "Bill Qian"
                },
                {
                    "authorId": "2226184989",
                    "name": "Sihan Zhao"
                },
                {
                    "authorId": "2214603370",
                    "name": "Runchu Tian"
                },
                {
                    "authorId": "3360722",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "49640256",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "152573911",
                    "name": "M. Gerstein"
                },
                {
                    "authorId": "2144118403",
                    "name": "Dahai Li"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "352420ee61a8da783ca7750170793613b18b8d9c",
            "title": "Tool Learning with Foundation Models",
            "abstract": "Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 18 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally, we discuss several open problems that require further investigation for tool learning. In general, we hope this paper could inspire future research in integrating tools with foundation models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "1576223501",
                    "name": "Shengding Hu"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2109136284",
                    "name": "Weize Chen"
                },
                {
                    "authorId": "46649145",
                    "name": "Ning Ding"
                },
                {
                    "authorId": "52297757",
                    "name": "Ganqu Cui"
                },
                {
                    "authorId": "1633538428",
                    "name": "Zheni Zeng"
                },
                {
                    "authorId": "2115640120",
                    "name": "Yufei Huang"
                },
                {
                    "authorId": "51131083",
                    "name": "Chaojun Xiao"
                },
                {
                    "authorId": "2118642562",
                    "name": "Chi Han"
                },
                {
                    "authorId": "51135899",
                    "name": "Y. Fung"
                },
                {
                    "authorId": "48576745",
                    "name": "Yusheng Su"
                },
                {
                    "authorId": "2155242767",
                    "name": "Huadong Wang"
                },
                {
                    "authorId": "2082473972",
                    "name": "Cheng Qian"
                },
                {
                    "authorId": "2214603370",
                    "name": "Runchu Tian"
                },
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2163374235",
                    "name": "Shi Liang"
                },
                {
                    "authorId": "145781166",
                    "name": "Xingyu Shen"
                },
                {
                    "authorId": "2052218689",
                    "name": "Bokai Xu"
                },
                {
                    "authorId": "2170500945",
                    "name": "Zhen Zhang"
                },
                {
                    "authorId": "2114059497",
                    "name": "Yining Ye"
                },
                {
                    "authorId": "2155882844",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2214664440",
                    "name": "Ziwei Tang"
                },
                {
                    "authorId": "2106388389",
                    "name": "Jing Yi"
                },
                {
                    "authorId": "2109388429",
                    "name": "Yu Zhu"
                },
                {
                    "authorId": "2146517842",
                    "name": "Zhenning Dai"
                },
                {
                    "authorId": "2214613855",
                    "name": "Lan Yan"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "2191753738",
                    "name": "Ya-Ting Lu"
                },
                {
                    "authorId": "2150606888",
                    "name": "Weilin Zhao"
                },
                {
                    "authorId": "2214586078",
                    "name": "Yuxiang Huang"
                },
                {
                    "authorId": "2213334016",
                    "name": "Jun-Han Yan"
                },
                {
                    "authorId": "48506411",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2143570016",
                    "name": "Xian Sun"
                },
                {
                    "authorId": "2144118403",
                    "name": "Dahai Li"
                },
                {
                    "authorId": "80842917",
                    "name": "Jason Phang"
                },
                {
                    "authorId": "3443627",
                    "name": "Cheng Yang"
                },
                {
                    "authorId": "2116417519",
                    "name": "Tongshuang Wu"
                },
                {
                    "authorId": "2072975663",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "439cae3cf30efda24467ee6142072ef542b7c869",
            "title": "QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation",
            "abstract": "Recent years have witnessed the success of question answering (QA), especially its potential to be a foundation paradigm for tackling diverse NLP tasks. However, obtaining sufficient data to build an effective and stable QA system still remains an open problem. For this problem, we introduce an iterative bootstrapping framework for QA data augmentation (named QASnowball), which can iteratively generate large-scale high-quality QA data based on a seed set of supervised examples. Specifically, QASnowball consists of three modules, an answer extractor to extract core phrases in unlabeled documents as candidate answers, a question generator to generate questions based on documents and candidate answers, and a QA data filter to filter out high-quality QA data. Moreover, QASnowball can be self-enhanced by reseeding the seed set to fine-tune itself in different iterations, leading to continual improvements in the generation quality. We conduct experiments in the high-resource English scenario and the medium-resource Chinese scenario, and the experimental results show that the data generated by QASnowball can facilitate QA models: (1) training models on the generated data achieves comparable results to using supervised data, and (2) pre-training on the generated data and fine-tuning on supervised data can achieve better performance. Our code and generated data will be released to advance further work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2242676146",
                    "name": "Shihao Liang"
                },
                {
                    "authorId": "48506411",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2115548452",
                    "name": "Zhi Zheng"
                },
                {
                    "authorId": "1398454307",
                    "name": "Guoyang Zeng"
                },
                {
                    "authorId": "49293587",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "46ac88bb0acbf736840ff8a392cec2bf43d917e1",
            "title": "Exploring Format Consistency for Instruction Tuning",
            "abstract": "Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we propose a framework named Unified Instruction Tuning (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we (1) demonstrate the necessity of maintaining format consistency in instruction tuning; (2) improve the generalization performance on unseen instructions on T5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the noise of automatic format transfer to make the UIT framework more practical and a smaller offline model based on GPT-J that achieves comparable format transfer capability to OpenAI APIs to reduce costs in practice. Further analysis regarding variations of targeted formats and other effects is intended.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163374235",
                    "name": "Shi Liang"
                },
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2214603370",
                    "name": "Runchu Tian"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2155242767",
                    "name": "Huadong Wang"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "3028405",
                    "name": "Xiaojiang Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "c3ed333a37a6d9a0fcf1dad3106a114f66a45b99",
            "title": "WebCPM: Interactive Web Search for Chinese Long-form Question Answering",
            "abstract": "Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web search, which engages with a search engine in real time. Following WebGPT, we develop a web search interface. We recruit annotators to search for relevant information using our interface and then answer questions. Meanwhile, the web search behaviors of our annotators would be recorded. In total, we collect 5,500 high-quality question-answer pairs, together with 15,372 supporting facts and 125,954 web search actions. We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts. Our LFQA pipeline, built on these fine-tuned models, generates answers that are no worse than human-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader, respectively. The interface, dataset, and codes are publicly available at https://github.com/thunlp/WebCPM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2113441349",
                    "name": "Zihan Cai"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2214613855",
                    "name": "Lan Yan"
                },
                {
                    "authorId": "2163374235",
                    "name": "Shi Liang"
                },
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "48506411",
                    "name": "Xu Han"
                },
                {
                    "authorId": "46649145",
                    "name": "Ning Ding"
                },
                {
                    "authorId": "2155242767",
                    "name": "Huadong Wang"
                },
                {
                    "authorId": "3360722",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "51466208",
                    "name": "Fanchao Qi"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                },
                {
                    "authorId": "49640256",
                    "name": "Jie Zhou"
                }
            ]
        }
    ]
}