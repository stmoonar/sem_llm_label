{
    "authorId": "1785372925",
    "papers": [
        {
            "paperId": "0a89829b68a10ee441357b64cb521a6379e953b2",
            "title": "Open-Domain Text Evaluation via Contrastive Distribution Methods",
            "abstract": "Recent advancements in open-domain text generation, driven by the power of large pre-trained language models (LLMs), have demonstrated remarkable performance. However, assessing these models' generation quality remains a challenge. In this paper, we introduce a novel method for evaluating open-domain text generation called Contrastive Distribution Methods (CDM). Leveraging the connection between increasing model parameters and enhanced LLM performance, CDM creates a mapping from the _contrast_ of two probabilistic distributions -- one known to be superior to the other -- to quality measures. We investigate CDM for open-domain text generation evaluation under two paradigms: 1) _Generative_ CDM, which harnesses the contrast of two language models' distributions to generate synthetic examples for training discriminator-based metrics; 2) _Discriminative_ CDM, which directly uses distribution disparities between two language models for evaluation. Our experiments on coherence evaluation for multi-turn dialogue and commonsense evaluation for controllable generation demonstrate CDM's superior correlate with human judgment than existing automatic evaluation metrics, highlighting the strong performance and generalizability of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167235968",
                    "name": "Sidi Lu"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "16fa155e4b3ec74565283def01a12d3497074c07",
            "title": "Variation of Gender Biases in Visual Recognition Models Before and After Finetuning",
            "abstract": "We introduce a framework to measure how biases change before and after fine-tuning a large scale visual recognition model for a downstream task. Deep learning models trained on increasing amounts of data are known to encode societal biases. Many computer vision systems today rely on models typically pretrained on large scale datasets. While bias mitigation techniques have been developed for tuning models for downstream tasks, it is currently unclear what are the effects of biases already encoded in a pretrained model. Our framework incorporates sets of canonical images representing individual and pairs of concepts to highlight changes in biases for an array of off-the-shelf pretrained models across model sizes, dataset sizes, and training objectives. Through our analyses, we find that (1) supervised models trained on datasets such as ImageNet-21k are more likely to retain their pretraining biases regardless of the target dataset compared to self-supervised models. We also find that (2) models finetuned on larger scale datasets are more likely to introduce new biased associations. Our results also suggest that (3) biases can transfer to finetuned models and the finetuning objective and dataset can impact the extent of transferred biases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1819649550",
                    "name": "Jaspreet Ranjit"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "31631000",
                    "name": "Baishakhi Ray"
                },
                {
                    "authorId": "2004053",
                    "name": "Vicente Ordonez"
                }
            ]
        },
        {
            "paperId": "36f7bc27c9a37eb337c35df4ae86f148e13d4e9a",
            "title": "Understanding In-Context Learning via Supportive Pretraining Data",
            "abstract": "In-context learning (ICL) improves language models\u2019 performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model\u2019s ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL. Our work takes a first step towards understanding ICL via analyzing instance-level pretraining data. Our insights have a potential to enhance the ICL ability of language models by actively guiding the construction of pretraining data in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40500540",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                }
            ]
        },
        {
            "paperId": "5427efe3a0e45d3f5386d3e7eb4d0f2d6c800312",
            "title": "TC-CycleGAN: Improved CycleGAN with Texture Constraints for Virtual Staining of Pathological Images",
            "abstract": "Pathological slides are crucial for the diagnosis of cancer, and their staining is the basis for pathologists to assess the disease. Pathological sections with different staining statuses have different guiding functions for clinical diagnosis. In this paper, we propose a new method based on CycleGAN to improve the conversion from Hematoxylin-Eosin(H&E) sections to immunohistochemical (IHC) sections by adding texture loss and a self-attention mechanism to help the model better capture image details and establish the relationship between features. The proposed method is validated on the BCI dataset. Extensive experimental results demonstrate that the proposed method effectively improves the generation quality of virtual stained sections and reveals the great potential of virtual staining technology in clinical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155070954",
                    "name": "Sheng Huang"
                },
                {
                    "authorId": "2155123026",
                    "name": "Hongyu Wang"
                },
                {
                    "authorId": "9229269",
                    "name": "Yingguang Hao"
                },
                {
                    "authorId": "2219604234",
                    "name": "Siyu Guo"
                },
                {
                    "authorId": "2212045957",
                    "name": "Ye Wang"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                }
            ]
        },
        {
            "paperId": "7f55ef29a6f8b2771c5435bbeba29c87264fdc88",
            "title": "Shepherd: A Critic for Language Model Generation",
            "abstract": "As large language models improve, there is increasing interest in techniques that leverage these models' capabilities to refine their own outputs. In this work, we introduce Shepherd, a language model specifically tuned to critique responses and suggest refinements, extending beyond the capabilities of an untuned model to identify diverse errors and provide suggestions to remedy them. At the core of our approach is a high quality feedback dataset, which we curate from community feedback and human annotations. Even though Shepherd is small (7B parameters), its critiques are either equivalent or preferred to those from established models including ChatGPT. Using GPT-4 for evaluation, Shepherd reaches an average win-rate of 53-87% compared to competitive alternatives. In human evaluation, Shepherd strictly outperforms other models and on average closely ties with ChatGPT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "2114104308",
                    "name": "Ping Yu"
                },
                {
                    "authorId": "46879944",
                    "name": "Xiaoqing Tan"
                },
                {
                    "authorId": "2241351144",
                    "name": "Sean O'Brien"
                },
                {
                    "authorId": "10721120",
                    "name": "Ramakanth Pasunuru"
                },
                {
                    "authorId": "2173509991",
                    "name": "Jane Dwivedi-Yu"
                },
                {
                    "authorId": "100664938",
                    "name": "O. Yu. Golovneva"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "1399159921",
                    "name": "Maryam Fazel-Zarandi"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                }
            ]
        },
        {
            "paperId": "f61ff1744a70f9f6e08a82440766a561247fd989",
            "title": "Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning",
            "abstract": "Pretrained model-based evaluation metrics have demonstrated strong performance with high correlations with human judgments in various natural language generation tasks such as image captioning. Despite the impressive re-sults, their impact on fairness is under-explored \u2013 it is widely acknowledged that pretrained models can encode societal biases, and utilizing them for evaluation purposes may inadvertently manifest and potentially amplify biases. In this paper, we conduct a systematic study in gender biases of model-based evaluation metrics with a focus on image captioning tasks. Specifically, we first identify and quantify gender biases in different evaluation metrics regarding profession, activity, and object concepts. Then, we demonstrate the negative consequences of using these biased metrics, such as favoring biased generation models in deployment and propagating the biases to generation models through reinforcement learning. We also present a simple but effective alternative to reduce gender biases by combining n -gram matching-based and pretrained model-based evaluation metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185567809",
                    "name": "Haoyi Qiu"
                },
                {
                    "authorId": "14199369",
                    "name": "Zi-Yi Dou"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models",
            "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108244542",
                    "name": "Susan Zhang"
                },
                {
                    "authorId": "3849208",
                    "name": "Stephen Roller"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                },
                {
                    "authorId": "2108267192",
                    "name": "Moya Chen"
                },
                {
                    "authorId": "1782969",
                    "name": "Shuohui Chen"
                },
                {
                    "authorId": "2065332326",
                    "name": "Christopher Dewan"
                },
                {
                    "authorId": "2138579860",
                    "name": "Mona T. Diab"
                },
                {
                    "authorId": "2116235416",
                    "name": "Xian Li"
                },
                {
                    "authorId": "143724481",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "40511414",
                    "name": "Myle Ott"
                },
                {
                    "authorId": "88728159",
                    "name": "Sam Shleifer"
                },
                {
                    "authorId": "35752280",
                    "name": "Kurt Shuster"
                },
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "2146367061",
                    "name": "Punit Singh Koura"
                },
                {
                    "authorId": "5382923",
                    "name": "Anjali Sridhar"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ]
        },
        {
            "paperId": "2ec864b0aaf935b3ead513ee4fca86499a8e9c70",
            "title": "Text Characterization Toolkit",
            "abstract": "In NLP, models are usually evaluated by reporting single-number performance scores on a number of readily available benchmarks, without much deeper analysis. Here, we argue that - especially given the well-known fact that benchmarks often contain biases, artefacts, and spurious correlations - deeper results analysis should become the de-facto standard when presenting new models or benchmarks. We present a tool that researchers can use to study properties of the dataset and the influence of those properties on their models' behaviour. Our Text Characterization Toolkit includes both an easy-to-use annotation tool, as well as off-the-shelf scripts that can be used for specific analyses. We also present use-cases from three different domains: we use the tool to predict what are difficult examples for given well-known trained models and identify (potentially harmful) biases and heuristics that are present in a dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "74461595",
                    "name": "Verna Dankers"
                },
                {
                    "authorId": "40068904",
                    "name": "Peter Henderson"
                },
                {
                    "authorId": "2049136",
                    "name": "Khuyagbaatar Batsuren"
                },
                {
                    "authorId": "3449411",
                    "name": "Dieuwke Hupkes"
                },
                {
                    "authorId": "2138579860",
                    "name": "Mona T. Diab"
                }
            ]
        },
        {
            "paperId": "86d0d3855f94105e25d81cab9f3d269c6062a9c4",
            "title": "Selective Annotation Makes Language Models Better Few-Shot Learners",
            "abstract": "Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/HKUNLP/icl-selective-annotation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152173042",
                    "name": "Hongjin Su"
                },
                {
                    "authorId": "11348687",
                    "name": "Jungo Kasai"
                },
                {
                    "authorId": "114621402",
                    "name": "Chen Henry Wu"
                },
                {
                    "authorId": "3040379",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "2184142568",
                    "name": "Jiayi Xin"
                },
                {
                    "authorId": "15176410",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "144339506",
                    "name": "Mari Ostendorf"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "144365875",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "48881008",
                    "name": "Tao Yu"
                }
            ]
        },
        {
            "paperId": "95c11cc5820ba32c60d5f2671f6567b9914a4978",
            "title": "ALERT: Adapt Language Models to Reasoning Tasks",
            "abstract": "Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce {pasted macro \u2018OUR\u2019}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro \u2018OUR\u2019}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro \u2018OUR\u2019}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114104308",
                    "name": "Ping Yu"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "100664938",
                    "name": "O. Yu. Golovneva"
                },
                {
                    "authorId": "2006905770",
                    "name": "Badr AlKhamissi"
                },
                {
                    "authorId": "134007132",
                    "name": "Gargi Ghosh"
                },
                {
                    "authorId": "2138579860",
                    "name": "Mona T. Diab"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                }
            ]
        }
    ]
}