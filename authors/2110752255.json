{
    "authorId": "2110752255",
    "papers": [
        {
            "paperId": "0d694d196f8a50d015a3509e4d3d683c66c1ff37",
            "title": "UDA-DP: Unsupervised Domain Adaptation for Software Defect Prediction",
            "abstract": "Software defect prediction can automatically locate defective code modules to focus testing resources better. Traditional defect prediction methods mainly focus on manually designing features, which are input into machine learning classifiers to identify defective code. However, there are mainly two problems in prior works. First manually designing features is time consuming and unable to capture the semantic information of programs, which is an important capability for accurate defect prediction. Second the labeled data is limited along with severe class imbalance, affecting the performance of defect prediction.In response to the above problems, we first propose a new unsupervised domain adaptation method using pseudo labels for defect prediction(UDA-DP). Compared to manually designed features, it can automatically extract defective features from source programs to save time and contain more semantic information of programs. Moreover, unsupervised domain adaptation using pseudo labels is a kind of transfer learning, which is effective in leveraging rich information of limited data, alleviating the problem of insufficient data.Experiments with 10 open source projects from the PROMISE data set show that our proposed UDA-DP method outperforms the state-of-the-art methods for both within-project and cross-project defect predictions. Our code and data are available at https://github.com/xsarvin/UDA-DP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2217880766",
                    "name": "Xiaosong Huang"
                },
                {
                    "authorId": "2156705217",
                    "name": "Yifan Wu"
                },
                {
                    "authorId": "151503698",
                    "name": "Hongyi Liu"
                },
                {
                    "authorId": "2172444921",
                    "name": "Ying Li"
                },
                {
                    "authorId": "2110752255",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2213872863",
                    "name": "Dadi Guo"
                },
                {
                    "authorId": "2109716565",
                    "name": "Zhonghai Wu"
                }
            ]
        },
        {
            "paperId": "6ab8aca8f631f42760a86cc614dfd7208b3fe58e",
            "title": "Learning-based Widget Matching for Migrating GUI Test Cases",
            "abstract": "GUI test case migration is to migrate GUI test cases from a source app to a target app. The key of test case migration is widget matching. Recently, researchers have proposed various approaches by formulating widget matching as a matching task. However, since these matching approaches depend on static word embeddings without using contextual information to represent widgets and manually formulated matching functions, there are main limitations of these matching approaches when handling complex matching relations in apps. To address the limitations, we propose the first learning-based widget matching approach named TEMdroid ( TEst Migration) for test case migration. Unlike the existing approaches, TEMdroid uses BERT to capture contextual information and learns a matching model to match widgets. Additionally, to balance the significant imbalance between positive and negative samples in apps, we design a two-stage training strategy where we first train a hard-negative sample miner to mine hard-negative samples, and further train a matching model using positive samples and mined hard-negative samples. Our evaluation on 34 apps shows that TEM-droid is effective in event matching (i.e., widget matching and target event synthesis) and test case migration. For event matching, TEM-droid's Top1 accuracy is 76%, improving over 17% compared to baselines. For test case migration, TEMdroid's F1 score is 89%, also 7% improvement compared to the baseline approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110752255",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2089966950",
                    "name": "Bo Shen"
                },
                {
                    "authorId": "2038503437",
                    "name": "Dezhi Ran"
                },
                {
                    "authorId": null,
                    "name": "Jiaxin Zhang"
                },
                {
                    "authorId": "2145906426",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2304523047",
                    "name": "Yuchi Ma"
                },
                {
                    "authorId": "2084524",
                    "name": "Guangtai Liang"
                },
                {
                    "authorId": "2172444921",
                    "name": "Ying Li"
                },
                {
                    "authorId": "2057038049",
                    "name": "Tao Xie"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                }
            ]
        },
        {
            "paperId": "bca9f9ef0b2fba6a42d865116577cf17fe2b3fa1",
            "title": "Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning",
            "abstract": "In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110752255",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2110049191",
                    "name": "Xing Hu"
                },
                {
                    "authorId": "2154591375",
                    "name": "Ge Li"
                },
                {
                    "authorId": "2172444921",
                    "name": "Ying Li"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                },
                {
                    "authorId": "2057038192",
                    "name": "Tao Xie"
                }
            ]
        },
        {
            "paperId": "dd03e9ac431ed52936bb81e7520e1aee515c8265",
            "title": "Mining Event Logic Graph from Open Q&A Site for Automated Program Repair",
            "abstract": "Automated program repair (APR) has been a hot topic due to its possible usage among large group of program-mers. To reduce the patch search space, APR tools usually rely on external sources like StackOverflow. However, most existing approaches simply associate a bug with a single post/patch, which has a negative impact on the performance. This paper proposes the idea of automated program repair event logic graph (APR-ELG), which links relevant bug reasons and possible patches with graph structure. Preliminary studies on Defects4J have shown that APR-ELG can correctly repair several new bugs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2028960931",
                    "name": "Chuanjia Hou"
                },
                {
                    "authorId": "2110654547",
                    "name": "Xiao-tao Liu"
                },
                {
                    "authorId": "2110752255",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2056066951",
                    "name": "Tong Jia"
                },
                {
                    "authorId": "2153643350",
                    "name": "Ying Li"
                }
            ]
        }
    ]
}