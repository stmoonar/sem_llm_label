{
    "authorId": "1945230198",
    "papers": [
        {
            "paperId": "48f404538e9c0b1af27b23518098b9ab78a01e41",
            "title": "Training Neural Networks for Execution on Approximate Hardware",
            "abstract": "Approximate computing methods have shown great potential for deep learning. Due to the reduced hardware costs, these methods are especially suitable for inference tasks on battery-operated devices that are constrained by their power budget. However, approximate computing hasn't reached its full potential due to the lack of work on training methods. In this work, we discuss training methods for approximate hardware. We demonstrate how training needs to be specialized for approximate hardware, and propose methods to speed up the training process by up to 18X.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10742390",
                    "name": "Tianmu Li"
                },
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "ef356de51a3dc84b0f1753a79e6156d18edc4f86",
            "title": "ReFOCUS: Reusing Light for Efficient Fourier Optics-Based Photonic Neural Network Accelerator",
            "abstract": "In recent years, there has been a significant focus on achieving low-latency and high-throughput convolutional neural network (CNN) inference. Integrated photonics offers the potential to substantially expedite neural networks due to its inherent low-latency properties. Recently, on-chip Fourier optics-based neural network accelerators have been demonstrated and achieved superior energy efficiency for CNN acceleration. By incorporating Fourier optics, computationally intensive convolution operations can be performed instantaneously through on-chip lenses at a significantly lower cost compared to other on-chip photonic neural network accelerators. This is thanks to the complexity reduction offered by the convolution theorem and the passive Fourier transforms computed by on-chip lenses. However, conversion overhead between optical and digital domains and memory access energy still hinder overall efficiency.We introduce ReFOCUS, a Joint Transform Correlator (JTC) based on-chip neural network accelerator that efficiently reuses light through optical buffers. By incorporating optical delay lines, wavelength-division multiplexing, dataflow, and memory hierarchy optimization, ReFOCUS minimizes both conversion overhead and memory access energy. As a result, ReFOCUS achieves 2\u00d7 throughput, 2.2\u00d7 energy efficiency, and 1.36\u00d7 area efficiency compared to state-of-the-art photonic neural network accelerators.CCS CONCEPTS\u2022 Computer systems organization \u2192 Architectures; \u2022 Hardware \u2192 Emerging technologies; Emerging optical and photonic technologies; \u2022 Computing methodologies \u2192 Artificial intelligence; Machine learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "1944367741",
                    "name": "Hangbo Yang"
                },
                {
                    "authorId": "2273700411",
                    "name": "C. W. Wong"
                },
                {
                    "authorId": "2249165298",
                    "name": "V. Sorger"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "6c3b4dc6e54e99f629954b10bc2c6194f15c500c",
            "title": "Batch processing and data streaming Fourier-based convolutional neural network accelerator",
            "abstract": "Decision-making through artificial neural networks with minimal latency is critical for numerous applications such as navigation, tracking, and real-time machine action systems. This requires machine learning hardware to process multidimensional data at high throughput. Unfortunately, handling convolution operations, the primary computational tool for data classification tasks, obeys challenging runtime complexity scaling laws. However, homomorphically implementing the convolution theorem in a Fourier optics display light processor can achieve a non-iterative O(1) runtime complexity for data inputs beyond 1,000 \u00d7 1,000 large matrices. Following this approach, here we demonstrate data streaming multi-kernel image batching using a Fourier Convolutional Neural Network (FCNN) accelerator. We show image batch processing of large-scale matrices as 2 million dot product multiplications performed by a digital light processing module in the Fourier domain. Furthermore, we further parallelize this optical FCNN system by exploiting multiple spatially parallel diffraction orders, achieving a 98x throughput improvement over state-of-the-art FCNN accelerators. A comprehensive discussion of the practical challenges associated with working at the edge of system capabilities highlights the problem of crosstalk and resolution scaling laws in the Fourier domain. Accelerating convolution by exploiting massive parallelism in display technology brings non-Van Neumann-based machine learning acceleration.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15368955",
                    "name": "Zibo Hu"
                },
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "2147107765",
                    "name": "Russell L. T. Schwartz"
                },
                {
                    "authorId": "1405208974",
                    "name": "Maria Solyanik-Gorgone"
                },
                {
                    "authorId": "50533784",
                    "name": "M. Miscuglio"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                },
                {
                    "authorId": "1995822",
                    "name": "V. Sorger"
                }
            ]
        },
        {
            "paperId": "6dd5ee1f89c6d60252edf4f5f92f9b8489f27677",
            "title": "Bit-serial Weight Pools: Compression and Arbitrary Precision Execution of Neural Networks on Resource Constrained Processors",
            "abstract": "Applications of neural networks on edge systems have proliferated in recent years but the ever-increasing model size makes neural networks not able to deploy on resource-constrained microcontrollers efficiently. We propose bit-serial weight pools, an end-to-end framework that includes network compression and acceleration of arbitrary sub-byte precision. The framework can achieve up to 8x compression compared to 8-bit networks by sharing a pool of weights across the entire network. We further propose a bit-serial lookup based software implementation that allows runtime-bitwidth tradeoff and is able to achieve more than 2.8x speedup and 7.5x storage compression compared to 8-bit weight pool networks, with less than 1% accuracy drop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "b636e99ea9a2f277b28ee4a3174266e9a19a7208",
            "title": "PhotoFourier: A Photonic Joint Transform Correlator-Based Neural Network Accelerator",
            "abstract": "The last few years have seen a lot of work to address the challenge of low-latency and high-throughput convolutional neural network inference. Integrated photonics has the potential to dramatically accelerate neural networks because of its low-latency nature. Combined with the concept of Joint Transform Correlator (JTC), the computationally expensive convolution functions can be computed instantaneously (time of flight of light) with almost no cost. This \u2018free\u2019 convolution computation provides the theoretical basis of the proposed PhotoFourier JTC-based CNN accelerator. PhotoFourier addresses a myriad of challenges posed by on-chip photonic computing in the Fourier domain including 1D lenses and high-cost optoelectronic conversions. The proposed PhotoFourier accelerator achieves more than 28\u00d7 better energy-delay product compared to state-of-art photonic neural network accelerators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "1944367741",
                    "name": "Hangbo Yang"
                },
                {
                    "authorId": "2109650903",
                    "name": "C. Wong"
                },
                {
                    "authorId": "1995822",
                    "name": "V. Sorger"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "085e86ba82fa36d3e0919d8502406e0f347bc148",
            "title": "SWIS - Shared Weight bIt Sparsity for Efficient Neural Network Acceleration",
            "abstract": "Quantization is spearheading the increase in performance and efficiency of neural network computing systems making headway into commodity hardware. We present SWIS - Shared Weight bIt Sparsity, a quantization framework for efficient neural network inference acceleration delivering improved performance and storage compression through an offline weight decomposition and scheduling algorithm. SWIS can achieve up to 54.3% (19.8%) point accuracy improvement compared to weight truncation when quantizing MobileNet-v2 to 4 (2) bits post-training (with retraining) showing the strength of leveraging shared bit-sparsity in weights. SWIS accelerator gives up to 6x speedup and 1.9x energy improvement overstate of the art bit-serial architectures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "102401101",
                    "name": "W. Romaszkan"
                },
                {
                    "authorId": "2051800694",
                    "name": "A. Graening"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "26abbf5e20ddbd072bbad0aeb2e01b5c693a1ad8",
            "title": "High\u2010Throughput Multichannel Parallelized Diffraction Convolutional Neural Network Accelerator",
            "abstract": "Convolutional neural networks are paramount in image and signal processing, and are responsible for the majority of image recognition power consumption today, concentrated mainly in convolution computations. With convolution operations being computationally intensive, next\u2010generation hardware accelerators need to offer parallelization and high efficiency. Diffractive optics offers the promise of low\u2010latency, highly parallel convolution operations. However, thus far parallelism is only partially harvested, thereby significantly underdelivering in comparison to its throughput potential. Here, a parallelized operation high\u2010throughput Fourier optic convolutional accelerator is demonstrated. For the first time, simultaneous processing of multiple kernels in Fourier domain enabled by optical diffraction orders is achieved alongside input parallelism. The proposed approach can offer \u2248100\u00d7 speedup over the previous generation optical diffraction\u2010based processor and 10\u00d7 speedup over other state\u2010of\u2010the\u2010art optical Fourier classifiers.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "15368955",
                    "name": "Zibo Hu"
                },
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "2147107765",
                    "name": "Russell L. T. Schwartz"
                },
                {
                    "authorId": "1405208974",
                    "name": "Maria Solyanik-Gorgone"
                },
                {
                    "authorId": "50533784",
                    "name": "M. Miscuglio"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                },
                {
                    "authorId": "1995822",
                    "name": "V. Sorger"
                }
            ]
        },
        {
            "paperId": "4fa547b7ccfaac5f3aebd7b8957d1ad699a892c2",
            "title": "Massively-parallel Amplitude-Only Fourier Optical Convolutional Neural Network",
            "abstract": "Here we introduce a novel amplitude-only Fourier-optical processor paradigm and demonstrate a prototype system capable of processing large-scale ~(2,000x1,000) matrices in a single time-step and 100 microsecond-short latency, for accelerating machine-learning applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50533784",
                    "name": "M. Miscuglio"
                },
                {
                    "authorId": "15368955",
                    "name": "Zibo Hu"
                },
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "51174084",
                    "name": "J. George"
                },
                {
                    "authorId": "116123609",
                    "name": "R. Capanna"
                },
                {
                    "authorId": "1766287",
                    "name": "H. Dalir"
                },
                {
                    "authorId": "46179055",
                    "name": "P. Bardet"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                },
                {
                    "authorId": "1995822",
                    "name": "V. Sorger"
                }
            ]
        },
        {
            "paperId": "c27ed0ba5f64ca3ebbd8324e8db61fec0425b9a7",
            "title": "Revisiting the double-well problem by deep learning with a hybrid network",
            "abstract": "Solving physical problems by deep learning is accurate and ef\ufb01cient mainly accounting for the use of an elaborate neural network. We propose a novel hybrid network which integrates two different kinds of neural networks: LSTM and ResNet, in order to overcome the dif\ufb01culty met in solving strongly-oscillating dynamics of the system\u2019s time evolution. By taking the double-well model as an example we show that our new method can bene\ufb01t from a pre-learning and veri\ufb01cation of the periodicity of frequency by using the LSTM network, simultaneously making a high-\ufb01delity prediction about the whole dynamics of system with ResNet, which is impossibly achieved in the case of single network. Such a hybrid network can be applied for solving cooperative dynamics in a system with fast spatial or temporal modulations, promising for realistic oscillation calculations under experimental conditions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "119858674",
                    "name": "Jianqin Xu"
                },
                {
                    "authorId": "2154032209",
                    "name": "Jing Qian"
                }
            ]
        },
        {
            "paperId": "197e97d7235de08a2c6d5dfbf62c20c1edc660cd",
            "title": "Million-Channel Parallelism Fourier-Optic Convolutional Filter and Neural Network Processor",
            "abstract": "Here we report on a massively-parallel Fourier-optics convolutional processor accelerated 160\u00d7 over spatial-light-modulators using digital-mirror-display technology as input and kernel. Testing the system on MNIST and CIFAR-10 datasets shows 96% and 54% accuracy, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50533784",
                    "name": "M. Miscuglio"
                },
                {
                    "authorId": "15368955",
                    "name": "Zibo Hu"
                },
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "2216587442",
                    "name": "Jiaqi Gu"
                },
                {
                    "authorId": "7714843",
                    "name": "A. Babakhani"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                },
                {
                    "authorId": "1727458",
                    "name": "C. Wong"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                },
                {
                    "authorId": "35257502",
                    "name": "S. Bank"
                },
                {
                    "authorId": "1766287",
                    "name": "H. Dalir"
                },
                {
                    "authorId": "1995822",
                    "name": "V. Sorger"
                }
            ]
        }
    ]
}