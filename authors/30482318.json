{
    "authorId": "30482318",
    "papers": [
        {
            "paperId": "10b59b888f3a9dd53b5ae393b5fd8bd9f82b215a",
            "title": "One-Stage Object Referring with Gaze Estimation",
            "abstract": "The classic object referring task aims at localizing the referred object in the image and requires a reference image and a natural language description as inputs. Given the facts that gaze signal can be easily obtained by a modern human-computer interaction system with a camera and that human tends to look at the object when referring to it, we propose a novel gaze-assisted object referring framework. The formulation not only simplifies the state-of-the-art gaze-assisted object referring system requiring many input signals besides gaze, but also incorporates the one-stage object detection idea to improve the inference efficiency. More importantly, it implicitly considers all object candidates and thus resolves the main pain point of existing two-stage object referring solutions for proposing an appropriate number of candidates \u2013 it cannot be too large, otherwise the computational cost can be prohibitive; it cannot be too small, otherwise the chance of missing a referred object can be significant. To utilize the gaze information, we propose to build a gaze heatmap by using the anchor position encoding map and the gaze prediction result. The gaze heatmap and the language feature are then merged into the feature pyramid in the object detection as the final one-stage referring system. In the CityScapes-OR dataset, the proposed method outperforms the state-of-the-art by 7.8% for Acc@1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30482318",
                    "name": "Jianhang Chen"
                },
                {
                    "authorId": "2115462987",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "2134759582",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "2122374530",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                }
            ]
        },
        {
            "paperId": "b1973a7c565c570b43add8644ffc2e8206d5e0e3",
            "title": "Semi-supervised Multi-task Network For Image Aesthetic Assessment",
            "abstract": "\n Image aesthetic assessment has always been regarded as a challenging task because of the variability of subjective preference. Besides, the assessment of a photo is also related to its style, semantic content, etc. Conventionally, the estimations of aesthetic score and style for\n an image are treated as separate problems. In this paper, we explore the inter-relatedness between the aesthetics and image style, and design a neural network that can jointly categorize image by styles and give an aesthetic score distribution.\n \n To this end, we propose a multi-task\n network (MTNet) with an aesthetic column serving as a score predictor and a style column serving as a style classifier. The angular-softmax loss is applied in training primary style classifiers to maximize the margin among classes in single-label training data; the semi-supervised method is\n applied to improve the network\u2019s generalization ability iteratively. We combine the regression loss and classification loss in training aesthetic score. Experiments on the AVA dataset show the superiority of our network in both image attributes classification and aesthetic ranking tasks.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056228837",
                    "name": "Xiaoyu Xiang"
                },
                {
                    "authorId": "2117877945",
                    "name": "Yang Cheng"
                },
                {
                    "authorId": "30482318",
                    "name": "Jianhang Chen"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                }
            ]
        },
        {
            "paperId": "bc9c41c61c3a2b56c7f82b96043b5c1a4adf85ac",
            "title": "Deep Learning for Printed Mottle Defect Grading",
            "abstract": "\n In this paper, we propose a new method for printed mottle defect grading. By training the data scanned from printed images, our deep learning method based on a Convolutional Neural Network (CNN) can classify various images with different mottle defect levels. Different from traditional\n methods to extract the image features, our method utilizes a CNN for the first time to extract the features automatically without manual feature design. Different data augmentation methods such as rotation, flip, zoom, and shift are also applied to the original dataset. The final network is\n trained by transfer learning using the ResNet-34 network pretrained on the ImageNet dataset connected with fully connected layers. The experimental results show that our approach leads to a 13.16% error rate in the T dataset, which is a dataset with a single image content, and a 20.73% error\n rate in a combined dataset with different contents.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30482318",
                    "name": "Jianhang Chen"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                }
            ]
        },
        {
            "paperId": "cd17ead74645e9f4338fbdcf7cc5f33a9c297315",
            "title": "Extra FAT: A Photorealistic Dataset for 6D Object Pose Estimation",
            "abstract": "\n We introduce a new image dataset for object detection and 6D pose estimation, named Extra FAT. The dataset consists of 825K photorealistic RGB images with annotations of groundtruth location and rotation for both the virtual camera and the objects. A registered pixel-level object\n segmentation mask is also provided for object detection and segmentation tasks. The dataset includes 110 different 3D object models. The object models were rendered in five scenes with diverse illumination, reflection, and occlusion conditions.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30482318",
                    "name": "Jianhang Chen"
                },
                {
                    "authorId": "51046403",
                    "name": "D. M. Montserrat"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741483",
                    "name": "E. Delp"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                }
            ]
        },
        {
            "paperId": "65fcc3b98535ce8d12275fbceb735c8860c74f5c",
            "title": "High-Accuracy Automatic Person Segmentation with Novel Spatial Saliency Map",
            "abstract": "In this work, we propose a high-efficiency person segmentation system that achieves high segmentation accuracy with a much smaller CNN network. In this approach, key-point detection annotation is incorporated for the first time and a novel spatial saliency map, in which the intensity of each pixel indicates the likelihood of forming a part of the human and reflects the distance from the body, is generated to provide more spatial information. Additionally, a lightweight automatic person segmentation network is proposed, which is small and efficient for person segmentation by leveraging atrous convolution. The experimental results prove that an image pyramid resizing augmentation can also improve efficiency. Our proposed segmentation method achieves an accuracy of 94.06% on the person segmentation dataset built in this work, which exceeds the results of previous state-of-the-art methods in accuracy and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72704386",
                    "name": "Weijuan Xi"
                },
                {
                    "authorId": "30482318",
                    "name": "Jianhang Chen"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                }
            ]
        },
        {
            "paperId": "a8b05b30d8af27f7ef8d3b8f4baa6e6afe0e07a8",
            "title": "Multi-View Matching Network for 6D Pose Estimation",
            "abstract": "Applications that interact with the real world such as augmented reality or robot manipulation require a good understanding of the location and pose of the surrounding objects. In this paper, we present a new approach to estimate the 6 Degree of Freedom (DoF) or 6D pose of objects from a single RGB image. Our approach can be paired with an object detection and segmentation method to estimate, refine and track the pose of the objects by matching the input image with rendered images.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51046403",
                    "name": "D. M. Montserrat"
                },
                {
                    "authorId": "30482318",
                    "name": "Jianhang Chen"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "1741483",
                    "name": "E. Delp"
                }
            ]
        }
    ]
}