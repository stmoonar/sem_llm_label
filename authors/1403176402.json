{
    "authorId": "1403176402",
    "papers": [
        {
            "paperId": "0058d85ea1fd068d51855062bf529ae772690f89",
            "title": "Discovering Transition Pathways Towards Coviability with Machine Learning",
            "abstract": "Coviability refers to the multiple socio-ecological arrange- ments and governance structures under which humans and nature can coexist in functional, fair, and persistent ways. Transitioning to a coviable state in environmentally degraded and socially vulnerable territories is challenging. This paper presents an ongoing French-Brazilian joint research project combining machine learning, agroecology, and social sci- ences to discover coviability pathways that can be adopted and implemented by local populations in the North-East re- gion of Brazil.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "90928935",
                    "name": "Rafael L. G. Raimundo"
                }
            ]
        },
        {
            "paperId": "1e3c0f4ef50fba8f74e7f32459de1a4fd1cbdeed",
            "title": "The Need for Interpretable Features",
            "abstract": "Through extensive experience developing and explaining machine learning (ML) applications for real-world domains, we have learned that ML models are only as interpretable as their features. Even simple, highly interpretable model types such as regression models can be difficult or impossible to understand if they use uninterpretable features. Different users, especially those using ML models for decision-making in their domains, may require different levels and types of feature interpretability. Furthermore, based on our experiences, we claim that the term \"interpretable feature\" is not specific nor detailed enough to capture the full extent to which features impact the usefulness of ML explanations. In this paper, we motivate and discuss three key lessons: 1) more attention should be given to what we refer to as the interpretable feature space, or the state of features that are useful to domain experts taking real-world actions, 2) a formal taxonomy is needed of the feature properties that may be required by these domain experts (we propose a partial taxonomy in this paper), and 3) transforms that take data from the model-ready state to an interpretable form are just as essential as traditional ML transforms that prepare features for the model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29977653",
                    "name": "Alexandra Zytek"
                },
                {
                    "authorId": "2205099",
                    "name": "Ignacio Arnaldo"
                },
                {
                    "authorId": "35386454",
                    "name": "Dongyu Liu"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1803567",
                    "name": "K. Veeramachaneni"
                }
            ]
        },
        {
            "paperId": "22a10985b694a285654414b87865a5a801a7937c",
            "title": "Sintel: A Machine Learning Framework to Extract Insights from Signals",
            "abstract": "The detection of anomalies in time series data is a critical task with many monitoring applications. Existing systems often fail to encompass an end-to-end detection process, to facilitate comparative analysis of various anomaly detection methods, or to incorporate human knowledge to refine output. This precludes current methods from being used in real-world settings by practitioners who are not ML experts. In this paper, we introduce Sintel, a machine learning framework for end-to-end time series tasks such as anomaly detection. The framework uses state-of-the-art approaches to support all steps of the anomaly detection process. Sintel logs the entire anomaly detection journey, providing detailed documentation of anomalies over time. It enables users to analyze signals, compare methods, and investigate anomalies through an interactive visualization tool, where they can annotate, modify, create, and remove events. Using these annotations, the framework leverages human knowledge to improve the anomaly detection pipeline. We demonstrate the usability, efficiency, and effectiveness of Sintel through a series of experiments on three public time series datasets, and through a real-world use case with spacecraft experts. Sintel's framework, code, and datasets are open-sourced at https://github.com/sintel-dev/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1414097513",
                    "name": "Sarah Alnegheimish"
                },
                {
                    "authorId": "35386454",
                    "name": "Dongyu Liu"
                },
                {
                    "authorId": "101345211",
                    "name": "Carles Sala Cladellas"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1803567",
                    "name": "K. Veeramachaneni"
                }
            ]
        },
        {
            "paperId": "5fee2e4f273ad3d605e8db86f5bbd20477757d95",
            "title": "Advances in Exploratory Data Analysis, Visualisation and Quality for Data Centric AI Systems",
            "abstract": "It is widely accepted that data preparation is one of the most time-consuming steps of the machine learning (ML) lifecycle. It is also one of the most important steps, as the quality of data directly influences the quality of a model. In this tutorial, we will discuss the importance and the role of exploratory data analysis (EDA) and data visualisation techniques to find data quality issues and for data preparation, relevant to building ML pipelines. We will also discuss the latest advances in these fields and bring out areas that need innovation. To make the tutorial actionable for practitioners, we will also discuss the most popular open-source packages that one can get started with along with their strengths and weaknesses. Finally, we will discuss on the challenges posed by industry workloads and the gaps to be addressed to make data-centric AI real in industry settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1557607422",
                    "name": "Hima Patel"
                },
                {
                    "authorId": "52205085",
                    "name": "Shanmukha C. Guttula"
                },
                {
                    "authorId": "41051307",
                    "name": "Ruhi Sharma Mittal"
                },
                {
                    "authorId": "3222980",
                    "name": "Naresh Manwani"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2181320255",
                    "name": "Abhijit Manatkar"
                }
            ]
        },
        {
            "paperId": "f35e2b062aa13166aabdaed7ae56666bec51b3ca",
            "title": "AER: Auto-Encoder with Regression for Time Series Anomaly Detection",
            "abstract": "Anomaly detection on time series data is increasingly common across various industrial domains that monitor metrics in order to prevent potential accidents and economic losses. However, a scarcity of labeled data and ambiguous definitions of anomalies can complicate these efforts. Recent unsupervised machine learning methods have made remarkable progress in tackling this problem using either single-timestamp predictions or time series reconstructions. While traditionally considered separately, these methods are not mutually exclusive and can offer complementary perspectives on anomaly detection. This paper first highlights the successes and limitations of prediction-based and reconstruction-based methods with visualized time series signals and anomaly scores. We then propose AER (Auto-encoder with Regression), a joint model that combines a vanilla auto-encoder and an LSTM regressor to incorporate the successes and address the limitations of each method. Our model can produce bi-directional predictions while simultaneously reconstructing the original time series by optimizing a joint objective function. Furthermore, we propose several ways of combining the prediction and reconstruction errors through a series of ablation studies. Finally, we compare the performance of the AER architecture against two prediction-based methods and three reconstruction-based methods on 12 well-known univariate time series datasets from NASA, Yahoo, Numenta, and UCR. The results show that AER has the highest averaged F1 score across all datasets (a 23.5% improvement compared to ARIMA) while retaining a runtime similar to its vanilla auto-encoder and regressor components. Our model is available in Orion1, an opensource benchmarking tool for time series anomaly detection.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2198487464",
                    "name": "Lawrence Wong"
                },
                {
                    "authorId": "35386454",
                    "name": "Dongyu Liu"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1414097513",
                    "name": "Sarah Alnegheimish"
                },
                {
                    "authorId": "1803567",
                    "name": "K. Veeramachaneni"
                }
            ]
        },
        {
            "paperId": "1dc47c70c5d4cb0e11391ef2b98120b8ae1796d3",
            "title": "QeNoBi: A System for QuErying and mining BehavIoral Patterns",
            "abstract": "We demonstrate QeNoBi, a system for mining and querying customer behavioral patterns. QeNoBi combines an interactive visual interface, on-demand mining, and efficient topk processing, to provide the exploration of customer behavior over time. QeNoBi relies on two distinct data models: a customercentric graph that represents customers with similar purchasing behaviors and is annotated with a change algebra to reflect their behavior evolution, and product-centric time series that reflect the evolution of customer purchases over time. Users can query both representations along three dimensions: shape (the sketched trend of the behavior), scope (the set of customers/products of interest), and time granularity. QeNoBi provides a holistic behavior exploration capability by allowing users to seamlessly switch between customer-centric and product-centric views in a coordinated manner, thereby catering to various needs. A demonstration of QeNoBi is available at https://bit.ly/2HlcO3S.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2037873034",
                    "name": "Abdelouahab Chibah"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "2debbef8475aa25ba594d25ae461941b2c83338d",
            "title": "Provenance-aware Discovery of Functional Dependencies on Integrated Views",
            "abstract": "The automatic discovery of functional dependencies (FDs) has been widely studied as one of the hardest problems in data profiling. Existing approaches have focused on making the FD computation efficient while inspecting single relations at a time. In this paper, for the first time we address the problem of inferring FDs for multiple relations as they occur in integrated views by solely using the functional dependencies of the base relations of the view itself. To this purpose, we leverage logical inference and selective mining and show that we can discover most of the exact FDs from the base relations and avoid the full computation of the FDs for the integrated view itself, while at the same time preserving the lineage of FDs of base relations. We propose algorithms to speedup the inferred FD discovery process and mine FDs on-the-fly only from necessary data partitions. We present InFine (INferred FunctIoNal dEpendency), an end-to-end solution to discover inferred FDs on integrated views by leveraging provenance information of base relations. Our experiments on a range of real-world and synthetic datasets demonstrate the benefits of our method over existing FD discovery methods that need to rerun the discovery process on the view from scratch and cannot exploit lineage information on the FDs. We show that InFine outperforms traditional methods necessitating the full integrated view computation by one to two order of magnitude in terms of runtime. It is also the most memory efficient method while preserving FD provenance information using mainly inference from base table with negligible execution time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10757249",
                    "name": "Ugo Comignani"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2928675",
                    "name": "No\u00ebl Novelli"
                },
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                }
            ]
        },
        {
            "paperId": "4cacd7073b1ae5db52327c3aa3d19e8d598778ed",
            "title": "R&R: Metric-guided Adversarial Sentence Generation",
            "abstract": "Adversarial examples are helpful for analyzing and improving the robustness of text classifiers. Generating high-quality adversarial examples is a challenging task as it requires generating fluent adversarial sentences that are semantically similar to the original sentences and preserve the original labels, while causing the classifier to misclassify them. Existing methods prioritize misclassification by maximizing each perturbation's effectiveness at misleading a text classifier; thus, the generated adversarial examples fall short in terms of fluency and similarity. In this paper, we propose a rewrite and rollback (R&R) framework for adversarial attack. It improves the quality of adversarial examples by optimizing a critique score which combines the fluency, similarity, and misclassification metrics. R&R generates high-quality adversarial examples by allowing exploration of perturbations that do not have immediate impact on the misclassification metric but can improve fluency and similarity metrics. We evaluate our method on 5 representative datasets and 3 classifier architectures. Our method outperforms current state-of-the-art in attack success rate by +16.2%, +12.8%, and +14.0% on the classifiers respectively. Code is available at https://github.com/DAI-Lab/fibber",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112281287",
                    "name": "Lei Xu"
                },
                {
                    "authorId": "1402348047",
                    "name": "Alfredo Cuesta-Infante"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1803567",
                    "name": "K. Veeramachaneni"
                }
            ]
        },
        {
            "paperId": "589158881e295ee568f8e11d92987b60e62f5f85",
            "title": "DORA THE EXPLORER: Exploring Very Large Data With Interactive Deep Reinforcement Learning",
            "abstract": "We demonstrate DORA THE EXPLORER, a system that guides users in finding items of interest in a very large data set. DORA THE EXPLORER provides users with the full spectrum of exploration modes and is driven by Data Familiarity or Curiosity, as well as User Interventions. DORA THE EXPLORER is able to handle data and search scenario complexity, i.e., the difficulty to find scattered/clustered individual records in the data set, and user ability to express what s/he needs. DORA THE EXPLORER relies on Deep Reinforcement Learning that combines intrinsic (curiosity) and extrinsic (familiarity) rewards. DORA's main goal is to support scientific discovery from data. We describe the system architecture and illustrate it with three demonstration scenarios on a 2.6 mil-lion galaxies SDSS, a large sky survey data set1. A video of DORA THE EXPLORER is available at https://bit.ly/dora-demo, the codehttps://github.com/apersonnaz/rl-guided-galaxy-exploration, and the application at https://bit.ly/dora-application",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072259106",
                    "name": "Aur\u00e9lien Personnaz"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "14469238",
                    "name": "M. Fabricius"
                },
                {
                    "authorId": "2075402169",
                    "name": "S. Subramanian"
                }
            ]
        },
        {
            "paperId": "654c8c71ffb34aaf29dfb285229d2f5198b4167b",
            "title": "Challenges in KDD and ML for Sustainable Development",
            "abstract": "Artificial Intelligence and machine learning techniques can offer powerful tools for addressing the greatest challenges facing humanity and helping society adapt to a rapidly changing climate, respond to disasters and pandemic crisis, and reach the United Nations (UN) Sustainable Development Goals (SDGs) by 2030. In recent approaches for mitigation and adaptation, data analytics and ML are only one part of the solution that requires interdisciplinary and methodological research and innovations. For example, challenges include multi-modal and multi-source data fusion to combine satellite imagery with other relevant data, handling noisy and missing ground data at various spatio-temporal scales, and ensembling multiple physical and ML models to improve prediction accuracy. Despite recognized successes, there are many areas where ML is not applicable, performs poorly or gives insights that are not actionable. This tutorial will survey the recent and significant contributions in KDD and ML for sustainable development and will highlight current challenges that need to be addressed to transform and equip engaged sustainability science with robust ML-based tools to support actionable decision-making for a more sustainable future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "46927039",
                    "name": "David Dao"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "2123019575",
                    "name": "Bedharta Goswami"
                }
            ]
        },
        {
            "paperId": "ad2b278ef28c0d7fa2ff9cc4d73616162629dee6",
            "title": "2nd International Workshop on Data Quality Assessment for Machine Learning",
            "abstract": "The 2nd International Workshop on Data Quality Assessment for Machine Learning (DQAML'21) is organized in conjunction with the Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD). This workshop aims to serve as a forum for the presentation of research related to data quality assessment and remediation in AI/ML pipeline. Data quality is a critical issue in the data preparation phase and involves numerous challenging problems related to detection, remediation, visualization and evaluation of data issues. The workshop aims to provide a platform to researchers and practitioners to discuss such challenges across different modalities of data like structured, time series, text and graphical. The aim is to attract perspectives from both industrial and academic circles.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1557607422",
                    "name": "Hima Patel"
                },
                {
                    "authorId": "1679182",
                    "name": "F. Ishikawa"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2115343577",
                    "name": "Nitin Gupta"
                },
                {
                    "authorId": "145640180",
                    "name": "S. Mehta"
                },
                {
                    "authorId": "2060026640",
                    "name": "Satoshi Masuda"
                },
                {
                    "authorId": "33906968",
                    "name": "Shashank Mujumdar"
                },
                {
                    "authorId": "2099539780",
                    "name": "S. Afzal"
                },
                {
                    "authorId": "1751538",
                    "name": "Srikanta J. Bedathur"
                },
                {
                    "authorId": "1873835",
                    "name": "Y. Nishi"
                }
            ]
        },
        {
            "paperId": "cb0ee17a0a9d709cad96bf65dc40d3f7bcc49fd9",
            "title": "A Framework for Statistically-Sound Customer Segment Search",
            "abstract": "We develop S4, a Statistically-Sound Segment Search framework that combines principled data partitioning and sound statistical testing to verify common hypotheses in retail data and return interpretable customer data segments. Our framework accommodates one-sample, two-sample, and multiple-sample testing, to provide various aggregations and comparisons of customer transactions. To control the proportion of false discoveries in multiple hypothesis testing, we enforce an FDR-controlling procedure and formulate a unified optimization problem that returns customer data segments that satisfy the test for a given significance level, maximize coverage of the input data, and are within a risk capital. We develop a greedy algorithm to explore different data partitions and test multiple hypotheses in a sound manner. Our extensive experiments on four retail data sets examine the interaction between significance, risk and coverage, and demonstrate the expressivity, usefulness, and scalability of S4 in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2037873034",
                    "name": "Abdelouahab Chibah"
                }
            ]
        },
        {
            "paperId": "f061cc855b45a535f07a217734514dbb6b9f9152",
            "title": "Balancing Familiarity and Curiosity in Data Exploration with Deep Reinforcement Learning",
            "abstract": "The ability to find a set of records in Exploratory Data Analysis (EDA) hinges on the scattering of objects in the data set and the on users\u2019 knowledge of data and their ability to express their needs. This yields a wide range of EDA scenarios and solutions that differ in the guidance they provide to users. In this paper, we investigate the interplay between modeling curiosity and familiarity in Deep Reinforcement Learning (DRL) and expressive data exploration operators. We formalize curiosity as intrinsic reward and familiarity as extrinsic reward. We examine the behavior of several policies learned for different weights for those rewards. Our experiments on SDSS, a very large sky survey data set1 provide several insights and justify the need for a deeper examination of combining DRL and data exploration operators that go beyond drill-downs and roll-ups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072259106",
                    "name": "Aur\u00e9lien Personnaz"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "14469238",
                    "name": "M. Fabricius"
                },
                {
                    "authorId": "2075402169",
                    "name": "S. Subramanian"
                }
            ]
        },
        {
            "paperId": "5d0ca8ab334ac85925ce00055560fb5bc91be441",
            "title": "Data Quality Checking for Machine Learning with MeSQuaL",
            "abstract": "This demo proposes MeSQuaL, a system for profiling and checking data quality before further tasks, such as data analytics and machine learning. MeSQuaL extends SQL for querying relational data with constraints on data quality and facilitates the verification of statistical tests. The system includes: (1) a query interpreter for SQuaL, the SQL-extended language we propose for declaring and querying data with data quality checks and statistical tests; (2) an extensible library of user-defined functions for profiling the data and computing various data quality indicators;and (3) a user interface for declaring data quality constraints,profiling data, monitoring data quality with SQuaL queries, and visualizing the results via data quality dashboards. We showcase our system in action with various scenarios on real-world datasets and show its usability for monitoring data quality over time and checking the quality of data on-demand",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10757249",
                    "name": "Ugo Comignani"
                },
                {
                    "authorId": "2928675",
                    "name": "No\u00ebl Novelli"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "7c8f47f40887898def83f01764b38985ff135ee4",
            "title": "Active Reinforcement Learning for Data Preparation: Learn2Clean with Human-In-The-Loop",
            "abstract": "Data cleaning and data preparation are challenging but necessary tasks to prevent incorrect results, biases, and misleading conclusions to be obtained from \u201cdirty\u201d data. For a given ML model and a dataset, a plethora of data preprocessing techniques and alternative data cleaning strategies with various con\ufb01gurations are available, but they may lead to dramatically different outputs with unequal result quality performances. As illustrated in Fig. 1, data cleaning and preparation require a sophisticated sequence of tasks for the detection and elimination of a variety of intricate data quality problems (e.g., duplicates, inconsistent, missing, and outlying values). Generally, the users may not know which preprocessing methods can be applied to optimize the \ufb01nal results downstream. This would require executing all possible methods for each task of preprocessing, as well as all the possible combinations of the methods with different orderings and con\ufb01gu-rations. These methods can be applied to the whole or some parts of the dataset with eventual re-iterations. Data cleaning and preparation are intrinsically \u201cAI-hard\u201d as they can hardly be achieved by a fully automated system. AutoML approaches can optimize the hyper-parameters of a considered ML model, but they support only a limited number of preprocessing steps with by-default methods. We argue that more efforts should be devoted to proposing a principled and ef\ufb01cient data preparation approach to help and learn from the user in selecting the optimal sequence of data preparation tasks. Improving the quality of input data for ML and leveraging human expertise, subsequent learning performance will bene\ufb01t.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "e35dbaf3b081874e3a2e99e007f22a22190e033c",
            "title": "Discovering Multi-Table Functional Dependencies Without Full Join Computation",
            "abstract": "In this paper, we study the problem of discovering join FDs, i.e., functional dependencies (FDs) that hold on multiple joined tables. We leverage logical inference, selective mining, and sampling and show that we can discover most of the exact join FDs from the single tables participating to the join and avoid the full computation of the join result. We propose algorithms to speed-up the join FD discovery process and mine FDs on the fly only from necessary data partitions. We introduce JEDI (Join dEpendency DIscovery), our solution to discover join FDs without computation of the full join beforehand. Our experiments on a range of real-world and synthetic data demonstrate the benefits of our method over existing FD discovery methods that need to precompute the join results before discovering the FDs. We show that the performance depends on the cardinalities and coverage of the join attribute values: for join operations with low coverage, JEDI with selective mining outperforms the competing methods using the straightforward approach of full join computation by one order of magnitude in terms of runtime and can discover three-quarters of the exact join FDs using mainly logical inference in half of its total execution time on average. For higher join coverage, JEDI with sampling reaches precision of 1 with only 63% of the table input size on average.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10757249",
                    "name": "Ugo Comignani"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2928675",
                    "name": "No\u00ebl Novelli"
                }
            ]
        },
        {
            "paperId": "5018481e5d89fb7e0bd750f7576701d5daee4d45",
            "title": "ML-Based Knowledge Graph Curation: Current Solutions and Challenges",
            "abstract": "With the success of machine learning (ML) techniques, ML has already proved a tremendous potential to impact the foundations, algorithms, and models of several data management tasks, such as error detection, data quality assessment, data cleaning, and data integration. In Knowledge Graphs, part of the data preparation and cleaning processes, such as data linking, identity disambiguation, or missing value inference and completion could be automated by making a ML model \u201clearn\u201d and predict the matches routinely with different degrees of supervision. This talk will survey the recent trends of applying machine learning solutions to improve and facilitate Knowledge Graph curation and enrichment, as one of the most critical tasks impacting Web search and query-answering. Finally, the talk will discuss the next research challenges in the convergence of machine learning and management of Knowledge Graph evolution and preservation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "ae61cb3f4bc4047d85aa91ab28c72a805bf77535",
            "title": "Learn2Clean: Optimizing the Sequence of Tasks for Web Data Preparation",
            "abstract": "Data cleaning and preparation has been a long-standing challenge in data science to avoid incorrect results and misleading conclusions obtained from dirty data. For a given dataset and a given machine learning-based task, a plethora of data preprocessing techniques and alternative data curation strategies may lead to dramatically different outputs with unequal quality performance. Most current work on data cleaning and automated machine learning, however, focus on developing either cleaning algorithms or user-guided systems or argue to rely on a principled method to select the sequence of data preprocessing steps that can lead to the optimal quality performance of. In this paper, we propose Learn2Clean, a method based on Q-Learning, a model-free reinforcement learning technique that selects, for a given dataset, a ML model, and a quality performance metric, the optimal sequence of tasks for preprocessing the data such that the quality of the ML model result is maximized. As a preliminary validation of our approach in the context of Web data analytics, we present some promising results on data preparation for clustering, regression, and classification on real-world data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "fe5a74ab9020db7091dd7c3d34110bbc3270234d",
            "title": "Are Outlier Detection Methods Resilient to Sampling?",
            "abstract": "Outlier detection is a fundamental task in data mining and has many applications including detecting errors in databases. While there has been extensive prior work on methods for outlier detection, modern datasets often have sizes that are beyond the ability of commonly used methods to process the data within a reasonable time. To overcome this issue, outlier detection methods can be trained over samples of the full-sized dataset. However, it is not clear how a model trained on a sample compares with one trained on the entire dataset. In this paper, we introduce the notion of resilience to sampling for outlier detection methods. Orthogonal to traditional performance metrics such as precision/recall, resilience represents the extent to which the outliers detected by a method applied to samples from a sampling scheme matches those when applied to the whole dataset. We propose a novel approach for estimating the resilience to sampling of both individual outlier methods and their ensembles. We performed an extensive experimental study on synthetic and real-world datasets where we study seven diverse and representative outlier detection methods, compare results obtained from samples versus those obtained from the whole datasets and evaluate the accuracy of our resilience estimates. We observed that the methods are not equally resilient to a given sampling scheme and it is often the case that careful joint selection of both the sampling scheme and the outlier detection method is necessary. It is our hope that the paper initiates research on designing outlier detection algorithms that are resilient to sampling.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2104305",
                    "name": "J. Loh"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                }
            ]
        },
        {
            "paperId": "27294e7701e3ede417b9fc55da628f627bb20db5",
            "title": "Discovery of Genuine Functional Dependencies from Relational Data with Missing Values",
            "abstract": "Functional dependencies (FDs) play an important role in maintaining data quality. They can be used to enforce data consistency and to guide repairs over a database. In this work, we investigate the problem of missing values and its impact on FD discovery. When using existing FD discovery algorithms, some genuine FDs could not be detected precisely due to missing values or some non-genuine FDs can be discovered even though they are caused by missing values with a certain NULL semantics. We define a notion of genuineness and propose algorithms to compute the genuineness score of a discovered FD. This can be used to identify the genuine FDs among the set of all valid dependencies that hold on the data. We evaluate the quality of our method over various real-world and semi-synthetic datasets with extensive experiments. The results show that our method performs well for relatively large FD sets and is able to accurately capture genuine FDs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "3417798",
                    "name": "Hazar Harmouch"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "2928675",
                    "name": "No\u00ebl Novelli"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                }
            ]
        },
        {
            "paperId": "37e8e0d5e15e4d07252f7bd70aa16a281a4e9860",
            "title": "Principled Data Preprocessing: Application to Biological Aquatic Indicators of Water Pollution",
            "abstract": "In many biological studies, statistical and data mining methods are extensively used to analyze the data and discover actionable knowledge. But, bad data quality causing incorrect analysis results and wrong interpretations may induce misleading conclusions and inadequate decisions. To ensure the validity of the results, avoid bias and data misuse, it is necessary to control not only the whole analytical pipeline, but most importantly the quality of the data with appropriate data preprocessing choices. Since various preprocessing techniques and alternative strategies may lead to dramatically different outputs, it is crucial to rely on a principled and rigorous method to select the optimal set of data preprocessing steps that depends both on the input data distributional characteristics and on the inherent characteristics of the targeted statistical or data mining methods. In this paper, we propose a method that selects, given a dataset, the optimal set of preprocessing tasks to apply to the data such that the overall data preprocessing output maximizes the quality of the analytical results for various techniques of clustering, regression, and classification. We present some promising results that validate our approach on biomonitoring data preparation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144609507",
                    "name": "Eva C. Serrano Balderas"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2110302567",
                    "name": "Maria Aurora Armienta Hernandez"
                },
                {
                    "authorId": "2081081",
                    "name": "C. Grac"
                }
            ]
        },
        {
            "paperId": "4cafe078e78a2140189f5bad6a7c9d757a79616e",
            "title": "Profiling DRDoS Attacks with Data Analytics Pipeline",
            "abstract": "A large amount of Distributed Reflective Denial-of-Service (DRDoS) attacks are launched every day, and our understanding of the modus operandi of their perpetrators is yet very limited as we are submerged with so Big Data to analyze and do not have reliable and complete ways to validate our findings. In this paper, we propose a first analytic pipeline that enables us to cluster and characterize attack campaigns into several main profiles that exhibit similarities. These similarities are due to common technical properties of the underlying infrastructures used to launch these attacks. Although we do not have access to the ground truth and we do not know how many perpetrators are acting behind the scene, we can group their attacks based on relevant commonalities with cluster ensembling to estimate their number and capture their profiles over time. Specifically, our results show that we can repeatably identify and group together common profiles of attacks while considering domain expert's constraint in the cluster ensembles. From the obtained consensus clusters, we can generate comprehensive rules that characterize past campaigns and that can be used for classifying the next ones despite the evolving nature of the attacks. Such rules can be further used to filter out garbage traffic in Internet Service Provider networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2361773",
                    "name": "Yury Zhauniarovich"
                }
            ]
        },
        {
            "paperId": "d671332e05122be7222e8064a7b0ed5b3e78c235",
            "title": "IndexMEED cases studies using \"Omics\" data with graph theory",
            "abstract": "Data produced within marine and terrestrial biodiversity research projects that evaluate and monitor Good Environmental Status, have a high potential for use by stakeholders involved in environmental management. However, environmental data, especially in ecology, are not readily accessible to various users. The specific scientific goals and the logics of project organization and information gathering lead to a decentralized data distribution. In such a heterogeneous system across different organizations and data formats, it is difficult to efficiently harmonize the outputs. Few tools are available to assist. For instance standards and specific protocols can be applied to interconnect databases. Such semantic approaches greatly increase data interoperability. \nThis communication present the recent results and the consortium IndexMEED (Indexing for Mining Ecological and Environmental Data) activity that aims to build new approaches to investigate complex research questions, and support the emergence of new scientific hypotheses based on graph theory Auber et al. 2014). Current developments in data mining based on graphs, as well as the potential for relevant contributions to environmental research, particularly about strategic decision-making, and new ways of organizing data will be presented (David et al. 2015). In particular, the consortium makes decisions on how i) to analyze heterogeneous distributed data spread throughout different databases combining molecular and habitat characteristics data [3], ii) to create matches and incorporate some approximations, iii) to identify statistical relationships between observed data and the emergence of contextual patterns using a calculation library and distributed calculation center at the European level, iv) to encourage openness and sharing data while complying with the general principles of FAIR (Findable, Accessible, Interoperable, Re-usable and citable) in order to enhance data value and their utilization. IndexMEED participants are now exploring the ability of two scientific communities (ecology sensu lato and computer sciences) to work together, using several studies cases. The ECOSCOPE project aims to meet the need to access structured and complementary omics-datasets to better understand biodiversity state and its dynamics. Indeed, the ECOSCOPE case study targets to visualize, through the graph approach, links between datasets and databases from genetics to ecosystems. Another case study, displaying anthropology fossils and omics on the same graph, will also be presented. DEVOTES (DEVelopment Of innovative Tools for understanding marine biodiversity and assessing good Environmental Status) and CIGESMED (Coralligenous based Indicators to evaluate and monitor the \"Good Environmental Status\" of the MEDiterranean coastal water) European projects, conducted by IMBE, are focused on photo quadrats, cartography and omics data of the marine hard bottom in order to discover context patterns helpful to build decision support system building. Study case \u201c65 Millions d\u2019observateurs\u201d French project is testing AskOmics to provide a graph-based querying interface using RDF (Resource Description Framework) and SPARQL technologies. \nScientific questions can be resolved by the new data mining approaches that offer new ways to investigate heterogeneous environmental data with graph mining (Munoz et al. 2017). The uses of data from biodiversity research demonstrate the prototype functionalities (David et al. 2016) and introduce new perspectives to analyze environmental and societal responses including decision-making at large scale, both at the information system level and the observing system level than at the observed system level.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143654877",
                    "name": "R. David"
                },
                {
                    "authorId": "34217236",
                    "name": "J. F\u00e9ral"
                },
                {
                    "authorId": "1405561098",
                    "name": "Sophie Archambeau"
                },
                {
                    "authorId": "8133171",
                    "name": "F. Arnaud"
                },
                {
                    "authorId": "2460043",
                    "name": "D. Auber"
                },
                {
                    "authorId": "2045444",
                    "name": "N. Bailly"
                },
                {
                    "authorId": "103867968",
                    "name": "L. Bernard"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "38272806",
                    "name": "C. Blanpain"
                },
                {
                    "authorId": "1699425",
                    "name": "V. Breton"
                },
                {
                    "authorId": "1405561074",
                    "name": "Anne Chenuil-Maurel"
                },
                {
                    "authorId": "65736999",
                    "name": "Anna Cohen Nabeiro"
                },
                {
                    "authorId": "3198998",
                    "name": "Alrick Dias"
                },
                {
                    "authorId": "17837444",
                    "name": "A. Delavaud"
                },
                {
                    "authorId": "1405561084",
                    "name": "Robin Goffaud"
                },
                {
                    "authorId": "3109629",
                    "name": "S. Gachet"
                },
                {
                    "authorId": "144829520",
                    "name": "K. Gibert"
                },
                {
                    "authorId": "152816488",
                    "name": "Manuel Herrera Fernandez"
                },
                {
                    "authorId": "1917764",
                    "name": "Luc Hogie"
                },
                {
                    "authorId": "1789397",
                    "name": "D. Ienco"
                },
                {
                    "authorId": "2001448",
                    "name": "R. Julliard"
                },
                {
                    "authorId": "8674482",
                    "name": "Y. L. Bras"
                },
                {
                    "authorId": "2455909",
                    "name": "J. Lecubin"
                },
                {
                    "authorId": "3016922",
                    "name": "Y. Legr\u00e9"
                },
                {
                    "authorId": "8710412",
                    "name": "M. Leydet"
                },
                {
                    "authorId": "21457442",
                    "name": "G. Lo\u00efs"
                },
                {
                    "authorId": "5411262",
                    "name": "B. Madon"
                },
                {
                    "authorId": "2055640200",
                    "name": "Fran\u00e7ois Marchal"
                },
                {
                    "authorId": "144994610",
                    "name": "V. Mu\u00f1oz"
                },
                {
                    "authorId": "144202198",
                    "name": "J. Meunier"
                },
                {
                    "authorId": "88124476",
                    "name": "J. Mihoub"
                },
                {
                    "authorId": "1918610",
                    "name": "I. Mougenot"
                },
                {
                    "authorId": "66547937",
                    "name": "Sophie Pamerlon"
                },
                {
                    "authorId": "1405561104",
                    "name": "Eric Peletier"
                },
                {
                    "authorId": "15259531",
                    "name": "G. Romier"
                },
                {
                    "authorId": "1403297797",
                    "name": "D. Roux-Michollet"
                },
                {
                    "authorId": "40511052",
                    "name": "A. Specht"
                },
                {
                    "authorId": "36548707",
                    "name": "C. Surace"
                },
                {
                    "authorId": "144011066",
                    "name": "J. Raynal"
                },
                {
                    "authorId": "5899667",
                    "name": "T. Tatoni"
                }
            ]
        },
        {
            "paperId": "e135848a7c9d7f8c2607459e93ae5cd5122e61d0",
            "title": "UGuide: User-Guided Discovery of FD-Detectable Errors",
            "abstract": "Error detection is the process of identifying problematic data cells that are different from their ground truth. Functional dependencies (FDs) have been widely studied in support of this process. Oftentimes, it is assumed that FDs are given by experts. Unfortunately, it is usually hard and expensive for the experts to define such FDs. In addition, automatic data profiling over dirty data in order to find correct FDs is known to be a hard problem. In this paper, we propose an end-to-end solution to detect FD-detectable errors from dirty data. The broad intuition is that given a dirty dataset, it is feasible to automatically find approximate FDs, as well as data that is possibly erroneous. Arguably, at this point, only experts can confirm true FDs or true errors. However, in practice, experts never have enough budget to find all errors. Hence, our problem is, given a limited budget of expert's time, which questions we should ask, either FDs, cells, or tuples, such that we can find as many data errors as possible. We present efficient algorithms to interact with the user. Extensive experiments demonstrate that our proposed framework is effective in detecting errors from dirty data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2168047",
                    "name": "M. Ouzzani"
                },
                {
                    "authorId": "1399355221",
                    "name": "Jorge-Arnulfo Quian\u00e9-Ruiz"
                },
                {
                    "authorId": "8669763",
                    "name": "N. Tang"
                }
            ]
        },
        {
            "paperId": "17c6295a0ace5919b1f8fa6c35180461cb631433",
            "title": "Using Twitter to Understand Public Interest in Climate Change: The Case of Qatar",
            "abstract": "\n \n Climate change has received an extensive attention from public opinion in the last couple of years, after being considered for decades as an exclusive scientific debate. Governments and world-wide organizations such as the United Nations are working more than ever on raising and maintaining public awareness toward this global issue. In the present study, we examine and analyze Climate Change conversations in Qatar's Twittersphere, and sense public awareness towards this global and shared problem in general, and its various related topics in particular. Such topics include but are not limited to politics, economy, disasters, energy and sandstorms. To address this concern, we collect and analyze a large dataset of 109 million tweets posted by 98K distinct users living in Qatar -- one of the largest emitters of CO2 worldwide. We use a taxonomy of climate change topics created as part of the United Nations Pulse project to capture the climate change discourse in more than 36K tweets. We also examine which topics people refer to when they discuss climate change, and perform different analysis to understand the temporal dynamics of public interest toward these topics.\n \n",
            "fieldsOfStudy": [
                "Computer Science",
                "Political Science"
            ],
            "authors": [
                {
                    "authorId": "3009678",
                    "name": "Sofiane Abbar"
                },
                {
                    "authorId": "3391438",
                    "name": "Tahar Zanouda"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1390131709",
                    "name": "Javier Borge-Holthoefer"
                }
            ]
        },
        {
            "paperId": "3be8f544d5f5ec37959a967376602846f098d47e",
            "title": "On the Use of Ontology as A Priori Knowledge into Constrained Clustering",
            "abstract": "Recent studies have shown that the use of a priori knowledge can significantly improve the results of unsupervised classification. However, capturing and formatting such knowledge as constraints is not only very expensive requiring the sustained involvement of an expert but it is also very difficult because some valuable information can be lost when it cannot be encoded as constraints. In this paper, we propose a new constraint-based clustering approach based on ontology reasoning for automatically generating constraints and bridging the semantic gap in satellite image labeling. The use of ontology as a priori knowledge has many advantages that we leverage in the context of satellite image interpretation. The experiments we conduct have shown that our proposed approach can deal with incomplete knowledge while completely exploiting the available one.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34992512",
                    "name": "Hatim Chahdi"
                },
                {
                    "authorId": "1730718",
                    "name": "Nistor Grozavu"
                },
                {
                    "authorId": "1918610",
                    "name": "I. Mougenot"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1734694",
                    "name": "Youn\u00e8s Bennani"
                }
            ]
        },
        {
            "paperId": "540f4c458612353b581ca55a22ef919ca7fb436e",
            "title": "Collaborative segmentation and classification for remote sensing image analysis",
            "abstract": "In this article we present CoSC, a generic framework for collaborative segmentation and classification. The framework is guided by both radiometric homogeneity based criteria and implicit semantic criteria to segment and extract the objects of a given thematic class. We present a proof-of-concept case-study and show that CoSC is able to reach higher confidence for object classification and results in significant improvement of the whole segmentation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176417",
                    "name": "Andres Troya-Galvis"
                },
                {
                    "authorId": "1808290",
                    "name": "P. Gan\u00e7arski"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "5b1c8b1ad5d7f18df6124e5dcb7833bfba027901",
            "title": "Rheem: Enabling Multi-Platform Task Execution",
            "abstract": "Many emerging applications, from domains such as healthcare and oil & gas, require several data processing systems for complex analytics. This demo paper showcases system, a framework that provides multi-platform task execution for such applications. It features a three-layer data processing abstraction and a new query optimization approach for multi-platform settings. We will demonstrate the strengths of system by using real-world scenarios from three different applications, namely, machine learning, data cleaning, and data fusion.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "3113746",
                    "name": "M. Ba"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "50793091",
                    "name": "S. Chawla"
                },
                {
                    "authorId": "145188857",
                    "name": "A. Elmagarmid"
                },
                {
                    "authorId": "40596127",
                    "name": "Hossam M. Hammady"
                },
                {
                    "authorId": "3416143",
                    "name": "Yasser Idris"
                },
                {
                    "authorId": "2827559",
                    "name": "Zoi Kaoudi"
                },
                {
                    "authorId": "2407411",
                    "name": "Zuhair Khayyat"
                },
                {
                    "authorId": "143924672",
                    "name": "Sebastian Kruse"
                },
                {
                    "authorId": "2168047",
                    "name": "M. Ouzzani"
                },
                {
                    "authorId": "1802817",
                    "name": "Paolo Papotti"
                },
                {
                    "authorId": "1399355221",
                    "name": "Jorge-Arnulfo Quian\u00e9-Ruiz"
                },
                {
                    "authorId": "8669763",
                    "name": "N. Tang"
                },
                {
                    "authorId": "1693515",
                    "name": "Mohammed J. Zaki"
                }
            ]
        },
        {
            "paperId": "66839569dc50ac3ad460d7b23d622d757f346ae9",
            "title": "The 11th International Workshop on Quality in DataBases in conjunction with VLDB 2016",
            "abstract": "Data quality problems arise frequently when data is integrated from disparate sources. In the context of Big Data applications, data quality is becoming more important because of the unprecedented volume, large variety, and high velocity. The challenges caused by volume and velocity of Big Data have been addressed by many research projects and commercial solutions and can be partially solved by modern, scalable data management systems. However, variety remains to be a daunting challenge for Big Data Integration and requires also special methods for data quality management. Variety (or heterogeneity) exists at several levels: at the instance level, the same entity might be described with different attributes; at the schema level, the data is structured with various schemas; but also at the level of the modeling language, different data models can be used (e.g., relational, XML, or a document-oriented JSON representation). This might lead to data quality issues such as consistency, understandability, or completeness. \n \nThe heterogeneity of data sources in the Big Data Era requires new integration approaches which can handle the large volume and speed of the generated data as well as the variety and quality of the data. Traditional \u2018schema first\u2019 approaches as in the relational world with data warehouse systems and ETL (Extract-Transform-Load) processes are inappropriate for a flexible and dynamically changing data management landscape. The requirement for pre-defined, explicit schemas is a limitation which has drawn interest of many developers and researchers to NoSQL data management systems as these systems should provide data management features for a high amount of schema-less data. Nevertheless, a one-size-fits-all Big Data system is unlikely to solve all the challenges which are required from data management systems today. Instead, multiple classes of systems, optimized for specific requirements or hardware platforms, will co-exist in a data management landscape. \n \nThus, heterogeneity and data quality are challenges for many Big Data applications. While in some applications, a limited data quality for individual data items does not cause serious problems when a huge amount of data is aggregated, data quality problems in data sources are often revealed by the integration of these sources with other information. Data quality has been coined as \u2018fitness for use\u2019; thus, if data is used in another context than originally planned, data quality might become an issue. Similar observations have been also made for data warehouses which lead to a separate research area about data warehouse quality. \n \nThe workshop QDB 2016 aims at discussing recent advances and challenges on data quality management in database systems, and focuses especially on problems in related to Big Data Integration and Big Data Quality. The workshop will provide a forum for the presentation of research results, a panel discussion, and an attractive keynote speaker.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1761310",
                    "name": "C. Quix"
                },
                {
                    "authorId": "73357762",
                    "name": "Verikat N. Gudivada"
                },
                {
                    "authorId": "34610553",
                    "name": "Rihan Hai"
                },
                {
                    "authorId": "2145338419",
                    "name": "Hongzhi Wang"
                }
            ]
        },
        {
            "paperId": "c2aca5f265f9825ca47c9ec975428e25745fe60a",
            "title": "Metabolomic Data Profiling for Diabetes Research in Qatar",
            "abstract": "Diabetes is a leading health problem inthe developed world. The recent surge of wealth inQatar has made it one of the most vulnerable nationsto diabetes and related diseases. Recent technologicaladvances in 1H nuclear magnetic resonance (NMR) spectroscopy techniques for metabolomics profilingoffer a great opportunity for biomarkers discovery tobetter understand the disease. Using this technology, we present in this study, an integrative approach witha newly proposed algorithm named Kernel SpectralClustering (KSC) to discover new metabolites andpossibly new biomarkers. We performed an integrativeanalysis of 1H NMR spectras measured in urine, from348 participants of the Qatar Metabolomics Study onDiabetes (QMDiab). Our analyses revealed groupedmetabolites that correlate with diabetes and identifiedspecific metabolites affected by antidiabetes medication, which constraints differentiation between diabetic andcontrol patients.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39113751",
                    "name": "Raghvendra Mall"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1804689",
                    "name": "H. Bensmail"
                }
            ]
        },
        {
            "paperId": "c603e7ec021b7710601ced4c83d024f54586ce63",
            "title": "Veracity of Big Data: Challenges of Cross-Modal Truth Discovery",
            "abstract": "As online user-generated content grows exponentially, the reliance on web and social media data is increasing. Truth discovery from the web has significant practical importance as online rumors and misinformation can have tremendous impacts on our society and everyday life. One of the fundamental difficulties is that data can be biased, noisy, outdated, incorrect, misleading, and thus unreliable. Conflicting data from multiple sources amplifies this problem, and the veracity of data has to be estimated. Beyond the emerging field of computational journalism and the success of online fact-checkers (e.g., FactCheck1 ClaimBuster2), truth discovery is a long-standing and challenging problem studied by many research communities in artificial intelligence, databases, and complex systems, and under various names: fact-checking, data and knowledge fusion, information trustworthiness, credibility, and information corroboration (see Berti-Equille and Borge-Holthoefer [2015] for a survey). The ultimate goal is to predict the truth label of a set of assertions claimed by multiple information sources and to infer sources\u2019 reliability with no or little prior knowledge. One major line of previous work aimed at iteratively computing and updating the source\u2019s trustworthiness as a belief function in its claims, and then the belief score of each claim as a function of its sources\u2019 trustworthiness [Yin et al. 2008]. More complex probabilistic models have then incorporated various aspects beyond source trustworthiness and claim belief, such as the dependence between sources, the correlation of claims [Pochampally et al. 2014], and the notion of evolving truth. Recent contributions have further relaxed",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "3113746",
                    "name": "M. Ba"
                }
            ]
        },
        {
            "paperId": "cc9e09c0a236af79b4f561e7602d88b3f4ffaf85",
            "title": "Discovering the Truth on the Web Data: One Facet of Data Forensics",
            "abstract": "Data Forensics with Analytics, or DAFNA for short, is an ambitious project initiated by the Data Analytics Research Group in Qatar Computing Research Institute, Hamad Bin Khalifa University. It main goal is to provide effective algorithms and tools for determining the veracity of structured information when they originate from multiple sources. The ability to efficiently estimate the veracity of data, along with the reliability level of the information sources, is a challenging problem with many real-world use cases (e.g., data fusion, social data analytics, rumour detection, etc.) in which users rely on a semi-automated data extraction and integration process in order to consume high quality information for personal or business purposes. DAFNA's vision is to provide a suite of tools for Data Forensics and investigate various research topics such as fact-checking and truth discovery and their practical applicability. We will present our ongoing development (dafna.qcri.org) on extensively comparing the state-of-the-art truth discovery algorithms, releasing a new system and the first REST API for truth discovery, and designing a novel hybrid truth discovery approach using active ensembling. Finally, we will briefly present real-world applications of truth discovery from Web data. Efficient Truth Discovery. Truth discovery is a hard problem to deal with since there is no a priori knowledge about the veracity of provided information and the reliability level of online sources. This raises many questions about a thorough understanding of the state-of-the-art truth discovery algorithms and their applicability for actionable truth discovery. A new truth discovery approach is needed and it should be rather comprehensible and domain-independent. In addition, it should take advantage of the benefits of existing solutions, while being built on realistic assumptions for an easy use in real-world applications. In this context, we propose an approach that deals with open truth discovery challenges and consists of the following contributions: (i) The thorough comparative study of existing truth discovery algorithms; (ii) The design and release of the first online truth discovery system and the first REST API for truth discovery available at dafna.qcri.org; (iii) An hybrid truth discovery method using active ensembling; and (iv) An application to query answering related to Qatar where the veracity of information provided by multiple Web sources is estimated.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3113746",
                    "name": "M. Ba"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "40596127",
                    "name": "Hossam M. Hammady"
                }
            ]
        },
        {
            "paperId": "d18aeed398dfe5fec2f2def10ef5d9c179961a6e",
            "title": "VERA: A Platform for Veracity Estimation over Web Data",
            "abstract": "Social networks and the Web in general are characterized by multiple information sources often claiming conflicting data values. Data veracity is hard to estimate, especially when there is no prior knowledge about the sources or the claims and in time-dependent scenarios where initially very few observers can report first information. Despite the wide set of recently proposed truth discovery approaches, \"no-one-fits-all\" solution emerges for estimating the veracity of on-line information in open contexts. However, analyzing the space of conflicting information and disagreeing sources might be relevant, as well as ensembling multiple truth discovery methods. This demonstration presents VERA, a Web-based platform that supports information extraction from Web textual data and micro-texts from Twitter and estimates data veracity. Given a user query, VERA systematically extracts entities and relations from Web content, structures them as claims relevant to the query and gathers more conflicting/corroborating information. VERA combines multiple truth discovery algorithms through ensembling returns the veracity label and score of each data value and the trustworthiness scores of the sources. VERA will be demonstrated through several real-world scenarios to show its potential value for fact-checking from Web data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3113746",
                    "name": "M. Ba"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2053917208",
                    "name": "Kushal Shah"
                },
                {
                    "authorId": "40596127",
                    "name": "Hossam M. Hammady"
                }
            ]
        },
        {
            "paperId": "e936b50eba9f7a08729579dcbfce2e52be58dc0e",
            "title": "Scaling up truth discovery",
            "abstract": "The evolution of the Web from a technology platform to a social ecosystem has resulted in unprecedented data volumes being continuously generated, exchanged, and consumed. User-generated content on the Web is massive, highly dynamic, and characterized by a combination of factual data and opinion data. False information, rumors, and fake contents can be easily spread across multiple sources, making it hard to distinguish between what is true and what is not. Truth discovery (also known as fact-checking) has recently gained lot of interest from Data Science communities. This tutorial will attempt to cover recent work on truth-finding and how it can scale Big Data. We will provide a broad overview with new insights, highlighting the progress made on truth discovery from information extraction, data and knowledge fusion, as well as modeling of misinformation dynamics in social networks. We will review in details current models, algorithms, and techniques proposed by various research communities whose contributions converge towards the same goal of estimating the veracity of data in a dynamic world. Our aim is to bridge theory and practice and introduce recent work from diverse disciplines to database people to be better equipped for addressing the challenges of truth discovery in Big Data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "2b5b9dcbd36054107ebc19b9b0fb823f76ec3dd0",
            "title": "Towards Principled Data Science Assessment - The Personal Data Science Process (PdsP)",
            "abstract": "With the Unstoppable Advance of Big Data, the Role of Data Scientist Is Becoming More Important than Ever before, in This Position Paper, We Argue That Scientists Should Be Able to Acknowledge the Importance of Data Quality Management in Data Science and Rely on a Principled Methodology When Performing Tasks Related to Data Management, in Order to Quantify How Much a Data Scientist Is Able to Perform the Core of Data Management Activities We Propose the Personal Data Science Process (PdsP), Which Includes Five Staged Qualifications for Data Science Professionals, the Qualifications Are based on Two Dimensions: Personal Data Management Maturity (PDMM) and Personal Data Science Performance (PDSPf), the First One Is Defined According to Dgmr, a Data Management Maturity Model, Which Include Processes Related to the Areas of Data Management: Data Governance, Data Management, and Data Quality Management, the Second One, PDSPf, Is Grounded on PSP (Personal Software Process) and Cover the Personal Skills and Knowledge of Data Scientist When Participating in a Data Science Project, These Dimensions Will Allow to Developing a Measure of How Well a Data Scientist Can Contribute to the Success of the Organization in Terms of Performance and Skills Appraisal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709884",
                    "name": "I. Caballero"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1702096",
                    "name": "M. Piattini"
                }
            ]
        },
        {
            "paperId": "3853a0d43c98ebd0d6b5da21abfe73a0cb115785",
            "title": "Unsupervised Quantification of Under- and Over-Segmentation for Object-Based Remote Sensing Image Analysis",
            "abstract": "Object-based image analysis (OBIA) has been widely adopted as a common paradigm to deal with very high-resolution remote sensing images. Nevertheless, OBIA methods strongly depend on the results of image segmentation. Many segmentation quality metrics have been proposed. Supervised metrics give accurate quality estimation but require a ground-truth segmentation as reference. Unsupervised metrics only make use of intrinsic image and segment properties; yet most of them strongly depend on the application and do not deal well with the variability of objects in remote sensing images. Furthermore, the few metrics developed in a remote sensing context mainly focus on global evaluation. In this paper, we propose a novel unsupervised metric, which evaluates local quality (per segment) by analyzing segment neighborhood, thus quantifying under- and over-segmentation given a certain homogeneity criterion. Additionally, we propose two variants of this metric, for estimating global quality of remote sensing image segmentation by the aggregation of local quality scores. Finally, we analyze the behavior of the proposed metrics and validate their applicability for finding segmentation results having good tradeoff between both kinds of errors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176417",
                    "name": "Andres Troya-Galvis"
                },
                {
                    "authorId": "1808290",
                    "name": "P. Gan\u00e7arski"
                },
                {
                    "authorId": "1790064",
                    "name": "Nicolas Passat"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "6351d287fd5f5c357b84f0a8e3709165b75b0937",
            "title": "Veracity of Big Data",
            "abstract": "The evolution of the Web from a technology platform to a social ecosystem has resulted in unprecedented data volumes being continuously generated, exchanged, and consumed. User-generated content on the Web is massive, highly dynamic, and characterized by a combination of factual data and opinion data. False information, rumors, and fake con- tents across multiple sources can be easily spread, making it hard to distinguish between what is true and what is not. Truth discovery also called fact-checking has recently gained lot of interest in Data Science communities. Ascertaining the veracity of data and understanding the dynamics of misinformation in the Web are two inter-dependent challenges for researchers and practitioners in Databases, Information Retrieval, and Knowledge Management. \nThis tutorial explores the progress that has been made in discovering truth, checking facts, and modeling the propagation of falsified and distorted information in the context of Big Data. We will review in details current models, algorithms, and techniques proposed by various research communities in Complex System Modeling, Data Management, and Knowledge Discovery, for ascertaining the veracity of data in a dynamic world. Finally, this tutorial will identify a wide range of open problems and research directions for discovering truth from falsehood(s) in the Web Data and understanding the evolution and propagation of information source trustworthiness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1390131709",
                    "name": "Javier Borge-Holthoefer"
                }
            ]
        },
        {
            "paperId": "90e948cc3d4511f23b0139043eafab5d0ef8d2f9",
            "title": "AllegatorTrack: Combining and reporting results of truth discovery from multi-source data",
            "abstract": "In the Web, a massive amount of user-generated contents is available through various channels, e.g., texts, tweets, Web tables, databases, multimedia-sharing platforms, etc. Conflicting information, rumors, erroneous and fake contents can be easily spread across multiple sources, making it hard to distinguish between what is true and what is not. How do you figure out that a lie has been told often enough that it is now considered to be true? How many lying sources are required to introduce confusion in what you knew before to be the truth? To answer these questions, we present AllegatorTrack, a system that discovers true claims among conflicting data from multiple sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2229335",
                    "name": "Dalia Attia Waguih"
                },
                {
                    "authorId": "1828017",
                    "name": "Naman Goel"
                },
                {
                    "authorId": "40596127",
                    "name": "Hossam M. Hammady"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "a463b452889cf82a669eef3ac06b42469c60224f",
            "title": "Veracity of Data: From Truth Discovery Computation Algorithms to Models of Misinformation Dynamics",
            "abstract": "In the Web, a massive amount of user-generated contents are available through various channels (e.g., texts, tweets, Web tables, databases, multimedia-sharing platforms, etc.). Conflicting information, rumors, erroneous and fake contents can be easily spread across multiple sources, making it hard to distinguish between what is true and what is not. This monograph gives an overview of fundamental issues and recent contributions for ascertaining the veracity of data in the era of Big Data. The text is organized into six chapters, focusing on structured data extracted from texts. Chapter One introduces the problem of ascertaining the veracity of data in a multi-source and evolving context. Issues related to information extraction are presented in chapter Two. It is followed by practical techniques for evaluating data source reputation and authoritativeness in Chapter Three, including a review of the main models and Bayesian approaches of trust management. Current truth discovery computation algorithms are presented in details in Chapter Four. The theoretical foundations and various approaches for modeling diffusion phenomenon of misinformation spreading in networked systems is studied in Chapter Five. Finally, truth discovery computation from extracted data in a dynamic context of misinformation propagation raises interesting challenges that are explored in Chapter Six. Supplementary material including source codes, datasets, and slides are offered online. This text is intended for a seminar course at the graduate level. It is also to serve as a useful resource for researchers and practitioners who are interested in the study of fact-checking, truth discovery or rumor spreading.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1390131709",
                    "name": "Javier Borge-Holthoefer"
                }
            ]
        },
        {
            "paperId": "e0bb1e561f651901d68c25f800e17283b2dbe8a2",
            "title": "Data veracity estimation with ensembling truth discovery methods",
            "abstract": "Estimation of data veracity is recognized as one of the grand challenges of big data. Typically, the goal of truth discovery is to determine the veracity of multi-source, conflicting data and return, as outputs, a veracity label and a confidence score for each data value, along with the trustworthiness score of each source claiming it. Although a plethora of methods has been proposed, it is unlikely a technique dominates all others across all data sets. Furthermore, the performance evaluation of the methods entirely depends on the availability of labeled ground truth data (i.e., data whose veracity has been manually checked). In the context of Big Data, acquiring the complete ground truth data is out-of-reach. In this paper, we propose an ensembling method that mitigates the two problems of method selection and ground truth data sparsity. Our approach combines the results of a set of truth discovery methods and preliminary experiments suggest that it improves the quality performance over the single methods when samples of ground truth data are used.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "701239a3f657dd50ba4d125803e05424c48e0f92",
            "title": "Truth Discovery Algorithms: An Experimental Evaluation",
            "abstract": "A fundamental problem in data fusion is to determine the veracity of multi-source data in order to resolve conflicts. While previous work in truth discovery has proved to be useful in practice for specific settings, sources' behavior or data set characteristics, there has been limited systematic comparison of the competing methods in terms of efficiency, usability, and repeatability. We remedy this deficit by providing a comprehensive review of 12 state-of-the art algorithms for truth discovery. We provide reference implementations and an in-depth evaluation of the methods based on extensive experiments on synthetic and real-world data. We analyze aspects of the problem that have not been explicitly studied before, such as the impact of initialization and parameter setting, convergence, and scalability. We provide an experimental framework for extensively comparing the methods in a wide range of truth discovery scenarios where source coverage, numbers and distributions of conflicts, and true positive claims can be controlled and used to evaluate the quality and performance of the algorithms. Finally, we report comprehensive findings obtained from the experiments and provide new insights for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2229335",
                    "name": "Dalia Attia Waguih"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "97a30fe9ef01950f33cca9315f542ec7912545c5",
            "title": "Web Data Quality: Current State and New Challenges",
            "abstract": "The standardization and adoption of Semantic Web technologies has resulted in an unprecedented volume of data being published as Linked Data (LD). However, the \"publish first, refine later\" philosophy leads to various quality problems arising in the underlying data such as incompleteness, inconsistency and semantic ambiguities. In this article, we describe the current state of Data Quality in the Web of Data along with details of the three papers accepted for the International Journal on Semantic Web and Information Systems' (IJSWIS) Special Issue on Web Data Quality. Additionally, we identify new challenges that are specific to the Web of Data and provide insights into the current progress and future directions for each of those challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3305769",
                    "name": "A. Zaveri"
                },
                {
                    "authorId": "1743266",
                    "name": "A. Maurino"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "547e66a4eca3ef3ae832b477bd6c40146203b6e3",
            "title": "Don't be SCAREd: use SCalable Automatic REpairing with maximal likelihood and bounded changes",
            "abstract": "Various computational procedures or constraint-based methods for data repairing have been proposed over the last decades to identify errors and, when possible, correct them. However, these approaches have several limitations including the scalability and quality of the values to be used in replacement of the errors. In this paper, we propose a new data repairing approach that is based on maximizing the likelihood of replacement data given the data distribution, which can be modeled using statistical machine learning techniques. This is a novel approach combining machine learning and likelihood methods for cleaning dirty databases by value modification. We develop a quality measure of the repairing updates based on the likelihood benefit and the amount of changes applied to the database. We propose SCARE (SCalable Automatic REpairing), a systematic scalable framework that follows our approach. SCARE relies on a robust mechanism for horizontal data partitioning and a combination of machine learning techniques to predict the set of possible updates. Due to data partitioning, several updates can be predicted for a single record based on local views on each data partition. Therefore, we propose a mechanism to combine the local predictions and obtain accurate final predictions. Finally, we experimentally demonstrate the effectiveness, efficiency, and scalability of our approach on real-world datasets in comparison to recent data cleaning approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "6378303",
                    "name": "Mohamed Yakout"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "145188857",
                    "name": "A. Elmagarmid"
                }
            ]
        },
        {
            "paperId": "7978cc01d75aa9d2f382a9e036706a37cdecbe4c",
            "title": "ADVISU: Interactive Visualization of Anomalies and Dependencies from Massive Scientific Datasets",
            "abstract": "In this demo, we present ADVISU (Anomaly and Dependency VISUalization), a powerful interactive system for visual analytics from massive datasets. ADVISU efficiently computes different types of dependencies (FDs, CFDs) and detects data anomalies from databases of large size, i.e., up to several thousands of attributes and millions of records. Real-time and scalable computational methods have been implemented in ADVISU to ensure interactivity and the demonstration is intended to show how these methods scale up for realworld massive scientific datasets in astrophysical and oceanographic application domains. ADVISU provides the users informative and interactive graphical interfaces for visualizing data dependencies and anomalies. It enables the analysis to be refined interactively while recomputing the dependencies and anomalies in user selected subspaces with good performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2928675",
                    "name": "No\u00ebl Novelli"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2433007",
                    "name": "C. Hurter"
                }
            ]
        },
        {
            "paperId": "94f8a2229c538c36840c4b96d0a0ef10e51820a7",
            "title": "Multi-Scale Data Integration Challenges in the Observational Science Data Space",
            "abstract": "Abstract In Europe, more than one thousand of laboratories intensively collect data to measure various properties of the Earth. Scientists observe environmental conditions, ecosystems and biological species. The ability to understand complex phenomena (e. g., global warming) and predict trends from spatio-temporal data becomes a major issue in observational science. However, theoretical and technical advances in multi-scale data integration are necessary to achieve this. This paper will present some challenging research directions for integrating such massive multi-scale scientific data. Zusammenfassung In Europa sammeln mehr als tausend Laboratorien Daten, um verschiedene Eigenschaften der Erde zu messen. Die F\u00e4higkeit, komplexe Ph\u00e4nomene (z. B. die globale Erw\u00e4rmung) zu verstehen und Trends aus r\u00e4umlich-zeitlichen Daten zu prognostizieren ist ein wichtiges Thema in der beobachtenden Wissenschaft. Allerdings sind theoretische und technische Fortschritte in der Integration von Daten unterschiedlichster Gr\u00f6\u00dfenordnung (Multi-Scale Data Integration) notwendig, um dies zu erreichen. Dieser Beitrag stellt einige Forschungsherausforderungen in diesem Kontext vor.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "d4100ebe2e502b9cf692b9930872d02b70b9c944",
            "title": "Proceedings of the 17th International Conference on Information Quality, IQ 2012, Paris, France, November 16-17, 2012",
            "abstract": "Master data management (MDM) provides an access t o the consistent views of the organization \u0301s most important data, also referred to as master data. In ddition to technical issues, there are many organ izational items related to MDM and its organizational implementatio n. However, current academic literature lacks empir ical studies on organizational challenges influencing the MD M initiatives. Consequently organizational issues i n establishing master data management function in an organizat ion re studied in this paper. Data collection is c onducted by participatory observations of a year-long MDM proje ct. Reflecting the findings to the literature shows that several new issues have emerged. These indicate that the im pl mentation of MDM is also affected by the organiz tion \u0301s ability to identify data owners and associate them with appropriate roles and responsibilities, and to create a unified understanding of the key terms and concepts regardi ng MDM. Also the importance of communication is emp hasized.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1398238537",
                    "name": "I. Comyn-Wattiau"
                },
                {
                    "authorId": "1789318",
                    "name": "M. Scannapieco"
                }
            ]
        },
        {
            "paperId": "0f1dc6e02cc314c92f6aec92f316573d3b1313e1",
            "title": "DAQ_UWE: A framework for designing data quality aware web applications",
            "abstract": "The use of Web applications in order to provide data with an acceptable level of quality is currently of paramount importance for any enterprise that wishes its business processes to succeed. The adequate management of the corresponding data resources through the introduction of all those aspects whose aim is to monitor the levels of quality for the task in hand is therefore essential. We claim that the introduction of such elements and mechanisms should take place during the Web application development process. To the best of our knowledge, there is still a lack of methodological and technological proposals with which to design data quality aware applications in the field of Web application development. Based principally on the benefits provided by the Model Driven Web Engineering (MDWE), this paper proposes a metamodel and a UML profile ( DAQ_UWE) for the management of Data Quality elements in the design of Web applications. The main objective is to provide the designer with the necessary tools to design Web applications, thus preventing data quality problems and ensuring data quality through design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403140472",
                    "name": "C. Guerra-Garc\u00eda"
                },
                {
                    "authorId": "1709884",
                    "name": "I. Caballero"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1702096",
                    "name": "M. Piattini"
                }
            ]
        },
        {
            "paperId": "3e26ec8e78ebd4b02382dd924349608b729bbd19",
            "title": "Assessment and analysis of information quality: a multidimensional model and case studies",
            "abstract": "Information quality is a complex and multidimensional notion. In the context of information system engineering, it is also a transversal notion and to be fully understood, it needs to be evaluated jointly considering the quality of data, the quality of the underlying conceptual data model and the quality of the software system that manages these data. This paper presents a multidimensional model for exploring information in a multidimensional way, which aids in the navigation, filtering, and interpretation of quality measures, and thus in the identification of the most appropriate actions to improve information quality. Two application scenarios are presented to illustrate and validate the multidimensional approach: the first one concerns the quality of customer information at Electricite de France, a French Electricity Company, and the second concerns the quality of patient records at Curie Institute, a well-known medical institute in France. The instantiation of our multidimensional model in these contexts shows first illustrations of its applicability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1398238537",
                    "name": "I. Comyn-Wattiau"
                },
                {
                    "authorId": "2090997283",
                    "name": "Mireille Cosquer"
                },
                {
                    "authorId": "2340543",
                    "name": "Zoubida Kedad"
                },
                {
                    "authorId": "2754805",
                    "name": "Sylvaine Nugier"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2609425",
                    "name": "S. Cherfi"
                },
                {
                    "authorId": "2673479",
                    "name": "Virginie Thion"
                }
            ]
        },
        {
            "paperId": "4d6f5aea9725c68dc6867ed3e09c3173e9990320",
            "title": "Discovery of complex glitch patterns: A novel approach to Quantitative Data Cleaning",
            "abstract": "Quantitative Data Cleaning (QDC) is the use of statistical and other analytical techniques to detect, quantify, and correct data quality problems (or glitches). Current QDC approaches focus on addressing each category of data glitch individually. However, in real-world data, different types of data glitches co-occur in complex patterns. These patterns and interactions between glitches offer valuable clues for developing effective domain-specific quantitative cleaning strategies. In this paper, we address the shortcomings of the extant QDC methods by proposing a novel framework, the DEC (Detect-Explore-Clean) framework. It is a comprehensive approach for the definition, detection and cleaning of complex, multi-type data glitches. We exploit the distributions and interactions of different types of glitches to develop data-driven cleaning strategies that may offer significant advantages over blind strategies. The DEC framework is a statistically rigorous methodology for evaluating and scoring glitches and selecting the quantitative cleaning strategies that result in cleaned data sets that are statistically proximal to user specifications. We demonstrate the efficacy and scalability of the DEC framework on very large real-world and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "17b2424cb7ec084397da11f164e396736db1fd73",
            "title": "Processing top-k join queries",
            "abstract": "We consider the problem of efficiently finding the top-k answers for join queries over web-accessible databases. Classical algorithms for finding top-k answers use branch-and-bound techniques to avoid computing scores of all candidates in identifying the top-k answers. To be able to apply such techniques, it is critical to efficiently compute (lower and upper) bounds and expected scores of candidate answers in an incremental fashion during the evaluation. In this paper, we describe novel techniques for these problems. The first contribution of this paper is a method to efficiently compute bounds for the score of a query result when tuples in tables from the \"FROM\" clause are discovered incrementally, through either sorted or random access. Our second contribution is an algorithm that, given a set of partially evaluated candidate answers, determines a good order in which to access the tables to minimize wasted efforts in the computation of top-k answers. We evaluate our algorithms on a variety of queries and data sets and demonstrate the significant benefits they provide.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147773",
                    "name": "Minji Wu"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "144173226",
                    "name": "A. Marian"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "fa24cbdfcee53529aacdcb791f6e4e61c5d5b4ee",
            "title": "Global detection of complex copying relationships between sources",
            "abstract": "Web technologies have enabled data sharing between sources but also simplified copying (and often publishing without proper attribution). The copying relationships can be complex: some sources copy from multiple sources on different subsets of data; some co-copy from the same source, and some transitively copy from another. Understanding such copying relationships is desirable both for business purposes and for improving many key components in data integration, such as resolving conflicts across various sources, reconciling distinct references to the same real-world entity, and efficiently answering queries over multiple sources. Recent works have studied how to detect copying between a pair of sources, but the techniques can fall short in the presence of complex copying relationships. In this paper we describe techniques that discover global copying relationships between a set of structured sources. Towards this goal we make two contributions. First, we propose a global detection algorithm that identifies co-copying and transitive copying, returning only source pairs with direct copying. Second, global detection requires accurate decisions on copying direction; we significantly improve over previous techniques on this by considering various types of evidence for copying and correlation of copying on different data items. Experimental results on real-world data and synthetic data show high effectiveness and efficiency of our techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "50819900",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "2863c1ecc5ea22da3ad8c3d18bc4b7e35dd67a43",
            "title": "Multidimensional Management and Analysis of Quality Measures for CRM Applications in an Electricity Company",
            "abstract": "This paper presents an approach integrating data quality into the business intelligence chain in the context of CRM applications at EDF (Electricite de France), the major electricity company in France. The main contribution of this paper is the definition and instantiation of a generic multi-dimensional star-like model for storing, analyzing and capitalizing data quality indicators, measurements and metadata. This approach is illustrated in one of EDF's CRM applications implementing the data quality-driven information supply chain for business intelligence where the role of the data quality expert is highly emphasized.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2673479",
                    "name": "Virginie Thion"
                },
                {
                    "authorId": "2340543",
                    "name": "Zoubida Kedad"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1398238537",
                    "name": "I. Comyn-Wattiau"
                },
                {
                    "authorId": "2754805",
                    "name": "Sylvaine Nugier"
                },
                {
                    "authorId": "2609425",
                    "name": "S. Cherfi"
                }
            ]
        },
        {
            "paperId": "2b74f7804eab6c6b01e598514dd008f49c5e953c",
            "title": "Multidimensional Management and Analysis of Quality Measures for CRM Applications at EDF",
            "abstract": "This paper presents an approach integrating data quality into the business intelligence chain in the context of customer-relationship management (CRM) applications at EDF (Electricite de France), the major electricity company in France. The main contribution of this paper is the definition and instantiation of a generic multi-dimensional star-like model for storing, analyzing and capitalizing data quality indicators, measurements and metadata. This approach is illustrated through one of EDF's CRM applications, implementing domain-specific quality indicators and providing quality-driven information management as a business intelligence chain. The role of the data quality expert is highly emphasized.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1398238537",
                    "name": "I. Comyn-Wattiau"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2098102392",
                    "name": "Samira Sisa\u00efd-Cherfi"
                },
                {
                    "authorId": "2754805",
                    "name": "Sylvaine Nugier"
                },
                {
                    "authorId": "2340543",
                    "name": "Zoubida Kedad"
                },
                {
                    "authorId": "2096471628",
                    "name": "V. Thion-Goasdou\u00e9"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                }
            ]
        },
        {
            "paperId": "66ff64274b57caf3fb573b0ef44feef56ebb913a",
            "title": "Truth Discovery and Copying Detection in a Dynamic World",
            "abstract": "Modern information management applications often require integrating data from a variety of data sources, some of which may copy or buy data from other sources. When these data sources model a dynamically changing world (e.g., people's contact information changes over time, restaurants open and go out of business), sources often provide out-of-date data. Errors can also creep into data when sources are updated often. Given out-of-date and erroneous data provided by different, possibly dependent, sources, it is challenging for data integration systems to provide the true values. Straightforward ways to resolve such inconsistencies (e.g., voting) may lead to noisy results, often with detrimental consequences. \n \nIn this paper, we study the problem of finding true values and determining the copying relationship between sources, when the update history of the sources is known. We model the quality of sources over time by their coverage, exactness and freshness. Based on these measures, we conduct a probabilistic analysis. First, we develop a Hidden Markov Model that decides whether a source is a copier of another source and identifies the specific moments at which it copies. Second, we develop a Bayesian model that aggregates information from the sources to decide the true value for a data item, and the evolution of the true values over time. Experimental results on both real-world and synthetic data show high accuracy and scalability of our techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "70fd8392d104ef35d85b07fbfab1990f3e4145a2",
            "title": "Integrating Conflicting Data: The Role of Source Dependence",
            "abstract": "Many data management applications, such as setting up Web portals, managing enterprise data, managing community data, and sharing scientific data, require integrating data from multiple sources. Each of these sources provides a set of values and different sources can often provide conflicting values. To present quality data to users, it is critical that data integration systems can resolve conflicts and discover true values. Typically, we expect a true value to be provided by more sources than any particular false one, so we can take the value provided by the majority of the sources as the truth. Unfortunately, a false value can be spread through copying and that makes truth discovery extremely tricky. In this paper, we consider how to find true values from conflicting information when there are a large number of sources, among which some may copy from others. \n \nWe present a novel approach that considers dependence between data sources in truth discovery. Intuitively, if two data sources provide a large number of common values and many of these values are rarely provided by other sources (e.g., particular false values), it is very likely that one copies from the other. We apply Bayesian analysis to decide dependence between sources and design an algorithm that iteratively detects dependence and discovers truth from conflicting information. We also extend our model by considering accuracy of data sources and similarity between values. Our experiments on synthetic data as well as real-world data show that our algorithm can significantly improve accuracy of truth discovery and is scalable when there are a large number of data sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "83d6701fccd057ca78645db7a0e72c219c5e869d",
            "title": "Sailing the Information Ocean with Awareness of Currents: Discovery and Application of Source Dependence",
            "abstract": "The Web has enabled the availability of a huge amount of useful information, but has also eased the ability to spread false information and rumors across multiple sources, making it hard to distinguish between what is true and what is not. Recent examples include the premature Steve Jobs obituary, the second bankruptcy of United airlines, the creation of Black Holes by the operation of the Large Hadron Collider, etc. Since it is important to permit the expression of dissenting and conflicting opinions, it would be a fallacy to try to ensure that the Web provides only consistent information. However, to help in separating the wheat from the chaff, it is essential to be able to determine dependence between sources. Given the huge number of data sources and the vast volume of conflicting data available on the Web, doing so in a scalable manner is extremely challenging and has not been addressed by existing work yet. \nIn this paper, we present a set of research problems and propose some preliminary solutions on the issues involved in discovering dependence between sources. We also discuss how this knowledge can benefit a variety of technologies, such as data integration and Web 2.0, that help users manage and access the totality of the available information from various sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1753537",
                    "name": "A. Sarma"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "144173226",
                    "name": "A. Marian"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0543539a7126e959251682faf1d0fe18121e31d3",
            "title": "Tracing Data Pollution in Large Business Applications",
            "abstract": "In large business applications, various data processing activities can be done locally or outsourced, split or combined and the resulting data flows have to be exchanged, shared or integrated from multiple data processing units. There are indeed various alternative paths for data processing and data consolidation. But some data flows and data processing applications are most likely exposed to generating and propagating data errors; some of them are more critical too. Actually, we usually ignore the impact of data errors in large and complex business applications because : 1) it is often very difficult to systematically audit data, detect and trace data errors in such large applications, 2) we usually don't have the complete picture of all the data processing units involved in every data processing paths; they are viewed as black-boxes, and 3) we usually ignore the total cost of detecting and eliminating data anomalies and surprisingly we also ignore the cost of \" doing nothing \" to resolve them. In this paper, the objectives of our ongoing research are the following: to propose a probabilistic model reflecting data error propagation in large business applications, to determine the most critical or impacted data processing paths and their weak points or vulnerabilities in terms of data quality, to advocate adequate locations for data quality checkpoints, and to predict the cost of doing-nothing versus the cost of data cleaning activities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "ce2671b90e7978ca13bd71ea999f0b557bce13d8",
            "title": "Measuring and Constraining Data Quality with Analytic Workflows",
            "abstract": "One challenging aspects of data quality modeling and management is to provide flexible, declarative and appropriate ways to express requirements on the quality of data. The paper presents a framework for specifying and checking constraints on data quality in RDBMS. The evaluation of the quality of data (QoD) is based on the declaration of data quality metrics that are computed and combined into so-called QoD analytic workflows. These workflows are designed as a composition of statistical methods and data mining techniques used to detect patterns of anomalies in the data sets. As metadata they are used to characterize various quantifiable dimensions of data quality (e.g., completeness, freshness, consistency, accuracy). The paper proposes a query language extension for constraining data quality when querying both data and its associated QoD metadata. Probabilistic approximate constraints are checked to determine if the quality of data is (or not) acceptable to build quality-constrained query results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "676daa174bf5009c3ab2ba3541ac78f50f8c7814",
            "title": "A Framework for Quality Evaluation in Data Integration Systems",
            "abstract": "Ensuring and maximizing the quality and integrity of information is a crucial process for today enterprise information systems (EIS). It requires a clear understanding of the interdependencies between the dimensions characterizing quality of data (QoD), quality of conceptual data model (QoM) of the database, keystone of the EIS, and quality of data management and integration processes (QoP). The improvement of one quality dimension (such as data accuracy or model expressiveness) may have negative consequences on other quality dimensions (e.g., freshness or completeness of data). In this paper we briefly present a framework, called QUADRIS, relevant for adopting a quality improvement strategy on one or many dimensions of QoD or QoM with considering the collateral effects on the other interdependent quality dimensions. We also present the scenarios of our ongoing validations on a CRM EIS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285406",
                    "name": "J. Akoka"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1763550",
                    "name": "Omar Boucelma"
                },
                {
                    "authorId": "1750088",
                    "name": "M. Bouzeghoub"
                },
                {
                    "authorId": "1398238537",
                    "name": "I. Comyn-Wattiau"
                },
                {
                    "authorId": "2090997283",
                    "name": "Mireille Cosquer"
                },
                {
                    "authorId": "2673479",
                    "name": "Virginie Thion"
                },
                {
                    "authorId": "2340543",
                    "name": "Zoubida Kedad"
                },
                {
                    "authorId": "2754805",
                    "name": "Sylvaine Nugier"
                },
                {
                    "authorId": "39719048",
                    "name": "Ver\u00f3nika Peralta"
                },
                {
                    "authorId": "2609425",
                    "name": "S. Cherfi"
                }
            ]
        },
        {
            "paperId": "b0304902607c0209c008aafcfb495ddd4d4e8acd",
            "title": "Contributions to Quality-Aware Online Query Processing",
            "abstract": "For non-collaborative data sources, quality-aware query processing is difficult to achieve because the sources generally do not export data quality indicators. This paper presents a prospective work on the declaration of metadata describing data quality and on the adaptation of query processing for taking into account constraints on the quality of data and finding dynamically the best trade-off between the cost of the query and the quality of the result.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "d31f150fb79980674c541b1d424405cbe2125fb1",
            "title": "Report from the First and Second International Workshops on Information Quality in Information Systems: IQIS 2004 and IQIS 2005 in conjunction with ACM SIGMOD/PODS Conferences",
            "abstract": "This report summarizes the constructive discussions of the first two editions of the International Workshop on Information Quality in Information Systems, IQIS 2004 and IQIS 2005, held respectively in Paris, France, on June 13, 2004 and in Baltimore, MD, USA, on June 17, 2005.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1789318",
                    "name": "M. Scannapieco"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "0e5149de05f29c7db065b05082ac2872f70a8b60",
            "title": "Enriching multimedia content description for broadcast environments: from a unified metadata model to a new generation of authoring tool",
            "abstract": "In this paper, we propose a novel approach for authoring a diversity of multimedia resources (audio, video, text, images, etc). We introduce a prototype authoring tool (called M-Tool) relying on a metadata model that unifies MPEG-21 and TV-anytime descriptions to edit and enrich audiovisual contents with metadata. Additional innovative functionalities extending the M-Tool are also presented. This new generation of metadata authoring tools is designed and currently used for scenarios of TV and news broadcasting, and video on demand broadcasting in the framework of the IST Integrated European Project ENTHRONE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31054328",
                    "name": "Boris Rousseau"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2742502",
                    "name": "Wilfried Jouve"
                }
            ]
        },
        {
            "paperId": "25b20e890959d640e9e55e42c22bd8bae527485a",
            "title": "Quality-Aware Integration and Warehousing of Genomic Data",
            "abstract": "In human health and life sciences, researchers extensively collaborate with each other, sharing biomedical and genomic data and their experimental results. This necessitates dynamically integrating different databases or warehousing them into a single repository. Based on our past experience of building a data warehouse called GEDAW (Gene Expression Data Warehouse) that stores data on genes expressed in the liver during iron overload and liver pathologies, and also relevant information from public databanks (mostly in XML format), DNA chips home experiments and medical records, we present the lessons learned, the data quality issues in this context and the current solutions we propose for integrating and warehousing biomedical data. This paper provides a functional and modular architecture for data quality enhancement and awareness in the complex processes of integration and warehousing of biomedical data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2420120",
                    "name": "F. Moussouni"
                }
            ]
        },
        {
            "paperId": "50a25e1db8af81a03c7dc05b349e7592aed65cc1",
            "title": "Cost of Low-Quality Data over Association Rules Discovery",
            "abstract": "Quality in data mining critically depends on the preparation and on the quality of processed data sets. Indeed data mining processes and applications require various forms of data preparation (and repair) with several data formatting and cleaning techniques, because the data input to the mining algorithms is assumed to conform to nice data distributions, containing no missing, inconsistent or incorrect values. This leaves a large gap between the available dirty data and the available machinery to process and analyze the data for discovering knowledge. This paper presents a theoretical probabilistic framework for modeling the cost of low-quality data on discovered association rules.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "6b21bce90944baf5d3f1a6ce4de32dc08b2439eb",
            "title": "Optimizing progressive query-by-example over pre-clustered large image databases",
            "abstract": "The typical mode for querying in an image content-based information system is query-by-example, which allows the user to provide an image as a query and to search for similar images (i.e., the nearest neighbors) based on one or a combination of low-level multidimensional features of the query example. Off-lime, this requires the time-consuming pre-computing of the whole set of visual descriptors over the image database. On-line, one major drawback is that multidimensional sequential NN-search is usually exhaustive over the whole image set face to the user who has a very limited patience. In this paper, we propose a technique for improving the performance of image query-by-example execution strategies over multiple visual features. This includes first, the pre-clustering of the large image database and then, the scheduling of the processing of the feature clusters before providing progressively the query results (i.e., intermediate results are sent continuously before the end of the exhaustive scan over the whole database). A cluster eligibility criterion and two filtering rules are proposed to select the most relevant clusters to a query-by-example. Experiments over more than 110 000 images and five MPEG-7 global features show that our approach significantly reduces the query time in two experimental cases: the query time is divided by 4.8 for 100 clusters per descriptor type and by 7 for 200 clusters per descriptor type compared to a \"blind\" sequential NN-search with keeping the same final query result. This constitutes a promising perspective for optimizing image query-by-example execution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2078453903",
                    "name": "A. Choupo"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "52043285",
                    "name": "A. Morin"
                }
            ]
        },
        {
            "paperId": "8d99de9b0e4557c2648e4ab7a840def5c3fba3ab",
            "title": "Proceedings of the 2nd international workshop on Information quality in information systems",
            "abstract": "The problem of poor data quality stored in database-backed information systems is widespread in the governmental, commercial and industrial environments. Alarming situations with various information quality problems can not be ignored anymore and theoretical as well as pragmatic approaches are urgently needed to be proposed and validated. As a consequence, information quality is now becoming one of the hot topics of emerging interest in the academic and industrial communities.Many processes and applications (such as information system integration, information retrieval, and knowledge discovery from databases) require various forms of data preparation or repair with several data processing techniques, because the data input to the application-dedicated algorithms is assumed to conform to nice data distributions, containing no missing, inconsistent or incorrect values. This leaves a large gap between the available \"dirty\" data and the available machinery to process the data for application purposes.The Second Edition of the International Workshop IQIS 2005 (Information Quality in Information Systems) is held in Baltimore, MD, USA, on June 17, 2005. The workshop is sponsored by ACM and in conjunction with the Symposium on Principles of Database System (PODS) and the ACM SIGMOD International Conference on Management of Data. IQIS workshop focuses on database-centric issues in data quality (scalability, quality-aware query processing, applications like data integration). It intends to address methods, techniques of massive data processing and analysis, methodologies, new algorithmic approaches or frameworks for designing data quality metrics in order to understand and to explore data quality, to end data glitches (as data quality problems such as duplicates, errors, outliers, contradictions, inconsistencies, etc.) and to ensure both data and information quality of database-backed information systems.The 11 papers collected in this volume, out of 26 papers that were submitted (with 10 short papers and 16 research papers), are a significant sample of recent achievements in the various areas of information and data quality, ranging from quality models to record linkage and statistical techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1693220",
                    "name": "C. Batini"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "b082a1470052196913fc71e1e88d339abb030e23",
            "title": "Recommendation of XML Documents exploiting Quality Metadata and Views",
            "abstract": "In this paper, we propose to query XML documents with a quality-based recommendation of the results. The document quality is modeled as a set of (criterion, value) pairs collected in metadata sets, and are associated with the indexed XML documents. We implemented four basic operations to achieve quality recommendation: 1) annotation with metadata describing the documents quality, 2) indexing the documents, 3) matching queries and quality requirements , and 4) viewing the recommended parts of the documents. The quality requirements of each user are kept as individual quality profiles (called XPS files). Every XML document in the document database refers to a quality style sheets (called XQS files) which allow the specification of several matching strategies with rules that associate parts (sub-trees) of XML documents to user profile quality requirements. An algorithm is described for evaluation of the quality style sheets and user profiles in order to build an \"adaptive quality view\" of the retrieved XML document. The paper describes the general architecture of our quality-based recommender system for XML documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "0a8bbe3ac6e96eec9342f2c11d2ca8d0df5dadf8",
            "title": "Multimedia indexing and retrieval with features association rules mining [image databases]",
            "abstract": "The administration of very large collections of images, accentuates the classical problems of indexing and efficiently querying information. This paper describes a new method applied to very large still image databases that combines two data mining techniques: clustering and association rules mining in order to better organize image collections and to improve the performance of queries. The objective of our work is to exploit association rules discovered by mining, global MPEG-7 features data and to adapt the query processing. In our experiment, we use five MPEG-7 features to describe several thousands of still images. For each feature, we initially determine several clusters of images by using a K-mean algorithm. Then, we generate association rules between different clusters of features and exploit these rules to rewrite the query and to optimize the query-by-content processing",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2078453903",
                    "name": "A. Choupo"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "52043285",
                    "name": "A. Morin"
                }
            ]
        },
        {
            "paperId": "a83960bb62bbd5795f0dde75b038f099858b8958",
            "title": "Quality-Adaptive Query Processing Over Distributed Sources",
            "abstract": "For non-collaborative data sources, both cost estimate-based optimization and quality-driven query processing are difficult to achieve because the sources do not export cost information nor data quality indicators. In this paper, we first propose an expressive query language extension using QML 1 syntax for defining in a flexible way dimensions, metrics of data quality and data source quality. We present a new framework for adaptive query processing on quality-extended query declarations. This processing includes the negotiation of quality contracts between the distributed data sources. The principle is to find dynamically the best trade-off between the cost of the query and the quality of the result retrieved from several distributed sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "978657cdc219389d9da08337ad8590b51c7c15bc",
            "title": "Quality-Based Recommendation of XML Documents",
            "abstract": "Information quality is multidimensional and can be modeled as an ordered set of weighted criteria. For a collection of XML documents, our approach consists firstly in harvesting and generating information quality indicators and enriching the meta-description of XML documents. Quality metadata are then exploited within the query processing for metadata-driven information retrieval and filtering in order to propose quality-adaptive recommendation strategies (with a quality view as a result) of the queried XML documents. This quality view depends on the user's profile and on his specific information quality requirements. The paper describes the architecture of a quality-based re-commender system for XML documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "3592673fbf348821ff2cee9744b9e92de4081873",
            "title": "Multi-Source Model and Architecture for Quality Negotiation and Integration of Biological Data",
            "abstract": "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L\u2019archive ouverte pluridisciplinaire HAL, est destin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents scientifiques de niveau recherche, publi\u00e9s ou non, \u00e9manant des \u00e9tablissements d\u2019enseignement et de recherche fran\u00e7ais ou \u00e9trangers, des laboratoires publics ou priv\u00e9s. Multi-Source Model and Architecture for Quality Negotiation and Integration of Biological Data Laure Berti-\u00c9quille",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                }
            ]
        },
        {
            "paperId": "f3ad7b3ad98c70e0f706a21b871ae6cfcc1790bd",
            "title": "Integration of biological data on transcriptome",
            "abstract": "A major concern in modern biology and medical research consists of the use of a \"high flow\" technology named bio-arrays or DNA chips that allows the study of thousands of genes simultaneously. The medical research institute, INSERM U522, specialized in the liver, uses the transcriptome techniques to diagnose liver disease states and to point the way towards new therapies. For this sake, the design of a bioinformatic integrated environment, named Gedaw (Gene Expression DAta Warehouse) has been initiated for storing, managing and analyzing such specific data. As an object-oriented data warehouse, it includes knowledge and complex data on genes expressed in the liver. The concept of ontology is the keystone of the application for integrating both genomic data available on public databanks, as well as experimental data on genes delivered from laboratory experiments and clinical statements. This paper describes the data modeling and processing that allow (i) to capture data from public databanks on genes (e.g., GenBank) (ii) to extract relevant information by selecting objects imported in XML format (iii) to make them persistent into the object- oriented warehouse. RESUME: De nouvelles techniques d'analyse biologique, dites \"a haut debit\" generent une masse considerable de donnees qu'il est necessaire d'organiser, de stocker et de gerer. L'unite de recherche U522 de l'INSERM, utilisant ces techniques pour l'etude du transcriptome hepatique, a initie le developpement d'un environnement integre nomme Gedaw (Gene Expression DAta Warehouse), dedie a la gestion, a l'integration et a l'analyse de ces nombreuses donnees. Entrepot de donnees oriente objet, il regroupe des connaissances et des donnees complexes sur les genes du foie. Le concept d'ontologie, au centre de l'application, permet d'integrer a la fois les donnees sur les sequences genomiques issues des banques de donnees publiques, ainsi que les donnees issues des experiences du laboratoire et des releves cliniques. Cet article presente la problematique de l'integration des donnees biologiques liees au transcriptome. Il decrit le modele de donnees associe a ce cas d'application et implante sous forme de classes d'objets persistants au sein d'une base orientee objet. La chaine de traitements developpee dans Gedaw permet : (i) d'integrer les donnees genomiques a partir de banques publiques (telles que GenBank) (ii) d'extraire les informations pertinentes par selection d'objets au format XML (iii) de les rendre persistantes dans l'entrepot.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2420120",
                    "name": "F. Moussouni"
                },
                {
                    "authorId": "2640272",
                    "name": "A. Arcade"
                }
            ]
        }
    ]
}