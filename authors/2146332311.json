{
    "authorId": "2146332311",
    "papers": [
        {
            "paperId": "0b6b235621ce36d07b2e1759b33bf989b05cc99e",
            "title": "DeepLens: Interactive Out-of-distribution Data Detection in NLP Models",
            "abstract": "Machine Learning (ML) has been widely used in Natural Language Processing (NLP) applications. A fundamental assumption in ML is that training data and real-world data should follow a similar distribution. However, a deployed ML model may suffer from out-of-distribution (OOD) issues due to distribution shifts in the real-world data. Though many algorithms have been proposed to detect OOD data from text corpora, there is still a lack of interactive tool support for ML developers. In this work, we propose DeepLens, an interactive system that helps users detect and explore OOD issues in massive text corpora. Users can efficiently explore different OOD types in DeepLens with the help of a text clustering method. Users can also dig into a specific text by inspecting salient words highlighted through neuron activation analysis. In a within-subjects user study with 24 participants, participants using DeepLens were able to find nearly twice more types of OOD issues accurately with 22% more confidence compared with a variant of DeepLens that has no interaction or visualization support.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382534445",
                    "name": "D. Song"
                },
                {
                    "authorId": "2108157560",
                    "name": "Zhijie Wang"
                },
                {
                    "authorId": "1739753031",
                    "name": "Yuheng Huang"
                },
                {
                    "authorId": "2193640276",
                    "name": "Lei Ma"
                },
                {
                    "authorId": "2146332311",
                    "name": "Tianyi Zhang"
                }
            ]
        },
        {
            "paperId": "785bd5915f83f941d36c7996a9742ae695880111",
            "title": "FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models",
            "abstract": "Studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on. Various researchers have proposed mathematical tools for quantifying and identifying these biases. There have been methods proposed to mitigate such biases. In this paper, we present a comprehensive quantitative evaluation of different kinds of biases such as race, gender, ethnicity, age etc. exhibited by popular pretrained language models such as BERT, GPT-2 etc. and also present a toolkit that provides plug-and-play interfaces to connect mathematical tools to identify biases with large pretrained language models such as BERT, GPT-2 etc. and also present users with the opportunity to test custom models against these metrics. The toolkit also allows users to debias existing and custom models using the debiasing techniques proposed so far. The toolkit is available at https://github.com/HrishikeshVish/Fairpy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121195389",
                    "name": "Hrishikesh Viswanath"
                },
                {
                    "authorId": "2146332311",
                    "name": "Tianyi Zhang"
                }
            ]
        },
        {
            "paperId": "a4a41319d5805a29316f24ed9519f09db77d4c29",
            "title": "Benchmarking Large Language Models for News Summarization",
            "abstract": "Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM\u2019s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146332311",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "8759332",
                    "name": "Faisal Ladhak"
                },
                {
                    "authorId": "41152329",
                    "name": "Esin Durmus"
                },
                {
                    "authorId": "145419642",
                    "name": "Percy Liang"
                },
                {
                    "authorId": "145590324",
                    "name": "K. McKeown"
                },
                {
                    "authorId": "2117567142",
                    "name": "Tatsunori Hashimoto"
                }
            ]
        },
        {
            "paperId": "f48287e9ed131ff8ffa79b66717887c5af74f203",
            "title": "When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization",
            "abstract": "Large language models (LLMs) are subject to sociocultural and other biases previously identified using intrinsic evaluations. However, when and how these intrinsic biases in pre-trained LM representations propagate to downstream, fine-tuned NLP tasks like summarization is not well understood. In this work, we investigate one type of bias\u2014name-nationality bias\u2014and trace it from the pre-training stage to a downstream summarization task across multiple summarization modeling choices. We show that these biases manifest themselves as hallucinations in summarization, leading to factually incorrect summaries. We also find that this propagation of biases is algorithm-dependent: more abstractive models allow biases to propagate more directly to downstream tasks as hallucinated facts. Building on these observations, we further analyze how changes to the adaptation method and fine-tuning data set affect name nationality biases and show that while they can reduce the overall rate of hallucinations, they do not change the types of biases that do appear.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8759332",
                    "name": "Faisal Ladhak"
                },
                {
                    "authorId": "41152329",
                    "name": "Esin Durmus"
                },
                {
                    "authorId": "51903517",
                    "name": "Mirac Suzgun"
                },
                {
                    "authorId": "2146332311",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "1746807",
                    "name": "Dan Jurafsky"
                },
                {
                    "authorId": "145590324",
                    "name": "K. McKeown"
                },
                {
                    "authorId": "2117567142",
                    "name": "Tatsunori Hashimoto"
                }
            ]
        },
        {
            "paperId": "212a6e29b4579b9334c605fdf3624c2c103423ec",
            "title": "A Logic Aware Neural Generation Method for Explainable Data-to-text",
            "abstract": "The most notable neural data-to-text approaches generate natural language from structural data relying on the surface form of the structural content, which ignores the underlying logical correlation between the input data and the target text. Moreover, identifying such logical associations and explaining them in natural language is desirable but not yet studied. In this paper, we introduce a practical data-to-text method for the logic-critical scenario, specifically for anti-money laundering applications. It involves detecting risks from input data and explaining any abnormal behaviors in natural language. The proposed method is a Logic Aware Neural Generation framework (LANG), which is a preliminary attempt to explore the integration of logic modeling and text generation. Concretely, we first convert expert rules to a logic graph. Then, the model utilizes meta path based encoder to exploit the expert knowledge. Besides, a retriever module with the encoded logic knowledge is used to bridge the gap between numeric input and target text. Finally, a rule-constrained loss is leveraged to improve the generation probability of tokens in rule recalled statements to ensure accuracy. We conduct extensive experiments on anti-money laundering data. Results show that the proposed method significantly outperforms baselines in both objective measures with relative 35% improvements in F1 score and subjective measures with 30% improvement in human preference.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Xiexiong Lin"
                },
                {
                    "authorId": "2181548125",
                    "name": "Huaisong Li"
                },
                {
                    "authorId": "2181652492",
                    "name": "Tao Huang"
                },
                {
                    "authorId": "2145758048",
                    "name": "Feng Wang"
                },
                {
                    "authorId": "1850313",
                    "name": "Linlin Chao"
                },
                {
                    "authorId": "2162961864",
                    "name": "Fuzhen Zhuang"
                },
                {
                    "authorId": "1799672",
                    "name": "Taifeng Wang"
                },
                {
                    "authorId": "2146332311",
                    "name": "Tianyi Zhang"
                }
            ]
        },
        {
            "paperId": "4aafb522d20f570211324f4054761f146da80800",
            "title": "CREATIVESUMM: Shared Task on Automatic Summarization for Creative Writing",
            "abstract": "This paper introduces the shared task of summrizing documents in several creative domains, namely literary texts, movie scripts, and television scripts. Summarizing these creative documents requires making complex literary interpretations, as well as understanding non-trivial temporal dependencies in texts containing varied styles of plot development and narrative structure. This poses unique challenges and is yet underexplored for text summarization systems. In this shared task, we introduce four sub-tasks and their corresponding datasets, focusing on summarizing books, movie scripts, primetime television scripts, and daytime soap opera scripts. We detail the process of curating these datasets for the task, as well as the metrics used for the evaluation of the submissions. As part of the CREATIVESUMM workshop at COLING 2022, the shared task attracted 18 submissions in total. We discuss the submissions and the baselines for each sub-task in this paper, along with directions for facilitating future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057234256",
                    "name": "Divyansh Agarwal"
                },
                {
                    "authorId": "22281632",
                    "name": "A. R. Fabbri"
                },
                {
                    "authorId": "3226782",
                    "name": "Simeng Han"
                },
                {
                    "authorId": "51232396",
                    "name": "Wojciech Kryscinski"
                },
                {
                    "authorId": "8759332",
                    "name": "Faisal Ladhak"
                },
                {
                    "authorId": "2112208843",
                    "name": "B. Li"
                },
                {
                    "authorId": "145590324",
                    "name": "K. McKeown"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                },
                {
                    "authorId": "2146332311",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "2067208277",
                    "name": "Sam Wiseman"
                }
            ]
        },
        {
            "paperId": "5dd9f6eeca1196321cc55955bafeeb653754dba5",
            "title": "Using Natural Language Processing to Accelerate Deep Analysis of Open-Ended Survey Data",
            "abstract": "In this poster session, we share our work exploring the use of natural language processing (NLP) to assist in the thematic analysis of qualitative data. With the affordances currently provided by artificial intelligence and machine learning to process textual data for meaning, our interdisciplinary team of computer and social science researchers is using a data set of open-ended text responses to test procedures for creating thematic categories to organize participant responses by content. By comparing parallel analyses of our text corpus using traditional and NLP thematic-analysis techniques, we identify key areas of congruity and incongruity between human analysts and NLP algorithms. While the areas of congruity point to the potential usefulness of NLP systems in qualitative research, the areas of incongruity suggest the need for a \u201chuman in the loop\u201d NLP platform which iteratively refines the content analysis outputs based on feedback from an expert researcher until a satisfactorily coherent thematic description is achieved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146332311",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "144808711",
                    "name": "Monica Moody"
                },
                {
                    "authorId": "2289453557",
                    "name": "Julia P. Nelon"
                },
                {
                    "authorId": "2289210453",
                    "name": "D. Matthew Boyer"
                },
                {
                    "authorId": "2289480237",
                    "name": "D. Hudson Smith"
                },
                {
                    "authorId": "2289131445",
                    "name": "Ryan D. Visser"
                }
            ]
        },
        {
            "paperId": "cdc895c02f7f19b2490a5100d4b8f4c037057200",
            "title": "Crowd-O-Meter: Predicting if a Person Is Vulnerable to Believe Political Claims",
            "abstract": "\n \n Social media platforms have been criticized for promoting false information during the 2016 U.S. presidential election campaign. Our work is motivated by the idea that a platform could reduce the circulation of false information if it could estimate whether its users are vulnerable to believing political claims. We here explore whether such a vulnerability could be measured in a crowdsourcing setting. We propose Crowd-O-Meter, a framework that automatically predicts if a crowd worker will be consistent in his/her beliefs about political claims; i.e., consistently believes the claims are true or consistently believes the claims are not true. Crowd-O-Meter is a user-centered approach which interprets a combination of cues characterizing the user's implicit and explicit opinion bias. Experiments on 580 quotes from PolitiFact's fact checking corpus of 2016 U.S. presidential candidates show that Crowd-O-Meter is precise and accurate for two news modalities: text and video. Our analysis also reveals which are the most informative cues of a person's vulnerability.\n \n",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "2128305",
                    "name": "M. Sameki"
                },
                {
                    "authorId": "2146332311",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "2117199306",
                    "name": "Linli Ding"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "2028946",
                    "name": "D. Gurari"
                }
            ]
        }
    ]
}