{
    "authorId": "2066216451",
    "papers": [
        {
            "paperId": "3445b70a84cca27cfb3a54035c907b5bb3e02dac",
            "title": "Why Do We Need Neurosymbolic AI to Model Pragmatic Analogies?",
            "abstract": "A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of large language models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical, syntactic, semantic, and pragmatic. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. We discuss neurosymbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction, and guide the mapping process. This maintains the efficiency of LLMs while preserving the ability to explain analogies for pedagogical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                },
                {
                    "authorId": "2064342742",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "2890773",
                    "name": "V. Shalin"
                },
                {
                    "authorId": "48806891",
                    "name": "Amitava Das"
                }
            ]
        },
        {
            "paperId": "6389f962cf3f1eb4f1a6a5013453f6a8e4d1db8e",
            "title": "IMKG: The Internet Meme Knowledge Graph",
            "abstract": ". Internet Memes (IMs) are creative media that combine text and vision modalities that people use to describe their situation by reusing an existing, familiar situation. Prior work on IMs has focused on analyzing their spread over time or high-level classification tasks like hate speech detection, while a principled analysis of their stratified semantics is missing. Hypothesizing that Semantic Web technologies are appropriate to help us bridge this gap, we build the first Internet Meme Knowledge Graph (IMKG) : an explicit representation with 2 million edges that capture the semantics encoded in the text, vision, and metadata of thousands of media frames and their adaptations as memes. IMKG is designed to fulfil seven requirements derived from the inherent characteristics of IMs. IMKG is based on a comprehensive semantic model, it is populated with data from representative IM sources, and enriched with entities extracted from text and vision connected through background knowledge from Wikidata. IMKG integrates its knowledge both in RDF and as a labelled property graph. We provide insights into the structure of IMKG, analyze its central concepts, and measure the effect of knowledge enrichment from different information modalities. We demonstrate its ability to support novel use cases, like querying for IMs that are based on films, and we provide insights into the signal captured by the structure and the content of its nodes. As a novel publicly available resource, IMKG opens the possibility for further work to study the semantics of IMs, develop novel reasoning tasks, and improve its quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50271459",
                    "name": "Riccardo Tommasini"
                },
                {
                    "authorId": "2267335678",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                }
            ]
        },
        {
            "paperId": "90b20012b3bc4434f19fc68fdf86f39589061eaf",
            "title": "Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models",
            "abstract": "Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of \u201cA is to B as C is to D.\u201d However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LLMs is positively correlated with their ability to encode syntactic and semantic structures of sentences. Specifically, we find that the LLMs which capture syntactic structures better, also have higher abilities in identifying sentence analogies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                },
                {
                    "authorId": "70510564",
                    "name": "Ruwan Wickramarachchi"
                },
                {
                    "authorId": "8856206",
                    "name": "Aishwarya N. Reganti"
                },
                {
                    "authorId": "2212131028",
                    "name": "Vinija Jain"
                },
                {
                    "authorId": "2275226689",
                    "name": "Aman Chadha"
                },
                {
                    "authorId": "2064342742",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "2258322706",
                    "name": "Amitava Das"
                }
            ]
        },
        {
            "paperId": "eafc4e5667826a3b9d7eaec9516d7f2141eaca23",
            "title": "On the Relationship between Sentence Analogy Identification and Sentence Structure Encoding in Large Language Models",
            "abstract": "The ability of Large Language Models (LLMs) to encode syntactic and semantic structures of language is well examined in NLP. Additionally, analogy identification, in the form of word analogies are extensively studied in the last decade of language modeling literature. In this work we specifically look at how LLMs\u2019 abilities to capture sentence analogies (sentences that convey analogous meaning to each other) vary with LLMs\u2019 abilities to encode syntactic and semantic structures of sentences. Through our analysis, we find that LLMs\u2019 ability to identify sentence analogies is positively correlated with their ability to encode syntactic and semantic structures of sentences. Specifically, we find that the LLMs which capture syntactic structures better, also have higher abilities in identifying sentence analogies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                },
                {
                    "authorId": "70510564",
                    "name": "Ruwan Wickramarachchi"
                },
                {
                    "authorId": "8856206",
                    "name": "Aishwarya N. Reganti"
                },
                {
                    "authorId": "2212131028",
                    "name": "Vinija Jain"
                },
                {
                    "authorId": "40016108",
                    "name": "Aman Chadha"
                },
                {
                    "authorId": "2064342742",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "2258322706",
                    "name": "Amitava Das"
                }
            ]
        },
        {
            "paperId": "fc09ee18ab94884cb8026a7db645dd0f0fc04d38",
            "title": "ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models",
            "abstract": "Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space. Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                },
                {
                    "authorId": "70510564",
                    "name": "Ruwan Wickramarachchi"
                },
                {
                    "authorId": "2216606354",
                    "name": "Bimal Gajera"
                },
                {
                    "authorId": "2216605596",
                    "name": "Shreeyash Mukul Gowaikar"
                },
                {
                    "authorId": "153232070",
                    "name": "Chandan Gupta"
                },
                {
                    "authorId": "40016108",
                    "name": "Aman Chadha"
                },
                {
                    "authorId": "8856206",
                    "name": "Aishwarya N. Reganti"
                },
                {
                    "authorId": "2064342742",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "48806891",
                    "name": "Amitava Das"
                }
            ]
        },
        {
            "paperId": "0d1aacf97cb67c6b4af9f7297f6affbdf559caa8",
            "title": "UBERT: A Novel Language Model for Synonymy Prediction at Scale in the UMLS Metathesaurus",
            "abstract": "The UMLS Metathesaurus integrates more than 200 biomedical source vocabularies. During the Metathesaurus construction process, synonymous terms are clustered into concepts by human editors, assisted by lexical similarity algorithms. This process is error-prone and time-consuming. Recently, a deep learning model (LexLM) has been developed for the UMLS Vocabulary Alignment (UVA) task. This work introduces UBERT, a BERT-based language model, pretrained on UMLS terms via a supervised Synonymy Prediction (SP) task replacing the original Next Sentence Prediction (NSP) task. The effectiveness of UBERT for UMLS Metathesaurus construction process is evaluated using the UMLS Vocabulary Alignment (UVA) task. We show that UBERT outperforms the LexLM, as well as biomedical BERT-based models. Key to the performance of UBERT are the synonymy prediction task specifically developed for UBERT, the tight alignment of training data to the UVA task, and the similarity of the models used for pretrained UBERT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                },
                {
                    "authorId": "35089024",
                    "name": "Vinh Phu Nguyen"
                },
                {
                    "authorId": "27549522",
                    "name": "Goonmeet Bajaj"
                },
                {
                    "authorId": "35655815",
                    "name": "H. Y. Yip"
                },
                {
                    "authorId": "79596923",
                    "name": "Vishesh Javangula"
                },
                {
                    "authorId": "2012953",
                    "name": "Yuqing Mao"
                },
                {
                    "authorId": "33407506",
                    "name": "K. Fung"
                },
                {
                    "authorId": "2739353",
                    "name": "Srinivas Parthasarathy"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "1950081",
                    "name": "O. Bodenreider"
                }
            ]
        },
        {
            "paperId": "493677b136a9c37cf375364759dea267a49a0267",
            "title": "Context-Enriched Learning Models for Aligning Biomedical Vocabularies at Scale in the UMLS Metathesaurus",
            "abstract": "The Unified Medical Language System (UMLS) Metathesaurus construction process mainly relies on lexical algorithms and manual expert curation for integrating over 200 biomedical vocabularies. A lexical-based learning model (LexLM) was developed to predict synonymy among Metathesaurus terms and largely outperforms a rule-based approach (RBA) that approximates the current construction process. However, the LexLM has the potential for being improved further because it only uses lexical information from the source vocabularies, while the RBA also takes advantage of contextual information. We investigate the role of multiple types of contextual information available to the UMLS editors, namely source synonymy (SS), source semantic group (SG), and source hierarchical relations (HR), for the UMLS vocabulary alignment (UVA) problem. In this paper, we develop multiple variants of context-enriched learning models (ConLMs) by adding to the LexLM the types of contextual information listed above. We represent these context types in context-enriched knowledge graphs (ConKGs) with four variants ConSS, ConSG, ConHR, and ConAll. We train these ConKG embeddings using seven KG embedding techniques. We create the ConLMs by concatenating the ConKG embedding vectors with the word embedding vectors from the LexLM. We evaluate the performance of the ConLMs using the UVA generalization test datasets with hundreds of millions of pairs. Our extensive experiments show a significant performance improvement from the ConLMs over the LexLM, namely +5.0% in precision (93.75%), +0.69% in recall (93.23%), +2.88% in F1 (93.49%) for the best ConLM. Our experiments also show that the ConAll variant including the three context types takes more time, but does not always perform better than other variants with a single context type. Finally, our experiments show that the pairs of terms with high lexical similarity benefit most from adding contextual information, namely +6.56% in precision (94.97%), +2.13% in recall (93.23%), +4.35% in F1 (94.09%) for the best ConLM. The pairs with lower degrees of lexical similarity also show performance improvement with +0.85% in F1 (96%) for low similarity and +1.31% in F1 (96.34%) for no similarity. These results demonstrate the importance of using contextual information in the UVA problem.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35089024",
                    "name": "Vinh Phu Nguyen"
                },
                {
                    "authorId": "35655815",
                    "name": "H. Y. Yip"
                },
                {
                    "authorId": "27549522",
                    "name": "Goonmeet Bajaj"
                },
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                },
                {
                    "authorId": "79596923",
                    "name": "Vishesh Javangula"
                },
                {
                    "authorId": "2739353",
                    "name": "Srinivas Parthasarathy"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "1950081",
                    "name": "O. Bodenreider"
                }
            ]
        },
        {
            "paperId": "7bb115c518eab1e3a421b7fa3823159455fadbb6",
            "title": "Evaluating Biomedical Word Embeddings for Vocabulary Alignment at Scale in the UMLS Metathesaurus Using Siamese Networks",
            "abstract": "Recent work uses a Siamese Network, initialized with BioWordVec embeddings (distributed word embeddings), for predicting synonymy among biomedical terms to automate a part of the UMLS (Unified Medical Language System) Metathesaurus construction process. We evaluate the use of contextualized word embeddings extracted from nine different biomedical BERT-based models for synonym prediction in the UMLS by replacing BioWordVec embeddings with embeddings extracted from each biomedical BERT model using different feature extraction methods. Finally, we conduct a thorough grid search, which prior work lacks, to find the best set of hyperparameters. Surprisingly, we find that Siamese Networks initialized with BioWordVec embeddings still out perform the Siamese Networks initialized with embedding extracted from biomedical BERT model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "27549522",
                    "name": "Goonmeet Bajaj"
                },
                {
                    "authorId": "35089024",
                    "name": "Vinh Phu Nguyen"
                },
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                },
                {
                    "authorId": "35655815",
                    "name": "H. Y. Yip"
                },
                {
                    "authorId": "79596923",
                    "name": "Vishesh Javangula"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "2739353",
                    "name": "Srinivas Parthasarathy"
                },
                {
                    "authorId": "1950081",
                    "name": "O. Bodenreider"
                }
            ]
        },
        {
            "paperId": "f26c422fa3e033d5faf8a667a03289c1d4f601c0",
            "title": "Towards efficient scoring of student-generated long-form analogies in STEM",
            "abstract": "Switching from an analogy pedagogy based on comprehension to analogy pedagogy based on production raises an impractical manual analogy scoring problem. Conventional symbol-matching approaches to computational analogy evaluation focus on positive cases, and challenge computational feasibility. This work presents the Discriminative Analogy Features (DAF) pipeline to identify the discriminative features of strong and weak long-form text analogies. We introduce four feature categories (semantic, syntactic, sentiment, and statistical) used with supervised vector-based learning methods to discriminate between strong and weak analogies. Using a modestly sized vector of engineered features with SVM attains a 0.67 macro F1 score. While a semantic feature is the most discriminative, out of the top 15 discriminative features, most are syntactic. Combining this engineered features with an ELMo-generated embedding still improves classification relative to an embedding alone. While an unsupervised K-Means clustering-based approach falls short, similar hints of improvement appear when inputs include the engineered features used in supervised learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                },
                {
                    "authorId": "70510564",
                    "name": "Ruwan Wickramarachchi"
                },
                {
                    "authorId": "2890773",
                    "name": "V. Shalin"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "325cb1fd2aff335c4aa04d95e1d9d14e9a6b5b2d",
            "title": "Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in the UMLS Metathesaurus",
            "abstract": "The current UMLS (Unified Medical Language System) Metathesaurus construction process for integrating over 200 biomedical source vocabularies is expensive and error-prone as it relies on the lexical algorithms and human editors for deciding if the two biomedical terms are synonymous. Recent advances in Natural Language Processing such as Transformer models like BERT and its biomedical variants with contextualized word embeddings have achieved state-of-the-art (SOTA) performance on downstream tasks. We aim to validate if these approaches using the BERT models can actually outperform the existing approaches for predicting synonymy in the UMLS Metathesaurus. In the existing Siamese Networks with LSTM and BioWordVec embeddings, we replace the BioWordVec embeddings with the biomedical BERT embeddings extracted from each BERT model using different ways of extraction. In the Transformer architecture, we evaluate the use of the different biomedical BERT models that have been pre-trained using different datasets and tasks. Given the SOTA performance of these BERT models for other downstream tasks, our experiments yield surprisingly interesting results: (1) in both model architectures, the approaches employing these biomedical BERT-based models do not outperform the existing approaches using Siamese Network with BioWordVec embeddings for the UMLS synonymy prediction task, (2) the original BioBERT large model that has not been pre-trained with the UMLS outperforms the SapBERT models that have been pre-trained with the UMLS, and (3) using the Siamese Networks yields better performance for synonymy prediction when compared to using the biomedical BERT models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27549522",
                    "name": "Goonmeet Bajaj"
                },
                {
                    "authorId": "35089024",
                    "name": "Vinh Phu Nguyen"
                },
                {
                    "authorId": "2066216451",
                    "name": "Thilini Wijesiriwardene"
                },
                {
                    "authorId": "35655815",
                    "name": "H. Y. Yip"
                },
                {
                    "authorId": "79596923",
                    "name": "Vishesh Javangula"
                },
                {
                    "authorId": "145022640",
                    "name": "S. Parthasarathy"
                },
                {
                    "authorId": "144463965",
                    "name": "A. Sheth"
                },
                {
                    "authorId": "1950081",
                    "name": "O. Bodenreider"
                }
            ]
        }
    ]
}