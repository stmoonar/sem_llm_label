{
    "authorId": "37864689",
    "papers": [
        {
            "paperId": "0f044daf897b75bb3b0cf83db4ead90e6e39a9d6",
            "title": "Road Extraction by Multiscale Deformable Transformer From Remote Sensing Images",
            "abstract": "Rapid progress has been made in the research of high-resolution remote sensing road extraction tasks in the past years but due to the diversity of road types and the complexity of road context, extracting the perfect road network is still fraught with difficulties and challenges. Many convolutional neural networks (CNNs) based on encoder\u2013decoder structures have demonstrated their effectiveness. Transformer\u2019s self-attention mechanism shows more powerful performance than CNNs in modeling global feature dependencies. In this letter, we propose a multiscale deformable transformer network (MDTNet) based on encoder\u2013decoder structure to extract road networks from remote sensing images. The core of MDTNet is our proposed multiscale deformable self-attention (MDSA) mechanism. MDSA can capture more comprehensive features than conventional self-attention. In addition, roads are not present in certain blocks of areas like other objects, but are interwoven throughout the image in such a long, linear fashion that information about certain road segments may be overlooked. To minimize residual errors in road segmentations, our MDSA incorporates a deformable design on feature maps, which effectively enhances the salience of road features relative to their surroundings. Extensive experiments on several public remote sensing road datasets show that our MDTNet achieves higher segmentation [F1 score and intersection over union (IoU)] and connectivity [average path length similarity (APLS)] accuracy, which verifies the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067770565",
                    "name": "Peng Hu"
                },
                {
                    "authorId": "50358605",
                    "name": "Sibao Chen"
                },
                {
                    "authorId": "2164820498",
                    "name": "Lili Huang"
                },
                {
                    "authorId": "153042918",
                    "name": "Gui Wang"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                },
                {
                    "authorId": "2175688252",
                    "name": "Bin Luo"
                }
            ]
        },
        {
            "paperId": "1371b5dc3a2f1434b89d41a0bdc97f100d19a4c7",
            "title": "Graph Neural Network Meets Sparse Representation: Graph Sparse Neural Networks via Exclusive Group Lasso",
            "abstract": "Existing GNNs usually conduct the layer-wise message propagation via the \u2018full\u2019 aggregation of all neighborhood information which are usually sensitive to the structural noises existed in the graphs, such as incorrect or undesired redundant edge connections. To overcome this issue, we propose to exploit Sparse Representation (SR) theory into GNNs and propose Graph Sparse Neural Networks (GSNNs) which conduct sparse aggregation to select reliable neighbors for message aggregation. GSNNs problem contains discrete/sparse constraint which is difficult to be optimized. Thus, we then develop a tight continuous relaxation model Exclusive Group Lasso GNNs (EGLassoGNNs) for GSNNs. An effective algorithm is derived to optimize the proposed EGLassoGNNs model. Experimental results on several benchmark datasets demonstrate the better performance and robustness of the proposed EGLassoGNNs model.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152390773",
                    "name": "Bo Jiang"
                },
                {
                    "authorId": "49292271",
                    "name": "Beibei Wang"
                },
                {
                    "authorId": "50358605",
                    "name": "Sibao Chen"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                },
                {
                    "authorId": "2151264175",
                    "name": "B. Luo"
                }
            ]
        },
        {
            "paperId": "1e7e2eeac837dcc7f1109215cda9064efb3b1a3b",
            "title": "AMatFormer: Efficient Feature Matching via Anchor Matching Transformer",
            "abstract": "Learning based feature matching methods have been commonly studied in recent years. The core issue for learning feature matching is to how to learn (1) discriminative representations for feature points (or regions) within each intra-image and (2) consensus representations for feature points across inter-images. Recently, self- and cross-attention models have been exploited to address this issue. However, in many scenes, features are coming with large-scale, redundant and outliers contaminated. Previous self-/cross-attention models generally conduct message passing on all primal features which thus lead to redundant learning and high computational cost. To mitigate limitations, inspired by recent seed matching methods, in this article, we propose a novel efficient Anchor Matching Transformer (AMatFormer) for the feature matching problem. AMatFormer has two main aspects: First, it mainly conducts self-/cross-attention on some anchor features and leverages these anchor features as message bottleneck to learn the representations for all primal features. Thus, it can be implemented efficiently and compactly. Second, AMatFormer adopts a shared FFN module to further embed the features of two images into the common domain and thus learn the consensus feature representations for the matching problem. Experiments on several benchmarks demonstrate the effectiveness and efficiency of the proposed AMatFormer matching approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152390773",
                    "name": "Bo Jiang"
                },
                {
                    "authorId": "1646713373",
                    "name": "S. Luo"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2218482422",
                    "name": "Chuanfu Li"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                }
            ]
        },
        {
            "paperId": "3a1772d5eae2a92caebc0a66201cd4972e67219c",
            "title": "Semisupervised Edge-Aware Road Extraction via Cross Teaching Between CNN and Transformer",
            "abstract": "Semisupervised semantic segmentation of remote sensing images has been proven to be an effective approach to reduce manual annotation costs and leverage available unlabeled data to improve segmentation performance. However, some existing methods that focus on self-training and consistent regularization fail to consider large-scale characteristics of remote sensing images and the importance of incorporating road edge information. In this article, we propose a novel semisupervised edge-aware network (SSEANet) for remote sensing image semantic segmentation by jointly training convolutional neural network and transformer. SSEANet focuses on the consistency loss of multiscale features and uses an attention mechanism to fuse road edge information. Extensive experiments on DeepGlobe, Massachusetts, and AerialKITTI\u2013Bavaria datasets show that the proposed method outperforms state-of-the-art, demonstrating its effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237062069",
                    "name": "Zi-Xiong Yang"
                },
                {
                    "authorId": "2174899656",
                    "name": "Zhi-Hui You"
                },
                {
                    "authorId": "50358605",
                    "name": "Sibao Chen"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                },
                {
                    "authorId": "2075399909",
                    "name": "Bin Luo"
                }
            ]
        },
        {
            "paperId": "c38d8daa2a0f98cdc2a45a52665bb90797968f62",
            "title": "Generalizing Aggregation Functions in GNNs: Building High Capacity and Robust GNNs via Nonlinear Aggregation",
            "abstract": "The main aspect powering GNNs is the multi-layer network architecture to learn the nonlinear representation for graph learning task. The core operation in GNNs is the message propagation in which each node updates its information by aggregating the information from its neighbors. Existing GNNs usually adopt either linear neighborhood aggregation (e.g. mean, sum) or max aggregator in their message propagation. 1) For linear aggregators, the whole nonlinearity and network's capacity of GNNs are generally limited because deeper GNNs usually suffer from the over-smoothing issue due to their inherent information propagation mechanism. Also, linear aggregators are usually vulnerable to the spatial perturbations. 2) For max aggregator, it usually fails to be aware of the detailed information of node representations within neighborhood. To overcome these issues, we re-think the message propagation mechanism in GNNs and develop the new general nonlinear aggregators for neighborhood information aggregation in GNNs. One main aspect of our nonlinear aggregators is that they all provide the optimally balanced aggregator between max and mean/sum aggregators. Thus, they can inherit both i) high nonlinearity that enhances network's capacity, robustness and ii) detail-sensitivity that is aware of the detailed information of node representations in GNNs\u2019 message propagation. Promising experiments show the effectiveness, high capacity and robustness of the proposed methods.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49292271",
                    "name": "Beibei Wang"
                },
                {
                    "authorId": "152390773",
                    "name": "Bo Jiang"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                },
                {
                    "authorId": "2151264175",
                    "name": "B. Luo"
                }
            ]
        },
        {
            "paperId": "ea1e3f208ff7e0e5258f78112ed00293a26c7e0f",
            "title": "A Robust Feature Downsampling Module for Remote-Sensing Visual Tasks",
            "abstract": "Remote-sensing (RS) images present unique challenges for computer vision (CV) due to lower resolution, smaller objects, and fewer features. Mainstream backbone networks show promising results for traditional visual tasks. However, they use convolution to reduce feature map dimensionality, which can result in information loss for small objects in RS images and decreased performance. To address this problem, we propose a new and universal downsampling module named robust feature downsampling (RFD). RFD fuses multiple feature maps extracted by different downsampling techniques, creating a more robust feature map with a complementary set of features. Leveraging this, we overcome the limitations of conventional convolutional downsampling, resulting in a more accurate and robust analysis of RS images. We develop two versions of the RFD module, shallow RFD (SRFD) and deep RFD (DRFD), tailored to adapt to different stages of feature capture and improve feature robustness. We replace the downsampling layers (DSL) of existing mainstream backbones with the RFD module and conduct comparative experiments on several public RS image datasets. The results show significant improvements compared to baseline approaches in RS image classification, object detection, and semantic segmentation. Specifically, our RFD module achieved an average performance gain of 1.5% on the NWPU-RESISC45 classification dataset without utilizing any additional pretraining data, resulting in state-of-the-art performance on this dataset. Moreover, in detection and segmentation tasks on dataset for object detection in aerial images (DOTA) and instance segmentation in aerial images dataset (iSAID), our RFD module outperforms the baseline approaches by 2%\u20137% when utilizing pretraining data from NWPU-RESISC45. These results highlight the value of the RFD module in enhancing the performance of RS visual tasks. The code is available at https://github.com/lwCVer/RFD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143844110",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "50358605",
                    "name": "Sibao Chen"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                },
                {
                    "authorId": "2064886483",
                    "name": "Chris H. Q. Ding"
                },
                {
                    "authorId": "2075399909",
                    "name": "Bin Luo"
                }
            ]
        },
        {
            "paperId": "1212ec2d31727b6143367dc3d589750a9cee739c",
            "title": "BDTNet: Road Extraction by Bi-Direction Transformer From Remote Sensing Images",
            "abstract": "The past several years have witnessed the rapid development of the task of road extraction in high-resolution remote sensing images. However, due to the complex background and road distribution, road extraction is still a challenging research in remote sensing images. In convolutional neural networks (CNNs), the U-shaped architecture network has shown its effectiveness. But the global representation cannot be captured effectively by CNNs. While in the transformer, the self-attention (SA) module can capture the long-distance feature dependencies. A hybrid encoder\u2013decoder method called bi-direction transformer network (BDTNet) is proposed in this letter, which enhances the extraction of global and local information in remote sensing images. First, feature maps of different scales are obtained through the backbone network. And then, on the basis of reducing the computational cost of SA, the bi-direction transformer module (BDTM) is constructed to capture the contextual road information in feature maps of different scales. Finally, the feature refinement module (FRM) is introduced to integrate the features extracted from the backbone network and BDTM, which enhance the semantic information of the feature maps and obtain more detailed segmentation results. The results show that the proposed method achieved a high intersection over union (IoU) of 67.09% in the DeepGlobe dataset. Extensive experiments also verify the effectiveness of the proposed method on three public remote sensing road datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2173005211",
                    "name": "Lin Luo"
                },
                {
                    "authorId": "2109784517",
                    "name": "Jiaxin Wang"
                },
                {
                    "authorId": "50358605",
                    "name": "Sibao Chen"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                },
                {
                    "authorId": "2075399909",
                    "name": "Bin Luo"
                }
            ]
        },
        {
            "paperId": "15c2ed954146318cc5bf5d72c1eefeb776198c75",
            "title": "Reliable Contrastive Learning for Semi-Supervised Change Detection in Remote Sensing Images",
            "abstract": "With the development of deep learning in remote sensing (RS) image change detection (CD), the dependence of CD models on labeled data has become an important problem. To make better use of the comparatively resource-saving unlabeled data, the CD method based on semi-supervised learning (SSL) is worth further study. This article proposes a reliable contrastive learning (RCL) method for semi-supervised RS image CD. First, according to the task characteristics of CD, we design the contrastive loss based on the changed areas to enhance the model\u2019s feature extraction ability for changed objects. Then, to improve the quality of pseudo labels in SSL, we use the uncertainty of unlabeled data to select reliable pseudo labels for model training. Combining these methods, semi-supervised CD models can make full use of unlabeled data. Extensive experiments on three widely used CD datasets demonstrate the effectiveness of the proposed method. The results show that our semi-supervised approach has a better performance than related methods. The code is available at https://github.com/VCISwang/RC-Change-Detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109784517",
                    "name": "Jiaxin Wang"
                },
                {
                    "authorId": "2115646646",
                    "name": "Teng Li"
                },
                {
                    "authorId": "50358605",
                    "name": "Sibao Chen"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                },
                {
                    "authorId": "2075399909",
                    "name": "Bin Luo"
                },
                {
                    "authorId": "2111010157",
                    "name": "Richard C. Wilson"
                }
            ]
        },
        {
            "paperId": "33da384ab9cea5c9292d87c04e51fec4e846a998",
            "title": "Electrical Thermal Image Semantic Segmentation: Large-Scale Dataset and Baseline",
            "abstract": "The fault diagnosis of electrical equipment plays a vital role in the safe operation of the power system. The task of electrical thermal image semantic segmentation is to segment all electrical equipment in thermal images, which is a key step in the automatic fault diagnosis of electrical equipment. However, there lacks of a large-scale dataset in this research field. Therefore, we contribute to a large-scale dataset for electrical thermal image semantic segmentation. It contains 4839 thermal images and 17 types of electrical equipment. We provide pixel-level annotations to facilitate the performance evaluation of different semantic segmentation algorithms. To provide a strong baseline, we propose a cross-guidance network (CGNet), which jointly infers semantic segmentation maps and edge extraction results in an end-to-end learning framework, for electrical thermal image semantic segmentation. Extensive experiments on our launched dataset demonstrate the effectiveness of the proposed CGNet, and it also achieves the best performance on the general thermal image segmentation dataset. We will release our code and dataset at https://github.com/guo49/CGNet-pytorch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46429086",
                    "name": "Futian Wang"
                },
                {
                    "authorId": "2188131457",
                    "name": "Yin Guo"
                },
                {
                    "authorId": "2199572",
                    "name": "Chenglong Li"
                },
                {
                    "authorId": "2055010656",
                    "name": "Andong Lu"
                },
                {
                    "authorId": "2187985764",
                    "name": "Zhongfeng Ding"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                },
                {
                    "authorId": "2151905208",
                    "name": "Bin Luo"
                }
            ]
        },
        {
            "paperId": "63fbaf9789e8d97393b87275397aa509b0c63c14",
            "title": "MsKAT: Multi-Scale Knowledge-Aware Transformer for Vehicle Re-Identification",
            "abstract": "Existing vehicle re-identification (Re-ID) methods usually suffer from intra-instance discrepancy and inter-instance similarity. The key to solving this problem lies in filtering out identity-irrelevant interference and collecting identity-relevant vehicle details. In this paper, we aim to design a robust vehicle Re-ID framework that trains a model guided by knowledge vectors yet is able to disentangle the identity-relevant features and identity-irrelevant features. Toward this end, we propose a novel Multi-scale Knowledge-Aware Transformer (MsKAT) to build a knowledge-guided multi-scale feature alignment framework. First, we construct a Knowledge-Aware Transformer (KAT) to interact with semantic knowledge and visual feature. KAT mainly includes State elimination Transformer (SeT) to eliminate state (camera, viewpoint) interference and Attribute aggregation Transformer (AaT) to gather attribute (color, type) information. Second, to learn the knowledge-guided sample differences, we propose to encourage the separation of identity-relevant features and identity-irrelevant features by a Knowledge-Guided Alignment loss (<inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {L}_{KGA}$ </tex-math></inline-formula>). Specifically, <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {L}_{KGA}$ </tex-math></inline-formula> suppresses the difference between knowledge-guided positive pairs and the similarity between knowledge-guided negative pairs. Third, with the multi-scale settings of KAT and <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {L}_{KGA}$ </tex-math></inline-formula>, our model can capture knowledge-guided visual consistency features at different scales. Extensive evidence demonstrates our approach achieves new state-of-the-art on three widely-used vehicle re-identification benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108530355",
                    "name": "Hongchao Li"
                },
                {
                    "authorId": "2199572",
                    "name": "Chenglong Li"
                },
                {
                    "authorId": "4520695",
                    "name": "A. Zheng"
                },
                {
                    "authorId": "37864689",
                    "name": "Jin Tang"
                },
                {
                    "authorId": "2151905208",
                    "name": "Bin Luo"
                }
            ]
        }
    ]
}