{
    "authorId": "2260339714",
    "papers": [
        {
            "paperId": "4525feb8500de21bd99144365ad32b188c297eb2",
            "title": "Unsupervised Episode Detection for Large-Scale News Events",
            "abstract": "Episodic structures are inherently interpretable and adaptable to evolving large-scale key events. However, state-of-the-art automatic event detection methods overlook event episodes and, therefore, struggle with these crucial characteristics. This paper introduces a novel task, episode detection, aimed at identifying episodes from a news corpus containing key event articles. An episode describes a cohesive cluster of core entities (e.g.,\"protesters\",\"police\") performing actions at a specific time and location. Furthermore, an episode is a significant part of a larger group of episodes under a particular key event. Automatically detecting episodes is challenging because, unlike key events and atomic actions, we cannot rely on explicit mentions of times and locations to distinguish between episodes or use semantic similarity to merge inconsistent episode co-references. To address these challenges, we introduce EpiMine, an unsupervised episode detection framework that (1) automatically identifies the most salient, key-event-relevant terms and segments, (2) determines candidate episodes in an article based on natural episodic partitions estimated through shifts in discriminative term combinations, and (3) refines and forms final episode clusters using large language model-based reasoning on the candidate episodes. We construct three diverse, real-world event datasets annotated at the episode level. EpiMine outperforms all baselines on these datasets by an average 59.2% increase across all metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1490933248",
                    "name": "Priyanka Kargupta"
                },
                {
                    "authorId": "48379289",
                    "name": "Yunyi Zhang"
                },
                {
                    "authorId": "1381900594",
                    "name": "Yizhu Jiao"
                },
                {
                    "authorId": "2260339714",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2307763214",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "ab944b8561dc0280180469bdb94f189be1cc2e80",
            "title": "ActionIE: Action Extraction from Scientific Literature with Programming Languages",
            "abstract": "Extraction of experimental procedures from human language in scientific literature and patents into actionable sequences in robotics language holds immense significance in scientific domains. Such an action extraction task is particularly challenging given the intricate details and context-dependent nature of the instructions, especially in fields like chemistry where reproducibility is paramount. In this paper, we introduce A CTION IE, a method that leverages Large Language Models (LLMs) to bridge this divide by converting actions written in natural language into executable Python code. This enables us to capture the entities of interest, and the relationship between each action, given the features of Programming Languages. Utilizing linguistic cues identified by frequent patterns, ActionIE provides an improved mechanism to discern entities of interest. While our method is broadly applicable, we exemplify its power in the domain of chemical literature, wherein we focus on extracting experimental procedures for chemical synthesis. The code generated by our method can be easily transformed into robotics language which is in high demand in scientific fields. Comprehensive experiments demonstrate the superiority of our method. In addition, we propose a graph-based metric to more accurately reflect the precision of extraction. We also develop a dataset to address the scarcity of scientific literature occurred in existing datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2269835943",
                    "name": "Xianrui Zhong"
                },
                {
                    "authorId": "2312034033",
                    "name": "Yufeng Du"
                },
                {
                    "authorId": "2260339714",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2316709408",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "2316653022",
                    "name": "Tingfeng Luo"
                },
                {
                    "authorId": "2316633143",
                    "name": "Qirong Ho"
                },
                {
                    "authorId": "2316788204",
                    "name": "Hao Peng"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "b2fda33b7c122c044a7faa185d250d59ce9e4453",
            "title": "Investigating Data Contamination for Pre-training Language Models",
            "abstract": "Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}. We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy. Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "2298016051",
                    "name": "Ken Ziyu Liu"
                },
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "1749176844",
                    "name": "Rylan Schaeffer"
                },
                {
                    "authorId": "2260339714",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "123593472",
                    "name": "Sanmi Koyejo"
                }
            ]
        },
        {
            "paperId": "db40e0e2a7557bb4c9e4ab2e4f81bc3ec85abb17",
            "title": "Multi-LoRA Composition for Image Generation",
            "abstract": "Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate rendition of specific elements like distinct characters or unique styles in generated images. Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of complex imagery. In this paper, we study multi-LoRA composition through a decoding-centric perspective. We present two training-free methods: LoRA Switch, which alternates between different LoRAs at each denoising step, and LoRA Composite, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research. It features a diverse range of LoRA categories with 480 composition sets. Utilizing an evaluation framework based on GPT-4V, our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "2237948786",
                    "name": "Yelong Shen"
                },
                {
                    "authorId": "2287785375",
                    "name": "Shuohang Wang"
                },
                {
                    "authorId": "2238052953",
                    "name": "Yadong Lu"
                },
                {
                    "authorId": "1381900594",
                    "name": "Yizhu Jiao"
                },
                {
                    "authorId": "2260339714",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2287794511",
                    "name": "Donghan Yu"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2249538838",
                    "name": "Weizhu Chen"
                }
            ]
        },
        {
            "paperId": "2967ab775f6cdabc6ab59010734f352dd3ebc8d6",
            "title": "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction",
            "abstract": "Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction - a classic task in natural language processing - most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users. To address this, we propose a novel paradigm, termed On-Demand Information Extraction, to fulfill the personalized demands of real-world users. Our task aims to follow the instructions to extract the desired content from the associated text and present it in a structured tabular format. The table headers can either be user-specified or inferred contextually by the model. To facilitate research in this emerging area, we present a benchmark named InstructIE, inclusive of both automatically generated training data, as well as the human-annotated test set. Building on InstructIE, we further develop an On-Demand Information Extractor, ODIE. Comprehensive evaluations on our benchmark reveal that ODIE substantially outperforms the existing open-source models of similar size. Our code and dataset are released on https://github.com/yzjiao/On-Demand-IE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1381900594",
                    "name": "Yizhu Jiao"
                },
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "2263798673",
                    "name": "Sha Li"
                },
                {
                    "authorId": "2261459570",
                    "name": "Ruining Zhao"
                },
                {
                    "authorId": "2260339714",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "62454a3694e2e52b8698458440612505a3f7404b",
            "title": "The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions",
            "abstract": "Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between current NLP research and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as ``design'' and ``planning'' are prevalent in user interactions but are largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges they pose, and provide insights toward a roadmap to make LLMs better aligned with user needs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260339714",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2260822008",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "1381900594",
                    "name": "Yizhu Jiao"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "4099006",
                    "name": "Reid Pryzant"
                },
                {
                    "authorId": "2256797847",
                    "name": "Chenguang Zhu"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                }
            ]
        }
    ]
}