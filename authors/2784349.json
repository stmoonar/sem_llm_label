{
    "authorId": "2784349",
    "papers": [
        {
            "paperId": "01a2dfb88d25b3c1a0924938acb8c94757e3326c",
            "title": "Towards Croppable Implicit Neural Representations",
            "abstract": "Implicit Neural Representations (INRs) have peaked interest in recent years due to their ability to encode natural signals using neural networks. While INRs allow for useful applications such as interpolating new coordinates and signal compression, their black-box nature makes it difficult to modify them post-training. In this paper we explore the idea of editable INRs, and specifically focus on the widely used cropping operation. To this end, we present Local-Global SIRENs -- a novel INR architecture that supports cropping by design. Local-Global SIRENs are based on combining local and global feature extraction for signal encoding. What makes their design unique is the ability to effortlessly remove specific portions of an encoded signal, with a proportional weight decrease. This is achieved by eliminating the corresponding weights from the network, without the need for retraining. We further show how this architecture can be used to support the straightforward extension of previously encoded signals. Beyond signal editing, we examine how the Local-Global approach can accelerate training, enhance encoding of various signals, improve downstream performance, and be applied to modern INRs such as INCODE, highlighting its potential and flexibility. Code is available at https://github.com/maorash/Local-Global-INRs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1410953749",
                    "name": "Maor Ashkenazi"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                }
            ]
        },
        {
            "paperId": "2206250b2b5700ae0a3d18494f874b3270fd7116",
            "title": "Wavelet Convolutions for Large Receptive Fields",
            "abstract": "In recent years, there have been attempts to increase the kernel size of Convolutional Neural Nets (CNNs) to mimic the global receptive field of Vision Transformers' (ViTs) self-attention blocks. That approach, however, quickly hit an upper bound and saturated way before achieving a global receptive field. In this work, we demonstrate that by leveraging the Wavelet Transform (WT), it is, in fact, possible to obtain very large receptive fields without suffering from over-parameterization, e.g., for a $k \\times k$ receptive field, the number of trainable parameters in the proposed method grows only logarithmically with $k$. The proposed layer, named WTConv, can be used as a drop-in replacement in existing architectures, results in an effective multi-frequency response, and scales gracefully with the size of the receptive field. We demonstrate the effectiveness of the WTConv layer within ConvNeXt and MobileNetV2 architectures for image classification, as well as backbones for downstream tasks, and show it yields additional properties such as robustness to image corruption and an increased response to shapes over textures. Our code is available at https://github.com/BGU-CS-VIL/WTConv.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1704338184",
                    "name": "Shahaf E. Finder"
                },
                {
                    "authorId": "2310340195",
                    "name": "Roy Amoyal"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                },
                {
                    "authorId": "2546556",
                    "name": "O. Freifeld"
                }
            ]
        },
        {
            "paperId": "50b7ede63dbe350c2d0bbd06aa3d954673a88c33",
            "title": "An Over Complete Deep Learning Method for Inverse Problems",
            "abstract": "Obtaining meaningful solutions for inverse problems has been a major challenge with many applications in science and engineering. Recent machine learning techniques based on proximal and diffusion-based methods have shown promising results. However, as we show in this work, they can also face challenges when applied to some exemplary problems. We show that similar to previous works on over-complete dictionaries, it is possible to overcome these shortcomings by embedding the solution into higher dimensions. The novelty of the work proposed is that we jointly design and learn the embedding and the regularizer for the embedding vector. We demonstrate the merit of this approach on several exemplary and common inverse problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104172497",
                    "name": "Moshe Eliasof"
                },
                {
                    "authorId": "2257221683",
                    "name": "Eldad Haber"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                }
            ]
        },
        {
            "paperId": "57254c8083a6fe4f39ea4b61b190c97bb047ab60",
            "title": "Graph Neural Reaction Diffusion Models",
            "abstract": "The integration of Graph Neural Networks (GNNs) and Neural Ordinary and Partial Differential Equations has been extensively studied in recent years. GNN architectures powered by neural differential equations allow us to reason about their behavior, and develop GNNs with desired properties such as controlled smoothing or energy conservation. In this paper we take inspiration from Turing instabilities in a Reaction Diffusion (RD) system of partial differential equations, and propose a novel family of GNNs based on neural RD systems. We \\textcolor{black}{demonstrate} that our RDGNN is powerful for the modeling of various data types, from homophilic, to heterophilic, and spatio-temporal datasets. We discuss the theoretical properties of our RDGNN, its implementation, and show that it improves or offers competitive performance to state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104172497",
                    "name": "Moshe Eliasof"
                },
                {
                    "authorId": "2257221683",
                    "name": "Eldad Haber"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                }
            ]
        },
        {
            "paperId": "9a227e7014c89095b713bdd0263b13ec6d6256f4",
            "title": "On The Temporal Domain of Differential Equation Inspired Graph Neural Networks",
            "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in modeling complex relationships in graph-structured data. A recent innovation in this field is the family of Differential Equation-Inspired Graph Neural Networks (DE-GNNs), which leverage principles from continuous dynamical systems to model information flow on graphs with built-in properties such as feature smoothing or preservation. However, existing DE-GNNs rely on first or second-order temporal dependencies. In this paper, we propose a neural extension to those pre-defined temporal dependencies. We show that our model, called TDE-GNN, can capture a wide range of temporal dynamics that go beyond typical first or second-order methods, and provide use cases where existing temporal models are challenged. We demonstrate the benefit of learning the temporal dependencies using our method rather than using pre-defined temporal dynamics on several graph benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104172497",
                    "name": "Moshe Eliasof"
                },
                {
                    "authorId": "145761835",
                    "name": "E. Haber"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                },
                {
                    "authorId": "1711104",
                    "name": "C. Sch\u00f6nlieb"
                }
            ]
        },
        {
            "paperId": "e1d1a3462a82c5a21921cf7c49b21a23fffae0f5",
            "title": "Physics-guided Full Waveform Inversion using Encoder-Solver Convolutional Neural Networks",
            "abstract": "\n Full Waveform Inversion (FWI) is an inverse problem for estimating the wave velocity distribution in a given domain, based on observed data on the boundaries. The inversion is computationally demanding because we are required to solve multiple forward problems, either in time or frequency domains, to simulate data that are then iteratively fitted to the observed data. We consider FWI in the frequency domain, where the Helmholtz equation is used as a forward model, and its repeated solution is the main computational bottleneck of the inversion process. To ease this cost, we integrate a learning process of an Encoder-Solver preconditioner that is based on convolutional neural networks (CNNs). The Encoder-Solver is trained to effectively precondition the discretized Helmholtz operator given velocity medium parameters. Then, by re-training the CNN between the iterations of the optimization process, the Encoder-Solver is adapted to the iteratively evolving velocity medium as part of the inversion. Without retraining, the performance of the solver deteriorates as the medium changes. Using our light retraining procedures, we obtain the forward simulations effectively throughout the process. We demonstrate our approach to solving FWI problems using 2D geophysical models with high-frequency data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2303466123",
                    "name": "Matan Goren"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                }
            ]
        },
        {
            "paperId": "2e4bcdd638a15354e10500eaab5db73ebf924e0e",
            "title": "Graph Positional Encoding via Random Feature Propagation",
            "abstract": "Two main families of node feature augmentation schemes have been explored for enhancing GNNs: random features and spectral positional encoding. Surprisingly, however, there is still no clear understanding of the relation between these two augmentation schemes. Here we propose a novel family of positional encoding schemes which draws a link between the above two approaches and improves over both. The new approach, named Random Feature Propagation (RFP), is inspired by the power iteration method and its generalizations. It concatenates several intermediate steps of an iterative algorithm for computing the dominant eigenvectors of a propagation matrix, starting from random node features. Notably, these propagation steps are based on graph-dependent propagation operators that can be either predefined or learned. We explore the theoretical and empirical benefits of RFP. First, we provide theoretical justifications for using random features, for incorporating early propagation steps, and for using multiple random initializations. Then, we empirically demonstrate that RFP significantly outperforms both spectral PE and random features in multiple node classification and graph classification benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104172497",
                    "name": "Moshe Eliasof"
                },
                {
                    "authorId": "51484149",
                    "name": "Fabrizio Frasca"
                },
                {
                    "authorId": "2079900490",
                    "name": "Beatrice Bevilacqua"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                },
                {
                    "authorId": "1732280",
                    "name": "Gal Chechik"
                },
                {
                    "authorId": "2118871788",
                    "name": "Haggai Maron"
                }
            ]
        },
        {
            "paperId": "35faf8cc89042bd816d1ea449661fb8479c9970e",
            "title": "Multigrid-Augmented Deep Learning Preconditioners for the Helmholtz Equation Using Compact Implicit Layers",
            "abstract": "We present a deep learning-based iterative approach to solve the discrete heterogeneous Helmholtz equation for high wavenumbers. Combining classical iterative multigrid solvers and convolutional neural networks (CNNs) via preconditioning, we obtain a learned neural solver that is faster and scales better than a standard multigrid solver. Our approach offers three main contributions over previous neural methods of this kind. First, we construct a multilevel U-Net-like encoder-solver CNN with an implicit layer on the coarsest grid of the U-Net, where convolution kernels are inverted. This alleviates the field of view problem in CNNs and allows better scalability. Second, we improve upon the previous CNN preconditioner in terms of the number of parameters, computation time, and convergence rates. Third, we propose a multiscale training approach that enables the network to scale to problems of previously unseen dimensions while still maintaining a reasonable training procedure. Our encoder-solver architecture can be used to generalize over different slowness models of various difficulties and is efficient at solving for many right-hand sides per slowness model. We demonstrate the benefits of our novel architecture with numerical experiments on a variety of heterogeneous two-dimensional problems at high wavenumbers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2228823757",
                    "name": "Bar Lerer"
                },
                {
                    "authorId": "1419449987",
                    "name": "Ido Ben-Yair"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                }
            ]
        },
        {
            "paperId": "38baee574980808dc04adcaaf3631c99dc260c18",
            "title": "Efficient Graph Laplacian Estimation by Proximal Newton",
            "abstract": "The Laplacian-constrained Gaussian Markov Random Field (LGMRF) is a common multivariate statistical model for learning a weighted sparse dependency graph from given data. This graph learning problem can be formulated as a maximum likelihood estimation (MLE) of the precision matrix, subject to Laplacian structural constraints, with a sparsity-inducing penalty term. This paper aims to solve this learning problem accurately and efficiently. First, since the commonly used $\\ell_1$-norm penalty is inappropriate in this setting and may lead to a complete graph, we employ the nonconvex minimax concave penalty (MCP), which promotes sparse solutions with lower estimation bias. Second, as opposed to existing first-order methods for this problem, we develop a second-order proximal Newton approach to obtain an efficient solver, utilizing several algorithmic features, such as using Conjugate Gradients, preconditioning, and splitting to active/free sets. Numerical experiments demonstrate the advantages of the proposed method in terms of both computational complexity and graph learning accuracy compared to existing methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2205544649",
                    "name": "Yakov Medvedovsky"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                },
                {
                    "authorId": "2983158",
                    "name": "T. Routtenberg"
                }
            ]
        },
        {
            "paperId": "4b7d4115569bc4811eef296f92dabff709dd2c0c",
            "title": "LFA-tuned matrix-free multigrid method for the elastic Helmholtz equation",
            "abstract": "We present an efficient matrix-free geometric multigrid method for the elastic Helmholtz equation, and a suitable discretization. Many discretization methods had been considered in the literature for the Helmholtz equations, as well as many solvers and preconditioners, some of which are adapted for the elastic version of the equation. However, there is very little work considering the reciprocity of discretization and a solver. In this work, we aim to bridge this gap. By choosing an appropriate stencil for re-discretization of the equation on the coarse grid, we develop a multigrid method that can be easily implemented as matrix-free, relying on stencils rather than sparse matrices. This is crucial for efficient implementation on modern hardware. Using two-grid local Fourier analysis, we validate the compatibility of our discretization with our solver, and tune a choice of weights for the stencil for which the convergence rate of the multigrid cycle is optimal. It results in a scalable multigrid preconditioner that can tackle large real-world 3D scenarios.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053530213",
                    "name": "Rachel Yovel"
                },
                {
                    "authorId": "2784349",
                    "name": "Eran Treister"
                }
            ]
        }
    ]
}