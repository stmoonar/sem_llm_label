{
    "authorId": "1711271",
    "papers": [
        {
            "paperId": "21b4777948797377deedf4a9f1f58ad13f6b8b5d",
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2887412",
                    "name": "Jinchao Li"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2066415714",
                    "name": "Zhengcong Fei"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2257374643",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2116575668",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "1979505",
                    "name": "A. Geramifard"
                },
                {
                    "authorId": "1765212",
                    "name": "Chiori Hori"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "06ba6597fd497f02895d49772fe3c6ca81c08565",
            "title": "Optimization of Cross-Lingual Voice Conversion With Linguistics Losses to Reduce Foreign Accents",
            "abstract": "Cross-lingual voice conversion (XVC) transforms the speaker identity of a source speaker to that of a target speaker who speaks a different language. Due to the intrinsic differences between languages, the converted speech may carry an unwanted foreign accent. In this paper, we first investigate the intelligibility of the converted speech and confirm the performance degradation caused by the accent/intelligibility issue. With the goal of generating native-sounding speech, this paper further proposes a novel training scheme with two additional linguistic losses for speech waveform generation: 1) a frame-wise phonetic content loss derived from bottleneck features, and 2) an automatic speech recognition loss on characters. Experiments were conducted between English and Mandarin Chinese conversions. The experimental results confirmed that the generated speech sounds more natural with the proposed linguistic losses and the proposed solution significantly improves speech intelligibility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118764729",
                    "name": "Yi Zhou"
                },
                {
                    "authorId": "2758425",
                    "name": "Zhizheng Wu"
                },
                {
                    "authorId": "2339600",
                    "name": "Xiaohai Tian"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "12fde6b43200d7634375666f1bf168c04c309f93",
            "title": "ADD 2023: the Second Audio Deepfake Detection Challenge",
            "abstract": "Audio deepfake detection is an emerging topic in the artificial intelligence community. The second Audio Deepfake Detection Challenge (ADD 2023) aims to spur researchers around the world to build new innovative technologies that can further accelerate and foster research on detecting and analyzing deepfake speech utterances. Different from previous challenges (e.g. ADD 2022), ADD 2023 focuses on surpassing the constraints of binary real/fake classification, and actually localizing the manipulated intervals in a partially fake speech as well as pinpointing the source responsible for generating any fake audio. Furthermore, ADD 2023 includes more rounds of evaluation for the fake audio game sub-challenge. The ADD 2023 challenge includes three subchallenges: audio fake game (FG), manipulation region location (RL) and deepfake algorithm recognition (AR). This paper describes the datasets, evaluation metrics, and protocols. Some findings are also reported in audio deepfake detection tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "3491973",
                    "name": "Jiangyan Yi"
                },
                {
                    "authorId": "2163134242",
                    "name": "Jianhua Tao"
                },
                {
                    "authorId": "3418514",
                    "name": "Ruibo Fu"
                },
                {
                    "authorId": "2046737435",
                    "name": "Xinrui Yan"
                },
                {
                    "authorId": "2144523153",
                    "name": "Chenglong Wang"
                },
                {
                    "authorId": "2156633072",
                    "name": "Tao Wang"
                },
                {
                    "authorId": "2109300331",
                    "name": "Chu Yuan Zhang"
                },
                {
                    "authorId": "2145565556",
                    "name": "Xiaohui Zhang"
                },
                {
                    "authorId": "2119327826",
                    "name": "Yan Zhao"
                },
                {
                    "authorId": "2218205673",
                    "name": "Yong Ren"
                },
                {
                    "authorId": "51159684",
                    "name": "Leling Xu"
                },
                {
                    "authorId": "2210315603",
                    "name": "Jun Zhou"
                },
                {
                    "authorId": "2218151642",
                    "name": "Hao Gu"
                },
                {
                    "authorId": "1718662",
                    "name": "Zhengqi Wen"
                },
                {
                    "authorId": "2087041209",
                    "name": "Shan Liang"
                },
                {
                    "authorId": "145824699",
                    "name": "Zheng Lian"
                },
                {
                    "authorId": "2066360665",
                    "name": "Shuai Nie"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "2fe05c08718fa7db57adeb171fc53e68070894cd",
            "title": "Spiking-Leaf: A Learnable Auditory Front-End for Spiking Neural Networks",
            "abstract": "Brain-inspired spiking neural networks (SNNs) have demonstrated great potential for temporal signal processing. However, their performance in speech processing remains limited due to the lack of an effective auditory front-end. To address this limitation, we introduce Spiking-LEAF, a learnable auditory front-end meticulously designed for SNN-based speech processing. Spiking-LEAF combines a learnable filter bank with a novel two-compartment spiking neuron model called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure of inner hair cells (IHC) and they leverage segregated dendritic and somatic compartments to effectively capture multi-scale temporal dynamics of speech signals. Additionally, the IHC-LIF neurons incorporate the lateral feedback mechanism along with spike regularization loss to enhance spike encoding efficiency. On keyword spotting and speaker identification tasks, the proposed Spiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional real-valued acoustic features in terms of classification accuracy, noise robustness, and encoding efficiency.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2087073535",
                    "name": "Zeyang Song"
                },
                {
                    "authorId": "47876444",
                    "name": "Jibin Wu"
                },
                {
                    "authorId": "1953388",
                    "name": "Malu Zhang"
                },
                {
                    "authorId": "2269732177",
                    "name": "Mike Zheng Shou"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "47332a5a73abb5d0159ba66e604f524bd403fe1e",
            "title": "USED: Universal Speaker Extraction and Diarization",
            "abstract": "Speaker extraction and diarization are two enabling techniques for real-world speech applications. Speaker extraction aims to extract a target speaker's voice from a speech mixture, while speaker diarization demarcates speech segments by speaker, annotating `who spoke when'. Previous studies have typically treated the two tasks independently. In practical applications, it is more meaningful to have knowledge about `who spoke what and when', which is captured by the two tasks. The two tasks share a similar objective of disentangling speakers. Speaker extraction operates in the frequency domain, whereas diarization is in the temporal domain. It is logical to believe that speaker activities obtained from speaker diarization can benefit speaker extraction, while the extracted speech offers more accurate speaker activity detection than the speech mixture. In this paper, we propose a unified model called Universal Speaker Extraction and Diarization (USED) to address output inconsistency and scenario mismatch issues. It is designed to manage speech mixture with varying overlap ratios and variable number of speakers. We show that the USED model significantly outperforms the competitive baselines for speaker extraction and diarization tasks on LibriMix and SparseLibriMix datasets. We further validate the diarization performance on CALLHOME, a dataset based on real recordings, and experimental results indicate that our model surpasses recently proposed approaches.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2052108603",
                    "name": "Junyi Ao"
                },
                {
                    "authorId": "2243050643",
                    "name": "Mehmet Sinan Yildirim"
                },
                {
                    "authorId": "144484854",
                    "name": "Mengyao Ge"
                },
                {
                    "authorId": "2243949363",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "1866636284",
                    "name": "Ruijie Tao"
                },
                {
                    "authorId": "2199167167",
                    "name": "Yan-min Qian"
                },
                {
                    "authorId": "2244344346",
                    "name": "Liqun Deng"
                },
                {
                    "authorId": "2243925312",
                    "name": "Longshuai Xiao"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "4f8e07f3c531aefe1418da031e8193625c7af7a4",
            "title": "Evaluation analysis and promotion paths of regional green innovation vitality in China",
            "abstract": "PurposeThe purpose of this work is to construct a grey entropy comprehensive evaluation model to measure the regional green innovation vitality (GIV) of 31 provinces in China.Design/methodology/approachThe traditional grey relational proximity and grey relational similarity degree are integrated into the novel comprehensive grey evaluation framework. The evaluation system of regional green innovation vitality is constructed from three dimensions: economic development vitality, innovative transformation power and environmental protection efficacy. The weights of each indicator are obtained by the entropy weight method. The GIV of 31 provinces in China is measured based on provincial panel data from 2016 to 2020. The ward clustering and K-nearest-neighbor (KNN) algorithms are utilized to explore the regional green innovation discrepancies and promotion paths.FindingsThe novel grey evaluation method exhibits stronger ability to capture intrinsic patterns compared with two separate traditional grey relational models. Green innovation vitality shows obvious regional discrepancies. The Matthew effect of China's regional GIV is obvious, showing a basic trend of strong in the eastern but weak in the western areas. The comprehensive innovation vitality of economically developed provinces exhibits steady increasing trend year by year, while the innovation vitality of less developed regions shows an overall steady state of no fluctuation.Practical implicationsThe grey entropy comprehensive relational model in this study is applied for the measurement and evaluation of regional GIV, which improves the one-sidedness of traditional grey relational analysis on the proximity or similarity among sequences. In addition, a three-dimensional evaluation system of regional GIV is constructed, which provides the practical guidance for the research of regional development strategic planning as well as promotion paths.Originality/valueA comprehensive grey entropy relational model based on traditional grey incidence analysis (GIA) in terms of proximity and similarity is proposed. The three-dimensional evaluation system of China's regional GIV is constructed, which provides a new research perspective for regional innovation evaluation and expands the application scope of grey system theory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153168964",
                    "name": "Wenhao Zhou"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2117808557",
                    "name": "Liping Zhang"
                },
                {
                    "authorId": "2223465384",
                    "name": "Huimin Tian"
                },
                {
                    "authorId": "2223561160",
                    "name": "Meng Fu"
                }
            ]
        },
        {
            "paperId": "61a9755e17362d5117bd8d1b06d6421e1ac8b865",
            "title": "Ripple Sparse Self-Attention for Monaural Speech Enhancement",
            "abstract": "The use of Transformer represents a recent success in speech enhancement. However, as its core component, self-attention suffers from quadratic complexity, which is computationally prohibited for long speech recordings. Moreover, it allows each time frame to attend to all time frames, neglecting the strong local correlations of speech signals. This study presents a simple yet effective sparse self-attention for speech enhancement, called ripple attention, which simultaneously performs fine- and coarse-grained modeling for local and global dependencies, respectively. Specifically, we employ local band attention to enable each frame to attend to its closest neighbor frames in a window at fine granularity, while employing dilated attention outside the window to model the global dependencies at a coarse granularity. We evaluate the efficacy of our ripple attention for speech enhancement on two commonly used training objectives. Extensive experimental results consistently confirm the superior performance of the ripple attention design over standard full self-attention, blockwise attention, and dual-path attention (Sep-Former) in terms of speech quality and intelligibility.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "8326475",
                    "name": "Qiquan Zhang"
                },
                {
                    "authorId": "2149670437",
                    "name": "Hongxu Zhu"
                },
                {
                    "authorId": "2154024128",
                    "name": "Qi Song"
                },
                {
                    "authorId": "49258424",
                    "name": "Xinyuan Qian"
                },
                {
                    "authorId": "3877669",
                    "name": "Zhaoheng Ni"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "6b53fc854052f8e17bb1c165fa91cad9809796af",
            "title": "Audio-Visual Active Speaker Extraction for Sparsely Overlapped Multi-Talker Speech",
            "abstract": "Target speaker extraction aims to extract the speech of a specific speaker from a multi-talker mixture as specified by an auxiliary reference. Most studies focus on the scenario where the target speech is highly overlapped with the interfering speech. However, this scenario only accounts for a small percentage of real-world conversations. In this paper, we aim at the sparsely overlapped scenarios in which the auxiliary reference needs to perform two tasks simultaneously: detect the activity of the target speaker and disentangle the active speech from any interfering speech. We propose an audio-visual speaker extraction model named ActiveExtract, which leverages speaking activity from audio-visual active speaker detection (ASD). The ASD directly provides the frame-level activity of the target speaker, while its intermediate feature representation is trained to discriminate speech-lip synchronization that could be used for speaker disentanglement. Experimental results show our model outperforms baselines across various overlapping ratios, achieving an average improvement of more than 4 dB in terms of SI-SNR.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "46276037",
                    "name": "Jun Yu Li"
                },
                {
                    "authorId": "1866636284",
                    "name": "Ruijie Tao"
                },
                {
                    "authorId": "1932103679",
                    "name": "Zexu Pan"
                },
                {
                    "authorId": "2242568154",
                    "name": "Meng Ge"
                },
                {
                    "authorId": "2243949363",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "7ebff41444a4deaf34f4d233e121ba8bfd273580",
            "title": "Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion",
            "abstract": "Audio Deepfake Detection (ADD) aims to detect the fake audio generated by text-to-speech (TTS), voice conversion (VC) and replay, etc., which is an emerging topic. Traditionally we take the mono signal as input and focus on robust feature extraction and effective classifier design. However, the dual-channel stereo information in the audio signal also includes important cues for deepfake, which has not been studied in the prior work. In this paper, we propose a novel ADD model, termed as M2S-ADD, that attempts to discover audio authenticity cues during the mono-to-stereo conversion process. We first projects the mono to a stereo signal using a pretrained stereo synthesizer, then employs a dual-branch neural architecture to process the left and right channel signals, respectively. In this way, we effectively reveal the artifacts in the fake audio, thus improve the ADD performance. The experiments on the ASVspoof2019 database show that M2S-ADD outperforms all baselines that input mono. We release the source code at \\url{https://github.com/AI-S2-Lab/M2S-ADD}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1820776989",
                    "name": "Rui Liu"
                },
                {
                    "authorId": "2218446983",
                    "name": "Jinhua Zhang"
                },
                {
                    "authorId": "1807620",
                    "name": "Guanglai Gao"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "8ced04492608d077028137ad63508b864e8f1715",
            "title": "Multi-Head Attention and GRU for Improved Match-Mismatch Classification of Speech Stimulus and EEG Response",
            "abstract": "This work is based on the participation by the HyperAttention team in the Auditory EEG Decoding Challenge, 2023 (ICASSP 2023 Signal Processing Grand Challenge) task 1, which deals with the match-mismatch classification of speech stimuli and EEG responses of human listeners. We demonstrate the benefits of using mel-spectrograms instead of speech envelopes as input features as well as the effectiveness of Multi-Head Attention and GRU for EEG and speech processing. With a total score of 79.05 %, we reach the second place in the challenge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135178957",
                    "name": "Marvin Borsdorf"
                },
                {
                    "authorId": "2216441264",
                    "name": "Saurav Pahuja"
                },
                {
                    "authorId": "2038111086",
                    "name": "Gabriel Ivucic"
                },
                {
                    "authorId": "2139952127",
                    "name": "Siqi Cai"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "145618636",
                    "name": "Tanja Schultz"
                }
            ]
        }
    ]
}