{
    "authorId": "2055489471",
    "papers": [
        {
            "paperId": "0925cb188cb1863b2e2d672a0788dc9687ec7694",
            "title": "MGEL: Multigrained Representation Analysis and Ensemble Learning for Text Moderation",
            "abstract": "In this work, we describe our efforts in addressing two typical challenges involved in the popular text classification methods when they are applied to text moderation: the representation of multibyte characters and word obfuscations. Specifically, a multihot byte-level scheme is developed to significantly reduce the dimension of one-hot character-level encoding caused by the multiplicity of instance-scarce non-ASCII characters. In addition, we introduce a simple yet effective weighting approach for fusing n-gram features to empower the classical logistic regression. Surprisingly, it outperforms well-tuned representative neural networks greatly. As a continual effort toward text moderation, we endeavor to analyze the current state-of-the-art (SOTA) algorithm bidirectional encoder representations from transformers (BERT), which works well in context understanding but performs poorly on intentional word obfuscations. To resolve this crux, we then develop an enhanced variant and remedy this drawback by integrating byte and character decomposition. It advances the SOTA performance on the largest abusive language datasets as demonstrated by our comprehensive experiments. Our work offers a feasible and effective framework to tackle word obfuscations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1491233580",
                    "name": "Fei Tan"
                },
                {
                    "authorId": "46622514",
                    "name": "Changwei Hu"
                },
                {
                    "authorId": "1809492",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "2153551136",
                    "name": "Zhi Wei"
                },
                {
                    "authorId": "35164325",
                    "name": "Aasish Pappu"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                },
                {
                    "authorId": "40662871",
                    "name": "Keqian Li"
                }
            ]
        },
        {
            "paperId": "0aa0936f8cb7dc7d83b54ab347492a7c5c1ba82c",
            "title": "Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense",
            "abstract": "We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K human-written text perturbations in the wild and leverages them for realistic adversarial attack. Unlike existing character-based attacks which often deductively hypothesize a set of manipulation strategies, our work is grounded on actual observations from real-world texts. We find that adversarial texts generated by ANTHRO achieve the best trade-off between (1) attack success rate, (2) semantic preservation of the original text, and (3) stealthiness\u2013i.e. indistinguishable from human writings hence harder to be flagged as suspicious. Specifically, our attacks accomplished around 83% and 91% attack success rates on BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger baseline with an increase of 50% and 40% in terms of semantic preservation and stealthiness when evaluated by both layperson and professional human workers. ANTHRO can further enhance a BERT classifier\u2019s performance in understanding different variations of human-written toxic texts via adversarial training when compared to the Perspective API.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "2159644449",
                    "name": "Jooyoung Lee"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "1809492",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "0d609607411fc21f011d7360e04a7f2ac7a1b5af",
            "title": "SmartGD: A GAN-Based Graph Drawing Framework for Diverse Aesthetic Goals",
            "abstract": "While a multitude of studies have been conducted on graph drawing, many existing methods only focus on optimizing a single aesthetic aspect of graph layouts, which can lead to sub-optimal results. There are a few existing methods that have attempted to develop a flexible solution for optimizing different aesthetic aspects measured by different aesthetic criteria. Furthermore, thanks to the significant advance in deep learning techniques, several deep learning-based layout methods were proposed recently. These methods have demonstrated the advantages of deep learning approaches for graph drawing. However, none of these existing methods can be directly applied to optimizing non-differentiable criteria without special accommodation. In this work, we propose a novel Generative Adversarial Network (GAN) based deep learning framework for graph drawing, called SmartGD, which can optimize different quantitative aesthetic goals, regardless of their differentiability. To demonstrate the effectiveness and efficiency of SmartGD, we conducted experiments on minimizing stress, minimizing edge crossing, maximizing crossing angle, maximizing shape-based metrics, and a combination of multiple aesthetics. Compared with several popular graph drawing algorithms, the experimental results show that SmartGD achieves good performance both quantitatively and qualitatively.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2144685915",
                    "name": "Xiaoqi Wang"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "1809492",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "2110771216",
                    "name": "Hang Shen"
                }
            ]
        },
        {
            "paperId": "4fe908b338babb21e379f87ce87278f885322c74",
            "title": "SmartGD: A Self-Challenging Generative Adversarial Network for Graph Drawing",
            "abstract": "\u2014A multitude of studies have been conducted on graph drawing, but many existing methods only focus on optimizing particular aesthetic aspects of graph layout. Given a graph, generating a good layout that satis\ufb01es certain human aesthetic preference remains a challenging task, especially if such preference can not be expressed as a differentiable objective function. In this paper, we propose a student-teacher GAN-based graph drawing framework, SmartGD, which learns to draw graphs just like how humans learn to perform tasks. The student network in the SmartGD learns graph drawing by imitating good layout examples, while the teacher network in SmartGD is responsible for providing ratings regarding the goodness of the generated layouts. When there is a lack of concrete aesthetic criteria to specify what constitutes a good layout, the student network can learn from the good layout examples. On the other hand, when the goodness of a layout can be assessed by quantitative criteria (even if not differentiable), the student network can use it as a concrete goal to optimize the target aesthetics. To accomplish the goal, we propose a novel variant of GAN, self-challenging GAN , to learn the optimal layout distribution with respect to any aesthetic criterion, whether the criterion is differentiable or not. The proposed graph drawing framework can not only draw graphs in a similar style as the good layout examples but also optimize the graph layouts according to any given aesthetic criteria when available. Once the model is trained, it can be used to visualize arbitrary graphs according to the style of the example layouts or the chosen aesthetic criteria. The comprehensive experimental studies show that SmartGD signi\ufb01cantly outperforms 12 benchmark methods according to the commonly agreed metrics. In addition, we conduct qualitative evaluations on visualizing large graphs with hundreds to thousands of nodes to show that the proposed algorithm works perfectly on large graphs as well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144685915",
                    "name": "Xiaoqi Wang"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "50819900",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "39073212",
                    "name": "Han-Wei Shen"
                }
            ]
        },
        {
            "paperId": "24de6c7ab2ed216b4f1982a2d69927f2325a1d14",
            "title": "TSI: An Ad Text Strength Indicator using Text-to-CTR and Semantic-Ad-Similarity",
            "abstract": "Coming up with effective ad text is a time consuming process, and particularly challenging for small businesses with limited advertising experience. When an inexperienced advertiser onboards with a poorly written ad text, the ad platform has the opportunity to detect low performing ad text, and provide improvement suggestions. To realize this opportunity, we propose an ad text strength indicator (TSI) which: (i) predicts the click-through-rate (CTR) for an input ad text, (ii) fetches similar existing ads to create a neighborhood around the input ad, (iii) and compares the predicted CTRs in the neighborhood to declare whether the input ad is strong or weak. In addition, as suggestions for ad text improvement, TSI shows anonymized versions of superior ads (higher predicted CTR) in the neighborhood. For (i), we propose a BERT based text-to-CTR model trained on impressions and clicks associated with an ad text. For (ii), we propose a sentence-BERT based semantic-ad-similarity model trained using weak labels from ad campaign setup data. Offline experiments demonstrate that our BERT based text-to-CTR model achieves a significant lift in CTR prediction AUC for cold start (new) advertisers compared to bag-of-words based baselines. In addition, our semantic-textual-similarity model for similar ads retrieval achieves a precision@1 of 0.93 (for retrieving ads from the same product category); this is significantly higher compared to unsupervised TF-IDF, word2vec, and sentence-BERT baselines. Finally, we share promising online results from advertisers in the Yahoo (Verizon Media) ad platform where a variant of TSI was implemented with sub-second end-to-end latency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145668423",
                    "name": "Shaunak Mishra"
                },
                {
                    "authorId": "46622514",
                    "name": "Changwei Hu"
                },
                {
                    "authorId": "1840437995",
                    "name": "Manisha Verma"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "50819900",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "145917065",
                    "name": "M. Sviridenko"
                }
            ]
        },
        {
            "paperId": "470dc50a7551f0b228fd37595e5daa4ec2ad7d7d",
            "title": "Hadoop-MTA: a system for Multi Data-center Trillion Concepts Auto-ML atop Hadoop",
            "abstract": "The ever-growing computation capability distributed infrastructure brings tremendous opportunities for mining and analysis of data that was impossible otherwise. Meanwhile, the inherent computation model of distributed system also brings unique and non-trivial challenges for traditional Auto-ML, including the explosion of data dimensions, the expected absence of features, and the heterogeneity of information. This is especially the case in modern Internet enterprises, where data in the scale of trillions are stored in multiple data centers, and the discovery of subtle signals could incur significant impact in revenue and welfare. How can we best harness the large scale distributed machine learning, but without keeping engineers constantly in the loop? In this work, we present Hadoop-MTA, a system for Multi Data-center, Trillion Concepts, Auto-ML on top of the Hadoop distributed computation environment that leverages sparsity aware heterogeneous knowledge graph representation and dimensionality agnostic parallel learning. Through multiple large scale experiments, we find that Hadoop-MTA significantly output-performs competitive state of the art distributed learning algorithms and scales well to trillion scale data-sets. Our model is rolled out to Hadoop serving infrastructure in Yahoo covering billions of unique identities and shows improvements 129.5% accuracy and 106.5 % weighted F1-score (more than 2x) on key targeting use cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40662871",
                    "name": "Keqian Li"
                },
                {
                    "authorId": "1809492",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "1840437995",
                    "name": "Manisha Verma"
                },
                {
                    "authorId": "1491233580",
                    "name": "Fei Tan"
                },
                {
                    "authorId": "46622514",
                    "name": "Changwei Hu"
                },
                {
                    "authorId": "145007960",
                    "name": "Tejaswi Kasturi"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                }
            ]
        },
        {
            "paperId": "48933f5457a77bbd1d4f871f63ebfe9bae4c18fe",
            "title": "BAN: Large Scale Brand ANonymization for Creative Recommendation via Label Light Adaptation",
            "abstract": "One of the primary component in ads creative recommendation system is the brand anonymization that removes brand-specific information from ad text for legal compliance and providing ready to use template for the advertisers to customize and consume. In our previous work [1] on ads creative recommendation system, the anonymization is done via a block list created solely based on manual reviewing, which is expensive and limits in the scale of the deployment of the ads recommendation. In this work we investigate a large scale, automated approach for brand anonymization. Such a problem presents many unique and non-trivial challenges, including the domain specificity of the brand entities, the fine-granularity requirements of structured output, the tight constraint of the limited contexts, the high level of grammatical noise in the advertisement data, and the heterogeneity of information required to perform anonymization. We propose a transformer model that leverage implicit knowledge together with a label-light adaptation procedure for this task. Our model is rolled out to ads systems in Yahoo that cover billions of impression traffic per month and improved previous production system by 68.3% F1-score on token level prediction and 61.6% on ad level prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40662871",
                    "name": "Keqian Li"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "2150074296",
                    "name": "Shaunak Misra"
                },
                {
                    "authorId": "1809492",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "46622514",
                    "name": "Changwei Hu"
                },
                {
                    "authorId": "1840437995",
                    "name": "Manisha Verma"
                }
            ]
        },
        {
            "paperId": "4f58d052a8c4b4891e06fac41807863c1452185e",
            "title": "DeepGD: A Deep Learning Framework for Graph Drawing Using GNN",
            "abstract": "In the past decades, many graph drawing techniques have been proposed for generating aesthetically pleasing graph layouts. However, it remains a challenging task since different layout methods tend to highlight different characteristics of the graphs. Recently, studies on deep-learning-based graph drawing algorithms have emerged but they are often not generalizable to arbitrary graphs without retraining. In this article, we propose a Convolutional-Graph-Neural-Network-based deep learning framework, DeepGD, which can draw arbitrary graphs once trained. It attempts to generate layouts by compromising among multiple prespecified aesthetics considering a good graph layout usually complies with multiple aesthetics simultaneously. In order to balance the tradeoff, we propose two adaptive training strategies, which adjust the weight factor of each aesthetic dynamically during training. The quantitative and qualitative assessment of DeepGD demonstrates that it is capable of drawing arbitrary graphs effectively, while being flexible at accommodating different aesthetic criteria.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2144685915",
                    "name": "Xiaoqi Wang"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "50819900",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "39073212",
                    "name": "Han-Wei Shen"
                }
            ]
        },
        {
            "paperId": "c8b2780824c37140eeada3c451caf2d0a8b77a84",
            "title": "BERT-Beta: A Proactive Probabilistic Approach to Text Moderation",
            "abstract": "Text moderation for user generated content, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting. Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments. Beta regression is then introduced to do the probabilistic modeling, which is demonstrated to function well in comprehensive experiments. We also propose an explanation method to communicate the model decision clearly. Both propensity scoring and interpretation benefit text moderation in a novel manner. Finally, the proposed scaling mechanism for the linear model offers useful insights beyond this work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491233580",
                    "name": "Fei Tan"
                },
                {
                    "authorId": "50819900",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "46622514",
                    "name": "Changwei Hu"
                }
            ]
        },
        {
            "paperId": "3ae6450d4d1181b72d1a93aab832afec1b258c03",
            "title": "HABERTOR: An Efficient and Effective Deep Hatespeech Detector",
            "abstract": "We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. HABERTOR inherits BERT's architecture, but is different in four aspects: (i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; (ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; (iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our HABERTOR is 4~5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1% of the number of words. Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072580889",
                    "name": "T. Tran"
                },
                {
                    "authorId": "1809492",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "46622514",
                    "name": "Changwei Hu"
                },
                {
                    "authorId": "2055489471",
                    "name": "Kevin Yen"
                },
                {
                    "authorId": "1491233580",
                    "name": "Fei Tan"
                },
                {
                    "authorId": "2848353",
                    "name": "Kyumin Lee"
                },
                {
                    "authorId": "152662346",
                    "name": "Serim Park"
                }
            ]
        }
    ]
}