{
    "authorId": "7417844",
    "papers": [
        {
            "paperId": "44b3b9752e0644ce43b2af3557b39aa6ac3f02fe",
            "title": "Identifying Vulnerable Third-Party Java Libraries from Textual Descriptions of Vulnerabilities and Libraries",
            "abstract": "To address security vulnerabilities arising from third-party libraries, security researchers maintain databases monitoring and curating vulnerability reports. Application developers can identify vulnerable libraries by directly querying the databases with their used libraries. However, the querying results of vulnerable libraries are not reliable due to the incompleteness of vulnerability reports. Thus, current approaches model the task of identifying vulnerable libraries as a named-entity-recognition (NER) task or an extreme multi-label learning (XML) task. These approaches suffer from highly inaccurate results in identifying vulnerable libraries with complex and similar names, e.g., Java libraries. To address these limitations, in this paper, we propose VulLibMiner, the first to identify vulnerable libraries from textual descriptions of both vulnerabilities and libraries, together with VulLib, a Java vulnerability dataset with their affected libraries. VulLibMiner consists of a TF-IDF matcher to efficiently screen out a small set of candidate libraries and a BERT-FNN model to identify vulnerable libraries from these candidates effectively. We evaluate VulLibMiner using four state-of-the-art/practice approaches of identifying vulnerable libraries on both their dataset named VeraJava and our VulLib dataset. Our evaluation results show that VulLibMiner can effectively identify vulnerable libraries with an average F1 score of 0.657 while the state-of-the-art/practice approaches achieve only 0.521.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117180359",
                    "name": "Tianyu Chen"
                },
                {
                    "authorId": "2148946070",
                    "name": "Lin Li"
                },
                {
                    "authorId": "2223644462",
                    "name": "Bingjie Shan"
                },
                {
                    "authorId": "2084524",
                    "name": "Guangtai Liang"
                },
                {
                    "authorId": "2223703882",
                    "name": "Ding Li"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                },
                {
                    "authorId": "2144071699",
                    "name": "Tao Xie"
                }
            ]
        },
        {
            "paperId": "6ab8aca8f631f42760a86cc614dfd7208b3fe58e",
            "title": "Learning-based Widget Matching for Migrating GUI Test Cases",
            "abstract": "GUI test case migration is to migrate GUI test cases from a source app to a target app. The key of test case migration is widget matching. Recently, researchers have proposed various approaches by formulating widget matching as a matching task. However, since these matching approaches depend on static word embeddings without using contextual information to represent widgets and manually formulated matching functions, there are main limitations of these matching approaches when handling complex matching relations in apps. To address the limitations, we propose the first learning-based widget matching approach named TEMdroid ( TEst Migration) for test case migration. Unlike the existing approaches, TEMdroid uses BERT to capture contextual information and learns a matching model to match widgets. Additionally, to balance the significant imbalance between positive and negative samples in apps, we design a two-stage training strategy where we first train a hard-negative sample miner to mine hard-negative samples, and further train a matching model using positive samples and mined hard-negative samples. Our evaluation on 34 apps shows that TEM-droid is effective in event matching (i.e., widget matching and target event synthesis) and test case migration. For event matching, TEM-droid's Top1 accuracy is 76%, improving over 17% compared to baselines. For test case migration, TEMdroid's F1 score is 89%, also 7% improvement compared to the baseline approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110752255",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2089966950",
                    "name": "Bo Shen"
                },
                {
                    "authorId": "2038503437",
                    "name": "Dezhi Ran"
                },
                {
                    "authorId": null,
                    "name": "Jiaxin Zhang"
                },
                {
                    "authorId": "2145906426",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2304523047",
                    "name": "Yuchi Ma"
                },
                {
                    "authorId": "2084524",
                    "name": "Guangtai Liang"
                },
                {
                    "authorId": "2172444921",
                    "name": "Ying Li"
                },
                {
                    "authorId": "2057038049",
                    "name": "Tao Xie"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                }
            ]
        },
        {
            "paperId": "b092ac4a0bcfec38eae7819e40fd8c9e97f4cc7f",
            "title": "Can Programming Languages Boost Each Other via Instruction Tuning?",
            "abstract": "When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2134434187",
                    "name": "Daoguang Zan"
                },
                {
                    "authorId": "2151121986",
                    "name": "Ailun Yu"
                },
                {
                    "authorId": "2089966950",
                    "name": "Bo Shen"
                },
                {
                    "authorId": null,
                    "name": "Jiaxin Zhang"
                },
                {
                    "authorId": "2225208156",
                    "name": "Taihong Chen"
                },
                {
                    "authorId": "2225236858",
                    "name": "Bing Geng"
                },
                {
                    "authorId": "143876723",
                    "name": "B. Chen"
                },
                {
                    "authorId": "2225077064",
                    "name": "Jichuan Ji"
                },
                {
                    "authorId": "2236731239",
                    "name": "Yafen Yao"
                },
                {
                    "authorId": "2108097250",
                    "name": "Yongji Wang"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                }
            ]
        },
        {
            "paperId": "d07a15aba636f04b2d021779eee40f21d2fc295b",
            "title": "VulLibGen: Identifying Vulnerable Third-Party Libraries via Generative Pre-Trained Model",
            "abstract": "To avoid potential risks posed by vulnerabilities in third-party libraries, security researchers maintain vulnerability databases (e.g., NVD) containing vulnerability reports, each of which records the description of a vulnerability and the name list of libraries affected by the vulnerability (a.k.a. vulnerable libraries). However, recent studies on about 200,000 vulnerability reports in NVD show that 53.3% of these reports do not include the name list of vulnerable libraries, and 59.82% of the included name lists of vulnerable libraries are incomplete or incorrect. To address the preceding issue, in this paper, we propose the first generative approach named VulLibGen to generate the name list of vulnerable libraries (out of all the existing libraries) for the given vulnerability by utilizing recent enormous advances in Large Language Models (LLMs), in order to achieve high accuracy. VulLibGen takes only the description of a vulnerability as input and achieves high identification accuracy based on LLMs\u2019 prior knowledge of all the existing libraries. VulLibGen also includes the input augmentation technique to help identify zero-shot vulnerable libraries (those not occurring during training) and the post-processing technique to help address VulLibGen\u2019s hallucinations. We evaluate VulLib-Gen using three state-of-the-art/practice approaches (LightXML, Chronos, and VulLibMiner) that identify vulnerable libraries on an open-source dataset (VulLib). Our evaluation results show that VulLibGen can accurately identify vulnerable libraries with an average F1 score of 0.626 while the state-of-the-art/practice approaches achieve only 0.561. The post-processing technique helps VulLib-Gen achieve an average improvement of F1@1 by 9.3%. The input",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117180359",
                    "name": "Tianyu Chen"
                },
                {
                    "authorId": "2148946070",
                    "name": "Lin Li"
                },
                {
                    "authorId": "2230201293",
                    "name": "Liuchuan Zhu"
                },
                {
                    "authorId": "2118207557",
                    "name": "Zongyang Li"
                },
                {
                    "authorId": "2084524",
                    "name": "Guangtai Liang"
                },
                {
                    "authorId": "2223703882",
                    "name": "Ding Li"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                },
                {
                    "authorId": "2144071699",
                    "name": "Tao Xie"
                }
            ]
        },
        {
            "paperId": "06ea568379211ffa07d9605f66f26f6f736ea5e0",
            "title": "PanGu-Coder: Program Synthesis with Function-Level Language Modeling",
            "abstract": "We present PanGu-Coder, a pretrained decoder-only language model adopting the PanGu-Alpha architecture for text-to-code generation, i.e. the synthesis of programming language solutions given a natural language problem description. We train PanGu-Coder using a two-stage strategy: the first stage employs Causal Language Modelling (CLM) to pre-train on raw programming language data, while the second stage uses a combination of Causal Language Modelling and Masked Language Modelling (MLM) training objectives that focus on the downstream task of text-to-code generation and train on loosely curated pairs of natural language program definitions and code functions. Finally, we discuss PanGu-Coder-FT, which is fine-tuned on a combination of competitive programming problems and code with continuous integration tests. We evaluate PanGu-Coder with a focus on whether it generates functionally correct programs and demonstrate that it achieves equivalent or better performance than similarly sized models, such as CodeX, while attending a smaller context window and training on less data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48810605",
                    "name": "Fenia Christopoulou"
                },
                {
                    "authorId": "2346538",
                    "name": "Gerasimos Lampouras"
                },
                {
                    "authorId": "22168669",
                    "name": "Milan Gritta"
                },
                {
                    "authorId": "2044459",
                    "name": "Guchun Zhang"
                },
                {
                    "authorId": "121083081",
                    "name": "Yinpeng Guo"
                },
                {
                    "authorId": "2145415560",
                    "name": "Zhong-yi Li"
                },
                {
                    "authorId": "2145908386",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "6250798",
                    "name": "M. Xiao"
                },
                {
                    "authorId": "2089966950",
                    "name": "Bo Shen"
                },
                {
                    "authorId": "2155689998",
                    "name": "Lin Li"
                },
                {
                    "authorId": "145772431",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "14906343",
                    "name": "Li-yu Yan"
                },
                {
                    "authorId": "8157288",
                    "name": "Pingyi Zhou"
                },
                {
                    "authorId": "2153689079",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2304523047",
                    "name": "Yuchi Ma"
                },
                {
                    "authorId": "2676143",
                    "name": "Ignacio Iacobacci"
                },
                {
                    "authorId": "2136912252",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2084524",
                    "name": "Guangtai Liang"
                },
                {
                    "authorId": "2111613690",
                    "name": "Jia Wei"
                },
                {
                    "authorId": "2110310493",
                    "name": "Xin Jiang"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                },
                {
                    "authorId": "30738758",
                    "name": "Qun Liu"
                }
            ]
        },
        {
            "paperId": "bca9f9ef0b2fba6a42d865116577cf17fe2b3fa1",
            "title": "Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning",
            "abstract": "In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110752255",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2110049191",
                    "name": "Xing Hu"
                },
                {
                    "authorId": "2154591375",
                    "name": "Ge Li"
                },
                {
                    "authorId": "2172444921",
                    "name": "Ying Li"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                },
                {
                    "authorId": "2057038192",
                    "name": "Tao Xie"
                }
            ]
        },
        {
            "paperId": "558bd79e68893b0e8d31a3e91c294537d5beb501",
            "title": "IntelliMerge: a refactoring-aware software merging technique",
            "abstract": "In modern software development, developers rely on version control systems like Git to collaborate in the branch-based development workflow. One downside of this workflow is the conflicts occurred when merging contributions from different developers: these conflicts are tedious and error-prone to be correctly resolved, reducing the efficiency of collaboration and introducing potential bugs. The situation becomes even worse, with the popularity of refactorings in software development and evolution, because current merging tools (usually based on the text or tree structures of source code) are unaware of refactorings. In this paper, we present IntelliMerge, a graph-based refactoring-aware merging algorithm for Java programs. We explicitly enhance this algorithm's ability in detecting and resolving refactoring-related conflicts. Through the evaluation on 1,070 merge scenarios from 10 popular open-source Java projects, we show that IntelliMerge reduces the number of merge conflicts by 58.90% comparing with GitMerge (the prevalent unstructured merging tool) and 11.84% comparing with jFSTMerge (the state-of-the-art semi-structured merging tool) without sacrificing the auto-merging precision (88.48%) and recall (90.22%). Besides, the evaluation of performance shows that IntelliMerge takes 539 milliseconds to process one merge scenario on the median, which indicates its feasibility in real-world applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2089966950",
                    "name": "Bo Shen"
                },
                {
                    "authorId": "144713310",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "47941283",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2084524",
                    "name": "Guangtai Liang"
                },
                {
                    "authorId": "1700880",
                    "name": "Zhi Jin"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                }
            ]
        },
        {
            "paperId": "ba16f1b749283195aee72d37cfc967c9f5b9a790",
            "title": "Neural Detection of Semantic Code Clones Via Tree-Based Convolution",
            "abstract": "Code clones are similar code fragments that share the same semantics but may differ syntactically to various degrees. Detecting code clones helps reduce the cost of software maintenance and prevent faults. Various approaches of detecting code clones have been proposed over the last two decades, but few of them can detect semantic clones, i.e., code clones with dissimilar syntax. Recent research has attempted to adopt deep learning for detecting code clones, such as using tree-based LSTM over Abstract Syntax Tree (AST). However, it does not fully leverage the structural information of code fragments, thereby limiting its clone-detection capability. To fully unleash the power of deep learning for detecting code clones, we propose a new approach that uses tree-based convolution to detect semantic clones, by capturing both the structural information of a code fragment from its AST and lexical information from code tokens. Additionally, our approach addresses the limitation that source code has an unlimited vocabulary of tokens and models, and thus exploiting lexical information from code tokens is often ineffective when dealing with unseen tokens. Particularly, we propose a new embedding technique called position-aware character embedding (PACE), which essentially treats any token as a position-weighted combination of character one-hot embeddings. Our experimental results show that our approach substantially outperforms an existing state-of-the-art approach with an increase of 0.42 and 0.15 in F1-score on two popular code-clone benchmarks (OJClone and BigCloneBench), respectively, while being more computationally efficient. Our experimental results also show that PACE enables our approach to be substantially more effective when code clones contain unseen tokens.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110750045",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2055478258",
                    "name": "Wing Lam"
                },
                {
                    "authorId": "2145116654",
                    "name": "Long Chen"
                },
                {
                    "authorId": "1410115257",
                    "name": "Ge Li"
                },
                {
                    "authorId": "145906750",
                    "name": "Tao Xie"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                }
            ]
        },
        {
            "paperId": "0c24ad4d981cb6d0524add2d278a1d3b2fb8fadc",
            "title": "A Characterization Study of Repeated Bug Fixes",
            "abstract": "Programmers always fix bugs when maintaining software. Previous studies showed that developers apply repeated bug fixes\u2014similar or identical code changes\u2014to multiple locations. Based on the observation, researchers built tools to identify code locations in need of similar changes, or to suggest similar bug fixes to multiple code fragments. However, some fundamental research questions, such as what are the characteristics of repeated bug fixes, are still unexplored. In this paper, we present a comprehensive empirical study with 341,856 bug fixes from 3 open source projects to investigate repeated fixes in terms of their frequency, edit locations, and semantic meanings. Specifically, we sampled bug reports and retrieved the corresponding fixing patches in version history. Then we chopped patches into smaller fixes (edit fragments). Among all the fixes related to a bug, we identified repeated fixes using clone detection, and put a fix and its repeated ones into one repeated-fix group. With these groups, we characterized the edit locations, and investigated the common bug patterns as well as common fixes.Our study on Eclipse JDT, Mozilla Firefox, and LibreOffice shows that (1) 15-20% of bugs involved repeated fixes; (2) 73-92% of repeated-fix groups were applied purely to code clones; and (3) 39% of manually examined groups focused on bugs relevant to additions or deletions of whole if-structures. These results deepened our understanding of repeated fixes. They enabled us to assess the effectiveness of existing tools, and will further provide insights for future research directions in automatic software maintenance and program repair.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31857311",
                    "name": "Ruru Yue"
                },
                {
                    "authorId": "46720340",
                    "name": "Na Meng"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                }
            ]
        },
        {
            "paperId": "38b7502f4ff7f4261365ca838cf4026a79a48a30",
            "title": "Relationship-aware code search for JavaScript frameworks",
            "abstract": "JavaScript frameworks, such as jQuery, are widely used for developing web applications. To facilitate using these JavaScript frameworks to implement a feature (e.g., functionality), a large number of programmers often search for code snippets that implement the same or similar feature. However, existing code search approaches tend to be ineffective, without taking into account the fact that JavaScript code snippets often implement a feature based on various relationships (e.g., sequencing, condition, and callback relationships) among the invoked framework API methods. To address this issue, we present a novel Relationship-Aware Code Search (RACS) approach for finding code snippets that use JavaScript frameworks to implement a specific feature. In advance, RACS collects a large number of code snippets that use some JavaScript frameworks, mines API usage patterns from the collected code snippets, and represents the mined patterns with method call relationship (MCR) graphs, which capture framework API methods\u2019 signatures and their relationships. Given a natural language (NL) search query issued by a programmer, RACS conducts NL processing to automatically extract an action relationship (AR) graph, which consists of actions and their relationships inferred from the query. In this way, RACS reduces code search to the problem of graph search: finding similar MCR graphs for a given AR graph. We conduct evaluations against representative real-world jQuery questions posted on Stack Overflow, based on 308,294 code snippets collected from over 81,540 files on the Internet. The evaluation results show the effectiveness of RACS: the top 1 snippet produced by RACS matches the target code snippet for 46% questions, compared to only 4% achieved by a relationship-oblivious approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108262809",
                    "name": "Xuan Li"
                },
                {
                    "authorId": "2137431548",
                    "name": "Zerui Wang"
                },
                {
                    "authorId": "7417844",
                    "name": "Qianxiang Wang"
                },
                {
                    "authorId": "3024017",
                    "name": "Shoumeng Yan"
                },
                {
                    "authorId": "145906750",
                    "name": "Tao Xie"
                },
                {
                    "authorId": "145642974",
                    "name": "Hong Mei"
                }
            ]
        }
    ]
}