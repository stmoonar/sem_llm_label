{
    "authorId": "51043791",
    "papers": [
        {
            "paperId": "024d740fc6f1826b06154d37f2d074512b522c70",
            "title": "Deception in Reinforced Autonomous Agents",
            "abstract": "We explore the ability of large language model (LLM)-based agents to engage in subtle deception such as strategically phrasing and intentionally manipulating information to misguide and deceive other agents. This harmful behavior can be hard to detect, unlike blatant lying or unintentional hallucination. We build an adversarial testbed mimicking a legislative environment where two LLMs play opposing roles: a corporate *lobbyist* proposing amendments to bills that benefit a specific company while evading a *critic* trying to detect this deception. We use real-world legislative bills matched with potentially affected companies to ground these interactions. Our results show that LLM lobbyists initially exhibit limited deception against strong LLM critics which can be further improved through simple verbal reinforcement, significantly enhancing their deceptive capabilities, and increasing deception rates by up to 40 points. This highlights the risk of autonomous agents manipulating other agents through seemingly neutral language to attain self-serving goals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2175479084",
                    "name": "Atharvan Dogra"
                },
                {
                    "authorId": "2324576796",
                    "name": "Krishna Pillutla"
                },
                {
                    "authorId": null,
                    "name": "Ameet Deshpande"
                },
                {
                    "authorId": "145338991",
                    "name": "Ananya B. Sai"
                },
                {
                    "authorId": "2300174267",
                    "name": "John Nay"
                },
                {
                    "authorId": "2590556",
                    "name": "Tanmay Rajpurohit"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "2268206514",
                    "name": "Balaraman Ravindran"
                }
            ]
        },
        {
            "paperId": "70bde9a9f87c8fdc7c4a12b2530dfc70b829dcca",
            "title": "Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation",
            "abstract": "Recent developments in large language models (LLMs) offer a powerful foundation for developing natural language agents capable of varied, intricate tasks and, recently, also for assisting legislation and judicial decisions. With this, safety concerns about LLMs and autonomous agents built upon them are on the rise. Deception is one potential capability of AI agents of particular concern, which we refer to as an act or statement that misleads, hides the truth, or promotes a belief that is not true in its entirety or in part. \u201cCommon sense knowledge,\u201d \u201creasoning and planning\u201d abilities, \u201cautonomy,\u201d and the tendency to improve in goal-driven tasks raise the possibility of agents formulating harmful and deceptive methods when achieving goals is the only requirement with no moral or legal constraints. We move away from the conventional understanding of deception through straight-out lying, making objective selfish decisions, or giving false information, as seen in previous AI safety research. We target a specific category of deception achieved through obfuscation and equivocation. We broadly explain the two types of deception by analogizing them with the rabbit-out-of-hat 1 magic trick, where (i) the rabbit either comes out of a hidden trap door or (ii) (our focus) the audience is completely distracted to see the magician bring out the rabbit right in front of them using sleight of hand or misdirection. Our novel testbed framework displays the intrinsic deception capabilities of LLM agents in a goal-driven environment when the agents are directed to be deceptive in their natural language generations ( speech acts ) in a two-agent (a lobbyist and a critic ) adversarial dialogue system built upon the legislative task of \u201clobbying\u201d for a bill. Along the lines of a goal-driven environment, we show developing deceptive capacity through a reinforcement learning setup, building it around the theories of language philosophy and cognitive psychology. We find that the lobbyist agent increases its deceptive capabilities by \u223c 40% (relative) through subsequent reinforcement trials of adversarial interactions, and our deception detection mechanism shows a detection capability of up to 92%. Our results highlight potential issues in agent-human interaction, with agents potentially manipulating humans towards its programmed end-goal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2175479084",
                    "name": "Atharvan Dogra"
                },
                {
                    "authorId": "33341943",
                    "name": "A. Deshpande"
                },
                {
                    "authorId": "2300174267",
                    "name": "John Nay"
                },
                {
                    "authorId": "2590556",
                    "name": "Tanmay Rajpurohit"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "2268206514",
                    "name": "Balaraman Ravindran"
                }
            ]
        },
        {
            "paperId": "8a8dc735939f75d0329926fe3de817203a47cb2f",
            "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs",
            "abstract": "State-of-the-art large language models (LLMs) have become indispensable tools for various tasks. However, training LLMs to serve as effective assistants for humans requires careful consideration. A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework. In this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals, dedicating substantial focus to the core component of RLHF -- the reward model. Our study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. We characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275344575",
                    "name": "Shreyas Chaudhari"
                },
                {
                    "authorId": "2114841965",
                    "name": "Pranjal Aggarwal"
                },
                {
                    "authorId": "46258988",
                    "name": "Vishvak Murahari"
                },
                {
                    "authorId": "2590556",
                    "name": "Tanmay Rajpurohit"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "2135381714",
                    "name": "Karthik Narasimhan"
                },
                {
                    "authorId": "33341943",
                    "name": "A. Deshpande"
                },
                {
                    "authorId": "2296662415",
                    "name": "Bruno Castro da Silva"
                }
            ]
        },
        {
            "paperId": "d43286c135778fa6172caa51fe185ce4a09fb20d",
            "title": "PersonaGym: Evaluating Persona Agents and LLMs",
            "abstract": "Persona agents, which are LLM agents that act according to an assigned persona, have demonstrated impressive contextual response capabilities across various applications. These persona agents offer significant enhancements across diverse sectors, such as education, healthcare, and entertainment, where model developers can align agent responses to different user requirements thereby broadening the scope of agent applications. However, evaluating persona agent performance is incredibly challenging due to the complexity of assessing persona adherence in free-form interactions across various environments that are relevant to each persona agent. We introduce PersonaGym, the first dynamic evaluation framework for assessing persona agents, and PersonaScore, the first automated human-aligned metric grounded in decision theory for comprehensive large-scale evaluation of persona agents. Our evaluation of 6 open and closed-source LLMs, using a benchmark encompassing 200 personas and 10,000 questions, reveals significant opportunities for advancement in persona agent capabilities across state-of-the-art models. For example, Claude 3.5 Sonnet only has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite being a much more advanced model. Importantly, we find that increased model size and complexity do not necessarily imply enhanced persona agent capabilities thereby highlighting the pressing need for algorithmic and architectural invention towards faithful and performant persona agents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298040858",
                    "name": "Vinay Samuel"
                },
                {
                    "authorId": "2261285492",
                    "name": "Henry Peng Zou"
                },
                {
                    "authorId": "2261321156",
                    "name": "Yue Zhou"
                },
                {
                    "authorId": "2275344575",
                    "name": "Shreyas Chaudhari"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "2590556",
                    "name": "Tanmay Rajpurohit"
                },
                {
                    "authorId": "33341943",
                    "name": "A. Deshpande"
                },
                {
                    "authorId": "2135381714",
                    "name": "Karthik Narasimhan"
                },
                {
                    "authorId": "46258988",
                    "name": "Vishvak Murahari"
                }
            ]
        },
        {
            "paperId": "1b0e3360b3341fc411a6c7841173a8d78ac2ab43",
            "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
            "abstract": "Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152953535",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "2130460479",
                    "name": "Vaishnavi Shrivastava"
                },
                {
                    "authorId": "33341943",
                    "name": "A. Deshpande"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "2265493391",
                    "name": "Peter Clark"
                },
                {
                    "authorId": "48229640",
                    "name": "Ashish Sabharwal"
                },
                {
                    "authorId": "2236429",
                    "name": "Tushar Khot"
                }
            ]
        },
        {
            "paperId": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
            "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
            "abstract": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33341943",
                    "name": "A. Deshpande"
                },
                {
                    "authorId": "46258988",
                    "name": "Vishvak Murahari"
                },
                {
                    "authorId": "2590556",
                    "name": "Tanmay Rajpurohit"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "2135381714",
                    "name": "Karthik Narasimhan"
                }
            ]
        },
        {
            "paperId": "36a8e2185dfeb65259a6ca12dbcd80266319565f",
            "title": "GEO: Generative Engine Optimization",
            "abstract": "The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), can generate accurate and personalized responses, rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them using LLMs. While this shift significantly improves $\\textit{user}$ utility and $\\textit{generative search engine}$ traffic, it poses a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over $\\textit{when}$ and $\\textit{how}$ their content is displayed. With generative engines here to stay, we must ensure the creator economy is not disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), the first novel paradigm to aid content creators in improving their content visibility in generative engine responses through a flexible black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation by introducing GEO-bench, a large-scale benchmark of diverse user queries across multiple domains, along with relevant web sources to answer these queries. Through rigorous evaluation, we demonstrate that GEO can boost visibility by up to $40\\%$ in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific optimization methods. Our work opens a new frontier in information discovery systems, with profound implications for both developers of generative engines and content creators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114841965",
                    "name": "Pranjal Aggarwal"
                },
                {
                    "authorId": "46258988",
                    "name": "Vishvak Murahari"
                },
                {
                    "authorId": "2590556",
                    "name": "Tanmay Rajpurohit"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "2135381714",
                    "name": "Karthik Narasimhan"
                },
                {
                    "authorId": "33341943",
                    "name": "A. Deshpande"
                }
            ]
        },
        {
            "paperId": "6f8d5b9383a85ed6aa481968d0f8451bfbaca848",
            "title": "Distraction-free Embeddings for Robust VQA",
            "abstract": "The generation of effective latent representations and their subsequent refinement to incorporate precise information is an essential prerequisite for Vision-Language Understanding (VLU) tasks such as Video Question Answering (VQA). However, most existing methods for VLU focus on sparsely sampling or fine-graining the input information (e.g., sampling a sparse set of frames or text tokens), or adding external knowledge. We present a novel\"DRAX: Distraction Removal and Attended Cross-Alignment\"method to rid our cross-modal representations of distractors in the latent space. We do not exclusively confine the perception of any input information from various modalities but instead use an attention-guided distraction removal method to increase focus on task-relevant information in latent embeddings. DRAX also ensures semantic alignment of embeddings during cross-modal fusions. We evaluate our approach on a challenging benchmark (SUTD-TrafficQA dataset), testing the framework's abilities for feature and event queries, temporal relation understanding, forecasting, hypothesis, and causal analysis through extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2175479084",
                    "name": "Atharvan Dogra"
                },
                {
                    "authorId": "2237426399",
                    "name": "Deeksha Varshney"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "33341943",
                    "name": "A. Deshpande"
                },
                {
                    "authorId": "2237462066",
                    "name": "Neeraj Kumar"
                }
            ]
        },
        {
            "paperId": "86f163283adcab14fc7f2c021027b1139432b0af",
            "title": "Anthropomorphization of AI: Opportunities and Risks",
            "abstract": "Anthropomorphization is the tendency to attribute human-like traits to non-human entities. It is prevalent in many social contexts \u2013 children anthropomorphize toys, adults do so with brands, and it is a literary device. It is also a versatile tool in science, with behavioral psychology and evolutionary biology meticulously documenting its consequences. With widespread adoption of AI systems, and the push from stakeholders to make it human-like through alignment techniques, human voice, and pictorial avatars, the tendency for users to anthropomorphize it increases significantly. We take a dyadic approach to understanding this phenomenon with large language models (LLMs) by studying (1) the objective legal implications, as analyzed through the lens of the recent blueprint of AI bill of rights and the (2) subtle psychological aspects customization and anthropomorphization. We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction, with potential for manipulation and negative influence. With LLMs being hyper-personalized for vulnerable groups like children and patients among others, our work is a timely and important contribution. We propose a conservative strategy for the cautious use of anthropomorphization to improve trustworthiness of AI systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33341943",
                    "name": "A. Deshpande"
                },
                {
                    "authorId": "2590556",
                    "name": "Tanmay Rajpurohit"
                },
                {
                    "authorId": "2135381714",
                    "name": "Karthik Narasimhan"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                }
            ]
        },
        {
            "paperId": "9ceed28b20acfcc7e8e3ddee519eb11d7f2aef86",
            "title": "ProKnow: Process knowledge for safety constrained and explainable question generation for mental health diagnostic assistance",
            "abstract": "Virtual Mental Health Assistants (VMHAs) are utilized in health care to provide patient services such as counseling and suggestive care. They are not used for patient diagnostic assistance because they cannot adhere to safety constraints and specialized clinical process knowledge (ProKnow) used to obtain clinical diagnoses. In this work, we define ProKnow as an ordered set of information that maps to evidence-based guidelines or categories of conceptual understanding to experts in a domain. We also introduce a new dataset of diagnostic conversations guided by safety constraints and ProKnow that healthcare professionals use (ProKnow-data). We develop a method for natural language question generation (NLG) that collects diagnostic information from the patient interactively (ProKnow-algo). We demonstrate the limitations of using state-of-the-art large-scale language models (LMs) on this dataset. ProKnow-algo incorporates the process knowledge through explicitly modeling safety, knowledge capture, and explainability. As computational metrics for evaluation do not directly translate to clinical settings, we involve expert clinicians in designing evaluation metrics that test four properties: safety, logical coherence, and knowledge capture for explainability while minimizing the standard cross entropy loss to preserve distribution semantics-based similarity to the ground truth. LMs with ProKnow-algo generated 89% safer questions in the depression and anxiety domain (tested property: safety). Further, without ProKnow-algo generations question did not adhere to clinical process knowledge in ProKnow-data (tested property: knowledge capture). In comparison, ProKnow-algo-based generations yield a 96% reduction in our metrics to measure knowledge capture. The explainability of the generated question is assessed by computing similarity with concepts in depression and anxiety knowledge bases. Overall, irrespective of the type of LMs, ProKnow-algo achieved an averaged 82% improvement over simple pre-trained LMs on safety, explainability, and process-guided question generation. For reproducibility, we will make ProKnow-data and the code repository of ProKnow-algo publicly available upon acceptance.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "1491238594",
                    "name": "Manas Gaur"
                },
                {
                    "authorId": "2199859429",
                    "name": "Misagh Soltani"
                },
                {
                    "authorId": "9460529",
                    "name": "Vipula Rawte"
                },
                {
                    "authorId": "51043791",
                    "name": "A. Kalyan"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        }
    ]
}