{
    "authorId": "3395852",
    "papers": [
        {
            "paperId": "5d8fbdf6f21215dc34ba2fad0979bc4639a65487",
            "title": "ManiFPT: Defining and Analyzing Fingerprints of Generative Models",
            "abstract": "Recent works have shown that generative models leave traces of their underlying generative process on the generated samples, broadly referred to as fingerprints of a generative model, and have studied their utility in detecting synthetic images from real ones. However, the extend to which these fingerprints can distinguish between various types of synthetic image and help identify the underlying generative process remain under-explored. In particular, the very definition of a fingerprint remains unclear, to our knowledge. To that end, in this work, we formalize the definition of artifact and fingerprint in generative models, propose an algorithm for computing them in practice, and finally study its effectiveness in distinguishing a large array of different generative models. We find that using our proposed definition can significantly improve the performance on the task of identifying the underlying generative process from samples (model attribution) compared to existing methods. Additionally, we study the structure of the fingerprints, and observe that it is very predictive of the effect of different design choices on the generative process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145285306",
                    "name": "Hae Jin Song"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "2066250992",
                    "name": "Wael AbdAlmageed"
                }
            ]
        },
        {
            "paperId": "7133b4f840abf70c23782a8cb405053880406b5a",
            "title": "Exploring Perceptual Limitation of Multimodal Large Language Models",
            "abstract": "Multimodal Large Language Models (MLLMs) have recently shown remarkable perceptual capability in answering visual questions, however, little is known about the limits of their perception. In particular, while prior works have provided anecdotal evidence of MLLMs' sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively. In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering questions about small objects in images. Next, we identify four independent factors that can contribute to this limitation -- object quality, size, distractors, and location -- and conduct controlled intervention studies to measure the effect of each factor on MLLMs' perception. In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions. More surprisingly, we find that the location of the object in the image and the presence of visual distractors can also significantly reduce MLLMs' question answering accuracy. Our study provides a better understanding of the perceptual limitation of MLLMs and contributes new evaluation protocols for analyzing the perception of future MLLMs. To facilitate further investigations, we release our code and data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "92837695",
                    "name": "Jinyi Hu"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "2282025206",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "7cc20edc6c7de242302dcb8c2746fbc162218954",
            "title": "An Investigation on The Position Encoding in Vision-Based Dynamics Prediction",
            "abstract": "Despite the success of vision-based dynamics prediction models, which predict object states by utilizing RGB images and simple object descriptions, they were challenged by environment misalignments. Although the literature has demonstrated that unifying visual domains with both environment context and object abstract, such as semantic segmentation and bounding boxes, can effectively mitigate the visual domain misalignment challenge, discussions were focused on the abstract of environment context, and the insight of using bounding box as the object abstract is under-explored. Furthermore, we notice that, as empirical results shown in the literature, even when the visual appearance of objects is removed, object bounding boxes alone, instead of being directly fed into the network, can indirectly provide sufficient position information via the Region of Interest Pooling operation for dynamics prediction. However, previous literature overlooked discussions regarding how such position information is implicitly encoded in the dynamics prediction model. Thus, in this paper, we provide detailed studies to investigate the process and necessary conditions for encoding position information via using the bounding box as the object abstract into output features. Furthermore, we study the limitation of solely using object abstracts, such that the dynamics prediction performance will be jeopardized when the environment context varies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168996315",
                    "name": "Jiageng Zhu"
                },
                {
                    "authorId": "2029877863",
                    "name": "Hanchen Xie"
                },
                {
                    "authorId": "2257096667",
                    "name": "Jiazhi Li"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "2066250992",
                    "name": "Wael AbdAlmageed"
                }
            ]
        },
        {
            "paperId": "d9c2f179a831060935100fc4e1c2e3da901d9eb5",
            "title": "Look, Learn and Leverage (L3): Mitigating Visual-Domain Shift and Discovering Intrinsic Relations via Symbolic Alignment",
            "abstract": "Modern deep learning models have demonstrated outstanding performance on discovering the underlying mechanisms when both visual appearance and intrinsic relations (e.g., causal structure) data are sufficient, such as Disentangled Representation Learning (DRL), Causal Representation Learning (CRL) and Visual Question Answering (VQA) methods. However, generalization ability of these models is challenged when the visual domain shifts and the relations data is absent during finetuning. To address this challenge, we propose a novel learning framework, Look, Learn and Leverage (L$^3$), which decomposes the learning process into three distinct phases and systematically utilize the class-agnostic segmentation masks as the common symbolic space to align visual domains. Thus, a relations discovery model can be trained on the source domain, and when the visual domain shifts and the intrinsic relations are absent, the pretrained relations discovery model can be directly reused and maintain a satisfactory performance. Extensive performance evaluations are conducted on three different tasks: DRL, CRL and VQA, and show outstanding results on all three tasks, which reveals the advantages of L$^3$.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2029877863",
                    "name": "Hanchen Xie"
                },
                {
                    "authorId": "2168996315",
                    "name": "Jiageng Zhu"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "2257096667",
                    "name": "Jiazhi Li"
                },
                {
                    "authorId": "2066250992",
                    "name": "Wael AbdAlmageed"
                }
            ]
        },
        {
            "paperId": "29b3ce4de9dd9d784ca1d876957950f4b2d3796a",
            "title": "Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs",
            "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                }
            ]
        },
        {
            "paperId": "3fe9dc931cb261ce76ae9877fd600c35122107c0",
            "title": "SABAF: Removing Strong Attribute Bias from Neural Networks with Adversarial Filtering",
            "abstract": "Ensuring a neural network is not relying on protected attributes (e.g., race, sex, age) for prediction is crucial in advancing fair and trustworthy AI. While several promising methods for removing attribute bias in neural networks have been proposed, their limitations remain under-explored. To that end, in this work, we mathematically and empirically reveal the limitation of existing attribute bias removal methods in presence of strong bias and propose a new method that can mitigate this limitation. Specifically, we first derive a general non-vacuous information-theoretical upper bound on the performance of any attribute bias removal method in terms of the bias strength, revealing that they are effective only when the inherent bias in the dataset is relatively weak. Next, we derive a necessary condition for the existence of any method that can remove attribute bias regardless of the bias strength. Inspired by this condition, we then propose a new method using an adversarial objective that directly filters out protected attributes in the input space while maximally preserving all other attributes, without requiring any specific target label. The proposed method achieves state-of-the-art performance in both strong and moderate bias settings. We provide extensive experiments on synthetic, image, and census datasets, to verify the derived theoretical bound and its consequences in practice, and evaluate the effectiveness of the proposed method in removing strong attribute bias.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257096667",
                    "name": "Jiazhi Li"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "2168996315",
                    "name": "Jiageng Zhu"
                },
                {
                    "authorId": "2029877863",
                    "name": "Hanchen Xie"
                },
                {
                    "authorId": "2257051984",
                    "name": "Mohamed E. Hussein"
                },
                {
                    "authorId": "2066250992",
                    "name": "Wael AbdAlmageed"
                }
            ]
        },
        {
            "paperId": "5c07fe90fb8e1fe4bee7ff40190c3f1e83667310",
            "title": "Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models",
            "abstract": "Multimodal Large Language Models (LLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) \u2013 a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether multimodal LLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose three automatic visual cropping methods as inference time mechanisms to improve the zero-shot performance of multimodal LLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that multimodal LLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. Our code and data are publicly available. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261394194",
                    "name": "Jiarui Zhang"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "1416534710",
                    "name": "P. Chhikara"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                }
            ]
        },
        {
            "paperId": "8607aefd66bcb4e62943a69384850c8f633efd8c",
            "title": "Shadow Datasets, New challenging datasets for Causal Representation Learning",
            "abstract": "Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168996315",
                    "name": "Jiageng Zhu"
                },
                {
                    "authorId": "2029877863",
                    "name": "Hanchen Xie"
                },
                {
                    "authorId": "2229075816",
                    "name": "Jianhua Wu"
                },
                {
                    "authorId": "2109589200",
                    "name": "Jiazhi Li"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "1572148863",
                    "name": "Mohamed E. Hussein"
                },
                {
                    "authorId": "2066250992",
                    "name": "Wael AbdAlmageed"
                }
            ]
        },
        {
            "paperId": "9547177682273eba1c3779a4fce21117b782de18",
            "title": "Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions",
            "abstract": "Precision and Recall are two prominent metrics of generative performance, which were proposed to separately measure the fidelity and diversity of generative models. Given their central role in comparing and improving generative models, understanding their limitations are crucially important. To that end, in this work, we identify a critical flaw in the common approximation of these metrics using k-nearest-neighbors, namely, that the very interpretations of fidelity and diversity that are assigned to Precision and Recall can fail in high dimensions, resulting in very misleading conclusions. Specifically, we empirically and theoretically show that as the number of dimensions grows, two model distributions with supports at equal point-wise distance from the support of the real distribution, can have vastly different Precision and Recall regardless of their respective distributions, hence an emergent asymmetry in high dimensions. Based on our theoretical insights, we then provide simple yet effective modifications to these metrics to construct symmetric metrics regardless of the number of dimensions. Finally, we provide experiments on real-world datasets to illustrate that the identified flaw is not merely a pathological case, and that our proposed metrics are effective in alleviating its impact.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "17806729",
                    "name": "Wael AbdAlmageed"
                }
            ]
        },
        {
            "paperId": "c6cbb266a8e4ae2b5a6435e286f13718a8117073",
            "title": "Unsupervised Multimodal Deepfake Detection Using Intra- and Cross-Modal Inconsistencies",
            "abstract": "Deepfake videos present an increasing threat to society with potentially negative impact on criminal justice, democracy, and personal safety and privacy. Meanwhile, detecting deepfakes, at scale, remains a very challenging task that often requires labeled training data from existing deepfake generation methods. Further, even the most accurate supervised deepfake detection methods do not generalize to deepfakes generated using new generation methods. In this paper, we propose a novel unsupervised method for detecting deepfake videos by directly identifying intra-modal and cross-modal inconsistency between video segments. The fundamental hypothesis behind the proposed detection method is that motion or identity inconsistencies are inevitable in deepfake videos. We will mathematically and empirically support this hypothesis, and then proceed to constructing our method grounded in our theoretical analysis. Our proposed method outperforms prior state-of-the-art unsupervised deepfake detection methods on the challenging FakeAVCeleb dataset, and also has several additional advantages: it is scalable because it does not require pristine (real) samples for each identity during inference and therefore can apply to arbitrarily many identities, generalizable because it is trained only on real videos and therefore does not rely on a particular deepfake method, reliable because it does not rely on any likelihood estimation in high dimensions, and explainable because it can pinpoint the exact location of modality inconsistencies which are then verifiable by a human expert.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167018629",
                    "name": "Mulin Tian"
                },
                {
                    "authorId": "3395852",
                    "name": "Mahyar Khayatkhoei"
                },
                {
                    "authorId": "51502677",
                    "name": "Joe Mathai"
                },
                {
                    "authorId": "2066250992",
                    "name": "Wael AbdAlmageed"
                }
            ]
        }
    ]
}