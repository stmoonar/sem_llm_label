{
    "authorId": "1880394",
    "papers": [
        {
            "paperId": "260282639dce1984c1a065aa4feae41cea0fed06",
            "title": "Bilingual Adaptation of Monolingual Foundation Models",
            "abstract": "We present an efficient method for adapting a monolingual Large Language Model (LLM) to another language, addressing challenges of catastrophic forgetting and tokenizer limitations. We focus this study on adapting Llama 2 to Arabic. Our two-stage approach begins with expanding the vocabulary and training only the embeddings matrix, followed by full model continual pre-training on a bilingual corpus. By continually pre-training on a mix of Arabic and English corpora, the model retains its proficiency in English while acquiring capabilities in Arabic. Our approach results in significant improvements in Arabic and slight enhancements in English, demonstrating cost-effective cross-lingual transfer. We perform ablations on embedding initialization techniques, data mix ratios, and learning rates and release a detailed training recipe. To demonstrate generalizability of this approach we also adapted Llama 3 8B to Arabic and Llama 2 13B to Hindi.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238627171",
                    "name": "Gurpreet Gosal"
                },
                {
                    "authorId": "2312005159",
                    "name": "Yishi Xu"
                },
                {
                    "authorId": "2311889065",
                    "name": "Gokul Ramakrishnan"
                },
                {
                    "authorId": "2311894591",
                    "name": "Rituraj Joshi"
                },
                {
                    "authorId": "2311892291",
                    "name": "Avraham Sheinin"
                },
                {
                    "authorId": "2311991181",
                    "name": "Zhiming Chen"
                },
                {
                    "authorId": "2311891083",
                    "name": "Biswajit Mishra"
                },
                {
                    "authorId": "2243228182",
                    "name": "Natalia Vassilieva"
                },
                {
                    "authorId": "3130228",
                    "name": "Joel Hestness"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "2311888768",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "2087720002",
                    "name": "Bokang Jia"
                },
                {
                    "authorId": "2311887646",
                    "name": "Onkar Pandit"
                },
                {
                    "authorId": "2235818050",
                    "name": "Satheesh Katipomu"
                },
                {
                    "authorId": "2203791403",
                    "name": "Samta Kamboj"
                },
                {
                    "authorId": "2313594182",
                    "name": "Samujjwal Ghosh"
                },
                {
                    "authorId": "2235794681",
                    "name": "Rahul Pal"
                },
                {
                    "authorId": "2311889115",
                    "name": "Parvez Mullah"
                },
                {
                    "authorId": "2311887505",
                    "name": "Soundar Doraiswamy"
                },
                {
                    "authorId": "2311887344",
                    "name": "Mohamed El Karim Chami"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "5c577988ccebfea96de86678d04fd94fad367d2e",
            "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
            "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "3422905",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "2087720002",
                    "name": "Bokang Jia"
                },
                {
                    "authorId": "2235818050",
                    "name": "Satheesh Katipomu"
                },
                {
                    "authorId": "49404498",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2203791403",
                    "name": "Samta Kamboj"
                },
                {
                    "authorId": "22171629",
                    "name": "O. Pandit"
                },
                {
                    "authorId": "2235794681",
                    "name": "Rahul Pal"
                },
                {
                    "authorId": "2076256459",
                    "name": "Lalit Pradhan"
                },
                {
                    "authorId": "123838298",
                    "name": "Zainul Mujahid"
                },
                {
                    "authorId": "1380273855",
                    "name": "Massa Baali"
                },
                {
                    "authorId": "2110982198",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "100468503",
                    "name": "Zhengzhong Liu"
                },
                {
                    "authorId": "2235826325",
                    "name": "Andy Hock"
                },
                {
                    "authorId": "77917645",
                    "name": "Andrew Feldman"
                },
                {
                    "authorId": "2235945609",
                    "name": "Jonathan Lee"
                },
                {
                    "authorId": "2064974174",
                    "name": "A. Jackson"
                },
                {
                    "authorId": "1683562",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "145465286",
                    "name": "Timothy Baldwin"
                },
                {
                    "authorId": "2064963077",
                    "name": "Eric P. Xing"
                }
            ]
        },
        {
            "paperId": "038103632b24619818b159f5ca37b848744817db",
            "title": "DENTRA: Denoising and Translation Pre-training for Multilingual Machine Translation",
            "abstract": "In this paper, we describe our submission to the WMT-2022: Large-Scale Machine Translation Evaluation for African Languages under the Constrained Translation track. We introduce DENTRA, a novel pre-training strategy for a multilingual sequence-to-sequence transformer model. DENTRA pre-training combines denoising and translation objectives to incorporate both monolingual and bitext corpora in 24 African, English, and French languages. To evaluate the quality of DENTRA, we fine-tuned it with two multilingual machine translation configurations, one-to-many and many-to-one. In both pre-training and fine-tuning, we employ only the datasets provided by the organizers. We compare DENTRA against a strong baseline, M2M-100, in different African multilingual machine translation scenarios and show gains in 3 out of 4 subtasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2203791403",
                    "name": "Samta Kamboj"
                },
                {
                    "authorId": "3422905",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                }
            ]
        },
        {
            "paperId": "7184b3714fd81c26143a55f0cc2eb29c31dd46a6",
            "title": "Relation Extraction with Self-determined Graph Convolutional Network",
            "abstract": "Relation Extraction is a way of obtaining the semantic relationship between entities in text. The state-of-the-art methods use linguistic tools to build a graph for the text in which the entities appear and then a Graph Convolutional Network (GCN) is employed to encode the pre-built graphs. Although their performance is promising, the reliance on linguistic tools results in a non end-to-end process. In this work, we propose a novel model, the Self-determined Graph Convolutional Network (SGCN), which determines a weighted graph using a self-attention mechanism, rather using any linguistic tool. Then, the self-determined graph is encoded using a GCN. We test our model on the TACRED dataset and achieve the state-of-the-art result. Our experiments show that SGCN outperforms the traditional GCN, which uses dependency parsing tools to build the graph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3422905",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "2111218448",
                    "name": "Derek Thomas"
                },
                {
                    "authorId": "50493332",
                    "name": "Billy Chiu"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "1754924481",
                    "name": "Mohammady Mahdy"
                }
            ]
        },
        {
            "paperId": "80b13b6500983667c509cb52a470b9946bf368c5",
            "title": "Attending to Inter-sentential Features in Neural Text Classification",
            "abstract": "Text classification requires a deep understanding of the linguistic features in text; in particular, the intra-sentential (local) and inter-sentential features (global). Models that operate on word sequences have been successfully used to capture the local features, yet they are not effective in capturing the global features in long-text. We investigate graph-level extensions to such models and propose a novel architecture for combining alternative text features. It uses an attention mechanism to dynamically decide how much information to use from a sequence- or graph-level component. We evaluated different architectures on a range of text classification datasets, and graph-level extensions were found to improve performance on most benchmarks. In addition, the attention-based architecture, as adaptively-learned from the data, outperforms the generic and fixed-value concatenation ones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50493332",
                    "name": "Billy Chiu"
                },
                {
                    "authorId": "3422905",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "2111218448",
                    "name": "Derek Thomas"
                },
                {
                    "authorId": "1754924481",
                    "name": "Mohammady Mahdy"
                }
            ]
        },
        {
            "paperId": "dd9bab9a91501c1ddc31c2bbf9ed7a3d5cf43edb",
            "title": "Finding Minimum Connected Subgraphs With Ontology Exploration on Large RDF Data",
            "abstract": "In this paper, we study the following problem: given a knowledge graph (KG) and a set of input vertices (representing concepts or entities) and edge labels, we aim to find the smallest connected subgraphs containing all of the inputs. This problem plays a key role in KG-based search engines and natural language question answering systems, and it is a natural extension of the Steiner tree problem, which is known to be NP-hard. We present RECON, a system for finding approximate answers. RECON aims at achieving high accuracy with instantaneous response (i.e., sub-second/millisecond delay) over KGs with hundreds of millions edges without resorting to expensive computational resources. Furthermore, when no answer exists due to disconnection between concepts and entities, RECON refines the input to a semantically similar one based on the ontology, and attempts to find answers with respect to the refined input. We conduct a comprehensive experimental evaluation of RECON. In particular we compare it with five existing approaches for finding approximate Steiner trees. Our experiments on four large real and synthetic KGs show that RECON significantly outperforms its competitors and incurs a much smaller memory footprint.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3464627",
                    "name": "Xiangnan Ren"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "2256826197",
                    "name": "Xuguang Ren"
                },
                {
                    "authorId": "2343882",
                    "name": "Junhu Wang"
                },
                {
                    "authorId": "2141773",
                    "name": "Olivier Cur\u00e9"
                }
            ]
        },
        {
            "paperId": "e78b1c6cf0fbe935f12adf3a5ce3cde629252316",
            "title": "Autoencoding Keyword Correlation Graph for Document Clustering",
            "abstract": "Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global). Existing representation learning models do not fully capture these features. To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph. The graph is constructed with topical keywords as nodes and multiple local and global features as edges. A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them. Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes. Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50493332",
                    "name": "Billy Chiu"
                },
                {
                    "authorId": "3422905",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "2111218448",
                    "name": "Derek Thomas"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "1754924481",
                    "name": "Mohammady Mahdy"
                }
            ]
        },
        {
            "paperId": "28b6eace7e71553a406124ea6938819f6642ba1b",
            "title": "Reachability in Large Graphs Using Bloom Filters",
            "abstract": "Reachability queries are a fundamental graph operation with applications in several domains. There has been extensive research over several decades on answering reachability queries efficiently using sophisticated index structures. However, most of these methods are built for static graphs. For graphs that are updated very frequently and are massive in size, maintaining such index structures is often infeasible due to a large memory footprint and extremely slow updates. In this paper, we introduce a technique to compute reachability queries for very large and highly dynamic graphs that minimizes the memory footprint and update time. In particular, we enable a previously proposed, index-free, approximate method for reachability called ARROW on a compact graph representation called Bloom graphs. Bloom graphs use collections of the well known summary data structure called the Bloom filter to store the edges of the graph. In our experimental evaluation with real world graph datasets with up to millions of nodes and edges, we show that using ARROW with a Bloom graph achieves memory savings of up to 50%, while having accuracy close to 100% for all graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150029298",
                    "name": "Arkaprava Saha"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "1680366",
                    "name": "Maya Ramanath"
                }
            ]
        },
        {
            "paperId": "7fb41fcf27940db482b1130b48de8ca10974019c",
            "title": "ARROW: Approximating Reachability Using Random Walks Over Web-Scale Graphs",
            "abstract": "Efficiently answering reachability queries on a directed graph is a fundamental problem and many solutions - theoretical and practical - have been proposed. A common strategy to make reachability query processing efficient, accurate and scalable is to precompute indexes on the graph. However this often becomes impractical, particularly when dealing with large graphs that are highly dynamic or when queries have additional constraints known only at the time of querying. In the former case, indexes become stale very quickly and keeping them up to date at the same speed as changes to the graph is untenable. For the latter setting, currently proposed indexes are often quite bulky and are highly customized to handle only a small class of constraints. In this paper, we propose a first practical attempt to address these issues by abandoning the traditional indexing approach altogether and operating directly on the graph as it evolves. Our approach, called ARROW, uses random walks to efficiently approximate reachability between vertices, building on ideas that have been prevalent in the theory community but ignored by practitioners. Not only is ARROW well suited for highly dynamic settings - as it is index-free, but it can also be easily adapted to handle many different forms of ad-hoc constraints while being competitive with custom-made index structures. In this paper, we show that ARROW, despite its simplicity, is near-accurate and scales to graphs with tens of millions of vertices and hundreds of millions of edges. We present extensive empirical evidence to illustrate these advantages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "1799402",
                    "name": "A. Bagchi"
                },
                {
                    "authorId": "1680366",
                    "name": "Maya Ramanath"
                },
                {
                    "authorId": "1751538",
                    "name": "Srikanta J. Bedathur"
                }
            ]
        },
        {
            "paperId": "4e03913df219ea26e677ad28af34006dc5e14624",
            "title": "Sampling and Reconstruction Using Bloom Filters",
            "abstract": "In this paper, we address the problem of sampling from a set and reconstructing a set stored as a Bloom filter. To the best of our knowledge our work is the first to address this question. We introduce a novel hierarchical data structure called BloomSampleTree that helps us design efficient algorithms to extract an almost uniform sample from the set stored in a Bloom filter and also allows us to reconstruct the set efficiently. In the case where the hash functions used in the Bloom filter implementation are partially invertible, in the sense that it is easy to calculate the set of elements that map to a particular hash value, we propose a second, more space-efficient method called HashInvert for the reconstruction. We study the properties of these two methods both analytically as well as experimentally. We provide bounds on run times for both methods and sample quality for the BloomSampleTree based algorithm, and show through an extensive experimental evaluation that our methods are efficient and effective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "1799402",
                    "name": "A. Bagchi"
                },
                {
                    "authorId": "1751538",
                    "name": "Srikanta J. Bedathur"
                },
                {
                    "authorId": "1680366",
                    "name": "Maya Ramanath"
                }
            ]
        }
    ]
}