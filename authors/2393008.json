{
    "authorId": "2393008",
    "papers": [
        {
            "paperId": "14523a3b267ff13e6548f90982045875a3dd6822",
            "title": "Beyond Facts: 4th International Workshop on Computational Methods for Online Discourse Analysis",
            "abstract": "Expressing opinions and interacting with others on the Web has led to the production of an abundance of online discourse data, such as claims and viewpoints on controversial topics, their sources and contexts (events, entities). This data constitutes a valuable source of insights for studies into misinformation spread, bias reinforcement, echo chambers or political agenda setting. Computational methods, mostly from the field of NLP, have emerged that tackle a wide range of tasks in this context, including argument and opinion mining, claim detection, checkworthiness detection, stance detection or fact verification. However, computational models require robust definitions of classes and concepts under investigation. Thus, these computational tasks require a strong interdisciplinary and epistemological foundation, specifically with respect to the underlying definitions of key concepts such as claims, arguments, stances, check-worthiness or veracity. This requires a highly interdisciplinary approach combining expertise from fields such as communication studies, computational linguistics and computer science. As opposed to facts, claims are inherently more complex. Their interpretation strongly depends on the context and a variety of intentional or unintended meanings, where terminology and conceptual understandings strongly diverge across communities. From a computational perspective, in order to address this complexity, the synergy of multiple approaches, coming both from symbolic (knowledge representation) and statistical AI seem to be promising to tackle such challenges. This workshop aims at strengthening the relations between these communities, providing a forum for shared works on the modeling, extraction and analysis of discourse on the Web. It will address the need for a shared understanding and structured knowledge about discourse data in order to enable machine-interpretation, discoverability and reuse, in support of scientific or journalistic studies into the analysis of societal debates on the Web. Beyond research into information and knowledge extraction, data consolidation and modeling for knowledge graphs building, the workshop targets communities focusing on the analysis of online discourse, relying on methods from machine learning, natural language processing, large language models and Web data mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301211436",
                    "name": "Konstantin Todorov"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "2206084622",
                    "name": "Stefan Dietze"
                },
                {
                    "authorId": "2274951751",
                    "name": "Dimitar Dimitrov"
                }
            ]
        },
        {
            "paperId": "72e3c7fa5cb39e3846dbc1020e62e24a07286d1a",
            "title": "BeyondFacts\u201923: 3rd International Workshop on Knowledge Graphs for Online Discourse Analysis",
            "abstract": "WORKSHOP SCOPE Expressing opinions and interacting with others on the Web has led to the production of an abundance of online discourse data, such as claims and viewpoints on controversial topics, their sources and contexts (events, entities). This data constitutes a valuable source of insights for studies into misand dis-information spread, bias reinforcement, echo chambers, or political agenda setting. While knowledge graphs (KGs) promise to provide the key to a Web of structured information, they are mainly focused on facts without keeping track of the diversity, connection or temporal evolution of online discourse. As opposed to facts, claims are inherently more complex. Their interpretation strongly depends on the context and a variety of intentional or unintended meanings, where terminology and conceptual understanding strongly diverge across communities from computational social science and journalism, to argumentation mining, stance detection, or computational fact-checking. This workshop aims at strengthening the relations between these communities, providing a forum for shared works on the modeling, extraction and analysis of discourse on the Web. It will address the need for a shared understanding and structured knowledge about discourse data in order to enable machine-interpretation, discoverability and reuse, in support of scientifc or journalistic studies into the analysis of societal debates on the Web. Beyond research into information and knowledge extraction, data consolidation and modeling for KG building, the workshop",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2594085",
                    "name": "Konstantin Todorov"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "2105813793",
                    "name": "Dimitar I. Dimitrov"
                },
                {
                    "authorId": "3081683",
                    "name": "S. Dietze"
                }
            ]
        },
        {
            "paperId": "7ac83ebe36d492aadbf0affe355e6e8a3d861fac",
            "title": "The SeaLiT Ontology \u2013 An Extension of CIDOC-CRM for the Modeling and Integration of Maritime History Information",
            "abstract": "We describe the construction and use of the SeaLiT Ontology, an extension of the ISO standard CIDOC-CRM for the modelling and integration of maritime history information. The ontology has been developed gradually, following a bottom-up approach that required the analysis of large amounts of real primary data (archival material) as well as knowledge and validation by domain experts (maritime historians). We present the specification of the ontology, RDFS and OWL implementations, as well as knowledge graphs that make use of this data model for integrating information originating from a large and diverse set of archival documents, such as crew lists, sailors registers, naval ship registers, and payrolls. We also describe an application that operates over these knowledge graphs and which supports historians in exploring and quantitatively analysing the integrated data through a user-friendly interface. Finally, we discuss aspects related to the use, evolution, and sustainability of the ontology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "41203240",
                    "name": "A. Kritsotaki"
                },
                {
                    "authorId": "38587181",
                    "name": "M. Doerr"
                }
            ]
        },
        {
            "paperId": "b46ffaa9b1f0c8142aaef6b9c7159617b4a70e01",
            "title": "Truth or Dare: Investigating Claims Truthfulness with ClaimsKG",
            "abstract": "Searching and exploring online information is fundamental for our society. However, it is common to find inaccurate information on the Internet, that can quickly spread and be hard to identify. Fortunately, today, many fact-checking sources verify online information to provide online users with a means to recognize its truthfulness. These sources use different languages and scoring systems, which makes fact validation challenging and time-consuming. To address this issue, we propose a new release of ClaimsKG, a knowledge graph of about 59,580 claims, which covers 13 different fact-checking sources and provides a structured way to retrieve verified online claims. ClaimsKG is built using a pipeline that makes use of entity linking and disambiguation tools to fetch entities from DBpedia and an ad-hoc scoring normalization system. ClaimsKG is used as a showcase to provide the public with interesting and verified information about events of our times.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053142903",
                    "name": "Susmita Gangopadhyay"
                },
                {
                    "authorId": "2143348",
                    "name": "K. Boland"
                },
                {
                    "authorId": "39876094",
                    "name": "D. Dess\u00ed"
                },
                {
                    "authorId": "3081683",
                    "name": "S. Dietze"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "2556805",
                    "name": "Andon Tchechmedjiev"
                },
                {
                    "authorId": "2594085",
                    "name": "Konstantin Todorov"
                },
                {
                    "authorId": "2362078",
                    "name": "Hajira Jabeen"
                }
            ]
        },
        {
            "paperId": "c8ee95c6353883d4c76832a95a53a41141374344",
            "title": "FastCat Catalogues: Interactive Entity-Based Exploratory Analysis of Archival Documents",
            "abstract": "We describe FastCat Catalogues, a web application that supports researchers studying archival material, such as historians, in exploring and quantitatively analysing the data (transcripts) of archival documents. The application was designed based on real information needs provided by a large group of researchers, makes use of JSON technology, and is configurable for use over any type of archival documents whose contents have been transcribed and exported in JSON format. The supported functionalities include a) sourceor record-specific entity browsing, b) source-independent entity browsing, c) data filtering, d) inspection of provenance information, e) data aggregation and visualisation in charts, f) table and chart data export for further (external) analysis. The application is provided as open source and is currently used by historians in maritime history research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204578584",
                    "name": "Georgios Rinakakis"
                },
                {
                    "authorId": "81223145",
                    "name": "K. Petrakis"
                },
                {
                    "authorId": "1801959",
                    "name": "Yannis Tzitzikas"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                }
            ]
        },
        {
            "paperId": "de06796ad67b9b8549c3991d4a1c5cbd1aab4ba8",
            "title": "A Workflow Model for Holistic Data Management and Semantic Interoperability in Quantitative Archival Research",
            "abstract": "\n Archival research is a complicated task that involves several diverse activities for the extraction of evidence and knowledge from a set of archival documents. The involved activities are usually unconnected, in terms of data connection and flow, making difficult their recursive revision and execution, as well as the inspection of provenance information at data element level. This article proposes a workflow model for holistic data management in archival research: from transcribing and documenting a set of archival documents, to curating the transcribed data, integrating it to a rich semantic network (knowledge graph), and then exploring the integrated data quantitatively. The workflow is provenance-aware, highly recursive and focuses on semantic interoperability, aiming at the production of sustainable data of high value and long-term validity. We provide implementation details for each step of the workflow and present its application in maritime history research. We also discuss relevant quality aspects and lessons learned from its application in a real context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "3046488",
                    "name": "Yannis Marketakis"
                },
                {
                    "authorId": "2079988",
                    "name": "A. Axaridou"
                },
                {
                    "authorId": "1801959",
                    "name": "Yannis Tzitzikas"
                },
                {
                    "authorId": "38587181",
                    "name": "M. Doerr"
                }
            ]
        },
        {
            "paperId": "637eade3b2e2ab52e450ec6cba6f4d3bc0f2cd1a",
            "title": "Estimating the Cost of Executing Link Traversal based SPARQL Queries",
            "abstract": "An increasing number of organisations in almost all fields have started adopting semantic web technologies for publishing their data as open, linked and interoperable (RDF) datasets, queryable through the SPARQL language and protocol. Link traversal has emerged as a SPARQL query processing method that exploits the Linked Data principles and the dynamic nature of the Web to dynamically discover data relevant for answering a query by resolving online resources (URIs) during query evaluation. However, the execution time of link traversal queries can become prohibitively high for certain query types due to the high number of resources that need to be accessed during query execution. In this paper we propose and evaluate baseline methods for estimating the evaluation cost of link traversal queries. Such methods can be very useful for deciding on-the-fly the query execution strategy to follow for a given query, thereby reducing the load of a SPARQL endpoint and increasing the overall reliability of the query service. To evaluate the performance of the proposed methods, we have created (and make publicly available) a ground truth dataset consisting of 2,425 queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3193470",
                    "name": "A. Sklavos"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "1801959",
                    "name": "Yannis Tzitzikas"
                }
            ]
        },
        {
            "paperId": "b42def2c144f276aa725bd316acff8895d961132",
            "title": "BeyondFacts\u201922: 2nd International Workshop on Knowledge Graphs for Online Discourse Analysis",
            "abstract": "Expressing opinions and interacting with others on the Web has led to an abundance of online discourse: claims and viewpoints on controversial topics, their sources and contexts. This constitutes a valuable source of insights for studies into mis- / disinformation spread, bias reinforcement, echo chambers or political agenda setting. While knowledge graphs promise to provide the key to a Web of structured information, they are mainly focused on facts without keeping track of the diversity, connection or temporal evolution of online discourse. As opposed to facts, claims and viewpoints are inherently more complex. Their interpretation strongly depends on the context and a variety of intentional or unintended meanings, where terminology and conceptual understandings strongly diverge across communities from computational social science, to argumentation mining, fact-checking, or viewpoint/stance detection. The 2nd International Workshop on Knowledge Graphs for Online Discourse Analysis (BeyondFacts\u201922, equivalently abbreviated as KnOD\u201922) aims at strengthening the relations between these communities, providing a forum for shared works on the modeling, extraction and analysis of discourse on the Web. It addresses the need for a shared understanding and structured knowledge about discourse in order to enable machine-interpretation, discoverability and reuse, in support of studies into the analysis of societal debates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2594085",
                    "name": "Konstantin Todorov"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "3081683",
                    "name": "S. Dietze"
                }
            ]
        },
        {
            "paperId": "eb815ef656c2c6e22ebe20765420aabaef2bc204",
            "title": "Beyond facts - a survey and conceptualisation of claims in online discourse analysis",
            "abstract": "Analyzing statements of facts and claims in online discourse is subject of a multitude of research areas. Methods from natural language processing and computational linguistics help investigate issues such as the spread of biased narratives and falsehoods on the Web. Related tasks include fact-checking, stance detection and argumentation mining. Knowledge-based approaches, in particular works in knowledge base construction and augmentation, are concerned with mining, verifying and representing factual knowledge. While all these fields are concerned with strongly related notions, such as claims, facts and evidence, terminology and conceptualisations used across and within communities vary heavily, making it hard to assess commonalities and relations of related works and how research in one field may contribute to address problems in another. We survey the state-of-the-art from a range of fields in this interdisciplinary area across a range of research tasks. We assess varying definitions and propose a conceptual model \u2013 Open Claims \u2013 for claims and related notions that takes into consideration their inherent complexity, distinguishing between their meaning, linguistic representation and context. We also introduce an implementation of this model by using established vocabularies and discuss applications across various tasks related to online discourse analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143348",
                    "name": "K. Boland"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "2556805",
                    "name": "Andon Tchechmedjiev"
                },
                {
                    "authorId": "3081683",
                    "name": "S. Dietze"
                },
                {
                    "authorId": "2594085",
                    "name": "Konstantin Todorov"
                }
            ]
        },
        {
            "paperId": "5ddfe34c611235c5dba2dde897f24750ac96f7bb",
            "title": "ClaimLinker: Linking Text to a Knowledge Graph of Fact-checked Claims",
            "abstract": "We present ClaimLinker, a Web service and API that links arbitrary text to a knowledge graph of fact-checked claims, offering a novel kind of semantic annotation of unstructured content. Given a text, ClaimLinker matches parts of it to fact-checked claims mined from popular fact-checking sites and integrated into a rich knowledge graph, thus allowing the further exploration of the linked claims and their associations. The application is based on a scalable, fully unsupervised and modular approach that does not require training or tuning and which can serve high quality results at real time (outperforming existing unsupervised methods). This allows its easy deployment for different contexts and application scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2088931717",
                    "name": "Evangelos Maliaroudakis"
                },
                {
                    "authorId": "2143348",
                    "name": "K. Boland"
                },
                {
                    "authorId": "3081683",
                    "name": "S. Dietze"
                },
                {
                    "authorId": "2594085",
                    "name": "Konstantin Todorov"
                },
                {
                    "authorId": "1801959",
                    "name": "Yannis Tzitzikas"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                }
            ]
        }
    ]
}