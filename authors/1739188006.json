{
    "authorId": "1739188006",
    "papers": [
        {
            "paperId": "21244c4f175c31f2bd86c8d4cdca1ee1294d6eb4",
            "title": "RemoCap: Disentangled Representation Learning for Motion Capture",
            "abstract": "Reconstructing 3D human bodies from realistic motion sequences remains a challenge due to pervasive and complex occlusions. Current methods struggle to capture the dynamics of occluded body parts, leading to model penetration and distorted motion. RemoCap leverages Spatial Disentanglement (SD) and Motion Disentanglement (MD) to overcome these limitations. SD addresses occlusion interference between the target human body and surrounding objects. It achieves this by disentangling target features along the dimension axis. By aligning features based on their spatial positions in each dimension, SD isolates the target object's response within a global window, enabling accurate capture despite occlusions. The MD module employs a channel-wise temporal shuffling strategy to simulate diverse scene dynamics. This process effectively disentangles motion features, allowing RemoCap to reconstruct occluded parts with greater fidelity. Furthermore, this paper introduces a sequence velocity loss that promotes temporal coherence. This loss constrains inter-frame velocity errors, ensuring the predicted motion exhibits realistic consistency. Extensive comparisons with state-of-the-art (SOTA) methods on benchmark datasets demonstrate RemoCap's superior performance in 3D human body reconstruction. On the 3DPW dataset, RemoCap surpasses all competitors, achieving the best results in MPVPE (81.9), MPJPE (72.7), and PA-MPJPE (44.1) metrics. Codes are available at https://wanghongsheng01.github.io/RemoCap/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299738158",
                    "name": "Hongsheng Wang"
                },
                {
                    "authorId": "2302444646",
                    "name": "Lizao Zhang"
                },
                {
                    "authorId": "2300144685",
                    "name": "Zhangnan Zhong"
                },
                {
                    "authorId": "2302567463",
                    "name": "Shuolin Xu"
                },
                {
                    "authorId": "2302557898",
                    "name": "Xinrui Zhou"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2302557134",
                    "name": "Huahao Xu"
                },
                {
                    "authorId": "2299583502",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "2303256088",
                    "name": "Feng Lin"
                }
            ]
        },
        {
            "paperId": "41a8c09d0be167342b395ecb2e32aab1f6fbb615",
            "title": "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery",
            "abstract": "Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human reconstruction, it primarily relies on 2D pixel-level supervision, overlooking the geometric complexity and topological relationships of different body parts. To address this gap, we introduce the Hierarchical Graph Human Gaussian Control (HUGS) framework for achieving high-fidelity 3D human reconstruction. Our approach involves leveraging explicitly semantic priors of body parts to ensure the consistency of geometric topology, thereby enabling the capture of the complex geometrical and topological associations among body parts. Additionally, we disentangle high-frequency features from global human features to refine surface details in body parts. Extensive experiments demonstrate that our method exhibits superior performance in human body reconstruction, particularly in enhancing surface details and accurately reconstructing body part junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299738158",
                    "name": "Hongsheng Wang"
                },
                {
                    "authorId": "2302463094",
                    "name": "Weiyue Zhang"
                },
                {
                    "authorId": "2302568977",
                    "name": "Sihao Liu"
                },
                {
                    "authorId": "2302557898",
                    "name": "Xinrui Zhou"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2299583502",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "2303256088",
                    "name": "Feng Lin"
                }
            ]
        },
        {
            "paperId": "5c02e6394b8f271b027e4e216b01ed86baa0679b",
            "title": "MergeNet: Knowledge Migration across Heterogeneous Models, Tasks, and Modalities",
            "abstract": "In this study, we focus on heterogeneous knowledge transfer across entirely different model architectures, tasks, and modalities. Existing knowledge transfer methods (e.g., backbone sharing, knowledge distillation) often hinge on shared elements within model structures or task-specific features/labels, limiting transfers to complex model types or tasks. To overcome these challenges, we present MergeNet, which learns to bridge the gap of parameter spaces of heterogeneous models, facilitating the direct interaction, extraction, and application of knowledge within these parameter spaces. The core mechanism of MergeNet lies in the parameter adapter, which operates by querying the source model's low-rank parameters and adeptly learning to identify and map parameters into the target model. MergeNet is learned alongside both models, allowing our framework to dynamically transfer and adapt knowledge relevant to the current stage, including the training trajectory knowledge of the source model. Extensive experiments on heterogeneous knowledge transfer demonstrate significant improvements in challenging settings, where representative approaches may falter or prove less applicable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2297889408",
                    "name": "Kunxi Li"
                },
                {
                    "authorId": "2297771146",
                    "name": "Tianyu Zhan"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "33870528",
                    "name": "Kun Kuang"
                },
                {
                    "authorId": "2297822670",
                    "name": "Jiwei Li"
                },
                {
                    "authorId": "2238156073",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2273351067",
                    "name": "Fei Wu"
                }
            ]
        },
        {
            "paperId": "6a4cc1a573e1db5cbc8b1c3feeee193587f708fc",
            "title": "ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation",
            "abstract": "The rapid advancement of Large Language Models (LLMs) has revolutionized various sectors by automating routine tasks, marking a step toward the realization of Artificial General Intelligence (AGI). However, they still struggle to accommodate the diverse and specific needs of users and simplify the utilization of AI models for the average user. In response, we propose ModelGPT, a novel framework designed to determine and generate AI models specifically tailored to the data or task descriptions provided by the user, leveraging the capabilities of LLMs. Given user requirements, ModelGPT is able to provide tailored models at most 270x faster than the previous paradigms (e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV, and Tabular datasets attest to the effectiveness of our framework in making AI models more accessible and user-friendly. Our code is available at https://github.com/IshiKura-a/ModelGPT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284834010",
                    "name": "Zihao Tang"
                },
                {
                    "authorId": "1475705618",
                    "name": "Zheqi Lv"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2273351067",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "33870528",
                    "name": "Kun Kuang"
                }
            ]
        },
        {
            "paperId": "7437518acb0c2271bdd8b32048c233f434a8f11a",
            "title": "Semantic Codebook Learning for Dynamic Recommendation Models",
            "abstract": "Dynamic sequential recommendation (DSR) can generate model parameters based on user behavior to improve the personalization of sequential recommendation under various user preferences. However, it faces the challenges of large parameter search space and sparse and noisy user-item interactions, which reduces the applicability of the generated model parameters. The Semantic Codebook Learning for Dynamic Recommendation Models (SOLID) framework presents a significant advancement in DSR by effectively tackling these challenges. By transforming item sequences into semantic sequences and employing a dual parameter model, SOLID compresses the parameter generation search space and leverages homogeneity within the recommendation system. The introduction of the semantic metacode and semantic codebook, which stores disentangled item representations, ensures robust and accurate parameter generation. Extensive experiments demonstrates that SOLID consistently outperforms existing DSR, delivering more accurate, stable, and robust recommendations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1475705618",
                    "name": "Zheqi Lv"
                },
                {
                    "authorId": "2314130871",
                    "name": "Shaoxuan He"
                },
                {
                    "authorId": "2297771146",
                    "name": "Tianyu Zhan"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2273572940",
                    "name": "Wenqiao Zhang"
                },
                {
                    "authorId": "2306340514",
                    "name": "Jingyuan Chen"
                },
                {
                    "authorId": "2238156073",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2301259790",
                    "name": "Fei Wu"
                }
            ]
        },
        {
            "paperId": "92dfed4957921da07b9f8814e8a6a30d19516c6a",
            "title": "GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian Splatting",
            "abstract": "Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299277915",
                    "name": "Hongyun Yu"
                },
                {
                    "authorId": "2298898534",
                    "name": "Zhan Qu"
                },
                {
                    "authorId": "2298958723",
                    "name": "Qihang Yu"
                },
                {
                    "authorId": "2298930533",
                    "name": "Jianchuan Chen"
                },
                {
                    "authorId": "2298952367",
                    "name": "Zhonghua Jiang"
                },
                {
                    "authorId": "2298945335",
                    "name": "Zhiwen Chen"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2298933426",
                    "name": "Jimin Xu"
                },
                {
                    "authorId": "2299177173",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "2298896049",
                    "name": "Chengfei Lv"
                },
                {
                    "authorId": "2298943584",
                    "name": "Gang Yu"
                }
            ]
        },
        {
            "paperId": "94ab32f93cc31840b527186d75e3093cf386d583",
            "title": "MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video",
            "abstract": "Single-view clothed human reconstruction holds a central position in virtual reality applications, especially in contexts involving intricate human motions. It presents notable challenges in achieving realistic clothing deformation. Current methodologies often overlook the influence of motion on surface deformation, resulting in surfaces lacking the constraints imposed by global motion. To overcome these limitations, we introduce an innovative framework, Motion-Based 3D Clo}thed Humans Synthesis (MOSS), which employs kinematic information to achieve motion-aware Gaussian split on the human surface. Our framework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS) and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher distribution to propagate global motion across the body surface. The density and rotation factors of this distribution explicitly control the Gaussians, thereby enhancing the realism of the reconstructed surface. Additionally, to address local occlusions in single-view, based on KGAS, UID identifies significant surfaces, and geometric reconstruction is performed to compensate for these deformations. Experimental results demonstrate that MOSS achieves state-of-the-art visual quality in 3D clothed human synthesis from monocular videos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94% and 16.75% in LPIPS* respectively. Codes are available at https://wanghongsheng01.github.io/MOSS/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299738158",
                    "name": "Hongsheng Wang"
                },
                {
                    "authorId": "2303230491",
                    "name": "Xiang Cai"
                },
                {
                    "authorId": "2302527415",
                    "name": "Xi Sun"
                },
                {
                    "authorId": "2302405241",
                    "name": "Jinhong Yue"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2303256088",
                    "name": "Feng Lin"
                },
                {
                    "authorId": "2299583502",
                    "name": "Fei Wu"
                }
            ]
        },
        {
            "paperId": "9c05bdb236e6a5dc6a1c49789d7f4ec8f1b6deb6",
            "title": "Transferring Causal Mechanism over Meta-representations for Target-Unknown Cross-domain Recommendation",
            "abstract": "Tackling the pervasive issue of data sparsity in recommender systems, we present an insightful investigation into the burgeoning area of non-overlapping cross-domain recommendation, a technique that facilitates the transfer of interaction knowledge across domains without necessitating inter-domain user/item correspondence. Existing approaches have predominantly depended on auxiliary information, such as user reviews and item tags, to establish inter-domain connectivity, but these resources may become inaccessible due to privacy and commercial constraints. To address these limitations, our study introduces an in-depth exploration of Target-unknown Cross-domain Recommendation (CDR), which contends with the distinct challenge of lacking target domain information during the training phase in the source domain. We illustrate two critical obstacles inherent to Target-unknown CDR: the lack of an inter-domain bridge due to insufficient user/item correspondence or side information and the potential pitfalls of source-domain training biases when confronting distribution shifts across domains. To surmount these obstacles, we propose the CMCDR framework, a novel approach that leverages causal mechanisms extracted from meta-user/item representations. The CMCDR framework employs a vector-quantized encoder\u2013decoder architecture, enabling the disentanglement of user/item characteristics. We posit that domain-transferable knowledge is more readily discernible from user/item characteristics, i.e., the meta-representations, rather than raw users and items. Capitalizing on these meta-representations, our CMCDR framework adeptly incorporates an attention-driven predictor that approximates the front-door adjustment method grounded in causal theory. This cutting-edge strategy effectively mitigates source-domain training biases and enhances generalization capabilities against distribution shifts. Extensive experiments demonstrate the empirical effectiveness and the rationality of CMCDR for target-unknown cross-domain recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "1580304095",
                    "name": "Qiaowei Miao"
                },
                {
                    "authorId": "2282612003",
                    "name": "Ping Nie"
                },
                {
                    "authorId": "2261906948",
                    "name": "Mengze Li"
                },
                {
                    "authorId": "2272001249",
                    "name": "Zhengyu Chen"
                },
                {
                    "authorId": "2238005533",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "33870528",
                    "name": "Kun Kuang"
                },
                {
                    "authorId": "2279789214",
                    "name": "Fei Wu"
                }
            ]
        },
        {
            "paperId": "a09d1f58d39550601c0e40ece1fbccf7c1c474d0",
            "title": "MPOD123: One Image to 3D Content Generation Using Mask-Enhanced Progressive Outline-to-Detail Optimization",
            "abstract": "Recent advancements in single image driven 3D content generation have been propelled by leveraging prior knowledge from pretrained 2D diffusion models. However, the 3D content generated by existing methods often exhibits distorted outline shapes and inadequate details. To solve this problem, we propose a novel framework called Mask-enhanced Progressive Outline-to-Detail optimization (aka. MPOD123), which consists of two stages. Specifically, in the first stage, MPOD123 utilizes the pretrained view-conditioned diffusion model to guide the outline shape optimization of the 3D content. Given certain viewpoint, we estimate outline shape priors in the form of 2D mask from the 3D content by leveraging opacity calculation. In the second stage, MPOD123 incorporates Detail Appearance Inpainting (DAI) to guide the refinement on local geometry and texture with the shape priors. The essence of DAI lies in the Mask Rectified Cross-Attention (MRCA), which can be conveniently plugged in the stable diffusion model. The MRCA module utilizes the mask to rectify the attention map from each cross-attention layer. Accompanied with this new module, DAI is capable of guiding the detail refinement of the 3D content, while better preserves the outline shape. To assess the applicability in practical scenarios, we contribute a new dataset modeled on real-world e-commerce environments. Extensive quantitative and qualitative experiments on this dataset and open benchmarks demonstrate the effectiveness of MPOD123 over the state-of-the-arts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298933426",
                    "name": "Jimin Xu"
                },
                {
                    "authorId": "2158957515",
                    "name": "Tianbao Wang"
                },
                {
                    "authorId": "2321714153",
                    "name": "Tao Jin"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2321683750",
                    "name": "Dongjie Fu"
                },
                {
                    "authorId": "2321700962",
                    "name": "Zhe Wang"
                },
                {
                    "authorId": "2321682516",
                    "name": "Jiangjing Lyu"
                },
                {
                    "authorId": "2298896049",
                    "name": "Chengfei Lv"
                },
                {
                    "authorId": "2321647870",
                    "name": "Chaoyue Niu"
                },
                {
                    "authorId": "2321656588",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2187385241",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2299177173",
                    "name": "Fei Wu"
                }
            ]
        },
        {
            "paperId": "a6a0f849ca628e847e989ea9c6cd97600bd486c7",
            "title": "AutoGeo: Automating Geometric Image Dataset Creation for Enhanced Geometry Understanding",
            "abstract": "With the rapid advancement of large language models, there has been a growing interest in their capabilities in mathematical reasoning. However, existing research has primarily focused on text-based algebra problems, neglecting the study of geometry due to the lack of high-quality geometric datasets. To address this gap, this paper introduces AutoGeo, a novel approach for automatically generating mathematical geometric images to fulfill the demand for large-scale and diverse geometric datasets. AutoGeo facilitates the creation of AutoGeo-100k, an extensive repository comprising 100k high-quality geometry image-text pairs. By leveraging precisely defined geometric clauses, AutoGeo-100k contains a wide variety of geometric shapes, including lines, polygons, circles, and complex spatial relationships, etc. Furthermore, this paper demonstrates the efficacy of AutoGeo-100k in enhancing the performance of multimodal large language models through fine-tuning. Experimental results indicate significant improvements in the model's ability in handling geometric images, as evidenced by enhanced accuracy in tasks such as geometric captioning and mathematical reasoning. This research not only fills a critical gap in the availability of geometric datasets but also paves the way for the advancement of sophisticated AI-driven tools in education and research. Project page: https://autogeo-official.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2320511441",
                    "name": "Zihan Huang"
                },
                {
                    "authorId": "2312655144",
                    "name": "Tao Wu"
                },
                {
                    "authorId": "2316887051",
                    "name": "Wang Lin"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2312716081",
                    "name": "Jingyuan Chen"
                },
                {
                    "authorId": "2301259790",
                    "name": "Fei Wu"
                }
            ]
        }
    ]
}