{
    "authorId": "2119197417",
    "papers": [
        {
            "paperId": "0f84514e42c080efbc94f4aa2b2261e81d08ca24",
            "title": "Curricular Object Manipulation in LiDAR-based Object Detection",
            "abstract": "This paper explores the potential of curriculum learning in LiDAR-based 3D object detection by proposing a curricular object manipulation (COM) framework. The framework embeds the curricular training strategy into both the loss design and the augmentation process. For the loss design, we propose the COMLoss to dynamically predict object-level difficulties and emphasize objects of different difficulties based on training stages. On top of the widely-used augmentation technique called GT-Aug in Li-DAR detection tasks, we propose a novel COMAug strategy which first clusters objects in ground-truth database based on well-designed heuristics. Group-level difficulties rather than individual ones are then predicted and updated during training for stable results. Model performance and generalization capabilities can be improved by sampling and augmenting progressively more difficult objects into the training samples. Extensive experiments and ablation studies reveal the superior and generality of the proposed framework. The code is available at https://github.com/ZZY816/COM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121394809",
                    "name": "Ziyue Zhu"
                },
                {
                    "authorId": "2112721678",
                    "name": "Q. Meng"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2133842786",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "2218779354",
                    "name": "Liujiang Yan"
                },
                {
                    "authorId": "2119197417",
                    "name": "Jian Yang"
                }
            ]
        },
        {
            "paperId": "1c5fefeaa2206988c654226cf12ecd93d9ba1739",
            "title": "Semantics-Guided Moving Object Segmentation with 3D LiDAR",
            "abstract": "Moving object segmentation (MOS) is a task to distinguish moving objects, e.g., moving vehicles and pedestrians, from the surrounding static environment. The segmentation accuracy of MOS can have an influence on odometry, map construction, and planning tasks. In this paper, we propose a semantics-guided convolutional neural network for moving object segmentation. The network takes sequential LiDAR range images as inputs. Instead of segmenting the moving objects directly, the network conducts single-scan-based semantic segmentation and multiple-scan-based moving object segmentation in turn. The semantic segmentation module provides semantic priors for the MOS module, where we propose an adjacent scan association (ASA) module to convert the semantic features of adjacent scans into the same coordinate system to fully exploit the cross-scan semantic features. Finally, by analyzing the difference between the transformed features, reliable MOS result can be obtained quickly. Experimental results on the SemanticKITTI MOS dataset proves the effectiveness of our work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057716779",
                    "name": "Shuo Gu"
                },
                {
                    "authorId": "2164342834",
                    "name": "Suling Yao"
                },
                {
                    "authorId": "2119197417",
                    "name": "Jian Yang"
                },
                {
                    "authorId": "145771879",
                    "name": "Hui Kong"
                }
            ]
        },
        {
            "paperId": "3dd7e11f54373d4de16fe03495872d2099105edd",
            "title": "Building Block Extraction from Historical Maps Using Deep Object Attention Networks",
            "abstract": "The geographical feature extraction of historical maps is an important foundation for realizing the transition from human map reading to machine map reading. The current methods for building block extraction from historical maps have many problems, such as low accuracy and poor scalability. Moreover, the high cost of annotating historical maps further limits its applications. In this study, a method for extracting building blocks from historical maps is proposed based on the deep object attention network. Based on the OCRNet framework, multiple attention mechanisms were used to improve the ability of the network to extract the contextual information of the target. Moreover, through the optimization of the feature extraction network structure, the impact of the down-sampling process on local information and boundary contours was reduced, in order to improve the network\u2019s ability to capture boundary information. Subsequently, the transfer learning method was used to jointly train the network model on both remote sensing datasets and few-shot historical map datasets to further improve the feature learning ability of the network, which overcomes the constraints of small sample sizes. The experimental results show that the proposed method can effectively improve the extraction accuracy of building blocks from historical maps.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2191360036",
                    "name": "Yao Zhao"
                },
                {
                    "authorId": "2152582072",
                    "name": "Guangxia Wang"
                },
                {
                    "authorId": "2119197417",
                    "name": "Jian Yang"
                },
                {
                    "authorId": "2109006669",
                    "name": "Lantian Zhang"
                },
                {
                    "authorId": "2191334340",
                    "name": "Xiaofei Qi"
                }
            ]
        },
        {
            "paperId": "5c87cddb33c01aeb6031b32c89c02e07709749ff",
            "title": "LiDAR-Based Real-Time Panoptic Segmentation via Spatiotemporal Sequential Data Fusion",
            "abstract": "Fast and accurate semantic scene understanding is essential for mobile robots to operate in complex environments. An emerging research topic, panoptic segmentation, serves such a purpose by performing the tasks of semantic segmentation and instance segmentation in a unified framework. To improve the performance of LiDAR-based real-time panoptic segmentation, this study proposes a spatiotemporal sequential data fusion strategy that fused points in \u201cthing classes\u201d based on accurate data statistics. The data fusion strategy could increase the proportion of valuable data in unbalanced datasets, and thus managed to mitigate the adverse impact of class imbalance in the limited training data. Subsequently, by improving the codec network, the multiscale features shared by semantic and instance branches were efficiently aggregated to achieve accurate panoptic segmentation for each LiDAR scan. Experiments on the publicly available dataset SemanticKITTI showed that our approach could achieve an effective balance between accuracy and efficiency, and it was also applicable to other point cloud segmentation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116070589",
                    "name": "Weiqi Wang"
                },
                {
                    "authorId": "49713003",
                    "name": "Xiong You"
                },
                {
                    "authorId": "2119197417",
                    "name": "Jian Yang"
                },
                {
                    "authorId": "1476820421",
                    "name": "Mingzhan Su"
                },
                {
                    "authorId": "2109006669",
                    "name": "Lantian Zhang"
                },
                {
                    "authorId": "47087058",
                    "name": "Zhenkai Yang"
                },
                {
                    "authorId": "46486681",
                    "name": "Yingcai Kuang"
                }
            ]
        },
        {
            "paperId": "0337e1b2f0b7cc290187fd00f5ec62a0961578ee",
            "title": "Survey on Urban Warfare Augmented Reality",
            "abstract": "Urban warfare has become one of the main forms of modern combat in the twenty-first century. The main reason why urban warfare results in hundreds of casualties is that the situational information of the combatant is insufficient. Accessing information via an Augmented Reality system can elevate combatants\u2019 situational awareness to effectively improve the efficiency of decision-making and reduce the injuries. This paper begins with the concept of Urban Warfare Augmented Reality (UWAR) and illuminates the objectives of developing UWAR, i.e., transparent battlefield, intuitional perception and natural interaction. Real-time outdoor registration, information presentation and natural interaction are presented as key technologies of a practical UWAR system. Then, the history and current research state of these technologies are summarized and their future developments are highlighted from three perspectives, i.e., (1) Better integration with Geographic Information System and Virtual Geographic Environment; (2) More intelligent software; (3) More powerful hardware.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059537879",
                    "name": "Xiong You"
                },
                {
                    "authorId": "2108478472",
                    "name": "Weiwei Zhang"
                },
                {
                    "authorId": "2113428426",
                    "name": "Meng Ma"
                },
                {
                    "authorId": "2057947154",
                    "name": "Chen Deng"
                },
                {
                    "authorId": "2119197417",
                    "name": "Jian Yang"
                }
            ]
        },
        {
            "paperId": "96afd2a08ee230a938adca1640cf713ec3a1a580",
            "title": "Combining ElasticFusion with PSPNet for RGB-D Based Indoor Semantic Mapping",
            "abstract": "Semantic segmentation has been a research focus of scene parsing and robotic manipulation. And increasing efforts are being made to empower geometry-solely robotic mapping with semantic sensing ability in complex spatial cognition tasks, the so-called semantic mapping. However, semantic mapping has not yet fully exploited the state-of-the-art of semantic segmentation and suffered limitations of application scenario, number of object category and accuracy of object detection. Towards a robust semantic mapping solution, we propose a RGB-D based fusion method that combines an improved ElasticFusion with PSPNet. PSPNet has the potential for breaking restrictions of limited object category in semantic segmentation of RGB images. As for map rendering, the improved ElasticFusion algorithm is validated for its robust performance in indoor scenes in terms of overall model accuracy and integral object shape geometry. Based on the two aforementioned modules, we fuses the semantic images and the depth images to generate 3D point cloud with semantic information. Experiments on the ICL-NUIM datasets have proven the feasibility of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116070589",
                    "name": "Weiqi Wang"
                },
                {
                    "authorId": "2119197417",
                    "name": "Jian Yang"
                },
                {
                    "authorId": "49713003",
                    "name": "Xiong You"
                }
            ]
        }
    ]
}