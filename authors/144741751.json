{
    "authorId": "144741751",
    "papers": [
        {
            "paperId": "27cc05d2ad2f48123db8fb6b38690862b34ac75c",
            "title": "Large Generative Graph Models",
            "abstract": "Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno are trained on a huge amount of language corpus, images, videos, and audio that are extremely diverse from numerous domains. This training paradigm over diverse well-curated data lies at the heart of generating creative and sensible content. However, all previous graph generative models (e.g., GraphRNN, MDVAE, MoFlow, GDSS, and DiGress) have been trained only on one dataset each time, which cannot replicate the revolutionary success achieved by LGMs in other fields. To remedy this crucial gap, we propose a new class of graph generative model called Large Graph Generative Model (LGGM) that is trained on a large corpus of graphs (over 5000 graphs) from 13 different domains. We empirically demonstrate that the pre-trained LGGM has superior zero-shot generative capability to existing graph generative models. Furthermore, our pre-trained LGGM can be easily fine-tuned with graphs from target domains and demonstrate even better performance than those directly trained from scratch, behaving as a solid starting point for real-world customization. Inspired by Stable Diffusion, we further equip LGGM with the capability to generate graphs given text prompts (Text-to-Graph), such as the description of the network name and domain (i.e.,\"The power-1138-bus graph represents a network of buses in a power distribution system.\"), and network statistics (i.e.,\"The graph has a low average degree, suitable for modeling social media interactions.\"). This Text-to-Graph capability integrates the extensive world knowledge in the underlying language model, offering users fine-grained control of the generated graphs. We release the code, the model checkpoint, and the datasets at https://lggm-lg.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284900711",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2268675415",
                    "name": "Namyong Park"
                },
                {
                    "authorId": "2305588553",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "30440868",
                    "name": "Puja Trivedi"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "2290558635",
                    "name": "Tyler Derr"
                }
            ]
        },
        {
            "paperId": "4fb825a8454e6a7f69f987424fd799b65ae1c6a1",
            "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text",
            "abstract": "Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step reasoning capabilities of LLMs in a zero-shot setting. This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses. By effectively organizing information and guiding navigation, it enables LLMs to provide more accurate and context-aware responses. Our experiments show that this framework significantly enhances the reasoning capabilities of LLMs, enabling them to excel in a broader spectrum of natural language scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260755600",
                    "name": "Kewei Cheng"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "2268234685",
                    "name": "T. Willke"
                },
                {
                    "authorId": "2257321412",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "84557580980d28af40581143c62a856988d64eef",
            "title": "Leveraging Graph Diffusion Models for Network Refinement Tasks",
            "abstract": "Most real-world networks are noisy and incomplete samples from an unknown target distribution. Refining them by correcting corruptions or inferring unobserved regions typically improves downstream performance. Inspired by the impressive generative capabilities that have been used to correct corruptions in images, and the similarities between\"in-painting\"and filling in missing nodes and edges conditioned on the observed graph, we propose a novel graph generative framework, SGDM, which is based on subgraph diffusion. Our framework not only improves the scalability and fidelity of graph diffusion models, but also leverages the reverse process to perform novel, conditional generation tasks. In particular, through extensive empirical analysis and a set of novel metrics, we demonstrate that our proposed model effectively supports the following refinement tasks for partially observable networks: T1: denoising extraneous subgraphs, T2: expanding existing subgraphs and T3: performing\"style\"transfer by regenerating a particular subgraph to match the characteristics of a different node or subgraph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30440868",
                    "name": "Puja Trivedi"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "2268674629",
                    "name": "David Arbour"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2261424174",
                    "name": "Sungchul Kim"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2268675415",
                    "name": "Namyong Park"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                }
            ]
        },
        {
            "paperId": "be496d40ef1d67080329f467916c6d9624b80600",
            "title": "A ML-based Approach for HTML-based Style Recommendation",
            "abstract": "Given a large corpus of HTML-based emails (or websites, posters, documents) collected from the web, how can we train a model capable of learning from such rich heterogeneous data for HTML-based style recommendation tasks such as recommending useful design styles or suggesting alternative HTML designs? To address this new learning task, we first decompose each HTML document in the corpus into a sequence of smaller HTML fragments where each fragment may consist of a set of HTML entities such as buttons, images, textual content (titles, paragraphs) and stylistic entities such as background-style, font-style, button-style, among others. From these HTML fragments, we then derive a single large heterogeneous hypergraph that captures the higher-order dependencies between HTML fragments and entities in such fragments, both within the same HTML document as well as across the HTML documents in the corpus. We then formulate this new HTML style recommendation task as a hypergraph representation learning problem and propose an approach to solve it. Our approach is able to learn effective low-dimensional representations of the higher-order fragments that consist of sets of heterogeneous entities as well as low-dimensional representations of the individual entities themselves. We demonstrate the effectiveness of the approach across several design style recommendation tasks. To the best of our knowledge, this work is the first to develop an ML-based model for the task of HTML-based email style recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2199012507",
                    "name": "Ryan Aponte"
                },
                {
                    "authorId": "2176092943",
                    "name": "R. Rossi"
                },
                {
                    "authorId": "30518075",
                    "name": "Shunan Guo"
                },
                {
                    "authorId": "1890683",
                    "name": "J. Hoffswell"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2163452020",
                    "name": "Chang Xiao"
                },
                {
                    "authorId": "51192588",
                    "name": "G. Chan"
                },
                {
                    "authorId": "2176108906",
                    "name": "E. Koh"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                }
            ]
        },
        {
            "paperId": "dd3760fd99acf88a4c1e0401e87c4cc1e39bc4fd",
            "title": "Augmenting Recurrent Graph Neural Networks with a Cache",
            "abstract": "While graph neural networks (GNNs) provide a powerful way to learn structured representations, it remains challenging to learn long-range dependencies in graphs. Recurrent GNNs only partly address this problem. In this paper, we propose a general approach for augmenting recurrent GNNs with a cache memory to improve their expressivity, especially for modeling long-range dependencies. Specifically, we first introduce a method of augmenting recurrent GNNs with a cache of previous hidden states. Then we further propose a general Cache-GNN framework by adding additional modules, including attention mechanism and positional/structural encoders, to improve the expressivity. We show that the Cache-GNNs outperforms other models on synthetic datasets as well as tasks on real-world datasets that require long-range information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068988549",
                    "name": "Guixiang Ma"
                },
                {
                    "authorId": "3369353",
                    "name": "Vy A. Vo"
                },
                {
                    "authorId": "102992766",
                    "name": "Ted Willke"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                }
            ]
        },
        {
            "paperId": "f0606e2d975cda134392877e4064924a13915301",
            "title": "Fairness-Aware Graph Neural Networks: A Survey",
            "abstract": "Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. We categorize these techniques by whether they focus on improving fairness in the pre-processing, in-processing (during training), or post-processing phases. We discuss how such techniques can be used together whenever appropriate and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics, including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fairness metrics. In addition, graph datasets that are useful for benchmarking the fairness of GNN models are summarized succinctly. Finally, we highlight key open problems and challenges that remain to be addressed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2198208547",
                    "name": "April Chen"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "2867343",
                    "name": "Namyong Park"
                },
                {
                    "authorId": "30440868",
                    "name": "Puja Trivedi"
                },
                {
                    "authorId": "2153607948",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2109571021",
                    "name": "Sungchul Kim"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                }
            ]
        },
        {
            "paperId": "f151dc9700aca01d9c33569549838bd35f47063d",
            "title": "On Graph Time-Series Representations for Temporal Networks",
            "abstract": "Representations of temporal networks arising from a stream of edges lie at the heart of models learned on it and its performance on downstream applications. While previous work on dynamic modeling and embedding have focused on representing a stream of timestamped edges using a time-series of graphs based on a specific time-scale \u03c4 (e.g., 1 month), we introduce the notion of an \u03f5 -graph time-series that uses a fixed number of edges for each graph, and show its effectiveness in capturing fundamental structural graph statistics over time. The results indicate that the \u03f5 -graph time-series representation effectively captures the structural properties of the graphs across time whereas the commonly used \u03c4 -graph time-series representation captures the frequency of edges and temporal patterns with respect to their arrival in the application time. These results have many important implications especially on the design of new GNN-based models for temporal networks as well as for understanding existing models and their limitations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1862090",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "2867343",
                    "name": "Namyong Park"
                }
            ]
        },
        {
            "paperId": "0bd934dec43bba4a10e31907c2dc0dbbfe334e6a",
            "title": "Graph Learning with Localized Neighborhood Fairness",
            "abstract": "Learning fair graph representations for downstream applications is becoming increasingly important, but existing work has mostly focused on improving fairness at the global level by either modify-ing the graph structure or objective function without taking into account the local neighborhood of a node. In this work, we formally introduce the notion of neighborhood fairness and develop a computational framework for learning such locally fair embeddings. We argue that the notion of neighborhood fairness is more appropriate since GNN-based models operate at the local neighborhood level of a node. Our neighborhood fairness framework has two main components that are \ufb02exible for learning fair graph representations from arbitrary data: the \ufb01rst aims to construct fair neighborhoodsfor any arbitrary node in a graph and the second enables adaptionof these fair neighborhoodsto bettercapture certain application or data-dependent constraints, such as allowing neigh-borhoodstobe more biased towards certain attributesorneighbors in the graph. Furthermore, while link prediction has been exten-sively studied, we are the \ufb01rst to investigate the graph representation learning task of fair link classi\ufb01cation. We demonstrate the e\ufb00ectiveness of the proposed neighborhood fairness framework for a variety of graph machine learning tasks including fair link prediction, link classi\ufb01cation, and learning fair graph embeddings. Notably, our approach achieves not only better fairness but also increases the accuracy in the majority of cases across a wide variety of graphs, problem settings, and metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2198208547",
                    "name": "April Chen"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "1890683",
                    "name": "J. Hoffswell"
                },
                {
                    "authorId": "51192588",
                    "name": "G. Chan"
                },
                {
                    "authorId": "30518075",
                    "name": "Shunan Guo"
                },
                {
                    "authorId": "2176108906",
                    "name": "E. Koh"
                },
                {
                    "authorId": "2109571021",
                    "name": "Sungchul Kim"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                }
            ]
        },
        {
            "paperId": "28558d4d26f46c5dbf68bd11930645a96d3a4fc5",
            "title": "Technology Growth Ranking Using Temporal Graph Representation Learning",
            "abstract": "A key component of technology sector business strategy is understanding the mechanisms by which technologies are adopted and the rate of their growth over time. Furthermore, predicting how technologies grow in relation to each other informs business decision-making in terms of product definition, research and development, and marketing strategies. An important avenue for exploring technology trends is by looking at activity in the software community. Social networks for developers can provide useful technology trend insights and have an inherent temporal graph structure. We demonstrate an approach to technology growth ranking that adapts spatiotemporal graph neural networks to work with structured temporal relational graph data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064635302",
                    "name": "Daniel Cummings"
                },
                {
                    "authorId": "1403494581",
                    "name": "Ashrita Brahmaroutu"
                },
                {
                    "authorId": "1678319",
                    "name": "M. Nassar"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                }
            ]
        },
        {
            "paperId": "39798c6213759f45144569a3bb034c1b4bb34789",
            "title": "A Hypergraph Neural Network Framework for Learning Hyperedge-Dependent Node Embeddings",
            "abstract": "In this work, we introduce a hypergraph representation learning framework called Hypergraph Neural Networks (HNN) that jointly learns hyperedge embeddings along with a set of hyperedge-dependent embeddings for each node in the hypergraph. HNN derives multiple embeddings per node in the hypergraph where each embedding for a node is dependent on a specific hyperedge of that node. Notably, HNN is accurate, data-efficient, flexible with many interchangeable components, and useful for a wide range of hypergraph learning tasks. We evaluate the effectiveness of the HNN framework for hyperedge prediction and hypergraph node classification. We find that HNN achieves an overall mean gain of 7.72% and 11.37% across all baseline models and graphs for hyperedge prediction and hypergraph node classification, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2199012507",
                    "name": "Ryan Aponte"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "30518075",
                    "name": "Shunan Guo"
                },
                {
                    "authorId": "1890683",
                    "name": "J. Hoffswell"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2163452020",
                    "name": "Chang Xiao"
                },
                {
                    "authorId": "51192588",
                    "name": "G. Chan"
                },
                {
                    "authorId": "2176108906",
                    "name": "E. Koh"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                }
            ]
        }
    ]
}