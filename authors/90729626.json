{
    "authorId": "90729626",
    "papers": [
        {
            "paperId": "17f3091e31705d022f55b75964283f1115e735e3",
            "title": "Modulating Language Model Experiences through Frictions",
            "abstract": "Language models are transforming the ways that their users engage with the world. Despite impressive capabilities, over-consumption of language model outputs risks propagating unchecked errors in the short-term and damaging human capabilities for critical thinking in the long-term, particularly in knowledge-based tasks. How can we develop scaffolding around language models to curate more appropriate use? We propose selective frictions for language model experiences, inspired by behavioral science interventions, to dampen misuse. Frictions involve small modifications to a user's experience, e.g., the addition of a button impeding model access and reminding a user of their expertise relative to the model. Through a user study with real humans, we observe shifts in user behavior from the imposition of a friction over LLMs in the context of a multi-topic question-answering task as a representative task that people may use LLMs for, e.g., in education and information retrieval. We find that frictions modulate over-reliance by driving down users' click rates while minimally affecting accuracy for those topics. Yet, frictions may have unintended effects. We find marked differences in users' click behaviors even on topics where frictions were not provisioned. Our contributions motivate further study of human-AI behavioral interaction to inform more effective and appropriate LLM use.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055306799",
                    "name": "Katherine M. Collins"
                },
                {
                    "authorId": "15110752",
                    "name": "Valerie Chen"
                },
                {
                    "authorId": "2226897111",
                    "name": "Ilia Sucholutsky"
                },
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "2240021256",
                    "name": "Malak Sadek"
                },
                {
                    "authorId": "2161835433",
                    "name": "Holli Sargeant"
                },
                {
                    "authorId": "145532827",
                    "name": "Ameet Talwalkar"
                },
                {
                    "authorId": "2267117825",
                    "name": "Adrian Weller"
                },
                {
                    "authorId": "32326200",
                    "name": "Umang Bhatt"
                }
            ]
        },
        {
            "paperId": "299cbb7e4c7e676e81d87d28efb8701df8892ec4",
            "title": "LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages",
            "abstract": "In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242554313",
                    "name": "Andrew M. Bean"
                },
                {
                    "authorId": "2305621214",
                    "name": "Simi Hellsten"
                },
                {
                    "authorId": "2305622619",
                    "name": "Harry Mayne"
                },
                {
                    "authorId": "2305622172",
                    "name": "Jabez Magomere"
                },
                {
                    "authorId": "2253469028",
                    "name": "Ethan A. Chi"
                },
                {
                    "authorId": "2305623222",
                    "name": "Ryan Chi"
                },
                {
                    "authorId": "1741886127",
                    "name": "Scott A. Hale"
                },
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                }
            ]
        },
        {
            "paperId": "39fd3d41f5ab882eea29dbe27eef8d0954b29856",
            "title": "PERSONA: A Reproducible Testbed for Pluralistic Alignment",
            "abstract": "The rapid advancement of language models (LMs) necessitates robust alignment with diverse user values. However, current preference optimization approaches often fail to capture the plurality of user opinions, instead reinforcing majority viewpoints and marginalizing minority perspectives. We introduce PERSONA, a reproducible test bed designed to evaluate and improve pluralistic alignment of LMs. We procedurally generate diverse user profiles from US census data, resulting in 1,586 synthetic personas with varied demographic and idiosyncratic attributes. We then generate a large-scale evaluation dataset containing 3,868 prompts and 317,200 feedback pairs obtained from our synthetic personas. Leveraging this dataset, we systematically evaluate LM capabilities in role-playing diverse users, verified through human judges, and the establishment of both a benchmark, PERSONA Bench, for pluralistic alignment approaches as well as an extensive dataset to create new and future benchmarks. The full dataset and benchmarks are available here: https://www.synthlabs.ai/research/persona.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1486307451",
                    "name": "Yuntao Bai"
                },
                {
                    "authorId": "2149890773",
                    "name": "Andy Jones"
                },
                {
                    "authorId": "1978097132",
                    "name": "Kamal Ndousse"
                },
                {
                    "authorId": "2314125309",
                    "name": "Anna Askell"
                },
                {
                    "authorId": "2314226831",
                    "name": "Dawn Chen"
                },
                {
                    "authorId": "2314165568",
                    "name": "Stanislav Drain"
                },
                {
                    "authorId": "2314164302",
                    "name": "Fort"
                },
                {
                    "authorId": "3422038",
                    "name": "Su Lin Blodgett"
                },
                {
                    "authorId": "2881033",
                    "name": "Solon Barocas"
                },
                {
                    "authorId": "1722360",
                    "name": "Hal Daum\u00e9"
                },
                {
                    "authorId": "28933528",
                    "name": "Louis Castricato"
                },
                {
                    "authorId": "2283848553",
                    "name": "Nathan Lile"
                },
                {
                    "authorId": "2283849116",
                    "name": "Suraj Anand"
                },
                {
                    "authorId": "49081354",
                    "name": "Souradip Chakraborty"
                },
                {
                    "authorId": "2279349525",
                    "name": "Jiahao Qiu"
                },
                {
                    "authorId": "2314547939",
                    "name": "Hui Yuan"
                },
                {
                    "authorId": "2314164789",
                    "name": "Alec Kop-681"
                },
                {
                    "authorId": "2261325066",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2271083353",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "2314328839",
                    "name": "Amrit Singh"
                },
                {
                    "authorId": "2314166201",
                    "name": "Bedi Mengdi"
                },
                {
                    "authorId": "2314301347",
                    "name": "Wang"
                },
                {
                    "authorId": "2314166019",
                    "name": "Maxmin-rlhf"
                },
                {
                    "authorId": "2307483237",
                    "name": "Jiangjie Chen"
                },
                {
                    "authorId": "2314125810",
                    "name": "Xintao Wang"
                },
                {
                    "authorId": "2281767825",
                    "name": "Rui Xu"
                },
                {
                    "authorId": null,
                    "name": "Siyu Yuan"
                },
                {
                    "authorId": "2295591951",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2314182212",
                    "name": "Jian Shi"
                },
                {
                    "authorId": "2314481299",
                    "name": "Shuang Xie"
                },
                {
                    "authorId": "2314515221",
                    "name": "Ruihan Li"
                },
                {
                    "authorId": "2299964096",
                    "name": "Yang"
                },
                {
                    "authorId": "2314129314",
                    "name": "Tinghui Zhu"
                },
                {
                    "authorId": "2298944454",
                    "name": "Aili Chen"
                },
                {
                    "authorId": "2296503181",
                    "name": "Nianqi Li"
                },
                {
                    "authorId": "2314165226",
                    "name": "Caiyu Lida Chen"
                },
                {
                    "authorId": "2314331628",
                    "name": "Siye Hu"
                },
                {
                    "authorId": "2314334726",
                    "name": "Scott Wu"
                },
                {
                    "authorId": "2314698693",
                    "name": "Ziquan Ren"
                },
                {
                    "authorId": "2314239147",
                    "name": "Yanghua Fu"
                },
                {
                    "authorId": "52297757",
                    "name": "Ganqu Cui"
                },
                {
                    "authorId": null,
                    "name": "Lifan Yuan"
                },
                {
                    "authorId": "2275678413",
                    "name": "Ning Ding"
                },
                {
                    "authorId": "2253396591",
                    "name": "Guanming Yao"
                },
                {
                    "authorId": "2310046098",
                    "name": "Wei Zhu"
                },
                {
                    "authorId": "2277008608",
                    "name": "Yuan Ni"
                },
                {
                    "authorId": "2239568222",
                    "name": "Guotong Xie"
                },
                {
                    "authorId": "2304901890",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2314165560",
                    "name": "Amanda Schiefer"
                },
                {
                    "authorId": "2314125311",
                    "name": "Anton Askell"
                },
                {
                    "authorId": "2314166045",
                    "name": "Bakhtin"
                },
                {
                    "authorId": "2314164340",
                    "name": "Carol"
                },
                {
                    "authorId": "2314320479",
                    "name": "Zac Chen"
                },
                {
                    "authorId": "2314160483",
                    "name": "Danny Hatfield-Dodds"
                },
                {
                    "authorId": "2314150258",
                    "name": "Hernandez"
                },
                {
                    "authorId": "2278437215",
                    "name": "Jan-Philipp Fr\u00e4nken"
                },
                {
                    "authorId": "2262214987",
                    "name": "Sam Kwok"
                },
                {
                    "authorId": "2262215242",
                    "name": "Peixuan Ye"
                },
                {
                    "authorId": "144841994",
                    "name": "Kanishk Gandhi"
                },
                {
                    "authorId": "2240292858",
                    "name": "Dilip Arumugam"
                },
                {
                    "authorId": "2262434646",
                    "name": "Jared Moore"
                },
                {
                    "authorId": "88726969",
                    "name": "Alex Tamkin"
                },
                {
                    "authorId": "2264218532",
                    "name": "Tobias Gerstenberg"
                },
                {
                    "authorId": "49456763",
                    "name": "E. Zelikman"
                },
                {
                    "authorId": "102801230",
                    "name": "Rafael Rafailov"
                },
                {
                    "authorId": "144841994",
                    "name": "Kanishk Gandhi"
                },
                {
                    "authorId": "2314161869",
                    "name": "Noah D Goodman. 2024"
                },
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "2298272188",
                    "name": "Alexander Whitefield"
                },
                {
                    "authorId": "2215330658",
                    "name": "Paul R\u00f6ttger"
                },
                {
                    "authorId": "2242554313",
                    "name": "Andrew M. Bean"
                },
                {
                    "authorId": "2298277004",
                    "name": "Katerina Margatina"
                },
                {
                    "authorId": "2273741123",
                    "name": "Juan Ciro"
                },
                {
                    "authorId": "2177337955",
                    "name": "Rafael Mosquera"
                },
                {
                    "authorId": "2267728360",
                    "name": "Max Bartolo"
                },
                {
                    "authorId": "2297757196",
                    "name": "Adina Williams"
                },
                {
                    "authorId": "2304745643",
                    "name": "Yaswanth Chittepu"
                },
                {
                    "authorId": "2293725656",
                    "name": "Ryan Park"
                },
                {
                    "authorId": "51521430",
                    "name": "Harshit S. Sikchi"
                },
                {
                    "authorId": "2122700519",
                    "name": "Joey Hejna"
                },
                {
                    "authorId": "2304745596",
                    "name": "Bradley Knox"
                },
                {
                    "authorId": "2284774407",
                    "name": "Chelsea Finn"
                },
                {
                    "authorId": "2187928231",
                    "name": "Shyam Sundhar Ramesh"
                },
                {
                    "authorId": "2295021245",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "2240522628",
                    "name": "Iason Chaimalas"
                },
                {
                    "authorId": "2142461728",
                    "name": "Luca Rettenberger"
                },
                {
                    "authorId": "2243431570",
                    "name": "Markus Reischl"
                },
                {
                    "authorId": "7232294",
                    "name": "Mark Schutera"
                },
                {
                    "authorId": "2852106",
                    "name": "Shibani Santurkar"
                },
                {
                    "authorId": "41152329",
                    "name": "Esin Durmus"
                },
                {
                    "authorId": "8759332",
                    "name": "Faisal Ladhak"
                },
                {
                    "authorId": "2314299381",
                    "name": "Cinoo Lee"
                },
                {
                    "authorId": "2271898270",
                    "name": "Percy Liang"
                },
                {
                    "authorId": "2253575091",
                    "name": "Tatsunori Hashimoto"
                },
                {
                    "authorId": "2314160040",
                    "name": "Omar Shaikh"
                },
                {
                    "authorId": "2244295379",
                    "name": "Michelle Lam"
                },
                {
                    "authorId": "2305603644",
                    "name": "Yijia Shao"
                },
                {
                    "authorId": "1413775966",
                    "name": "Anand Siththaranjan"
                },
                {
                    "authorId": null,
                    "name": "Cassidy Laidlaw"
                },
                {
                    "authorId": "122436831",
                    "name": "Taylor Sorensen"
                },
                {
                    "authorId": "33772445",
                    "name": "Jillian R. Fisher"
                },
                {
                    "authorId": "2283139413",
                    "name": "Mitchell Gordon"
                },
                {
                    "authorId": "2254272878",
                    "name": "Niloofar Mireshghallah"
                },
                {
                    "authorId": "2073457922",
                    "name": "Michael Rytting"
                },
                {
                    "authorId": "2265491697",
                    "name": "Andre Ye"
                },
                {
                    "authorId": "2112504145",
                    "name": "Liwei Jiang"
                },
                {
                    "authorId": "50085131",
                    "name": "Ximing Lu"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "2316432499",
                    "name": "Tim Althoff"
                },
                {
                    "authorId": "1387983862",
                    "name": "Nisan Stiennon"
                },
                {
                    "authorId": "2257278245",
                    "name": "Ouyang Long"
                },
                {
                    "authorId": "2314334380",
                    "name": "Jeffrey Wu"
                },
                {
                    "authorId": "2070716013",
                    "name": "Ryan Ziegler"
                },
                {
                    "authorId": "2314165049",
                    "name": "Chelsea Lowe"
                },
                {
                    "authorId": "2314160468",
                    "name": "Alec Voss"
                },
                {
                    "authorId": "2289348718",
                    "name": "Radford"
                },
                {
                    "authorId": "2698777",
                    "name": "Dario Amodei"
                },
                {
                    "authorId": "2314162040",
                    "name": "Christiano. 2020. Learn-860"
                },
                {
                    "authorId": "2118831287",
                    "name": "Chenkai Sun"
                },
                {
                    "authorId": "2314167682",
                    "name": "Revanth Ke Yang"
                },
                {
                    "authorId": "2314168484",
                    "name": "Gangi Reddy"
                },
                {
                    "authorId": "2261082492",
                    "name": "Hou Pong Chan"
                },
                {
                    "authorId": "2261082008",
                    "name": "Chengxiang Zhai"
                }
            ]
        },
        {
            "paperId": "5bd44a34457d3f323eea4d961dd762003be3961d",
            "title": "Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models",
            "abstract": "Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2043232919",
                    "name": "Paul R\u00f6ttger"
                },
                {
                    "authorId": "1667898858",
                    "name": "Valentin Hofmann"
                },
                {
                    "authorId": "2287831986",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "2287831755",
                    "name": "Musashi Hinck"
                },
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "2261745622",
                    "name": "Hinrich Schutze"
                },
                {
                    "authorId": "2267334203",
                    "name": "Dirk Hovy"
                }
            ]
        },
        {
            "paperId": "60eb887f38b100d462be2f59d32676ec16bb4681",
            "title": "The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models",
            "abstract": "Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts. We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "2298272188",
                    "name": "Alexander Whitefield"
                },
                {
                    "authorId": "2298277333",
                    "name": "Paul Rottger"
                },
                {
                    "authorId": "2242554313",
                    "name": "Andrew M. Bean"
                },
                {
                    "authorId": "2298277004",
                    "name": "Katerina Margatina"
                },
                {
                    "authorId": "2273741123",
                    "name": "Juan Ciro"
                },
                {
                    "authorId": "2177337955",
                    "name": "Rafael Mosquera"
                },
                {
                    "authorId": "2267728360",
                    "name": "Max Bartolo"
                },
                {
                    "authorId": "2297757196",
                    "name": "Adina Williams"
                },
                {
                    "authorId": "2298416563",
                    "name": "He He"
                },
                {
                    "authorId": "2737827",
                    "name": "Bertie Vidgen"
                },
                {
                    "authorId": "1741886127",
                    "name": "Scott A. Hale"
                }
            ]
        },
        {
            "paperId": "9775bb6870d0c80881487be57de0a0f31cea08be",
            "title": "Introducing v0.5 of the AI Safety Benchmark from MLCommons",
            "abstract": "This paper introduces v0.5 of the AI Safety Benchmark, which has been created by the MLCommons AI Safety Working Group. The AI Safety Benchmark has been designed to assess the safety risks of AI systems that use chat-tuned language models. We introduce a principled approach to specifying and constructing the benchmark, which for v0.5 covers only a single use case (an adult chatting to a general-purpose assistant in English), and a limited set of personas (i.e., typical users, malicious users, and vulnerable users). We created a new taxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark. We plan to release version 1.0 of the AI Safety Benchmark by the end of 2024. The v1.0 benchmark will provide meaningful insights into the safety of AI systems. However, the v0.5 benchmark should not be used to assess the safety of AI systems. We have sought to fully document the limitations, flaws, and challenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes (1) a principled approach to specifying and constructing the benchmark, which comprises use cases, types of systems under test (SUTs), language and context, personas, tests, and test items; (2) a taxonomy of 13 hazard categories with definitions and subcategories; (3) tests for seven of the hazard categories, each comprising a unique set of test items, i.e., prompts. There are 43,090 test items in total, which we created with templates; (4) a grading system for AI systems against the benchmark; (5) an openly available platform, and downloadable tool, called ModelBench that can be used to evaluate the safety of AI systems on the benchmark; (6) an example evaluation report which benchmarks the performance of over a dozen openly available chat-tuned language models; (7) a test specification for the benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2737827",
                    "name": "Bertie Vidgen"
                },
                {
                    "authorId": "2297190740",
                    "name": "Adarsh Agrawal"
                },
                {
                    "authorId": "2297807964",
                    "name": "Ahmed M. Ahmed"
                },
                {
                    "authorId": "2257002651",
                    "name": "Victor Akinwande"
                },
                {
                    "authorId": "2023118917",
                    "name": "Namir Al-nuaimi"
                },
                {
                    "authorId": "2124768",
                    "name": "Najla Alfaraj"
                },
                {
                    "authorId": "102661476",
                    "name": "Elie Alhajjar"
                },
                {
                    "authorId": "2257256357",
                    "name": "Lora Aroyo"
                },
                {
                    "authorId": "2297187181",
                    "name": "Trupti Bavalatti"
                },
                {
                    "authorId": "1517663067",
                    "name": "Borhane Blili-Hamelin"
                },
                {
                    "authorId": "1742448",
                    "name": "K. Bollacker"
                },
                {
                    "authorId": "2297187179",
                    "name": "Rishi Bomassani"
                },
                {
                    "authorId": "2498618",
                    "name": "Marisa Ferrara Boston"
                },
                {
                    "authorId": "2297188715",
                    "name": "Sim'eon Campos"
                },
                {
                    "authorId": "2297188195",
                    "name": "Kal Chakra"
                },
                {
                    "authorId": "2163546329",
                    "name": "Canyu Chen"
                },
                {
                    "authorId": "2091029496",
                    "name": "Cody Coleman"
                },
                {
                    "authorId": "2297187212",
                    "name": "Zacharie Delpierre Coudert"
                },
                {
                    "authorId": "113320522",
                    "name": "Leon Derczynski"
                },
                {
                    "authorId": "2267726934",
                    "name": "Debojyoti Dutta"
                },
                {
                    "authorId": "2297188410",
                    "name": "Ian Eisenberg"
                },
                {
                    "authorId": "46449010",
                    "name": "J. Ezick"
                },
                {
                    "authorId": "2238786311",
                    "name": "Heather Frase"
                },
                {
                    "authorId": "2223748737",
                    "name": "Brian Fuller"
                },
                {
                    "authorId": "2297187207",
                    "name": "Ram Gandikota"
                },
                {
                    "authorId": "2199260209",
                    "name": "Agasthya Gangavarapu"
                },
                {
                    "authorId": "1972481155",
                    "name": "Ananya Gangavarapu"
                },
                {
                    "authorId": "9250608",
                    "name": "J. Gealy"
                },
                {
                    "authorId": "2213553962",
                    "name": "Rajat Ghosh"
                },
                {
                    "authorId": "2297187194",
                    "name": "James Goel"
                },
                {
                    "authorId": "1386345955",
                    "name": "Usman Gohar"
                },
                {
                    "authorId": "2297188541",
                    "name": "Sujata Goswami"
                },
                {
                    "authorId": "1741886127",
                    "name": "Scott A. Hale"
                },
                {
                    "authorId": "2297187154",
                    "name": "Wiebke Hutiri"
                },
                {
                    "authorId": "151472158",
                    "name": "Joseph Marvin Imperial"
                },
                {
                    "authorId": "2230701705",
                    "name": "Surgan Jandial"
                },
                {
                    "authorId": "2294772116",
                    "name": "Nicholas C. Judd"
                },
                {
                    "authorId": "1389940060",
                    "name": "Felix Juefei-Xu"
                },
                {
                    "authorId": "1703493",
                    "name": "Foutse Khomh"
                },
                {
                    "authorId": "2208575928",
                    "name": "B. Kailkhura"
                },
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "2260341611",
                    "name": "Kevin Klyman"
                },
                {
                    "authorId": "2297186980",
                    "name": "Chris Knotz"
                },
                {
                    "authorId": "52176720",
                    "name": "Michael Kuchnik"
                },
                {
                    "authorId": "2109680564",
                    "name": "Shachi H. Kumar"
                },
                {
                    "authorId": "2297188456",
                    "name": "Chris Lengerich"
                },
                {
                    "authorId": "2296831558",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2284224390",
                    "name": "Zeyi Liao"
                },
                {
                    "authorId": "2297188657",
                    "name": "E. Long"
                },
                {
                    "authorId": "2297188789",
                    "name": "Victor Lu"
                },
                {
                    "authorId": "2054708905",
                    "name": "Yifan Mai"
                },
                {
                    "authorId": "46213894",
                    "name": "P. Mammen"
                },
                {
                    "authorId": "2297187982",
                    "name": "Kelvin Manyeki"
                },
                {
                    "authorId": "2297187225",
                    "name": "Sean McGregor"
                },
                {
                    "authorId": "2130578944",
                    "name": "Virendra Mehta"
                },
                {
                    "authorId": "2297825575",
                    "name": "Shafee Mohammed"
                },
                {
                    "authorId": "2297187017",
                    "name": "Emanuel Moss"
                },
                {
                    "authorId": "1896095",
                    "name": "L. Nachman"
                },
                {
                    "authorId": "2297187185",
                    "name": "Dinesh Jinenhally Naganna"
                },
                {
                    "authorId": "2076564",
                    "name": "Amin Nikanjam"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "1594025351",
                    "name": "Luis Oala"
                },
                {
                    "authorId": "2297187978",
                    "name": "Iftach Orr"
                },
                {
                    "authorId": "2265756309",
                    "name": "Alicia Parrish"
                },
                {
                    "authorId": "3259057",
                    "name": "\u00c7igdem Patlak"
                },
                {
                    "authorId": "2424199",
                    "name": "William Pietri"
                },
                {
                    "authorId": "1405364889",
                    "name": "Forough Poursabzi-Sangdeh"
                },
                {
                    "authorId": "6072807",
                    "name": "Eleonora Presani"
                },
                {
                    "authorId": "2296442894",
                    "name": "Fabrizio Puletti"
                },
                {
                    "authorId": "2043232919",
                    "name": "Paul R\u00f6ttger"
                },
                {
                    "authorId": "38531701",
                    "name": "Saurav Sahay"
                },
                {
                    "authorId": "2297431493",
                    "name": "Tim Santos"
                },
                {
                    "authorId": "1742339548",
                    "name": "Nino Scherrer"
                },
                {
                    "authorId": "47540766",
                    "name": "Alice Schoenauer Sebag"
                },
                {
                    "authorId": "40896023",
                    "name": "P. Schramowski"
                },
                {
                    "authorId": "2297187191",
                    "name": "Abolfazl Shahbazi"
                },
                {
                    "authorId": "2297338785",
                    "name": "Vin Sharma"
                },
                {
                    "authorId": "2144058688",
                    "name": "Xudong Shen"
                },
                {
                    "authorId": "2297188681",
                    "name": "Vamsi Sistla"
                },
                {
                    "authorId": "2297445874",
                    "name": "Leonard Tang"
                },
                {
                    "authorId": "2273657478",
                    "name": "Davide Testuggine"
                },
                {
                    "authorId": "51153332",
                    "name": "Vithursan Thangarasa"
                },
                {
                    "authorId": "29830026",
                    "name": "E. A. Watkins"
                },
                {
                    "authorId": "2297190630",
                    "name": "Rebecca Weiss"
                },
                {
                    "authorId": "2062396538",
                    "name": "Christoper A. Welty"
                },
                {
                    "authorId": "2297188261",
                    "name": "Tyler Wilbers"
                },
                {
                    "authorId": "2297757196",
                    "name": "Adina Williams"
                },
                {
                    "authorId": "2293742452",
                    "name": "Carole-Jean Wu"
                },
                {
                    "authorId": "2297189809",
                    "name": "Poonam Yadav"
                },
                {
                    "authorId": "2145170944",
                    "name": "Xianjun Yang"
                },
                {
                    "authorId": "2297444637",
                    "name": "Yi Zeng"
                },
                {
                    "authorId": "2297247783",
                    "name": "Wenhui Zhang"
                },
                {
                    "authorId": "2297188641",
                    "name": "Fedor Zhdanov"
                },
                {
                    "authorId": "2297319602",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "2260342171",
                    "name": "Percy Liang"
                },
                {
                    "authorId": "2065823421",
                    "name": "Peter Mattson"
                },
                {
                    "authorId": "1717534",
                    "name": "J. Vanschoren"
                }
            ]
        },
        {
            "paperId": "f568a7bf423121780a552d6819ca6623112578f7",
            "title": "Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation",
            "abstract": "With text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on \u201cimplicitly adversarial\u201d prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. We present an in-depth account of our methodology, a systematic study of novel attack strategies and safety failures, and a visualization tool for easy exploration of the dataset. The first challenge round resulted in over 10k prompt-image pairs with machine annotations for safety. A subset of 1.5k samples contains rich human annotations of harm types and attack styles. Our findings emphasize the necessity of continual auditing and adaptation as new vulnerabilities emerge. This work will enable proactive, iterative safety assessments and promote responsible development of T2I models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261168311",
                    "name": "Jessica Quaye"
                },
                {
                    "authorId": "2265756309",
                    "name": "Alicia Parrish"
                },
                {
                    "authorId": "2692909",
                    "name": "Oana Inel"
                },
                {
                    "authorId": "31211315",
                    "name": "Charvi Rastogi"
                },
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "1768057",
                    "name": "Minsuk Kahng"
                },
                {
                    "authorId": "2283841573",
                    "name": "Erin van Liemt"
                },
                {
                    "authorId": "2267728360",
                    "name": "Max Bartolo"
                },
                {
                    "authorId": "2292201640",
                    "name": "Jess Tsang"
                },
                {
                    "authorId": "2292884342",
                    "name": "Justin White"
                },
                {
                    "authorId": "2292198190",
                    "name": "Nathan Clement"
                },
                {
                    "authorId": "2177337955",
                    "name": "Rafael Mosquera"
                },
                {
                    "authorId": "2273741123",
                    "name": "Juan Ciro"
                },
                {
                    "authorId": "1805668",
                    "name": "V. Reddi"
                },
                {
                    "authorId": "2257256357",
                    "name": "Lora Aroyo"
                }
            ]
        },
        {
            "paperId": "066041e81878d4ac852be3c3d625a7de9f8b9fce",
            "title": "Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models",
            "abstract": "The generative AI revolution in recent years has been spurred by an expansion in compute power and data quantity, which together enable extensive pre-training of powerful text-to-image (T2I) models. With their greater capabilities to generate realistic and creative content, these T2I models like DALL-E, MidJourney, Imagen or Stable Diffusion are reaching ever wider audiences. Any unsafe behaviors inherited from pretraining on uncurated internet-scraped datasets thus have the potential to cause wide-reaching harm, for example, through generated images which are violent, sexually explicit, or contain biased and derogatory stereotypes. Despite this risk of harm, we lack systematic and structured evaluation datasets to scrutinize model behavior, especially adversarial attacks that bypass existing safety filters. A typical bottleneck in safety evaluation is achieving a wide coverage of different types of challenging examples in the evaluation set, i.e., identifying 'unknown unknowns' or long-tail problems. To address this need, we introduce the Adversarial Nibbler challenge. The goal of this challenge is to crowdsource a diverse set of failure modes and reward challenge participants for successfully finding safety vulnerabilities in current state-of-the-art T2I models. Ultimately, we aim to provide greater awareness of these issues and assist developers in improving the future safety and reliability of generative AI models. Adversarial Nibbler is a data-centric challenge, part of the DataPerf challenge suite, organized and supported by Kaggle and MLCommons.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "119389860",
                    "name": "Alicia Parrish"
                },
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "2261168311",
                    "name": "Jessica Quaye"
                },
                {
                    "authorId": "31211315",
                    "name": "Charvi Rastogi"
                },
                {
                    "authorId": "153408953",
                    "name": "Max Bartolo"
                },
                {
                    "authorId": "2692909",
                    "name": "Oana Inel"
                },
                {
                    "authorId": "2086202631",
                    "name": "Juan Ciro"
                },
                {
                    "authorId": "2177337955",
                    "name": "Rafael Mosquera"
                },
                {
                    "authorId": "1884277902",
                    "name": "Addison Howard"
                },
                {
                    "authorId": "3155742",
                    "name": "William J. Cukierski"
                },
                {
                    "authorId": "1733143",
                    "name": "D. Sculley"
                },
                {
                    "authorId": "1805668",
                    "name": "V. Reddi"
                },
                {
                    "authorId": "1745337",
                    "name": "Lora Aroyo"
                }
            ]
        },
        {
            "paperId": "49f1fa0d609ff06564b46270cbc022b7d9d195f4",
            "title": "Assessing Language Model Deployment with Risk Cards",
            "abstract": "This paper introduces RiskCards, a framework for structured assessment and documentation of risks associated with an application of language models. As with all language, text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text. Prior work establishes a wide variety of language model harms to many different actors: existing taxonomies identify categories of harms posed by language models; benchmarks establish automated tests of these harms; and documentation standards for models, tasks and datasets encourage transparent reporting. However, there is no risk-centric framework for documenting the complexity of a landscape in which some risks are shared across models and contexts, while others are specific, and where certain conditions may be required for risks to manifest as harms. RiskCards address this methodological gap by providing a generic framework for assessing the use of a given language model in a given scenario. Each RiskCard makes clear the routes for the risk to manifest harm, their placement in harm taxonomies, and example prompt-output pairs. While RiskCards are designed to be open-source, dynamic and participatory, we present a\"starter set\"of RiskCards taken from a broad literature survey, each of which details a concrete risk presentation. Language model RiskCards initiate a community knowledge base which permits the mapping of risks and harms to a specific model or its application scenario, ultimately contributing to a better, safer and shared understanding of the risk landscape.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "113320522",
                    "name": "Leon Derczynski"
                },
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "51467955",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "119004240",
                    "name": "M. Leiser"
                },
                {
                    "authorId": "2057036852",
                    "name": "Saif Mohammad"
                }
            ]
        },
        {
            "paperId": "4d658fa16463d789746f45a8240f9409b10d8b0f",
            "title": "SemEval-2023 Task 10: Explainable Detection of Online Sexism",
            "abstract": "Online sexism is a widespread and harmful phenomenon. Automated tools can assist the detection of sexism at scale. Binary detection, however, disregards the diversity of sexist content, and fails to provide clear explanations for why something is sexist. To address this issue, we introduce SemEval Task 10 on the Explainable Detection of Online Sexism (EDOS). We make three main contributions: i) a novel hierarchical taxonomy of sexist content, which includes granular vectors of sexism to aid explainability; ii) a new dataset of 20,000 social media comments with fine-grained labels, along with larger unlabelled datasets for model adaptation; and iii) baseline models as well as an analysis of the methods, results and errors for participant submissions to our task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "48275690",
                    "name": "Wenjie Yin"
                },
                {
                    "authorId": "2737827",
                    "name": "Bertie Vidgen"
                },
                {
                    "authorId": "2043232919",
                    "name": "Paul R\u00f6ttger"
                }
            ]
        }
    ]
}