{
    "authorId": "2256132624",
    "papers": [
        {
            "paperId": "6d9690ab7674d70a3d8e41870186acba7325485b",
            "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM",
            "abstract": "Vision-extended LLMs have made significant strides in Visual Question Answering (VQA). Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses. In this work, we introduce a novel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for entity-centric VQA. This task aims to test the models' capabilities in identifying entities and providing detailed, entity-specific knowledge. We have developed the \\textbf{SnapNTell Dataset}, distinct from traditional VQA datasets: (1) It encompasses a wide range of categorized entities, each represented by images and explicitly named in the answers; (2) It features QA pairs that require extensive knowledge for accurate responses. The dataset is organized into 22 major categories, containing 7,568 unique entities in total. For each entity, we curated 10 illustrative images and crafted 10 knowledge-intensive QA pairs. To address this novel task, we devised a scalable, efficient, and transparent retrieval-augmented multimodal LLM. Our approach markedly outperforms existing methods on the SnapNTell dataset, achieving a 66.5\\% improvement in the BELURT score. We will soon make the dataset and the source code publicly accessible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267397829",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "2111680936",
                    "name": "Andrea Madotto"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2290243809",
                    "name": "Y. Xu"
                },
                {
                    "authorId": "2215596266",
                    "name": "Xin Luna Dong"
                },
                {
                    "authorId": "2290179598",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "2290642943",
                    "name": "Lei Li"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                }
            ]
        },
        {
            "paperId": "723a6340ee641e190b22bb47455d05e3b4237179",
            "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
            "abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at https://github.com/facebookresearch/FnCTOD",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2284639678",
                    "name": "Zhiyu Zoey Chen"
                },
                {
                    "authorId": "2284594951",
                    "name": "Mike Ross"
                },
                {
                    "authorId": "2287921432",
                    "name": "Patrick Huber"
                },
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "2215596266",
                    "name": "Xin Luna Dong"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                }
            ]
        },
        {
            "paperId": "b34d7ae7a9238258a049099bf66bdefa9450e0c1",
            "title": "Doppelg\\\"anger's Watch: A Split Objective Approach to Large Language Models",
            "abstract": "In this paper, we investigate the problem of\"generation supervision\"in large language models, and present a novel bicameral architecture to separate supervision signals from their core capability, helpfulness. Doppelg\\\"anger, a new module parallel to the underlying language model, supervises the generation of each token, and learns to concurrently predict the supervision score(s) of the sequences up to and including each token. In this work, we present the theoretical findings, and leave the report on experimental results to a forthcoming publication.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2709180",
                    "name": "S. Ghasemlou"
                },
                {
                    "authorId": "2320456971",
                    "name": "Ashish Katiyar"
                },
                {
                    "authorId": "51912276",
                    "name": "Aparajita Saraf"
                },
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2320456906",
                    "name": "Mangesh Pujari"
                },
                {
                    "authorId": "2320457185",
                    "name": "Pinar Donmez"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "2320593762",
                    "name": "Anuj Kumar"
                }
            ]
        },
        {
            "paperId": "36d369fd77f01f37347aa13c835c4210c8a6eb67",
            "title": "IMU2CLIP: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning",
            "abstract": "We present IMU2CLIP, a novel pre-training approach to align Inertial Measurement Unit (IMU) motion sensor recordings with text and video, by projecting them into the joint representation space of Contrastive Language-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to translate human motions (as measured by IMU sensors) into their corresponding textual descriptions and videos \u2013 while preserving the transitivity across these modalities. We introduce several new IMU-based Wearable AI applications such as motion-based media search, or an LM-based multi-modal reasoning with motion sensor data \u2013 all using text as the grounding platform. In addition, we show that IMU2CLIP significantly improves downstream performances when fine-tuned for each application, demonstrating its universal usage as a new pre-trained resource. Our code and models will be released publicly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2111680936",
                    "name": "Andrea Madotto"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "51912276",
                    "name": "Aparajita Saraf"
                },
                {
                    "authorId": "1927362",
                    "name": "Amy Bearman"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                }
            ]
        },
        {
            "paperId": "f2f9c02a7eb484dd7b7ac46892856e3f278eed77",
            "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
            "abstract": "We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM's capabilities, we fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2111680936",
                    "name": "Andrea Madotto"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "2248184174",
                    "name": "Tushar Nagarajan"
                },
                {
                    "authorId": "2249027674",
                    "name": "Matt Smith"
                },
                {
                    "authorId": "2249741629",
                    "name": "Shashank Jain"
                },
                {
                    "authorId": "2248041474",
                    "name": "Chun-Fu Yeh"
                },
                {
                    "authorId": "2248184913",
                    "name": "Prakash Murugesan"
                },
                {
                    "authorId": "2248176677",
                    "name": "Peyman Heidari"
                },
                {
                    "authorId": "2247965931",
                    "name": "Yue Liu"
                },
                {
                    "authorId": "27693639",
                    "name": "Kavya Srinet"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "2247977368",
                    "name": "Anuj Kumar"
                }
            ]
        }
    ]
}