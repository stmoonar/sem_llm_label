{
    "authorId": "2237585282",
    "papers": [
        {
            "paperId": "2c93e6b1f04887b059b1a3bb714c30d038772a45",
            "title": "Generation is better than Modification: Combating High Class Homophily Variance in Graph Anomaly Detection",
            "abstract": "Graph-based anomaly detection is currently an important research topic in the field of graph neural networks (GNNs). We find that in graph anomaly detection, the homophily distribution differences between different classes are significantly greater than those in homophilic and heterophilic graphs. For the first time, we introduce a new metric called Class Homophily Variance, which quantitatively describes this phenomenon. To mitigate its impact, we propose a novel GNN model named Homophily Edge Generation Graph Neural Network (HedGe). Previous works typically focused on pruning, selecting or connecting on original relationships, and we refer to these methods as modifications. Different from these works, our method emphasizes generating new relationships with low class homophily variance, using the original relationships as an auxiliary. HedGe samples homophily adjacency matrices from scratch using a self-attention mechanism, and leverages nodes that are relevant in the feature space but not directly connected in the original graph. Additionally, we modify the loss function to punish the generation of unnecessary heterophilic edges by the model. Extensive comparison experiments demonstrate that HedGe achieved the best performance across multiple benchmark datasets, including anomaly detection and edgeless node classification. The proposed model also improves the robustness under the novel Heterophily Attack with increased class homophily variance on other graph classification tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292388223",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "2291943365",
                    "name": "Dawei Cheng"
                },
                {
                    "authorId": "2292416817",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "2283836234",
                    "name": "Jie Yang"
                },
                {
                    "authorId": "2275187481",
                    "name": "Ouyang Yi"
                },
                {
                    "authorId": "2290857499",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "38a8d0e5fa83dc823744beab08a62511ad9ad227",
            "title": "Biomedical Entity Linking as Multiple Choice Question Answering",
            "abstract": "Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms state-of-the-art baselines on several datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258562926",
                    "name": "Zhenxi Lin"
                },
                {
                    "authorId": "2030976630",
                    "name": "Ziheng Zhang"
                },
                {
                    "authorId": "2258675923",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "5cdba098f7b91106333008244fd8286d83af229b",
            "title": "Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models",
            "abstract": "Knowledge graph completion (KGC) is a widely used method to tackle incompleteness in knowledge graphs (KGs) by making predictions for missing links. Description-based KGC leverages pre-trained language models to learn entity and relation representations with their names or descriptions, which shows promising results. However, the performance of description-based KGC is still limited by the quality of text and the incomplete structure, as it lacks sufficient entity descriptions and relies solely on relation names, leading to sub-optimal results. To address this issue, we propose MPIKGC, a general framework to compensate for the deficiency of contextualized knowledge and improve KGC by querying large language models (LLMs) from various perspectives, which involves leveraging the reasoning, explanation, and summarization capabilities of LLMs to expand entity descriptions, understand relations, and extract structures, respectively. We conducted extensive evaluation of the effectiveness and improvement of our framework based on four description-based KGC models, for both link prediction and triplet classification tasks. All codes and generated data will be publicly available after review.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262514619",
                    "name": "Derong Xu"
                },
                {
                    "authorId": "2030976630",
                    "name": "Ziheng Zhang"
                },
                {
                    "authorId": "2258562926",
                    "name": "Zhenxi Lin"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2288043255",
                    "name": "Zhihong Zhu"
                },
                {
                    "authorId": "2277237058",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2265580543",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "7c8609b93871c49e2e0cef2a0e11f9ec9b1ce921",
            "title": "Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item",
            "abstract": "Sequential recommendation systems (SRS) serve the purpose of predicting users' subsequent preferences based on their past interactions and have been applied across various domains such as e-commerce and social networking platforms. However, practical SRS encounters challenges due to the fact that most users engage with only a limited number of items, while the majority of items are seldom consumed. These challenges, termed as the long-tail user and long-tail item dilemmas, often create obstacles for traditional SRS methods. Mitigating these challenges is crucial as they can significantly impact user satisfaction and business profitability. While some research endeavors have alleviated these issues, they still grapple with issues such as seesaw or noise stemming from the scarcity of interactions. The emergence of large language models (LLMs) presents a promising avenue to address these challenges from a semantic standpoint. In this study, we introduce the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR), which leverages semantic embeddings from LLMs to enhance SRS performance without increasing computational overhead. To combat the long-tail item challenge, we propose a dual-view modeling approach that fuses semantic information from LLMs with collaborative signals from traditional SRS. To address the long-tail user challenge, we introduce a retrieval augmented self-distillation technique to refine user preference representations by incorporating richer interaction data from similar users. Through comprehensive experiments conducted on three authentic datasets using three widely used SRS models, our proposed enhancement framework demonstrates superior performance compared to existing methodologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2269463602",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2244621655",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "803f8cb9bdd70f11ebb3e34b19e71e2a98137e64",
            "title": "AutoPal: Autonomous Adaptation to Users for Personal AI Companisonship",
            "abstract": "Previous research has demonstrated the potential of AI agents to act as companions that can provide constant emotional support for humans. In this paper, we emphasize the necessity of autonomous adaptation in personal AI companionship, an underexplored yet promising direction. Such adaptability is crucial as it can facilitate more tailored interactions with users and allow the agent to evolve in response to users' changing needs. However, imbuing agents with autonomous adaptability presents unique challenges, including identifying optimal adaptations to meet users' expectations and ensuring a smooth transition during the adaptation process. To address them, we devise a hierarchical framework, AutoPal, that enables controllable and authentic adjustments to the agent's persona based on user interactions. A personamatching dataset is constructed to facilitate the learning of optimal persona adaptations. Extensive experiments demonstrate the effectiveness of AutoPal and highlight the importance of autonomous adaptability in AI companionship.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117232627",
                    "name": "Yi Cheng"
                },
                {
                    "authorId": "2109562691",
                    "name": "Wenge Liu"
                },
                {
                    "authorId": "2119184118",
                    "name": "Kaishuai Xu"
                },
                {
                    "authorId": "150337518",
                    "name": "Wenjun Hou"
                },
                {
                    "authorId": "2275187481",
                    "name": "Ouyang Yi"
                },
                {
                    "authorId": "2257035605",
                    "name": "Chak Tou Leong"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "8f65c1fc631f3bd38a9f184e72088ffb48379c27",
            "title": "Pre-trained Online Contrastive Learning for Insurance Fraud Detection",
            "abstract": "Medical insurance fraud has always been a crucial challenge in the field of healthcare industry. Existing fraud detection models mostly focus on offline learning scenes. However, fraud patterns are constantly evolving, making it difficult for models trained on past data to detect newly emerging fraud patterns, posing a severe challenge in medical fraud detection. Moreover, current incremental learning models are mostly designed to address catastrophic forgetting, but often exhibit suboptimal performance in fraud detection. To address this challenge, this paper proposes an innovative online learning method for medical insurance fraud detection, named POCL. This method combines contrastive learning pre-training with online updating strategies. In the pre-training stage, we leverage contrastive learning pre-training to learn on historical data, enabling deep feature learning and obtaining rich risk representations. In the online learning stage, we adopt a Temporal Memory Aware Synapses online updating strategy, allowing the model to perform incremental learning and optimization based on continuously emerging new data. This ensures timely adaptation to fraud patterns and reduces forgetting of past knowledge. Our model undergoes extensive experiments and evaluations on real-world insurance fraud datasets. The results demonstrate our model has significant advantages in accuracy compared to the state-of-the-art baseline methods, while also exhibiting lower running time and space consumption. Our sources are released at https://github.com/finint/POCL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292388223",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "2291943365",
                    "name": "Dawei Cheng"
                },
                {
                    "authorId": "2290952083",
                    "name": "Jie Yang"
                },
                {
                    "authorId": "2275187481",
                    "name": "Ouyang Yi"
                },
                {
                    "authorId": "2290857499",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2243825234",
                    "name": "Changjun Jiang"
                }
            ]
        },
        {
            "paperId": "b0633ccf235e467c35b963ad012f6b8c54aba19f",
            "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
            "abstract": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262514619",
                    "name": "Derong Xu"
                },
                {
                    "authorId": "2030976630",
                    "name": "Ziheng Zhang"
                },
                {
                    "authorId": "2288043255",
                    "name": "Zhihong Zhu"
                },
                {
                    "authorId": "2258562926",
                    "name": "Zhenxi Lin"
                },
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2258675923",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2277237058",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2265580543",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "c1fc2546b1476b77448e01b9d7d0a50d1bf632d3",
            "title": "Large Language Model Distilling Medication Recommendation Model",
            "abstract": "The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the straightforward integration of LLMs into recommender systems leads to an out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a novel output layer and a refined tuning loss function. Although LLM-based models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector. To mitigate this, we have developed a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model. Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient. To ease the reproducibility of our experiments, we release the implementation code online.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2258675923",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2261673614",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2051691467",
                    "name": "Yuanshao Zhu"
                },
                {
                    "authorId": "2269463602",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2244621655",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "f795f0b7380dbbbe8bd48f0c4505ba0d64155007",
            "title": "Large Language Model Empowered Embedding Generator for Sequential Recommendation",
            "abstract": "Sequential Recommender Systems (SRS) are extensively applied across various domains to predict users' next interaction by modeling their interaction sequences. However, these systems typically grapple with the long-tail problem, where they struggle to recommend items that are less popular. This challenge results in a decline in user discovery and reduced earnings for vendors, negatively impacting the system as a whole. Large Language Model (LLM) has the potential to understand the semantic connections between items, regardless of their popularity, positioning them as a viable solution to this dilemma. In our paper, we present LLMEmb, an innovative technique that harnesses LLM to create item embeddings that bolster the performance of SRS. To align the capabilities of general-purpose LLM with the needs of the recommendation domain, we introduce a method called Supervised Contrastive Fine-Tuning (SCFT). This method involves attribute-level data augmentation and a custom contrastive loss designed to tailor LLM for enhanced recommendation performance. Moreover, we highlight the necessity of incorporating collaborative filtering signals into LLM-generated embeddings and propose Recommendation Adaptation Training (RAT) for this purpose. RAT refines the embeddings to be optimally suited for SRS. The embeddings derived from LLMEmb can be easily integrated with any SRS model, showcasing its practical utility. Extensive experimentation on three real-world datasets has shown that LLMEmb significantly improves upon current methods when applied across different SRS models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240559309",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2211473272",
                    "name": "Wanyu Wang"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2292569421",
                    "name": "Yuanshao Zhu"
                },
                {
                    "authorId": "2261673614",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2244621655",
                    "name": "Feng Tian"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                }
            ]
        },
        {
            "paperId": "feb1bd0efd2ed3a846f9a97e35aedf1e781a647f",
            "title": "MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding",
            "abstract": "With appropriate data selection and training techniques, Large Language Models (LLMs) have demonstrated exceptional success in various medical examinations and multiple-choice questions. However, the application of LLMs in medical dialogue generation-a task more closely aligned with actual medical practice-has been less explored. This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses. In this work, we introduce the Medical dialogue with Knowledge enhancement and clinical Pathway encoding (MedKP) framework, which integrates an external knowledge enhancement module through a medical knowledge graph and an internal clinical pathway encoding via medical entities and physician actions. Evaluated with comprehensive metrics, our experiments on two large-scale, real-world online medical consultation datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines and mitigates the incidence of hallucinations, achieving a new state-of-the-art. Extensive ablation studies further reveal the effectiveness of each component of MedKP. This enhancement advances the development of reliable, automated medical consultation responses using LLMs, thereby broadening the potential accessibility of precise and real-time medical assistance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109412978",
                    "name": "Jiageng Wu"
                },
                {
                    "authorId": "2290857499",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2290952083",
                    "name": "Jie Yang"
                }
            ]
        }
    ]
}