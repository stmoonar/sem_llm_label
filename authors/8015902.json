{
    "authorId": "8015902",
    "papers": [
        {
            "paperId": "2cfeff8d064e843f20876063e9aced668775dd91",
            "title": "Frequency-Spatial Domain Feature Fusion for Spectral Super-Resolution",
            "abstract": "The purpose of spectral super-resolution (SSR) is to reconstruct hyperspectral image (HSI) from RGB image, which significantly reduces the difficulty of acquiring HSI. Most existing SSR methods adopt convolutional neural networks (CNNs) as the basic framework. The capability of CNNs to capture global context is limited, which constrains the performance of SSR. In this paper, we propose a novel frequency-spatial domain feature fusion network (FSDFF) for SSR, which simultaneously learns and fuses the frequency and spatial domain features of HSI. Frequency domain features can reflect the global information of image, which can be used to obtain the global context of HSI, thereby alleviating the limitations of CNNs in capturing global context. Spatial domain features contain abundant local structural information, which is beneficial for preserving spatial details in the SSR task. The mutual fusion of the two can better model the interrelationship between HSI and RGB image, thereby achieving better SSR performance. In FSDFF, we design a frequency domain feature learning branch (FDFL) and a spatial domain feature learning branch (SDFL) to learn the frequency and spatial domain features of HSI. Furthermore, a cross-domain feature fusion module (CDFF) is designed to facilitate the complementary fusion of the two types of features. The experimental results on two public datasets indicate that FSDFF has achieved state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2295772524",
                    "name": "Lishan Tan"
                },
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                },
                {
                    "authorId": "2238918156",
                    "name": "Shutao Li"
                },
                {
                    "authorId": "2218550174",
                    "name": "Jinyang Liu"
                }
            ]
        },
        {
            "paperId": "32945683b01cc5a1650cbef8d4823b5b8615951d",
            "title": "PAPS: Progressive Attention-Based Pan-sharpening",
            "abstract": "Pan-sharpening aims to seek high-resolution multi-spectral (HRMS) images from paired multispectral images of low resolution (LRMS) and panchromatic (PAN) images, the key to which is how to maximally integrate spatial and spectral information from PAN and LRMS images. Following the principle of gradual advance, this paper designs a novel network that contains two main logical functions, i.e., detail enhancement and progressive fusion, to solve the problem. More specifically, the detail enhancement module attempts to produce enhanced MS results with the same spatial sizes as corresponding PAN images, which are of higher quality than directly up-sampling LRMS images. Having a better MS base (enhanced MS) and its PAN, we progressively extract information from the PAN and enhanced MS images, expecting to capture pivotal and complementary information of the two modalities for the purpose of constructing the desired HRMS. Extensive experiments together with ablation studies on widely-used datasets are provided to verify the efficacy of our design, and demonstrate its superiority over other state-of-the-art methods both quantitatively and qualitatively. Our code has been released at https://github.com/JiaYN1/PAPS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47416894",
                    "name": "Yanan Jia"
                },
                {
                    "authorId": "2087008409",
                    "name": "Qiming Hu"
                },
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                },
                {
                    "authorId": "2173256903",
                    "name": "Jiayi Ma"
                },
                {
                    "authorId": "46909769",
                    "name": "Xiaojie Guo"
                }
            ]
        },
        {
            "paperId": "48afb16a2483bb07e299968d996fca2ff18abcbc",
            "title": "Focus Relationship Perception for Unsupervised Multi-Focus Image Fusion",
            "abstract": "Multi-focus image fusion can extract the focus regions from different source images and combine them into a fully clear image. Existing unsupervised methods typically use gradient information to measure the focus regions in images and generate a fusion weight map, but ordinary gradient operators are difficult to measure information accurately in regions with weaker textures. In addition, using only gradient information as a constraint cannot make the model fully distinguish all the focus regions in the image, which seriously restricts the clarity of the fusion image. To address these issues, a novel unsupervised multi-focus image fusion method is proposed in this paper. Specifically, a neighborhood information fusion network is designed to generate an initial fusion weight map. It can capture features within different neighborhood ranges at once, which enhances the information association between different regions. In addition, to further improve the feature extraction ability of the model in the regions with low texture information, a local difference evaluation loss function is proposed. It is combined with the gradient measure loss function to constrain the network. Finally, a fusion weight optimization module is proposed to improve the clarity of the fusion image in the repeated defocusing regions and overexposed regions of different source images, which redistributes the weights of different source images. The proposed fusion method is compared with advanced methods on three public multi-focus datasets. Experimental results indicate that the proposed method has achieved better performance in qualitative and quantitative aspects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218550174",
                    "name": "Jinyang Liu"
                },
                {
                    "authorId": "2238918156",
                    "name": "Shutao Li"
                },
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                },
                {
                    "authorId": "2218933029",
                    "name": "Ze Song"
                }
            ]
        },
        {
            "paperId": "753264a90823a4cd59ce3a2784cd4e6a45020162",
            "title": "MDENet: Multidomain Differential Excavating Network for Remote Sensing Image Change Detection",
            "abstract": "Remote sensing image change detection can analyze alterations on the Earth\u2019s surface within a specific region. However, the accuracy of change detection has consistently been hindered by the style differences in captured images caused by seasonal or lighting variations, as well as the challenge of distinguishing similar features between the background and foreground in the scene. To this end, a multidomain differential excavating network (MDENet) for change detection is introduced. Using the novel multidomain differential collaboration module (MDCM) to precisely capture object features on the frequency and spatial domains across diverse temporal domains, it enables simultaneous querying of global and local change information. Moreover, the multineighborhood frequency gate attention (MFGatt) is devised to eliminate the impact of image style relevance information and consolidate attention toward object localization, thereby enhancing the adaptability of the network to variations in image style. Extensive experiments have illustrated that our proposed network achieves better detection accuracy compared with current state-of-the-art (SOTA) methods on various datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218550174",
                    "name": "Jinyang Liu"
                },
                {
                    "authorId": "2238918156",
                    "name": "Shutao Li"
                },
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                },
                {
                    "authorId": "2218933029",
                    "name": "Ze Song"
                },
                {
                    "authorId": "2457540",
                    "name": "Xudong Kang"
                }
            ]
        },
        {
            "paperId": "787ab0b1139369da92a2bd1a49d2a9c4b28fb29d",
            "title": "Physics-Inspired Degradation Models for Hyperspectral Image Fusion",
            "abstract": "The fusion of a low-spatial-resolution hyperspectral image (LR-HSI) with a high-spatial-resolution multispectral image (HR-MSI) has garnered increasing research interest. However, most fusion methods solely focus on the fusion algorithm itself and overlook the degradation models, which results in unsatisfactory performance in practical scenarios. To fill this gap, we propose physics-inspired degradation models (PIDM) to model the degradation of LR-HSI and HR-MSI, which comprises a spatial degradation network (SpaDN) and a spectral degradation network (SpeDN). SpaDN and SpeDN are designed based on two insights. First, we employ spatial warping and spectral modulation operations to simulate lens aberrations, thereby introducing non-uniformity into the spatial and spectral degradation processes. Second, we utilize asymmetric downsampling and parallel downsampling operations to separately reduce the spatial and spectral resolutions of the images, thus ensuring the matching of spatial and spectral degradation processes with specific physical characteristics. Once SpaDN and SpeDN are established, we adopt a self-supervised training strategy to optimize the network parameters and provide a plug-and-play solution for fusion methods. Comprehensive experiments demonstrate that our proposed PIDM can boost the fusion performance of existing fusion methods in practical scenarios.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282535381",
                    "name": "Jie Lian"
                },
                {
                    "authorId": "2258671866",
                    "name": "Lizhi Wang"
                },
                {
                    "authorId": "2275786010",
                    "name": "Lin Zhu"
                },
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                },
                {
                    "authorId": "2061494979",
                    "name": "Zhiwei Xiong"
                },
                {
                    "authorId": "2271866232",
                    "name": "Hua Huang"
                }
            ]
        },
        {
            "paperId": "af72bf6c5e7e9c8b9ddb23152c08fc045f269fb9",
            "title": "Multishot Compressive Hyperspectral Imaging Based on Tensor Fibered Rank Minimization and Its Primal-Dual Algorithm",
            "abstract": "Coded aperture snapshot spectral imaging (CASSI) compresses tens to hundreds of spectral bands of the hyperspectral image (HSI) to a 2-D compressive measurement. For spatially or spectrally rich scenes, the compressive measurement provided by a single snapshot CASSI may not be sufficient. By taking multiple snapshots of the same scene, multishot CASSI leads to a less ill-posed inverse reconstruction problem, making the CASSI system more suitable for spatially or spectrally rich HSI. Considering the strong spectral correlation of HSI and the directional characteristics of mask shifting in multishot CASSI, the mode-1 tensor fibered rank (TFR) minimization is presented for its reconstruction in this article. Specifically, the mode-1 TFR is derived from the tensor singular value decomposition (t-SVD) to the mode-1 t-SVD, and the mode-1 TFR minimization is reduced to a mode-1 tensor nuclear norm minimization problem, to achieve more accurate HSI characterization in multishot CASSI reconstruction. The primal-dual algorithm (PDA) is applied to solve the objective optimization problem, which is flexible. Experimental results on the CAVE, Cuperite, and Urban datasets demonstrate the effectiveness of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057038122",
                    "name": "Ting Xie"
                },
                {
                    "authorId": "2457540",
                    "name": "Xudong Kang"
                },
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                },
                {
                    "authorId": "2282087766",
                    "name": "Tonghan Wang"
                },
                {
                    "authorId": "49480411",
                    "name": "Licheng Liu"
                }
            ]
        },
        {
            "paperId": "8f381ded8d167eeb4f8574dffe4dda01d16dfd7e",
            "title": "Multi-scale Conformer Fusion Network for Multi-participant Behavior Analysis",
            "abstract": "Understanding and elucidating human behavior across diverse scenarios represents a pivotal research challenge in pursuing seamless human-computer interaction. However, previous research on multi-participant dialogues has mostly relied on proprietary datasets, which are not standardized and openly accessible. To propel advancements in this domain, the MultiMediate'23 Challenge presents two sub-challenges: Eye contact detection and Next speaker prediction, aiming to foster a comprehensive understanding of multi-participant behavior. To tackle these challenges, we propose a multi-scale conformer fusion network (MSCFN) for enhancing the perception of multi-participant group behaviors. The conformer block combines the strengths of transformers and convolution networks to facilitate the establishment of global and local contextual relationships between sequences. Then the output features from all Conformer blocks are concatenated to fusion multi-scale representations. Our proposed method was evaluated using the officially provided dataset, and it achieves the best and second best performance in next speaker prediction and gaze detection tasks of MultiMediate'23, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159682817",
                    "name": "Qiya Song"
                },
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                },
                {
                    "authorId": "2153219906",
                    "name": "Bin Sun"
                },
                {
                    "authorId": "2261951209",
                    "name": "Jie Xie"
                },
                {
                    "authorId": "2238918156",
                    "name": "Shutao Li"
                }
            ]
        },
        {
            "paperId": "cb31ea9bc9aace6aa7929bd01fa44331ba03b82f",
            "title": "Spectral Super-Resolution via Model-Guided Cross-Fusion Network",
            "abstract": "Spectral super-resolution, which reconstructs a hyperspectral image (HSI) from a single red-green-blue (RGB) image, has acquired more and more attention. Recently, convolution neural networks (CNNs) have achieved promising performance. However, they often fail to simultaneously exploit the imaging model of the spectral super-resolution and complex spatial and spectral characteristics of the HSI. To tackle the above problems, we build a novel cross fusion (CF)-based model-guided network (called SSRNet) for spectral super-resolution. In specific, based on the imaging model, we unfold the spectral super-resolution into the HSI prior learning (HPL) module and imaging model guiding (IMG) module. Instead of just modeling one kind of image prior, the HPL module is composed of two subnetworks with different structures, which can effectively learn the complex spatial and spectral priors of the HSI, respectively. Furthermore, a CF strategy is used to establish the connection between the two subnetworks, which further improves the learning performance of the CNN. The IMG module results in solving a strong convex optimization problem, which adaptively optimizes and merges the two features learned by the HPL module by exploiting the imaging model. The two modules are alternately connected to achieve optimal HSI reconstruction performance. Experiments on both the simulated and real data demonstrate that the proposed method can achieve superior spectral reconstruction results with relatively small model size. The code will be available at https://github.com/renweidian.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                },
                {
                    "authorId": "2067161634",
                    "name": "Tian Shan"
                },
                {
                    "authorId": "2038360724",
                    "name": "Wei He"
                },
                {
                    "authorId": "2108974798",
                    "name": "Haibo Liu"
                }
            ]
        },
        {
            "paperId": "e1a8a951926c511f05368ea1cc23ca313c03f8f7",
            "title": "Multispectral Image Pan-Sharpening Guided by Component Substitution Model",
            "abstract": "Multispectral image pan-sharpening aims to increase the spatial details of multispectral images by fusing multispectral and panchromatic (PAN) images. Existing component substitution (CS)-based deep learning pan-sharpening is generally regarded as a black box and fails to mine the image interaction relation with physical significance in each step of pan-sharpening, which not only limits the improvement of image resolution but also ignores the physical interpretability of the models. To improve this situation, according to the traditional CS-based detail injection pan-sharpening model, we consider the matrix calculation in each step as the transformation between image pixel values and carry out linear transformations, and therefore the pan-sharpened multispectral image is represented as the sum of two multispectral images. Then given the spatial and spectral heterogeneity, the two summed images are decomposed based on the fact that any real number can be expressed as the product of two real numbers. Ultimately, the multispectral image pan-sharpening model can be constructed as the sum of two Hadamard products. We design a dual-branch network with attention mechanisms that merges the sum and the Hadamard products into a concise formulation. This method not only enhances physical interpretability but also improves spatial resolution. Experiments on five real-world datasets validate that the proposed multispectral image pan-sharpening model can improve performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238722334",
                    "name": "Huiling Gao"
                },
                {
                    "authorId": "2238918156",
                    "name": "Shutao Li"
                },
                {
                    "authorId": "2167560390",
                    "name": "Jun Li"
                },
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                }
            ]
        },
        {
            "paperId": "e6c07f18b42386e93cdbdb314547a972fcb54108",
            "title": "Zero-Shot Hyperspectral Sharpening",
            "abstract": "Fusing hyperspectral images (HSIs) with multispectral images (MSIs) of higher spatial resolution has become an effective way to sharpen HSIs. Recently, deep convolutional neural networks (CNNs) have achieved promising fusion performance. However, these methods often suffer from the lack of training data and limited generalization ability. To address the above problems, we present a zero-shot learning (ZSL) method for HSI sharpening. Specifically, we first propose a novel method to quantitatively estimate the spectral and spatial responses of imaging sensors with high accuracy. In the training procedure, we spatially subsample the MSI and HSI based on the estimated spatial response and use the downsampled HSI and MSI to infer the original HSI. In this way, we can not only exploit the inherent information in the HSI and MSI, but the trained CNN can also be well generalized to the test data. In addition, we take the dimension reduction on the HSI, which reduces the model size and storage usage without sacrificing fusion accuracy. Furthermore, we design an imaging model-based loss function for CNN, which further boosts the fusion performance. The experimental results show the significantly high efficiency and accuracy of our approach.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8015902",
                    "name": "Renwei Dian"
                },
                {
                    "authorId": "51059863",
                    "name": "Anjing Guo"
                },
                {
                    "authorId": "2116066317",
                    "name": "Shutao Li"
                }
            ]
        }
    ]
}