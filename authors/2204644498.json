{
    "authorId": "2204644498",
    "papers": [
        {
            "paperId": "113811e034e57b8364d8a3fa58704fa6e1faf881",
            "title": "Bridging the Literacy Gap for Adults: Streaming and Engaging in Adult Literacy Education through Livestreaming",
            "abstract": "Literacy\u2014the ability to read, write, and comprehend text\u2014is an important topic addressed by UNESCO. Despite global efforts to promote adult literacy education, rural areas with limited resources still lag behind. As livestreaming has gained popularity in China, many streamers leveraged its accessibility and affordability to reach low-literate adults. To gain a better understanding of the practices and challenges faced by adult literacy education through livestreaming, we conducted a mixed-methods study involving a 7-day observation of livestreaming sessions and an interview study with twelve streamers and ten viewers. We discovered streamers\u2019 altruistic motives and unique interactive approaches. Viewers perceived livestreaming as a more engaging, community-supportive method than traditional approaches. We also identified both shared and unique challenges for streamers and viewers that limit its efficacy as a learning tool. Finally, we recognized opportunities to enhance educational equity, emphasizing design implications for advancing adult literacy education and promoting diversity in livestreaming.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261003224",
                    "name": "Shihan Fu"
                },
                {
                    "authorId": "2301068176",
                    "name": "Jianhao Chen"
                },
                {
                    "authorId": "2115694650",
                    "name": "Emily Kuang"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                }
            ]
        },
        {
            "paperId": "2f2af859c01e16036f0858ccce6611a36ab0465c",
            "title": "Neural Canvas: Supporting Scenic Design Prototyping by Integrating 3D Sketching and Generative AI",
            "abstract": "We propose Neural Canvas, a lightweight 3D platform that integrates sketching and a collection of generative AI models to facilitate scenic design prototyping. Compared with traditional 3D tools, sketching in a 3D environment helps designers quickly express spatial ideas, but it does not facilitate the rapid prototyping of scene appearance or atmosphere. Neural Canvas integrates generative AI models into a 3D sketching interface and incorporates four types of projection operations to facilitate 2D-to-3D content creation. Our user study shows that Neural Canvas is an effective creativity support tool, enabling users to rapidly explore visual ideas and iterate 3D scenic designs. It also expedites the creative process for both novices and artists who wish to leverage generative AI technology, resulting in attractive and detailed 3D designs created more efficiently than using traditional modeling tools or individual generative AI platforms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307032402",
                    "name": "Yulin Shen"
                },
                {
                    "authorId": "2305904274",
                    "name": "Yifei Shen"
                },
                {
                    "authorId": "2301086901",
                    "name": "Jiawen Cheng"
                },
                {
                    "authorId": "2301757701",
                    "name": "Chutian Jiang"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                },
                {
                    "authorId": "2187419235",
                    "name": "Zeyu Wang"
                }
            ]
        },
        {
            "paperId": "3207424dba597acc4328eedb7c5e1aad37a0bf15",
            "title": "CharacterMeet: Supporting Creative Writers' Entire Story Character Construction Processes Through Conversation with LLM-Powered Chatbot Avatars",
            "abstract": "Support for story character construction is as essential as characters are for stories. Building upon past research on early character construction stages, we explore how conversation with chatbot avatars embodying characters powered by more recent technologies could support the entire character construction process for creative writing. Through a user study (N=14) with creative writers, we examine thinking and usage patterns of CharacterMeet, a prototype system allowing writers to progressively manifest characters through conversation while customizing context, character appearance, voice, and background image. We discover that CharacterMeet facilitates iterative character construction. Specifically, participants, including those with more linear usual approaches, alternated between writing and personalized exploration through visualization of ideas on CharacterMeet while visuals and audio enhanced immersion. Our findings support research on iterative creative processes and the growing potential of personalizable generative AI creativity support tools. We present design implications for leveraging chatbot avatars in the creative writing process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187575973",
                    "name": "Hua Xuan Qin"
                },
                {
                    "authorId": "2216501833",
                    "name": "Shan Jin"
                },
                {
                    "authorId": "2181257140",
                    "name": "Ze Gao"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                },
                {
                    "authorId": "2242340605",
                    "name": "Pan Hui"
                }
            ]
        },
        {
            "paperId": "6051a8f179ebac4ce76de872099526adebff3adc",
            "title": "Designing Unobtrusive Modulated Electrotactile Feedback on Fingertip Edge to Assist Blind and Low Vision (BLV) People in Comprehending Charts",
            "abstract": "Charts are crucial in conveying information across various fields but are inaccessible to blind and low vision (BLV) people without assistive technology. Chart comprehension tools leveraging haptic feedback have been used widely but are often bulky, expensive, and static, rendering them inefficient for conveying chart data. To increase device portability, enable multitasking, and provide efficient assistance in chart comprehension, we introduce a novel system that delivers unobtrusive modulated electrotactile feedback directly to the fingertip edge. Our three-part study with twelve participants confirmed the effectiveness of this system, demonstrating that electrotactile feedback, when applied for 0.5 seconds with a 0.12-second interval, provides the most accurate position and direction recognition. Furthermore, our electrotactile device has proven valuable in assisting BLV participants in comprehending four commonly used charts: line charts, scatterplots, bar charts, and pie charts. We also delve into the implications of our findings on recognition enhancement, presentation modes, and function synergy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301757701",
                    "name": "Chutian Jiang"
                },
                {
                    "authorId": "2301112646",
                    "name": "Yinan Fan"
                },
                {
                    "authorId": "2293319782",
                    "name": "Junan Xie"
                },
                {
                    "authorId": "2115694650",
                    "name": "Emily Kuang"
                },
                {
                    "authorId": "2301170142",
                    "name": "Kaihao Zhang"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                }
            ]
        },
        {
            "paperId": "7cc9b25538da8c5081088f20911e6e26202d152f",
            "title": "WieldingCanvas: Interactive Sketch Canvases for Freehand Drawing in VR",
            "abstract": "Sketching in Virtual Reality (VR) is challenging mainly due to the absence of physical surface support and virtual depth perception cues, which induce high cognitive and sensorimotor load. This paper presents WieldingCanvas, an interactive VR sketching platform that integrates canvas manipulations to draw lines and curves in 3D. Informed by real-life examples of two-handed creative activities, WieldingCanvas interprets users\u2019 spatial gestures to move, swing, rotate, transform, or fold a virtual canvas, whereby users simply draw primitive strokes on the canvas, which are turned into finer and more sophisticated shapes via the manipulation of the canvas. We evaluated the capability and user experience of WieldingCanvas with two studies where participants were asked to sketch target shapes. A set of freehand sketches of high aesthetic qualities were created, and the results demonstrated that WieldingCanvas can assist users with creating 3D sketches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2302505433",
                    "name": "Xiaohui Tan"
                },
                {
                    "authorId": "2158465686",
                    "name": "Zhenxuan He"
                },
                {
                    "authorId": "2261889410",
                    "name": "Can Liu"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                },
                {
                    "authorId": "36688126",
                    "name": "Tianren Luo"
                },
                {
                    "authorId": "2266391121",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "2301084654",
                    "name": "Mi Tian"
                },
                {
                    "authorId": "2154279289",
                    "name": "Teng Han"
                },
                {
                    "authorId": "2052643314",
                    "name": "Feng Tian"
                }
            ]
        },
        {
            "paperId": "865b81a1a873f589b11c7bdada1dcedb73616cad",
            "title": "AromaBlendz: An Olfactory System for Crafting Personalized Scents",
            "abstract": "Although the HCI community has recently begun to explore the usage of scent to enrich interactive system experiences (e.g., making VR more immersive), scent is often preset. In contrast, personalized scents might help trigger emotional responses and memory recall in many application scenarios, ranging from fostering relaxation to managing emotional states. We present AromaBlendz, a novel digital platform that enables users to create and customize their unique scent profiles. AromaBlendz comprises both hardware and software components that collectively deliver a seamless scent customization experience. The hardware includes a blending mechanism for essence oils and a user-friendly control unit, while the software component provides an intuitive interface for users to create, preview, and store their preferred scents. The platform not only allows for the generation of personalized scent profiles using a library of essential oils but also facilitates the process of scent creation through an accessible and interactive user interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261003224",
                    "name": "Shihan Fu"
                },
                {
                    "authorId": "2301068176",
                    "name": "Jianhao Chen"
                },
                {
                    "authorId": "2258335050",
                    "name": "Yi Cai"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                }
            ]
        },
        {
            "paperId": "db849a1920566a6da6c7dc5bf67ef4cf01a50394",
            "title": "Toward Making Virtual Reality (VR) More Inclusive for Older Adults: Investigating Aging Effect on Target Selection and Manipulation Tasks in VR",
            "abstract": "Recent studies show the promise of VR in improving physical, cognitive, and emotional health of older adults. However, prior work on optimizing object selection and manipulation performance in VR was mostly conducted among younger adults. It remains unclear how older adults would perform such tasks compared to younger adults and the challenges they might face. To fill in this gap, we conducted two studies with both older and younger adults to understand their performances and user experiences of object selection and manipulation in VR respectively. Based on the results, we delineated interaction difficulties that older adults exhibited in VR and identified multiple factors, such as headset-related neck fatigue, extra head movements from out-of-view interactions, and slow spatial perceptions, that significantly decreased the motor performance of older adults. We further proposed design recommendations for improving the accessibility of direct interaction experiences in VR for older adults.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2289132173",
                    "name": "Zhiqing Wu"
                },
                {
                    "authorId": "2290517217",
                    "name": "Duotun Wang"
                },
                {
                    "authorId": "2260834864",
                    "name": "Shumeng Zhang"
                },
                {
                    "authorId": "2238387520",
                    "name": "Yuru Huang"
                },
                {
                    "authorId": "2187419235",
                    "name": "Zeyu Wang"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                }
            ]
        },
        {
            "paperId": "ed812f816bbdcb18009ea0d4956456e35871bd89",
            "title": "See Widely, Think Wisely: Toward Designing a Generative Multi-agent System to Burst Filter Bubbles",
            "abstract": "The proliferation of AI-powered search and recommendation systems has accelerated the formation of \u201cfilter bubbles\u201d that reinforce people\u2019s biases and narrow their perspectives. Previous research has attempted to address this issue by increasing the diversity of information exposure, which is often hindered by a lack of user motivation to engage with. In this study, we took a human-centered approach to explore how Large Language Models (LLMs) could assist users in embracing more diverse perspectives. We developed a prototype featuring LLM-powered multi-agent characters that users could interact with while reading social media content. We conducted a participatory design study with 18 participants and found that multi-agent dialogues with gamification incentives could motivate users to engage with opposing viewpoints. Additionally, progressive interactions with assessment tasks could promote thoughtful consideration. Based on these findings, we provided design implications with future work outlooks for leveraging LLMs to help users burst their filter bubbles.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153635072",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2156016295",
                    "name": "Jingwei Sun"
                },
                {
                    "authorId": "2261737434",
                    "name": "Li Feng"
                },
                {
                    "authorId": "2301178857",
                    "name": "Cen Yao"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                },
                {
                    "authorId": "51151327",
                    "name": "Liuxin Zhang"
                },
                {
                    "authorId": "2128738231",
                    "name": "Qianying Wang"
                },
                {
                    "authorId": "2258073636",
                    "name": "Xin Geng"
                },
                {
                    "authorId": "2240856453",
                    "name": "Yong Rui"
                }
            ]
        },
        {
            "paperId": "f2498e15456e0d0e5d715b45508b84840f282ffb",
            "title": "Enhancing UX Evaluation Through Collaboration with Conversational AI Assistants: Effects of Proactive Dialogue and Timing",
            "abstract": "Usability testing is vital for enhancing the user experience (UX) of interactive systems. However, analyzing test videos is complex and resource-intensive. Recent AI advancements have spurred exploration into human-AI collaboration for UX analysis, particularly through natural language. Unlike user-initiated dialogue, our study investigated the potential of proactive conversational assistants to aid UX evaluators through automatic suggestions at three distinct times: before, in sync with, and after potential usability problems. We conducted a hybrid Wizard-of-Oz study involving 24 UX evaluators, using ChatGPT to generate automatic problem suggestions and a human actor to respond to impromptu questions. While timing did not significantly impact analytic performance, suggestions appearing after potential problems were preferred, enhancing trust and efficiency. Participants found the automatic suggestions useful, but they collectively identified more than twice as many problems, underscoring the irreplaceable role of human expertise. Our findings also offer insights into future human-AI collaborative tools for UX evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115694650",
                    "name": "Emily Kuang"
                },
                {
                    "authorId": "2265351430",
                    "name": "Minghao Li"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                },
                {
                    "authorId": "2301065140",
                    "name": "Kristen Shinohara"
                }
            ]
        },
        {
            "paperId": "fc74d2b7663f6668a7904184d23d2882c92f57fe",
            "title": "\u201cCan It Be Customized According to My Motor Abilities?\u201d: Toward Designing User-Defined Head Gestures for People with Dystonia",
            "abstract": "Recent studies proposed above-the-neck gestures for people with upper-body motor impairments interacting with mobile devices without finger touch, resulting in an appropriate user-defined gesture set. However, many gestures involve sustaining eyelids in closed or open states for a period. This is challenging for people with dystonia, who have difficulty sustaining and intermitting muscle contractions. Meanwhile, other facial parts, such as the tongue and nose, can also be used to alleviate the sustained use of eyes in the interaction. Consequently, we conducted a user study inviting 16 individuals with dystonia to design gestures based on facial muscle movements for 26 common smartphone commands. We collected 416 user-defined head gestures involving facial features and shoulders. Finally, we obtained the preferred gestures set for individuals with dystonia. Participants preferred to make the gestures with their heads and use unnoticeable gestures. Our findings provide valuable references for the universal design of natural interaction technology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301123576",
                    "name": "Qin Sun"
                },
                {
                    "authorId": "2301138913",
                    "name": "Yunqi Hu"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                },
                {
                    "authorId": "49297714",
                    "name": "Jingting Li"
                },
                {
                    "authorId": "134769402",
                    "name": "Su-Jing Wang"
                }
            ]
        }
    ]
}