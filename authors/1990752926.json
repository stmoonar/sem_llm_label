{
    "authorId": "1990752926",
    "papers": [
        {
            "paperId": "35f7cfdd8a768351f97dcfcd45f08fa832a28b38",
            "title": "Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion",
            "abstract": "Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting aims to predict the missing entity from a fact in the future, posing a challenge that aligns more closely with real-world prediction problems. Existing research mostly encodes entities and relations using sequential graph neural networks applied to recent snapshots. However, these approaches tend to overlook the ability to skip irrelevant snapshots according to entity-related relations in the query and disregard the importance of explicit temporal information. To address this, we propose our model, Re-Temp (Relation-Aware Temporal Representation Learning), which leverages explicit temporal embedding as input and incorporates skip information flow after each timestamp to skip unnecessary information for prediction. Additionally, we introduce a two-phase forward propagation method to prevent information leakage. Through the evaluation on six TKGC (extrapolation) datasets, we demonstrate that our model outperforms all eight recent state-of-the-art models by a significant margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                }
            ]
        },
        {
            "paperId": "3447acbd5688f7a78146c405e925cdfea04e25ef",
            "title": "ME-GCN: Multi-dimensional Edge-Embedded Graph Convolutional Networks for Semi-supervised Text Classification",
            "abstract": "Compared to sequential learning models, graph-based neural networks exhibit excellent ability in capturing global information and have been used for semi-supervised learning tasks. Most Graph Convolutional Networks are designed with the single-dimensional edge feature and failed to utilise the rich edge information about graphs. This paper introduces the ME-GCN (Multi-dimensional Edge-enhanced Graph Convolutional Networks) for semi-supervised text classification. A text graph for an entire corpus is firstly constructed to describe the undirected and multi-dimensional relationship of word-to-word, document-document, and word-to-document. The graph is initialised with corpus-trained multi-dimensional word and document node representation, and the relations are represented according to the distance of those words/documents nodes. Then, the generated graph is trained with ME-GCN, which considers the edge features as multi-stream signals, and each stream performs a separate graph convolutional operation. Our ME-GCN can integrate a rich source of graph edge information of the entire text corpus. The results have demonstrated that our proposed model has significantly outperformed the state-of-the-art methods across eight benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                }
            ]
        },
        {
            "paperId": "3fbe7f9a0ca640c915d1b4e2cca6e49a15ad710a",
            "title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction",
            "abstract": "In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143557418",
                    "name": "Yubo Ma"
                },
                {
                    "authorId": "2118402851",
                    "name": "Zehao Wang"
                },
                {
                    "authorId": "145014675",
                    "name": "Yixin Cao"
                },
                {
                    "authorId": "2027599235",
                    "name": "Mukai Li"
                },
                {
                    "authorId": "2108612706",
                    "name": "Meiqi Chen"
                },
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": "2156121678",
                    "name": "Jing Shao"
                }
            ]
        },
        {
            "paperId": "56fac0ced28ad22958da067795c5abfe5f85fce9",
            "title": "InducT-GCN: Inductive Graph Convolutional Networks for Text Classification",
            "abstract": "Text classification aims to assign labels to textual units by making use of global information. Recent studies have applied graph neural network (GNN) to capture the global word co-occurrence in a corpus. Existing approaches require that all the nodes (training and test) in a graph are present during training, which are transductive and do not naturally generalise to unseen nodes. To make those models inductive, they use extra resources, like pretrained word embedding. However, high-quality resource is not always available and hard to train. Under the extreme settings with no extra resource and limited amount of training set, can we still learn an inductive graph-based text classification model? In this paper, we introduce a novel inductive graph-based text classification framework, InducT-GCN (InducTive Graph Convolutional Networks for Text classification). Compared to transductive models that require test documents in training, we construct a graph based on the statistics of training documents only and represent document vectors with a weighted sum of word vectors. We then conduct one-directional GCN propagation during testing. Across five text classification benchmarks, our InducT-GCN outperformed state-of-the-art methods that are either transductive in nature or pre-trained additional resources. We also conducted scalability testing by gradually increasing the data size and revealed that our InducT-GCN can reduce the time and space complexity. The code is available on: https://github.com/usydnlp/InductTGCN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                }
            ]
        },
        {
            "paperId": "5913a9478b34ce93a3151256e715609c0b5147e1",
            "title": "Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy",
            "abstract": "Large-scale datasets play a vital role in computer vision. But current datasets are annotated blindly without differentiation to samples, making the data collection inefficient and unscalable. The open question is how to build a mega-scale dataset actively. Although advanced active learning algorithms might be the answer, we experimentally found that they are lame in the realistic annotation scenario where out-of-distribution data is extensive. This work thus proposes a novel active learning framework for realistic dataset annotation. Equipped with this framework, we build a high-quality vision dataset -- Bamboo, which consists of 69M image classification annotations with 119K categories and 28M object bounding box annotations with 809 categories. We organize these categories by a hierarchical taxonomy integrated from several knowledge bases. The classification annotations are four times larger than ImageNet22K, and that of detection is three times larger than Object365. Compared to ImageNet22K and Objects365, models pre-trained on Bamboo achieve superior performance among various downstream tasks (6.2% gains on classification and 2.1% gains on detection). We believe our active learning framework and Bamboo are essential for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145784327",
                    "name": "Yuanhan Zhang"
                },
                {
                    "authorId": "2112427874",
                    "name": "Qi Sun"
                },
                {
                    "authorId": "2110323281",
                    "name": "Yichun Zhou"
                },
                {
                    "authorId": "2182002504",
                    "name": "Zexin He"
                },
                {
                    "authorId": "13050405",
                    "name": "Zhen-fei Yin"
                },
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": "2053947248",
                    "name": "Lu Sheng"
                },
                {
                    "authorId": "145858545",
                    "name": "Y. Qiao"
                },
                {
                    "authorId": "1388486428",
                    "name": "Jing Shao"
                },
                {
                    "authorId": "2145253107",
                    "name": "Ziwei Liu"
                }
            ]
        },
        {
            "paperId": "9367e642fa47c844834e4415c8cac2a315ea5be6",
            "title": "MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities",
            "abstract": "Events are fundamental building blocks of real-world happenings. In this paper, we present a large-scale, multi-modal event knowledge graph named MMEKG. MMEKG unifies different modalities of knowledge via events, which complement and disambiguate each other.Specifically, MMEKG incorporates (i) over 990 thousand concept events with 644 relation types to cover most types of happenings, and (ii) over 863 million instance events connected through 934 million relations, which provide rich contextual information in texts and/or images. To collect billion-scale instance events and relations among them, we additionally develop an efficient yet effective pipeline for textual/visual knowledge extraction system. We also develop an induction strategy to create million-scale concept events and a schema organizing all events and relations in MMEKG. To this end, we also provide a pipeline enabling our system to seamlessly parse texts/images to event graphs and to retrieve multi-modal knowledge at both concept- and instance-levels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143557418",
                    "name": "Yubo Ma"
                },
                {
                    "authorId": "2118402851",
                    "name": "Zehao Wang"
                },
                {
                    "authorId": "2027599235",
                    "name": "Mukai Li"
                },
                {
                    "authorId": "145014675",
                    "name": "Yixin Cao"
                },
                {
                    "authorId": "2108612706",
                    "name": "Meiqi Chen"
                },
                {
                    "authorId": "2155028207",
                    "name": "Xinze Li"
                },
                {
                    "authorId": "49934730",
                    "name": "Wenqiao Sun"
                },
                {
                    "authorId": "2162737824",
                    "name": "Kunquan Deng"
                },
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": "1735962",
                    "name": "Aixin Sun"
                },
                {
                    "authorId": "2156121678",
                    "name": "Jing Shao"
                }
            ]
        },
        {
            "paperId": "e3dd0ea1810b96db83f4d2b7b7c9403ab7765a3a",
            "title": "Understanding Graph Convolutional Networks for Text Classification",
            "abstract": "Graph Convolutional Networks (GCN) have been effective at tasks that have rich relational structure and can preserve global structure information of a dataset in graph embeddings. Recently, many researchers focused on examining whether GCNs could handle different Natural Language Processing tasks, especially text classification. While applying GCNs to text classification is well-studied, its graph construction techniques, such as node/edge selection and their feature representation, and the optimal GCN learning mechanism in text classification is rather neglected. In this paper, we conduct a comprehensive analysis of the role of node and edge embeddings in a graph and its GCN learning techniques in text classification. Our analysis is the first of its kind and provides useful insights into the importance of each graph node/edge construction mechanism when applied at the GCN training/testing in different text classification benchmarks, as well as under its semi-supervised environment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "2251982",
                    "name": "Zihan Yuan"
                },
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                }
            ]
        },
        {
            "paperId": "5bc32a8f9a6245ae7f0761d539074dc7623ff61e",
            "title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection",
            "abstract": "Traditional toxicity detection models have focused on the single utterance level without deeper understanding of context. We introduce CONDA, a new dataset for in-game toxic language detection enabling joint intent classification and slot filling analysis, which is the core task of Natural Language Understanding (NLU). The dataset consists of 45K utterances from 12K conversations from the chat logs of 1.9K completed Dota 2 matches. We propose a robust dual semantic-level toxicity framework, which handles utterance and token-level patterns, and rich contextual chatting history. Accompanying the dataset is a thorough in-game toxicity analysis, which provides comprehensive understanding of context at utterance, token, and dual levels. Inspired by NLU, we also apply its metrics to the toxicity detection tasks for assessing toxicity and game-specific aspects. We evaluate strong NLU models on CONDA, providing fine-grained results for different intent classes and slot classes. Furthermore, we examine the coverage of toxicity nature in our dataset by comparing it with other toxicity datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69047048",
                    "name": "H. Weld"
                },
                {
                    "authorId": "123914136",
                    "name": "Guanghao Huang"
                },
                {
                    "authorId": "2282598754",
                    "name": "Jean Lee"
                },
                {
                    "authorId": "38144094",
                    "name": "T. Zhang"
                },
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": "2157818919",
                    "name": "Xinghong Guo"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                }
            ]
        },
        {
            "paperId": "0a5717feb016fd85c0327eef5165e172c5f09f68",
            "title": "Detect All Abuse! Toward Universal Abusive Language Detection Models",
            "abstract": "Online abusive language detection (ALD) has become a societal issue of increasing importance in recent years. Several previous works in online ALD focused on solving a single abusive language problem in a single domain, like Twitter, and have not been successfully transferable to the general ALD task or domain. In this paper, we introduce a new generic ALD framework, MACAS, which is capable of addressing several types of ALD tasks across different domains. Our generic framework covers multi-aspect abusive language embeddings that represent the target and content aspects of abusive language and applies a textual graph embedding that analyses the user\u2019s linguistic behaviour. Then, we propose and use the cross-attention gate flow mechanism to embrace multiple aspects of abusive language. Quantitative and qualitative evaluation results show that our ALD algorithm rivals or exceeds the six state-of-the-art ALD algorithms across seven ALD datasets covering multiple aspects of abusive language and different online community domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": null,
                    "name": "Dong Lu"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                }
            ]
        },
        {
            "paperId": "ce03885afb09eee11dfcea7eadd59153f2f9633b",
            "title": "VICTR: Visual Information Captured Text Representation for Text-to-Vision Multimodal Tasks",
            "abstract": "Text-to-image multimodal tasks, generating/retrieving an image from a given text description, are extremely challenging tasks since raw text descriptions cover quite limited information in order to fully describe visually realistic images. We propose a new visual contextual text representation for text-to-image multimodal tasks, VICTR, which captures rich visual semantic information of objects from the text input. First, we use the text description as initial input and conduct dependency parsing to extract the syntactic structure and analyse the semantic aspect, including object quantities, to extract the scene graph. Then, we train the extracted objects, attributes, and relations in the scene graph and the corresponding geometric relation information using Graph Convolutional Networks, and it generates text representation which integrates textual and visual semantic information. The text representation is aggregated with word-level and sentence-level embedding to generate both visual contextual word and sentence representation. For the evaluation, we attached VICTR to the state-of-the-art models in text-to-image generation.VICTR is easily added to existing models and improves across both quantitative and qualitative aspects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "39209233",
                    "name": "Siwen Luo"
                },
                {
                    "authorId": "1990752926",
                    "name": "Kunze Wang"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                }
            ]
        }
    ]
}