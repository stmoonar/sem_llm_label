{
    "authorId": "2302810573",
    "papers": [
        {
            "paperId": "0a9a9fc600eccdcf13e46044844f2fcc8f22920a",
            "title": "DataComp-LM: In search of the next generation of training sets for language models",
            "abstract": "We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63%&66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287821949",
                    "name": "Jeffrey Li"
                },
                {
                    "authorId": "46372713",
                    "name": "Alex Fang"
                },
                {
                    "authorId": "1438310376",
                    "name": "Georgios Smyrnis"
                },
                {
                    "authorId": "1962329805",
                    "name": "Maor Ivgi"
                },
                {
                    "authorId": "2306961054",
                    "name": "Matt Jordan"
                },
                {
                    "authorId": "1387466862",
                    "name": "S. Gadre"
                },
                {
                    "authorId": "103404553",
                    "name": "Hritik Bansal"
                },
                {
                    "authorId": "2179104354",
                    "name": "E. Guha"
                },
                {
                    "authorId": "150299584",
                    "name": "Sedrick Scott Keh"
                },
                {
                    "authorId": "2284685268",
                    "name": "Kushal Arora"
                },
                {
                    "authorId": "2269884773",
                    "name": "Saurabh Garg"
                },
                {
                    "authorId": "2115140387",
                    "name": "Rui Xin"
                },
                {
                    "authorId": "2037383772",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "2291068369",
                    "name": "Reinhard Heckel"
                },
                {
                    "authorId": "72847120",
                    "name": "Jean-Pierre Mercat"
                },
                {
                    "authorId": "2322453298",
                    "name": "Mayee Chen"
                },
                {
                    "authorId": "40895369",
                    "name": "Suchin Gururangan"
                },
                {
                    "authorId": "52193502",
                    "name": "Mitchell Wortsman"
                },
                {
                    "authorId": "2044198106",
                    "name": "Alon Albalak"
                },
                {
                    "authorId": "1938499056",
                    "name": "Yonatan Bitton"
                },
                {
                    "authorId": "2174178585",
                    "name": "Marianna Nezhurina"
                },
                {
                    "authorId": "2253410440",
                    "name": "Amro Abbas"
                },
                {
                    "authorId": "2650640",
                    "name": "Cheng-Yu Hsieh"
                },
                {
                    "authorId": "2143028576",
                    "name": "Dhruba Ghosh"
                },
                {
                    "authorId": "2262139047",
                    "name": "Josh Gardner"
                },
                {
                    "authorId": "2302771628",
                    "name": "Maciej Kilian"
                },
                {
                    "authorId": "2307629849",
                    "name": "Hanlin Zhang"
                },
                {
                    "authorId": "2287946530",
                    "name": "Rulin Shao"
                },
                {
                    "authorId": "2306959183",
                    "name": "Sarah Pratt"
                },
                {
                    "authorId": "2296600111",
                    "name": "Sunny Sanyal"
                },
                {
                    "authorId": "1387994137",
                    "name": "Gabriel Ilharco"
                },
                {
                    "authorId": "1432234296",
                    "name": "Giannis Daras"
                },
                {
                    "authorId": "2149708673",
                    "name": "Kalyani Marathe"
                },
                {
                    "authorId": "2273789852",
                    "name": "Aaron Gokaslan"
                },
                {
                    "authorId": "2288099380",
                    "name": "Jieyu Zhang"
                },
                {
                    "authorId": "2302810573",
                    "name": "K. Chandu"
                },
                {
                    "authorId": "2167669221",
                    "name": "Thao Nguyen"
                },
                {
                    "authorId": "2291068185",
                    "name": "Igor Vasiljevic"
                },
                {
                    "authorId": "144695232",
                    "name": "S. Kakade"
                },
                {
                    "authorId": "2265621012",
                    "name": "Shuran Song"
                },
                {
                    "authorId": "2303847572",
                    "name": "Sujay Sanghavi"
                },
                {
                    "authorId": "2978170",
                    "name": "Fartash Faghri"
                },
                {
                    "authorId": "2303461111",
                    "name": "Sewoong Oh"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2306959711",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "1388811741",
                    "name": "Alaaeldin El-Nouby"
                },
                {
                    "authorId": "2239094185",
                    "name": "Hadi Pouransari"
                },
                {
                    "authorId": "2238621175",
                    "name": "Alexander Toshev"
                },
                {
                    "authorId": "2307035318",
                    "name": "Stephanie Wang"
                },
                {
                    "authorId": "3458736",
                    "name": "Dirk Groeneveld"
                },
                {
                    "authorId": "2306959571",
                    "name": "Luca Soldani"
                },
                {
                    "authorId": "2303396379",
                    "name": "Pang Wei Koh"
                },
                {
                    "authorId": "2191688",
                    "name": "J. Jitsev"
                },
                {
                    "authorId": "2283843631",
                    "name": "Thomas Kollar"
                },
                {
                    "authorId": "2257301126",
                    "name": "Alexandros G. Dimakis"
                },
                {
                    "authorId": "2444742",
                    "name": "Y. Carmon"
                },
                {
                    "authorId": "2298523",
                    "name": "Achal Dave"
                },
                {
                    "authorId": "2253541812",
                    "name": "Ludwig Schmidt"
                },
                {
                    "authorId": "34961417",
                    "name": "Vaishaal Shankar"
                }
            ]
        },
        {
            "paperId": "5d12dfd7278cb8da26f9fd1956cad3c15cea9863",
            "title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
            "abstract": "We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of ``slightly better/worse'' to ``tie'' if the winner response exceeds the loser one by more than $K$ characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273918810",
                    "name": "Bill Yuchen Lin"
                },
                {
                    "authorId": "2268298457",
                    "name": "Yuntian Deng"
                },
                {
                    "authorId": "2302810573",
                    "name": "K. Chandu"
                },
                {
                    "authorId": "2223951216",
                    "name": "Faeze Brahman"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "2069676542",
                    "name": "R. L. Bras"
                },
                {
                    "authorId": "2257385142",
                    "name": "Yejin Choi"
                }
            ]
        },
        {
            "paperId": "6c94d59a5f8ab934b9259967d59101c32072f9c2",
            "title": "WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries",
            "abstract": "While hallucinations of large language models (LLMs) prevail as a major challenge, existing evaluation benchmarks on factuality do not cover the diverse domains of knowledge that the real-world users of LLMs seek information about. To bridge this gap, we introduce WildHallucinations, a benchmark that evaluates factuality. It does so by prompting LLMs to generate information about entities mined from user-chatbot conversations in the wild. These generations are then automatically fact-checked against a systematically curated knowledge source collected from web search. Notably, half of these real-world entities do not have associated Wikipedia pages. We evaluate 118,785 generations from 15 LLMs on 7,919 entities. We find that LLMs consistently hallucinate more on entities without Wikipedia pages and exhibit varying hallucination rates across different domains. Finally, given the same base models, adding a retrieval component only slightly reduces hallucinations but does not eliminate hallucinations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266434757",
                    "name": "Wenting Zhao"
                },
                {
                    "authorId": "2312760350",
                    "name": "Tanya Goyal"
                },
                {
                    "authorId": "2277244466",
                    "name": "Yu Ying Chiu"
                },
                {
                    "authorId": "2112504145",
                    "name": "Liwei Jiang"
                },
                {
                    "authorId": "2263557371",
                    "name": "Benjamin Newman"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "2302810573",
                    "name": "K. Chandu"
                },
                {
                    "authorId": "2069676542",
                    "name": "R. L. Bras"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                },
                {
                    "authorId": "2268298457",
                    "name": "Yuntian Deng"
                },
                {
                    "authorId": "2285939518",
                    "name": "Yejin Choi"
                }
            ]
        },
        {
            "paperId": "a1fa960fdfcc08510d348f7c66028f3d91b497f8",
            "title": "The Art of Saying No: Contextual Noncompliance in Language Models",
            "abstract": "Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of\"unsafe\"queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223951216",
                    "name": "Faeze Brahman"
                },
                {
                    "authorId": "2308339428",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "2279337376",
                    "name": "Sarah Wiegreffe"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "2302810573",
                    "name": "K. Chandu"
                },
                {
                    "authorId": "2689239",
                    "name": "Jack Hessel"
                },
                {
                    "authorId": "2258958466",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2292425227",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2259707400",
                    "name": "Yejin Choi"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "e36d65120910869aa4d8a87e578dc36545ec4258",
            "title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text",
            "abstract": "Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass human creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we find that distinguished authors like Hemingway exhibit measurably higher CREATIVITY INDEX compared to other human writers. Finally, we demonstrate that CREATIVITY INDEX can be used as a surprisingly effective criterion for zero-shot machine text detection, surpassing the strongest existing zero-shot system, DetectGPT, by a significant margin of 30.2%, and even outperforming the strongest supervised system, GhostBuster, in five out of six domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50085131",
                    "name": "Ximing Lu"
                },
                {
                    "authorId": "1947172233",
                    "name": "Melanie Sclar"
                },
                {
                    "authorId": "1474550731",
                    "name": "Skyler Hallinan"
                },
                {
                    "authorId": "2254272878",
                    "name": "Niloofar Mireshghallah"
                },
                {
                    "authorId": "2324837283",
                    "name": "Jiacheng Liu"
                },
                {
                    "authorId": "2324837321",
                    "name": "Seungju Han"
                },
                {
                    "authorId": "2262217080",
                    "name": "Allyson Ettinger"
                },
                {
                    "authorId": "2112504145",
                    "name": "Liwei Jiang"
                },
                {
                    "authorId": "2302810573",
                    "name": "K. Chandu"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "2257385142",
                    "name": "Yejin Choi"
                }
            ]
        }
    ]
}