{
    "authorId": "1993581583",
    "papers": [
        {
            "paperId": "0708fccedd5e66638222c129554706842671e55e",
            "title": "CooBa: Cross-project Bug Localization via Adversarial Transfer Learning",
            "abstract": "Bug localization plays an important role in software quality control. Many supervised machine learning models have been developed based on historical bug-fix information. Despite being successful, these methods often require sufficient historical data (i.e., labels), which is not always available especially for newly developed software projects. In response, cross-project bug localization techniques have recently emerged whose key idea is to transferring knowledge from label-rich source project to locate bugs in the target project. However, a major limitation of these existing techniques lies in that they fail to capture the specificity of each individual project, and are thus prone to negative transfer.\n\nTo address this issue, we propose an adversarial transfer learning bug localization approach, focusing on only transferring the common characteristics (i.e., public information) across projects. Specifically, our approach (CooBa) learns the indicative public information from cross-project bug reports through a shared encoder, and extracts the private information from code files by an individual feature extractor for each project. CooBa further incorporates adversarial learning mechanism to ensure that public information shared between multiple projects could be effectively extracted. Extensive experiments on four large-scale real-world data sets demonstrate that the proposed CooBa significantly outperforms the state of the art techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118214458",
                    "name": "Zhi Li"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "50384136",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "12892739",
                    "name": "Likang Wu"
                },
                {
                    "authorId": "2869628",
                    "name": "Hongke Zhao"
                },
                {
                    "authorId": "144025741",
                    "name": "Tao Mei"
                }
            ]
        },
        {
            "paperId": "33558aa3311b256880346524981408449b58bc58",
            "title": "General Partial Label Learning via Dual Bipartite Graph Autoencoder",
            "abstract": "We formulate a practical yet challenging problem: General Partial Label Learning (GPLL). Compared to the traditional Partial Label Learning (PLL) problem, GPLL relaxes the supervision assumption from instance-level \u2014 a label set partially labels an instance \u2014 to group-level: 1) a label set partially labels a group of instances, where the within-group instance-label link annotations are missing, and 2) cross-group links are allowed \u2014 instances in a group may be partially linked to the label set from another group. Such ambiguous group-level supervision is more practical in real-world scenarios as additional annotation on the instance-level is no longer required, e.g., face-naming in videos where the group consists of faces in a frame, labeled by a name set in the corresponding caption. In this paper, we propose a novel graph convolutional network (GCN) called Dual Bipartite Graph Autoencoder (DB-GAE) to tackle the label ambiguity challenge of GPLL. First, we exploit the cross-group correlations to represent the instance groups as dual bipartite graphs: within-group and cross-group, which reciprocally complements each other to resolve the linking ambiguities. Second, we design a GCN autoencoder to encode and decode them, where the decodings are considered as the refined results. It is worth noting that DB-GAE is self-supervised and transductive, as it only uses the group-level supervision without a separate offline training stage. Extensive experiments on two real-world datasets demonstrate that DB-GAE significantly outperforms the best baseline over absolute 0.159 F1-score and 24.8% accuracy. We further offer analysis on various levels of label ambiguities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "2778637",
                    "name": "Alireza Zareian"
                },
                {
                    "authorId": "5462268",
                    "name": "Hanwang Zhang"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "377f49e7893236370ab6159875afddcd64fc1979",
            "title": "MemNAS: Memory-Efficient Neural Architecture Search With Grow-Trim Learning",
            "abstract": "Recent studies on automatic neural architecture search techniques have demonstrated significant performance, competitive to or even better than hand-crafted neural architectures. However, most of the existing search approaches tend to use residual structures and a concatenation connection between shallow and deep features. A resulted neural network model, therefore, is non-trivial for resource-constraint devices to execute since such a model requires large memory to store network parameters and intermediate feature maps along with excessive computing complexity. To address this challenge, we propose MemNAS, a novel growing and trimming based neural architecture search framework that optimizes not only performance but also memory requirement of an inference network. Specifically, in the search process, we consider running memory use, including network parameters and the essential intermediate feature maps memory requirement, as an optimization objective along with performance. Besides, to improve the accuracy of the search, we extract the correlation information among multiple candidate architectures to rank them and then choose the candidates with desired performance and memory efficiency. On the ImageNet classification task, our MemNAS achieves 75.4% accuracy, 0.7% higher than MobileNetV2 with 42.1% less memory requirement. Additional experiments confirm that the proposed MemNAS can perform well across the different targets of the trade-off between accuracy and memory consumption.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8699055",
                    "name": "Peiye Liu"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "144258295",
                    "name": "Huadong Ma"
                },
                {
                    "authorId": "2045759",
                    "name": "Mingoo Seok"
                }
            ]
        },
        {
            "paperId": "51c8975d88aa66781300e8ca88272ab3112445c0",
            "title": "GAIA: A Fine-grained Multimedia Knowledge Extraction System",
            "abstract": "We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology. Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation. The system is publicly available at GitHub and DockerHub, with a narrated video that documents the system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3361240",
                    "name": "Manling Li"
                },
                {
                    "authorId": "2778637",
                    "name": "Alireza Zareian"
                },
                {
                    "authorId": "2117032681",
                    "name": "Ying Lin"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "1817166",
                    "name": "Clare R. Voss"
                },
                {
                    "authorId": "1413386261",
                    "name": "Dan Napierski"
                },
                {
                    "authorId": "2052513135",
                    "name": "Marjorie Freedman"
                }
            ]
        },
        {
            "paperId": "8320ea909c38a616f9daccff4e5a49cfce4d9735",
            "title": "Analogical Reasoning for Visually Grounded Language Acquisition",
            "abstract": "Children acquire language subconsciously by observing the surrounding world and listening to descriptions. They can discover the meaning of words even without explicit language knowledge, and generalize to novel compositions effortlessly. In this paper, we bring this ability to AI, by studying the task of Visually grounded Language Acquisition (VLA). We propose a multimodal transformer model augmented with a novel mechanism for analogical reasoning, which approximates novel compositions by learning semantic mapping and reasoning operations from previously seen compositions. Our proposed method, Analogical Reasoning Transformer Networks (ARTNet), is trained on raw multimedia data (video frames and transcripts), and after observing a set of compositions such as \"washing apple\" or \"cutting carrot\", it can generalize and recognize new compositions in new video frames, such as \"washing carrot\" or \"cutting apple\". To this end, ARTNet refers to relevant instances in the training data and uses their visual features and captions to establish analogies with the query image. Then it chooses the suitable verb and noun to create a new composition that describes the new image best. Extensive experiments on an instructional video dataset demonstrate that the proposed method achieves significantly better generalization capability and recognition accuracy compared to state-of-the-art transformer models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "71660806",
                    "name": "Haoyu Qin"
                },
                {
                    "authorId": "2778637",
                    "name": "Alireza Zareian"
                },
                {
                    "authorId": "1856025",
                    "name": "Carl Vondrick"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "a9f7ebdae1f9df18a1f786cf4b92a0e5270753a9",
            "title": "Tensor FISTA-Net for Real-Time Snapshot Compressive Imaging",
            "abstract": "Snapshot compressive imaging (SCI) cameras capture high-speed videos by compressing multiple video frames into a measurement frame. However, reconstructing video frames from the compressed measurement frame is challenging. The existing state-of-the-art reconstruction algorithms suffer from low reconstruction quality or heavy time consumption, making them not suitable for real-time applications. In this paper, exploiting the powerful learning ability of deep neural networks (DNN), we propose a novel Tensor Fast Iterative Shrinkage-Thresholding Algorithm Net (Tensor FISTA-Net) as a decoder for SCI video cameras. Tensor FISTA-Net not only learns the sparsest representation of the video frames through convolution layers, but also reduces the reconstruction time significantly through tensor calculations. Experimental results on synthetic datasets show that the proposed Tensor FISTA-Net achieves average PSNR improvement of 1.63\u223c3.89dB over the state-of-the-art algorithms. Moreover, Tensor FISTA-Net takes less than 2 seconds running time and 12MB memory footprint, making it practical for real-time IoT applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118233782",
                    "name": "Xiao Han"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "2195345",
                    "name": "Zheng Shou"
                },
                {
                    "authorId": "2111039012",
                    "name": "Xiao-Yang Liu"
                },
                {
                    "authorId": "2108440701",
                    "name": "Yimeng Zhang"
                },
                {
                    "authorId": "3254296",
                    "name": "L. Kong"
                }
            ]
        },
        {
            "paperId": "fda5998c9ad122c9b1228b722bec3f76a66bd4d4",
            "title": "Participants Selection for From-Scratch Mobile Crowdsensing via Reinforcement Learning",
            "abstract": "Participant selection is a major research challenge in Mobile Crowdsensing (MCS). Previous approaches commonly assume that adequately long and fixed periods of candidate participants\u2019 historical mobility trajectories are available before the selection process. This enables the frameworks to accurately model mobility which enables the optimization of selection. However, this assumption may not be realistic for newly-released MCS applications or platforms because the candidates have just boarded without previous mobility profiles. The sparsity or even absence of mobility traces will incur inaccurate location prediction of the individual participant, thus imposing negative effects on the participant selection process and hindering the practical deployment of new MCS applications. To this end, this paper investigates a novel problem called \"From-Scratch MCS\" (FS-MCS for short), in which we study how to intelligently select participants to minimize such \"cold-start\" effect. Specifically, we propose a novel framework based on reinforcement learning, which we name RL-Recruiter. With the gradual accumulation of mobility trajectories over time, RL-Recruiter can make a good sequence of participant selection decisions for each sensing slot by incrementally extracting and utilizing the collective mobility patterns of all candidate participants, thus avoiding the prediction of individual participant\u2019s location that is very inaccurate when the training data is sparse. We test our approach experimentally based on two real-world mobility datasets. Our experiment results demonstrate that RL-Recruiter outperforms the baseline approaches under various settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1784629259",
                    "name": "Yunfan Hu"
                },
                {
                    "authorId": "2237603",
                    "name": "Jiangtao Wang"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "145044117",
                    "name": "A. Helal"
                }
            ]
        },
        {
            "paperId": "fe2437c17736da4b57791b0ce6d3084d4d3c4db7",
            "title": "Video Synthesis via Transform-Based Tensor Neural Network",
            "abstract": "Video frame synthesis is an important task in computer vision and has drawn great interests in wide applications. However, existing neural network methods do not explicitly impose tensor low-rankness of videos to capture the spatiotemporal correlations in a high-dimensional space, while existing iterative algorithms require hand-crafted parameters and take relatively long running time. In this paper, we propose a novel multi-phase deep neural network Transform-Based Tensor-Net that exploits the low-rank structure of video data in a learned transform domain, which unfolds an Iterative Shrinkage-Thresholding Algorithm (ISTA) for tensor signal recovery. Our design is based on two observations: (i) both linear and nonlinear transforms can be implemented by a neural network layer, and (ii) the soft-thresholding operator corresponds to an activation function. Further, such an unfolding design is able to achieve nearly real-time at the cost of training time and enjoys an interpretable nature as a byproduct. Experimental results on the KTH and UCF-101 datasets show that compared with the state-of-the-art methods, i.e., DVF and Super SloMo, the proposed scheme improves Peak Signal-to-Noise Ratio (PSNR) of video interpolation and prediction by 4.13 dB and 4.26 dB, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108440701",
                    "name": "Yimeng Zhang"
                },
                {
                    "authorId": "4029028",
                    "name": "Xiao-Yang Liu"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "30910424",
                    "name": "A. Walid"
                }
            ]
        },
        {
            "paperId": "24b72274f4d088f059056d50f086b4423fc518ba",
            "title": "Hard-Aware Fashion Attribute Classification",
            "abstract": "Fashion attribute classification is of great importance to many high-level tasks such as fashion item search, fashion trend analysis, fashion recommendation, etc. The task is challenging due to the extremely imbalanced data distribution, particularly the attributes with only a few positive samples. In this paper, we introduce a hard-aware pipeline to make full use of \"hard\" samples/attributes. We first propose Hard-Aware BackPropagation (HABP) to efficiently and adaptively focus on training \"hard\" data. Then for the identified hard labels, we propose to synthesize more complementary samples for training. To stabilize training, we extend semi-supervised GAN by directly deactivating outputs for synthetic complementary samples (Deact). In general, our method is more effective in addressing \"hard\" cases. HABP weights more on \"hard\" samples. For \"hard\" attributes with insufficient training data, Deact brings more stable synthetic samples for training and further improve the performance. Our method is verified on large scale fashion dataset, outperforming other state-of-the-art without any additional supervisions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "70419573",
                    "name": "Yun Ye"
                },
                {
                    "authorId": "2116173958",
                    "name": "Yixin Li"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "2155468228",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2055790840",
                    "name": "Ling-Yu Duan"
                },
                {
                    "authorId": "144025741",
                    "name": "Tao Mei"
                }
            ]
        },
        {
            "paperId": "86b7512e23470c26ffc3d6c7488e045c3fad7cfc",
            "title": "Dressing for Attention: Outfit Based Fashion Popularity Prediction",
            "abstract": "analysis of fashion trends is crucial. However, existing predictive algorithms of fashion popularity are restricted to be feasible on the coarse style level but not a finer item level. That is, they are only predictive in the future popularity of a given type of fashion styles (e.g., Rocker), but cannot be precisely down to a particular outfit look chosen by individuals. This paper thus proposes the first solution directly aimed at predicting the fine-grained fashion popularity of an outfit look by taking social media as the learning source. Particularly, a deep temporal sequence learning framework is developed and the proposed framework is evaluated on a real dataset of 380,000 street fashion images collected from the fashion website lookbook.nu. The experimental results show that our proposed framework outperforms the state-of-the-art approaches, with a relative increase of 11.51% to 27.62% (MSE metric) and 7.02% to 32.61% (CSE metric) in the prediction accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388566371",
                    "name": "Ling Lo"
                },
                {
                    "authorId": "2116362536",
                    "name": "Chia-Lin Liu"
                },
                {
                    "authorId": "2068171242",
                    "name": "Rongyu Lin"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "2426757",
                    "name": "Hong-Han Shuai"
                },
                {
                    "authorId": "1711298",
                    "name": "Wen-Huang Cheng"
                }
            ]
        }
    ]
}