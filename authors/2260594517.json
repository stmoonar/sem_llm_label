{
    "authorId": "2260594517",
    "papers": [
        {
            "paperId": "1961368c26f42f0e6777d34b6e9c40e179651bb1",
            "title": "Learning High-Quality and General-Purpose Phrase Representations",
            "abstract": "Phrase representations play an important role in data science and natural language processing, benefiting various tasks like Entity Alignment, Record Linkage, Fuzzy Joins, and Paraphrase Classification.The current state-of-the-art method involves fine-tuning pre-trained language models for phrasal embeddings using contrastive learning. However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences.Second, leveraging the phrase type and morphology gives phrase representations that are both more precise and more flexible.We propose an improved framework to learn phrase representations in a context-free fashion.The framework employs phrase type classification as an auxiliary task and incorporates character-level information more effectively into the phrase representation.Furthermore, we design three granularities of data augmentation to increase the diversity of training samples.Our experiments across a wide range of tasks reveal that our approach generates superior phrase embeddings compared to previous methods while requiring a smaller model size.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260594517",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                },
                {
                    "authorId": "2277286707",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "754f9c903754d909cb754364f4d6416ada0ab2b5",
            "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
            "abstract": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \\emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidence alignment with the accuracy of their responses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260594517",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "2149725904",
                    "name": "Alexandre Perez-Lebel"
                },
                {
                    "authorId": "2277286707",
                    "name": "Fabian M. Suchanek"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                }
            ]
        },
        {
            "paperId": "e79981c91c1ac40d747377b4af7409793d8e7350",
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "abstract": "Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at https://github.com/tigerchen52/role_of_small_models",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260594517",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                }
            ]
        },
        {
            "paperId": "2fd5de4b4f75234f980157b00e4db8af7a228578",
            "title": "The Locality and Symmetry of Positional Encodings",
            "abstract": "Positional Encodings (PEs) are used to inject word-order information into transformer-based language models. While they can significantly enhance the quality of sentence representations, their specific contribution to language models is not fully understood, especially given recent findings that various positional encodings are insensitive to word order. In this work, we conduct a systematic study of positional encodings in \\textbf{Bidirectional Masked Language Models} (BERT-style) , which complements existing work in three aspects: (1) We uncover the core function of PEs by identifying two common properties, Locality and Symmetry; (2) We show that the two properties are closely correlated with the performances of downstream tasks; (3) We quantify the weakness of current PEs by introducing two new probing tasks, on which current PEs perform poorly. We believe that these results are the basis for developing better PEs for transformer-based language models. The code is available at \\faGithub~ \\url{https://github.com/tigerchen52/locality\\_symmetry}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260594517",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        }
    ]
}