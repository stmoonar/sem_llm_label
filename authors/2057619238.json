{
    "authorId": "2057619238",
    "papers": [
        {
            "paperId": "0d2b80e189a350aab8747934b6cbffc2a0039474",
            "title": "Towards Controllable Natural Language Inference through Lexical Inference Types",
            "abstract": "Explainable natural language inference aims to provide a mechanism to produce explanatory (abductive) inference chains which ground claims to their supporting premises. A recent corpus called EntailmentBank strives to advance this task by explaining the answer to a question using an entailment tree \\cite{dalvi2021explaining}. They employ the T5 model to directly generate the tree, which can explain how the answer is inferred. However, it lacks the ability to explain and control the generation of intermediate steps, which is crucial for the multi-hop inference process. % One recent corpus, EntailmentBank, aims to push this task forward by explaining an answer to a question according to an entailment tree \\cite{dalvi2021explaining}. They employ T5 to generate the tree directly, which can explain how the answer is inferred but cannot explain how the intermediate is generated, which is essential to the multi-hop inference process. In this work, we focus on proposing a controlled natural language inference architecture for multi-premise explanatory inference. To improve control and enable explanatory analysis over the generation, we define lexical inference types based on Abstract Meaning Representation (AMR) graph and modify the architecture of T5 to learn a latent sentence representation (T5 bottleneck) conditioned on said type information. We also deliver a dataset of approximately 5000 annotated explanatory inference steps, with well-grounded lexical-symbolic operations. Experimental results indicate that the inference typing induced at the T5 bottleneck can help T5 to generate a conclusion under explicit control.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108307966",
                    "name": "Yingji Zhang"
                },
                {
                    "authorId": "38424153",
                    "name": "Danilo S. Carvalho"
                },
                {
                    "authorId": "1400949657",
                    "name": "Ian Pratt-Hartmann"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                }
            ]
        },
        {
            "paperId": "4703cbd3743ff81297c64007db0109d96dec98c0",
            "title": "Generating Mathematical Derivations with Large Language Models",
            "abstract": "The derivation of mathematical results in specialised fields, using Large Language Models (LLMs), is an emerging research direction that can help identify models' limitations, and potentially support mathematical discovery. In this paper, we leverage a symbolic engine to generate derivations of equations at scale, and investigate the capabilities of LLMs when deriving goal equations from premises. Specifically, we employ in-context learning for GPT and fine-tune a range of T5 models to compare the robustness and generalisation of pre-training strategies to specialised models. Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in conventional scores. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse 1.7K equations, and over 200 derivations, to highlight common reasoning errors such as the inclusion of incorrect, irrelevant, and redundant equations. Finally, we explore the suitability of existing metrics for evaluating mathematical derivations and find evidence that, while they can capture general properties such as sensitivity to perturbations, they fail to highlight fine-grained reasoning errors and essential differences between models. Overall, this work demonstrates that training models on synthetic data may improve their math capabilities beyond much larger LLMs, but current metrics are not appropriately assessing the quality of generated mathematical text.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "151056786",
                    "name": "Jordan Meadows"
                },
                {
                    "authorId": "34102057",
                    "name": "Marco Valentino"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                }
            ]
        },
        {
            "paperId": "4f0c7f4df04f07609bdb67944af2a529d5a4517b",
            "title": "A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation",
            "abstract": "Large language models (LLMs) have exploded a new heatwave of AI for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V&V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107903140",
                    "name": "Xiaowei Huang"
                },
                {
                    "authorId": "8103173",
                    "name": "Wenjie Ruan"
                },
                {
                    "authorId": "2152492444",
                    "name": "Wei Huang"
                },
                {
                    "authorId": "2071131446",
                    "name": "Gao Jin"
                },
                {
                    "authorId": "1410228759",
                    "name": "Yizhen Dong"
                },
                {
                    "authorId": "93366400",
                    "name": "Changshun Wu"
                },
                {
                    "authorId": "144887026",
                    "name": "S. Bensalem"
                },
                {
                    "authorId": "2139856775",
                    "name": "Ronghui Mu"
                },
                {
                    "authorId": "2117830923",
                    "name": "Yi Qi"
                },
                {
                    "authorId": "47039303",
                    "name": "Xingyu Zhao"
                },
                {
                    "authorId": "2128294595",
                    "name": "Kaiwen Cai"
                },
                {
                    "authorId": "95681022",
                    "name": "Yanghao Zhang"
                },
                {
                    "authorId": "2218423966",
                    "name": "Sihao Wu"
                },
                {
                    "authorId": "50591107",
                    "name": "Peipei Xu"
                },
                {
                    "authorId": "2115875650",
                    "name": "Dengyu Wu"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                },
                {
                    "authorId": "144411037",
                    "name": "Mustafa A. Mustafa"
                }
            ]
        },
        {
            "paperId": "95fef3a8cd12d3fe9be85c42e94741a6bee03fb1",
            "title": "Interventional Probing in High Dimensions: An NLI Case Study",
            "abstract": "Probing strategies have been shown to detectthe presence of various linguistic features inlarge language models; in particular, seman-tic features intermediate to the \u201cnatural logic\u201dfragment of the Natural Language Inferencetask (NLI). In the case of natural logic, the rela-tion between the intermediate features and theentailment label is explicitly known: as such,this provides a ripe setting for interventionalstudies on the NLI models\u2019 representations, al-lowing for stronger causal conjectures and adeeper critical analysis of interventional prob-ing methods. In this work, we carry out newand existing representation-level interventionsto investigate the effect of these semantic fea-tures on NLI classification: we perform am-nesic probing (which removes features as di-rected by learned linear probes) and introducethe mnestic probing variation (which forgetsall dimensions except the probe-selected ones).Furthermore, we delve into the limitations ofthese methods and outline some pitfalls havebeen obscuring the effectivity of interventionalprobing studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8471045",
                    "name": "Julia Rozanova"
                },
                {
                    "authorId": "34102057",
                    "name": "Marco Valentino"
                },
                {
                    "authorId": "2163521113",
                    "name": "Lucas C. Cordeiro"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                }
            ]
        },
        {
            "paperId": "9a20f044fbb9c3dce0feebe0339eca56d68781dc",
            "title": "Estimating the Causal Effects of Natural Logic Features in Neural NLI Models",
            "abstract": "Rigorous evaluation of the causal effects of semantic features on language model predictions can be hard to achieve for natural language reasoning problems. However, this is such a desirable form of analysis from both an interpretability and model evaluation perspective, that it is valuable to zone in on specific patterns of reasoning with enough structure and regularity to be able to identify and quantify systematic reasoning failures in widely-used models. In this vein, we pick a portion of the NLI task for which an explicit causal diagram can be systematically constructed: in particular, the case where across two sentences (the premise and hypothesis), two related words/terms occur in a shared context. In this work, we apply causal effect estimation strategies to measure the effect of context interventions (whose effect on the entailment label is mediated by the semantic monotonicity characteristic) and interventions on the inserted word-pair (whose effect on the entailment label is mediated by the relation between these words.). Following related work on causal analysis of NLP models in different settings, we adapt the methodology for the NLI task to construct comparative model profiles in terms of robustness to irrelevant changes and sensitivity to impactful changes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8471045",
                    "name": "Julia Rozanova"
                },
                {
                    "authorId": "34102057",
                    "name": "Marco Valentino"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                }
            ]
        },
        {
            "paperId": "ad73c9595878d826da5450685d4bdbb7a9fb2df3",
            "title": "A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers",
            "abstract": "This paper proposes a methodology for generating and perturbing detailed derivations of equations at scale, aided by a symbolic engine, to evaluate the generalisability of Transformers to out-of-distribution mathematical reasoning problems. Instantiating the framework in the context of sequence classification tasks, we compare the capabilities of GPT-4, GPT-3.5, and a canon of fine-tuned BERT models, exploring the relationship between specific operators and generalisation failure via the perturbation of reasoning aspects such as symmetry and variable surface forms. Surprisingly, our empirical evaluation reveals that the average in-distribution performance of fine-tuned models surpasses GPT-3.5, and rivals GPT-4. However, perturbations to input reasoning can reduce their performance by up to 80 F1 points. Overall, the results suggest that the in-distribution performance of smaller open-source models may potentially rival GPT by incorporating appropriately structured derivation dependencies during training, and highlight a shared weakness between BERT and GPT involving a relative inability to decode indirect references to mathematical entities. We release the full codebase, constructed datasets, and fine-tuned models to encourage future progress in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151056786",
                    "name": "Jordan Meadows"
                },
                {
                    "authorId": "34102057",
                    "name": "Marco Valentino"
                },
                {
                    "authorId": "2406263",
                    "name": "Damien Teney"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                }
            ]
        },
        {
            "paperId": "25867f1d98c19e55ea34c1137778081749fdfcdd",
            "title": "EnnCore: End-to-End Conceptual Guarding of Neural Architectures",
            "abstract": ".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7415464",
                    "name": "Edoardo Manino"
                },
                {
                    "authorId": "38424153",
                    "name": "Danilo S. Carvalho"
                },
                {
                    "authorId": "1410228759",
                    "name": "Yizhen Dong"
                },
                {
                    "authorId": "8471045",
                    "name": "Julia Rozanova"
                },
                {
                    "authorId": "40809275",
                    "name": "Xidan Song"
                },
                {
                    "authorId": "27020937",
                    "name": "A. Mustafa"
                },
                {
                    "authorId": "2158565366",
                    "name": "Mustafa"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                },
                {
                    "authorId": "2148612970",
                    "name": "Gavin Brown"
                },
                {
                    "authorId": "145141724",
                    "name": "M. Luj\u00e1n"
                },
                {
                    "authorId": "2107903140",
                    "name": "Xiaowei Huang"
                },
                {
                    "authorId": "144040212",
                    "name": "L. Cordeiro"
                }
            ]
        },
        {
            "paperId": "51f4eb5d91f65bbfeff0a9a015092fe8f64f6873",
            "title": "Decomposing Natural Logic Inferences for Neural NLI",
            "abstract": "In the interest of interpreting neural NLI models and their reasoning strategies, we carry out a systematic probing study which investigates whether these modelscapture the crucial semantic features central to natural logic: monotonicity and concept inclusion.Correctly identifying valid inferences in downward-monotone contexts is a known stumbling block for NLI performance,subsuming linguistic phenomena such as negation scope and generalized quantifiers.To understand this difficulty, we emphasize monotonicity as a property of a context and examine the extent to which models capture relevant monotonicity information in the vector representations which are intermediate to their decision making process.Drawing on the recent advancement of the probing paradigm,we compare the presence of monotonicity features across various models.We find that monotonicity information is notably weak in the representations of popularNLI models which achieve high scores on benchmarks, and observe that previous improvements to these models based on fine-tuning strategies have introduced stronger monotonicity features together with their improved performance on challenge sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8471045",
                    "name": "Julia Rozanova"
                },
                {
                    "authorId": "2058261256",
                    "name": "Deborah Ferreira"
                },
                {
                    "authorId": "2139400798",
                    "name": "Mokanarangan Thayaparan"
                },
                {
                    "authorId": "34102057",
                    "name": "Marco Valentino"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                }
            ]
        },
        {
            "paperId": "77985506b8a0c036f05247087adce9e7f469b9ea",
            "title": "Montague semantics and modifier consistency measurement in neural language models",
            "abstract": "In recent years, distributional language representation models have demonstrated great practical success. At the same time, the need for interpretability has elicited questions on their intrinsic properties and capabilities. Crucially, distributional models are often inconsistent when dealing with compositional phenomena in natural language, which has significant implications for their safety and fairness. Despite this, most current research on compositionality is directed towards improving their performance on similarity tasks only. This work takes a different approach, and proposes a methodology for measuring compositional behavior in contemporary language models. Specifically, we focus on adjectival modifier phenomena in adjective-noun phrases. We introduce three novel tests of compositional behavior inspired by Montague semantics. Our experimental results indicate that current neural language models behave according to the expected linguistic theories to a limited extent only. This raises the question of whether these language models are not able to capture the semantic properties we evaluated, or whether linguistic theories from Montagovian tradition would not match the expected capabilities of distributional models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38424153",
                    "name": "Danilo S. Carvalho"
                },
                {
                    "authorId": "7415464",
                    "name": "Edoardo Manino"
                },
                {
                    "authorId": "8471045",
                    "name": "Julia Rozanova"
                },
                {
                    "authorId": "2163521113",
                    "name": "Lucas C. Cordeiro"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                }
            ]
        },
        {
            "paperId": "82be2b02c8fbedb94ef9170174e026c5d315b8ac",
            "title": "Systematicity, Compositionality and Transitivity of Deep NLP Models: a Metamorphic Testing Perspective",
            "abstract": "Metamorphic testing has recently been used to check the safety of neural NLP models. Its main advantage is that it does not rely on a ground truth to generate test cases. However, existing studies are mostly concerned with robustness-like metamorphic relations, limiting the scope of linguistic properties they can test. We propose three new classes of metamorphic relations, which address the properties of systematicity, compositionality and transitivity. Unlike robustness, our relations are defined over multiple source inputs, thus increasing the number of test cases that we can produce by a polynomial factor. With them, we test the internal consistency of state-of-the-art NLP models, and show that they do not always behave according to their expected linguistic properties. Lastly, we introduce a novel graphical notation that efficiently summarises the inner structure of metamorphic relations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7415464",
                    "name": "Edoardo Manino"
                },
                {
                    "authorId": "8471045",
                    "name": "Julia Rozanova"
                },
                {
                    "authorId": "38424153",
                    "name": "Danilo S. Carvalho"
                },
                {
                    "authorId": "2057619238",
                    "name": "Andr\u00e9 Freitas"
                },
                {
                    "authorId": "2163521113",
                    "name": "Lucas C. Cordeiro"
                }
            ]
        }
    ]
}