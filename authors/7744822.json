{
    "authorId": "7744822",
    "papers": [
        {
            "paperId": "fddc440bcb7ad2b444e27b72fe95f52bf4c1315f",
            "title": "A Dataset for Large Language Model-Driven AI Accelerator Generation",
            "abstract": "\u2014In the ever-evolving landscape of Deep Neural Networks (DNN) hardware acceleration, unlocking the true potential of systolic array accelerators has long been hindered by the daunting challenges of expertise and time investment. Large Language Models (LLMs) offer a promising solution for automating code generation which is key to unlocking unprecedented efficiency and performance in various domains, including hardware descriptive code. However, the successful application of LLMs to hardware accelerator design is contingent upon the availability of specialized datasets tailored for this purpose. To bridge this gap, we introduce the Systolic Array-based Accelerator DataSet (SA-DS). SA-DS comprises of a diverse collection of spatial arrays following the standardized Berkeley\u2019s Gemmini accelerator generator template, enabling design reuse, adaptation, and customization. SA-DS is intended to spark LLM-centred research on DNN hardware accelerator architecture. We envision that SA-DS provides a framework which will shape the course of DNN hardware acceleration research for generations to come. SA-DS is open-sourced under the permissive MIT license at this https://github.com/ACADLab/SA-DS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "2226523621",
                    "name": "Deepak Vungarala"
                },
                {
                    "authorId": "1475748342",
                    "name": "Mehrdad Morsali"
                },
                {
                    "authorId": "2297250450",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2297352834",
                    "name": "Arnob Ghosh"
                },
                {
                    "authorId": "1750196",
                    "name": "Abdallah Khreishah"
                },
                {
                    "authorId": "1824271",
                    "name": "Shaahin Angizi"
                }
            ]
        },
        {
            "paperId": "374ff52faeea2a519a3b8eafcf0fc10e9e1b96ad",
            "title": "Adversarial NLP for Social Network Applications: Attacks, Defenses, and Research Directions",
            "abstract": "The growing use of media has led to the development of several machine learning (ML) and natural language processing (NLP) tools to process the unprecedented amount of social media content to make actionable decisions. However, these ML and NLP algorithms have been widely shown to be vulnerable to adversarial attacks. These vulnerabilities allow adversaries to launch a diversified set of adversarial attacks on these algorithms in different applications of social media text processing. In this article, we provide a comprehensive review of the main approaches for adversarial attacks and defenses in the context of social media applications with a particular focus on key challenges and future research directions. In detail, we cover literature on six key applications: 1) rumors detection; 2) satires detection; 3) clickbaits and spams identification; 4) hate speech detection; 5) misinformation detection; and 6) sentiment analysis. We then highlight the concurrent and anticipated future research questions and provide recommendations and directions for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1770016",
                    "name": "I. Alsmadi"
                },
                {
                    "authorId": "143623051",
                    "name": "Kashif Ahmad"
                },
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1404786833",
                    "name": "Ala I. Al-Fuqaha"
                },
                {
                    "authorId": "1750196",
                    "name": "Abdallah Khreishah"
                },
                {
                    "authorId": "3027509",
                    "name": "A. Algosaibi"
                }
            ]
        },
        {
            "paperId": "42e7ea6eb841fe41d949f72d6778519a4b5b3542",
            "title": "Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection",
            "abstract": "Malicious domain detection (MDD) is an open security challenge that aims to detect if an Internet domain name is associated with cyber attacks. Many techniques have been applied to tackle this problem, among which graph neural networks (GNNs) are deemed one of the most effective approaches. GNN-based MDD employs domain name system (DNS) logs to represent Internet domains as nodes in a graph, dubbed domain maliciousness graph (DMG) and trains a GNN model to infer the maliciousness of Internet domains by leveraging the maliciousness of already identified ones. As this method heavily relies on the \"publicly\" accessible DNS logs to build DMGs, it creates a vulnerability for adversaries to manipulate the features and edges of their domain nodes within these graphs. The current body of literature primarily focuses on threat models that involve manipulating individual adversary (attacker) nodes. Nonetheless, adversaries usually create numerous domains to accomplish their attack objectives, aiming to reduce costs and evade detection. Hence, they aim to remain undetected across as many domains as possible. In this work, we call the attack that manipulates several nodes in the DMG concurrently a multi-instance evasion attack. To the best of our knowledge, this type of attack has not been explored in the prior art. We present both theoretical and empirical evidence to show that the existing single-instance evasion techniques for GNN-based MDDs are inadequate to launch multi-instance evasion attacks. Therefore, we propose an inference-time, multi-instance adversarial attack, dubbed MintA, against GNN-based MDD. MintA optimizes node perturbations to enhance the evasiveness of a node and its neighborhood. MintA only requires black-box access to the target model to launch the attack successfully. In other words, MintA does not require any knowledge of the MDD model\u2019s parameters, architecture, or information on non-adversary nodes. We formulate an optimization problem that satisfies the attack objectives of MintA and devise an approximate solution for it. We evaluate MintA on a state-of-the-art GNN-based MDD technique using real-world data, and our experiments demonstrate an attack success rate of over 80%. The findings of this study serve as a cautionary note for security experts, highlighting the vulnerability of GNN-based MDD to practical attacks that can impede the effectiveness and advantages of this approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "1783739",
                    "name": "Issa M. Khalil"
                },
                {
                    "authorId": "1750196",
                    "name": "Abdallah Khreishah"
                },
                {
                    "authorId": "11032760",
                    "name": "Nhathai Phan"
                },
                {
                    "authorId": "2148984778",
                    "name": "Yao Ma"
                }
            ]
        },
        {
            "paperId": "83c974649f026682ee1ae57faa6097bae51af88d",
            "title": "Estimating Multi-Dimensional Sparsity Level for Spectrum Sensing",
            "abstract": "Identifying spectrum opportunities is a crucial element of efficient spectrum utilization for future wireless networks. Spectrum sensing offers a convenient means for revealing such opportunities. Studies showed that usage of the spectrum has a high correlation over multi-dimensions, including time and frequency. However, multi-dimensional spectrum sensing requires high-cost processes. Applying compressive sensing allows for subNyquist sampling. This reduces associated training, feedback, and computation overheads of a spectrum sensing method. However, the accuracy of the signal sparsity assumption and knowledge of the precise sparsity level are necessary for the applicability of compressive sensing. It is common practice to assume a level of known sparsity. On the other hand, in reality, this presumption is incorrect. This paper proposes a method for estimating the multidimensional sparsity for spectrum sensing. By extrapolating it from its counterpart with respect to a compact discrete Fourier basis, the proposed method calculates the sparsity level over a dictionary. A machine learning estimation method achieves this inference. Extensive simulations validate a high-quality sparsity estimation. To validate this observation, real-world measurements are used, where one of the biggest Turkish telecom operators has private uplink bands in the frequency range between 852-856 MHz.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151105004",
                    "name": "M. A. Ayg\u00fcl"
                },
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "143977219",
                    "name": "H. Arslan"
                }
            ]
        },
        {
            "paperId": "85e265c696de23bc88420b1d31f38b74890d1717",
            "title": "Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification",
            "abstract": "Federated learning enables multiple clients to collaboratively contribute to the learning of a global model orchestrated by a central server. This learning scheme promotes clients' data privacy and requires reduced communication overheads. In an application like network traffic classification, this helps hide the network vulnerabilities and weakness points. However, federated learning is susceptible to backdoor attacks, in which adversaries inject manipulated model updates into the global model. These updates inject a salient functionality in the global model that can be launched with specific input patterns. Nonetheless, the vulnerability of network traffic classification models based on federated learning to these attacks remains unexplored. In this paper, we propose GABAttack, a novel genetic algorithm-based backdoor attack against federated learning for network traffic classification. GABAttack utilizes a genetic algorithm to optimize the values and locations of backdoor trigger patterns, ensuring a better fit with the input and the model. This input-tailored dynamic attack is promising for improved attack evasiveness while being effective. Extensive experiments conducted over real-world network datasets validate the success of the proposed GABAttack in various situations while maintaining almost invisible activity. This research serves as an alarming call for network security experts and practitioners to develop robust defense measures against such attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "8776688",
                    "name": "Nura Aljaafari"
                },
                {
                    "authorId": "9196469",
                    "name": "Ahmad H. Sawalmeh"
                },
                {
                    "authorId": "1750196",
                    "name": "Abdallah Khreishah"
                },
                {
                    "authorId": "2066279604",
                    "name": "Muhammad Anan"
                },
                {
                    "authorId": "2222196358",
                    "name": "A. Algosaibi"
                },
                {
                    "authorId": "2165725339",
                    "name": "Mohammed Alnaeem"
                },
                {
                    "authorId": "2063954417",
                    "name": "Adel Aldalbahi"
                },
                {
                    "authorId": "2165725254",
                    "name": "Abdulaziz Alhumam"
                },
                {
                    "authorId": "71332955",
                    "name": "C. Vizcarra"
                },
                {
                    "authorId": "2021311397",
                    "name": "Shadan Alhamed"
                }
            ]
        },
        {
            "paperId": "b68288844a297b608df2f1cac985e70974b4a45a",
            "title": "IMA-GNN: In-Memory Acceleration of Centralized and Decentralized Graph Neural Networks at the Edge",
            "abstract": "In this paper, we propose IMA-GNN as an In-Memory Accelerator for centralized and decentralized Graph Neural Network inference, explore its potential in both settings and provide a guideline for the community targeting flexible and efficient edge computation. Leveraging IMA-GNN, we first model the computation and communication latencies of edge devices. We then present practical case studies on GNN-based taxi demand and supply prediction and also adopt four large graph datasets to quantitatively compare and analyze centralized and decentralized settings. Our cross-layer simulation results demonstrate that on average, IMA-GNN in the centralized setting can obtain ~790x communication speed-up compared to the decentralized GNN setting. However, the decentralized setting performs computation ~1400x faster while reducing the power consumption per device. This further underlines the need for a hybrid semi-decentralized GNN approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1475748342",
                    "name": "Mehrdad Morsali"
                },
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "1750196",
                    "name": "Abdallah Khreishah"
                },
                {
                    "authorId": "1824271",
                    "name": "Shaahin Angizi"
                }
            ]
        },
        {
            "paperId": "c54f940b99664bd29c1766a965d1f4c5af96fa8b",
            "title": "Semi-decentralized Inference in Heterogeneous Graph Neural Networks for Traffic Demand Forecasting: An Edge-Computing Approach",
            "abstract": "Prediction of taxi service demand and supply is essential for improving customer's experience and provider's profit. Recently, graph neural networks (GNNs) have been shown promising for this application. This approach models city regions as nodes in a transportation graph and their relations as edges. GNNs utilize local node features and the graph structure in the prediction. However, more efficient forecasting can still be achieved by following two main routes; enlarging the scale of the transportation graph, and simultaneously exploiting different types of nodes and edges in the graphs. However, both approaches are challenged by the scalability of GNNs. An immediate remedy to the scalability challenge is to decentralize the GNN operation. However, this creates excessive node-to-node communication. In this paper, we first characterize the excessive communication needs for the decentralized GNN approach. Then, we propose a semi-decentralized approach utilizing multiple cloudlets, moderately sized storage and computation devices, that can be integrated with the cellular base stations. This approach minimizes inter-cloudlet communication thereby alleviating the communication overhead of the decentralized approach while promoting scalability due to cloudlet-level decentralization. Also, we propose a heterogeneous GNN-LSTM algorithm for improved taxi-level demand and supply forecasting for handling dynamic taxi graphs where nodes are taxis. Extensive experiments over real data show the advantage of the semi-decentralized approach as tested over our heterogeneous GNN-LSTM algorithm. Also, the proposed semi-decentralized GNN approach is shown to reduce the overall inference time by about an order of magnitude compared to centralized and decentralized inference schemes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "1750196",
                    "name": "Abdallah Khreishah"
                },
                {
                    "authorId": "3172194",
                    "name": "Joyoung Lee"
                },
                {
                    "authorId": "1824271",
                    "name": "Shaahin Angizi"
                },
                {
                    "authorId": "1389945327",
                    "name": "Ala Al-Fuqaha"
                },
                {
                    "authorId": "2065439840",
                    "name": "M. Guizani"
                }
            ]
        },
        {
            "paperId": "3de9b1c9e0e63b186a961e33d6b468386536eedc",
            "title": "Self-Optimizing Data Offloading in Mobile Heterogeneous Radio-Optical Networks: A Deep Reinforcement Learning Approach",
            "abstract": "In addition to the exploration of more spectrum at high-frequency bands, next-generation wireless networks will witness an intelligent convergence of radio frequency (RF) and non-RF links such as optical and visible light communication. Optical attocell (OAC) networks provide an additional layer to RF-based wireless networks with gigabit-per-second data transmission rate and centimeter-level location accuracy. However, the directionality, line-of-sight constraints, as well as strong sensitivity to the location and orientation of user terminals challenge the stringent requirements for throughput and latency. In this article, we consider mobile heterogeneous networks (HetNets) incorporating indoor OAC with femtocells and macrocells to provide a low-cost and energy-efficient solution. The HetNets solution satisfies diverse service requirements in terms of user-experienced data rate, mobility, latency, accuracy, and security in the Internet of Things. To support seamless connectivity and optimal resource allocation in the proposed HetNets with mobility awareness, handover in dynamic environments needs to be addressed efficiently. Incorporating rich environmental parameters into such a decision making problem facilitates the self-optimization process, but extensively expands the state space. To achieve a fast convergence speed, a deep reinforcement learning approach is proposed to optimize the handover parameters (e.g., time-to-trigger and hysteresis margin). This is a model-free and off-policy reinforcement setting that trains and employs a deep neural network to predict future rewards for successions of states and actions. Thus, the optimal parameters are obtained by selecting the best actions to take. Through numerical simulation and performance analysis, we discover the gain from enriching the state space and the adaptability of the system to dynamic environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2194154",
                    "name": "Sihua Shao"
                },
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "1750196",
                    "name": "Abdallah Khreishah"
                },
                {
                    "authorId": "2693447",
                    "name": "M. Ayyash"
                }
            ]
        },
        {
            "paperId": "4ed32676549a7123482f80fb24e98f3db16caa89",
            "title": "Adversarial Machine Learning in Text Processing: A Literature Survey",
            "abstract": "Machine learning algorithms represent the intelligence that controls many information systems and applications around us. As such, they are targeted by attackers to impact their decisions. Text created by machine learning algorithms has many types of applications, some of which can be considered malicious especially if there is an intention to present machine-generated text as human-generated. In this paper, we surveyed major subjects in adversarial machine learning for text processing applications. Unlike adversarial machine learning in images, text problems and applications are heterogeneous. Thus, each problem can have its own challenges. We focused on some of the evolving research areas such as: malicious versus genuine text generation metrics, defense against adversarial attacks, and text generation models and algorithms. Our study showed that as applications of text generation will continue to grow in the near future, the type and nature of attacks on those applications and their machine learning algorithms will continue to grow as well. Literature survey indicated an increasing trend in using pre-trained models in machine learning. Word/sentence embedding models and transformers are examples of those pre-trained models. Adversarial models may utilize same or similar pre-trained models as well. In another trend related to text generation models, literature showed effort to develop universal text perturbations to be used in both black-and white-box attack settings. Literature showed also using conditional GANs to create latent representation for writing types. This usage will allow for a seamless lexical and grammatical transition between various writing styles. In text generation metrics, research trends showed developing successful automated or semi-automated assessment metrics that may include human judgement. Literature showed also research trends of designing and developing new memory models that increase performance and memory utilization efficiency without validating real-time constraints. Many research efforts evaluate different defense model approaches and algorithms. Researchers evaluated different types of targeted attacks, and methods to distinguish human versus machine generated text.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1770016",
                    "name": "I. Alsmadi"
                },
                {
                    "authorId": "8776688",
                    "name": "Nura Aljaafari"
                },
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "2021311397",
                    "name": "Shadan Alhamed"
                },
                {
                    "authorId": "9196469",
                    "name": "Ahmad H. Sawalmeh"
                },
                {
                    "authorId": "71332955",
                    "name": "C. Vizcarra"
                },
                {
                    "authorId": "1750196",
                    "name": "Abdallah Khreishah"
                },
                {
                    "authorId": "2066279604",
                    "name": "Muhammad Anan"
                },
                {
                    "authorId": "3027509",
                    "name": "A. Algosaibi"
                },
                {
                    "authorId": "1403809459",
                    "name": "Mohammed Alnaeem"
                },
                {
                    "authorId": "2063954417",
                    "name": "Adel Aldalbahi"
                },
                {
                    "authorId": "2047936574",
                    "name": "Abdulaziz Alhumam"
                }
            ]
        },
        {
            "paperId": "6b58d6dbefab1646fbd6375959fd590ceabcc1f8",
            "title": "Deep RL-Based Spectrum Occupancy Prediction Exploiting Time and Frequency Correlations",
            "abstract": "In cognitive radio systems, predicting spectrum occupancies is a convenient alternative way to continuous spectrum sensing. It can provide information on spectrum usage and so empty spectrum bands can be used by secondary users. The usage of the spectrum bands is highly correlated over both time and frequency. Recently, machine learning algorithms are used to predict spectrum occupancy by exploiting such correlations. However, this approach primarily assumes a supervised learning setting. Despite its outstanding performance, this setting requires the availability of sufficiently large datasets (of labeled data) and is not adaptive to environment changes. In this paper, different from the existing literature, a deep reinforcement learning (RL) algorithm is used to alleviate those shortcomings. In this algorithm, we define the reward functions of the deep RL setting and its state and action spaces such that it is applicable to work dynamically, in an online fashion, in real world settings. Extensive experiments validate the capability of the proposed algorithm in predicting spectrum occupancies as examined over real world spectrum measurements. These are carried out in the 832-862 megahertz frequency bands, which are used by the leading Turkish telecom providers as private uplink bands. This is a significant step towards realizing a standalone spectrum occupancy prediction operation without any control from the operator and minimizing memory requirements while alleviating the need for the labeled dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151105004",
                    "name": "M. A. Ayg\u00fcl"
                },
                {
                    "authorId": "7744822",
                    "name": "Mahmoud Nazzal"
                },
                {
                    "authorId": "143977219",
                    "name": "H. Arslan"
                }
            ]
        }
    ]
}