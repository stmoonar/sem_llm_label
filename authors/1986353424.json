{
    "authorId": "1986353424",
    "papers": [
        {
            "paperId": "65b59f4ffae4f5b9e5d63fc1d11bf9ca7f7df34d",
            "title": "Calibration and Correctness of Language Models for Code",
            "abstract": "Machine learning models are widely used, but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. A well-calibrated confidence measure can serve as a basis for rational, graduated decision-making on how much review and care is needed when using generated code. Calibration has so far been studied in mostly non-generative (e.g. classification) settings, especially in software engineering. However, generated code can quite often be wrong: Given generated code, developers must decide whether to use directly, use after varying intensity of careful review, or discard model-generated code. Thus, calibration is vital in generative settings. We make several contributions. We develop a framework for evaluating the calibration of code-generating models. We consider several tasks, correctness criteria, datasets, and approaches, and find that, by and large, generative code models we test are not well-calibrated out of the box. We then show how calibration can be improved using standard methods, such as Platt scaling. Since Platt scaling relies on the prior availability of correctness data, we evaluate the applicability and generalizability of Platt scaling in software engineering, discuss settings where it has good potential for practical use, and settings where it does not. Our contributions will lead to better-calibrated decision-making in the current use of code generated by language models, and offers a framework for future research to further improve calibration methods for generative models in software engineering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268784373",
                    "name": "Claudio Spiess"
                },
                {
                    "authorId": "1986353424",
                    "name": "David Gros"
                },
                {
                    "authorId": "2214523312",
                    "name": "Kunal Suresh Pai"
                },
                {
                    "authorId": "2282536979",
                    "name": "Michael Pradel"
                },
                {
                    "authorId": "9392397",
                    "name": "Md Rafiqul Islam Rabin"
                },
                {
                    "authorId": "2282536532",
                    "name": "Susmit Jha"
                },
                {
                    "authorId": "114875459",
                    "name": "Prem Devanbu"
                },
                {
                    "authorId": "2271468417",
                    "name": "Toufique Ahmed"
                }
            ]
        },
        {
            "paperId": "64bb97277140067aa4cedc996d0d5c2e34e4319b",
            "title": "AI Safety Subproblems for Software Engineering Researchers",
            "abstract": "In this 4-page manuscript we discuss the problem of long-term AI Safety from a Software Engineering (SE) research viewpoint. We briefly summarize long-term AI Safety, and the challenge of avoiding harms from AI as systems meet or exceed human capabilities, including software engineering capabilities (and approach AGI /\"HLMI\"). We perform a quantified literature review suggesting that AI Safety discussions are not common at SE venues. We make conjectures about how software might change with rising capabilities, and categorize\"subproblems\"which fit into traditional SE areas, proposing how work on similar problems might improve the future of AI and SE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1986353424",
                    "name": "David Gros"
                },
                {
                    "authorId": "69414245",
                    "name": "P. Devanbu"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "3eacaa1a4a4de6d653cb9b53835d97bf9c560a23",
            "title": "A Goal-Driven Natural Language Interface for Creating Application Integration Workflows",
            "abstract": "Web applications and services are increasingly important in a distributed internet filled with diverse cloud services and applications, each of which enable the completion of narrowly defined tasks. Given the explosion in the scale and diversity of such services, their composition and integration for achieving complex user goals remains a challenging task for end-users and requires a lot of development effort when specified by hand. We present a demonstration of the Goal Oriented Flow Assistant (GOFA) system, which provides a natural language solution to generate workflows for application integration. Our tool is built on a three-step pipeline: it first uses Abstract Meaning Representation (AMR) to parse utterances; it then uses a knowledge graph to validate candidates; and finally uses an AI planner to compose the candidate flow. We provide a video demonstration of the deployed system as part of our submission.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072258741",
                    "name": "Michelle Brachman"
                },
                {
                    "authorId": "2158531108",
                    "name": "Christopher Bygrave"
                },
                {
                    "authorId": "2500065",
                    "name": "Tathagata Chakraborti"
                },
                {
                    "authorId": "1380490598",
                    "name": "Arunima Chaudhary"
                },
                {
                    "authorId": "2158530827",
                    "name": "Zhining"
                },
                {
                    "authorId": "2158529338",
                    "name": "Ding"
                },
                {
                    "authorId": "2391727",
                    "name": "Casey Dugan"
                },
                {
                    "authorId": "1986353424",
                    "name": "David Gros"
                },
                {
                    "authorId": "1776596",
                    "name": "T. Gschwind"
                },
                {
                    "authorId": "2115731876",
                    "name": "James M. Johnson"
                },
                {
                    "authorId": "40643656",
                    "name": "Jim Laredo"
                },
                {
                    "authorId": "2158528950",
                    "name": "Christoph"
                },
                {
                    "authorId": "2158529361",
                    "name": "Miksovic"
                },
                {
                    "authorId": "1665404678",
                    "name": "Qian Pan"
                },
                {
                    "authorId": "2158530835",
                    "name": "Priyanshu Rai"
                },
                {
                    "authorId": "2158528595",
                    "name": "Ramkumar Ramalingam"
                },
                {
                    "authorId": "30176321",
                    "name": "P. Scotton"
                },
                {
                    "authorId": "2158528546",
                    "name": "Nagarjuna Surabathina"
                },
                {
                    "authorId": "2940762",
                    "name": "Kartik Talamadupula"
                }
            ]
        },
        {
            "paperId": "8c5dfc418b937ba78e481ca46a5f43ac61863059",
            "title": "Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in Dialog Systems",
            "abstract": "Dialog systems are often designed or trained to output human-like responses. However, some responses may be impossible for a machine to truthfully say (e.g. \u201cthat movie made me cry\u201d). Highly anthropomorphic responses might make users uncomfortable or implicitly deceive them into thinking they are interacting with a human. We collect human ratings on the feasibility of approximately 900 two-turn dialogs sampled from 9 diverse data sources. Ratings are for two hypothetical machine embodiments: a futuristic humanoid robot and a digital assistant. We find that for some data-sources commonly used to train dialog systems, 20-30% of utterances are not viewed as possible for a machine. Rating is marginally affected by machine embodiment. We explore qualitative and quantitative reasons for these ratings. Finally, we build classifiers and explore how modeling configuration might affect output permissibly, and discuss implications for building less falsely anthropomorphic dialog systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1986353424",
                    "name": "David Gros"
                },
                {
                    "authorId": "40058381",
                    "name": "Yu Li"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "e6f4006f8bc9867cbc33b0765e7831147ce5162e",
            "title": "Cross-Domain Detection of GPT-2-Generated Technical Text",
            "abstract": "Machine-generated text presents a potential threat not only to the public sphere, but also to the scientific enterprise, whereby genuine research is undermined by convincing, synthetic text. In this paper we examine the problem of detecting GPT-2-generated technical research text. We first consider the realistic scenario where the defender does not have full information about the adversary\u2019s text generation pipeline, but is able to label small amounts of in-domain genuine and synthetic text in order to adapt to the target distribution. Even in the extreme scenario of adapting a physics-domain detector to a biomedical detector, we find that only a few hundred labels are sufficient for good performance. Finally, we show that paragraph-level detectors can be used to detect the tampering of full-length documents under a variety of threat models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40636184",
                    "name": "Juan Diego Rodriguez"
                },
                {
                    "authorId": "2068574990",
                    "name": "Todd Hay"
                },
                {
                    "authorId": "1986353424",
                    "name": "David Gros"
                },
                {
                    "authorId": "38799346",
                    "name": "Zain Shamsi"
                },
                {
                    "authorId": "143889804",
                    "name": "R. Srinivasan"
                }
            ]
        },
        {
            "paperId": "4f9ced6a7a7b19e8607fc9028e8a8d55f89e9a52",
            "title": "The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity",
            "abstract": "Humans are increasingly interacting with machines through language, sometimes in contexts where the user may not know they are talking to a machine (like over the phone or a text chatbot). We aim to understand how system designers and researchers might allow their systems to confirm its non-human identity. We collect over 2,500 phrasings related to the intent of \u201cAre you a robot?\u201d. This is paired with over 2,500 adversarially selected utterances where only confirming the system is non-human would be insufficient or disfluent. We compare classifiers to recognize the intent and discuss the precision/recall and model complexity tradeoffs. Such classifiers could be integrated into dialog systems to avoid undesired deception. We then explore how both a generative research model (Blender) as well as two deployed systems (Amazon Alexa, Google Assistant) handle this intent, finding that systems often fail to confirm their non-human identity. Finally, we try to understand what a good response to the intent would be, and conduct a user study to compare the important aspects when responding to this intent.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1986353424",
                    "name": "David Gros"
                },
                {
                    "authorId": "40058381",
                    "name": "Yu Li"
                },
                {
                    "authorId": "1564034697",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "642e280df732665249315d6c144871f0e2ceeae6",
            "title": "NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands",
            "abstract": "The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural language processing to the command line. Participants were tasked with building models that can transform descriptions of command line tasks in English to their Bash syntax. This is a report on the competition with details of the task, metrics, data, attempted solutions, and lessons learned.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145625761",
                    "name": "Mayank Agarwal"
                },
                {
                    "authorId": "2500065",
                    "name": "Tathagata Chakraborti"
                },
                {
                    "authorId": "31669889",
                    "name": "Quchen Fu"
                },
                {
                    "authorId": "1986353424",
                    "name": "David Gros"
                },
                {
                    "authorId": "143724481",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "2051865997",
                    "name": "Jaron Maene"
                },
                {
                    "authorId": "2940762",
                    "name": "Kartik Talamadupula"
                },
                {
                    "authorId": "51900001",
                    "name": "Zhongwei Teng"
                },
                {
                    "authorId": "2111231491",
                    "name": "Jules White"
                }
            ]
        },
        {
            "paperId": "d944bf7942297f5670192c5cd33191c26a87973e",
            "title": "Code to Comment \u201cTranslation\u201d: Data, Metrics, Baselining & Evaluation",
            "abstract": "The relationship of comments to code, and in particular, the task of generating useful comments given the code, has long been of interest. The earliest approaches have been based on strong syntactic theories of comment-structures, and relied on textual templates. More recently, researchers have applied deep-learning methods to this task-specifically, trainable generative translation models which are known to work very well for Natural Language translation (e.g., from German to English). We carefully examine the underlying assumption here: that the task of generating comments sufficiently resembles the task of translating between natural languages, and so similar models and evaluation metrics could be used. We analyze several recent code-comment datasets for this task: CODENN, DEEPCOM, FUNCOM, and Docstring. We compare them with WMT19, a standard dataset frequently used to train state-of-the-art natural language translators. We found some interesting differences between the code-comment data and the WMT19 natural language data. Next, we describe and conduct some studies to calibrate BLEU (which is commonly used as a measure of comment quality). using \u201caffinity pairs\u201d of methods, from different projects, in the same project, in the same class, etc; Our study suggests that the current performance on some datasets might need to be improved substantially. We also argue that fairly naive information retrieval (IR) methods do well enough at this task to be considered a reasonable baseline. Finally, we make some suggestions on how our findings might be used in future research in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1986353424",
                    "name": "David Gros"
                },
                {
                    "authorId": "1419484735",
                    "name": "Hariharan Sezhiyan"
                },
                {
                    "authorId": "114875459",
                    "name": "Prem Devanbu"
                },
                {
                    "authorId": "1564034697",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "444dcd6b84b1ce1d4aa02aaab71812974735cabb",
            "title": "AInix: An open platform for natural language interfaces to shell commands",
            "abstract": "This report discusses initial work on the AInix Platform. This platform is designed to allow developers to add natural language interfaces to Unix-like shell commands. This can be used with the aish shell, which allows users to intermix natural language with shell commands. We create a high-level way of specifying semantic parsing grammars and collect a dataset of basic shell commands. We experiment with seq2seq models, abstract syntax networks (ASN), and embedded nearest neighbor-based models. We \ufb01nd highest accuracy is achieved with seq2seq models and ASN\u2019s. While not as accurate, we \ufb01nd that when embedders are pretrained on large-scale code-related text, nearest neighbor models can achieve decent performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1986353424",
                    "name": "David Gros"
                }
            ]
        }
    ]
}