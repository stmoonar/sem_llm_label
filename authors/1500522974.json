{
    "authorId": "1500522974",
    "papers": [
        {
            "paperId": "87bc8ab8756d0996e056f87ca0a3ef570599e4b5",
            "title": "Flatness Improves Backbone Generalisation in Few-shot Classification",
            "abstract": "Deployment of deep neural networks in real-world settings typically requires adaptation to new tasks with few examples. Few-shot classification (FSC) provides a solution to this problem by leveraging pre-trained backbones for fast adaptation to new classes. Surprisingly, most efforts have only focused on developing architectures for easing the adaptation to the target domain without considering the importance of backbone training for good generalisation. We show that flatness-aware backbone training with vanilla fine-tuning results in a simpler yet competitive baseline compared to the state-of-the-art. Our results indicate that for in- and cross-domain FSC, backbone training is crucial to achieving good generalisation across different adaptation methods. We advocate more care should be taken when training these models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "2249760043",
                    "name": "Martin Trapp"
                },
                {
                    "authorId": "2295989756",
                    "name": "Marcus Klasson"
                },
                {
                    "authorId": "2249761330",
                    "name": "Arno Solin"
                }
            ]
        },
        {
            "paperId": "33832998d3f9c60b74df998a7b9a29fba4aadc26",
            "title": "Improving Hyperparameter Learning under Approximate Inference in Gaussian Process Models",
            "abstract": "Approximate inference in Gaussian process (GP) models with non-conjugate likelihoods gets entangled with the learning of the model hyperparameters. We improve hyperparameter learning in GP models and focus on the interplay between variational inference (VI) and the learning target. While VI's lower bound to the marginal likelihood is a suitable objective for inferring the approximate posterior, we show that a direct approximation of the marginal likelihood as in Expectation Propagation (EP) is a better learning objective for hyperparameter optimization. We design a hybrid training procedure to bring the best of both worlds: it leverages conjugate-computation VI for inference and uses an EP-like marginal likelihood approximation for hyperparameter learning. We compare VI, EP, Laplace approximation, and our proposed training procedure and empirically demonstrate the effectiveness of our proposal across a wide range of data sets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "144104356",
                    "name": "S. T. John"
                },
                {
                    "authorId": "145410662",
                    "name": "A. Solin"
                }
            ]
        },
        {
            "paperId": "f7e78f1a1b436d28bf8761380b91bff7d2f83c4a",
            "title": "To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion",
            "abstract": "Embedding models have shown great power in knowledge graph completion (KGC) task. By learning structural constraints for each training triple, these methods implicitly memorize intrinsic relation rules to infer missing links. However, this paper points out that the multi-hop relation rules are hard to be reliably memorized due to the inherent deficiencies of such implicit memorization strategy, making embedding models underperform in predicting links between distant entity pairs. To alleviate this problem, we present Vertical Learning Paradigm (VLP), which extends embedding models by allowing to explicitly copy target information from related factual triples for more accurate prediction. Rather than solely relying on the implicit memory, VLP directly provides additional cues to improve the generalization ability of embedding models, especially making the distant link prediction significantly easier. Moreover, we also propose a novel relative distance based negative sampling technique (ReD) for more effective optimization. Experiments demonstrate the validity and generality of our proposals on two standard benchmarks. Our code is available at https://github.com/rui9812/VLP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "2144230222",
                    "name": "Xu Chen"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "48018967",
                    "name": "Jianan Zhao"
                },
                {
                    "authorId": "46394401",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2114924471",
                    "name": "Weihao Han"
                },
                {
                    "authorId": "2118180377",
                    "name": "Hao Sun"
                },
                {
                    "authorId": "2066621592",
                    "name": "Weiwei Deng"
                },
                {
                    "authorId": "2145908764",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                }
            ]
        },
        {
            "paperId": "63836e669416668744c3676a831060e8de3f58a1",
            "title": "HousE: Knowledge Graph Embedding with Householder Parameterization",
            "abstract": "The effectiveness of knowledge graph embedding (KGE) largely depends on the ability to model intrinsic relation patterns and mapping properties. However, existing approaches can only capture some of them with insufficient modeling capacity. In this work, we propose a more powerful KGE framework named HousE, which involves a novel parameterization based on two kinds of Householder transformations: (1) Householder rotations to achieve superior capacity of modeling relation patterns; (2) Householder projections to handle sophisticated relation mapping properties. Theoretically, HousE is capable of modeling crucial relation patterns and mapping properties simultaneously. Besides, HousE is a generalization of existing rotation-based models while extending the rotations to high-dimensional spaces. Empirically, HousE achieves new state-of-the-art performance on five benchmark datasets. Our code is available at https://github.com/anrep/HousE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "48018967",
                    "name": "Jianan Zhao"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2064929062",
                    "name": "Di He"
                },
                {
                    "authorId": "2108941389",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "2108104523",
                    "name": "Yuming Liu"
                },
                {
                    "authorId": "2118180377",
                    "name": "Hao Sun"
                },
                {
                    "authorId": "3210262",
                    "name": "Senzhang Wang"
                },
                {
                    "authorId": "2066621592",
                    "name": "Weiwei Deng"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "2145908764",
                    "name": "Qi Zhang"
                }
            ]
        },
        {
            "paperId": "8bb37e8ae7dd6fa8cab2407f63a61f697152717f",
            "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
            "abstract": "This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and GNNs together. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows training the two modules separately while simultaneously allowing the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48018967",
                    "name": "Jianan Zhao"
                },
                {
                    "authorId": "145252498",
                    "name": "Meng Qu"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2152208710",
                    "name": "Hao Yan"
                },
                {
                    "authorId": "1409707585",
                    "name": "Qian Liu"
                },
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "152226504",
                    "name": "Jian Tang"
                }
            ]
        },
        {
            "paperId": "c794a63d1adf52653c0ef91d7b41f41ff2aae38d",
            "title": "Test-Time Training for Graph Neural Networks",
            "abstract": "Graph Neural Networks (GNNs) have made tremendous progress in the graph classification task. However, a performance gap between the training set and the test set has often been noticed. To bridge such gap, in this work we introduce the first test-time training framework for GNNs to enhance the model generalization capacity for the graph classification task. In particular, we design a novel test-time training strategy with self-supervised learning to adjust the GNN model for each test graph sample. Experiments on the benchmark datasets have demonstrated the effectiveness of the proposed framework, especially when there are distribution shifts between training set and test set. We have also conducted exploratory studies and theoretical analysis to gain deeper understandings on the rationality of the design of the proposed graph test time training framework (GT3).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108941389",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "48018967",
                    "name": "Jianan Zhao"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                }
            ]
        },
        {
            "paperId": "28da08d10d371914bf79aea0c846110de2294629",
            "title": "A New Perspective on the Effects of Spectrum in Graph Neural Networks",
            "abstract": "Many improvements on GNNs can be deemed as operations on the spectrum of the underlying graph matrix, which motivates us to directly study the characteristics of the spectrum and their effects on GNN performance. By generalizing most existing GNN architectures, we show that the correlation issue caused by the $unsmooth$ spectrum becomes the obstacle to leveraging more powerful graph filters as well as developing deep architectures, which therefore restricts GNNs' performance. Inspired by this, we propose the correlation-free architecture which naturally removes the correlation issue among different channels, making it possible to utilize more sophisticated filters within each channel. The final correlation-free architecture with more powerful filters consistently boosts the performance of learning graph representations. Code is available at https://github.com/qslim/gnn-spectrum.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150426854",
                    "name": "Mingqi Yang"
                },
                {
                    "authorId": "2115437382",
                    "name": "Yanming Shen"
                },
                {
                    "authorId": "1500522974",
                    "name": "Rui Li"
                },
                {
                    "authorId": "2072590343",
                    "name": "Heng Qi"
                },
                {
                    "authorId": "1737486",
                    "name": "Qian Zhang"
                },
                {
                    "authorId": "1714354",
                    "name": "Baocai Yin"
                }
            ]
        }
    ]
}