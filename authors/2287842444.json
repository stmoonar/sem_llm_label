{
    "authorId": "2287842444",
    "papers": [
        {
            "paperId": "240ac1cc47a00d2c4fe62e228c1fb0dfe85b0c39",
            "title": "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation",
            "abstract": "We study the code generation behavior of instruction-tuned models built on top of code pre-trained language models when they could access an auxiliary function to implement a function. We design several ways to provide auxiliary functions to the models by adding them to the query or providing a response prefix to incorporate the ability to utilize auxiliary functions with the instruction-following capability. Our experimental results show the effectiveness of combining the base models' auxiliary function utilization ability with the instruction following ability. In particular, the performance of adopting our approaches with the open-sourced language models surpasses that of the recent powerful proprietary language models, i.e., gpt-4o.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287842444",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "2303885004",
                    "name": "Suyeon Kim"
                },
                {
                    "authorId": "2291705807",
                    "name": "Joonwon Jang"
                },
                {
                    "authorId": "2317010068",
                    "name": "Heejae Chon"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "2286165699",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "5db9709ceb0a81b7dfc741e9fe309f32a53d6d8c",
            "title": "Is Functional Correctness Enough to Evaluate Code Language Models? Exploring Diversity of Generated Codes",
            "abstract": "Language models (LMs) have exhibited impressive abilities in generating codes from natural language requirements. In this work, we highlight the diversity of code generated by LMs as a critical criterion for evaluating their code generation capabilities, in addition to functional correctness. Despite its practical implications, there is a lack of studies focused on assessing the diversity of generated code, which overlooks its importance in the development of code LMs. We propose a systematic approach to evaluate the diversity of generated code, utilizing various metrics for inter-code similarity as well as functional correctness. Specifically, we introduce a pairwise code similarity measure that leverages large LMs' capabilities in code understanding and reasoning, demonstrating the highest correlation with human judgment. We extensively investigate the impact of various factors on the quality of generated code, including model sizes, temperatures, training approaches, prompting strategies, and the difficulty of input problems. Our consistent observation of a positive correlation between the test pass score and the inter-code similarity score indicates that current LMs tend to produce functionally correct code with limited diversity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2317010068",
                    "name": "Heejae Chon"
                },
                {
                    "authorId": "2287842444",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "2258712913",
                    "name": "Jinyoung Yeo"
                },
                {
                    "authorId": "2258907627",
                    "name": "Dongha Lee"
                }
            ]
        },
        {
            "paperId": "87ec5cd167e972b3352d06eada73ef2acb1b7e74",
            "title": "Exploring Language Model's Code Generation Ability with Auxiliary Functions",
            "abstract": "Auxiliary function is a helpful component to improve language model's code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other. With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models' various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models' promising ability to utilize auxiliary functions including their self-improving behavior by implementing the two functions step-by-step. However, our analysis also reveals the model's underutilized behavior to call the auxiliary function, suggesting the future direction to enhance their implementation by eliciting the auxiliary function call ability encoded in the models. We release our code and dataset to facilitate this research direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287842444",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "2287988207",
                    "name": "Sanghwan Jang"
                },
                {
                    "authorId": "1523619467",
                    "name": "Seongbo Jang"
                },
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "2286165699",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "a23599aef366be9492e924a2861dc381fed048af",
            "title": "KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark",
            "abstract": "As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user\u2019s first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a benchmark designed to assess language models\u2019 conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improvement in models\u2019 conversation skills. Furthermore, our in-depth comparisons across different language models highlight the effectiveness of recent training techniques in enhancing conversational proficiency. We anticipate that KoDialogBench will promote the progress towards conversation-aware Korean language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1523619467",
                    "name": "Seongbo Jang"
                },
                {
                    "authorId": "2287842444",
                    "name": "Seonghyeon Lee"
                },
                {
                    "authorId": "2286165699",
                    "name": "Hwanjo Yu"
                }
            ]
        }
    ]
}