{
    "authorId": "25919018",
    "papers": [
        {
            "paperId": "1a17c004b168f8bfd385f52da629937509bc4fda",
            "title": "Loss Compensation in Multi-Session Recommendation Under Limited Availability",
            "abstract": "In many recommendation applications, items may have limited availability thereby causing conflict among users interested in the same items. Over time, this results in unequal user treat-ment: few users are recommended the limited items and receive preferential treatment, while the rest is left with sub-optimal recommendations, ultimately leading them to leave. In this paper, we formalize the novel problem of compensating users in multi-session recommendations under limited item availability. Our aim is to generate recommendations that not only optimize accuracy, but also compensate users over time for the loss of accuracy incurred in previous iterations. We design compensation strategies that serve users and items in different orders and accommodate various recommendation adoption models. Our algorithms are integrated into SoCRATe (System for Compensating Recommendations with Availability and Time), a framework that enables us to study loss compensation over time. Our experiments on real data demonstrate that to best compensate users for the incurred loss, traditional recommenders need to be revisited to account for item availability. Our experiments on synthetic data explore different parameters of our solution and show that it is much faster than an optimal (brute-force) compensation strategy, while achieving comparable results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2239390808",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "2292416780",
                    "name": "Davide Martinenghi"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "5881c7d349c303a1d0feafdcccfcaffbfeace1ea",
            "title": "HEALER: A Data Lake Architecture for Healthcare",
            "abstract": "With the growth of the Internet of Things and the rapid progress of social networks, everything appears to generate data. The ever-increasingnumberofconnecteddevicesisaccompaniedbyagrowthofthevolumeofdata, producedatanever-increasing rate, and this massive flow includes data types that are difficult to process using standard database techniques. One of the most critical scenarios is healthcare, whose activities need to store and manage a variety of data types \u2013 reports written in natural language, medical images, genomic data and waveforms of vital signs \u2013 which do not have a well-defined structure. In order to benefit from this large amount of complex data, Data Lakes have recently emerged as a solution to grant central storage and flexible analysis for all types of data. However, there is no Data Lake architecture that fits all the possible scenarios, since the architecture depends heavily on the application domain and, so far, there are no Data Lake architectures that support the specific needs of the healthcare domain. This work proposes HEALER: a Data Lake architecture that effectively performs data ingestion, data storage, and data access with the aim of providing a single central repository for efficient storage of different types of healthcare data. The architecture also enables the analysis and querying of the data, which can be loaded into the Data Lake regardless of their format and type. To verify the effectiveness of the architecture, a proof-of-concept of HEALER has been developed, that allows ingestion of various data, performs waveforms processing to make them more interpretable to researchers and analysts, grants access to the saved data and allows the analysis of natural language reports. Finally we studied the performance of the system in each of its main phases: ingestion, processing, data access and analysis. The results lead us to some important considerations to be taken into account when using and configuring the system components",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185847395",
                    "name": "Carlo Manco"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2945086",
                    "name": "Enrico Barbierato"
                },
                {
                    "authorId": "1702227",
                    "name": "M. Gribaudo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "96ff519428c517942072f07b6fb45e7d918b486d",
            "title": "Data Quality and Fairness: Rivals or Friends?",
            "abstract": "In the last decade, data-driven decision-making is considered one of the main drivers for organizational success. Within this approach, decisions are based on insights and patterns identified through data analysis. In this scenario, input data must be reliable to guarantee the accuracy of the results: they should be correct and complete but also unbiased, i.e., both Data Quality (DQ) and Fairness should be guaranteed. However, maximizing DQ and Fairness simultaneously is not trivial, since data quality improvement techniques can negatively affect Fairness and vice versa. Understanding and thoroughly analyzing this relationship between DQ and Fairness is therefore paramount, and is this paper\u2019s goal. The results of our experiments, based on a well-known biased dataset (the Adult Census Income) provided details about this trade-off and allowed us to draw some guidelines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "1723724",
                    "name": "C. Cappiello"
                },
                {
                    "authorId": "2239390808",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2239381912",
                    "name": "Sergio Cuzzucoli"
                },
                {
                    "authorId": "2239390968",
                    "name": "Alessandro Dangelo"
                },
                {
                    "authorId": "2182555347",
                    "name": "Camilla Sancricca"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "a83378bd8e1482f484a4702f4239bf0f98d8407c",
            "title": "An Interpretable Deep-Learning Framework for Predicting Hospital Readmissions From Electronic Health Records",
            "abstract": "With the increasing availability of patients' data, modern medicine is shifting towards prospective healthcare. Electronic health records contain a variety of information useful for clinical patient description and can be exploited for the construction of predictive models, given that similar medical histories will likely lead to similar progressions. One example is unplanned hospital readmission prediction, an essential task for reducing hospital costs and improving patient health. Despite predictive models showing very good performances especially with deep-learning models, they are often criticized for the poor interpretability of their results, a fundamental characteristic in the medical field, where incorrect predictions might have serious consequences for the patient health. In this paper we propose a novel, interpretable deep-learning framework for predicting unplanned hospital readmissions, supported by NLP findings on word embeddings and by neural-network models (ConvLSTM) for better handling temporal data. We validate our system on the two predictive tasks of hospital readmission within 30 and 180 days, using real-world data. In addition, we introduce and test a model-dependent technique to make the representation of results easily interpretable by the medical staff. Our solution achieves better performances compared to traditional models based on machine learning, while providing at the same time more interpretable results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "2258717610",
                    "name": "Marco Vagaggini"
                }
            ]
        },
        {
            "paperId": "a8caf23b86b050ad217a05db6aac94396e73d37a",
            "title": "Bias Score: Estimating Gender Bias in Sentence Representations",
            "abstract": "The ever-increasing number of applications based on semantic text analysis is making natural language understanding a fundamental task. Language models are used for a variety of tasks, such as parsing CVs or improving web search results. At the same time, concern is growing around embedding-based language models, which often exhibit social bias and lack of transparency, despite their popularity and widespread use. Word embeddings in particular exhibit a large amount of gender bias, and they have been shown to reflect social stereotypes. Recently, sentence embeddings have been introduced as a novel and powerful technique to represent entire sentences as vectors. However, traditional methods for estimating gender bias cannot be applied to sentence representations, because gender-neutral entities cannot be easily identified and listed. We propose a new metric to estimate gender bias in sentence embeddings, named bias score. Our solution, leveraging the semantic importance of individual words and previous research on gender bias in word embeddings, is able to discern between correct and biased gender information at sentence level. Experiments on a real-world dataset demonstrates that our novel metric identifies gender stereotyped sentences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "1709652",
                    "name": "M. Tanelli"
                }
            ]
        },
        {
            "paperId": "b04aae9a1dac75f7e83f717d41bd5116e2c537ca",
            "title": "Functional Dependencies to Mitigate Data Bias",
            "abstract": "Technologies based on data are frequently adopted in many sensitive environments to build models that support important and life-changing decisions. As a result, for an application to be ethically reliable, it should be associated with tools to discover and mitigate bias in data, in order to avoid (possibly unintentional) unethical behaviors and the associated consequences. In this paper we propose a novel solution that, exploiting the notion of Functional Dependency and its variants - well-known data constraints - aims at enforcing fairness by discovering and solving discrimination in datasets. Our system first identifies the attributes of a dataset that encompass discrimination (e.g. gender, ethnicity or religion), generating a list of dependencies, then, based on this information, determines the smallest set of tuples that must be added or removed to mitigate such bias in the dataset. Experimental results on two real-world datasets demonstrated that our approach can greatly improve the ethical quality of data sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "b1119c6a7775c9c65bec45791e2668592039c565",
            "title": "SoCRATe: a Framework for Compensating Users Over Time with Limited Availability Recommendations",
            "abstract": "We present our preliminary ideas for developing SoCRATe, a framework and an online system dedicated to providing recommendations to users when items\u2019 availability is limited. SoCRATe is relevant to several real-world applications, among which movie and task recommendations. SoCRATe has several appealing features: it watches users as they consume recommendations and accounts for user feedback in refining recommendations in the next round, it implements loss compensation strategies to make up for sub-optimal recommendations, in terms of accuracy, when items have limited availability, and it decides when to re-generate recommendations on a need-based fashion. SoCRATe accommodates real users as well as simulated users to enable testing multiple recommendation choice models. To frame evaluation, SoCRATe introduces a new set of measures that capture recommendation accuracy over time as well as throughput and user satisfaction. All these features make SoCRATe unique and able to adapt recommendations to user preferences in a resource-limited setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "c0922becac3bfe298ff632f1999bf060775bd9ea",
            "title": "FAIR-DB: A system to discover unfairness in datasets",
            "abstract": "In our everyday lives, technologies based on data play an increasingly important role. With the widespread adoption of decision making systems also in very sensitive environments, fairness has become a very important topic of discussion within the data science community. In this context, it is crucial to ensure that the data on which we base these decisions, are fair, and do not reflect historical biases. In this demo, we propose FAIR-DB (FunctionAl dependencIes to discoveR Data Bias), a system that exploiting the notion of Functional Dependency, a particular type of constraint on the data, can discover unethical behaviours in a dataset. The proposed solution is implemented as a web-based application, that, given an input dataset, generates such dependencies, walks the user trough their analysis, and finally provides many insights about bias present in the data. Our tool uses a novel metric to evaluate the unfairness present in datasets, identifies the attributes that encompass discrimination (e.g. ethnicity, sex or religion), and provides very precise information about the groups treated unequally. We also provide a detailed description of the system architecture and present a demonstration scenario, based on a real-world dataset frequently used in the field of computer ethics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "c9c84ab6b1bb88cd3368092ea0feadf263c47d8a",
            "title": "E-FAIR-DB: Functional Dependencies to Discover Data Bias and Enhance Data Equity",
            "abstract": "Decisions based on algorithms and systems generated from data have become essential tools that pervade all aspects of our daily lives; for these advances to be reliable, the results should be accurate but should also respect all the facets of data equity [11]. In this context, the concepts of Fairness and Diversity have become relevant topics of discussion within the field of Data Science Ethics and, in general, in Data Science. Although data equity is desirable, reconciling this property with accurate decision-making is a critical tradeoff, because applying a repair procedure to restore equity might modify the original data in such a way that the final decision is inaccurate w.r.t. the ultimate objective of the analysis. In this work, we propose E-FAIR-DB, a novel solution that, exploiting the notion of Functional Dependency\u2014a type of data constraint\u2014aims at restoring data equity by discovering and solving discrimination in datasets. The proposed solution is implemented as a pipeline that, first, mines functional dependencies to detect and evaluate fairness and diversity in the input dataset, and then, based on these understandings and on the objective of the data analysis, mitigates data bias, minimizing the number of modifications. Our tool can identify, through the mined dependencies, the attributes of the database that encompass discrimination (e.g., gender, ethnicity, or religion); then, based on these dependencies, it determines the smallest amount of data that must be added and/or removed to mitigate such bias. We evaluate our proposal both through theoretical considerations and experiments on two real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "daff53367af4565f5c32e308155df57601f6eb26",
            "title": "SoCRATe: A Recommendation System with Limited-Availability Items",
            "abstract": "We demonstrate SoCRATe, an online system dedicated to providing adaptive recommendations to users when items have limited availability. SoCRATe is relevant to several real-world applications, among which movie and task recommendations. SoCRATe has several appealing features: (i) watching users as they consume recommendations and accounting for user feedback in refining recommendations in the next round; (ii) implementing loss compensation strategies to make up for sub-optimal recommendations, in terms of accuracy, when items have limited availability; (iii) deciding when to re-generate recommendations on a need-based fashion. SoCRATe accommodates real users as well as simulated users to enable testing multiple recommendation choice models. To frame evaluation, SoCRATe introduces a new set of measures that capture recommendation accuracy, user satisfaction and item consumption over time. All these features make SoCRATe unique and able to adapt recommendations to user preferences in a resource-limited setting. A video of SoCRATe is available at https://youtu.be/4wlaScc_rUo.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        }
    ]
}