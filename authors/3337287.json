{
    "authorId": "3337287",
    "papers": [
        {
            "paperId": "cff068d3af9c54d59c137a8c9fa43cfdf8f5b11e",
            "title": "Applying the FAIR Principles to Computational Workflows",
            "abstract": "Recent trends within computational and data sciences show an increasing recognition and adoption of computational workflows as tools for productivity, reproducibility, and democratized access to platforms and processing know-how. As digital objects to be shared, discovered, and reused, computational workflows benefit from the FAIR principles, which stand for Findable, Accessible, Interoperable, and Reusable. The Workflows Community Initiative's FAIR Workflows Working Group (WCI-FW), a global and open community of researchers and developers working with computational workflows across disciplines and domains, has systematically addressed the application of both FAIR data and software principles to computational workflows. We present our recommendations with commentary that reflects our discussions and justifies our choices and adaptations. Like the software and data principles on which they are based, these are offered to workflow users and authors, workflow management system developers, and providers of workflow services as guide rails for adoption and fodder for discussion. Workflows are becoming more prevalent as documented, automated instruments for data analysis, data collection, AI-based predictions, and simulations. The FAIR recommendations for workflows that we propose in this paper will maximize their value as research assets and facilitate their adoption by the wider community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293849917",
                    "name": "Sean R. Wilkinson"
                },
                {
                    "authorId": "2324567907",
                    "name": "Meznah Aloqalaa"
                },
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                },
                {
                    "authorId": "144361904",
                    "name": "M. Crusoe"
                },
                {
                    "authorId": "3362335",
                    "name": "B. Kinoshita"
                },
                {
                    "authorId": "2324574684",
                    "name": "Luiz Gadelha"
                },
                {
                    "authorId": "1398926410",
                    "name": "D. Garijo"
                },
                {
                    "authorId": "2253446617",
                    "name": "Ove Johan Ragnar Gustafsson"
                },
                {
                    "authorId": "50790096",
                    "name": "N. Juty"
                },
                {
                    "authorId": "2319201952",
                    "name": "Sehrish Kanwal"
                },
                {
                    "authorId": null,
                    "name": "Farah Zaib Khan"
                },
                {
                    "authorId": "2324581406",
                    "name": "Johannes Koster"
                },
                {
                    "authorId": "2121923674",
                    "name": "Karsten Peters-von Gehlen"
                },
                {
                    "authorId": "2277749459",
                    "name": "Line Pouchard"
                },
                {
                    "authorId": "102419669",
                    "name": "Randy K. Rannow"
                },
                {
                    "authorId": "1399487720",
                    "name": "S. Soiland-Reyes"
                },
                {
                    "authorId": "2250071927",
                    "name": "N. Soranzo"
                },
                {
                    "authorId": "2626019",
                    "name": "Shoaib Sufi"
                },
                {
                    "authorId": "2324549568",
                    "name": "Ziheng Sun"
                },
                {
                    "authorId": "2047555",
                    "name": "B. Vilne"
                },
                {
                    "authorId": "31694398",
                    "name": "M. A. Wouters"
                },
                {
                    "authorId": "2324575348",
                    "name": "Denis Yuen"
                },
                {
                    "authorId": "2285245801",
                    "name": "Carole A. Goble"
                }
            ]
        },
        {
            "paperId": "9351461b20cde5662bf233bd02d87b9ebc43abd4",
            "title": "Efficient Maintenance of Agree-Sets Against Dynamic Datasets",
            "abstract": "Constraint discovery is a fundamental task in data profiling, which involves identifying the dependencies that are satisfied by a dataset. As published datasets are increasingly dynamic, a number of researchers have begun to investigate the problem of dependencies\u2019 discovery in dynamic datasets. Proposals this far in this area can be viewed as schema-based in the sense that they model and explore the solution space using a lattice built on the basis of the attributes (columns) of the dataset. It is recognized that proposals that belong to this class, like their static counterpart, tend to perform well for datasets with a large number of tuples but a small number of attributes. The second class of proposals that have been examined for static datasets (but not in dynamic settings) is data-driven and is known to perform well for datasets with a large number of attributes and a small number of tuples. The main bottleneck of this class of solutions is the generation of agree-sets, which involves pairwise comparison of the tuples in the dataset. We present in this paper DynASt , a system for the efficient maintenance of agree-sets in dynamic datasets. We investigate the performance of DynASt and its scalability in terms of the number of tuples and the number of attributes of the target dataset. We also show that it outperforms existing (static and dynamic) state-of-the-art solutions for datasets with a large number of attributes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                }
            ]
        },
        {
            "paperId": "822faca9ed7535fa27bcd2736d26264162b180a8",
            "title": "DS4ALL: All you need for democratizing data exploration and analysis",
            "abstract": "Today, large amounts of data are collected in various domains, presenting unprecedented economic and societal opportunities. Yet, at present, the exploitation of these data sets through data science methods is primarily dominated by AI-savvy users. From an inclusive perspective, there is a need for solutions that can democratise data science that can guide non-specialists intuitively to explore data collections and extract knowledge out of them. This paper introduces the vision of a new data science engine, called DS4ALL (Data Science for ALL), that empowers users who are neither computer nor AI experts to perform sophisticated data exploration and analysis tasks. Therefore, DS4ALL is based on a conversational and intuitive approach that insulates users from the complexity of AI algorithms. DS4ALL allows a dialogue-based approach that gives the user greater freedom of expression. It will enable them to communicate using natural language without requiring a high level of expertise on data-driven algorithms. User requests are interpreted and handled internally by the system in an automated manner, providing the user with the required output by masking the complexity of the data science workflow. The system can also collect feedback on the displayed results, leveraging these comments to address personalized data analysis sessions. The benefits of the envisioned system are discussed, and a use case is also presented to describe the innovative aspects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1703865721",
                    "name": "P. Bethaz"
                },
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                },
                {
                    "authorId": "1393643717",
                    "name": "Genoveva Vargas-Solar"
                },
                {
                    "authorId": "1684646",
                    "name": "T. Cerquitelli"
                }
            ]
        },
        {
            "paperId": "86fabf06f7026469397c07d169ae118cd99fc188",
            "title": "On the Anonymization of Workflow Provenance without Compromising the Transparency of Lineage",
            "abstract": "Workflows have been adopted in several scientific fields as a tool for the specification and execution of scientific experiments. In addition to automating the execution of experiments, workflow systems often include capabilities to record provenance information, which contains, among other things, data records used and generated by the workflow as a whole but also by its component modules. It is widely recognized that provenance information can be useful for the interpretation, verification, and re-use of workflow results, justifying its sharing and publication among scientists. However, workflow execution in some branches of science can manipulate sensitive datasets that contain information about individuals. To address this problem, we investigate, in this article, the problem of anonymizing the provenance of workflows. In doing so, we consider a popular class of workflows in which component modules use and generate collections of data records as a result of their invocation, as opposed to a single data record. The solution we propose offers guarantees of confidentiality without compromising lineage information, which provides transparency as to the relationships between the data records used and generated by the workflow modules. We provide algorithmic solutions that show how the provenance of a single module and an entire workflow can be anonymized and present the results of experiments that we conducted for their evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                }
            ]
        },
        {
            "paperId": "f0831d8d9c2d2f62306ce0bd1cb22625425db848",
            "title": "Lineage-Preserving Anonymization of the Provenance of Collection-Based Workflows",
            "abstract": "We examine in this paper the problem of anonymizing the provenance of collection-oriented workflows, in which the constituent modules use and generate sets of data records. Despite their popularity, this kind of workflows has been overlooked in the literature w.r.t privacy. We, therefore, set out in this paper to examine the following questions: How the provenance of a collection-based module can be anonymized? Can lineage information be preserved? Beyond a single module, how can the provenance of a whole work-flow be anonymized? As well as addressing the above questions, we report on evaluation exercises that assess the effectiveness and efficiency of our solution. In particular, we tease apart the parameters that impact the quality of the obtained anonymized provenance information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                }
            ]
        },
        {
            "paperId": "fb5d4b0d3f2f8d4e9dd1efa73e22fc0d7a72aeb0",
            "title": "Findable and reusable workflow data products: A genomic workflow case study",
            "abstract": "While workflow systems have improved the repeatability of scientific experiments, the value of the processed (intermediate) data have been overlooked so far. In this paper, we argue that the intermediate data products of workflow executions should be seen as first-class objects that need to be curated and published. Not only will this be exploited to save time and resources needed when re-executing workflows, but more importantly, it will improve the reuse of data products by the same or peer scientists in the context of new hypotheses and experiments. To assist curator in annotating (intermediate) workflow data, we exploit in this work multiple sources of information, namely: (i) the provenance information captured by the workflow system, and (ii) domain annotations that are provided by tools registries, such as Bio.Tools. Furthermore, we show, on a concrete bioinformatics scenario, how summarising techniques can be used to reduce the machine-generated provenance information of such data products into concise human- and machine-readable annotations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1753138",
                    "name": "A. Gaignard"
                },
                {
                    "authorId": "1398384830",
                    "name": "H. Skaf-Molli"
                },
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                }
            ]
        },
        {
            "paperId": "0cb098c6c62c6d936a7f8926535d0b3d531e20e9",
            "title": "On the elicitation and annotation of business activities based on emails",
            "abstract": "We describe an approach that is able to discover business process activities from emails. In addition, for each activity type, we extract metadata such as the roles of the people exchanging the email, type of the attached documents, or the domains of the mentioned links.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3468981",
                    "name": "Diana Jlailaty"
                },
                {
                    "authorId": "1800804",
                    "name": "Daniela Grigori"
                },
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                }
            ]
        },
        {
            "paperId": "15ef439d312558944c0b955c412688dec4fd2c8c",
            "title": "Streaming saturation for large RDF graphs with dynamic schema information",
            "abstract": "In the Big Data era, RDF data are produced in high volumes. While there exist proposals for reasoning over large RDF graphs using big data platforms, there is a dearth of solutions that do so in environments where RDF data are dynamic, and where new instance and schema triples can arrive at any time. In this work, we present the first solution for reasoning over large streams of RDF data using big data platforms. In doing so, we focus on the saturation operation, which seeks to infer implicit RDF triples given RDF schema constraints. Indeed, unlike existing solutions which saturate RDF data in bulk, our solution carefully identifies the fragment of the existing (and already saturated) RDF dataset that needs to be considered given the fresh RDF statements delivered by the stream. Thereby, it performs the saturation in an incremental manner. Experimental analysis shows that our solution outperforms existing bulk-based saturation solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2455420",
                    "name": "Mohammad Amin Farvardin"
                },
                {
                    "authorId": "1691789",
                    "name": "Dario Colazzo"
                },
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                },
                {
                    "authorId": "1731035",
                    "name": "C. Sartiani"
                }
            ]
        },
        {
            "paperId": "2a1f9eccd66b7498e357689d28cc40451122958d",
            "title": "Privacy-Preserving Data Analysis Workflows for eScience",
            "abstract": "Computing-intensive experiences in modern sciences have become increasingly data-driven illustrating perfectly the Big-Data era\u2019s challenges. These experiences are usually specified and enacted in the form of workflows that would need to manage (i.e., read, write, store, and retrieve) sensitive data like persons\u2019 past diseases and treatments. While there is an active research body on how to protect sensitive data by, for instance, anonymizing datasets, there is a limited number of approaches that would assist scientists identifying the datasets, generated by the work-flows,thatneed tobeanonymizedalongwithsettingthe anonymization degree that must be met. We present in this paper a pre-liminary for setting and inferring anonymization requirements of datasets used and generated by a workflow execution. The approach was implemented and showcased using a concrete example, and its efficiency assessed through validation exercises.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                },
                {
                    "authorId": "2799488",
                    "name": "Noura Faci"
                },
                {
                    "authorId": "1724195",
                    "name": "Z. Maamar"
                },
                {
                    "authorId": "2570740",
                    "name": "V. Bur\u00e9gio"
                },
                {
                    "authorId": "20666019",
                    "name": "Edvan Soares"
                },
                {
                    "authorId": "3293164",
                    "name": "M. Barhamgi"
                }
            ]
        },
        {
            "paperId": "87a6f96a0fbe9a5a631d3d94fd7b874b90910acb",
            "title": "Mining Business Process Information from Email Logs for Business Process Models Discovery",
            "abstract": "\u2014Exchanged information in emails\u2019 texts is usually concerned by complex events or business processes in which the entities exchanging emails are collaborating to achieve the processes\u2019 \ufb01nal goals. Thus, the \ufb02ow of information in the sent and received emails constitutes an essential part of such processes i.e. the tasks or the business activities. An email can be harvested for understanding the undocumented business process information it contains. Our goal in this work is to recast emails into a resource of business-oriented information. We describe a framework that is constituted of several analytical approaches able to extract such kind of information from email logs i.e. transforming an email log into an event log. The ef\ufb01ciency of all approaches is studied by applying different experiments on the open Enron email dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3468981",
                    "name": "Diana Jlailaty"
                },
                {
                    "authorId": "1800804",
                    "name": "Daniela Grigori"
                },
                {
                    "authorId": "3337287",
                    "name": "Khalid Belhajjame"
                }
            ]
        }
    ]
}