{
    "authorId": "1738685382",
    "papers": [
        {
            "paperId": "6778fbc38179f3f79afd36f7ac091bcb3ad111f6",
            "title": "Evaluating the Knowledge Base Completion Potential of GPT",
            "abstract": "Structured knowledge bases (KBs) are an asset for search engines and other applications, but are inevitably incomplete. Language models (LMs) have been proposed for unsupervised knowledge base completion (KBC), yet, their ability to do this at scale and with high accuracy remains an open question. Prior experimental studies mostly fall short because they only evaluate on popular subjects, or sample already existing facts from KBs. In this work, we perform a careful evaluation of GPT's potential to complete the largest public KB: Wikidata. We find that, despite their size and capabilities, models like GPT-3, ChatGPT and GPT-4 do not achieve fully convincing results on this task. Nonetheless, they provide solid improvements over earlier approaches with smaller LMs. In particular, we show that, with proper thresholding, GPT-3 enables to extend Wikidata by 27M facts at 90% precision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738685382",
                    "name": "Blerta Veseli"
                },
                {
                    "authorId": "2066327465",
                    "name": "S. Razniewski"
                },
                {
                    "authorId": "3245041",
                    "name": "Jan-Christoph Kalo"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                }
            ]
        },
        {
            "paperId": "f823ba85e3a7502b1afd618c6d8dbe436658befb",
            "title": "Evaluating Language Models for Knowledge Base Completion",
            "abstract": "Structured knowledge bases (KBs) are a foundation of many intelligent applications, yet are notoriously incomplete. Language models (LMs) have recently been proposed for unsupervised knowledge base completion (KBC), yet, despite encouraging initial results, questions regarding their suitability remain open. Existing evaluations often fall short because they only evaluate on popular subjects, or sample already existing facts from KBs. In this work, we introduce a novel, more challenging benchmark dataset, and a methodology tailored for a realistic assessment of the KBC potential of LMs. For automated assessment, we curate a dataset called WD-KNOWN, which provides an unbiased random sample of Wikidata, containing over 3.9 million facts. In a second step, we perform a human evaluation on predictions that are not yet in the KB, as only this provides real insights into the added value over existing KBs. Our key finding is that biases in dataset conception of previous benchmarks lead to a systematic overestimate of LM performance for KBC. However, our results also reveal strong areas of LMs. We could, for example, perform a significant completion of Wikidata on the relations nativeLanguage, by a factor of ~21 (from 260k to 5.8M) at 82% precision, usedLanguage, by a factor of ~2.1 (from 2.1M to 6.6M) at 82% precision, and citizenOf by a factor of ~0.3 (from 4.2M to 5.3M) at 90% precision. Moreover, we find that LMs possess surprisingly strong generalization capabilities: even on relations where most facts were not directly observed in LM training, prediction quality can be high.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738685382",
                    "name": "Blerta Veseli"
                },
                {
                    "authorId": "29001552",
                    "name": "Sneha Singhania"
                },
                {
                    "authorId": "2066327465",
                    "name": "S. Razniewski"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                }
            ]
        },
        {
            "paperId": "8a48b907c489bc78e4b67d56cdc11f3449d6aa4c",
            "title": "Improving Sentiment Analysis with Biofeedback Data",
            "abstract": "Humans frequently are able to read and interpret emotions of others by directly taking verbal and non-verbal signals in human-to-human communication into account or to infer or even experience emotions from mediated stories. For computers, however, emotion recognition is a complex problem: Thoughts and feelings are the roots of many behavioural responses and they are deeply entangled with neurophysiological changes within humans. As such, emotions are very subjective, often are expressed in a subtle manner, and are highly depending on context. For example, machine learning approaches for text-based sentiment analysis often rely on incorporating sentiment lexicons or language models to capture the contextual meaning. This paper explores if and how we further can enhance sentiment analysis using biofeedback of humans which are experiencing emotions while reading texts. Specifically, we record the heart rate and brain waves of readers that are presented with short texts which have been annotated with the emotions they induce. We use these physiological signals to improve the performance of a lexicon-based sentiment classifier. We find that the combination of several biosignals can improve the ability of a text-based classifier to detect the presence of a sentiment in a text on a per-sentence level.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3404943",
                    "name": "Daniel Schl\u00f6r"
                },
                {
                    "authorId": "33463364",
                    "name": "Albin Zehe"
                },
                {
                    "authorId": "1493276636",
                    "name": "Konstantin Kobs"
                },
                {
                    "authorId": "1738685382",
                    "name": "Blerta Veseli"
                },
                {
                    "authorId": "1393517268",
                    "name": "Franziska Westermeier"
                },
                {
                    "authorId": "1396331957",
                    "name": "Larissa Br\u00fcbach"
                },
                {
                    "authorId": "2038535684",
                    "name": "D. Roth"
                },
                {
                    "authorId": "2142582172",
                    "name": "M. Latoschik"
                },
                {
                    "authorId": "1792623",
                    "name": "A. Hotho"
                }
            ]
        }
    ]
}