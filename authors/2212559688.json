{
    "authorId": "2212559688",
    "papers": [
        {
            "paperId": "0706d3d5c43269d812d5f931c38fb31b22ef3248",
            "title": "Averaging Rate Scheduler for Decentralized Learning on Heterogeneous Data",
            "abstract": "State-of-the-art decentralized learning algorithms typically require the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the data distribution across the agents can have significant heterogeneity. In this work, we propose averaging rate scheduling as a simple yet effective way to reduce the impact of heterogeneity in decentralized learning. Our experiments illustrate the superiority of the proposed method (~3% improvement in test accuracy) compared to the conventional approach of employing a constant averaging rate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "2242959790",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "6bae288e831c26e0fa8724578558daa2cd0f4780",
            "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
            "abstract": "Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059786774",
                    "name": "Utkarsh Saxena"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "2268786741",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "a94299289608efa10252a205ce2c12f109e0f7bb",
            "title": "SADDLe: Sharpness-Aware Decentralized Deep Learning with Heterogeneous Data",
            "abstract": "Decentralized training enables learning with distributed datasets generated at different locations without relying on a central server. In realistic scenarios, the data distribution across these sparsely connected learning agents can be significantly heterogeneous, leading to local model over-fitting and poor global model generalization. Another challenge is the high communication cost of training models in such a peer-to-peer fashion without any central coordination. In this paper, we jointly tackle these two-fold practical challenges by proposing SADDLe, a set of sharpness-aware decentralized deep learning algorithms. SADDLe leverages Sharpness-Aware Minimization (SAM) to seek a flatter loss landscape during training, resulting in better model generalization as well as enhanced robustness to communication compression. We present two versions of our approach and conduct extensive experiments to show that SADDLe leads to 1-20% improvement in test accuracy compared to other existing techniques. Additionally, our proposed approach is robust to communication compression, with an average drop of only 1% in the presence of up to 4x compression.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "2242959790",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "14fc27f151a12e141e8cc84da050e64b8d90eb56",
            "title": "CoDeC: Communication-Efficient Decentralized Continual Learning",
            "abstract": "Training at the edge utilizes continuously evolving data generated at different locations. Privacy concerns prohibit the co-location of this spatially as well as temporally distributed data, deeming it crucial to design training algorithms that enable efficient continual learning over decentralized private data. Decentralized learning allows serverless training with spatially distributed data. A fundamental barrier in such distributed learning is the high bandwidth cost of communicating model updates between agents. Moreover, existing works under this training paradigm are not inherently suitable for learning a temporal sequence of tasks while retaining the previously acquired knowledge. In this work, we propose CoDeC, a novel communication-efficient decentralized continual learning algorithm which addresses these challenges. We mitigate catastrophic forgetting while learning a task sequence in a decentralized learning setup by combining orthogonal gradient projection with gossip averaging across decentralized agents. Further, CoDeC includes a novel lossless communication compression scheme based on the gradient subspaces. We express layer-wise gradients as a linear combination of the basis vectors of these gradient subspaces and communicate the associated coefficients. We theoretically analyze the convergence rate for our algorithm and demonstrate through an extensive set of experiments that CoDeC successfully learns distributed continual tasks with minimal forgetting. The proposed compression scheme results in up to 4.8x reduction in communication costs with iso-performance as the full communication baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "addc01be10f5bfa132cf9111cdff5584e4af3132",
            "title": "A 65 nm 1.4-6.7 TOPS/W Adaptive-SNR Sparsity-Aware CIM Core with Load Balancing Support for DL workloads",
            "abstract": "The growing trends of developing domain-specific accelerators for Deep Learning (DL) applications has led to exploration of compute-in-memory (CIM) primitives based on SRAM [1] \u2013[5]. Multiple research chips have demonstrated macro and core-level designs supporting multi-bit Matrix-Vector Multiplication (MVM) and sparsity to increase energy-efficiency and performance. However, CIM designs suffer from the following challenges, as shown in Fig. 1: (1) Difficulty in leveraging both input and weight unstructured sparsity in existing DL accelerators. Note, unstructured sparsity is more amenable during DL model training than structured sparsity. Fig. 1 (top) shows input and weight bit-level sparsity of ResNet20 running a CIFAR10 task and mapped on a $64 \\times 64 \\mathrm{CIM}$ macro. We observe that activations and weights of each layer experience different bit-level sparsity, also, sparsity levels vary significantly across layers.(2) Mixed-signal CIM macros suffer from noise and variation-based computation errors and signal-to-noise ratio (SNR) degradation. Moreover, the macro errors get accumulated in scaled-up CIM architectures leading to significant model accuracy drop. (3) Sparsity-aware CIM compute units encounter different sparsity; hence they might finish their corresponding MVMs at different times leading to load imbalance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120861201",
                    "name": "M. Ali"
                },
                {
                    "authorId": "1832280",
                    "name": "I. Chakraborty"
                },
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "51231183",
                    "name": "Muya Chang"
                },
                {
                    "authorId": "2155184371",
                    "name": "Dong Eun Kim"
                },
                {
                    "authorId": "10225325",
                    "name": "A. Raychowdhury"
                },
                {
                    "authorId": "2257216834",
                    "name": "K. Roy"
                }
            ]
        }
    ]
}