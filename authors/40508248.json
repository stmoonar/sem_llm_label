{
    "authorId": "40508248",
    "papers": [
        {
            "paperId": "38ccfbb4d36615375d2c95d877a4496f393092c8",
            "title": "Physics-based Motion Retargeting from Sparse Inputs",
            "abstract": "Avatars are important to create interactive and immersive experiences in virtual worlds. One challenge in animating these characters to mimic a user's motion is that commercial AR/VR products consist only of a headset and controllers, providing very limited sensor data of the user's pose. Another challenge is that an avatar might have a different skeleton structure than a human and the mapping between them is unclear. In this work we address both of these challenges. We introduce a method to retarget motions in real-time from sparse human sensor data to characters of various morphologies. Our method uses reinforcement learning to train a policy to control characters in a physics simulator. We only require human motion capture data for training, without relying on artist-generated animations for each avatar. This allows us to use large motion capture datasets to train general policies that can track unseen users from real and sparse data in real-time. We demonstrate the feasibility of our approach on three characters with different skeleton structure: a dinosaur, a mouse-like creature and a human. We show that the avatar poses often match the user surprisingly well, despite having no sensor information of the lower body available. We discuss and ablate the important components in our framework, specifically the kinematic retargeting step, the imitation, contact and action reward as well as our asymmetric actor-critic observations. We further explore the robustness of our method in a variety of settings including unbalancing, dancing and sports motions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51134127",
                    "name": "Daniele Reda"
                },
                {
                    "authorId": "2497902",
                    "name": "Jungdam Won"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                },
                {
                    "authorId": "1745029",
                    "name": "M. V. D. Panne"
                },
                {
                    "authorId": "40238512",
                    "name": "Alexander W. Winkler"
                }
            ]
        },
        {
            "paperId": "6a6cb107b59d8d8cb119320de85491ebe62a15ea",
            "title": "Simulation and Retargeting of Complex Multi-Character Interactions",
            "abstract": "We present a method for reproducing complex multi-character interactions for physically simulated humanoid characters using deep reinforcement learning. Our method learns control policies for characters that imitate not only individual motions, but also the interactions between characters, while maintaining balance and matching the complexity of reference data. Our approach uses a novel reward formulation based on an interaction graph that measures distances between pairs of interaction landmarks. This reward encourages control policies to efficiently imitate the character\u2019s motion while preserving the spatial relationships of the interactions in the reference motion. We evaluate our method on a variety of activities, from simple interactions such as a high-five greeting to more complex interactions such as gymnastic exercises, Salsa dancing, and box carrying and throwing. This approach can be used to \u201cclean-up\u201d existing motion capture data to produce physically plausible interactions or to retarget motion to new characters with different sizes, kinematics or morphologies while maintaining the interactions in the original data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108285123",
                    "name": "Yunbo Zhang"
                },
                {
                    "authorId": "32167409",
                    "name": "D. Gopinath"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                },
                {
                    "authorId": "1788773",
                    "name": "J. Hodgins"
                },
                {
                    "authorId": "1713189",
                    "name": "Greg Turk"
                },
                {
                    "authorId": "2497902",
                    "name": "Jungdam Won"
                }
            ]
        },
        {
            "paperId": "93de2ac102ba00c34facef64350736a8659e42d4",
            "title": "QuestEnvSim: Environment-Aware Simulated Motion Tracking from Sparse Sensors",
            "abstract": "Replicating a user\u2019s pose from only wearable sensors is important for many AR/VR applications. Most existing methods for motion tracking avoid environment interaction apart from foot-floor contact due to their complex dynamics and hard constraints. However, in daily life people regularly interact with their environment, e.g. by sitting on a couch or leaning on a desk. Using Reinforcement Learning, we show that headset and controller pose, if combined with physics simulation and environment observations can generate realistic full-body poses even in highly constrained environments. The physics simulation automatically enforces the various constraints necessary for realistic poses, instead of manually specifying them as in many kinematic approaches. These hard constraints allow us to achieve high-quality interaction motions without typical artifacts such as penetration or contact sliding. We discuss three features, the environment representation, the contact reward and scene randomization, crucial to the performance of the method. We demonstrate the generality of the approach through various examples, such as sitting on chairs, a couch and boxes, stepping over boxes, rocking a chair and turning an office chair. We believe these are some of the highest-quality results achieved for motion tracking from sparse sensor with scene interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108213418",
                    "name": "Sunmin Lee"
                },
                {
                    "authorId": "3387008",
                    "name": "S. Starke"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                },
                {
                    "authorId": "2497902",
                    "name": "Jungdam Won"
                },
                {
                    "authorId": "40238512",
                    "name": "Alexander W. Winkler"
                }
            ]
        },
        {
            "paperId": "a1884e3378894c73dcae7170132cc7dcfa34dd8a",
            "title": "How Important are Detailed Hand Motions for Communication for a Virtual Character Through the Lens of Charades?",
            "abstract": "Detailed hand motions play an important role in face-to-face communication to emphasize points, describe objects, clarify concepts, or replace words altogether. While shared virtual reality (VR) spaces are becoming more popular, these spaces do not, in most cases, capture and display accurate hand motions. In this article, we investigate the consequences of such errors in hand and finger motions on comprehension, character perception, social presence, and user comfort. We conduct three perceptual experiments where participants guess words and movie titles based on motion captured movements. We introduce errors and alterations to the hand movements and apply techniques to synthesize or correct hand motions. We collect data from more than 1000 Amazon Mechanical Turk participants in two large experiments, and conduct a third experiment in VR. As results might differ depending on the virtual character used, we investigate all effects on two virtual characters of different levels of realism. We furthermore investigate the effects of clip length in our experiments. Amongst other results, we show that the absence of finger motion significantly reduces comprehension and negatively affects people\u2019s perception of a virtual character and their social presence. Adding some hand motions, even random ones, does attenuate some of these effects when it comes to the perception of the virtual character or social presence, but it does not necessarily improve comprehension. Slightly inaccurate or erroneous hand motions are sufficient to achieve the same level of comprehension as with accurate hand motions. They might however still affect the viewers\u2019 impression of a character. Finally, jittering hand motions should be avoided as they significantly decrease user comfort.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2074852184",
                    "name": "A. Adkins"
                },
                {
                    "authorId": "1682962",
                    "name": "Aline Normoyle"
                },
                {
                    "authorId": "2281825",
                    "name": "Lorraine Lin"
                },
                {
                    "authorId": "2117104129",
                    "name": "Yu Sun"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                },
                {
                    "authorId": "39186214",
                    "name": "Massimiliano Di Luca"
                },
                {
                    "authorId": "144892110",
                    "name": "S. J\u00f6rg"
                }
            ]
        },
        {
            "paperId": "af7e2ad5f4e91bbe5fe1e9ed31942647026e3c46",
            "title": "AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation",
            "abstract": "This paper presents a method that can quickly adapt dynamic 3D avatars to arbitrary text descriptions of novel styles. Among existing approaches for avatar stylization, direct optimization methods can produce excellent results for arbitrary styles but they are unpleasantly slow. Furthermore, they require redoing the optimization process from scratch for every new input. Fast approximation methods using feed-forward networks trained on a large dataset of style images can generate results for new inputs quickly, but tend not to generalize well to novel styles and fall short in quality. We therefore investigate a new approach, AlteredAvatar, that combines those two approaches using the meta-learning framework. In the inner loop, the model learns to optimize to match a single target style well; while in the outer loop, the model learns to stylize efficiently across many styles. After training, AlteredAvatar learns an initialization that can quickly adapt within a small number of update steps to a novel style, which can be given using texts, a reference image, or a combination of both. We show that AlteredAvatar can achieve a good balance between speed, flexibility and quality, while maintaining consistency across a wide range of novel views and facial expressions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279754290",
                    "name": "Thu Nguyen-Phuoc"
                },
                {
                    "authorId": "40116153",
                    "name": "Gabriel Schwartz"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                },
                {
                    "authorId": "153360678",
                    "name": "Stephen Lombardi"
                },
                {
                    "authorId": "2144478697",
                    "name": "Lei Xiao"
                }
            ]
        },
        {
            "paperId": "bae48ad93e4668b84fc6c96983e0fcb4cbee6a6d",
            "title": "DROP: Dynamics Responses from Human Motion Prior and Projective Dynamics",
            "abstract": "Synthesizing realistic human movements, dynamically responsive to the environment, is a long-standing objective in character animation, with applications in computer vision, sports, and healthcare, for motion prediction and data augmentation. Recent kinematics-based generative motion models offer impressive scalability in modeling extensive motion data, albeit without an interface to reason about and interact with physics. While simulator-in-the-loop learning approaches enable highly physically realistic behaviors, the challenges in training often affect scalability and adoption. We introduce DROP, a novel framework for modeling Dynamics Responses of humans using generative mOtion prior and Projective dynamics. DROP can be viewed as a highly stable, minimalist physics-based human simulator that interfaces with a kinematics-based generative motion prior. Utilizing projective dynamics, DROP allows flexible and simple integration of the learned motion prior as one of the projective energies, seamlessly incorporating control provided by the motion prior with Newtonian dynamics. Serving as a model-agnostic plug-in, DROP enables us to fully leverage recent advances in generative motion models for physics-based motion synthesis. We conduct extensive evaluations of our model across different motion tasks and various physical perturbations, demonstrating the scalability and diversity of responses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146419599",
                    "name": "Yifeng Jiang"
                },
                {
                    "authorId": "2497902",
                    "name": "Jungdam Won"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                },
                {
                    "authorId": "2282970129",
                    "name": "C. Liu"
                }
            ]
        },
        {
            "paperId": "ce211df49cb2fc836d7d49c628c63df291f96a6b",
            "title": "Learning to Transfer In\u2010Hand Manipulations Using a Greedy Shape Curriculum",
            "abstract": "In\u2010hand object manipulation is challenging to simulate due to complex contact dynamics, non\u2010repetitive finger gaits, and the need to indirectly control unactuated objects. Further adapting a successful manipulation skill to new objects with different shapes and physical properties is a similarly challenging problem. In this work, we show that natural and robust in\u2010hand manipulation of simple objects in a dynamic simulation can be learned from a high quality motion capture example via deep reinforcement learning with careful designs of the imitation learning problem. We apply our approach on both single\u2010handed and two\u2010handed dexterous manipulations of diverse object shapes and motions. We then demonstrate further adaptation of the example motion to a more complex shape through curriculum learning on intermediate shapes morphed between the source and target object. While a naive curriculum of progressive morphs often falls short, we propose a simple greedy curriculum search algorithm that can successfully apply to a range of objects such as a teapot, bunny, bottle, train, and elephant.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108285123",
                    "name": "Yunbo Zhang"
                },
                {
                    "authorId": "30933599",
                    "name": "Alexander Clegg"
                },
                {
                    "authorId": "2248552",
                    "name": "Sehoon Ha"
                },
                {
                    "authorId": "1713189",
                    "name": "Greg Turk"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                }
            ]
        },
        {
            "paperId": "6239a83689d30f769c09c874003cd843750343b1",
            "title": "QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars",
            "abstract": "Real-time tracking of human body motion is crucial for interactive and immersive experiences in AR/VR. However, very limited sensor data about the body is available from standalone wearable devices such as HMDs (Head Mounted Devices) or AR glasses. In this work, we present a reinforcement learning framework that takes in sparse signals from an HMD and two controllers, and simulates plausible and physically valid full body motions. Using high quality full body motion as dense supervision during training, a simple policy network can learn to output appropriate torques for the character to balance, walk, and jog, while closely following the input signals. Our results demonstrate surprisingly similar leg motions to ground truth without any observations of the lower body, even when the input is only the 6D transformations of the HMD. We also show that a single policy can be robust to diverse locomotion styles, different body sizes, and novel environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40238512",
                    "name": "Alexander W. Winkler"
                },
                {
                    "authorId": "2497902",
                    "name": "Jungdam Won"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                }
            ]
        },
        {
            "paperId": "70d502019db3dc182d99331a4de07d437e97474d",
            "title": "Transformer Inertial Poser: Attention-based Real-time Human Motion Reconstruction from Sparse IMUs",
            "abstract": "smaller in size. We evaluate our method extensively on synthesized and real IMU data, and with real-time live demos.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146419599",
                    "name": "Yifeng Jiang"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                },
                {
                    "authorId": "32167409",
                    "name": "D. Gopinath"
                },
                {
                    "authorId": "2497902",
                    "name": "Jungdam Won"
                },
                {
                    "authorId": "40238512",
                    "name": "Alexander W. Winkler"
                },
                {
                    "authorId": "2282970129",
                    "name": "C. Liu"
                }
            ]
        },
        {
            "paperId": "d33b3fa98fe353baf1ba5d6a243aff862e2a8c11",
            "title": "Transformer Inertial Poser: Real-time Human Motion Reconstruction from Sparse IMUs with Simultaneous Terrain Generation",
            "abstract": "Real-time human motion reconstruction from a sparse set of (e.g. six) wearable IMUs provides a non-intrusive and economic approach to motion capture. Without the ability to acquire position information directly from IMUs, recent works took data-driven approaches that utilize large human motion datasets to tackle this under-determined problem. Still, challenges remain such as temporal consistency, drifting of global and joint motions, and diverse coverage of motion types on various terrains. We propose a novel method to simultaneously estimate full-body motion and generate plausible visited terrain from only six IMU sensors in real-time. Our method incorporates 1. a conditional Transformer decoder model giving consistent predictions by explicitly reasoning prediction history, 2. a simple yet general learning target named \"stationary body points\u201d (SBPs) which can be stably predicted by the Transformer model and utilized by analytical routines to correct joint and global drifting, and 3. an algorithm to generate regularized terrain height maps from noisy SBP predictions which can in turn correct noisy global motion estimation. We evaluate our framework extensively on synthesized and real IMU data, and with real-time live demos, and show superior performance over strong baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146419599",
                    "name": "Yifeng Jiang"
                },
                {
                    "authorId": "40508248",
                    "name": "Yuting Ye"
                },
                {
                    "authorId": "32167409",
                    "name": "D. Gopinath"
                },
                {
                    "authorId": "2497902",
                    "name": "Jungdam Won"
                },
                {
                    "authorId": "40238512",
                    "name": "Alexander W. Winkler"
                },
                {
                    "authorId": "2282970129",
                    "name": "C. Liu"
                }
            ]
        }
    ]
}