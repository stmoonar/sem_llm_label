{
    "authorId": "2075461978",
    "papers": [
        {
            "paperId": "2c82c551b151835cec78df7a2ab86f2a58d0a682",
            "title": "Acoustic Word Embeddings for Untranscribed Target Languages with Continued Pretraining and Learned Pooling",
            "abstract": "Acoustic word embeddings are typically created by training a pooling function using pairs of word-like units. For unsupervised systems, these are mined using k-nearest neighbor (KNN) search, which is slow. Recently, mean-pooled representations from a pre-trained self-supervised English model were suggested as a promising alternative, but their performance on target languages was not fully competitive. Here, we explore improvements to both approaches: we use continued pre-training to adapt the self-supervised model to the target language, and we use a multilingual phone recognizer (MPR) to mine phone n-gram pairs for training the pooling function. Evaluating on four languages, we show that both methods outperform a recent approach on word discrimination. Moreover, the MPR method is orders of magnitude faster than KNN, and is highly data efficient. We also show a small improvement from performing learned pooling on top of the continued pre-trained representations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "3285011",
                    "name": "Ondrej Klejch"
                },
                {
                    "authorId": "40860114",
                    "name": "Hao Tang"
                },
                {
                    "authorId": "1991315",
                    "name": "S. Goldwater"
                }
            ]
        },
        {
            "paperId": "30bc28ed9fb1b211698be9a3946a92952002af93",
            "title": "The Edinburgh International Accents of English Corpus: Towards the Democratization of English ASR",
            "abstract": "English is the most widely spoken language in the world, used daily by millions of people as a first or second language in many different contexts. As a result, there are many varieties of English. Although the great many advances in English automatic speech recognition (ASR) over the past decades, results are usually reported based on test datasets which fail to represent the diversity of English as spoken today around the globe. We present the first release of The Edinburgh International Accents of English Corpus (EdAcc). This dataset attempts to better represent the wide diversity of English, encompassing almost 40 hours of dyadic video call conversations between friends. Unlike other datasets, EdAcc includes a wide range of first and second-language varieties of English and a linguistic background profile of each speaker. Results on latest public, and commercial models show that EdAcc highlights shortcomings of current English ASR models. The best performing model, trained on 680 thousand hours of transcribed data, obtains an average of 19.7% word error rate (WER) \u2013 in contrast to the 2.7% WER obtained when evaluated on US English clean read speech. Across all models, we observe a drop in performance on Indian, Jamaican, and Nigerian English speakers. Recordings, linguistic backgrounds, data statement, and evaluation scripts are released on our website under CC-BY-SA1 license.2 We hope that this work will encourage future research on a wider range of English varieties to create more accessible speech technologies.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "3444222",
                    "name": "Nikolay Bogoychev"
                },
                {
                    "authorId": "2238271025",
                    "name": "Nina Markl"
                },
                {
                    "authorId": "1387268922",
                    "name": "Andrea Carmantini"
                },
                {
                    "authorId": "3285011",
                    "name": "Ondrej Klejch"
                },
                {
                    "authorId": "144906854",
                    "name": "P. Bell"
                }
            ]
        },
        {
            "paperId": "b02a34c1cf63dec21e763d905e277ac021fa15a4",
            "title": "Measuring the Impact of Domain Factors in Self-Supervised Pre-Training",
            "abstract": "Human speech data comprises a rich set of domain factors such as accent, syntactic and semantic variety, or acoustic environment. Previous work explores the effect of domain mismatch in automatic speech recognition between pre-training and fine-tuning as a whole [1] but does not dissect the contribution of individual factors. In this paper, we present a controlled study to better understand the effect of such factors on the performance of pre-trained representations on automatic speech recognition. To do so, we pre-train models either on modified natural speech or synthesized audio, with a single domain factor modified, and then measure performance after fine-tuning. Results show that phonetic domain factors play an important role during pre-training while grammatical and syntactic factors are far less important. To our knowledge, this is the first study to better understand the domain characteristics of pre-trained sets in self-supervised pre-training for speech.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "2957796",
                    "name": "Wei-Ning Hsu"
                },
                {
                    "authorId": "14667698",
                    "name": "Alexei Baevski"
                },
                {
                    "authorId": "2325985",
                    "name": "Michael Auli"
                }
            ]
        },
        {
            "paperId": "c48f9906377e3af8fd0b98bf4d77c37841b9778c",
            "title": "Analyzing Acoustic Word Embeddings from Pre-Trained Self-Supervised Speech Models",
            "abstract": "Given the strong results of self-supervised models on various tasks, there have been surprisingly few studies exploring self-supervised representations for acoustic word embeddings (AWE), fixed-dimensional vectors representing variable-length spoken word segments. In this work, we study several pre-trained models and pooling methods for constructing AWEs with self-supervised representations. Owing to the contextualized nature of self-supervised representations, we hy-pothesize that simple pooling methods, such as averaging, might already be useful for constructing AWEs. When evaluating on a standard word discrimination task, we find that HuBERT representations with mean-pooling rival the state of the art on English AWEs. More surprisingly, despite being trained only on English, HuBERT representations evaluated on Xitsonga, Mandarin, and French consistently outperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on English).",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "40860114",
                    "name": "Hao Tang"
                },
                {
                    "authorId": "1991315",
                    "name": "S. Goldwater"
                }
            ]
        },
        {
            "paperId": "448ce0fa2990fa3e9d48313aedb4981455b044b9",
            "title": "On the Difficulty of Segmenting Words with Attention",
            "abstract": "Word segmentation, the problem of finding word boundaries in speech, is of interest for a range of tasks. Previous papers have suggested that for sequence-to-sequence models trained on tasks such as speech translation or speech recognition, attention can be used to locate and segment the words. We show, however, that even on monolingual data this approach is brittle. In our experiments with different input types, data sizes, and segmentation algorithms, only models trained to predict phones from words succeed in the task. Models trained to predict words from either phones or speech (i.e., the opposite direction needed to generalize to new data), yield much worse results, suggesting that attention-based segmentation is only useful in limited scenarios.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "40860114",
                    "name": "Hao Tang"
                },
                {
                    "authorId": "1991315",
                    "name": "S. Goldwater"
                }
            ]
        },
        {
            "paperId": "7fda2c0a086541b65038c39cb3aca7d2e40c0fa5",
            "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval",
            "abstract": "Speech-based image retrieval has been studied as a proxy for joint representation learning, usually without emphasis on retrieval itself. As such, it is unclear how well speech-based retrieval can work in practice -- both in an absolute sense and versus alternative strategies that combine automatic speech recognition (ASR) with strong text encoders. In this work, we extensively study and expand choices of encoder architectures, training methodology (including unimodal and multimodal pretraining), and other factors. Our experiments cover different types of speech in three datasets: Flickr Audio, Places Audio, and Localized Narratives. Our best model configuration achieves large gains over state of the art, e.g., pushing recall-at-one from 21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also show our best speech-based models can match or exceed cascaded ASR-to-text encoding when speech is spontaneous, accented, or otherwise hard to automatically transcribe.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "40418206",
                    "name": "Austin Waters"
                },
                {
                    "authorId": "1387994164",
                    "name": "Jason Baldridge"
                }
            ]
        },
        {
            "paperId": "14aadd24040edaa9ce9978b53b00aeede015f859",
            "title": "Grounded Sequence to Sequence Transduction",
            "abstract": "Speech recognition and machine translation have made major progress over the past decades, providing practical systems to map one language sequence to another. Although multiple modalities such as sound and video are becoming increasingly available, the state-of-the-art systems are inherently unimodal, in the sense that they take a single modality \u2014 either speech or text \u2014 as input. Evidence from human learning suggests that additional modalities can provide disambiguating signals crucial for many language tasks. In this article, we describe the How2 dataset\u00a0, a large, open-domain collection of videos with transcriptions and their translations. We then show how this single dataset can be used to develop systems for a variety of language tasks and present a number of models meant as starting points. Across tasks, we find that building multimodal architectures that perform better than their unimodal counterpart remains a challenge. This leaves plenty of room for the exploration of more advanced solutions that fully exploit the multimodal nature of the How2 dataset\u00a0, and the general direction of multimodal learning with other datasets as well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702974",
                    "name": "Lucia Specia"
                },
                {
                    "authorId": "143886499",
                    "name": "Raman Arora"
                },
                {
                    "authorId": "2934336",
                    "name": "Lo\u00efc Barrault"
                },
                {
                    "authorId": "10791325",
                    "name": "Ozan Caglayan"
                },
                {
                    "authorId": "1381525702",
                    "name": "A. Duarte"
                },
                {
                    "authorId": "50369944",
                    "name": "Desmond Elliott"
                },
                {
                    "authorId": "2921001",
                    "name": "Spandana Gella"
                },
                {
                    "authorId": "2269457539",
                    "name": "Nils Holzenberger"
                },
                {
                    "authorId": "1908331",
                    "name": "Chiraag Lala"
                },
                {
                    "authorId": "2108140944",
                    "name": "S. Lee"
                },
                {
                    "authorId": "3448602",
                    "name": "Jind\u0159ich Libovick\u00fd"
                },
                {
                    "authorId": "3238408",
                    "name": "P. Madhyastha"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "2065159752",
                    "name": "Karl Mulligan"
                },
                {
                    "authorId": "1752987954",
                    "name": "Alissa Ostapenka"
                },
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "2635321",
                    "name": "Josiah Wang"
                }
            ]
        },
        {
            "paperId": "b5f3222572ca8f7cfb50848040d063a4d9069f42",
            "title": "Multimodal Speech Recognition with Unstructured Audio Masking",
            "abstract": "Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fixed set of words are systematically masked in the audio. In this paper, we simulate a more realistic masking scenario during model training, called RandWordMask, where the masking can occur for any word segment. Our experiments on the Flickr 8K Audio Captions Corpus show that multimodal ASR can generalize to recover different types of masked words in this unstructured masking setting. Moreover, our analysis shows that our models are capable of attending to the visual signal when the audio signal is corrupted. These results show that multimodal ASR systems can leverage the visual signal in more generalized noisy scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150324514",
                    "name": "Tejas Srinivasan"
                },
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "50369944",
                    "name": "Desmond Elliott"
                }
            ]
        },
        {
            "paperId": "cefadbc725429f43700bba049782541595a0ce34",
            "title": "Looking Enhances Listening: Recovering Missing Speech Using Images",
            "abstract": "Speech is understood better by using visual context; for this reason, there have been many attempts to use images to adapt automatic speech recognition (ASR) systems. Current work, however, has shown that visually adapted ASR models only use images as a regularization signal, while completely ignoring their semantic content. In this paper, we present a set of experiments where we show the utility of the visual modality under noisy conditions. Our results show that multimodal ASR models can recover words which are masked in the input acoustic signal, by grounding its transcriptions using the visual representations. We observe that integrating visual context can result in up to 35% relative improvement in masked word recovery. These results demonstrate that end-to-end multimodal ASR systems can become more robust to noise by leveraging the visual context.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "150324514",
                    "name": "Tejas Srinivasan"
                },
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                }
            ]
        },
        {
            "paperId": "fc34c448b8b01be06716ec6f508ce262adbb5f53",
            "title": "Fine-Grained Grounding for Multimodal Speech Recognition",
            "abstract": "Multimodal automatic speech recognition systems integrate information from images to improve speech recognition quality, by grounding the speech in the visual context. While visual signals have been shown to be useful for recovering entities that have been masked in the audio, these models should be capable of recovering a broader range of word types. Existing systems rely on global visual features that represent the entire image, but localizing the relevant regions of the image will make it possible to recover a larger set of words, such as adjectives and verbs. In this paper, we propose a model that uses finer-grained visual information from different parts of the image, using automatic object proposals. In experiments on the Flickr8K Audio Captions Corpus, we find that our model improves over approaches that use global visual features, that the proposals enable the model to recover entities and other related words, such as adjectives, and that improvements are due to the model\u2019s ability to localize the correct proposals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150324514",
                    "name": "Tejas Srinivasan"
                },
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "50369944",
                    "name": "Desmond Elliott"
                }
            ]
        }
    ]
}