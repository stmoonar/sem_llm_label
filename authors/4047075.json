{
    "authorId": "4047075",
    "papers": [
        {
            "paperId": "2b054eb2be8fbb261503370323e4442602eddeed",
            "title": "Fact Ranking over Large-Scale Knowledge Graphs with Reasoning Embedding Models",
            "abstract": "Knowledge graphs (KGs) serve as the backbone of many applications such as recommendation systems and question answering. All these applications require reasoning about the relevance of facts in a KG to downstream applications. In this work, we describe our efforts in building a solution to reason about the importance of facts over continuously updated industry-scale KGs. We focus on the problem of fact ranking and evaluate to what extent modern knowledge graph embedding (KGE) models provide a representation for addressing this problem. To this end, we discuss unique challenges associated with solving this task in industrial settings and evaluate how accurately different KGE models and text-based embedding models can solve the problem of fact ranking.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40046694",
                    "name": "Hongyu Ren"
                },
                {
                    "authorId": "143661472",
                    "name": "A. Mousavi"
                },
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "2087061163",
                    "name": "S. R. Chowdhury"
                },
                {
                    "authorId": "2047146404",
                    "name": "J. Mohoney"
                },
                {
                    "authorId": "1743316",
                    "name": "Ihab F. Ilyas"
                },
                {
                    "authorId": "1718694",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                }
            ]
        },
        {
            "paperId": "4f662c42b2bd52422b70d8f44175f33cec4f8969",
            "title": "High-Throughput Vector Similarity Search in Knowledge Graphs",
            "abstract": "There is an increasing adoption of machine learning for encoding data into vectors to serve online recommendation and search use cases. As a result, recent data management systems propose augmenting query processing with online vector similarity search. In this work, we explore vector similarity search in the context of Knowledge Graphs (KGs). Motivated by the tasks of finding related KG queries and entities for past KG query workloads, we focus on hybrid vector similarity search (hybrid queries for short) where part of the query corresponds to vector similarity search and part of the query corresponds to predicates over relational attributes associated with the underlying data vectors. For example, given past KG queries for a song entity, we want to construct new queries for new song entities whose vector representations are close to the vector representation of the entity in the past KG query. But entities in a KG also have non-vector attributes such as a song associated with an artist, a genre, and a release date. Therefore, suggested entities must also satisfy query predicates over non-vector attributes beyond a vector-based similarity predicate. While these tasks are central to KGs, our contributions are generally applicable to hybrid queries. In contrast to prior works that optimize online queries, we focus on enabling efficient batch processing of past hybrid query workloads. We present our system, HQI, for high-throughput batch processing of hybrid queries. We introduce a workload-aware vector data partitioning scheme to tailor the vector index layout to the given workload and describe a multi-query optimization technique to reduce the overhead of vector similarity computations. We evaluate our methods on industrial workloads and demonstrate that HQI yields a 31\u00d7 improvement in throughput for finding related KG queries compared to existing hybrid query processing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047146404",
                    "name": "J. Mohoney"
                },
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "2087061163",
                    "name": "S. R. Chowdhury"
                },
                {
                    "authorId": "143661472",
                    "name": "A. Mousavi"
                },
                {
                    "authorId": "1743316",
                    "name": "Ihab F. Ilyas"
                },
                {
                    "authorId": "1856878",
                    "name": "U. F. Minhas"
                },
                {
                    "authorId": "32546616",
                    "name": "Jeffrey Pound"
                },
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                }
            ]
        },
        {
            "paperId": "8156446f2b96186da13e775b8461e9cda2e45f24",
            "title": "Evaluating Complex Queries on Streaming Graphs",
            "abstract": "We study the problem of evaluating persistent queries over streaming graphs in a principled fashion. These queries need to be evaluated over unbounded and very high speed graph streams. We define a streaming graph data model and query model incorporating navigational queries, subgraph queries and paths as first-class citizens. To support this full-fledged query model we develop a streaming graph algebra that describes the precise semantics of persistent graph queries with their complex constructs. We present transformation rules and describe query formulation and plan generation for persistent graph queries over streaming graphs. Our implementation of a streaming graph query processor shows the feasibility of our approach and allows us to gauge the high performance gains obtained for query processing over streaming graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "35cb89e6ac4545d15256e220b5db4a9d140edb38",
            "title": "Regular Path Query Evaluation on Streaming Graphs",
            "abstract": "We study persistent query evaluation over streaming graphs, which is becoming increasingly important. We focus on navigational queries that determine if there exists a path between two entities that satisfies a user-specified constraint. We adopt the Regular Path Query (RPQ) model that specifies navigational patterns with labeled constraints. We propose deterministic algorithms to efficiently evaluate persistent RPQs under both arbitrary and simple path semantics in a uniform manner. Experimental analysis on real and synthetic streaming graphs shows that the proposed algorithms can process up to tens of thousands of edges per second and efficiently answer RPQs that are commonly used in real-world workloads.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "af3c1c507c5d4a8d227474527ccdca8ba7d29a03",
            "title": "Experimental Analysis of Streaming Algorithms for Graph Partitioning",
            "abstract": "We report a systematic performance study of streaming graph partitioning algorithms. Graph partitioning plays a crucial role in overall system performance as it has a significant impact on both load balancing and inter-machine communication. The streaming model for graph partitioning has recently gained attention due to its ability to scale to very large graphs with limited resources. The main objective of this study is to understand how the choice of graph partitioning algorithm affects system performance, resource usage and scalability. We focus on both offline graph analytics and online graph query workloads. The study considers both edge-cut and vertex-cut approaches. Our results show that the no partitioning algorithms performs best in all cases, and the choice of graph partitioning algorithm depends on: (i) type and degree distribution of the graph, (ii) characteristics of the workloads, and (iii) specific application requirements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "2e816dce52fef2782201cf3173a67e2878285283",
            "title": "Distribution-Aware Stream Partitioning for Distributed Stream Processing Systems",
            "abstract": "The performance of modern distributed stream processing systems is largely dependent on balanced distribution of the workload across cluster. Input streams with large, skewed domains pose challenges to these systems, especially for stateful applications. Key splitting, where state of a single key is partially maintained across multiple workers, is a simple yet effective technique to reduce load imbalance in such systems. However it comes with the cost of increased memory overhead which has been neglected by existing techniques so far. In this paper we present a novel stream partitioning algorithm for intra-operator parallelism which adapts to the underlying stream distribution in an online manner and provides near-optimal load imbalance with minimal memory overhead. Our technique relies on explicitly routing frequent items using a greedy heuristic which considers both load imbalance and space requirements. It uses hashing for in frequent items to keep the size of routing table small. Through extensive experimentation with real and synthetic datasets, we show that our proposed solution consistently provides near-optimal load imbalance and memory footprint over variety of distributions. Our experiments on Apache Storm show up to an order of magnitude increase in overall throughput and up to 80% space savings over state-of-the-art stream partitioning techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "d27e792f4dedcab1d425dfa2d1722e73871c8253",
            "title": "Apollo: Learning Query Correlations for Predictive Caching in Geo-Distributed Systems",
            "abstract": "The performance of modern geo-distributed database applications is increasingly dependent on remote access latencies. Systems that cache query results to bring data closer to clients are gaining popularity, but they do not dynamically learn and exploit access patterns in client workloads. We present a novel prediction framework that identifies and makes use of workload characteristics obtained from data access patterns to exploit query relationships within an application\u2019s database workload. We have designed and implemented this framework as Apollo, a system that learns query patterns and adaptively uses them to predict future queries and cache their results. Through extensive experimentation with two different benchmarks, we show that Apollo provides significant performance gains over popular caching solutions through reduced query response time. Our experiments demonstrate Apollo\u2019s robustness and scalability as a predictive cache for geo-distributed database applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39045083",
                    "name": "Brad Glasbergen"
                },
                {
                    "authorId": "2068738700",
                    "name": "Michael Abebe"
                },
                {
                    "authorId": "2176381",
                    "name": "K. Daudjee"
                },
                {
                    "authorId": "39345188",
                    "name": "Scott Foggo"
                },
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                }
            ]
        },
        {
            "paperId": "f43ce88130f12f27f61748b128399a556bb84991",
            "title": "A Semantic Transformation Methodology for the Secondary Use of Observational Healthcare Data in Postmarketing Safety Studies",
            "abstract": "Background: Utilization of the available observational healthcare datasets is key to complement and strengthen the postmarketing safety studies. Use of common data models (CDM) is the predominant approach in order to enable large scale systematic analyses on disparate data models and vocabularies. Current CDM transformation practices depend on proprietarily developed Extract\u2014Transform\u2014Load (ETL) procedures, which require knowledge both on the semantics and technical characteristics of the source datasets and target CDM. Purpose: In this study, our aim is to develop a modular but coordinated transformation approach in order to separate semantic and technical steps of transformation processes, which do not have a strict separation in traditional ETL approaches. Such an approach would discretize the operations to extract data from source electronic health record systems, alignment of the source, and target models on the semantic level and the operations to populate target common data repositories. Approach: In order to separate the activities that are required to transform heterogeneous data sources to a target CDM, we introduce a semantic transformation approach composed of three steps: (1) transformation of source datasets to Resource Description Framework (RDF) format, (2) application of semantic conversion rules to get the data as instances of ontological model of the target CDM, and (3) population of repositories, which comply with the specifications of the CDM, by processing the RDF instances from step 2. The proposed approach has been implemented on real healthcare settings where Observational Medical Outcomes Partnership (OMOP) CDM has been chosen as the common data model and a comprehensive comparative analysis between the native and transformed data has been conducted. Results: Health records of ~1 million patients have been successfully transformed to an OMOP CDM based database from the source database. Descriptive statistics obtained from the source and target databases present analogous and consistent results. Discussion and Conclusion: Our method goes beyond the traditional ETL approaches by being more declarative and rigorous. Declarative because the use of RDF based mapping rules makes each mapping more transparent and understandable to humans while retaining logic-based computability. Rigorous because the mappings would be based on computer readable semantics which are amenable to validation through logic-based inference methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "2563929",
                    "name": "Suat G\u00f6n\u00fcl"
                },
                {
                    "authorId": "2549611",
                    "name": "Ali Anil Sinaci"
                },
                {
                    "authorId": "145831481",
                    "name": "Mustafa Yuksel"
                },
                {
                    "authorId": "7143224",
                    "name": "Gokce Banu Laleci Erturkmen"
                }
            ]
        },
        {
            "paperId": "7549a6e03afdd42ae94c19235db076319eeb34f8",
            "title": "Do We Need Specialized Graph Databases?: Benchmarking Real-Time Social Networking Applications",
            "abstract": "With the advent of online social networks, there is an increasing demand for storage and processing of graph-structured data. Social networking applications pose new challenges to data management systems due to demand for real-time querying and manipulation of the graph structure. Recently, several systems specialized systems for graph-structured data have been introduced. However, whether we should abandon mature RDBMS technology for graph databases remains an ongoing discussion. In this paper we present an graph database benchmarking architecture built on the existing LDBC Social Network Benchmark. Our proposed architecture stresses the systems with an interactive transactional workload to better simulate the real-time nature of social networking applications. Using this improved architecture, we evaluated a selection of specialized graph databases, RDF stores, and RDBMSes adapted for graphs. We do not find that specialized graph databases provide definitively better performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "2064473584",
                    "name": "Alice Zhou"
                },
                {
                    "authorId": "145580839",
                    "name": "Jimmy J. Lin"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "c1673c649014217a0a8cc9eac72718b8b67981c3",
            "title": "Postmarketing Safety Study Tool: A Web Based, Dynamic, and Interoperable System for Postmarketing Drug Surveillance Studies",
            "abstract": "Postmarketing drug surveillance is a crucial aspect of the clinical research activities in pharmacovigilance and pharmacoepidemiology. Successful utilization of available Electronic Health Record (EHR) data can complement and strengthen postmarketing safety studies. In terms of the secondary use of EHRs, access and analysis of patient data across different domains are a critical factor; we address this data interoperability problem between EHR systems and clinical research systems in this paper. We demonstrate that this problem can be solved in an upper level with the use of common data elements in a standardized fashion so that clinical researchers can work with different EHR systems independently of the underlying information model. Postmarketing Safety Study Tool lets the clinical researchers extract data from different EHR systems by designing data collection set schemas through common data elements. The tool interacts with a semantic metadata registry through IHE data element exchange profile. Postmarketing Safety Study Tool and its supporting components have been implemented and deployed on the central data warehouse of the Lombardy region, Italy, which contains anonymized records of about 16 million patients with over 10-year longitudinal data on average. Clinical researchers in Roche validate the tool with real life use cases.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "6954768",
                    "name": "A. Sinaci"
                },
                {
                    "authorId": "4374326",
                    "name": "G. B. Laleci Erturkmen"
                },
                {
                    "authorId": "4764093",
                    "name": "Suat Gonul"
                },
                {
                    "authorId": "145831481",
                    "name": "Mustafa Yuksel"
                },
                {
                    "authorId": "47456859",
                    "name": "P. Invernizzi"
                },
                {
                    "authorId": "3897937",
                    "name": "B. Thakrar"
                },
                {
                    "authorId": "4047075",
                    "name": "Anil Pacaci"
                },
                {
                    "authorId": "2077205858",
                    "name": "H. A. Cinar"
                },
                {
                    "authorId": "1699030",
                    "name": "N. Cicekli"
                }
            ]
        }
    ]
}